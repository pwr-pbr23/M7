[{"number": 29693, "title": "Tflite results dont match corresponding Tensorflow for Mobilenet.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 and Ubuntu16.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (or github SHA if from source):1.13\r\n\r\n\r\n[mobilenet_tflite_test.zip](https://github.com/tensorflow/tensorflow/files/3281327/mobilenet_tflite_test.zip)\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nhttps://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz\r\n\r\n**Any other info / logs**\r\nDescription : I am trying to quantize the Mobilenet using tflite. After converting , i used tensorflow to validate my results. I find that quantized results are not matching to float tensorflow based results.\r\nI have an output for about 10 elements and maximum value index , both of them are mismatching as shown.\r\nPlease let me know whether i am missing any thing or is this real problem ?\r\n\r\n_--------------------Float values are --------------------------\r\n[-1.8004932  -0.23941718  0.61870384 -0.4235621  -1.1386781   0.99633265\r\n  0.16455668  0.743189   -1.9882091  -1.270742  ]\r\nMax Index  (array([420], dtype=int64),)\r\n---------Fixed pt values are -----------------------------------\r\n[-2. -8.  0. -6. -2. -6.  2. -2. -2. -2.]\r\nMax Index  (array([812], dtype=int64),)_\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@ignvinay We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29693\">No</a>\n"]}, {"number": 29692, "title": "ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.8\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n\r\n\r\n**Describe the problem**\r\nAll of my Tensorflow codes work fine for version 1.5 but fail when I upgraded it to 1.8. The error is shown below:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\SVNRepo\\Python_codes\\scratch.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThe error occurs by only running `import tensorflow as tf`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@biggytruck I recommend you to uninstall older tensorflow and python version and install new tensorflow 1.8 and python. It worked for many users. Some time when you have installed different versions of python and tensorflow, there will be some modules that could affect reinstalling of TF. Follow the official [website](https://www.tensorflow.org/install/pip) to install Tensorflow. Let us know if that resolves. Thanks! ", "@gadagashwini I've uninstalled tensorflow using `pip uninstall tensorflow` and python and reinstalled them, but the same error still occurs.", "@biggytruck Can you follow steps 3,4, and 5 from [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). Then, Open a command prompt and type \"pip install tensorflow==1.8\". This should install tensorflow. Please let me know how it progresses. Thanks!"]}, {"number": 29691, "title": "Add tf.linalg.normalize", "body": "As discussed in #28741.\r\n\r\nNote: Now calling tf.linalg.l2_normalize(x) is identical to tf.linalg.normalize(x, axis=None).\r\nI did not peform any changes to l2_normalize to not break anything.\r\n\r\nThe only thing that would break here is the name scoping as that would result in l2_normalize>normalize>norm instead of just l2_normalize.\r\n\r\nRegarding the test. This test is based on the the test provided for tf.norm. I can change nn_test.py to add individual tests for each case if required.", "comments": ["@alextp Re-opened PR", "I do not see the connection between the failing tests and what I changed.", "@sleighsoft  Can you please check Ubuntu Sanity errors and keep us posted. Thanks!", "> \r\n> \r\n> @sleighsoft Can you please check Ubuntu Sanity errors and keep us posted. Thanks!\r\n\r\nThat one escaped me. Fixed the wrong indentations.", "@alextp norm_op_test.py has the following additional tags in BUILD\r\n```\r\n    tags = [\r\n        \"no_windows_gpu\",\r\n        \"nomsan\",\r\n    ],\r\n```\r\nMaybe I have to add them as well. Even though, again, I don't understand why.\r\n`nosman` is not very descriptive and why `tf.norm` has `no_windows_gpu` is unclear as well.", "Thank you for clarifying `nomsan`. Regarding the `no_windows_gpu`. I was wondering why it is there in the first place. Some test seem to fail again. I don't see my code being the reason in the logs.", "Any ideas why `Windows Bazel` and `MacOS Contrib` fail? ", "Any status update here?", "The windows failure looks like an unrelated flake, `ERROR: T:/src/github/tensorflow/tensorflow/python/tools/BUILD:140:1 C++ compilation of rule '//tensorflow/core/util/tensor_bundle:tensor_bundle' failed (Exit 2): cl.exe failed: error executing command`, so rerunning.", "@gbaned why request another review? I have already approved the latest snapshot of this", "> @gbaned why request another review? I have already approved the latest snapshot of this\r\n\r\n@alextp My apologies for that. Thank you."]}, {"number": 29690, "title": "bugfix: functionalize_control_flow_pass_registration.cc has been moved to another dir", "body": "functionalize_control_flow_pass_registration.cc has been moved from third_party/tensorflow/compiler/tf2xla/  to tensorflow/compiler/tf2xla/ in the current version", "comments": []}, {"number": 29689, "title": "Sharing output of one model as input for other models  ", "body": "I am trying something like where i have three models named model_1, model_2 and model_3. I want to use output of model_1 as input to model_2 and model_3. These models are running in separate threads parallelly. How can i do this? \r\n\r\nI stored output Tensor of model_1 in a queue and tried to get data for model_2 and model_3 from there using locks.  But it failed please help in solving this issue. \r\nHere is my code:\r\n\r\n```  python\r\nimport tensorflow as tf\r\nimport threading\r\nimport queue\r\n\r\ninputQ = queue.Queue()\r\noutputQ = queue.Queue()\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n\r\nlearning_rate = 0.001\r\nnum_steps = 300\r\nbatch_size = 128\r\ndisplay_step = 100\r\n\r\ninput = 784 # MNIST data input (img shape: 28*28)\r\nn_classes = 10 # MNIST total classes (0-9 digits)\r\ndropout = 0.75 # Dropout, probability to keep units\r\n\r\n\r\n\r\ng1 = tf.Graph()\r\nsess1 = tf.Session(graph=g1)\r\n\r\nwith g1.as_default():\r\n    dataset = tf.data.Dataset.from_tensor_slices(\r\n        (mnist.train.images, mnist.train.labels))\r\n\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.prefetch(batch_size)\r\n    iterator = dataset.make_initializable_iterator()\r\n    sess1.run(iterator.initializer)\r\n    X, Y = iterator.get_next()\r\n\r\n\r\ndef model_1 (g2,sess,x, n_classes, dropout, reuse):\r\n        with g2.as_default(), sess.as_default():\r\n            with tf.variable_scope('ConvNet', reuse=reuse):\r\n                    x = tf.reshape(x, shape=[-1, 28, 28, 1])\r\n                    conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\r\n                    conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\r\n                    conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\r\n                    conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\r\n                    fc1 = tf.contrib.layers.flatten(conv2)\r\n                    init = tf.global_variables_initializer()\r\n                    sess.run(init)\r\n                    sess.run(fc1)\r\n                    inputQ.put(fc1)\r\n\r\ndef model_2 (g2,sess,n_classes, dropout, reuse, is_training):\r\n    with g2.as_default(), sess.as_default():\r\n        with tf.variable_scope('ConvNet2', reuse=reuse):\r\n            fc1 = inputQ.queue[0]\r\n            fc1 = tf.layers.dense(fc1, 1024)\r\n            fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\r\n            out = tf.layers.dense(fc1, n_classes)\r\n            out = tf.nn.softmax(out) if not is_training else out\r\n            loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\r\n                        logits=out, labels=Y))\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n            train_op = optimizer.minimize(loss_op)\r\n            correct_pred = tf.equal(tf.argmax(out, 1), tf.argmax(Y, 1))\r\n            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n            init = tf.global_variables_initializer()\r\n            sess.run(init)\r\n\r\n            for step in range(1, num_steps + 1):\r\n                sess.run(train_op)\r\n                if step % 100 == 0 or step == 1:           \r\n                    loss, acc = sess.run([loss_op, accuracy])\r\n                    print(\"T1 Step \" + str(step) + \", Minibatch Loss= \" + \"{:.4f}\".format(loss) + \",Training Accuracy= \"+\"{:.3f}\".format(acc))\r\n\r\n\r\ndef model_3 (g2,sess,n_classes, dropout, reuse, is_training):\r\n    with g2.as_default(), sess.as_default():\r\n        with tf.variable_scope('ConvNet3', reuse=reuse):\r\n            fc1 = inputQ.queue[0]\r\n            fc1 = tf.layers.dense(fc1, 512)\r\n            fc1 = tf.layers.dense(fc1, 1024)\r\n            fc1 = tf.layers.dropout(fc1, rate=0.55, training=is_training)\r\n            out = tf.layers.dense(fc1, n_classes)          \r\n            out = tf.nn.softmax(out) if not is_training else out\r\n            loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\r\n                        logits=out, labels=Y))\r\n\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n            train_op = optimizer.minimize(loss_op)\r\n            correct_pred = tf.equal(tf.argmax(out, 1), tf.argmax(Y, 1))\r\n            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n            init = tf.global_variables_initializer()\r\n            sess.run(init)\r\n            \r\n            for step in range(1, num_steps + 1):\r\n                sess.run(train_op)\r\n                if step % 100 == 0 or step == 1:\r\n                    loss, acc = sess.run([loss_op, accuracy])\r\n                    print(\"T2 Step \" + str(step) + \", Minibatch Loss= \"+\"{:.4f}\".format(loss) + \",Training Accuracy=\"+\"{:.3f}\".format(acc))\r\n\r\n\r\nthread_Train0 = threading.Thread(target=model_1,args = (g1,sess1,X,n_classes,dropout, False))\r\nthread_Train1 = threading.Thread(target=model_2,args = (g1,sess1,n_classes,  dropout, False, True))\r\nthread_Train2 = threading.Thread(target=model_3,args = (g1,sess1,n_classes,  dropout, False, True))\r\n\r\nthread_Train0.start()\r\nthread_Train1.start()\r\nthread_Train2.start()\r\n\r\nthread_Train0.join()\r\nthread_Train1.join()\r\nthread_Train2.join()\r\n```", "comments": ["@yogeshprox This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks! \r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@gadagashwini  Thnak you, I will post it on github."]}, {"number": 29688, "title": "[TF API Docs] UX - `tf.io.FixedLenFeature` and `tf.FixedLenFeature `", "body": "#### Issue description:\r\nIn the `tf.data` pipelines guide on TensorFlow.org called 'Importing Data', under 'Processing data with `Dataset.map()`' there is an [example](https://www.tensorflow.org/guide/datasets#parsing_tfexample_protocol_buffer_messages) showcasing how to parse `tf.example` protocol buffer messages':\r\n\r\n```python\r\ndef _parse_function(example_proto):\r\n  features = {\"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n              \"label\": tf.FixedLenFeature((), tf.int64, default_value=0)}\r\n  parsed_features = tf.parse_single_example(example_proto, features)\r\n  return parsed_features[\"image\"], parsed_features[\"label\"]\r\n...\r\n```\r\nThe config class for parsing fixed length input features used in the example - `tf.FixedLenFeature` - may not be easily identifiable in the API docs since its description is under the `tf.io` module under the alias `tf.io.FixedLenFeature`. \r\n\r\nUX: Figuring out what  `tf.FixedLenFeature` does required using the search bar on TensorFlow.org vs the TF Python [API site](https://www.tensorflow.org/api_docs/python/tf).\r\n\r\n(For your reference: [1.13 (core) doc](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature),  [r1.14 doc](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/io/FixedLenFeature), [r2.0 beta doc](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/FixedLenFeature))\r\n\r\n#### Feature request:\r\nChange the class in the guide to `tf.io.FixedLenFeature` or reference with a link to `tf.io...` for better UX.", "comments": ["I've noticed the [guide](https://www.tensorflow.org/guide/data#parsing_tfexample_protocol_buffer_messages) has been revamped and `tf.FixedLenFeature` -> `tf.io.FixedLenFeature`:\r\n\r\n<img width=\"500\" alt=\"image\" src=\"https://user-images.githubusercontent.com/19637339/89464927-26c1e280-d769-11ea-9573-47f7be07898e.png\">\r\n\r\nClosing the issue! @ymodak @dynamicwebpaige "]}, {"number": 29687, "title": "HashTable lookup performance very low in comparison to plain Python dictionaries (~5,000x)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.2 LTS (Bionic Beaver)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Don't know (Colab)\r\n- **TensorFlow version (use command below)**: 1.13.1\r\n- **Python version**: 3.6.7\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: - \r\n- **CUDA/cuDNN version**: 10.0.130\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**:  tf.contrib.lookup.HashTable..lookup()\r\n\r\n\r\n\r\n\r\n### Describe the problem\r\nWhile writing a document ranking algorithm in TensorFlow we found out that TensorFlow `HashTable`s appear to be very slow (~5,000x) in comparison to plain Python `dict`ionaries. \r\n\r\nLooking into the TensorFlow source code in `lookup_table_op.cc` (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lookup_table_op.cc) shows that the underlying object structure appears to be an `unordered_map`. So we wonder, why the performance is so low? \r\n\r\n### Source code / logs\r\nHere is the source code that is similar to a part of what we use and can be easily executed on any system. There is actually a __Colab__ notebook that can be used: https://colab.research.google.com/drive/1bB_sir7-sVd3bNrSgkcdT9UlU9eoyA2Q \r\n\r\n```python\r\nimport urllib.request\r\nimport tensorflow as tf\r\nimport re, time\r\nfrom nltk.corpus import stopwords\r\nfrom nltk import word_tokenize, download\r\n# For loading the 'punkt' module of 'nltk'. This has to be done only once.\r\ndownload( 'punkt' )\r\ndownload( 'stopwords' )\r\n\r\nurl = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt'\r\nurllib.request.urlretrieve( url, './cowper.txt' )\r\n\r\ndef parse_text( input_path = './cowper.txt', output_path = 'cowper_tf.txt' ):\r\n    stop_words      = set( stopwords.words( 'english' ) )    \r\n    with open( output_path, 'w+' ) as output_file:\r\n        document = []\r\n        with open( input_path, 'r' ) as file:\r\n            for line in file:\r\n                for token in map( str.lower, word_tokenize( line ) ):                    \r\n                    if not token in stop_words and bool( re.match( r'[a-zA-Z]', token ) ):\r\n                        document.append(token)             \r\n            output_file.write(' '.join( document ) ) \r\n\r\nparse_text()\r\n\r\nwith open( 'cowper_tf.txt' ) as file:\r\n    for line in file:\r\n        document = re.split( r'\\s', line )[1:-1]\r\n\r\nk = 100\r\ndictionary_list = []\r\nhashtable_list  = []\r\nfrequencies     = {}\r\nfor i in range( k ):\r\n    for word in document:    \r\n        if word not in frequencies:\r\n            frequencies[word] = 0        \r\n            frequencies[word] += 1        \r\n    keys           = tf.constant( list( frequencies.keys() ) )\r\n    values         = tf.constant( list( frequencies.values() ) )\r\n    frequencies_tf = tf.contrib.lookup.HashTable( \r\n        tf.contrib.lookup.KeyValueTensorInitializer(\r\n            keys   = keys,\r\n            values = values ),\r\n        default_value = 0 )\r\n    hashtable_list.append( frequencies_tf )\r\n    dictionary_list.append( frequencies )\r\n\r\nn = 100\r\ntest_word = tf.constant( 'sing', dtype = tf.string, shape = () )\r\nglobal tf_time\r\ndef test_lookup( test_word, n = 100 ):\r\n    start = time.clock()\r\n    for i in range( n ):    \r\n        a = [hashtable.lookup( test_word ) for hashtable in hashtable_list]\r\n    tf_time = ( time.clock() - start ) / n\r\n    print( 'time elapsed per lookup: ', tf_time )\r\n    return a\r\nwith tf.Session() as sess:\r\n    sess.run( tf.global_variables_initializer() )\r\n    sess.run( tf.tables_initializer() )\r\n    sess.run( test_lookup( test_word, n ) )\r\n\r\ntest_word = 'add'\r\nstart = time.clock()\r\nfor i in range( n ):\r\n    a = [dictionary[test_word] for dictionary in dictionary_list]\r\nplain_python_time = ( time.clock() - start ) / n\r\nprint( 'elapsed time:', plain_python_time )\r\n```\r\n", "comments": ["I am able to reproduce the issue on Colab with Tensorflow 1.13.1. Thanks!", "1. I think you are measuring graph construction time not the runtime for the TF numbers.\r\n2 However even more importantly the benchmark does not even run any lookups at all since \"return test_word\" does not depend on looked up values,\r\n3. Overall once you run the lookup ops performance will be even more dismal than what you currently claim, since you are creating > 10000 ops and per op overhead is huge. For a more realistic comparison you should be looking up a batch of keys at a time.", "@simonsays1980, Did you get a chance to look at @azaks3's comment. Thanks!", "I didn't open this ticket, but I did try to create a simple test in TF 2.0 that addresses @azaks3's points. To allow the `StaticHashTable` to work efficiently, I batched the inputs in that mode in groups of 1000:\r\n\r\n    keys = range(1,11)\r\n    values = [0, 1, 1, 0, 1, 0, 1, 0, 0, 0]\r\n    randomizer = tf.random_uniform_initializer(1, 10, 0)\r\n    \r\n    py_dict = dict(zip(keys, values))\r\n    py_input = randomizer.__call__((sample_count,), tf.int64)\r\n    py_ds = tf.data.Dataset.from_tensor_slices(py_input)\r\n    py_iter = iter(py_ds)\r\n    py_start = time.clock()\r\n    py_results = [py_dict[x.numpy()] for x in py_iter]\r\n    py_time = (time.clock() - py_start)\r\n    py_time_str = '{:10f}'.format(py_time)\r\n    print(f'{py_time}s - total python time')\r\n\r\n    tf_dict_init = tf.lookup.KeyValueTensorInitializer(keys, values, tf.int64, tf.int64)\r\n    tf_dict = tf.lookup.StaticHashTable(tf_dict_init, -1)\r\n    tf_input = randomizer.__call__((round(sample_count/1000), 1000), tf.int64)\r\n    tf_ds = tf.data.Dataset.from_tensor_slices(tf_input)\r\n    tf_iter = iter(tf_ds)\r\n    tf_start = time.clock()\r\n    tf_results = [tf_dict.lookup(x) for x in tf_iter]\r\n    tf_time = (time.clock() - tf_start)\r\n    tf_time_str = '{:10f}'.format(tf_time)\r\n    print(f'{tf_time}s - total TensorFlow time')\r\n\r\nOn my computer, the python version took about 9.8s for 100k records, while the TensorFlow version took about 0.4s.", "> 1. I think you are measuring graph construction time not the runtime for the TF numbers.\r\n>    2 However even more importantly the benchmark does not even run any lookups at all since \"return test_word\" does not depend on looked up values,\r\n> 2. Overall once you run the lookup ops performance will be even more dismal than what you currently claim, since you are creating > 10000 ops and per op overhead is huge. For a more realistic comparison you should be looking up a batch of keys at a time.\r\n\r\n@azaks3 First of all, thank you for your efforts looking into this issue and for your comments. In the [Colab notebook](https://colab.research.google.com/drive/1bB_sir7-sVd3bNrSgkcdT9UlU9eoyA2Q) I added two more cells, in which I repeat the example with a batch of 1,000 keys. There is still a difference of 7x between TensorFlow and pure Python, however your second comment is totally right: increasing the batch size decreases relative overhead. It's actually impressive to see how the time remains almost the same for TensorFlow `HashTable`s in comparison to the significantly longer run times for pure Python dictionaries when looking up more keys at once. \r\n\r\nNow, our use case is actually rather the one with not many keys (~20-50) per lookup but a lookup through many `HashTable`s (or in pure Python dictionaries) and this operation repeated many times (e.g. in inference). I understand now that the overhead is quite a lot and that this might be the reason why we have such a long runtime for the TensorFlow version of our application. I was hoping to find a solution in TensorFlow - something about `HashTable`s I have not known, yet - to make the requests faster. This becomes especially demanding when turning towards inference. ", "> I didn't open this ticket, but I did try to create a simple test in TF 2.0 that addresses @azaks3's points. To allow the `StaticHashTable` to work efficiently, I batched the inputs in that mode in groups of 1000:\r\n> \r\n> ```\r\n> keys = range(1,11)\r\n> values = [0, 1, 1, 0, 1, 0, 1, 0, 0, 0]\r\n> randomizer = tf.random_uniform_initializer(1, 10, 0)\r\n> \r\n> py_dict = dict(zip(keys, values))\r\n> py_input = randomizer.__call__((sample_count,), tf.int64)\r\n> py_ds = tf.data.Dataset.from_tensor_slices(py_input)\r\n> py_iter = iter(py_ds)\r\n> py_start = time.clock()\r\n> py_results = [py_dict[x.numpy()] for x in py_iter]\r\n> py_time = (time.clock() - py_start)\r\n> py_time_str = '{:10f}'.format(py_time)\r\n> print(f'{py_time}s - total python time')\r\n> \r\n> tf_dict_init = tf.lookup.KeyValueTensorInitializer(keys, values, tf.int64, tf.int64)\r\n> tf_dict = tf.lookup.StaticHashTable(tf_dict_init, -1)\r\n> tf_input = randomizer.__call__((round(sample_count/1000), 1000), tf.int64)\r\n> tf_ds = tf.data.Dataset.from_tensor_slices(tf_input)\r\n> tf_iter = iter(tf_ds)\r\n> tf_start = time.clock()\r\n> tf_results = [tf_dict.lookup(x) for x in tf_iter]\r\n> tf_time = (time.clock() - tf_start)\r\n> tf_time_str = '{:10f}'.format(tf_time)\r\n> print(f'{tf_time}s - total TensorFlow time')\r\n> ```\r\n> \r\n> On my computer, the python version took about 9.8s for 100k records, while the TensorFlow version took about 0.4s.\r\n\r\n@novog Thanks for posting your example code in TensorFlow 2.0 here. This points again out - in a more elegant implementation - that overhead is actually the reason for the duration issue when looking up a lot of times only a few keys in many tables. Looking up many keys only a few times is where TensorFlow glances. \r\n\r\n@azaks3 @novog For the above mentioned use case of looking up many times only a few keys in a lot of tables: how would you implement this in TensorFlow to get good performance? ", "I think the standard technique would be to combine the tables and encode table name in the key/output values", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29687\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29687\">No</a>\n"]}, {"number": 29686, "title": "[Find header file after build manually tensor flow lite]", "body": "Hello, I built tensorflow lite for ARM 64 on Linux.\r\nAnd After building successful I have a library called \"libtensorflow-lite.a\"\r\nAs you know, if we want to use this lib, we have both lib and header file (.h file in my case).\r\nBut where can I get all header file for libtensorflow lib.\r\nIn result folder (gen folder in my case), I see bin, lib, object but include folder does't exist.", "comments": ["@TrungLM13 Could you please let us know if you still need help on this ? if it is resolved then please feel free to move this issue to close status ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29686\">No</a>\n"]}, {"number": 29685, "title": " [TF 2.0 API Docs] tf.keras.constraints.MinMaxNorm", "body": "# Existing URLs containing the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/constraints/MinMaxNorm\r\n## Description of the issue (what needs changing):\r\n- ### Correct links:\r\n     Yes\r\n\r\n- ###  Clear Description:\r\n  No, The description does not give recommendations of when and when not to use this symbol\r\n\r\n- ### Usage Example:\r\n   No usage example \r\n\r\n- ### Parameters Defined:\r\n   Parameters are Well defined\r\n\r\n  - ### Returns are not defined\r\n\r\n- ### Raises listed and Defined:\r\n  Errors are not defined.\r\n\r\n- ### Visuals if applicable:\r\n  No visuals are included.", "comments": ["Waiting for a response as soon as possible. ", "More details about constraints is listed here https://keras.io/api/layers/constraints/\r\n\r\nI am closing this issue as enough details are listed in the `keras.io`\r\n\r\nPlease note that Keras development moved to [keras-team/keras](keras-team/keras repo](https://github.com/keras-team/keras/issues) repository to focus entirely on only keras. Thanks!"]}, {"number": 29684, "title": "Error building hlo_verifier_test.cc", "body": "```\r\ntensorflow/compiler/xla/service/hlo_verifier_test.cc:719:31: error: 'StrReplaceAll' is not a member of 'absl'\r\n   return ParseHloString(absl::StrReplaceAll(\r\n                               ^~~~~~~~~~~~~\r\n```\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes - but no changes to this file.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source build\r\n- TensorFlow version (use command below): master\r\n- Python version: master build\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n\r\nWhen building from source including XLA I receive the error above.\r\n", "comments": ["Just to verify did you get chance to follow instructions from [TensorFlow website](https://www.tensorflow.org/install/source) .Please, let us know. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29683, "title": "[TF 2.0 API Docs]  tf.keras.metrics.BinaryAccuracy", "body": "TensorFlow version: 2.0\r\n## Existing URLs containing the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics/BinaryAccuracy\r\n\r\n## Description of the issue (what needs changing):\r\n\r\n- #### Correct links: \r\n       Yes\r\n\r\n- #### Clear Description: \r\n   No, The description does not  give recommendations of when and when not to use this symbol\r\n\r\n- #### Usage Example: \r\n       Yes\r\n- #### Parameters Defined:\r\n\r\n  Parameters are poorly defined, and not formatted appropriately.\r\n- ####  Returns Defined: \r\n\r\n    Returns are not defined\r\n. \r\n- ####  Raises listed and Defined:\r\n\r\n   Errors are not defined.\r\n\r\n- #### Visuals if applicable: \r\n   No visuals are included.\r\n", "comments": ["The docs are updated with TF 2.4 api docs. Thanks!\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy"]}, {"number": 29682, "title": "can not start session", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Python crash during import:\r\n\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17)\r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nroot@guyen-MS-7B22:/home/guyen# pip3 freeze | grep tensor\r\ntensorboard==1.13.1\r\ntensorflow-estimator==1.13.0\r\ntensorflow-gpu==1.13.1\r\n\r\n\r\nroot@guyen-MS-7B22:/home/guyen# uname -r\r\n4.18.0-21-generic\r\nroot@guyen-MS-7B22:/home/guyen# lsb_release --all\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 18.04.2 LTS\r\nRelease:        18.04\r\nCodename:       bionic\r\nroot@guyen-MS-7B22:/home/guyen#\r\n", "To use tensorflow-gpu, you have to install cudnn with cuda installed.\r\n\r\nif you have installed them, make sure the LD_LIBRARY_PATH is correctly set and pointing to where the cudnn libraries are located.\r\n\r\nThey usually reside in /usr/loca/cuda/, so `echo $LD_LIBRARY_PATH` command should give you some similar path to the library if you have set up your environment correctly.\r\n\r\nAlso, note that cudnn seem to be version specific, so you have to install a specific version, libcublas.so.10.0 in your case, to make it work.", "@gggh000 : Did you try following the steps mention in [TensorFlow website](https://www.tensorflow.org/install/pip). Please have a look on the instruction from the link to install TF using pip or source. Also have a look on software requirement for TensorFlow various versions [here](https://www.tensorflow.org/install/gpu#software_requirements). Thanks!", "I don't understand the response. You said cudnn to be installed but on nvidia website says cudnn is installed and supposedly installed as part of the cuda toolkit. After installation, i don't see cudnn and your instruction does not specify where to find it either. I don't see main cuda installer has parameter for installing cudnn. I don't see any specific download link available that downloads cudnn, any link for cudnn brings back to main cuda installer url. ", "I am using 10.0", "No my response below was wrong that was my initial attempt. I located and installed installer for cudnn both runtime and developer version but still did not work ofr ubuntu 1804. I will post the error message later.", "Sorry for being vague if you are responding to me.\r\n\r\nAnyway, the point here is that libcublas.so.10.0 should be present in the $LD_LIBRARY_PATH\r\n\r\nThat's where tensorflow-gpu looks for the necessary library for GPU acceleration.\r\n\r\nIf `echo $LD_LIBRARY_PATH` prints out something rather than empty string, then libcublas.so.10.0 is not present in that location.\r\n\r\nI didn't try the cuda toolkit myself so cannot say anything about it, but if it's indeed an automatic installation, it also had to set appropriate $LD_LIBRARY_PATH", "how could i expect the rest of the journey to be smooth? I can not do as simple as installing tensorflow-gpu version without huge complication encountered here and you guys are not even sure what is the problem? as a side note, i can install most linux application and pip libraries (except the some shittiest ones) with no issue. Including $LD_LIBRARY_PATH was nowhere mentioned in either book I am following nor in the tensorflow installation guide on tensorflow.org (https://www.tensorflow.org/install). How can i be sure including $LD_LIBRARY_PATH will solve anything? Do you say it with confidence after you test it or i am being led on a rabbit hole? So I dont see any issue on my side other than following the steps exactly. \r\nSo i dont think i should try including that to see if it works, as it is my job nor I am doing QA job for free. It shoud be your job to do such sanity testing before you release your product. ", "You may find this link useful: https://www.tensorflow.org/install/gpu", "it locked up the whole login screen when i followed the instruction", "Sorry to hear you are having a difficult time installing TF. From the error message,\r\n```\r\nFile \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\nreturn _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n```\r\n We are certain that it has arisen due to incorrect cuda path to the environment. Please take a look at [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup) if haven't already. What is your GPU model and memory?\r\n\r\nYou can also try installing TF-CPU version by simply,\r\n``` pip install tensorflow```\r\nThanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "please re-opened it, it is not fixed yet.", "why are you recommending cpu version when I specifically want GPU??", "Can you please attach a screenshot of your Environment variables?", "Closing due to lack of response. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29681, "title": "Cannot run a Process under a Thread when using tf.set_random_seed", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16 and 18\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n\r\n**No issue on:** tf 1.5.0, Python 3.6.8 and Ubuntu 16\r\n\r\n**Issue**\r\n\r\nWhen using `tf.set_random_seed` I cannot run a Process under a Thread\r\n\r\n**Code**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom threading import Thread\r\nfrom multiprocessing import Process\r\n\r\ndef misc():\r\n    print(\"misc\")\r\n\r\ndef do():\r\n    p = Process(target=misc)\r\n    p.start()\r\n    p.join()\r\n\r\ndef test():\r\n    a = Thread(target=do)\r\n    a.start()\r\n    a.join()\r\n\r\nif __name__ == '__main__':\r\n    print(\"main start\")\r\n    test()\r\n    print(\"first test done\")\r\n    tf.set_random_seed(0)\r\n    test()\r\n    print(\"second test done\")\r\n```\r\n\r\nThis script ouput:\r\n\r\n```\r\nmain start\r\nmisc\r\nfirst test done\r\n```\r\n \r\nAnd the process block.\r\n\r\n**Expected output**\r\n\r\n```\r\nmain start\r\nmisc\r\nfirst test done\r\nmisc\r\nsecond test done\r\n```\r\n", "comments": ["I could able to reproduce the above issue with Tensorflow 1.13.1 on my environment. Thanks!", "Apologies for the delay in response. This is fixed in TF 1.14.0 \r\nHowever if you select gpu accelerator the code will produce required result in TF 1.13.1", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29681\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29681\">No</a>\n"]}, {"number": 29680, "title": "TF2-BETA: Model inputs check bug with new feature: distribute model without cloning.", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform: Win10\r\n- TensorFlow installed: pip install\r\n- TensorFlow version: tensorflow-gpu==2.0.0-beta0 and tf-nightly-gpu-2.0-preview==2.0.0.dev20190509\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nDistribute strategy cannot properly handle dict/tuple type inputs in tensorflow-gpu==2.0.0-beta0 while there is no problem in non-distribute scenario.\r\n\r\nWhat the interesting thing is that in tf2.0-alpha, the bug existed already, but later in tf-nightly-gpu-2.0-preview==2.0.0.dev20190509. The problem seems to be solved. In that version, both distribute/non-distribute scenarios works good with dict type inputs. But I dont know why this bug appears again in beta version.\r\n\r\nI thought this problem is caused when distribute strategy trying to slice the inputs equally to all replica. Maybe the input parse method in distribute strategy is not fully tested. But since there is no public change log for tf-nightly. So i myself cannot dig this anymore without the help of tf team members.\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import layers as KL\r\nfrom tensorflow.python.keras import models as KM\r\nimport numpy as np\r\n\r\ndef custom_loss(predict, label, weight):\r\n    bce = tf.losses.binary_crossentropy(label, predict)\r\n    return tf.reduce_mean(bce * weight)\r\n\r\ndef create_model():\r\n    input_img = KL.Input([64, 64, 3], name=\"img\")\r\n    input_lbl = KL.Input([64, 64, 1], name=\"lbl\")\r\n    input_weight = KL.Input([64, 64], name=\"weight\")\r\n    predict = KL.Conv2D(2, [1, 1], padding=\"same\")(input_img)\r\n    my_loss = KL.Lambda(lambda x: custom_loss(*x), name=\"my_loss\")([predict, input_lbl, input_weight])\r\n    model = KM.Model(inputs=[input_img, input_lbl, input_weight], outputs=[predict, my_loss])\r\n    model.add_loss(model.get_layer(\"my_loss\").output)\r\n    model.compile(optimizer=\"adam\")\r\n    return model\r\n\r\ndef get_dict_dataset():\r\n    def map_fn(img, lbl, weight):\r\n        inputs = {\"img\": img, \"lbl\": lbl, \"weight\": weight}\r\n        targets = {}\r\n        return inputs, targets\r\n\r\n    fake_imgs = np.ones([50, 64, 64, 3])\r\n    fake_lbls = np.ones([50, 64, 64, 1])\r\n    fake_weights = np.ones([50, 64, 64])\r\n\r\n    return tf.data.Dataset.from_tensor_slices((fake_imgs, fake_lbls, fake_weights)).map(map_fn).batch(10)\r\n```\r\ntensorflow-gpu==2.0.0-beta0\r\nin non-distribute scenario:\r\n```python\r\nmodel = create_model()\r\ndata = get_dict_dataset()\r\nmodel.fit(data)\r\n10/10 [==============================] - 1s 143ms/step - loss: 0.0000e+00\r\n```\r\nin distribute scenario:\r\n```python\r\nstrategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\nwith strategy.scope():\r\n    model = create_model()\r\n    data = get_dict_dataset()\r\n    model.fit(data)\r\nAttributeError: 'dict' object has no attribute 'shape'\r\n```\r\nStill not working Even if i change the dict to tuple:\r\n```python\r\ndef get_tuple_dataset():\r\n    def map_fn(img, lbl, weight):\r\n        inputs = (img, lbl, weight)\r\n        targets = tuple()\r\n        return inputs, targets\r\n\r\n    fake_imgs = np.ones([50, 64, 64, 3])\r\n    fake_lbls = np.ones([50, 64, 64, 1])\r\n    fake_weights = np.ones([50, 64, 64])\r\n\r\n    return tf.data.Dataset.from_tensor_slices((fake_imgs, fake_lbls, fake_weights)).map(map_fn).batch(10)\r\n\r\nwith strategy.scope():\r\n    model = create_model()\r\n    data = get_tuple_dataset()\r\n    model.fit(data)\r\nValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 2 arrays\r\n```\r\n\r\nAgain, the scenarios described above working good in  tf-nightly-gpu-2.0-preview==2.0.0.dev20190509. And again there is no public change log for tf-nightly so I cannot dig this any deeper alone.\r\n\r\n\r\n**Describe the expected behavior**\r\nSince the model behavior should be consistent. Tf team should decide weather to support dict type inputs. If no, both distribute/non-distribute model should not accept dict input and vice versa.\r\n\r\n", "comments": ["After tracing the inputs values, i finally found that:\r\nthe problem is caused by method \"def _prepare_feed_values\" in tensorflow/python/keras/distribute/distributed_training_utils.py(~line565)\r\n```python\r\ndef _prepare_feed_values(model, inputs, targets, sample_weights, mode):\r\n  ...\r\n  if isinstance(inputs, dict):\r\n    inputs = [inputs[key] for key in model._feed_input_names]\r\n  if is_distributing_by_cloning(model):\r\n    inputs = flatten_per_replica_values(strategy, inputs)\r\n    targets = flatten_per_replica_values(strategy, targets)\r\n    # Expand 1-dimensional inputs.\r\n    # TODO(b/124535720): Remove once this standarize data logic is shared with\r\n    # main flow.\r\n    inputs, targets = nest.map_structure(\r\n        training_utils.standardize_single_array, (inputs, targets))\r\n  ...\r\n```\r\nIn tf2.0 beta, the return value of condition \"if is_distributing_by_cloning(model)\" is False, so that the inputs are not processed by \"flatten_per_replica_values(strategy, inputs)\" which will flat the dict type inputs to tensors. Then i continued to dig and found this in method \"def compile\" in tensorflow/python/keras/engine/training.py(~line250):\r\n```python\r\n# Check whether the experimental feature of distributing the Model without\r\n    # cloning is requested.\r\n    # TODO(b/124517980, b/124377929): Remove this temporary undocumented way\r\n    # of enabling the feature and graduate it to the main distributed code path.\r\n    self._cloning = kwargs.pop('cloning', False)\r\n    self._validate_compile_param_for_distribution_strategy(self.run_eagerly,\r\n                                                           sample_weight_mode,\r\n                                                           target_tensors,\r\n                                                           weighted_metrics)\r\n    self.optimizer = optimizers.get(optimizer)\r\n```\r\nNoticed that the arg: while in tf2.0-beta, the default value of \"cloning\" is False while in tf-nightly-0509 the value is True. **So, here is the temp solution: in tf2.0-beta, when compiling the model, one should pass a kwarg cloning = True and everything behave the same as the previous version.**\r\n\r\n**But we should noticed that the temp solution is not meaning that there is no bug. As we see in the comment left in the source code:** \r\n```python\r\n# Check whether the experimental feature of distributing the Model without cloning is requested.\r\n```\r\nAt least we know that the feature \"distributing model without cloning\" should reconsider the current implementation in \"def _prepare_feed_values\" so that machine can know how to process dict/tuple type inputs for \"non-cloning \" accordingly. **Otherwise, the non-cloning feature should not enabled in a stable release in my understanding.**\r\n\r\n\r\n", "Thank for you for reporting. We will take a look. ", "I submitted https://github.com/tensorflow/tensorflow/commit/6d00cb1a2f9303cb084ff2218700ab8479bc4e2b and https://github.com/tensorflow/tensorflow/commit/ce59aa0be80384d096a13efaca2b8c6e071b9a54 because of this issue and it should be resolved now.\r\n\r\nThanks for reporting and good debugging skills!  Re-open if still an issue.", "@isaprykin hi, if this mean i will get the fixed version using latest tf-nightly?", "Yes."]}, {"number": 29679, "title": "the tfdbg CLI does not apepar  in tensorflow alpha2.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):   pip install\r\n- TensorFlow version (use command below):   tensorflow apha 2.0\r\n- Python version:   3.6.3\r\n- CUDA/cuDNN version:   CUDA Version 10.1.105, cudnn 7.5\r\n- GPU model and memory:   GTX 2080TI\r\n\r\n*Describe the current behavior**\r\n```\r\nfrom tensorflow.python import debug as tf_debug\r\nkeras.backend.set_session(\\\r\n        tf_debug.LocalCLIDebugWrapperSession(tf.compat.v1.Session()))\r\n```\r\n**the tfdbg CLI does not apepar**, I want to know what is the problem, and how to solve it\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport sys\r\nimport os\r\nimport json\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\n\r\ndef basic_generator():\r\n    \"\"\"\r\n    \"\"\"\r\n    data_x = [[0, 1, 2, 3], [1, 2, 3, 4], \\\r\n        [0, 1, 3, 4], [0, 2, 1, 4]]\r\n    data_y = [[0, 1, 2, 3], [1, 2, 3, 4], \\\r\n        [0, 1, 3, 4], [0, 2, 1, 4]]\r\n    for x, y in zip(data_x, data_y):\r\n        yield (x, y)\r\n\r\nif __name__ == \"__main__\":\r\n    params = { \r\n       'seq_len': 8,\r\n        'use_bert': False,\r\n        'batch_size': 2,\r\n        'epochs': 5,\r\n        'vocab_len': 5,\r\n        'num_entities': 3,\r\n        'use_pre_emb': False,\r\n        'emb_dim': 20,\r\n        'use_emb_drop': False,\r\n        'emb_drop_rate': 0.8,\r\n        'num_layer': 1,\r\n        'num_lstm_cell': [15],\r\n        'rec_drops': [0.8],\r\n        'lr': 0.001\r\n    }\r\n\r\n    output_types = (tf.int32, tf.int32)\r\n    dataset = tf.data.Dataset.from_generator(\\\r\n        basic_generator, output_types)\r\n    dataset = dataset.padded_batch(\\\r\n        params['batch_size'], \\\r\n            ([params['seq_len'], ], [params['seq_len'], ]))\r\n\r\n    from tensorflow.python import debug as tf_debug\r\n    keras.backend.set_session(\\\r\n        tf_debug.LocalCLIDebugWrapperSession(tf.compat.v1.Session()))\r\n\r\n    dataset = dataset.map(lambda x, y: (x, tf.one_hot(y, params['num_entities'])))\r\n    dataset = dataset.repeat(params['epochs'])\r\n    \r\n    #model = LstmCrf(params)\r\n    model = keras.Sequential()\r\n    inputs = keras.Input(shape=(None, ))\r\n    emb_ini = keras.initializers.TruncatedNormal()\r\n    model.add(keras.layers.Embedding(\\\r\n                params['vocab_len'], params['emb_dim'], \\\r\n                    embeddings_initializer=emb_ini, mask_zero=True))        \r\n    model.add(keras.layers.TimeDistributed(\\\r\n        keras.layers.Dense(params['num_entities'])))\r\n    model.add(keras.layers.Activation('softmax'))\r\n       \r\n    model.compile(optimizer=keras.optimizers.Adam(params['lr']), \\\r\n        loss='categorical_crossentropy', \\\r\n            metrics=['accuracy'])\r\n\r\n    model.fit(dataset, epochs=params['epochs'], \\\r\n        steps_per_epoch=2)\r\n    \r\n```\r\n", "comments": ["Can you give a try with CUDA 10.0 and let us know if you are still stuck. I have tried with 2.0-alpha on CUDA 10.0  Colab (GPU- Tesla T4) and was able to execute the snippet provided without any issue. You can also refer [GPU support link](https://www.tensorflow.org/install/gpu#software_requirements) for information. Thanks!", "Adding myself (owner of tfdbg).", "Is this still an issue ? Please reach out to us in case you are stuck or else we can close the issue for now. Thanks!", "surprise, it give me new error: AttributeError: module 'tensorflow.keras.backend' has no attribute 'set_session'", "Have tried following snippet on Colab with TF version 2.0-alpha but did not get any error :\r\n\r\nimport tensorflow as tf \r\nfrom tensorflow.python import debug as tf_debug\r\nimport tensorflow.keras as keras\r\nkeras.backend.set_session(\\\r\n        tf_debug.LocalCLIDebugWrapperSession(tf.compat.v1.Session()))\r\n\r\nCan you try this and let us know if you are still stuck. Thanks!", "sorry, the new error(AttributeError: module 'tensorflow.python.keras.api._v2.keras.backend' has no attribute 'set_session), comes when i use tensorflow 2.0 beta\uff0c and the origin problem comes from 2.0 alpha and beta", "i think you use the \"import tensorflow.keras as keras\" point to tf2.0 api. but setup tfdebug using \"LocalCLIDebugWrapperSession\"  this call old version tf to get the session . then two version do not merge.\r\nbut i want to konw how to use  new tf 2.0 that no session api to load tfdebug? Tfdebug need session.", "@jimhoprogramming \r\n\r\nTF Debugger (tfdbg) for TF v2.0 is still being designed and will be made available at a later time. As you pointed out, tfdbg is originally designed for TF v1.x and is centered around the tf.Session API. As tf.Session is replaced by eager execution and tf.functions in 2.0, the debugger feature needs to be adapted to the architectural change.\r\n\r\nIf you have a specific use case (e.g., finding Infinities and NaNs, or other debugging workflows), please let us know. It'll inform the design of the new version of tfdbg.\r\n\r\ncc @alextp @mdan @wicke @bileschi ", "cc @GalOshri ", "Thanks,I debugging for some cnn having NaN case indeed.so,only way it was go black to tf v1.6 to do it.", "@aaronlyt,\r\nCould you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue?\r\n\r\nAlso, please go through [this debugging guide](https://www.tensorflow.org/tensorboard/debugger_v2) and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29679\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29679\">No</a>\n"]}, {"number": 29678, "title": "LoggingTensorHook used with TPUEstimator cause 'marked as not fetchable' exception.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDebian 4.9.168-1+deb9u2 (2019-05-13) x86_64 GNU/Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nN/A (GCP)\r\n- TensorFlow version (use command below):\r\nb'v1.13.0-rc0-0-g6ce86799c8' 1.13.0-rc0\r\n- Python version:\r\nPython 3.5.3\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWith use_tpu set to True, and LoggingTensorHook is used by uncommenting the commented part (see the code below), training on GCP with TPU fails with 'marked as not fetchable' exception.\r\n\r\n**Describe the expected behavior**\r\nThe behavior goes away and works without problem if I comment out the LoggingTensorHook parts, yet I have no idea what's the relevant issue.\r\n\r\nThe training fails with a very vague exception, which only says that the last op(tf.reduce_mean() in my case) has been marked as not fetchable, and no other useful information provided.\r\n\r\nIt makes it almost impossible to find the actual cause of the problem since the exception points to the wrong op, the reduce_mean(), which was actually not the problem.\r\n\r\n**Code to reproduce the issue**\r\nUncommenting the commented parts reproduce the problem.\r\n\r\nThis is just a sample. With any model with the LoggingTensorHook provided as the hooks parameter, it throws an exception when trained with TPUs.\r\n```\r\n    def model_fn(features, labels, mode, params):\r\n        del params\r\n\r\n        char_embedding = CharacterAwareEmbedding(\r\n            word_length=word_length,\r\n            embedding_size=FLAGS.embedding_size,\r\n            conv_size=FLAGS.conv_size,\r\n            output_unit=FLAGS.output_unit,\r\n        )\r\n\r\n        target = features[\"input_target\"]\r\n        target = char_embedding(target)\r\n\r\n        if mode == tf.estimator.ModeKeys.PREDICT:\r\n            predictions = {\r\n                'vectorized': target,\r\n                'word': features[\"word\"]\r\n            }\r\n            return tf.contrib.tpu.TPUEstimatorSpec(\r\n                mode=mode,\r\n                predictions=predictions)\r\n\r\n        context = features[\"input_context\"]\r\n        context = char_embedding(context)\r\n\r\n        cossim = cossim_similarity(target, context)\r\n\r\n        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(labels, tf.float32), logits=cossim))\r\n        optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\r\n\r\n        # hook = tf.train.LoggingTensorHook({\"loss\": loss}, every_n_iter=100)\r\n\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            if FLAGS.use_tpu:\r\n                optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\r\n\r\n            return tf.contrib.tpu.TPUEstimatorSpec(\r\n                mode=mode,\r\n                loss=loss,\r\n                #training_hooks=[hook],\r\n                train_op=optimizer.minimize(loss, tf.train.get_global_step()))\r\n\r\n        if mode == tf.estimator.ModeKeys.EVAL:\r\n            return tf.contrib.tpu.TPUEstimatorSpec(\r\n                mode=mode,\r\n                loss=loss,\r\n                #training_hooks=[hook],\r\n                eval_metrics=(metric_fn, [labels, cossim]))\r\n```\r\n**Other info / logs**\r\n", "comments": ["Looks like the code is incomplete.Can you please provide full code snippet to reproduce it on our environment.Thanks", "Same problem.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29677, "title": "tf.layers.dense deprecation update hint", "body": "Referencing the deprecation hint of https://www.tensorflow.org/api_docs/python/tf/layers/dense\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI wasn't able to find the referenced `tf.keras.layers.dense`, but only `tf.keras.layers.Dense`. Could this be a simple typo and the latter was ment to be pointed to?\r\n\r\n### Submit a pull request?\r\n\r\nIf this is a simple issue and not a functionality missing or a fault of me not finding the referenced method, then yes, I would PR it.", "comments": ["Submitted a [PR](https://github.com/tensorflow/tensorflow/pull/29827) to fix the issue. Thanks!"]}, {"number": 29676, "title": "Prevent spurious wakeup in threadpool test", "body": "It's good practice to check condition of a conditional variable in a while loop.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29676) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "@ccgrtwall  Please sign CLA in order to proceed with next steps. Thank you!", "I signed it!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29676) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 29675, "title": "Adam/Adagrad gives different derivatives when initializing globaly and localy", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (2.0):\r\n- Python version:(3.6)\r\n\r\n\r\n**Describe the current behavior**\r\nAdam and Adagrad, both gives different losses, when initialized once (globally) and initialized every time train function is called. \r\n\r\n````\r\n#### Locally\r\ndef train2(model, inputs, outputs, learning_rate):\r\n    trainable_variables = [model.W, model.b]\r\n    with tf.GradientTape() as t:\r\n        current_loss = loss(model(inputs), outputs)\r\n    optimizer = tf.keras.optimizers.Adagrad(\r\n    learning_rate=learning_rate,\r\n    initial_accumulator_value=0.1,\r\n    epsilon=1e-07,\r\n)\r\n    gradients = t.gradient(current_loss,  trainable_variables)    \r\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\r\n#----------------------------------------------------------------------------------------------------\r\n#### Globally\r\n\r\noptimizer_global = tf.keras.optimizers.Adagrad(\r\n    learning_rate=0.1,\r\n    initial_accumulator_value=0.1,\r\n    epsilon=1e-07,\r\n)\r\n\r\ndef train(model, inputs, outputs, learning_rate):\r\n    trainable_variables = [model.W, model.b]\r\n    with tf.GradientTape() as t:\r\n        current_loss = loss(model(inputs), outputs)\r\n    \r\n    gradients = t.gradient(current_loss,  trainable_variables)    \r\n    optimizer_global.apply_gradients(zip(gradients, trainable_variables))\r\n````\r\n\r\n\r\n**Describe the expected behavior**\r\nWe expect same loss from both train and train2. ( Is it because, some internal parameters are changing inside the optimizer, because SGD optimizer works fine in both the cases).\r\n\r\n\r\n**Code to reproduce the issue**\r\nThe plot of derivatives are provided in the image . Full code to reproduce is in the .ipynb file.\r\n![tf](https://user-images.githubusercontent.com/10637096/59318400-9151cc80-8ce4-11e9-9b65-1712a86d2218.png)\r\n\r\n````\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n\r\n\r\nimport tensorflow as tf\r\ntf.compat.v1.set_random_seed(1)\r\nimport numpy as np\r\ntf.__version__\r\n\r\n\r\n\r\nclass Model(object):\r\n  def __init__(self):\r\n    # Initialize variable to (5.0, 0.0)\r\n    # In practice, these should be initialized to random values.\r\n    self.W = tf.Variable(5.0)\r\n    self.b = tf.Variable(0.0)\r\n    \r\n  def __call__(self, x):\r\n    return self.W * x + self.b\r\n  \r\nmodel = Model()\r\ndef loss(predicted_y, desired_y):\r\n    return tf.reduce_mean(tf.square(predicted_y - desired_y))\r\n\r\nTRUE_W = 3.0\r\nTRUE_b = 2.0\r\nNUM_EXAMPLES = 1000\r\n\r\ninputs  = tf.random.normal(shape=[NUM_EXAMPLES])\r\nnoise   = tf.random.normal(shape=[NUM_EXAMPLES])\r\noutputs = inputs * TRUE_W + TRUE_b + noise # 3 x + 2 + noise()\r\n\r\nimport matplotlib.pyplot as plt\r\nget_ipython().run_line_magic('matplotlib', 'inline')\r\nplt.scatter(inputs, outputs, c='b')\r\nplt.scatter(inputs, model(inputs), c='r')\r\nplt.show()\r\n\r\ndef train2(model, inputs, outputs, learning_rate):\r\n    trainable_variables = [model.W, model.b]\r\n    with tf.GradientTape() as t:\r\n        current_loss = loss(model(inputs), outputs)\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \r\n                                         epsilon=1e-9)\r\n    gradients = t.gradient(current_loss,  trainable_variables)    \r\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\r\n    \r\n#-----------------------------------------------------------------------------------------------\r\n\r\noptimizer_global = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.98, \r\n                                         epsilon=1e-9)\r\ndef train(model, inputs, outputs, learning_rate):\r\n    trainable_variables = [model.W, model.b]\r\n    with tf.GradientTape() as t:\r\n        current_loss = loss(model(inputs), outputs)\r\n    \r\n    gradients = t.gradient(current_loss,  trainable_variables)    \r\n    optimizer_global.apply_gradients(zip(gradients, trainable_variables))\r\n\r\nmodel = Model()\r\n\r\n# Collect the history of W-values and b-values to plot later\r\nWs, bs = [], []\r\nepochs = range(100)\r\nfor epoch in epochs:\r\n  Ws.append(model.W.numpy())\r\n  bs.append(model.b.numpy())\r\n  current_loss = loss(model(inputs), outputs)\r\n\r\n  train(model, inputs, outputs, learning_rate=0.1)\r\n  print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\r\n        (epoch, Ws[-1], bs[-1], current_loss))\r\n\r\n# Let's plot it all\r\nplt.plot(epochs, Ws, 'r',\r\n         epochs, bs, 'b')\r\nplt.plot([TRUE_W] * len(epochs), 'r--',\r\n         [TRUE_b] * len(epochs), 'b--')\r\nplt.legend(['W', 'b', 'true W', 'true_b'])\r\nplt.show()\r\n  \r\n\r\n#---------------------------------------------------------\r\n\r\nmodel = Model()\r\n\r\n# Collect the history of W-values and b-values to plot later\r\nWs, bs = [], []\r\nepochs = range(100)\r\nfor epoch in epochs:\r\n  Ws.append(model.W.numpy())\r\n  bs.append(model.b.numpy())\r\n  current_loss = loss(model(inputs), outputs)\r\n\r\n  train2(model, inputs, outputs, learning_rate=0.1)\r\n  print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\r\n        (epoch, Ws[-1], bs[-1], current_loss))\r\n\r\n# Let's plot it all\r\nplt.plot(epochs, Ws, 'r',\r\n         epochs, bs, 'b')\r\nplt.plot([TRUE_W] * len(epochs), 'r--',\r\n         [TRUE_b] * len(epochs), 'b--')\r\nplt.legend(['W', 'b', 'true W', 'true_b'])\r\nplt.show()\r\n  ````", "comments": ["Was able to reproduce the reported issue on Colab with Tensorflow 2.0.0.beta0. Thanks!", "Adam is stateful, if you create it everytime calling train2, its behavior is meant to be different than if you create a global optimizer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29675\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29675\">No</a>\n", "@tanzhenyu - Thanks. Which is the preferred method ? Initializing globally or at each time train function is called.", "Initializing globally", "@s4sarath  Hi, I have a question about other script \uff0cWhen run ...'tf.compat.v1.set_random_seed(1)', it will raise error'module 'tensorflow.compat' has no attribute 'v1'', Can you help me? Waiting your reply! ", "Depending on your tf version. compat.v1 is introduced only after 1.14. So if you're in 1.14 you shouldn't see the error. Otherwise it would pop up."]}, {"number": 29674, "title": "Cannot find the placeholder op that is an input to ReadVariableOp in tf lite conversion.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2 beta\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda 10\r\n- GPU model and memory: Geforce GTX 1060\r\n\r\n\r\n**Describe the current behavior**\r\nI have a pretty complicated model with three different inputs  and I can save and load it with custom objects as a keras model with model.save() and model.load() methods. I want to convert it to  tf lite and I get this error:\r\nValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nloss_function = get_loss_function()\r\n    model = tf.keras.models.load_model(save_checkpoint_address, custom_objects={\r\n        'customlayer': CustomLayer,\r\n        'loss_function': loss_function\r\n    })\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n**Other info / logs**\r\n```\r\n2019-06-11 19:55:42.378746: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-06-11 19:55:42.378868: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-06-11 19:55:42.434841: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-06-11 19:55:42.434867: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 2213 nodes (393), 3499 edges (663), time = 18.406ms.\r\n2019-06-11 19:55:42.434874: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.785ms.\r\nTraceback (most recent call last):\r\n  File \"/home/siavash/programming/ximpa/carim_tensor/convert_to_tflite.py\", line 32, in <module>\r\n    convert_saved_model()\r\n  File \"/home/siavash/programming/ximpa/carim_tensor/convert_to_tflite.py\", line 27, in convert_saved_model\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 348, in convert\r\n    self._funcs[0])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 166, in convert_variables_to_constants_v2\r\n    raise ValueError(\"Cannot find the Placeholder op that is an input \"\r\nValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.\r\n```", "comments": ["Can you rerun your code with TensorFlow 2.0.0-beta1? There was an issue with beta0 that was fixed in beta1. If that doesn't work, can you provide a reproducible example?", "I have tried using TensorFlow 2.0.0-beta1 and see the same issue. But I get the error only when trying to convert tf.keras.layers.GRU or tf.keras.layers.LSTM layers to TFLite. Is this an issue with tf.lite.TFLiteConverter.from_saved_model ?", "I see the same error when using tfjs-converter (tf_saved_model --> tfjs_graph_model).", "There is currently limited support for LSTMs in TFLite. The documented path is available [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/g3doc/README.md). We are working on improving our support of control flow based operations and models.", "@siavash-khodadadeh Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Can you please provide a simple standalone code to reproduce the issue? Thanks!\r\n", "I will try it and let you know. It might take a couple of days since I do not have access to that code now.", "> @siavash-khodadadeh Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Can you please provide a simple standalone code to reproduce the issue? Thanks!\r\n\r\nI am using a model with LSTM, After getting the 'Cannot find placeholder op' issue with TF2.0b1, I updated to TF2.0,\r\nI get the following error :\r\n```\r\n\r\n>>> converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n>>> converter.convert()\r\n2019-10-11 15:16:45.711540: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-10-11 15:16:45.741723: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-11 15:16:45.750431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:04:00.0\r\n2019-10-11 15:16:45.750588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-11 15:16:45.750616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-11 15:16:45.750637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-11 15:16:45.750657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-11 15:16:45.750675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-11 15:16:45.750694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-11 15:16:45.750712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-11 15:16:45.753898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-11 15:16:45.753997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-11 15:16:45.754011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-10-11 15:16:45.754020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-10-11 15:16:45.760720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4956 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5)\r\n2019-10-11 15:16:45.884323: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-10-11 15:16:45.884563: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 41 nodes (0), 47 edges (0), time = 23.098ms.\r\n2019-10-11 15:16:45.884612: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 41 nodes (0), 47 edges (0), time = 18.946ms.\r\n2019-10-11 15:16:45.884641: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: __forward_cudnn_lstm_with_fallback_23859\r\n2019-10-11 15:16:45.884671: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-10-11 15:16:45.884701: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-10-11 15:16:45.884727: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: __inference_cudnn_lstm_with_fallback_23677\r\n2019-10-11 15:16:45.884756: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2019-10-11 15:16:45.884785: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-10-11 15:16:45.884824: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: while_body_23461\r\n2019-10-11 15:16:45.884856: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2019-10-11 15:16:45.884883: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-10-11 15:16:45.884908: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: while_cond_23460\r\n2019-10-11 15:16:45.884935: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2019-10-11 15:16:45.884963: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-10-11 15:16:45.884991: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: __inference___backward_cudnn_lstm_with_fallback_23678_23860\r\n2019-10-11 15:16:45.885025: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2019-10-11 15:16:45.885057: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-10-11 15:16:45.885096: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: __inference_standard_lstm_23566_specialized_for_sequential_lstm1_StatefulPartitionedCall_at_graph_to_optimize\r\n2019-10-11 15:16:45.885129: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 72 nodes (0), 102 edges (0), time = 9.739ms.\r\n2019-10-11 15:16:45.885159: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 72 nodes (0), 102 edges (0), time = 3.147ms.\r\n2019-10-11 15:16:45.885189: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: __inference_standard_lstm_23566\r\n2019-10-11 15:16:45.885218: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 76 nodes (0), 106 edges (0), time = 2.82ms.\r\n2019-10-11 15:16:45.885248: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 76 nodes (0), 106 edges (0), time = 2.649ms.\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 405, in convert\r\n    self._funcs[0], lower_control_flow=False)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/framework/convert_to_constants.py\", line 414, in convert_variables_to_constants_v2\r\n    function_data = _get_control_flow_function_data(node_defs, tensor_data)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/framework/convert_to_constants.py\", line 262, in _get_control_flow_function_data\r\n    arg_types[idx] = get_resource_type(input_name)\r\n  File \"/home/d1300/no_backup/d1300/tfRC/lib/python3.6/site-packages/tensorflow_core/python/framework/convert_to_constants.py\", line 228, in get_resource_type\r\n    numpy_type = tensor_data[node_name][\"data\"].dtype\r\nKeyError: 'kernel'\r\n\r\n```\r\n\r\nHere is now the model looks like :\r\n\r\n```\r\ndef MyModel_keras():\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.LSTM(conf.n_hidden_lstm, activation='tanh', return_sequences=False, name='lstm1'),\r\n        tf.keras.layers.Dense(conf.n_dense_1, activation='relu', name='dense1'),\r\n        tf.keras.layers.Dense(conf.num_output_classes, activation='softmax', name='dense2')\r\n    ])\r\n    return model\r\n```\r\n\r\nIs this issue specific to keras models with LSTMs?\r\nIs there a workaround for this?", "@Yuvaraj8blr Please check the comment [here](https://github.com/tensorflow/tensorflow/issues/29674#issuecomment-510966279). Thanks!", "I'm using the TF 2.0 gpu version and still get this error with trt.TrtGraphConverterV2:\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"optimize_graphv2.py\", line 33, in <module>\r\n    main(args)\r\n  File \"optimize_graphv2.py\", line 22, in main\r\n    converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py\", line 960, in convert\r\n    frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py\", line 479, in convert_variables_to_constants_v2\r\n    raise ValueError(\"Cannot find the Placeholder op that is an input \"\r\nValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.```\r\n\r\n", "@PolinaDemochkina Please provide a simple standalone code to reproduce the issue. Please check the [link](https://github.com/tensorflow/tensorflow/issues/29674#issuecomment-541231768) before providing standalone code. Thanks!", "@jvishnuvardhan I am not using TFLite, but I guess the issue has to do with the LSTM layer in my model. Tried to replace this layer with GRU, however, I got the same mistake. Does the optimizer not support any kind of recurrent layers?\r\n\r\nHere's the code to reproduce the issue:\r\n```\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow.keras.datasets import cifar100\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nkeras.backend.clear_session()\r\nkeras.backend.set_learning_phase(0)\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nimport os\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n\r\n(x_train, y_train), (x_test, y_test) = cifar100.load_data()\r\nx_train = np.float32(x_train)[:64*(x_train.shape[0]//64)]\r\nx_train /= 255.0\r\ni_shape = x_train[0].shape\r\n\r\ninputs = layers.Input(i_shape)\r\nbase_model = keras.models.Sequential([\r\n    layers.Conv2D(128, 3, padding='same', strides=(2, 2)),\r\n    layers.BatchNormalization(),\r\n    layers.LeakyReLU(0.2),\r\n\r\n    layers.Conv2D(256, 3, padding='same', strides=(2, 2)),\r\n    layers.BatchNormalization(),\r\n    layers.LeakyReLU(0.2),\r\n\r\n    layers.Conv2D(512*100, 3, padding='same', strides=(2, 2)),\r\n    layers.BatchNormalization(),\r\n    layers.LeakyReLU(0.2),\r\n    layers.Flatten(),\r\n    layers.Dense(5*128),\r\n    layers.Reshape((5, 128)),\r\n    layers.LSTM(128),\r\n    layers.Flatten(),\r\n])(inputs)\r\n\r\nprediction = layers.Dense(100, activation='softmax')(base_model)\r\nmodel = keras.Model(inputs=inputs, outputs=prediction)\r\noptimizer = optimizers.Adam(0.00001)\r\nmodel.compile(optimizer, 'categorical_crossentropy', metrics=['accuracy'])\r\nprint(model.summary())\r\nsaved_model_dir = os.getcwd()\r\noutput_directory = os.getcwd()\r\ntf.saved_model.save(model, saved_model_dir)\r\n\r\n# graph quantization\r\nloaded = tf.saved_model.load(saved_model_dir)\r\n\r\nparams = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\r\n    precision_mode='FP16',\r\n    is_dynamic_op=True,\r\n    maximum_cached_engines=16)\r\nconverter = trt.TrtGraphConverterV2(\r\n    input_saved_model_dir=saved_model_dir,\r\n    input_saved_model_tags=\"serve\",\r\n    input_saved_model_signature_key=\"serving_default\",\r\n    conversion_params=params)\r\nconverter.convert()\r\nsaved_model_dir_trt = output_directory\r\nconverter.save(saved_model_dir_trt)\r\n```", "@PolinaDemochkina Your issues is different from original issue (is with TF lite). Can you please post a new issue with details, platform, TF version and standalone code (as above). Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29674\">No</a>\n", "Run on tensorflow 2.1.0\r\n```\r\nimport tensorflow as tf\r\n\r\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\r\n                              batch_input_shape=[batch_size, embedding_dim]),\r\n    tf.keras.layers.LSTM(rnn_units,\r\n                        return_sequences=True,\r\n                        stateful=False,\r\n                        recurrent_activation='sigmoid',\r\n                        recurrent_initializer='glorot_uniform'),\r\n    tf.keras.layers.Dense(vocab_size)\r\n  ])\r\n  return model\r\n\r\nembedding_dim = 100\r\nunits = 256\r\nvocab_size = 300\r\nbatch_size = 32\r\n\r\nmodel = build_model(vocab_size, embedding_dim, units, batch_size)\r\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\r\n\r\nfrom tensorflow.python.keras.saving import saving_utils as _saving_utils\r\nfrom tensorflow.python.framework import convert_to_constants as _convert_to_constants\r\n\r\ntf.keras.backend.set_learning_phase(False)\r\nfunc = _saving_utils.trace_model_call(model)\r\nconcrete_func = func.get_concrete_function()\r\nfrozen_func = _convert_to_constants.convert_variables_to_constants_v2(concrete_func)\r\n```\r\nThis produces the same error\r\nCan you reopen this issue?", "@jvishnuvardhan Can you reopen this issue? ", "@jakesabathia2 and @srikris Can you please open a new issue with more details on the issue. It will be easy for others to follow learn for your issue. Thanks!", "https://github.com/tensorflow/tensorflow/issues/36391", "I have similar issue with the model NOT using LSTMs - I posted it here: https://github.com/tensorflow/tensorflow/issues/35987\r\n\r\n(with latest tf-nightly and experimental converter turned on)\r\n`File \"[...]/miniconda3/envs/tf-nightly-py3/lib/python3.7/site-packages/tensorflow_core/python/framework/convert_to_constants.py\", line 526, in _convert_variables_to_constants_v2_impl raise ValueError(\"Cannot find the Placeholder op that is an input \" ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.`"]}, {"number": 29673, "title": "Compiling TensorFlow when configured with CUDA and MPI support result\u2026", "body": "\u2026s in compilation failure:\r\n\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cu.cc(110): error: identifier \"CudaLaunchKernel\" is undefined\r\n\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cu.cc(111): error: identifier \"CudaLaunchKernel\" is undefined\r\n\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cu.cc(112): error: identifier \"CudaLaunchKernel\" is undefined\r\n\r\nInclude tensorflow/core/util/gpu_kernel_helper.h in ring.cu.cc to pull in declaartion of CudaLaunchKernel", "comments": []}, {"number": 29672, "title": "tf.image.crop_and_resize ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 14.04):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (1.9):\r\n\r\nHi all, \r\nI am looking at op crop_and_resize which takes image, boxes, box_index and crop_size as input. My question is why dimension of boxes is (num_boxes by, 4) while image has (batch, height, width, depth). Does it mean the same boxes for every image? And output is also (num_boxes, crop_height, crop_width, depth). Should it be (batch, num_boxes, crop_height, crop_width, depth)? \r\nThanks in advance for any help.\r\n", "comments": ["@ruinianxu It Looks This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "\"Should it be (batch, num_boxes, crop_height, crop_width, depth)?\" - It seems it shoud, but not implemented. In current state crop_and_resize breaks data flow in graph mode."]}, {"number": 29671, "title": "compact rendering of layer I/O shapes in plot_model", "body": "Render the input/output shapes of a layer more compactly by replacing `None` with `?`, e.g. render `(None,100,100)` to `(?,100,100)`.\r\n\r\nThis will impact only the human-read graph rendering of the model. By the way, I just tried\r\n```python\r\n    for name, tensor in zip(model.input_names, model.inputs):\r\n        print(name, ':', tensor.dtype.name, tensor.shape)\r\n```\r\nwhich produces output:\r\n```\r\ndecoder_inputs : int32 (?, 19)\r\n```\r\nTherefore, the Keras/TF tensor.shape is already formatted to use ```?``` instead of ```None```\r\n\r\n(last PR was stuck without response. try again)", "comments": ["Can we add a unit test?", "> Can we add a unit test?\r\n\r\n@pavithrasv why does this need a unit test? This is just for the rendering of human viewable icon. I attach a picture of what this will look like.\r\n\r\n![Screen Shot 2019-06-12 at 9 43 18 AM](https://user-images.githubusercontent.com/26907141/59370203-df6fcb80-8cf6-11e9-8e43-8443c25c8bff.png)\r\n\r\n", "> > Can we add a unit test?\r\n> \r\n> @pavithrasv why does this need a unit test? This is just for the rendering of human viewable icon. I attach a picture of what this will look like.\r\n> \r\n> ![Screen Shot 2019-06-12 at 9 43 18 AM](https://user-images.githubusercontent.com/26907141/59370203-df6fcb80-8cf6-11e9-8e43-8443c25c8bff.png)\r\n\r\nLooks good, thanks!"]}, {"number": 29670, "title": "Cherrypick important fixes to r2.0 branch.", "body": "", "comments": ["Btw, Kathy's change was rolled back due to memory leak. We will have to prepare a new PR with the rollforward cl once its submitted"]}, {"number": 29669, "title": "Refactor {Shard, Shuffle, & ShuffleAndRepeat} DatasetOps", "body": "This PR refactors `ShardDatasetOp`, `ShuffleDatasetOp`, and `ShuffleAndRepeatDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa Thanks for your quick review! The comments are helpful. The issues are addressed [here](https://github.com/tensorflow/tensorflow/pull/29669/commits/1574d5d24db17320ad19cca866488db54bb23593). Could you please have a look at the changes when you have time?", "@jsimsa The comments are addressed by this commit (https://github.com/tensorflow/tensorflow/pull/29669/commits/096d3c5b4c4f9bb91386d69749d9f77a3ab2e86c). Please take another look!"]}, {"number": 29668, "title": "The documentation has a cat instead of a bridge", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: https://www.tensorflow.org/tutorials/load_data/tf_records#fetch_the_images\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing): There are two cat images but the second cat image should be replace by the bridge image as the image refers to a bridge.\r\n\r\n### Clear description\r\n\r\nThe second cat image should be changed to the bridge as the url points a bridge but the cat image has been published\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@gowtham-kp I agree. Image was not rendered correctly on the TF website. But, images in google colab and GitHub source are showing correct images. Thanks!", "This is fixed."]}, {"number": 29667, "title": "Add release note about TF_CUDNN_DETERMINISTIC", "body": "This is the release note associated with:\r\n\r\n- PR [24747](https://github.com/tensorflow/tensorflow/pull/24747): Add cuDNN deterministic env variable (only for convolution)\r\n- PR [25269](https://github.com/tensorflow/tensorflow/pull/25269): Add deterministic cuDNN max-pooling\r\n- PR [25796](https://github.com/tensorflow/tensorflow/pull/25796): Added tests for TF_CUDNN_DETERMINISTIC", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac", "There is currently no release information for 1.14.0 in `RELEASE.md` in the master branch. How does one update the release notes for upcoming releases?", "Sorry, this was eager closing.", "Hi @bananabowl, this change missed the v1.14.0 tag. Do I need to do anything to get it included in a release version?", "Hi @duncanriach \r\n\r\nSorry for the late reply. Let's rebase this against the r1.14 branch (there should be release notes for 1.14.1 patch release, please add this there).\r\n\r\nPlease mention me (so I get notified) after you rebase. The goal is to get all cherrypicks by Wednesday so we can finalize the release by end of week", "Hi @mihaimaruseac, got it. Will do.", "Hi @mihaimaruseac, I've rebased this to the top of the `r1.14` branch. Also, note how I have included the same release note for `1.14.1` as for `1.14.0` with an additional note on `1.14.1` indicating that this changes was in `1.14.0`, but not mentioned in the `1.14.0` release notes tagged `v1.14.0`. Please review and confirm that this approach makes sense to you, or suggest an alternative. ", "This is perfect, thank you", "Note that there is a follow-up [PR 31389](https://github.com/tensorflow/tensorflow/pull/31389)."]}, {"number": 29665, "title": "more cuDNN problems following audio_recognition tutorial", "body": "The train.py script ran just fine, so I know my tensorflow-gpu installation is fine, and cuDNN is working.  But the label_wav.py script reports the infamous \"Failed to get convolution algorithm. This is probably because cuDNN failed to initialize...\"  During this exception the python process has successfully loaded cudnn64_7.dll and _pywrap_tensorflow_internal.pyd with no problem?  So question is, for v1.13.1 of tensorflow-gpu, on CUDA 10.0, what version of cudnn is needed exactly?  Would be nice if the release notes included the cudnn version required.\r\n\r\n**System information**\r\n- stock tensorflow example https://www.tensorflow.org/tutorials/sequences/audio_recognition\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, 1809\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951      \r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0 and cudnn64_7.dll \r\n- GPU model and memory: NVIDIA 1080 \r\n\r\n**Describe the current behavior**\r\nRunning the tutorial step fails with the cuDNN error:\r\n```\r\npython tensorflow/examples/speech_commands/label_wav.py --graph=/tmp/my_frozen_graph.pb --labels=/tmp/speech_commands_train/conv_labels.txt --wav=c:\\datasets\\speech_commands_v01\\audio\\left\\a5d485dc_nohash_0.wav\r\n```\r\n\r\n**Describe the expected behavior**\r\nShould work, especially since _pywrap_tensorflow_internal.pyd already successfully loaded into the python process, and so did cudnn64_7.dll\r\n\r\n**Code to reproduce the issue**\r\nSee your tutorial on audio recognition.\r\n\r\n**Other info / logs**\r\nAttached is the full output of the label_wav.py command, including exception details, and a list of all modules loaded into the python process at the time of the failure.\r\n[output.txt](https://github.com/tensorflow/tensorflow/files/3278584/output.txt)\r\n", "comments": ["@lovettchris Compatible version for tensorflow-gpu 1.13.1 is CUDA 10 and cuDNN 7.4. Let us know if this combination works for you. Thanks! ", "Thanks, I installed \"[Download cuDNN v7.4.2 (Dec 14, 2018), for CUDA 10.0](https://developer.nvidia.com/rdp/cudnn-archive#a-collapse742-10)\" from the cuDNN archive, now it works properly.  Can you add this version info to the release notes in the future?", "@lovettchris Sure, Glad it worked. Will close this issue. Thanks!"]}, {"number": 29664, "title": "Not all threads are terminated.", "body": "Windows 10 / Python 3.7 / TF 1.13 and TF from sources\r\n\r\nI used TF in embedded Python.\r\nAfter completion of the main program at the termination stage, an error occurs:\r\n```\r\n\tntdll.dll!NtWaitForAlertByThreadId\u001e()\tUnknown\r\n \tntdll.dll!RtlSleepConditionVariableSRW()\tUnknown\r\n \tKERNELBASE.dll!SleepConditionVariableSRW\u001e()\tUnknown\r\n \tmsvcp140.dll!__crtSleepConditionVariableSRW(_RTL_CONDITION_VARIABLE * pCond, _RTL_SRWLOCK * pLock, unsigned long dwMs, unsigned long flags) Line 659\tC++\r\n>\t[Inline Frame] msvcp140.dll!Concurrency::details::stl_condition_variable_win7::wait_for(Concurrency::details::stl_critical_section_interface *) Line 216\tC++\r\n \tmsvcp140.dll!Concurrency::details::stl_condition_variable_win7::wait(Concurrency::details::stl_critical_section_interface * lock) Line 210\tC++\r\n \tmsvcp140.dll!do_wait(_Cnd_internal_imp_t * cond, _Mtx_internal_imp_t * mtx, const xtime * target) Line 77\tC++\r\n \t_pywrap_tensorflow_internal.pyd!Eigen::ThreadPoolTempl<struct tensorflow::thread::EigenEnvironment>::WaitForWork(class Eigen::EventCount::Waiter *,struct tensorflow::thread::EigenEnvironment::Task *)\tC++\r\n \t_pywrap_tensorflow_internal.pyd!Eigen::ThreadPoolTempl<struct tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\tC++\r\n \t_pywrap_tensorflow_internal.pyd!std::_Func_impl_no_alloc<class <lambda_fe7aa395b13fe170862dcdb4d85eb030>,void>::_Do_call(void)\tC++\r\n \t_pywrap_tensorflow_internal.pyd!std::_LaunchPad<class std::unique_ptr<class std::tuple<class std::function<void > >,struct std::default_delete<class std::tuple<class std::function<void > > > > >::_Go(void)\tC++\r\n \t_pywrap_tensorflow_internal.pyd!std::_Pad::_Call_func(void *)\tC++\r\n \tucrtbase.dll!thread_start<unsigned int (__cdecl*)(void * __ptr64)>()\tUnknown\r\n \tkernel32.dll!BaseThreadInitThunk\u001e()\tUnknown\r\n \tntdll.dll!RtlUserThreadStart\u001e()\tUnknown\r\n```\r\nThis is a stack of one of the many threads in the dump. It looks like these threads have no background status.\r\n\r\nI can test this theory if you tell me what to fix in the source.\r\n", "comments": ["Will it be possible to provide the full code snippet that caused the error. It will help us proceed faster. Thanks! ", "```python\r\nimport tensorflow as tf\r\n\r\n    model = tf.keras.models.load_model(pathsave)\r\n    print(\"load\")\r\n\r\n    def getDouble(self, magic: int, value: float, array: tuple) -> tuple or list:\r\n        return model.predict(np.array([array], dtype=np.float64))[0].tolist()\r\n```\r\ngetDouble() is called in a loop.\r\nMy code runs and completes without errors.\r\n\r\nI understand that there is little information. But I do not know how to reproduce this error in another environment.", "After switching to normal Keras, the problem disappeared.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/29770", "Looks like the snippet provided is not complete. Can you help us to get the full code so as to reproduce it on Colab. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29663, "title": "tf.keras.models.Model's methods get_weights() and set_weights() should retrieve/return Tensors ", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRight now, get_weights() returns a numpy array, and set_weights expects a numpy array as input. Both functions should in my opinion also work for Tensor objects (and therefore be part of the computational graph).\r\n\r\n**Will this change the current api? How?**\r\nTwo new methods get_weights_tensor(), set_weights_tensor() could be added to tf.keras.models.Models\r\n\r\n**Who will benefit with this feature?**\r\nEverybody who wants to modify weights of layers in a loop style.\r\n\r\n**Any Other info.**\r\n", "comments": ["\"model.weights\" is actually the API to retrieve the weight/variable tensor. We don't expect user to reset the weight tensor for a layer/model, which is why we don't have a \"set_weights_tensor()\" as you proposed.\r\n\r\n\"Everybody who wants to modify weights of layers in a loop style.\" is bit confusing to me. Can you provide concrete code snippet to describe what you are trying to achieve?", "I am using the BFGS optimizer from tensorflow_probablity to train a neural network. This optimizer needs a vector of all variables that need to be updated, i.e. all weights and biases, hence why I would need something like \"get_weight_tensor()\". If I use \"get_weights()\" I will get a numpy array which is not part of the computational graph, and therefore I am not able to take derivatives. \r\n\r\nWhat has worked so far is to define a model in for example this way (I can easily change and retrieve weights as tensors):\r\n```\r\nself.hidden_layer_1 = tf.get_variable('hidden_layer_1', shape=[2, n_neurons], dtype=tf.float64, initializer=tf.initializers.random_normal)\r\nself.hidden_layer_2 = tf.get_variable('hidden_layer_2', shape=[n_neurons, n_neurons], dtype=tf.float64, initializer=tf.initializers.random_normal)\r\nself.hidden_layer_3 = tf.get_variable('hidden_layer_3', shape=[n_neurons, n_classes], dtype=tf.float64, initializer=tf.initializers.random_normal)\r\n\r\nself.bias_1 = tf.get_variable('bias_1', [1, n_neurons], dtype=tf.float64, initializer=tf.initializers.random_normal)\r\nself.bias_2 = tf.get_variable('bias_2', [1, n_neurons], dtype=tf.float64, initializer=tf.initializers.random_normal)\r\nself.bias_3 = tf.get_variable('bias_3', [1, n_classes], dtype=tf.float64, initializer=tf.initializers.random_normal)\r\n\r\nself.input_var = tf.placeholder(tf.float64, [None, 2]) \r\n\r\nlayer = tf.add(tf.matmul(self.input_var, self.hidden_layer_1), self.bias_1)\r\nlayer = tf.nn.relu(layer)\r\nlayer = tf.add(tf.matmul(layer, self.hidden_layer_2), self.bias_2)\r\nlayer = tf.nn.relu(layer)\r\nlayer = tf.add(tf.matmul(layer, self.hidden_layer_3), self.bias_3)\r\n```\r\n\r\nbut i would prefer to use a Sequential model (tf.keras.models.Sequential()) instead.", "As I stated, model.weights is just the get_weight_tensor() method you proposed. It will return you all the weight tensor that is attached to the model/layer. \r\n\r\nFrom the code snippet you provide, I don't see any part that is swapping the weight of a model. Can you be more explicit about the use case of changing weight tensor?", "> As I stated, model.weights is just the get_weight_tensor() method you proposed. It will return you all the weight tensor that is attached to the model/layer.\r\n\r\nOkay, that's great. How could I set the weights though?\r\n\r\n> From the code snippet you provide, I don't see any part that is swapping the weight of a model. Can you be more explicit about the use case of changing weight tensor?\r\n\r\nFor the above code, it looks like this:\r\n```\r\ndef model(self):\r\n\tlayer = tf.add(tf.matmul(self.input_var, self.hidden_layer_1), self.bias_1)\r\n\tlayer = tf.nn.relu(layer)\r\n\tlayer = tf.add(tf.matmul(layer, self.hidden_layer_2), self.bias_2)\r\n\tlayer = tf.nn.relu(layer)\r\n\tlayer = tf.add(tf.matmul(layer, self.hidden_layer_3), self.bias_3)\r\n\treturn layer\r\n\r\ndef function_evaluation(self, coord):\r\n\tsplit1, split2, split3, split4, split5, split6 = tf.split(coord, [2*self.n_neurons, self.n_neurons, self.n_neurons*self.n_neurons, self.n_neurons, self.n_classes*self.n_neurons, self.n_classes], 0)\r\n\r\n\tself.hidden_layer_1 = tf.reshape(split1, [2, self.n_neurons])\r\n\tself.bias_1 = tf.reshape(split2, [1, self.n_neurons])\r\n\tself.hidden_layer_2 = tf.reshape(split3, [self.n_neurons, self.n_neurons])\r\n\tself.bias_2 = tf.reshape(split4, [1, self.n_neurons])\r\n\tself.hidden_layer_3 = tf.reshape(split5, [self.n_neurons, self.n_classes])\r\n\tself.bias_3 = tf.reshape(split6, [1, self.n_classes])\r\n\r\n\toutput = self.model()\r\n\tloss = self.loss_function(output)\r\n\r\n\thidden_layer_1_gradients = tf.reshape(tf.gradients(loss, self.hidden_layer_1)[0], [2*self.n_neurons, 1])\r\n\tbias_1_gradients = tf.reshape(tf.gradients(loss, self.bias_1)[0], [self.n_neurons, 1])\r\n\thidden_layer_2_gradients = tf.reshape(tf.gradients(loss, self.hidden_layer_2)[0], [self.n_neurons*self.n_neurons, 1])\r\n\tbias_2_gradients = tf.reshape(tf.gradients(loss, self.bias_2)[0], [self.n_neurons, 1])\r\n\thidden_layer_3_gradients = tf.reshape(tf.gradients(loss, self.hidden_layer_3)[0], [self.n_classes*self.n_neurons, 1])\r\n\tbias_3_gradients = tf.reshape(tf.gradients(loss, self.bias_3)[0], [self.n_classes, 1])\r\n\r\n\tgradients = tf.concat([hidden_layer_1_gradients, bias_1_gradients, hidden_layer_2_gradients, bias_2_gradients, hidden_layer_3_gradients, bias_3_gradients], axis=0)\r\n\r\n\treturn loss, gradients\r\n\r\ndef training(self, session, x, y):\r\n\tresult = session.run(tfp.optimizer.bfgs_minimize(self.function_evaluation, initial_position=self.starting_vector, tolerance=self.tol, max_iterations=self.max_itr), feed_dict={self.input_var:x, self.output_var:y})\r\n\r\n\treturn result\r\n```\r\n\r\nThe argument \"coord\" in function_evaluation is the vector (Tensor) of weight&bias parameters, which is then distributed back on the weights and biases.", "OK, I see. Your model have 3 stacking dense layer, but somehow the weights of the dense layer need to be feed in, due the constrain of tfp.optimizer.bfgs_minimize. \r\n\r\nConceptually, the weight tensor of a layer/model shouldn't changed/swapped since the layer/model is a stateful container. Adding the API to swap the weight tensor will create all sorts of problem and pitfall. If you only need the math calculation part of dense layer, I would suggest to just stay with your current code snippet.\r\n\r\n@fchollet, do you have any other suggestions?", "Checked with Francois offline yesterday. He also agrees that allowing user to change the weight tensor will cause big issue and confuse user. We will close this issue as we don't intend to address it. You can leave your current code as is, or create a custom layer.", "@qlzh727 @fchollet @timudk \r\n\r\nhi, this makes it seemingly impossible to build a hypernetwork wrapper which uses \"get_weights\" and \"set_weights\" -- this API exists, but just doesnt work inside tf 2.0 bc we need numpy arrays to set tensorflow keras layer weights (which seems stupid, why don't we just use tensors?)\r\n\r\nI get that it \"might cause issues\" but ... the API as-is contains a broken \"set_weights\" feature, which fails in @tf.function decorated call methods, so, please either fix set_weights to use tensors or delete set_weights!  ", "I have the same problem with set_weights, when using tensorflow_probability. ", "@qlzh727 @fchollet @timudk\r\n\r\nIn popular metalearning approaches such as MAML, one needs to take gradients w.r.t. the initializations of the weights of the layers. \r\n\r\nIt seems that people so far are not using Keras model/layer for metalearning because of the API does not allow it. See e.g. https://github.com/cbfinn/maml/blob/master/maml.py#L202, https://github.com/dragen1860/MAML-TensorFlow/blob/master/maml.py, https://github.com/sudharsan13296/Hands-On-Meta-Learning-With-Python/tree/master/06.%20MAML%20and%20it's%20Variants .\r\n\r\nIn my understanding, addressing this issue would allow metalearning with Keras.\r\n\r\n"]}]