[{"number": 18064, "title": " reorg layer ", "body": "Any implementation of the reorg layer used in many models such as yolo v2....", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 49 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18063, "title": "Can't `import tensorflow` when `PYTHONOPTIMIZE=2`", "body": "### Issue\r\n\r\nThe Python runtime admits an environment variable called [`PYTHONOPTIMIZE`](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONOPTIMIZE). When `PYTHONOPTIMIZE` is set to `2` (eg. `export PYTHONOPTIMIZE=2`), Python can't `import tensorflow`, and I think the reason is that `PYTHONOPTIMIZE=2` strips away `.__doc__` strings.\r\n\r\n### Logs\r\n\r\n```\r\nPython 3.6.4 (default, Feb 19 2018, 12:57:53) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/da/git/py36/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"/home/da/git/py36/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"/home/da/git/py36/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py\", line 77, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"/home/da/git/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 41, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/home/da/git/py36/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 29, in <module>\r\n    from tensorflow.python.framework import c_api_util\r\n  File \"/home/da/git/py36/lib/python3.6/site-packages/tensorflow/python/framework/c_api_util.py\", line 23, in <module>\r\n    from tensorflow.python.util import compat\r\n  File \"/home/da/git/py36/lib/python3.6/site-packages/tensorflow/python/util/compat.py\", line 153, in <module>\r\n    remove_undocumented(__name__, _allowed_symbols)\r\n  File \"/home/da/git/py36/lib/python3.6/site-packages/tensorflow/python/util/all_util.py\", line 103, in remove_undocumented\r\n    should_have = make_all(module_name, doc_string_modules)\r\n  File \"/home/da/git/py36/lib/python3.6/site-packages/tensorflow/python/util/all_util.py\", line 55, in make_all\r\n    for m in _reference_pattern.finditer(doc_module.__doc__)\r\nTypeError: expected string or bytes-like object\r\n```\r\n\r\n### Workaround\r\n\r\nSet `PYTHONOPTIMIZE` to `0` or `1` (eg. `export PYTHONOPTIMIZE=0` or `export PYTHONOPTIMIZE=1`).\r\n\r\n--------\r\n**Have I written custom code.** No\r\n**OS Platform and Distribution.** Linux 4.15 Ubuntu 16.04\r\n**TensorFlow installed from.** Source\r\n**TensorFlow version.** 1.7.0-rc1\r\n**Bazel version.** N/A\r\n**CUDA/cuDNN version.** CUDA 9.1 / cuDNN 7.0.5\r\n**GPU model and memory.** NVIDIA Titan V 12GB\r\n**Exact command to reproduce.** `$ export PYTHONOPTIMIZE=2 && python -c \"import tensorflow\"`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler done", "@yifeif  can you please take a look or redirect? Thanks.\r\n", "Nagging Assignee @bignamehyp: It has been 136 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for bring this up @etale-cohomology. TensorFlow uses __doc__ in a lot of places, which won't work with setting PYTHONOPTIMIZE to 2. This is WAI. I will close this issue.", "It's not robust behavior that the library crashes when it encounters an empty `__doc__` string. But, still, thanks for looking at this."]}, {"number": 18062, "title": "[INTEL MKL] utilize test_util.IsMklEnabled() to check if the MKL support is turned on or not", "body": "", "comments": ["@cuixiaom thanks for the update."]}, {"number": 18061, "title": "Disable the toco binary in pip feature until it can used shared libs", "body": "The binary size was doubled by the saved model change. Since to process\r\nsaved models most of the TensorFlow runtime is needed. A workaround to\r\nthis is in the works and should be submitted in the next couple weeks.", "comments": ["@aselle please fix linter errors."]}, {"number": 18060, "title": "Tensorflow lite c++ shared library include headers file", "body": "Hi ,\r\n1.I build tensorflowlite.so library as describe in https://github.com/tensorflow/tensorflow/issues/16219\r\n2.I added lib to my Android studio project.\r\n3.In C++ class header I added \r\n#include \"tensorflow/contrib/lite/kernels/register.h\"\r\n#include \"tensorflow/contrib/lite/model.h\"\r\n#include \"tensorflow/contrib/lite/string_util.h\"\r\n#include \"tensorflow/contrib/lite/tools/mutable_op_resolver.h\"\r\n\r\n4.Error compilation :\r\n```\r\n  ../../../../../../../3rdparty/tensorflow/tf_lite/tensorflow/contrib/lite/schema/schema_generated.h:21:37: fatal error: flatbuffers/flatbuffers.h: No such file or directory\r\n   #include \"flatbuffers/flatbuffers.h\"\r\n                                       ^\r\n  compilation terminated.\r\n  ninja: build stopped: subcommand failed.\r\n```\r\nPlease advise how to resolve it,\r\nThanks.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu\r\n- **TensorFlow installed from (source or binary)**:\r\nSources\r\n- **TensorFlow version (use command below)**:\r\nMaster\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.11\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNDK 14\r\n- **CUDA/cuDNN version**:\r\nn/A\r\n- **GPU model and memory**:\r\nn/A\r\n- **Exact command to reproduce**:\r\nbazel build  //tensorflow/contrib/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"\r\n\r\n", "comments": ["@dimitryn how did you solve this? I have the same problem (flatbuffers.h)", "1.this is my include in app\r\n#include <tensorflow/contrib/lite/context.h>\r\n#include \"tensorflow/contrib/lite/builtin_op_data.h\"\r\n#include \"tensorflow/contrib/lite/kernels/internal/tensor.h\"\r\n#include \"tensorflow/contrib/lite/kernels/kernel_util.h\"\r\n#include \"tensorflow/contrib/lite/kernels/op_macros.h\"\r\n\r\n2.in  lite BUILD file\r\ncc_binary(\r\n    name = \"libtensorflowLite.so\",\r\n    linkshared=1,\r\ndeps = [\r\n    \":framework\",\r\n    \"//tensorflow/contrib/lite/kernels:builtin_ops\",\r\n],\r\n\r\n)", "I was trying to compile the minimal.cc, or something similar that can load a model and execute. Do you have that kind of example?", "@dimitryn I have the same problem, have you solved this by using these includes?\r\n#include \"tensorflow/contrib/lite/kernels/register.h\"\r\n#include \"tensorflow/contrib/lite/model.h\"\r\n#include \"tensorflow/contrib/lite/string_util.h\"\r\n#include \"tensorflow/contrib/lite/tools/mutable_op_resolver.h\".\r\n\r\nI find \"flatbuffers/flatbuffers.h\" is a third-party library, do we need to install it additionally?", "@conser12  we using this include\r\nin h file\r\n#include \"tensorflow/contrib/lite/string_util.h\"\r\n#include \"tensorflow/contrib/lite/model.h\"\r\n#include \"tensorflow/contrib/lite/context.h\"\r\n#include \"tensorflow/contrib/lite/interpreter.h\"\r\n#include \"tensorflow/contrib/lite/kernels/register.h\"\r\n#include \"tensorflow/contrib/lite/kernels/internal/tensor.h\"\r\nin cpp\r\n#include <tensorflow/contrib/lite/context.h>\r\n#include \"tensorflow/contrib/lite/builtin_op_data.h\"\r\n#include \"tensorflow/contrib/lite/kernels/internal/tensor.h\"\r\n#include \"tensorflow/contrib/lite/kernels/kernel_util.h\"\r\n#include \"tensorflow/contrib/lite/kernels/op_macros.h\""]}, {"number": 18059, "title": "Update version strings to 1.7", "body": "", "comments": []}, {"number": 18058, "title": "Large multinomial sampling on GPU causes OOM", "body": "Sampling m samples from `tf.multinomial` with support n allocates an unnecessarily large n x m tensor when running on a GPU. For large n and m, this causes out of memory errors. It should be possible to sample from a multinomial distribution without allocating such a large tensor (output tensor is only b x m, where b is the batch size).\r\n\r\n### Example and traceback:\r\n```\r\nsample_indices = tf.multinomial(tf.ones([1, 1e6]), 10000, output_dtype=tf.int32)  # sample_indices has shape [1 x 1e4]\r\nwith tf.Session() as sess:\r\n    output = sess.run([sample_indices])\r\n```\r\n\r\nTraceback:\r\n\r\n> ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,10000,1000000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[Node: multinomial/Multinomial = Multinomial[T=DT_FLOAT, output_dtype=DT_INT32, seed=0, seed2=0,\r\n_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ones, multinomial/Multinomial/num_samples)]]\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Springdale LInux 7.4 (Redhat)\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0.5\r\n- **GPU model and memory**: TITAN X (Pascal) 12GB\r\n- **Exact command to reproduce**: (see above)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Still an issue as of version 1.8.0. Workaround is to sample on the CPU and transfer results to GPU, though its a performance hit.", "@mbarnes1 sorry for the slow reply. Just to check, m and n are 1e6 and 1e4 in your example?\r\n\r\n@ebrevdo is this memory behavior intended for tf.multinomial?\r\n", "@cy89 to clarify:\r\nm = 1e4  (the number of requested samples)\r\nn = 1e6  (the distribution support size)", "I believe this is a performance optimization for GPU.  Certain noise is calculated all at once and then combined to generate the multinomial samples.  On the CPU the generator is executed sequentially.  I'm not sure the best way to trade off performance vs. size here.", "@ekelsen thoughts?  our implementation isn't the most memory efficient.", "@zheng-xq can you comment about whether the efficiency of GPU multinomial is on your radar? If not, should we mark this \"contributions welcome\"?", "This is also a big issue for me. \r\n\r\nI have: \r\nbatch size=1\r\nnumber of samples=75520\r\nnumber of classes=358287\r\n\r\nIn this context, allocating a tensor of shape (batch size, number of samples, number of classes) is crazy. If there is a trade-off with speed, an alternate function would be welcome, so that the user can choose what he needs.", "Hi @mbarnes1 ! 1.x are not supported any more. Can we move this issue to closed status as it is not replicating in [2.8 version](https://colab.sandbox.google.com/gist/mohantym/fe69faad81f77e8dd24137d04ce36a50/github_18058.ipynb#scrollTo=QofEH8R1FEhD)? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 18057, "title": "Fix broken wiki link of Positive-definite_matrix in linalg api guide", "body": "This PR is to fix the broken wiki link of Positive-definite_matrix in linalg related api docs.\r\n\r\nAs we can see in linalg related guides like [tf.linalg.LinearOperatorComposition](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorComposition#__init__), the below link is not correct, which should be https://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices instead.\r\n> Note that we do not require the operator to be self-adjoint to be positive-definite. See: **https://en.wikipedia.org/wiki/Positive-definite_matrix\\ #Extension_for_non_symmetric_matrices**", "comments": []}, {"number": 18056, "title": "Trying to fix libtensorflow GPU build.", "body": "CUDNN path error. \r\nInvalid path to cuDNN 7 toolkit. None of the following files can be found:\r\nC:/tools/cuda\\lib/x64/cudnn.lib\r\nC:/tools/cuda\\lib/x64/cudnn.lib", "comments": []}, {"number": 18055, "title": "run_test_in_graph_and_eager_modes does not work", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.3\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.6.0-0-gd2e24b6039 1.6.0\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\nI have code that was written to work in graph mode, and I am now converting it to work in eager mode as well.  I am trying to use the `run_test_in_graph_and_eager_modes` decorator to make my tests run in both modes.  When I add `@run_test_in_graph_and_eager_modes()` in front of a test method, it simply causes the test not to run at all, in either mode.  I run the tests with `nosetests` version 1.3.7.  There is no error message or any indication that something has gone wrong.  The decorated tests are just silently skipped.", "comments": ["Could you provide some more detail, or instructions to reproduce? Here's what I tried:\r\n\r\n- Create `test.py`:\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyTest(tf.test.TestCase):\r\n  @tf.contrib.eager.run_test_in_graph_and_eager_modes()\r\n  def test_foo(self):\r\n    print(\"Hello\")\r\n\r\nif __name__ == \"__main__\":\r\n  tf.test.main()\r\n```\r\n\r\n- `python ./test.py`\r\n\r\nAnd I see `Hello` printed twice, as expected.\r\nSo, if there is a bug, it's triggered by your specific use - if you could provide instructions to reproduce, we can investigate.\r\n\r\nThanks.", "I tried your test, and sure enough if I run it directly it works.  However, if I run it using `nosetests` it gets skipped:\r\n\r\n```\r\n$ nosetests --nocapture -v test.py\r\nReturns a TensorFlow Session for use in executing tests. ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.001s\r\n\r\nOK\r\n```\r\n\r\nIn addition, if I tell it to run just that one test rather than all tests in the file\r\n\r\n```\r\n$ nosetests test.py:MyTest.test_foo\r\n```\r\n\r\nthen it fails:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/peastman/miniconda3/lib/python3.6/site-packages/nose/failure.py\", line 39, in runTest\r\n    raise self.exc_val.with_traceback(self.tb)\r\n  File \"/Users/peastman/miniconda3/lib/python3.6/site-packages/nose/loader.py\", line 522, in makeTest\r\n    return self._makeTest(obj, parent)\r\n  File \"/Users/peastman/miniconda3/lib/python3.6/site-packages/nose/loader.py\", line 576, in _makeTest\r\n    return parent(obj.__name__)\r\n  File \"/Users/peastman/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/test_util.py\", line 708, in __init__\r\n    super(TensorFlowTestCase, self).__init__(methodName)\r\n  File \"/Users/peastman/miniconda3/lib/python3.6/unittest/case.py\", line 399, in __init__\r\n    (self.__class__, methodName))\r\nValueError: no such test method in <class 'test.MyTest'>: decorated\r\n```", "I'm not familiar enough with `nosetests` to provide help here. From the stacktrace it seems that somehow the use of `nosetests` is resulting in an incorrect import of TensorFlow where it is looking for the `decorated` symbol in the wrong module.\r\n\r\nMarking as \"Community Support\" since I don't believe anyone on the team here has the expertise to debug interactions with `nosetests`.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "This issue seems to be with Tensorflow version 1.6. Please try with the latest Tensorflow version and open a new issue if this problem still persists."]}, {"number": 18054, "title": "Fix the incorect format of math equation in factorization_ops", "body": "This PR is to fix the incorrect rendering of math equation in [tf.contrib.factorization.WALSModel](https://www.tensorflow.org/versions/r1.7/api_docs/python/tf/contrib/factorization/WALSModel).\r\n\r\nThis PR fixed the incorrect math equation rendering in above api guides according to the\r\n[Math in markdown](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md#math-in-markdown) guideline. That is using $$, \\( and \\) to surround the math equation.\r\n\r\nTake the [math equation](https://www.tensorflow.org/versions/r1.7/api_docs/python/tf/contrib/factorization/WALSModel#class_walsmodel) as an example to show the fixing result:\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1680977/38040047-3bd9af06-32e1-11e8-9fa3-a93a9820c5b4.png)\r\n\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1680977/38040098-5f2a0262-32e1-11e8-8239-aa02c87c2823.png)\r\n", "comments": ["Thanks!", "@imsheridan  The \"F\" should be a subscript on the norm (Frobenius norm). Can you fix that? I might have let another instance of that slip by yesterday too. Would you mind fixing?", "@rmlarsen Could you please help point out which \"F\"?", "@rmlarsen Fixed the \"F\" should be a subscript on the norm (Frobenius norm) as below:\r\n![image](https://user-images.githubusercontent.com/1680977/38041900-8a6171a0-32e5-11e8-92f9-3e80e7b72a39.png)", "@imsheridan Thanks!", "@imsheridan would you mind checking the same issue matrix_solve_ls again?", "@imsheridan please fix linter errors (probably >80 char lines)", "@rmlarsen I fixed the reported lint errors. Yeah sure I'm checking whether the same issue occurs in matrix_solve_ls.", "@rmlarsen Just FYI, I checked [matrix_solve_ls PR](https://github.com/tensorflow/tensorflow/pull/18022) didn't have the same above subscript \"F\" issue, so don't needs to change :)"]}, {"number": 18053, "title": "Lowercase tf.print when print is a function?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: n/a\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a\r\n- **TensorFlow installed from (source or binary)**: n/a\r\n- **TensorFlow version (use command below)**: n/a\r\n- **Python version**: n/a\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nSince TensorFlow must be compatible with Python 2.7, `tf.Print` is uppercase.  However, `tf.print` would work fine for Python 3 users and Python 2 users with `from __future__ import print_function`?  There's no difficulty adding this to the source, since all TensorFlow source files have `from __future__ import print_function`, and it wouldn't interfere with any other 2.7 users.\r\n\r\nObjections to me adding?", "comments": ["For API addition: Looks good, go ahead :)", "Will add once I can build TensorFlow again: looks like it's broken after an Xcode upgrade. :/\r\n\r\n    ERROR: /private/var/tmp/_bazel_irving/d4bad52af7e8ccc6c91f211db4181990/external/flatbuffers/BUILD:52:1: undeclared\r\n    this rule is missing dependency declarations for the following files included by 'external/flatbuffers/src/util.cp\r\n      '/Library/Developer/CommandLineTools/usr/lib/clang/9.1.0/include/stdarg.h'\r\n      '/Library/Developer/CommandLineTools/usr/lib/clang/9.1.0/include/stddef.h'\r\n      '/Library/Developer/CommandLineTools/usr/lib/clang/9.1.0/include/__stddef_max_align_t.h'\r\n      '/Library/Developer/CommandLineTools/usr/lib/clang/9.1.0/include/stdint.h'\r\n      '/Library/Developer/CommandLineTools/usr/lib/clang/9.1.0/include/limits.h'\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n    Use --verbose_failures to see the command lines of failed build steps.\r\n    INFO: Elapsed time: 7.703s, Critical Path: 1.21s\r\n    FAILED: Build did NOT complete successfully\r\n", "@girving did you find a fix or workaround for the issue regarding the Xcode upgrade? I'm getting the same error.", "@raliste I think eventually all it took was upgrading Bazel via`brew upgrade`.", "Thank you @girving, that did solve the issue.", "@girving : There are a few other improvements to `tf.Print` that we were thinking of folding into `tf.print` (so that it appears and acts a little more like `print`). So, we're considering undoing this export for the 1.10 release (which will be cut on July 12) and then restoring it for the next release.\r\n\r\nPlease speak up loudly if you object :)", "So the idea is that `tf.Print` and `tf.print` would have incompatible behavior?  This seems like a bad idea to me if so, but I don't object strongly enough to work to prevent the mistake.\r\n\r\nNote that `tf.print` is accessible only from Python 3 or Python 2.7 with `from __future__ import print_function`.  Making functionality available only with that constraint and making the natural alias do something different would be confusing.\r\n\r\nIf I've misunderstood and they'll have the same behavior, then I'm not unsure why it needs to be removed and then added back.", "Discussed with Martin in person.  I'm fine with the change.  Amused, even.", "Nagging Assignee @tomerk: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "`tf.print` has now been added as an operator."]}, {"number": 18052, "title": "Raise a nicer error message when trying to call gradients with while loop", "body": "This PR is to fix #7404.\r\nWhen you import a graph with a \"while\" loop, you can't calculate gradients as you could on the original graph. e.g.\r\n> import tensorflow as tf\r\n> i=tf.constant(0, name=\"input\")\r\n> out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=\"output\")\r\n> graph_def = tf.get_default_graph().as_graph_def()\r\n> \r\n> g = tf.Graph()\r\n> with g.as_default():\r\n>     tf.import_graph_def(graph_def)\r\n> s = tf.Session(graph=g)\r\n> i_imported = g.get_tensor_by_name(\"import/input:0\")\r\n> out_imported = g.get_tensor_by_name(\"import/output/Exit:0\")\r\n> tf.gradients(out_imported, i_imported)\r\n\r\nBut if you save and restore the metagraph (via `tf.train.export_meta_graph` and `tf.train.import_meta_graph`, then you can take the gradient in the restored session. So it's just a problem if you try serializing and importing the GraphDef itself\r\n/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in AddWhileContext(self, op, between_op_list, between_ops)\r\n>    1102     if grad_state is None:\r\n>    1103       # This is a new while loop so create a grad state for it.\r\n> -> 1104       outer_forward_ctxt = forward_ctxt.outer_context\r\n>    1105       if outer_forward_ctxt:\r\n>    1106         outer_forward_ctxt = outer_forward_ctxt.GetWhileContext()\r\n>    AttributeError: 'NoneType' object has no attribute 'outer_context'\r\n\r\nAs said in above discussion, we needs to raise a nicer error message when trying to call gradients on a while loop, so that users don't have to guess the graph vs. metagraph issue.\r\n> I believe that the forward_ctxt object here:\r\n> forward_ctxt.outer_context\r\n> should have a @property outer_context; and if no such internal object is set, it can raise a ValueError/InternalError that maybe you're trying to call gradients on a while loop without properly serializing your graph via MetaGraphDef.", "comments": ["@rmlarsen Thanks for your help. I fixed corresponding syntax and lint error, could you please kindly review again?", "@tensorflow-jenkins test this please"]}, {"number": 18051, "title": "documenting that init_op will not be run when loading from checkpoint", "body": "see #17969 \r\nI have also clarified the doc that if both `init_op` and `init_fn` are not `None`, both will be called. ", "comments": []}, {"number": 18050, "title": "what is the correct format of python command?", "body": "Hi guys,\r\ni am running tensorflow-gpu in windows environment of anaconda. the backslash \\ doesnt work in anaconda prompt eg:\r\n`python train_model.py \\\r\n    --training_data=rnn_tutorial_data/training.tfrecord-?????-of-????? \\\r\n    --eval_data=rnn_tutorial_data/eval.tfrecord-?????-of-????? \\\r\n    --classes_file=rnn_tutorial_data/training.tfrecord.classes`\r\nit works in my ubuntu terminal. i dont want to mess with ubuntu cuz the cuda is 9.1 which doesnt support tensorflow1.6 yet.  what is the correct format command in windows of \\ backslash?\r\n\r\nHope to hear your suggestions.", "comments": ["The eqiuvalent character in the Windows command prompt is `^`.\r\n\r\nhttps://stackoverflow.com/q/69068/3574081", "thank you @mrry! you are the best."]}, {"number": 18049, "title": "Update api.py", "body": "Avoid overwriting existing namespace items that might replace the converted functions.", "comments": ["@mdanatg looks reasonable to me. Who else should review this, if any?", "@rmlarsen Thanks! We reviewed this internally, so it should be good to go as soon as the builds are green."]}, {"number": 18048, "title": "Fix typos", "body": "This PR fixes some typos: `parition`, `avaliable`, `exection`, `ouput`, and `suports`.", "comments": []}, {"number": 18047, "title": "Replace PLATFORM_WINDOWS to _MSC_VER as it only applies to MSVC", "body": "Attempt to fix Bazel Windows build.\r\n\r\n/cc @meteorcloudy ", "comments": ["Hi @rongjiecomputer , I have a fix internally for this issue.\r\nThat's by changing `PLATFORM_WINDOWS` to `COMPILER_MSVC` in this file.\r\n`PLATFORM_WINDOWS` is only defined in `tf_copts`, `COMPILER_MSVC` is defined in CROSSTOOL, so it's guaranteed to exist.", "@meteorcloudy Can you change the internal fix to `_WIN32` and `_MSC_VER` where appropriate instead of relying on `COMPILER_MSVC` from `CROSSTOOL`?\r\n\r\nI was thinking about asking Bazel to remove `-DCOMPILER_MSVC` from `CROSSTOOL` completely. IMO, build system should add as little compile/link flags as possible.\r\n\r\nMany Google projects like Protobuf and Chromium do use `COMPILER_MSVC` macro, but they are defined in header file and IWYU rule is enforced. Only Tensorflow is not following the rule.\r\n\r\nEDIT: Abseil C++ just use `_WIN32` and `_MSC_VER` directly.", "I am not against using `PLATFORM_WINDOWS` and `COMPILER_MSVC` if they are defined in header files and IWYU rule is enforced (this is what Chromium does), but `-D` for platform/compiler configuration should be avoided (that is, `PLATFORM_WINDOWS` should not come from `tf_copts` and `COMPILER_MSVC` should not come from `CROSSTOOL`).", "@rongjiecomputer I think you are right. Here we also need to distinguish MSVC and clang, as you are working on Clang support for TensorFlow IIRC.\r\n\r\nSo maybe change it to:\r\n```\r\n#if defined(_MSC_VER) && !defined(__clang__)\r\n#include \"third_party/tensorflow/core/platform/windows/cpu_info.h\"\r\n#endif\r\n```\r\nThen remove the check in `third_party/tensorflow/core/platform/windows/cpu_info.h` ?\r\n", "BTW, I want to work on a CROSSTOOL for clang when I have more time, but recently I'm quite occupied. ", "> So maybe change it to:\r\n```\r\n#if defined(_MSC_VER) && !defined(__clang__)\r\n#include \"tensorflow/core/platform/windows/cpu_info.h\"\r\n#endif\r\n```\r\n> Then remove the check in `tensorflow/core/platform/windows/cpu_info.h` ?\r\n\r\n`tensorflow/core/platform/windows/cpu_info.h` will include `intrin.h` which Clang+MSVC also needs. I will change `PLATFORM_WINDOWS` to `_MSC_VER` in and still keep the `__clang__` check in `tensorflow/core/platform/windows/cpu_info.h`. Sounds good?", "Yes, please do that!", "Done.", "Testing at http://ci.tensorflow.org/view/TF%20pull%20requests/job/tensorflow-pr-win-bazel/61/console", "Everything compiles successfully! (Tests still fail though, but that is not what this PR is for).\r\n\r\nI envy the build speed of the CI server \ud83d\ude06 . A change in `tensorflow/core` header file normally will trigger a complete re-build and yet the CI job managed to finish < 30 minutes."]}, {"number": 18046, "title": "small update ctc_ops docstring", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed the CLA", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 18045, "title": "Feature Request: Raise Error when using queue without starting it", "body": "When using functions like `tf.train.batch()` or `tf.train.shuffle_batch()` and probably more,  one needs to start the queue with `tf.train.start_queue_runners(sess)`. If used without it, above functions are still executable but hang indefinitely. I suggest raising an error if the queue hasn't been started before, as this has costed me a lot of time to figure out.\r\n\r\nAlternatively point more directly to this usage in the documentation of the [functions](https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch) and the [guide](https://www.tensorflow.org/api_guides/python/reading_data#batching).\r\n\r\nThis might help a lot of people!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Thanks for reporting the issue. Please feel free to submit the PR that updates the documentation as you suggest."]}, {"number": 18044, "title": "FailedPreconditionError (see above for traceback): Failed to rename: <file_name> to: <file_name> : The process cannot access the file because it is being used by another process. ; Broken pipe", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Pro 64-bit (10.0, Build 16299)\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.6.0\r\n- **Python version**: \r\nb'unknown' 1.6.0\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI try to run a model using tf.train.MonitoredTrainingSession(), but get an error when the CheckPointSaverHook() tries to save the model. Having loked at the target saving directory, the saver seems to create a temporary directory from shards (something I assume is done because of the model being quite big) for later use. When the saver later on tries to use the sharded files in this directory, it seems to get blocked by another process accessing those files, resulting in a broken pipe error.\r\n\r\nI assume the problem has to do with the sharding mechanism, as this is the first time I've seen the saver having to save the checkpoints in shards. I am not sure though, so if you're sure something else is causing this error, you're probably right.\r\n\r\n### Source code / logs\r\nBelow is the error message I get when running my code. \r\n\r\n```\r\nCaused by op 'save/SaveV2', defined at:\r\n  File \"em_routing_train.py\", line 218, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"em_routing_train.py\", line 214, in main\r\n    train()\r\n  File \"em_routing_train.py\", line 135, in train\r\n    log_device_placement=FLAGS.log_device_placement)) as mon_sess:\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 384, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 795, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 518, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 981, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 986, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 675, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 437, in create_session\r\n    self._scaffold.finalize()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 212, in finalize\r\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 871, in _get_saver_or_default\r\n    saver = Saver(sharded=True, allow_empty=True)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1293, in __init__\r\n    self.build()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1302, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1339, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 787, in _build_internal\r\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 411, in _AddShardedSaveOps\r\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 385, in _AddShardedSaveOpsForV2\r\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 326, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 241, in save_op\r\n    tensors)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1286, in save_v2\r\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nFailedPreconditionError (see above for traceback): Failed to rename: \r\n./tmp/em_routing_train3\\model.ckpt-1_temp_79c748505e6941cfa43f30080736273a/part-00000-of-\r\n00001.index.tempstate15074727511180888636 to: ./tmp/em_routing_train3\\model.ckpt-\r\n1_temp_79c748505e6941cfa43f30080736273a/part-00000-of-00001.index : The process cannot access \r\nthe file because it is being used by another process.\r\n; Broken pipe\r\n         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., \r\nDT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], \r\n_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](save/ShardedFilename, \r\nsave/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Class_caps/capsule_cl_tr/beta_u/beta_u, \r\nClass_caps/capsule_cl_tr/beta_u/beta_u/Adam, Class_caps/capsule_cl_tr/beta_u/beta_u/Adam_1, \r\nClass_caps/capsule_cl_tr/weights/Class_caps/capsule_cl_tr/weights, \r\nClass_caps/capsule_cl_tr/weights/Class_caps/capsule_cl_tr/weights/Adam, \r\nClass_caps/capsule_cl_tr/weights/Class_caps/capsule_cl_tr/weights/Adam_1, ReLu_Conv1/conv2d/bias, \r\nReLu_Conv1/conv2d/bias/Adam, ReLu_Conv1/conv2d/bias/Adam_1, ReLu_Conv1/conv2d/kernel, \r\nReLu_Conv1/conv2d/kernel/Adam, ReLu_Conv1/conv2d/kernel/Adam_1, beta1_power, beta2_power, \r\nbeta_a/beta_a, beta_a/beta_a/Adam, beta_a/beta_a/Adam_1, convCaps1/beta_u/beta_u, \r\nconvCaps1/beta_u/beta_u/Adam, convCaps1/beta_u/beta_u/Adam_1, convCaps1/weights/weights, \r\nconvCaps1/weights/weights/Adam, convCaps1/weights/weights/Adam_1, convCaps2/beta_u/beta_u, \r\nconvCaps2/beta_u/beta_u/Adam, convCaps2/beta_u/beta_u/Adam_1, convCaps2/weights/weights, \r\nconvCaps2/weights/weights/Adam, convCaps2/weights/weights/Adam_1, global_step, \r\nprimaryCaps/weights1, primaryCaps/weights1/Adam, primaryCaps/weights1/Adam_1, \r\nprimaryCaps/weights2, primaryCaps/weights2/Adam, primaryCaps/weights2/Adam_1)]]\r\n```\r\n\r\nThe important part of this error message is, as I see it:\r\n\r\n```\r\nFailedPreconditionError (see above for traceback): Failed to rename: \r\n./tmp/em_routing_train3\\model.ckpt-1_temp_79c748505e6941cfa43f30080736273a/part-00000-of-\r\n00001.index.tempstate15074727511180888636 to: ./tmp/em_routing_train3\\model.ckpt-\r\n1_temp_79c748505e6941cfa43f30080736273a/part-00000-of-00001.index : The process cannot access \r\nthe file because it is being used by another process.\r\n; Broken pipe\r\n```\r\n", "comments": ["i m also having same problem while training a model. Can anyone please help me to solve this error", "Hi @prasilla487 and @aforslow ,\r\ncould one of you provide us a small self sufficient script which reproduces this issue?\r\nthanks", "Same here ! I'm working on an Azure virtual machine, which may cause delays. The only way I see to move around the problem is to space out the saver calls... but it's not fail-safe !", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to low activity", "Hello, I have an issue while training the data. \r\n![Screenshot (263)](https://user-images.githubusercontent.com/72600760/98488193-125f1b80-226b-11eb-9a8f-8ac46b331779.png)\r\n![Screenshot (263)](https://user-images.githubusercontent.com/72600760/98488213-3589cb00-226b-11eb-8ca8-cc441c95a0c9.png)\r\nCan anyone help me with this?"]}, {"number": 18043, "title": "module 'tensorflow' has no attribute 'Variable'", "body": "I used to install tensorflow.  But after I found that it was cpu version, I installed tensorflow-gpu and uninstalled the original tensorflow,which is the cpu version.  But when I used this module, there was something wrong, which said module 'tensorflow' had no attribute 'Variable'.  I don't know how to fix it.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18042, "title": "AttributeError: module 'pandas' has no attribute 'core'", "body": "Hi, \r\n\r\nPlease find the link to my [code](https://github.com/KathiravanNatarajan/MAWILAB_classfication/blob/master/Conv_1D.ipynb). \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-5960795275b2> in <module>()\r\n     10 \r\n     11 # Output of the model\r\n---> 12 output = conv1D_Net1(features)\r\n     13 \r\n     14 # Calculating cost\r\n\r\n<ipython-input-8-80fe61955189> in conv1D_Net1(input_1d)\r\n      8     pool1 = tf.nn.pool(conv1d, [2], 'MAX', 'SAME', strides = [2])\r\n      9      #Flattenning the output of ConvNets\r\n---> 10     flat_conv2 = tf.contrib.layers.flatten(pool1)\r\n     11 \r\n     12     datasize = flat_conv2.get_shape().as_list()[1]\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py in __getattr__(self, item)\r\n     33     # Replace the lazy loader with the imported module itself.\r\n     34     import importlib  # pylint: disable=g-import-not-at-top\r\n---> 35     contrib = importlib.import_module('tensorflow.contrib')\r\n     36     return getattr(contrib, item)\r\n     37 \r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    124                 break\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n    128 \r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\importlib\\_bootstrap_external.py in exec_module(self, module)\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\__init__.py in <module>()\r\n     27 from tensorflow.contrib import deprecated\r\n     28 from tensorflow.contrib import distributions\r\n---> 29 from tensorflow.contrib import factorization\r\n     30 from tensorflow.contrib import framework\r\n     31 from tensorflow.contrib import graph_editor\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\factorization\\__init__.py in <module>()\r\n     22 from tensorflow.contrib.factorization.python.ops.clustering_ops import *\r\n     23 from tensorflow.contrib.factorization.python.ops.factorization_ops import *\r\n---> 24 from tensorflow.contrib.factorization.python.ops.gmm import *\r\n     25 from tensorflow.contrib.factorization.python.ops.gmm_ops import *\r\n     26 # pylint: enable=wildcard-import\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\factorization\\python\\ops\\gmm.py in <module>()\r\n     30 from tensorflow.contrib.framework.python.framework import checkpoint_utils\r\n     31 from tensorflow.contrib.framework.python.ops import variables\r\n---> 32 from tensorflow.contrib.learn.python.learn import graph_actions\r\n     33 from tensorflow.contrib.learn.python.learn import monitors as monitor_lib\r\n     34 from tensorflow.contrib.learn.python.learn.estimators import estimator as estimator_lib\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\__init__.py in <module>()\r\n     81 \r\n     82 # pylint: disable=wildcard-import\r\n---> 83 from tensorflow.contrib.learn.python.learn import *\r\n     84 # pylint: enable=wildcard-import\r\n     85 \r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\__init__.py in <module>()\r\n     21 \r\n     22 # pylint: disable=wildcard-import\r\n---> 23 from tensorflow.contrib.learn.python.learn import *\r\n     24 # pylint: enable=wildcard-import\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\__init__.py in <module>()\r\n     23 from tensorflow.contrib.learn.python.learn import basic_session_run_hooks\r\n     24 from tensorflow.contrib.learn.python.learn import datasets\r\n---> 25 from tensorflow.contrib.learn.python.learn import estimators\r\n     26 from tensorflow.contrib.learn.python.learn import graph_actions\r\n     27 from tensorflow.contrib.learn.python.learn import learn_io as io\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\__init__.py in <module>()\r\n    308 from tensorflow.contrib.learn.python.learn.estimators._sklearn import NotFittedError\r\n    309 from tensorflow.contrib.learn.python.learn.estimators.constants import ProblemType\r\n--> 310 from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier\r\n    311 from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNRegressor\r\n    312 from tensorflow.contrib.learn.python.learn.estimators.dnn_linear_combined import DNNLinearCombinedClassifier\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py in <module>()\r\n     27 from tensorflow.contrib.layers.python.layers import optimizers\r\n     28 from tensorflow.contrib.learn.python.learn import metric_spec\r\n---> 29 from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined\r\n     30 from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n     31 from tensorflow.contrib.learn.python.learn.estimators import head as head_lib\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py in <module>()\r\n     31 from tensorflow.contrib.layers.python.layers import optimizers\r\n     32 from tensorflow.contrib.learn.python.learn import metric_spec\r\n---> 33 from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n     34 from tensorflow.contrib.learn.python.learn.estimators import head as head_lib\r\n     35 from tensorflow.contrib.learn.python.learn.estimators import model_fn\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py in <module>()\r\n     49 from tensorflow.contrib.learn.python.learn.estimators import tensor_signature\r\n     50 from tensorflow.contrib.learn.python.learn.estimators._sklearn import NotFittedError\r\n---> 51 from tensorflow.contrib.learn.python.learn.learn_io import data_feeder\r\n     52 from tensorflow.contrib.learn.python.learn.utils import export\r\n     53 from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\__init__.py in <module>()\r\n     30 from tensorflow.contrib.learn.python.learn.learn_io.graph_io import read_keyed_batch_examples\r\n     31 from tensorflow.contrib.learn.python.learn.learn_io.graph_io import read_keyed_batch_features\r\n---> 32 from tensorflow.contrib.learn.python.learn.learn_io.numpy_io import numpy_input_fn\r\n     33 from tensorflow.contrib.learn.python.learn.learn_io.pandas_io import extract_pandas_data\r\n     34 from tensorflow.contrib.learn.python.learn.learn_io.pandas_io import extract_pandas_labels\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\numpy_io.py in <module>()\r\n     20 \r\n     21 import collections\r\n---> 22 from tensorflow.contrib.learn.python.learn.dataframe.queues import feeding_functions\r\n     23 \r\n     24 # Key name to pack the target into dict of `features`. See\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\dataframe\\queues\\feeding_functions.py in <module>()\r\n     36 # pylint: disable=g-import-not-at-top\r\n     37 try:\r\n---> 38   import pandas as pd\r\n     39   HAS_PANDAS = True\r\n     40 except ImportError:\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\__init__.py in <module>()\r\n     35 \r\n     36 # let init-time option registration happen\r\n---> 37 import pandas.core.config_init\r\n     38 \r\n     39 from pandas.core.api import *\r\n\r\nC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\config_init.py in <module>()\r\n     12 import warnings\r\n     13 \r\n---> 14 import pandas.core.config as cf\r\n     15 from pandas.core.config import (is_int, is_bool, is_text, is_instance_factory,\r\n     16                                 is_one_of_factory, get_default_val,\r\n\r\nAttributeError: module 'pandas' has no attribute 'core'\r\n```\r\n\r\nRegards, \r\nKathir\r\n\u200b", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "maybe try tf.layers.flatten instead of tf.contrib.layers.flatten?\r\n\r\nalso see this: https://stackoverflow.com/questions/36521691/importing-pandas-gives-error-attributeerror-module-pandas-has-no-attribute-c", "Thanks @kr-ish!\r\n\r\nThis looks like a problem resolved by the links, so I'm closing the issue."]}, {"number": 18041, "title": "The one_hot_labels in acgan is not pooled in tfgan.gan_loss function.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GTX 980M 4G\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI am trying to use tfgan.acgan_model together with tfgan.features.tensor_pool. There are three inputs to the acgan_model, i.e. real_data, generator_inputs, and one_hot_labels. While only two of the inputs (read_data, generator_inputs) are manipulated by tensor_pool in tfgan.gan_loss, which means that the one_hot_labels are incorrectly used in the acgan_discriminator_loss function.\r\n\r\nThe following is the entrance to the error function call. The one_hot_labels should be read from the tensor pool before this function call.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L561", "comments": ["@joel-shor can you please take a look?", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @joel-shor: It has been 19 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @joel-shor: It has been 34 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @joel-shor: It has been 49 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @joel-shor: It has been 64 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @joel-shor: It has been 79 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @joel-shor: It has been 79 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed by: https://github.com/tensorflow/tensorflow/commit/931a3054d2c13c3438fc58978b3463a0bd268aee"]}, {"number": 18040, "title": "[distributed tensorflow] End of sequence after a few batch with dataset.shard", "body": "When running distributed training, my session gets terminated after a few batches when using `dataset.shard`. The issue disappears when I run distributed training with an independent dataset object on each worker (in which case the same data gets shuffled and read on each worker).\r\n\r\n\r\nHave I written custom code: No.\r\nOS Platform and Distribution: ubuntu\r\nTensorFlow installed from: standard TF1.5 distribution\r\nBazel version: NA\r\nCUDA/cuDNN version: CUDA 9.1\r\nGPU model and memory: No GPU\r\nExact command to reproduce: Cf Below\r\n\r\n\r\n\r\nDataset construction:\r\n```\r\ndef construct_dataset(filenames, labels, batch_size, num_workers,worker_index):\r\n    dataset = tf.data.TextLineDataset(filenames)\r\n    dataset = tf.data.Dataset.from_tensor_slices((filenames,labels))\r\n    dataset = dataset.shard(num_workers, worker_index) #works fine when commenting this out\r\n    dataset = dataset.shuffle(buffer_size=10000)  # Equivalent to min_after_dequeue=10000.\r\n    dataset = dataset.map(_parse_function)\r\n    dataset = dataset.batch(batch_size)\r\n    return dataset\r\n```\r\n\r\nmain\r\n```\r\nwith tf.device(device):\r\n        filelist, labels = get_filelist(FLAGS.data_dir)\r\n        dataset = construct_dataset(...)\r\n        iterator = dataset.make_one_shot_iterator()\r\n        batch = iterator.get_next()\r\n        img_batch, filepath_batch, label_batch = batch\r\n        ...\r\n\r\n       hooks=[tf.train.StopAtStepHook(last_step=1000000)] \r\n\r\n    with tf.train.MonitoredTrainingSession(master=target,\r\n        is_chief=(FLAGS.task_index == 0),checkpoint_dir=logs,hooks = hooks) as sess:\r\n        try:\r\n            while not sess.should_stop():\r\n                sess.run(train_op)\r\n        except Exception as e:\r\n            print(e)\r\n```\r\n\r\nStacktrace - \r\n```End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,300,300,?], [?], [?]], output_types=[DT_UINT8, DT_STRING, DT_INT32], _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\t [[Node: Training_Loss_S17 = _HostRecv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:ps/replica:0/task:0/device:CPU:0\", send_device_incarnation=-379211586651304706, tensor_name=\"edge_133_Training_Loss\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Done.", "I apologize, but I don't quite understand the issue here. Could you provide the full stack trace?\r\n\r\nAlso, a side observation, the first line in `construct_dataset` seems unnecessary, since the very next line ignores it and creates a new `Dataset` using `from_tensor_slices`?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18039, "title": "Error building contrib/nccl on CentOS 6.5 on 1.7.0-rc1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 6.5\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.7.0-rc1 release\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.11.1- (non-git)\r\n- **GCC/Compiler version (if compiling from source)**: gcc (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)\r\n- **CUDA/cuDNN version**: CUDA: 7.5 cuDNN: 6.0\r\n- **GPU model and memory**: NVIDIA Tesla K20m\r\n- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package   --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\"`\r\n\r\n\r\n### Describe the problem\r\nTensorflow fails to build from source release 1.7.0-rc1 with error from nccl_ops.cc\r\nI took the following steps:\r\n\r\n```\r\nsource /opt/rh/devtoolset-2/enable\r\ncd /tmp\r\nwget https://github.com/tensorflow/tensorflow/archive/v1.7.0-rc1.tar.gz\r\ntar -xvf v1.7.0-rc1.tar.gz\r\ncd /tensorflow-1.7.0.-rc1\r\nexport TEST_TMPDIR=/tmp/tf-build\r\nexport JAVA_HOME=/usr/java/jdk1.8.0_66/\r\n./configure\r\n\r\nFound possible Python library paths:\r\n  /home/mack0242/.pyenv/versions/3.6.4/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/mack0242/.pyenv/versions/3.6.4/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n\r\nNo jemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 7.5\r\n\r\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 6.0\r\n\r\nPlease specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]3.5\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /opt/rh/devtoolset-2/root/usr/bin/gcc]: \r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\nConfiguration finished\r\n\r\nexport LD_LIBRARY_PATH=/opt/rh/devtoolset-2/root/usr/lib64:/opt/rh/devtoolset-2/root/usr/lib:/opt/rh/devtoolset-2/root/usr/lib64:/opt/rh/devtoolset-2/root/usr/lib:/usr/local/cuda/lib64\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package   --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\"\r\n```\r\n\r\n### Source code / logs\r\nBuild fails at this point:\r\n```\r\nINFO: From Compiling tensorflow/contrib/nccl/kernels/nccl_ops.cc:\r\ntensorflow/contrib/nccl/kernels/nccl_ops.cc(207): error: no instance of overloaded function \"tensorflow::TensorShapeUtils::MakeShape\" matches the argument list\r\n            argument types are: (Eigen::TensorMap<Eigen::Tensor<const tensorflow::int32, 1, 1, long>, 16, Eigen::MakePointer>, tensorflow::TensorShape *)\r\n\r\n1 error detected in the compilation of \"/tmp/tmpxft_00001aab_00000000-7_nccl_ops.cpp1.ii\".\r\nERROR: /tmp/tensorflow-1.7.0-rc1/tensorflow/contrib/nccl/BUILD:23:1: output 'tensorflow/contrib/nccl/_objs/python/ops/_nccl_ops_gpu/tensorflow/contrib/nccl/kernels/nccl_ops.pic.o' was not created\r\nERROR: /tmp/tensorflow-1.7.0-rc1/tensorflow/contrib/nccl/BUILD:23:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 362.889s, Critical Path: 67.25s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nSo it looks like a parameter type problem here: [nccl_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/nccl/kernels/nccl_ops.cc#L207)", "comments": ["Hello, have you solved your problem? I have the same problem.", "I have the same error on ubuntu 16.04 on r1.6. I tried both gcc4 and gcc5\r\n\r\n```\r\nINFO: From Compiling tensorflow/contrib/nccl/kernels/nccl_ops.cc:\r\ntensorflow/contrib/nccl/kernels/nccl_ops.cc(207): error: no instance of overloaded function \"tensorflow::TensorShapeUtils::MakeShape\" matches the argument list\r\n            argument types are: (Eigen::TensorMap<Eigen::Tensor<const tensorflow::int32, 1, 1, long>, 16, Eigen::MakePointer>, tensorflow::TensorShape *)\r\n\r\n1 error detected in the compilation of \"/tmp/tmpxft_00005ed7_00000000-10_nccl_ops.compute_52.cpp1.ii\".\r\nERROR: /home/ubuntu/tensorflow/tensorflow/contrib/nccl/BUILD:23:1: output 'tensorflow/contrib/nccl/_objs/python/ops/_nccl_ops_gpu/tensorflow/contrib/nccl/kernels/nccl_ops.pic.o' was not created\r\nERROR: /home/ubuntu/tensorflow/tensorflow/contrib/nccl/BUILD:23:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 293.387s, Critical Path: 52.67s\r\nFAILED: Build did NOT complete successfully\r\n```", "Confimed as well.\r\n\r\n1.6.0 is not buildable on Debian Jessie or Ubuntu Xenial, same error. Newer distribution work.", "Guys, no response ?", "This is the code crashing, and the commit that introduced it.\r\n\r\nI'm pretty sure it's trivial, it looks like being some syntax not understood by older GCC....\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/5882ae35d304f813313a4456c087237c29b63b64#diff-3b4c6adf932d8de73e6df8f960002f85L126\r\n\r\n(seems the link does not bring you to the proper line, check for OP_REQUIRES_OK_ASYNC)", "@JonnoFTW it might be worth taking CentOS 6.5 out of the bug title, as that's not a supported configuration (but Ubuntu 16.04 is).", "@cwhipkey at least the source line pointed at by @eLvErDe looks like it's yours. Would you PTAL?", "Some notes:   The call says it is passing an Eigen TensorMap.  The two-argument MakeShape function require gtl::ArraySlice<int32>, so I would guess the automatic conversion is not happening for some reason.\r\n\r\nThe automatic conversion that should happen I _think_ is this one that uses EnableIfConvertibleFrom:\r\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/gtl/array_slice.h#L148\r\n\r\nThis particular callsite could be fixed by avoiding the implicit conversion (call the three-argument MakeShape, for example).  Or we could try to see why the implicit conversion is failing on older gcc.", "Nothing new ?", "Is this still an issue with the latest release?", "Nagging Assignee @cwhipkey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18038, "title": "static graph vs dynamic graph", "body": "since dynamic graph has so many advantages, why not tf use dynamic graph from the bey beginning? Is static graph has any advantages over the dynamic?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I think static graph can be optimized, so it has the advantage of the memory consumption and enables fast calculation.\r\nyou can also find the discussion about this in the other framework MXNet.\r\n\r\nhttps://mxnet.incubator.apache.org/architecture/note_memory.html\r\n\r\nI hope this answer gives the solution about your question", "In order to keep GitHub issues focused on bugs and feature requests, I'm going to close this out.\r\n\r\nIn general though, there are many advantages of static graphs - like memory optimizations that can be applied, parallel execution of independent operations etc. Currently TensorFlow uses static by default (`tf.enable_eager_execution()` must be called explicitly or `tf.contrib.eager.py_func` must be used explicitly for dynamic graphs) and this will have to remain so for at least all 1.x versions of TensorFlow to keep with our semantic versioning guarantees.\r\n\r\nHope that helps. Thanks."]}, {"number": 18037, "title": "tf.sparse_tensor_dense_matmul makes small errors with tf.float32 matrices on GPU", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, simple short code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: both Ubuntu 14.04 / Centos 7\r\n- **TensorFlow installed from (source or binary)**: pip binary on Ubuntu, source  on Centos\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: release 0.8.1\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 6.0.21\r\n- **GPU model and memory**: GTX 750 / GTX 1080\r\n- **Exact command to reproduce**: tf.sparse_tensor_dense_matmul\r\n\r\n### Describe the problem\r\n1. Given a sparse tensor sp and a dense tensor mat, both of tf.float32,\r\n2. Compute thier product with tf.sparse_tensor_dense_matmul(sp, mat),\r\n3. The product varies slightly.\r\n\r\n### Source code / logs\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ns = tf.Session()\r\n\r\nnum = 10\r\ndim = 10\r\ntotal_out = 100\r\n\r\nindices = [\r\n    [1, 0],\r\n    [2, 0],\r\n    [3, 0],\r\n    [5, 0], [5, 1], [5, 2],\r\n    [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 7],\r\n    [7, 0], [7, 1], [7, 2], [7, 7], [7, 8],\r\n    [8, 0],\r\n    [9, 0], [9, 1], [9, 2], [9, 7]\r\n]\r\nvalues = np.array([1.0] * len(indices), np.float32)\r\nfeature = tf.SparseTensor(indices, values, [tf.cast(num, tf.int64), tf.cast(dim, tf.int64)])\r\n\r\ndense = tf.sparse_tensor_to_dense(feature, validate_indices=False)\r\nmat = tf.contrib.stateless.stateless_random_uniform([dim, total_out], seed=[1, 2], dtype=tf.float32)\r\nprod = tf.sparse_tensor_dense_matmul(feature, mat)\r\n# prod2 = tf.sparse_matmul(dense, mat, False, True, True, False, name='cross_sum')\r\n\r\nT = ['dense', 'mat', 'prod']\r\nresults = s.run([dense, mat, prod])\r\n\r\ncomp0 = []\r\ncomp1 = []\r\nfor i, r in enumerate(results):\r\n    try:\r\n        comp0.append(np.sum(np.load('npy_{}.npy'.format(T[i]))) - np.sum(r))\r\n        comp1.append(np.load('npy_{}.npy'.format(T[i])) - r)\r\n    except:\r\n        np.save('npy_{}.npy'.format(T[i]), r)\r\nfor i in range(len(comp0)):\r\n    print(T[i])\r\n    print(comp0[i])\r\n    print(comp1[i])\r\n    print('\\n')\r\n```\r\nRun the code several times, you will see that the product will vary slightly. like this:\r\n```\r\ndense\r\n0.0\r\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r\n\r\n\r\nmat\r\n0.0\r\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0.]\r\n...\r\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0.]]\r\n\r\n\r\nprod\r\n0.0\r\n[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   2.3841858e-07 -4.7683716e-07  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n  -4.7683716e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   2.3841858e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  4.7683716e-07  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3841858e-07\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  4.7683716e-07  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  2.3841858e-07  2.3841858e-07  0.0000000e+00\r\n   0.0000000e+00  2.3841858e-07  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  2.3841858e-07\r\n  -2.3841858e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n  -2.3841858e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00 -2.3841858e-07  0.0000000e+00\r\n  -2.3841858e-07  4.7683716e-07  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00 -2.3841858e-07  2.3841858e-07  0.0000000e+00\r\n   2.3841858e-07  0.0000000e+00  4.7683716e-07  2.3841858e-07\r\n   0.0000000e+00  4.7683716e-07  2.3841858e-07  4.7683716e-07\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  2.3841858e-07\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\r\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  2.3841858e-07\r\n   2.3841858e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  2.3841858e-07  0.0000000e+00\r\n   0.0000000e+00 -2.3841858e-07  2.3841858e-07  0.0000000e+00\r\n   0.0000000e+00 -2.3841858e-07  0.0000000e+00 -2.3841858e-07\r\n   0.0000000e+00  2.3841858e-07  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.3841858e-07\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  4.7683716e-07\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  2.3841858e-07  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00 -2.3841858e-07  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00 -4.7683716e-07  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  2.3841858e-07]\r\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\r\n   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\r\n...\r\n]\r\n```\r\nThis only happens on GPU with float32. It should be a bug I guess.\r\n", "comments": ["After further test, I found that float64 also has the same problem, if the dense shape fo the sparse matrix is large enough.", "Any updates regarding with this issue? ", "Assigning to @asimshankar, who might be able to find someone to take a look.", "@zheng-xq for triage", "Hi, @wenscarl and I have reproduced this nondeterminism for fp32. We were not able to repro for fp64, and there does not seem to be any code above showing how to do that. We're reasonably confident that the source of nondeterminism is the use of CUDA `atomicAdd` in `sparse_tensor_dense_matmul_op_gpu.cu.cc`. I just wanted to let it be known that this item is on our radar and we plan to resolve it at some point.\r\n\r\nAlso, this source of nondeterminism has been documented in [github/NVIDIA/framework-determinism](https://github.com/NVIDIA/framework-determinism).", "@Palazor \r\nWe see that you are using old version of tensorflow ( 1.x)  ,which is not actively supported, We recommend that you upgrade to 2.4 or later version.Attaching [migration](https://www.tensorflow.org/guide/migrate) guide for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 18036, "title": "Using this issue to test TF GitHub helper. Please ignore.", "body": "I'm working on an internal extension.", "comments": ["Done. Great!"]}, {"number": 18035, "title": "1.7 cherry-pick request: Update tensorboard dependency to 1.7.0+", "body": "Cherrypick to update tensorboard dependency version.", "comments": []}]