[{"number": 26287, "title": "Refactoring change: IsLoopFusion() etc.", "body": "Create IsLoopFusion(), IsInputFusion(), IsOutputFusion() and IsCustomFusion() methods.\r\n\r\nTested using //tensorflow/compiler/xla/service/gpu/tests:gpu_fusion_test and //tensorflow/python/compiler", "comments": ["@sana-damani thanks for your contribution , is it possible to combine this PR #26301 with this.", "> @sana-damani thanks for your contribution , is it possible to combine this PR #26301 with this.\r\n\r\nI'd like to keep independent changes separate unless you think there is some benefit in combining the two.", "> > @sana-damani thanks for your contribution , is it possible to combine this PR #26301 with this.\r\n> \r\n> I'd like to keep independent changes separate unless you think there is some benefit in combining the two.\r\n\r\n@sana-damani combining helps to fasten the process of review and merging internally, let me know", "> > > @sana-damani thanks for your contribution , is it possible to combine this PR #26301 with this.\r\n> > \r\n> > \r\n> > I'd like to keep independent changes separate unless you think there is some benefit in combining the two.\r\n> \r\n> @sana-damani combining helps to fasten the process of review and merging internally, let me know\r\n\r\n@rthadur  Hi, I've been working with Sana and recommended to keep PRs small and focused. I'm doing the internal code reviews and prefer this approach. Does it create overhead for you when triaging PRs, for instance? If so, is there anything we could do to avoid this? I'd like to make sure the process works smoothly for all of us.", "> > > > @sana-damani thanks for your contribution , is it possible to combine this PR #26301 with this.\r\n> > > \r\n> > > \r\n> > > I'd like to keep independent changes separate unless you think there is some benefit in combining the two.\r\n> > \r\n> > \r\n> > @sana-damani combining helps to fasten the process of review and merging internally, let me know\r\n> \r\n> @rthadur Hi, I've been working with Sana and recommended to keep PRs small and focused. I'm doing the internal code reviews and prefer this approach. Does it create overhead for you when triaging PRs, for instance? If so, is there anything we could do to avoid this? I'd like to make sure the process works smoothly for all of us.\r\n\r\n@thomasjoerg no issues , i was trying to combine related PR so that it would be easy for everyone. @sana-damani could you please rebase the branch ", "I had to resolve some conflicts and merge new changes.", "Nagging Reviewer : You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 26286, "title": "resolved issue #26280", "body": "Please check.", "comments": ["@petewarden please review.", "@rthadur can you please look into this? It has been over 10 days.", "@tanzhenyu please review.", "@perfinion updated. Please check.", "@arp95 Git needs there to be a blank line after the commit title before the body. I also like to have a blank line before the Closes: bit but that is up to you.", "@perfinion anything more that needs to be done?", "@arp95 can you please resolve conflicts ", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26285, "title": "Changed to shape from get_shape()", "body": "This is the recommended way", "comments": ["@amitsrivastava78 thank you for your contribution, can you please combine these changes #26279 ", "@rthadur , i have merged this PR with #26279, you can close this and use the other one."]}, {"number": 26284, "title": "Rename tf.nn.batch_normalization", "body": "**System information**\r\n- TensorFlow version: 1.13\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization\r\n\r\n\r\n**Describe the documentation issue**\r\ntf.nn.batch_normalization can actually be used to implement layer_normalization or group_normalization as well. (https://github.com/tensorflow/addons/pull/14)  Therefore the name of the function is quite confusing since the \"batch normalization\" is more or less a special case of this function (when you feed the right mean and variance tensor). \r\n\r\nI think something like \"normalization()\" could work as well and would be less confusing.\r\n", "comments": ["@Smokrow \r\nPlease note tf 1.x is not actively supported any more and many fixes are made to tf 2.x, please verify and let us know if this is still an issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 26283, "title": "Disabling tf.cond name scope", "body": "**System information**\r\n- TensorFlow version: 2.0\r\n- Are you willing to contribute it: No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWith all variables name being resolved via `name_scope` in TensorFlow 2.0, mixing tf.cond and variable creation can lead to unexpected or inconvenient results. Please consider this class that implements a conditional logic for inference but not training (here, reading a cached value):\r\n\r\n```python\r\nclass MyModel(tf.keras.layers.Layer):\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.linear = tf.keras.layers.Dense(10)\r\n\r\n    def call(self, x, cache=None, training=None):\r\n        if training or cache is None:\r\n            return self.linear(x)\r\n        else:\r\n            return tf.cond(\r\n                tf.equal(tf.shape(cache)[0], 0),\r\n                true_fn=lambda: self.linear(x),\r\n                false_fn=lambda: cache)\r\n```\r\n\r\nThis creates incompatible variable names in inference.\r\n\r\n**Training:**\r\n\r\n```python\r\nx = tf.zeros([3, 5])\r\nmodel = MyModel()\r\n_ = model(x, training=True)\r\nprint(model.trainable_variables)\r\n```\r\n\r\n```text\r\n[<tf.Variable 'my_model/dense/kernel:0' shape=(5, 10) dtype=float32>, <tf.Variable 'my_model/dense/bias:0' shape=(10,) dtype=float32>]\r\n```\r\n\r\n**Inference:**\r\n\r\n```python\r\nx = tf.zeros([3, 5])\r\ncache = tf.ones([0, 5])\r\nmodel = MyModel()\r\n_ = model(x, cache=cache, training=False)\r\nprint(model.trainable_variables)\r\n```\r\n\r\n```text\r\n[<tf.Variable 'my_model/cond/dense/kernel:0' shape=(5, 10) dtype=float32>, <tf.Variable 'my_model/cond/dense/bias:0' shape=(10,) dtype=float32>]\r\n```\r\n\r\nThe developer should then fiddle with the name or build the layer beforehand but I believe this code should just work. In V1, this could be fixed by setting the `tf.cond` name to:\r\n\r\n```python\r\ntf.get_default_graph().get_name_scope() + \"/\"\r\n```\r\n\r\nbut there is no alternative in V2.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThere should be a way (e.g. argument) to disable the name scope introduced by `tf.cond`. Making this the default behavior is maybe unreasonable to ask.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople applying conditional logic on layers during inference but not during training.\r\n\r\n**Any Other info.**\r\n\r\nThere are 2 real world examples in the project I maintain, [OpenNMT-tf](https://github.com/OpenNMT/OpenNMT-tf):\r\n\r\n* When dynamically decoding from a Transformer model, the decoder can cache the projection of the encoder output: https://github.com/OpenNMT/OpenNMT-tf/blob/v1.21.0/opennmt/layers/transformer.py#L466-L476. To mitigate the scope issue, I manually build the layer before the `tf.cond`.\r\n* When generating from a language model, we should either get the initial decoder state or the context state (if a context exists): https://github.com/OpenNMT/OpenNMT-tf/blob/v1.21.0/opennmt/models/language_model.py#L83-L87. In this case, I know that `self.name` is the top level name scope and use it directly. ", "comments": ["Why is having differently named variables a problem? With the tf v2 object-based checkpointing TF does not rely on variable names for anything (other than deprecated parameter server training).", "> With the tf v2 object-based checkpointing TF does not rely on variable names for anything (other than deprecated parameter server training).\r\n\r\nSorry I missed that.\r\n\r\nSo this is a non issue for V2 and I guess there is little motivation to change that for V1. Thanks, this was helpful."]}, {"number": 26282, "title": "Changing the default initializer globally for tf.keras.layers", "body": "**System information**\r\n- TensorFlow version: 2.0\r\n- Are you willing to contribute it: No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nFor V1 `tf.layers`, one could set a default initializer to the global variable scope:\r\n\r\n```python\r\nwith tf.variable_scope(name, initializer=...):\r\n  ...\r\n```\r\n\r\nIn V2 `tf.keras.layers`, the default initializer is hardcoded to `initializers.glorot_uniform()`. I currently see 2 imperfect ways to change the default initializer globally:\r\n\r\n* change the initializer to every layers that are created **but** this is not convenient for large models\r\n* implement a manual assignation loop **but** AFAIK there is no clean way to detect if the variable initializer is the default one or an initializer that was explicitly passed by the developer (typically a constant initialization).\r\n\r\n**Will this change the current api? How?**\r\n\r\nI think the most user friendly approach to this is to add an endpoint to globally change the default initializer: each layer created after this statement will use this initializer by default.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who wants to easily apply a global initialization strategy while still ensuring that specific initializer can be set locally.\r\n\r\n---\r\n\r\nAny recommendation to achieve this with the current code is of course appreciated.", "comments": ["I think this is the kind of thing we're running away from in tf v2. If you want to do it you can use your own layer-wrapping mechanism to do that.", "I think everyone wants to easily apply a global initialization strategy. Why did you close this issue? @alextp ", "Because we do not want to add more global-behavior-changing endpoints to TF's API.\r\n\r\nInstead I recommend you do things like wrapping keras layers into something that passes a global initializer in your code, and use the wrapped layers instead. With some metaprogramming this is not too much code, and doesn't push complexity into TF."]}, {"number": 26281, "title": "Changed dp to sp for textsize attributes in TensorflowLite Android Example", "body": "Resolved issue #26280 . Please check.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26281) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 26280, "title": "Bug in TensorflowLite Android Example", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 5T\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.1.0-rc0-61-g1ec6ed5 1.1.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): - \r\n- GCC/Compiler version (if compiling from source): - \r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: - \r\n\r\n\r\nCurrently, the tensorflow lite example uses dp for textsize attribute for android example. As per definition, this is a bug. The dp has to be changed to sp, as per Android standards.\r\n", "comments": ["@arp95 What is the status of this issue? Is this still open?", "We've since moved our examples to https://github.com/tensorflow/examples/tree/master/lite, feel free to propose a PR for the samples there.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26280\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26280\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 26279, "title": "Changed to math_ops.cast from math_ops.to_float", "body": "Switched to recommended method", "comments": ["@rthadur, i have merged both the PRs, please check.", "@amitsrivastava78  gentle ping to resolve conflicts.", "@rthadur , thanks for pointing this out, i have resolved the merge conflicts.\r\n\r\n@allenlavoie , can you pls check and approve again."]}, {"number": 26277, "title": "Extract IsMultiOutputFusible() into gpu_fusible", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26277) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26277) for more info**.\n\n<!-- ok -->", "Hi Sana, looks like this branch has conflicts that need to be resolved. Otherwise this looks good to me!", "@sana-damani , thank you for your contribution, can you please resolve conflicts"]}, {"number": 26276, "title": "Good first issue tag?", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: N/A\r\n- Doc Link: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nIs there a 'good first issue' tag similar to the one used in pandas-dev?\r\nhttps://github.com/pandas-dev/pandas#contributing-to-pandas-\r\nI see there is a contributions welcome tag \r\nhttps://github.com/tensorflow/tensorflow/labels/stat%3Acontributions%20welcome\r\n\r\nPerhaps I could add a sentence saying that:\r\nIf you want to contribute but you're not sure where to start, take a look at the issues with the \"contributions welcome\" label. These are issues that we believe are particularly well suited for outside contributions *and first time contributors*, often because we probably won't get to them right now. If you decide to start on an issue, leave a comment so that other people know that you're working on it. If you want to help out, but not alone, use the issue comment thread to coordinate.\r\n\r\nI'm sure that could be worded better.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@raybellwaves - We will be adding this over the weekend. :slightly_smiling_face: ", "I would like to solve this issue.Can I contribute?\r\n", "Added the tag `good first issue`, as well as `help wanted`, and will continue to populate! Thanks for the suggestion -- and please make sure to suggest in comments if you feel that an issue would be a good first project for a newcomer. :slightly_smiling_face: "]}, {"number": 26275, "title": "`tensorflow.python.client.device_lib.list_local_devices()`  crushes when none of GPU has enough free memory", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n\r\n- TensorFlow version (use command below):\r\n1.13.1\r\n\r\n- Python version:\r\n3.6.7\r\n\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nCUDA10.0/cuDNN _7.5.0.56\r\n\r\n- GPU model and memory:\r\n0: GTX 1060 TI 6GB\r\n1: GTX 1050 Ti 4GB\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'v1.13.1-0-g6612da8951' 1.13.1\r\n\r\n**Describe the current behavior**\r\nIf I run list_local_devices() in order to get the number of GPUs available while almost all of memory is already allocated on all the GPUs, the process crashes due to CUDA_ERROR_OUT_OF_MEMORY.\r\nThere's no problem at all if at least one GPU has substantial amount of free memory.\r\n\r\nError Message:\r\n\r\n> 2019-03-02 12:26:38.852155: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2019-03-02 12:26:38.989613: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 6370295808\r\n> 2019-03-02 12:26:38.999115: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:1: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 4236312576\r\n> 2019-03-02 12:26:38.999399: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\n> \r\n\r\n\r\n**Describe the expected behavior**\r\nIf none of GPUs is available, it could just return the empty list, not crushing itself.\r\nI wrote my code for training a model in the way it fully makes use of all the available resources, but I have another code for doing various stuff and I want it to run on CPU when GPU is not available.\r\nI expected get_available_gpus() to return the empty list when none of them is available.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nfrom tensorflow.python.client import device_lib\r\n\r\ndef get_available_gpus():\r\n    local_device_protos = device_lib.list_local_devices()\r\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\r\n\r\nget_available_gpus()\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nthe result of `nvidia-smi`\r\n\r\n```\r\nSat Mar  2 12:41:52 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 106...  On   | 00000000:01:00.0  On |                  N/A |\r\n| 40%   55C    P2    41W / 120W |   6033MiB /  6075MiB |     51%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 105...  On   | 00000000:03:00.0 Off |                  N/A |\r\n| 30%   41C    P0    N/A /  75W |   3997MiB /  4040MiB |     59%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1960      G   /usr/lib/xorg/Xorg                            27MiB |\r\n|    0      2557      G   /usr/bin/gnome-shell                          60MiB |\r\n|    0     13385      C   python                                      5933MiB |\r\n|    1     13385      C   python                                      3985MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\n\r\nHere is my current workaround\r\n```python\r\ndef get_available_gpus(workaround=True):\r\n    if workaround:\r\n        ret = os.spawnl(os.P_WAIT, python_path, python_path, 'metax/get_num_gpus.py')\r\n        if ret < 0:\r\n            # detecting aborted process\r\n            return []\r\n    local_device_protos = device_lib.list_local_devices()\r\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\r\n```\r\nFor now, I'm creating a child process to see if it crushes.", "comments": ["I'm not sure why this was assigned to me: please reassign to someone working on the GPU or Eager runtime components.", "@jaingaurav I'm able to reproduce this by running a program that allocates all GPU memory and then concurrently launching the snippet in the issue description.  Do you know what the correct fix is here?", "@yoshihikoueno We see that you are using old version of tensorflow (1.x)which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.Please open a new issue in case you face any errors, we will get you the right help .Hence moving this to closed status.Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26275\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26275\">No</a>\n"]}, {"number": 26274, "title": "Custom model's `build()` method is not called automatically", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION: '2.0.0-dev20190301'\r\ntf.version.GIT_VERSION: 'v1.12.0-9345-g4eeb2714f4'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen creating a custom model with a `build()` method (e.g., if one of the model's layers has a size that depends on the input shape, such as a reconstruction layer), the model cannot be trained unless I explicitly call `build()` with a `tf.TensorShape()`. Moreover, I cannot specify an `input_shape`.\r\n\r\n**Describe the expected behavior**\r\nI expect the custom model to be built automatically the first time it is called (e.g., by the `fit()` method).\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nX_train = np.random.randn(1000, 8)\r\ny_train = np.random.rand(1000, 1)\r\n\r\nclass ReconstructingRegressor(keras.models.Model):\r\n    def __init__(self, output_dim, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.hidden = keras.layers.Dense(30, activation=\"elu\")\r\n        self.out = keras.layers.Dense(output_dim)\r\n\r\n    def build(self, batch_input_shape):\r\n        n_inputs = batch_input_shape[-1]\r\n        self.reconstruct = keras.layers.Dense(n_inputs)\r\n        super().build(batch_input_shape)\r\n\r\n    def call(self, inputs):\r\n        Z = self.hidden(inputs)\r\n        reconstruction = self.reconstruct(Z)\r\n        reconstruction_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\r\n        self.add_loss(0.1 * reconstruction_loss)\r\n        return self.out(Z)\r\n\r\nmodel = ReconstructingRegressor(1)\r\n#model.build(tf.TensorShape([None, 8])) # <= works if I add this line\r\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\r\nhistory = model.fit(X_train, y_train, epochs=2) # <= AttributeError (see below)\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stacktrace:\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-3cb185747474> in <module>\r\n     27 #model.build(tf.TensorShape([None, 8])) # <= works if I add this line\r\n     28 model.compile(loss=\"mse\", optimizer=\"nadam\")\r\n---> 29 history = model.fit(X_train, y_train, epochs=2) # <= ERROR!\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    804         steps=steps_per_epoch,\r\n    805         validation_split=validation_split,\r\n--> 806         shuffle=shuffle)\r\n    807\r\n    808     # Prepare validation data.\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2501       else:\r\n   2502         cast_inputs = x_input\r\n-> 2503       self._set_inputs(cast_inputs)\r\n   2504     else:\r\n   2505       y_input = y\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    454     self._setattr_tracking = False  # pylint: disable=protected-access\r\n    455     try:\r\n--> 456       result = method(self, *args, **kwargs)\r\n    457     finally:\r\n    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _set_inputs(self, inputs, outputs, training)\r\n   2773             outputs = self.call(inputs, training=training)\r\n   2774           else:\r\n-> 2775             outputs = self.call(inputs)\r\n   2776           # Reset to the previously saved value. If `call()` had `add_metric`\r\n   2777           # or `add_loss`, then `_contains_symbolic_tensors` will have been set\r\n\r\n<ipython-input-1-3cb185747474> in call(self, inputs)\r\n     19     def call(self, inputs):\r\n     20         Z = self.hidden(inputs)\r\n---> 21         reconstruction = self.reconstruct(Z)\r\n     22         reconstruction_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\r\n     23         self.add_loss(0.1 * reconstruction_loss)\r\n\r\nAttributeError: 'ReconstructingRegressor' object has no attribute 'reconstruct'\r\n```\r\n\r\n", "comments": ["Thanks for the bug report! I was able to reproduce this issue, looking into it now", "This should be fixed in the latest nightly build", "Has it been fixed? I still get an error in a specific case when using add_loss on a model.\r\nFor example, as illustrated in this [question](https://stackoverflow.com/questions/60106829/cannot-build-custom-keras-model-with-custom-loss/60986815#60986815)."]}, {"number": 26273, "title": "window import bug", "body": "python3.7\r\ntensorflow 1.13.1\r\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\r\nImportError: numpy.core.multiarray failed to import\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<frozen importlib._bootstrap>\", line 980, in _find_and_load\r\nSystemError: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set\r\nImportError: numpy.core._multiarray_umath failed to import\r\nImportError: numpy.core.umath failed to import\r\n2019-03-02 10:16:10.848675: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr\r\n", "comments": ["I do think it is related to the bug I have reported before but tensorflow ignored #25636 \r\n\r\nTensorflow windows build seems to need numpy>1.16", "@wm901115nwpu In some cases, I think there are compatibility issues between modules when you install TF using conda. Could you uninstall python and tensorflow and reinstall following the instructions [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). Please let me know how it progresses. For Faster resolutions, Please provide the details requested at issue template [here](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Thanks!", "I install a cpu version.", "I completely result in from numpy version, when I upgrade numpy version 1.16, it works.", "@wm901115nwpu Please check required packages [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py). Please close if your issue was resolved. In future, double check version of required packages before you install TF through Anaconda. Thanks!\r\n\r\nREQUIRED_PACKAGES = [\r\n    'absl-py >= 0.7.0',\r\n    'astor >= 0.6.0',\r\n    'gast >= 0.2.0',\r\n    'google_pasta >= 0.1.2',\r\n    'keras_applications >= 1.0.6',\r\n    'keras_preprocessing >= 1.0.5',\r\n    'numpy >= 1.14.5, < 2.0',\r\n    'six >= 1.10.0',\r\n    'protobuf >= 3.6.1',\r\n    'tensorboard >= 1.13.0, < 1.14.0',\r\n    'tensorflow_estimator >= 1.13.0rc0, < 1.14.0rc0',\r\n    'termcolor >= 1.1.0',\r\n]\r\n", "@wagonhelm I agree with @henrysky . This is related to #25636. So I will close this issue and keep the issue #25636 open until it is resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26273\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26273\">No</a>\n", "I also get this error when trying to use tensorflowjs_converter"]}, {"number": 26272, "title": "Update the version of TF to 2.0.0-alpha0", "body": "", "comments": ["Lgtm. @nfelt does the regular TB dependency also look good?"]}, {"number": 26271, "title": "[INTEL MKL WIP] Added new ops and kernels for fused quantized matmul (do not merge!)", "body": "**!!!Do not merge this PR!!!**\r\n**This PR is WIP** that introduces three new quantized operators: fusion of quantized matmul with (i) bias add, (ii) bias add + relu, and (iii) bias add + relu + requantize.\r\n\r\nWe plan to split these large changes into smaller pieces. ", "comments": []}, {"number": 26270, "title": "[TF 2.0 API Docs] tf.custom_gradient", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.13.1\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/custom_gradient\r\n\r\nI've been having some difficulty using `custom_gradient` recently. Before implementing the op I actually have in mind, I'm trying to make a [simple polynomial op](https://gist.github.com/tsbertalan/b6c02bf6e39116d8446faa0159a011af) with `custom_gradient` in an effort to get my head around the requirements.. The documentation is not at all clear on how summing over batch and output indices should be done, and (until recently) wasn't clear on whether we should return the full *Jacobian* of the operation, or just the vector-Jacobian-product (\"VJP\") function. (I did notice some explanation of the latter issue made it in in the 1.12 -> 1.13 update, which was helpful).\r\n\r\nI think this problem applies both the derivative WRT the op's input, and WRT to the parameters (called `grad_xs` and `grad_vars` in the documentation). However, when I incorporated the VJP tidbit in my sample code, I got correct results for the former, but not the latter, I believe because the implicit sum over batch was already present in the supplied `grad_ys`, and because the VJP also collapses out the index across `grad_ys`.\r\n\r\nSo, the lack of clarity about sums (accidentally) only hit me when trying to write the `grad_vars` part. In my [gist linked above](https://gist.github.com/tsbertalan/b6c02bf6e39116d8446faa0159a011af), I had to use a `reduce_sum` when computing $dy/dp$, which, for a polynomial, is the corresponding powers of $x$. (Since $dy/dx$ doesn't depend on $x$, this issue didn't appear when writing the `grad_xs` part.)\r\n\r\nIn addition to simply saying what you mean in this documentation (the sum of the gradient over examples is not \"the gradient\"--only the per-example gradient truly deserves that name! But I suppose that particular fight is a lost cause.), it would be good if there were at least some examples that exercised the `grad_vars` part and the `variables=None` keyword argument.", "comments": ["@tsbertalan - Thank you so much for mentioning this, and for creating a gist as an example. I agree that the API documentation for `tf.custom_gradient` could be improved, and this is a great start. \ud83d\ude42 \r\n\r\n**A few things:**\r\n\r\n- Would you like to make a PR to **update the documentation** for `tf.custom_gradient`? If yes, I'd be happy to point you to where to get started!\r\n\r\n- Would you be interested in **converting your [gist example](https://gist.github.com/tsbertalan/b6c02bf6e39116d8446faa0159a011af) to TensorFlow 2.0 style** (ping @alextp), and including it as an example in [`tensorflow/examples`](http://www.github.com/tensorflow/examples)?\r\n\r\n- I noticed that you're a post-doc at MIT; congratulations! Would you be interested in working on similar efforts as a [Google Summer of Code project](https://summerofcode.withgoogle.com/organizations/6137730124218368/)? TensorFlow has recently been added as a mentoring organization, and **updating the API docs with detailed technical examples** would be a great project.", "Can i also work on updation of docs for tf.custom_gradient !?", "Thanks for the comments! I'd love to have improved documentation for tf.custom_gradient, and would love to review such a pull request.\r\n\r\nOne minor nit though is that when you say \"the sum of the gradient over examples is not \"the gradient\"--only the per-example gradient truly deserves that name\" I do not quite agree; TensorFlow only differentiates scalar loss functions, so \"the gradient\" is shorthand for \"the gradient of the sum / average of per-example losses\" and then because the gradient of a sum is the sum of the gradients then I think the text there is mostly correct.\r\n\r\nThat said I agree with you that correct is not enough and that we need to clarify these comments so others don't make the same mistake.", "My original post is below, this is only because it might help others trying to get custom_gradient to work with keras in more complicated situations...\r\n\r\nOK, might be a documentation issue, but probably I was not reading enough documentation at all. If you use custom_gradient you are only allowed to use limited python statements, mainly tensorflow operations:\r\nthis gradient seems to work at least, maybe it helps others:\r\n```\r\n\r\n@tf.custom_gradient\r\ndef select_subnet_layer(x):\r\n    # the last in x are the select, before is divided into parts\r\n    x_select = x[:,:,-args.lstm_num:]\r\n    x_data = x[:,:,:-args.lstm_num]\r\n    print(\"x_select\",x_select.shape)\r\n    size_of_out = x_data.shape[2] // args.lstm_num\r\n    out = x_data[:,:,:size_of_out]\r\n    out = 0 * out\r\n    for i in range(args.lstm_num):\r\n        out += x_data[:,:,(i * size_of_out):((i+1)*size_of_out)]* x_select[:,:,i:i+1]\r\n    print(\"out\",out.shape)\r\n    print(\"x\",x.shape)\r\n    def custom_grad(dy):\r\n        size_of_out = (x.shape[2]-args.lstm_num) // args.lstm_num\r\n        gg = []\r\n        gs = []\r\n        for i in range(args.lstm_num):\r\n            gg.append(  x[:,:,size_of_out * args.lstm_num +i:size_of_out * args.lstm_num + i + 1] * dy)\r\n            tmp =  x[:,:,size_of_out * i:size_of_out * (i+1)] * dy\r\n            gs.append(keras.backend.sum(tmp, axis = 2, keepdims = True))\r\n        grad = keras.backend.concatenate(gg + gs)\r\n        print(gg,gs,grad)\r\n        return grad # keras.backend.clip(grad,-1,1)\r\n    return out, custom_grad\r\n\r\n```\r\n\r\n\r\nI am not sure, if I have a documentation related issue, or maybe it is a bug.\r\n\r\nI created a custom layer and I need to add a custom gradient:\r\n\r\nThe problem is: My layer is included in a LSTM network and therefore has changing input and output shapes. Initially it has e.g.  (?,?,153) e.g. which is (1, variable size, 153) during calculation.\r\n\r\nmy keras layer is defined as:\r\n\r\n```\r\n@tf.custom_gradient\r\ndef select_subnet_layer(x):\r\n    global ssss\r\n    # the last in x are the select, before is divided into parts\r\n    x_select = x[:,:,-args.lstm_num:]\r\n    x_data = x[:,:,:-args.lstm_num]\r\n    print(\"x_select\",x_select.shape)\r\n    size_of_out = x_data.shape[2] // args.lstm_num\r\n    out = x_data[:,:,:size_of_out]\r\n    out = 0 * out\r\n    for i in range(args.lstm_num):\r\n        out += x_data[:,:,(i * size_of_out):((i+1)*size_of_out)]* x_select[:,:,i:i+1]\r\n    print(\"out\",out.shape)\r\n    print(\"x\",x.shape)\r\n    def custom_grad(dy):\r\n        print('debugging',dy)\r\n        s1 = dy.shape.as_list()[0]\r\n        s2 = dy.shape.as_list()[1]\r\n        print(dy,[dy])\r\n        if s1 is None:\r\n            return tf.fill((1, 324, size_of_out*args.lstm_num + args.lstm_num), 1.0)\r\n        grad_nump = np.ones([s1,s2,153], dtype='float32')\r\n        if x.shape.as_list()[0] is not None:\r\n            for i in range(args.lstm_num):\r\n                print('???',args.lstm_num)\r\n                grad_nump[:,:, size_of_out*i : size_of_out*(i+1)] = x[:,:,size_of_out * args.lstm_num + i]\r\n                grad_nump[:,:,size_of_out * args + i] = np.sum(x[:,:,size_of_out*i : size_of_out*(i+1)])\r\n        grad = tf.convert_to_tensor(grad_nump)\r\n        return grad\r\n    return out, custom_grad\r\n\r\nclass CustomLayer(Layer):\r\n\r\n    def __init__(self, **kwargs):\r\n\r\n        super(CustomLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, x):\r\n        return select_subnet_layer(x[:,:])  # you don't need to explicitly define the custom gradient\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        print(input_shape[2])\r\n        return (input_shape[0], input_shape[1], (int(input_shape[2]) - args.lstm_num ) // args.lstm_num)\r\n\r\n```\r\nif s1 is None:     is not what I want. I tried to return a tf tensor placeholder with correct dimensions, but this dimension is not changed to a correct one during evaluation of the net too.\r\n\r\nLater it does not change the shape?! Therefore I get the Error\r\n\r\nInvalidArgumentError: shape of dy was [1,324,153] instead of [1,262,153]\r\n\t [[{{node training_1/RMSprop/gradients/custom_layer_3/strided_slice_grad/StridedSliceGrad}}]]\r\n\t [[loss_2/mul/_183]]\r\n\r\nwhen I do have a changed dimension in one training example.\r\n\r\nI have the full source of the file on github:\r\nhttps://github.com/dsmic/LearnMultiplyByHand/blob/5e550ae6435c623c325889d8d5a28d65a12a092a/learnmultiply_schriftlich_limit_traindata_subnets.py\r\n\r\nI would love to get any suggestion what the problem might be, and of cause would help in case it is a bug to fix. \r\n", "Is this still open? If so, I would like to take on this. ", "For me it is closed, I learned to use only limited python statements, but did not find any detailed documentation, what is allowed....", "@tsbertalan @dynamicwebpaige Has the issue been resolved?\r\nIf not I would like to improve it", "@bharatnishant I think, actually, the documentation that needs this clarification first is that for the regular `tf.gradients` function. It does claim, from the very start, that it's for computing \"symbolic derivatives of sum of `ys` w.r.t. `x` in `xs`.\" However, there appear to be cases in which this is not enforced, with confusing interactions between the presense of a batch dimension in either `ys` or `xs`.  \r\n\r\nI'd prefer more predictable behavior, with an exception raised rather than an automatic sum in the cases where a Jacobian-like computation is not possible, but, failing that, it seems reasonable at least to have documentation of when to expect that such an automatic sum will be added silently. As with convolution, we should be able to predict a-priori the shape of the gradient tensor(s) for all the possible input shapes for `x`(s) and `ys`, including batch dimensions in either.\r\n\r\n@alextp I get your point about how `tf.gradients` is only really intended for scalar functions. There would be no need for this to even be a shorthand in my ideal world where exceptions were raised when `ys` was not a scalar. However, in reality, there appear to be circumstances where the resulting tensor will provide unaggregated gradients $dL_i/d\\theta_j$, where $i$ is the batch index, and $j$ is the index across this particular parameter vector.\r\n\r\nI think that a distinction should be made (and, in some fashion, is made) between a batch index and a true index, where computation for each batch index value is conceptually independent (except for some final aggregation), whereas true indices interact in everyday linear algebraic computations. I think the point about `tf.gradients` being for scalars applies more to the second type of index than the first--a reasonable terminology would be to speak of unaggregated gradients (which are d/dtheta of a scalar, evaluated per-entry in some dataset), and then change to the term \"(un)aggregated Jacobians\" when discussing derivatives of vectors per-datum. Also, note here, that I think it makes sense to describe tensors with shapes of both `()` and `(None,)` as \"scalar\".\r\n\r\nFor that matter, it would be nice to know what the real performance tradeoffs are for computing a Jacobian. [Another issue \r\n](https://github.com/tensorflow/tensorflow/issues/675) discusses some methods for doing that, but do these scale, say, linearly in the output dimension `M`? That is, if you formulate this is the stacking of the gradients of `M` several scalar-valued functions, does it in fact just take `M` times longer to compute that? How does this change if you use the experimental `tf.python.ops.parallel_for.gradients.jacobian`? I believe it does raise warnings about space usage, and, in some circumstances, the \"Jacobian\" returned can have *two* batch dimensions `i` and `j`, and an enormous number of zero entries for all the cases where `i!=j`.\r\n\r\nAs for clarification of the `tf.custom_gradients` documentation, I understand that, in deriving backpropagation as the chain rule, several Jacobian-vector products emerge that can can be computed more efficiently as such rather than as explicit full Jacobian evaluations followed by matrix-vector products. However, it is not totally clear to me how this squares with the terminology of \"initial value gradients\", \"received gradients\", or sometimes \"upstream gradients\" that is used to refer to the argument of the wrapped function. A more mathematical treatment would be very welcome, showing the complete chain rule for a simple expression with proper equivalences drawn between the mathematical expressions and the code objects/arguments.\r\n\r\nI also see now that this is tagged as API v 2, though I'm still using the 1.x versions. I think that `tf.gradients` and `tf.custom_gradient` at least have similar documentation between the two versions, but in v2, there might be additional complications surrounding the \"gradient tape\" paradigm.\r\n\r\nSorry for the long-winded response. TensorFlow has been in a confusing state of flux over the past few versions, and so I (obviously) have many points of confusion on this.", "I think this issue is still open. If it is, I would like to work on it and improve the documentation.", "@tsbertalan @dynamicwebpaige @alextp Since the issue is still open, can I work on it and do the required changes in documentation?", "Sure, please send a PR!\n\nOn Fri, Jan 3, 2020 at 4:05 AM Saumil-Agarwal <notifications@github.com>\nwrote:\n\n> @tsbertalan <https://github.com/tsbertalan> @dynamicwebpaige\n> <https://github.com/dynamicwebpaige> @alextp <https://github.com/alextp>\n> Since the issue is still open, can I work on it and do the required changes\n> in documentation?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26270?email_source=notifications&email_token=AAABHRO2AEAY7EZOOOUAH7DQ34SZVA5CNFSM4G3ICUN2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEIA7TRY#issuecomment-570554823>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHROYGTKNGE2RREF4AGDQ34SZVANCNFSM4G3ICUNQ>\n> .\n>\n\n\n-- \n - Alex\n", "bump", "is this issues still occur?", "Hello, it's been quite some time since the issue has been open and looks like there aren't any active linked PR. If this issue is still open I will begin to work on it.\r\nI'm not too aware of the functionality of the custom_gradient function. However, judging by the comments made by @tsbertalan and @alextp the following changes have to be made to the documentation:\r\n- The document should clarify that under certain \"cases an automatic sum will be added silently\".\r\n- Clearing up the differences between \"a batch index and a true index\"\r\n\r\nFeel free to correct me if I'm wrong as my knowledge of gradients is fairly limited. Since it is also my first time working on an open-source issue I would greatly appreciate it if anyone could point me towards the guidelines for PR and making documentation changes.", "@Harsh188 I am open for this or any other type of work that needs improvement. I am just begining out my journey as an open-source cotributor and hence I would appreciate any pointers or useful tips in order to get started as a contributor.\r\n\r\nPlease connect with me if you find a suitable time for the same.", "Same here, I am also a beginner and would like to contribute. Is there any places where I can help?\r\n", "Hey there,\r\nI am new to open source environment,and i would like to contribute.Can anybody explain me 'How to get started?'", "I'm not sure if this is what you're looking for, but the first thing is to clone the repository (`git clone [url of repo]`). Then you make a new branch in which you will develop a new feature or fix a bug. Finally, when the work is done you open a pull request here on GitHub to have the new code reviewed and eventually merged.", "hey is this issue still open want to work on it and I am new to tensorflow community.\r\n @dynamicwebpaige  @ziofil ", "Hey, I am new to the open-source community and I would like to contribute, I have gone through the CONTRIBUTION.md file and gone through the good first issue and every possible thing I have gone, but still not getting where to start, can anyone help me out, where to start with?\r\n@dynamicwebpaige @alextp ", "Hello!\r\nI am newbie to open source contribution and really interested to contribute!! . But I have very very less knowledge about it . Can anyone help me please?\r\n", "I have no clue in programming and i am looking for someone who can write script and programme", "Is there anyone who can help me to create Program rock paper scissors game play with NAO robot? ", "Hey there! \r\n@dynamicwebpaige @tsbertalan I have experience in TensorFlow, but I'm new to open source. I see you would like to have some examples, could you please explain more?\ud83d\ude05", "Yeah, I had a project about NAO robot to play rock paper scissors games.\nAnd i did that in choregraphe. If you think that i can help you then leet\nme know.\n\nOn Fri, 11 Jun 2021 at 00:37, copybara-service[bot] <\n***@***.***> wrote:\n\n> Closed #26270 <https://github.com/tensorflow/tensorflow/issues/26270> via\n> #49488 <https://github.com/tensorflow/tensorflow/pull/49488>.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26270#event-4872614385>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ATZUPVX67BUQN6CQIG3QAW3TSDS5HANCNFSM4G3ICUNQ>\n> .\n>\n"]}, {"number": 26269, "title": "Data augmentation for N-D tensor #25295", "body": "", "comments": [" It seems that I triggered the ApiCompatibilityTest error by adding a new function random_flip(), which can be useful for 5D medical data @pldelisle.    @drpngx, do you know how I can fix the Compatibility error?", "Yes, you need to run `tensorflow/tools/api/tests/api_compatibility_test` under python2 with `--update_goldens`.", "> Yes, you need to run `tensorflow/tools/api/tests/api_compatibility_test` under python2 with `--update_goldens`.\r\n\r\n@drpngx, thank you for your feedback. I run the command as you suggested and I got the following  \r\n\r\n> W0303 10:56:02.430611 140736380490688 api_compatibility_test.py:254] Golden file update requested!\r\n> All test failures have been skipped, see the logs for detected diffs.\r\n> This test is now going to write new golden files.\r\n> Make sure to package the updates together with your change.\r\n> \r\n> You will need an explicit API approval. This may take longer than a normal\r\n> review.\r\n> \r\n\r\nWhat should I do next?  I don't quite understand the message \"package the updates together with your change\".", "> Can you run tensorflow/tools/api/tests:api_compatibility_test and regenerate the golden files?\r\n\r\n@alextp, @drpngx, I updated the golden files.", "Hi @alextp, @drpngx, I fixed the pylint non-whitelist errors.  could you rerun the internal checks? Thanks.", "Hi @alextp, @drpngx, how is the code review going?", "@alextp, how is this PR going? ", "It's awaiting API review\n\nOn Fri, Mar 29, 2019 at 7:27 AM musikisomorphie <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp>, how is this PR going?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26269#issuecomment-478015889>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWWAtMuRviL4erh4Yamm3MzepT1Tks5vbiK1gaJpZM4bZ_NB>\n> .\n>\n\n\n-- \n - Alex\n", "@drpngx, @alextp, hi how is this review going? ", "Hi @alextp, I am afraid I cannot maintain this PR anymore because i need to write my thesis in the next few months. Hopefully someone can take it over which my code could be a starting point.", "I'm going to go ahead and close this PR, because it seems to have stalled. please feel free to reopen!"]}, {"number": 26268, "title": "Even in eager mode, Keras is passing custom models non-eager tensors in 'fit'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 10.14\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): '1.14.1-dev20190301'\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nWhen calling `keras.model.fit` on a custom model, it seems the model is passed a graph-mode tensor instead of an eager tenser, even when in eager mode.\r\n\r\n**Describe the expected behavior**\r\nIf in eager mode, the tensors passed to the `call` method of a custom model should be eager tensors. Otherwise, the advantages of eager mode, like the ability to use native control flow, are lost.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_v2_behavior()\r\nfrom tensorflow import keras\r\n\r\nclass MyModel(keras.Model):\r\n    def call(self, x):\r\n        if x > 0:\r\n            return x + 1\r\n        else:\r\n            return x - 1\r\n            \r\nm = MyModel()\r\nm(tf.constant(0))  # This works, returns -1 as expected\r\nm.compile(loss='mse', optimizer='sgd')\r\nm.fit(tf.constant(0), tf.constant(1)) # This fails\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-55-2c7beacc3d4f> in <module>\r\n----> 1 m.fit(tf.constant(0), tf.constant(1))\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    804         steps=steps_per_epoch,\r\n    805         validation_split=validation_split,\r\n--> 806         shuffle=shuffle)\r\n    807 \r\n    808     # Prepare validation data.\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2501       else:\r\n   2502         cast_inputs = x_input\r\n-> 2503       self._set_inputs(cast_inputs)\r\n   2504     else:\r\n   2505       y_input = y\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    454     self._setattr_tracking = False  # pylint: disable=protected-access\r\n    455     try:\r\n--> 456       result = method(self, *args, **kwargs)\r\n    457     finally:\r\n    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _set_inputs(self, inputs, outputs, training)\r\n   2773             outputs = self.call(inputs, training=training)\r\n   2774           else:\r\n-> 2775             outputs = self.call(inputs)\r\n   2776           # Reset to the previously saved value. If `call()` had `add_metric`\r\n   2777           # or `add_loss`, then `_contains_symbolic_tensors` will have been set\r\n\r\n<ipython-input-52-94b7a4f52815> in call(self, x)\r\n      1 class MyModel(keras.Model):\r\n      2     def call(self, x):\r\n----> 3         if x > 0:\r\n      4             return x+1\r\n      5         else:\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in __bool__(self)\r\n    658       `TypeError`.\r\n    659     \"\"\"\r\n--> 660     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n    661                     \"Use `if t is not None:` instead of `if t:` to test if a \"\r\n    662                     \"tensor is defined, and use TensorFlow ops such as \"\r\n\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n```", "comments": ["Note that even adding `run_eagerly=True` to the `compile` call doesn't resolve this.", "I believe you need to pass `dynamic=True` to the model constructor:\r\n```python\r\nm = MyModel(dynamic=True)\r\n```\r\nOtherwise Keras assumes that your model is able to take symbolic tensors (and uses this for shape inference) even if you've asked it to compile in eager mode. This was added in https://github.com/tensorflow/tensorflow/commit/d0373dcae4810422f7575cb56a91d6695d2c4ec6, which has a little more context.", "There is a bug here, though, in that Keras should probably tell you why this is failing.", "Thanks, that does the trick! \r\n\r\nThere also might be a bit a documentation issue here - I don't recall this being prominently highlighted in the new tutorials and overviews of eager mode/keras.", "Thank you @jekbradbury for the quick response on this! We will definitely need to throw an error explaining the issue here. I will look into that.\r\n\r\n@malmaud I agree that this is not documented well. This concept is very new and we are working to see if we can remove the need for this hence the lack of documentation.", "Can we get some more explanation for how to add dynamic=True and what that does?", "I found a solution that works for me and posted it in this [StackOverflow answer](https://stackoverflow.com/a/60120935/10827646).\r\n\r\n**tl;dr** set the `run_eagerly` attribute manually **after** compiling your model:\r\n```python\r\nmodel.compile()\r\nmodel.run_eagerly = True\r\n```", "@brett-daley It works! Thank you.", "I am having the exact same problem with keras version 2.3.0-tf and tensorflow version 2.2.0. Adding dynamic=True gives me this error for a functional model:\r\n \r\n`TypeError: ('Functional models may only specify 'name' and 'trainable' keyword arguments during initialization. Got an unexpected argument:', 'dynamic')`", "I was able to fix my issue by subclassing tf.Keras.Model. Also, there was no need to pass `dynamic=True` to the model constructor; `run_eagerly=True` when compiling works. ", "@surGeonGG Can you be more specific please? I assume you did more than just simply subclassing because propagating the init via a super-call shouldn't make any difference.", "I am having the same issue when running this on 2.3.0. Doing this also doesnt resolve the issue\r\n\r\n``` python\r\nmodel.compile()\r\nmodel.run_eagerly = True\r\n```", "Same with PrattJena, running on 2.3.0 and have same issue even with run_eagerly = True", "I also have this same issue. Any pointers towards a solution?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Hi Alfred,\n\nThis issue has been resolved. Thanks so much for checking in though.\n\n\n\nOn Fri, Feb 19, 2021 at 7:10 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Hi There,\n>\n> We are checking to see if you still need help on this, as you are using an\n> older version of tensorflow which is officially considered end of life . We\n> recommend that you upgrade to the latest 2.x version and let us know if the\n> issue still persists in newer versions. Please open a new issue for any\n> help you need against 2.x, and we will get you the right help.\n>\n> This issue will be closed automatically 7 days from now. If you still need\n> help with this issue, please provide us with more information.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26268#issuecomment-782282428>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKEAU4WCOELDST4GRUNK6FDS72ZTZANCNFSM4G3H55ZQ>\n> .\n>\n", "Closing this issue since it is resolved. Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26268\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26268\">No</a>\n", "This isn't solved.  I was able to reproduce the error even after passing the suggested line after the compile() function is called."]}, {"number": 26267, "title": "Use localtime instead of gmtime for message logged", "body": "C++ logging and python logging both use localtime(), however this\r\none method uses gmtime() which causes the log of a run to show\r\ntimestamps that can be hours apart. Let's be consistent as use\r\nlocaltime() everywhere.", "comments": ["Looks reasonable to me. But ask Mustafa to review in case there is a requirement for the timezone in logging here."]}, {"number": 26266, "title": "AttributeError: module 'pandas' has no attribute 'compat'", "body": "Hi,\r\n\r\nI am trying to use ScipyOptimizerInterface() in the tensorflow, but it gave the following attribute error\r\n\r\nAttributeError: module 'pandas' has no attribute 'compat'.\r\n\r\nBy going through the discussion threads at Tensorflow github page, I have upgarded \"dask\", downgraded pandas, reinstalled tensorflow and scipy packages. Unfortunately, it is still giving me same AttributeError. Does anyone having similar issues and can help me to resolve it? I can use tensorflow normally for other minimizations algorithms (tested ADAMS) but for scipy's BFGS implementation, I am getting this attribute error.\r\n\r\nI am running code on Linux Centos system with python 3.6 and tensorflow 1.12.0. Version for pandas is 0.24.0. I tried to downgrade the pandas to 0.19.2 but it broke other parts of my code which use f2py library.\r\n\r\nAny thoughts on how to fix this issue?", "comments": ["I have just solved this problem. I had a similar issue, It may be a problem caused by package conflicts.  .You can try this in your console:\r\n\r\n`conda upgrade --all -y`\r\n\r\nThis command above can solve the following two questions.\r\n\r\n**Issue 1\uff1a**\r\n`>>> import pandas\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/hyg/soft/anaconda3/lib/python3.6/site-packages/pandas/__init__.py\", line 42, in <module>\r\n    from pandas.core.api import *\r\n  File \"/Users/hyg/soft/anaconda3/lib/python3.6/site-packages/pandas/core/api.py\", line 26, in <module>\r\n    from pandas.core.groupby import Grouper\r\n  File \"/Users/hyg/soft/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\r\n    from pandas.core.groupby.groupby import GroupBy  # noqa: F401\r\n  File \"/Users/hyg/soft/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\", line 20, in <module>\r\n    import pandas.compat as compat\r\nAttributeError: module 'pandas' has no attribute 'compat'`\r\n\r\n**Issue 2\uff1a**\r\n`In [3]: import pandas\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-38d4b0363d82> in <module>()\r\n----> 1 import pandas\r\n\r\n~/soft/anaconda3/lib/python3.6/site-packages/pandas/__init__.py in <module>()\r\n     40 import pandas.core.config_init\r\n     41 \r\n---> 42 from pandas.core.api import *\r\n     43 from pandas.core.sparse.api import *\r\n     44 from pandas.tseries.api import *\r\n\r\n~/soft/anaconda3/lib/python3.6/site-packages/pandas/core/api.py in <module>()\r\n      8 from pandas.core.dtypes.missing import isna, isnull, notna, notnull\r\n      9 from pandas.core.arrays import Categorical\r\n---> 10 from pandas.core.groupby.groupby import Grouper\r\n     11 from pandas.io.formats.format import set_eng_float_format\r\n     12 from pandas.core.index import (Index, CategoricalIndex, Int64Index,\r\n\r\n~/soft/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/__init__.py in <module>()\r\n      1 # flake8: noqa\r\n----> 2 from pandas.core.groupby.groupby import (\r\n      3     Grouper, GroupBy, SeriesGroupBy, DataFrameGroupBy\r\n      4 )\r\n\r\n~/soft/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py in <module>()\r\n     47                                CategoricalIndex, _ensure_index)\r\n     48 from pandas.core.arrays import ExtensionArray, Categorical\r\n---> 49 from pandas.core.frame import DataFrame\r\n     50 from pandas.core.generic import NDFrame, _shared_docs\r\n     51 from pandas.core.internals import BlockManager, make_block\r\n\r\n~/soft/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py in <module>()\r\n     72                                    create_block_manager_from_arrays,\r\n     73                                    create_block_manager_from_blocks)\r\n---> 74 from pandas.core.series import Series\r\n     75 from pandas.core.arrays import Categorical, ExtensionArray\r\n     76 import pandas.core.algorithms as algorithms\r\n\r\n~/soft/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in <module>()\r\n     65 from pandas.compat.numpy import function as nv\r\n     66 \r\n---> 67 import pandas.core.ops as ops\r\n     68 import pandas.core.algorithms as algorithms\r\n     69 \r\n\r\nAttributeError: module 'pandas' has no attribute 'core'\r\n`\r\n\r\n", "Thanks for your reply. I updated the conda as per your suggestion. I am having the following error now\r\n\r\nAttributeError: type object 'scipy.interpolate.interpnd.array' has no attribute '__reduce_cython__'\r\n", "Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Are you getting the error during installation or during running some code. It would be great if you can provide a small code to reproduce the error. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "For me downgrading pandas to 0.23.0 and using scipy 1.1.0 fixed this issue", "@art1 thanks for providing a solution. Thanks!", "@art1 Thank you. Your solution perfectly works for me.", "For 0.25,0.24,0.23, as in doc\r\n\r\n```\r\nWarning\r\n\r\nThe pandas.core, pandas.compat, and pandas.util top-level modules are PRIVATE. Stable functionality in such modules is not guaranteed. \r\n```\r\nas in 0.23 \r\nhttps://pandas.pydata.org/pandas-docs/version/0.23/api.html?highlight=compat\r\n\r\nupdate: downgrade to 0.23 works for me too, thx!\r\n\r\nand in 0.24\r\nhttps://pandas.pydata.org/pandas-docs/version/0.24/reference/index.html\r\n\r\nand in stable(Now 0.25)\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/index.html?highlight=compat", "In my case, this error was caused my mixing package installations via pip and conda. In my case, I had installed pandas via pip by mistake, while using conda as a main package manager.\r\n\r\nTo check use:\r\nconda list \r\n\r\nIf the Channel of a package is \"pypi\", it was installed via pip.\r\nThe solution was:\r\npip uninstall pandas\r\nconda install pandas", "Thanks @hongyonggan , after upgrading conda it works fine."]}, {"number": 26265, "title": "Clean up after removed linear_regression.py", "body": "In commit 1b114bf linear_regression.py was deleted from:\r\ntensorflow/examples/get_started/regression\r\n\r\nThe deleted file is still referenced in BUILD and in test.py. Cleaning up\r\nthose references.", "comments": []}, {"number": 26264, "title": "Merge pull request #1 from tensorflow/master", "body": "aa", "comments": []}, {"number": 26263, "title": "Error when using tf.constant to provide input to Keras model.predict", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Google Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 2.7/3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: K80\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nThis is a followup to the [issue ](https://github.com/aamini/introtodeeplearning_labs/issues/22#issue-416226096) raised in MIT's TensorFlow labs. When I use a TensorFlow tensor created using `tf.constant` as input to Keras `model.predict` it gives an error. However, when I directly feed in the same tensor to the Keras model it gives output as expected. \r\n\r\n**Describe the expected behavior**\r\nAccording to the documentation provided, model.predict should also be able to take a TensorFlow tensor as input.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\n# Import relevant packages\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\n# Define the number of inputs and outputs\r\nn_input_nodes = 2\r\nn_output_nodes = 3\r\n\r\n# First define the model \r\nmodel = Sequential()\r\n\r\n'''TODO: Define a dense (fully connected) layer to compute z'''\r\ndense_layer = Dense(n_output_nodes, input_shape=(n_input_nodes,),activation='sigmoid') # TODO \r\n\r\n# Add the dense layer to the model\r\nmodel.add(dense_layer)\r\n```\r\nNow when I do prediction using:\r\n```\r\n# Test model with example input\r\nx_input = tf.constant([[1.0,2.]], shape=(1,2))\r\n'''TODO: feed input into the model and predict the output!'''\r\nprint(model.predict(x_input)) # TODO\r\n```\r\nI get following error:\r\n\r\n> InvalidArgumentError: In[0] is not a matrix. Instead it has shape [2]\r\n> [[{{node MatMul_3}}]] [Op:StatefulPartitionedCall]\r\n\r\nWhen I use directly feed the tensor:\r\n```\r\n# Test model with example input\r\nx_input = tf.constant([[1,2.]], shape=(1,2))\r\n'''TODO: feed input into the model and predict the output!'''\r\nprint model(x_input)\r\n```\r\nI get the expected behaviour:\r\n\r\n> tf.Tensor([[0.31025296 0.48313126 0.7821198 ]], shape=(1, 3), dtype=float32)\r\n\r\n@aamini also had the same behavior. Is this expected in TensorFlow 2.0 or a bug?. If it is expected, please update the documentation.", "comments": ["Thanks for the bug report @sibyjackgrove, it looks like this is fixed in the latest tf-nightly as I'm unable to reproduce", "Thanks for update @omalleyt12 will try it out."]}, {"number": 26262, "title": "TF Boosted Trees doesn't consume the tf.Dataset as expected (incorrect checksum for freed object - object was probably modified after being freed)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs/CloudML\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 2.7\r\n- CPU training\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nThe model doesn't consume the dataset object properly.\r\n\r\nMy input_fn is:\r\n\r\n```\r\ndef build_training_input_fn(file_pattern, train=False):\r\n    \"\"\"Creates an input function reading from transformed data.\r\n    Args:\r\n      transformed_examples: Base filename of examples.\r\n    Returns:\r\n      The input function for training or valid.\r\n    \"\"\"\r\n\r\n    def parse_record(record):\r\n        transformed_metadata = metadata_io.read_metadata(\r\n            os.path.join(\r\n                'gs://path_to_bucket/transform',\r\n                transform_fn_io.TRANSFORMED_METADATA_DIR))\r\n        transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\r\n\r\n        transformed_features = tf.parse_single_example(record, transformed_feature_spec)\r\n        cols_to_remove = {'label_key'}\r\n        transformed_labels = transformed_features.pop('label_key')\r\n        transformed_features = {key: value for (key, value) in transformed_features.items() if\r\n                                key not in cols_to_remove}\r\n        return transformed_features, transformed_labels\r\n\r\n    def input_fn(train=train):\r\n        \"\"\"Input function for training and valid.\"\"\"\r\n        files = tf.data.Dataset.list_files(file_pattern=file_pattern)\r\n        dataset = files.apply(\r\n            tf.data.experimental.parallel_interleave(\r\n                lambda filename: tf.data.TFRecordDataset(filename),\r\n                cycle_length=32,\r\n                block_length=1,\r\n                sloppy=True,\r\n            ))\r\n        if train:\r\n            dataset = dataset.repeat(None)\r\n\r\n        dataset = dataset.apply(tf.data.experimental.map_and_batch(\r\n            map_func=parse_record, batch_size=64, drop_remainder=False,\r\n            num_parallel_batches=16))\r\n\r\n        return dataset\r\n\r\n    return input_fn\r\n```\r\n\r\nI train the model using:\r\n```\r\n    classifier = tf.estimator.BoostedTreesClassifier()\r\n    input_fn_train = build_training_input_fn(file_pattern=train_directory, train=True)\r\n    classifier.train(input_fn=input_fn_train)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should consume the dataset provided by the `train_input_fn` and build trees. \r\n\r\n**Code to reproduce the issue**\r\nUnfortunately, I'm unable to share this.\r\n\r\n**Other info / logs**\r\nWhen training on CloudML, the error is: ```The replica master 0 exited with a non-zero status of 11(SIGSEGV).```\r\n\r\nLocally, I get:\r\n\r\n```\r\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\r\npython(71585,0x700009986000) malloc: *** error for object 0x7fe0d6f99e00: incorrect checksum for freed object - object was probably modified after being freed.\r\n*** set a breakpoint in malloc_error_break to debug\r\n```\r\n\r\nAs a side note, if I simply yield the data using the snippet below, I get the `OutOfRangeError` error as expected.\r\n```\r\n    dataset = input_fn_eval()\r\n    next_val = dataset.make_one_shot_iterator().get_next()\r\n    with tf.Session() as sess:\r\n        while True:\r\n            try:\r\n                x, y = sess.run(next_val)\r\n                print(x)\r\n                sleep(0)\r\n                # predictions = classifier.predict(input_fn=(x, y))\r\n                # print([i for i in predictions])\r\n            except tf.errors.OutOfRangeError as e:\r\n                print(e)\r\n                print('Expected Behaiviour')\r\n                break\r\n```\r\n", "comments": ["@skiler07 Could you create a simple code (not your proprietary) to reproduce the bug? Also provide fill trace of error log. Thanks!", "@jvishnuvardhan I shall spend some time to make it reproducible. With regards to the error logs, there are no logs. I get C level errors with no trace. ", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26262\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26262\">No</a>\n"]}, {"number": 26261, "title": "[Intel MKL] Upgrading MKL DNN to 0.18 RC", "body": "", "comments": ["@penpornk Thanks for the quick review. I modified the PR based on your comments.", "@agramesh1 Merging is blocked because your workspace.bzl is now out of date. Could you please sync? Thank you!", "@penpornk  I re-based the code."]}, {"number": 26260, "title": "Creating an unused Mean metric in a custom model's constructor breaks the model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION: '2.0.0-dev20190301'\r\ntf.version.GIT_VERSION: 'v1.12.0-9345-g4eeb2714f4'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen creating a custom model, simply creating a `Mean` metric  in the constructor and setting it as one of the attributes leads to an exception when fitting the model.\r\nMoreover, if I use the metric, it seems to burn the batch size into the model, so it behaves like a stateful model.\r\n\r\n**Describe the expected behavior**\r\nCreating a `Mean` instance should be harmless, especially if it is unused. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\ntraining_set_size = 32 * 10 # <= works only if self.add_metric() is added and this is a multiple of 32\r\nX = np.random.randn(training_set_size, 8) \r\ny = np.random.randn(training_set_size, 1)\r\n\r\nclass MyModel(keras.models.Model):\r\n    def __init__(self, output_dim, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.out = keras.layers.Dense(output_dim)\r\n        self.my_metric = keras.metrics.Mean()\r\n\r\n    def call(self, inputs):\r\n        #self.add_metric(self.my_metric(5.)) # <= Works if you add this line, but model is stateful\r\n        return self.out(inputs)\r\n\r\nmodel = MyModel(1)\r\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\r\nhistory = model.fit(X, y, epochs=2)\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stacktrace:\r\n\r\n```\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-1-ca089e94cadc> in <module>\r\n     19 model = MyModel(1)\r\n     20 model.compile(loss=\"mse\", optimizer=\"nadam\")\r\n---> 21 history = model.fit(X, y, epochs=2)\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874\r\n    875   def evaluate(self,\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    149\r\n    150   # Get step function and loop type.\r\n--> 151   f = _make_execution_function(model, mode)\r\n    152   use_steps = is_dataset or steps_per_epoch is not None\r\n    153   do_validation = val_inputs is not None\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in _make_execution_function(model, mode)\r\n    519   if model._distribution_strategy:\r\n    520     return distributed_training_utils._make_execution_function(model, mode)\r\n--> 521   return model._make_execution_function(mode)\r\n    522\r\n    523\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _make_execution_function(self, mode)\r\n   2229   def _make_execution_function(self, mode):\r\n   2230     if mode == ModeKeys.TRAIN:\r\n-> 2231       self._make_fit_function()\r\n   2232       return self._fit_function\r\n   2233     if mode == ModeKeys.TEST:\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _make_fit_function(self)\r\n   2172   def _make_fit_function(self):\r\n   2173     metrics_tensors = [\r\n-> 2174         self._all_stateful_metrics_tensors[m] for m in self.metrics_names[1:]\r\n   2175     ]\r\n   2176     self._make_train_function_helper(\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in <listcomp>(.0)\r\n   2172   def _make_fit_function(self):\r\n   2173     metrics_tensors = [\r\n-> 2174         self._all_stateful_metrics_tensors[m] for m in self.metrics_names[1:]\r\n   2175     ]\r\n   2176     self._make_train_function_helper(\r\n\r\nKeyError: 'mean'\r\n```\r\n\r\n", "comments": ["This is fixed TF 2.0 nightly '2.0.0-dev20190718'\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26260\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26260\">No</a>\n"]}, {"number": 26258, "title": "Support matrix_inverse in XLA", "body": "It seems MatrixInverse op is not supported in the XLA. \r\n\r\nIndeed, trying to `tf.compile` a graph containing a `tf.matrix_inverse` op yields the following error \r\n\r\n    No registered 'MatrixInverse' OpKernel for XLA_CPU_JIT devices compatible with node \r\n\r\nCan you please add that support?\r\n\r\nIn the meanwhile, can you suggest a workaround to compile a graph containing a matrix_inverse operation? I thought about re-writing the matrix_inverse in terms of LU decomposition, but LU is not supported neither.", "comments": ["@jlebar ", "> Can you please add that support?\r\n\r\nWe aren't actively working on this, but patches would be welcome.  See e.g. the recent patches to add Cholesky support.\r\n\r\n> In the meanwhile, can you suggest a workaround to compile a graph containing a matrix_inverse operation? \r\n\r\nThe workaround is not to use tfcompile.  In JIT mode, you can use autoclustering to avoid sending unsupported operations to XLA.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 26257, "title": "[TF 2.0 API docs] expanding the tf.keras.activations.relu and tf.keras.activations.softmax documentation ", "body": "This addresses issue #26211 (https://github.com/tensorflow/tensorflow/issues/26211). I would be very happy to revise anything that's still incorrect here.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26257) for more info**.\n\n<!-- need_author_cla -->", "I have now signed the CLA with both this account and the one from which I made the changes. Hopefully this comment triggers the CLA check to run again.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26257) for more info**.\n\n<!-- need_author_consent -->", "Re: \"We need to confirm that all authors are ok with their commits being contributed to this project. Please have them confirm that here in the pull request.\"\r\n\r\nYes, I confirm that I am OK with my commits being contributed to this project.", "> I have now signed the CLA with both this account and the one from which I made the changes. Hopefully this comment triggers the CLA check to run again.\r\n\r\n@BlackHoundNate did you modify using other username, i see there is a commit using @NateMeyvis , if that the case you need to sign the CLA for other usernames", "> > I have now signed the CLA with both this account and the one from which I made the changes. Hopefully this comment triggers the CLA check to run again.\r\n> \r\n> @BlackHoundNate did you modify using other username, i see there is a commit using @NateMeyvis , if that the case you need to sign the CLA for other usernames\r\n\r\nI did modify using @NateMeyvis. Both of these accounts have signed the CLA (confirmed by an email I received at 10:08a). It appears that a project maintainer will have to change the CLA label to \"yes;\" is that correct? Again, if there's anything else i need to do, please just tell me.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26257) for more info**.\n\n<!-- cla_yes -->", "Thank you for contributing, Nate! We appreciate it. \ud83d\ude42 ", "> Thank you for contributing, Nate! We appreciate it. \ud83d\ude42\r\n\r\nHappy to help! I'm also eager to correct any mistakes I might have made. TensorFlow has been good to me and I'd like to give back a bit. Thanks for the note.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26257) for more info**.\n\n<!-- need_author_consent -->", "First, I apologize in advance if the softmax docstring should have been a separate PR.\r\n\r\nSecond, the same CLA warning has just been generated because the PR still contains commits from both BlackHoundNate and NateMeyvis (sorry again). I have signed CLAs for both these accounts.\r\n\r\nIf there is anything else I should be doing, please let me know!", "@BlackHoundNate could you please review comments , and resolve conflicts.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26256, "title": "Contrib AdaMax implementation producing NaNs on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): Tested on 1.12 and 1.13\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory: verified on 1080ti and titan v\r\n\r\n**Describe the current behavior**\r\n\r\nOn GPU AdaMax from `tf.contrib.opt.AdaMaxOptimizer` appears to apply NaNs to variables. Seems fine on CPU and bizarrely if any ops are put as a control_dependency to the apply_grads call then everything seems fine.\r\n\r\n**Describe the expected behavior**\r\nNot to produce NaNs.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.get_variable(\"a\", shape=[10000])\r\nb = tf.get_variable(\"b\", shape=[10000])\r\nloss = tf.minimum(tf.reduce_mean(a), tf.reduce_mean(b))\r\nsess = tf.Session()\r\n\r\nprint(\"===== NORMALLY ======\")\r\n\r\nopt_op= tf.contrib.opt.AdaMaxOptimizer().minimize(loss)\r\nsess.run(tf.global_variables_initializer())\r\nfor a in range(10):\r\n    print(sess.run([opt_op, loss])[1])\r\n\r\nprint(\"===== WITH NOOP ======\")\r\n\r\nopt = tf.contrib.opt.AdaMaxOptimizer()\r\ngrads_and_vars = opt.compute_gradients(loss)\r\nnoop = tf.no_op()\r\n    \r\nwith tf.control_dependencies([noop]):\r\n    opt_op_fixed = opt.apply_gradients(grads_and_vars)\r\n\r\nsess.run(tf.global_variables_initializer())\r\nfor a in range(10):\r\n    print(sess.run([opt_op, loss])[1])\r\n```\r\n\r\nThe above code produces on my machine:\r\n```\r\n===== NORMALLY ======\r\n-3.637023e-05\r\n-0.00037572667\r\nnan\r\nnan\r\nnan\r\nnan\r\nnan\r\nnan\r\nnan\r\nnan\r\n===== WITH NOOP ======\r\n-9.1626454e-05\r\n-0.00018933268\r\n-0.00028703889\r\n-0.00038474516\r\n-0.0004824514\r\n-0.0005801577\r\n-0.0006778639\r\n-0.00077557017\r\n-0.0008732763\r\n-0.00097098254\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I can reproduce on GPU device on tf 1.11.0 as well.", "Thank you for pointing it out. I'll take a look at it.", "Hi, could you take a try https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax ?  Because tf.contrib has been deprecated, and tf.contrib.Adamax will be deleted (replaced by tf.keras.AdaMax). Thank you :-)", "In TF 2.0, tf.keras.optimizers.Adamax (and I think Adam as well) has the same problem. I had to use a different optimizer (SDG) in order to get mobilenet_v2 to train and not produce a NaN loss", "@benleetownsend I could not reproduce this with TF1.12.0 (gist is [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/aee69fe92d830ba63f1f8b9b706d28f8/untitled118.ipynb)). But TF1.13.1 still has the issue (gist is [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/a38562b050931db33b09bddc34451ded/untitled117.ipynb)). Both the gists were created using Google colab with GPU. Thanks!", "@jvishnuvardhan Vishnuvardhan, as @maziello reported, keras Adamax has the same problem, could you take a try to reproduce it? I think we should address it in keras optimizer module, rather than contrib module.", "Here, with tensorflow 2 (tf.keras):\r\n* Adamax: CPU is ok and GPU NaN\r\n* Adam: CPU is ok and GPU is ok", "Also have this problem. The other keras optimizers work just fine, Adamax works on CPU but not on GPU, using tensorflow 2.", "@tanzhenyu @case540 Can you take a look? Thanks.", "Can you try Tensorflow 1.14 with tf.keras.optimizers.Adamax?", "Nan loss there as well. If it helps, I'm using a network with an embedding layer and LSTM (CuDNNLSTM), so I assume it could be a problem with either the dense or the sparse update.", "Hmm...does the same thing happen on CPU as well?", "Nope, Adamax works just fine on the CPU. To elaborate: it happens immediately* and every time with Adamax on the GPU, and never with Adamax on the CPU. Confirmed just now in colab with TF 1.14.\r\n\r\n\\* On the second batch, after updating the weights once, the loss is nan.\r\n\r\nIt's not a problem for me at the moment, though. I switched to LazyAdam from tensorflow addons, which also handles sparse updates well.", "Hello, I have this issue on GPU AWS 1.14.0 using tf.keras.optimizers.Adamax recently which did not occur prior to the update when they were still using 1.13.1", "1.13.1 probably doesn't have GPU kernel that's why it got relocated to CPU.\r\nThe GPU kernel was probably introduced after 1.14, which had a major bug.\r\nThis is fixed on a 09/11 code change and should be reflected in either 1.15 rc3, or 2.0.\r\nPlease try that and let us know if that fixed your issue.", "I tried it on 2.0 on my local machine and it works on GPU. However, AWS is still stuck on 1.14.0 for now which sucks and the bug is still there.", "@Edwin-Koh1 by \"it works on GPU\" you mean the NAN is gone, correct?", "Yes, the NAN's are gone for Adamax optimizer on GPU training on tensorflow 2.0.0 but still persists on 1.14", "Ok. I'm gonna close this for now. AWS could update to 1.15 later since that should get fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26256\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26256\">No</a>\n", "Still having this problem on `1.15` is there any solution besides updating to `2.0.0`?", "> Still having this problem on `1.15` is there any solution besides updating to `2.0.0`?\r\n\r\nDid you try tf.contrib optimizer or tf.keras optimizer?", "Yes, but for my specific use case I need the AdaMax optimizer to inherit `tf.train.Optimizer` which the `tf.keras` optimizer isn't doing.  ", "I faced the same inf validation loss using LSTMs with tf 1.15 and GPUs but not with CPUs. Once I switched from Adamax to Adam, it runs great on GPUs. So perhaps, the Adamax bug is only fixed on tf 2+", "BUG is in  tensorflow/core/kernel/training_ops_gpu.cc:\r\nchange :\r\nvar.device(d) -= \r\n        lr / (beta1_power.constant(one) - \r\n            beta1_power).reshape(single).broadcast(bcast)*\r\n                 (m / (v + epsilon));\r\nto:\r\nvar.device(d) -= \r\n        (lr / (beta1_power.constant(one) - \r\n            beta1_power)).reshape(single).broadcast(bcast)*\r\n                 (m / (v + epsilon.reshape(single).broadcast(bcast)));\r\nand the problem will not appear"]}]