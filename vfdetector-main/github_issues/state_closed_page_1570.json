[{"number": 5814, "title": "Manual Inception folder", "body": "Manually downloaded inception folder.", "comments": ["Can one of the admins verify this patch?", "@Research2, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden, @ilblackdragon and @tensorflower-gardener to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "With this change, all users would have to manually download the inception data, which is a non-starter for this tutorial. If I'm missing something, please kindly point it out and we can re-open this PR."]}, {"number": 5813, "title": "Fix a bug which repeated variable creation in optimizer when using bucketing", "body": "Hi, all,\r\n\r\nIt's a pull request mentioned in [#5786](https://github.com/tensorflow/tensorflow/issues/5786).\r\n\r\nWhen we use adam or momentum as optimizer in bucketing, the variables in optimizer will not be shared between different buckets.\r\n\r\nBecause it uses `name_scope` to create scope name (and potentially add a suffix to the scope name if a variable with such name already exists) and uses `Variable` to create variable.\r\nSee [_create_slot_var](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L47) and [create_slot](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L67).\r\n```python\r\ndef _create_slot_var(primary, val, scope):\r\n  \"\"\"Helper function for creating a slot variable.\"\"\"\r\n  slot = variables.Variable(val, name=scope, trainable=False)\r\n  ...\r\n  return slot\r\n\r\n\r\ndef create_slot(primary, val, name, colocate_with_primary=True):\r\n  with ops.name_scope(primary.op.name + \"/\" + name) as scope:\r\n    if colocate_with_primary:\r\n      with ops.colocate_with(primary):\r\n        return _create_slot_var(primary, val, scope)\r\n    else:\r\n      return _create_slot_var(primary, val, scope)\r\n```\r\n\r\nBut there is a potential bug when using bucketing in seq2seq task. \r\nEvery bucket will create a list of variables in its optimizer and they should be shared with each other. But now these variables will not be shared.\r\nDiscussion is [#5786](https://github.com/tensorflow/tensorflow/issues/5786).\r\n\r\nFor example, \r\nwhen I use two buckets and adam optimizer in my task, it will create two repeated variable lists:\r\n```\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_1:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_1:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam:0 (1600,)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_1:0 (1600,)\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_2:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_3:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_2:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_3:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_2:0 (1600,)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_3:0 (1600,)\r\n```\r\nAs a solution, I remove `name_scope` and replace `Variable` to `get_variable` for sharing variables in bucketing.\r\n\r\nAnd meanwhile adam optimizer will create two different lists of variables for parameter update.\r\n[Adam](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py#L118)\r\n```python\r\n    for v in var_list:\r\n      self._zeros_slot(v, \"m\", self._name)\r\n      self._zeros_slot(v, \"v\", self._name)\r\n```\r\nAnd we have to distinguish the creations of these two variables' lists.\r\nSo I think add `slot_name` as a suffix to `op_name` is a suitable solution. It use `op_name + '/' + slot_name` to create different shared variables.\r\n\r\n```python\r\n  def _zeros_slot(self, var, slot_name, op_name):\r\n    named_slots = self._slot_dict(slot_name)\r\n    if var not in named_slots:\r\n      named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\r\n    return named_slots[var]\r\n```\r\nSo I add `op_name += '/' + slot_name` in [Optimizer._zeros_slot](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L561\r\n) and [Optimizer._zeros_slot](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L579\r\n).\r\n\r\n## Testing\r\nI have run the tests in [training](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/training) and there are all OK.\r\nAnd I have run my code, and it seems that it works well.\r\nAnd I have run the example of mnist, the log is below:\r\n```\r\n#Result of MNIST in tensorflow/models/image/mnist/convolutional.py\r\nStep 8500 (epoch 9.89), 122.8 ms\r\nMinibatch loss: 1.618, learning rate: 0.006302\r\nMinibatch error: 0.0%\r\nValidation error: 0.9%\r\nTest error: 0.8%\r\n```\r\n\r\nI'm not sure if I miss some important things. \r\nAnd please read my pull request and give some advices if it could fix this bug. \r\n\r\nGetting a lot of help from @lukaszkaiser .Thanks so much.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@Syndrome777, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @tensorflower-gardener to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Small questions in the review, thanks for doing it!", "Thanks for the explanation. I think we should put a variable_scope somewhere with a default name, so it remain possible to create 2 optimizers without problem. Explained above, do you agree? Thanks!", "Yes, I really agree with you. I have replaced name_scope with variable_scope [HERE](https://github.com/Syndrome777/tensorflow/blob/0362d40a5ace928025d5c4518dbcc3ba87a4b8bd/tensorflow/python/training/slot_creator.py#L84) and explain it above.\r\nPlease review it again, thank you so much.", "Thank you so much. So will @tensorflow-jenkins help us to test this change?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@lukaszkaiser It seems that there may be still some bugs here to load the former saved models. \r\nI have read the log, and It seems that the test failed at saver.restore(). I think it's triggered by that we have changed the variable's name in optimizer.\r\nIf we remove the change that replacing 'Adam_N' with 'Adam:w' ('Adam: v'), it may be pass these failure.\r\nDo you have any idea to deal with it?\r\n```\r\nCaused by op u'save/RestoreV2', defined at:\r\n  File \"/workspace/pip_test/tests/io_test.py\", line 123, in <module>\r\n    tf.test.main()\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/platform/test.py\", line 91, in main\r\n    return _googletest.main()\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/platform/googletest.py\", line 84, in main\r\n    benchmark.benchmarks_main(true_main=g_main)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/platform/benchmark.py\", line 321, in benchmarks_main\r\n    true_main()\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/platform/googletest.py\", line 58, in g_main\r\n    return unittest_main(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/unittest/main.py\", line 95, in __init__\r\n    self.runTests()\r\n  File \"/usr/lib/python2.7/unittest/main.py\", line 232, in runTests\r\n    self.result = testRunner.run(self.test)\r\n  File \"/usr/lib/python2.7/unittest/runner.py\", line 151, in run\r\n    test(result)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 70, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 108, in run\r\n    test(result)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 70, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 108, in run\r\n    test(result)\r\n  File \"/usr/lib/python2.7/unittest/case.py\", line 395, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/usr/lib/python2.7/unittest/case.py\", line 331, in run\r\n    testMethod()\r\n  File \"/workspace/pip_test/tests/io_test.py\", line 64, in test_pandas_series\r\n    score = accuracy_score(labels, list(classifier.predict(data)))\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\", line 68, in _as_iterable\r\n    for pred in preds:\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 902, in _infer_model_as_iterable\r\n    restore_checkpoint_path=checkpoint_path):\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 858, in run_feeds_iter\r\n    _restore_from_checkpoint(session, g, restore_checkpoint_path)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 93, in _restore_from_checkpoint\r\n    saver = saver or _make_saver(graph)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 86, in _make_saver\r\n    max_to_keep=keep_checkpoint_max)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1028, in __init__\r\n    self.build()\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1058, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 648, in build\r\n    restore_sequentially, reshape)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 429, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 386, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 225, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2371, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1258, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Overlapping slices: existing slice = 0,4:0,3, new slice = 0,4:0,3\r\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n```", "Yes, I think we have to make the names the same, so we don't break old checkpoints. I'd prefer to still use variable_scope with default name, so that it's ok if someone uses 2 different Adams. But if there is only one, we need to make the names the same. Can you do that? Great thanks for the careful PR!", "BTW, \r\nI use the code below, get different prints between the master's implementation and ours.\r\nWe use `variable_scope.variable_scope(None, primary.op.name + '/' + name)` and master's implementation uses `ops.name_scope(primary.op.name + \"/\" + name) as scope` as scope name.\r\n```\r\nMaster's print:\r\n['global_step', 'linear//weight', 'linear//weight/Momentum', 'linear/bias_weight', 'linear/bias_weight/Momentum']\r\nOur print:\r\n['global_step', 'linear//weight', 'linear//weight/', 'linear/bias_weight', 'linear/bias_weight/']\r\n```\r\nMeanwhile, I find the scope name in slot_creator.py of master' implementation is `linear//weight/part_0/Momentum`, but in `LinearRegressor.get_variable_names()`, it is `linear//weight/Momentum`, `part_0` is removed somewhere. \r\nThe interesting thing is the scope name in our implementation is `linear//weight/part_0/Momentum`, but in `LinearRegressor.get_variable_names()`, it is `linear//weight/`\r\n```\r\n#scope name in slot_creator.py\r\nlinear//weight/part_0/Momentum\r\nlinear/bias_weight/part_0/Momentum\r\n\r\n#variable name in LinearRegressor.get_variable_names()\r\nlinear//weight/Momentum\r\nlinear//bias_weight/Momentum\r\n```\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef boston_input_fn():\r\n  boston = tf.contrib.learn.datasets.load_boston()\r\n  features = tf.cast(tf.reshape(tf.constant(boston.data), [-1, 13]), tf.float32)\r\n  labels = tf.cast(tf.reshape(tf.constant(boston.target), [-1, 1]), tf.float32)\r\n  return features, labels\r\n\r\n\r\nclass FeatureColumnTest(tf.test.TestCase):\r\n\r\n  def testTrain(self):\r\n    feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input_fn(\r\n        boston_input_fn)\r\n    optimizer = tf.train.MomentumOptimizer(1e-3, 0.9)\r\n    est = tf.contrib.learn.LinearRegressor(feature_columns=feature_columns, optimizer=optimizer)\r\n    est.fit(input_fn=boston_input_fn, steps=1)\r\n    print ('------------------------------------')\r\n    print (est.get_variable_names())\r\n    _ = est.evaluate(input_fn=boston_input_fn, steps=1)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.test.main()\r\n```", "Do the tests pass now? Why aren't they completing?", "@lukaszkaiser  No, the tests still have failures as last time.\r\nI'm afraid that the change in this PR will clash with some implementation in tf.contrib. The clash may be in `Saver.save` of tf.contrib.\r\nSee this for more detail.\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/2766/\r\nFor example, I use tf.contrib to build a LR model with Adam. After `fit` action, the model will be saved as `ckpt.meta` and `ckpt`. Then restore this model, I get variable name from  `ckpt.meta`, and it's right for me. But I get different names in `ckpt`, and two variables have the same name, so the `Saver.restore` will fail.\r\n```\r\n# Variable name in 'ckpt.meta'\r\nglobal_step:0 ()\r\nlinear//weight/part_0:0 (13, 1)\r\nlinear/bias_weight/part_0:0 (1,)\r\nbeta1_power:0 ()\r\nbeta2_power:0 ()\r\nlinear//weight/part_0/Adam:0 (13, 1)\r\nlinear//weight/part_0/Adam_1:0 (13, 1)\r\nlinear/bias_weight/part_0/Adam:0 (1,)\r\nlinear/bias_weight/part_0/Adam_1:0 (1,)\r\n\r\n# Variable name in 'ckpt' as master's implementation\r\nglobal_step:0 ()\r\nlinear//weight (13, 1)\r\nlinear/bias_weight (1,)\r\nbeta1_power ()\r\nbeta2_power ()\r\nlinear//weight/Adam (13, 1)\r\nlinear//weight/Adam_1 (13, 1)\r\nlinear/bias_weight/Adam (1,)\r\nlinear/bias_weight/Adam_1 (1,)\r\n\r\n# Variable name in 'ckpt' when this PR is merged\r\nglobal_step ()\r\nlinear//weight/ (13, 1)\r\nlinear/bias_weight/ (1,)\r\nbeta1_power ()\r\nbeta2_powe ()\r\nlinear//weight/ (13, 1)\r\nlinear//weight/ (13, 1)\r\nlinear/bias_weight/ (1,)\r\nlinear/bias_weight/ (1,)\r\n\r\n```\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef boston_input_fn():\r\n  boston = tf.contrib.learn.datasets.load_boston()\r\n  features = tf.cast(tf.reshape(tf.constant(boston.data), [-1, 13]), tf.float32)\r\n  labels = tf.cast(tf.reshape(tf.constant(boston.target), [-1, 1]), tf.float32)\r\n  return features, labels\r\n\r\n\r\nclass FeatureColumnTest(tf.test.TestCase):\r\n\r\n  def testTrain(self):\r\n    feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input_fn(\r\n        boston_input_fn)\r\n    #optimizer = tf.train.AdamOptimizer(1e-3) #fail!!\r\n    optimizer = MomentumOptimizer(1e-3, 0.9)\r\n    est = tf.contrib.learn.LinearRegressor(feature_columns=feature_columns, optimizer=optimizer)\r\n    est.fit(input_fn=boston_input_fn, steps=1)\r\n    print ('------------------------------------')\r\n    print (est.get_variable_names())\r\n    _ = est.evaluate(input_fn=boston_input_fn, steps=1)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.test.main()\r\n```\r\n```python\r\ndef load_meta(meta_name, ckpt_name):\r\n    sess = tf.Session()\r\n    new_saver = tf.train.import_meta_graph(meta_name)\r\n    for v in tf.global_variables():\r\n    #for v in tf.all_variables():\r\n        print(v.name)\r\n    print \"+++++++++++\"\r\n    #new_saver.restore(sess, ckpt_name)\r\n    #for v in tf.global_variables():\r\n    #for v in tf.all_variables():\r\n    #    print(v.name)\r\n\r\ndef load_ckpt(ckpt_name):\r\n    reader = tf.train.NewCheckpointReader(ckpt_name)\r\n    variable = reader.get_variable_to_shape_map()\r\n    for k, v in variable.iteritems():\r\n        print k, v\r\n```\r\n", "I think I have find this bug. I will fix it soon.\r\nI forgot to change the `variable._save_slice_info.full_name` in the previous commit, so there may be a potential bug in save and restore when user use `variable._save_slice_info.full_name` as scope name.\r\nNow I remove `real_slot_name = scope[len(primary.op.name + \"/\"):-1]` with `real_slot_name = slot.name[len(primary.op.name + \"/\"):-2]`, so the `real_slot_name` will be added as suffix of previous `full_name` correctly.\r\n@lukaszkaiser Please review it again, thank you!", "@caisq Shanqing, please ask @tensorflow-jenkins to test this PR again if possible. I have test its Linux CPU Tests in my server, all tests are passed. Thank you so much!", "@tensorflow-jenkins test this please.", "Great thanks for finding this bug! I only see 3 tests failing now, and I'm not sure if they are related to this change or not. Could you try them locally? It might be that it's all ok now, but we need to make sure.", "I'm so sorry that I don't have GPU now. But I have tested this PR in CPU locally. The tests of CPU are passed.\r\nI read the test log and the failure is strange for this PR. The GPU test is okay at last time. So  is it possible that the error of `failed to allocate GPU memory` is caused by the new commit? In my opinion, this PR will not cause `shape_ops_test`'s error like below. Do you agree with me ? \r\n```python\r\n  def _compareShapeSparse(self, x_np, use_gpu=False):\r\n    np_ans = np.array(np.shape(x_np))\r\n    x_tf, unused_nnz = _sparsify(x_np)\r\n    with self.test_session(use_gpu=use_gpu):\r\n      tf_ans = tf.shape(x_tf)\r\n      result = tf_ans.eval()\r\n    self.assertAllEqual(np_ans, result)\r\n    self.assertShapeEqual(np_ans, tf_ans)\r\n```", "I think the test is simply flaky, failing on some test hardware due to reasons unrelated to this change.\r\n\r\nShanqing, could we re-test? What's the process for unrelated test failures?", "@lukaszkaiser , can you try entering the magic phrase? You may have privilege to trigger tests as well. If not, we can get you added to the TensorFlow organization on GH.", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please.", "Failed again with different test failures. @lukaszkaiser @caisq could you give me some advice?\r\n```\r\n# testMaxPoolGradSamePadding2_2_3d\r\n\r\nInternalError (see above for traceback): cudnn PoolForward launch failed\r\n\t [[Node: max_pool3d = MaxPool3D[T=DT_FLOAT, ksize=[1, 2, 2, 2, 1], padding=\"VALID\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_input_0/_3)]]\r\n```", "I think this is just a failure of the test system -- I think this PR is ok. Let's wait a little bit, maybe the tests will get corrected. My advice: don't worry too much!", "@tensorflow-jenkins test this please.", "I have added comments for our previous modification. Please review my new commits, thank you so much.\r\nBTW the test failed again. It got error in CPU test in Python 3. [HERE](https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/2480/consoleFull) has more detail.\r\n", "@tensorflow-jenkins test this please.", "This looks good, let's merge!", "Great thanks @Syndrome777, sorry for the testing delays!"]}, {"number": 5812, "title": "Branch 139989725", "body": "Manually resolved conflicts in:\r\ntensorflow/contrib/training/BUILD\r\ntensorflow/contrib/learn/python/learn/datasets/base.py\r\ntensorflow/contrib/cmake/tf_tests.cmake\r\ntensorflow/core/kernels/cwise_op_floor_div.cc", "comments": ["@mrry, any idea about this error in the windows cmake build?\r\n\r\n```\r\n07:51:24 stderr: error: unable to create file third_party/llvm/expand_cmake_vars.py\": Invalid argument\r\n```", "@caisq It looks like there's a stray `\"` character in the filename of this file:\r\n\r\nhttps://github.com/caisq/tensorflow/blob/branch_139989725/third_party/llvm/expand_cmake_vars.py%22\r\n\r\nSince this is an oss-only file, I'm guessing one of the scripts for generating the git version got bad input? ", "Thanks for looking into it, @mrry. Since all other tests passed, shall we merge this PR and push a fix later?", "Whatever you think is easier... ISTR that changing a filename in the file-chooser script needs some manual intervention on the Git repo side, so it might be just as easy to remove manually it in this PR?", "The fix should have been submitted internally. Let me push again. Closing this PR."]}, {"number": 5811, "title": "[CMake] Make MNIST tutorial example part of the PIP package", "body": "Cherry-picking this pre-emptive fix from into the next issue of the 0.12 release.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->", "@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @rohan100jain and @danmane to be potential reviewers.", "@vit-stepanovs I'd like to cherry-pick your commits into the next release branch, but this confuses the @googlebot. If you get the chance, could you please reply to this thread saying it's okay to do this? Thanks!", "It's ok to do this."]}, {"number": 5810, "title": "Improved support for OpenCL", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @lukeiwanski, @keveman and @sherrym to be potential reviewers.", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "Jenkins, test this please.", "The failures in sanity checks are terminating all other tests.\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/2184/console", "I fixed the sanity check failure.", "Looks like after this PR on MacOS, one shard of the cwise_ops_test is consistently failing with the following error:\r\n\r\n```\r\nFAIL: testBCast_15B (__main__.BinaryOpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_jenkins/47802726cfb6783f0513af299bd0e285/execroot/tensorflow-master-mac/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cwise_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cwise_ops_test.py\", line 1031, in testBCast_15B\r\n    self._testBCastB([10, 3, 1, 2], [3, 1, 2])\r\n  File \"/private/var/tmp/_bazel_jenkins/47802726cfb6783f0513af299bd0e285/execroot/tensorflow-master-mac/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cwise_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cwise_ops_test.py\", line 829, in _testBCastB\r\n    self._testBCastByFunc(funcs, xs, ys)\r\n  File \"/private/var/tmp/_bazel_jenkins/47802726cfb6783f0513af299bd0e285/execroot/tensorflow-master-mac/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cwise_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cwise_ops_test.py\", line 814, in _testBCastByFunc\r\n    self._compareBCast(ys, xs, dtype, np_func, tf_func)\r\n  File \"/private/var/tmp/_bazel_jenkins/47802726cfb6783f0513af299bd0e285/execroot/tensorflow-master-mac/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cwise_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cwise_ops_test.py\", line 795, in _compareBCast\r\n    self._compareGpu(x, y+ 0.1, np_func, tf_func)\r\n  File \"/private/var/tmp/_bazel_jenkins/47802726cfb6783f0513af299bd0e285/execroot/tensorflow-master-mac/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cwise_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cwise_ops_test.py\", line 575, in _compareGpu\r\n    self.assertAllClose(np_ans, tf_gpu)\r\n  File \"/private/var/tmp/_bazel_jenkins/47802726cfb6783f0513af299bd0e285/execroot/tensorflow-master-mac/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cwise_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 449, in assertAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n  File \"/Library/Python/2.7/site-packages/numpy-1.11.0-py2.7-macosx-10.11-intel.egg/numpy/testing/utils.py\", line 1391, in assert_allclose\r\n    verbose=verbose, header=header)\r\n  File \"/Library/Python/2.7/site-packages/numpy-1.11.0-py2.7-macosx-10.11-intel.egg/numpy/testing/utils.py\", line 733, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-06, atol=1e-06\r\n\r\n(mismatch 1.66666666667%)\r\n x: array([[[[   1.000000  +0.j      ,    1.474711  +0.703009j]],\r\n\r\n        [[   2.064002  +1.518696j,    2.834290  +2.474841j]],...\r\n y: array([[[[   1.000000  +0.j      ,    1.474711  +0.703009j]],\r\n\r\n        [[   2.064002  +1.518697j,    2.834289  +2.47484j ]],...\r\n\r\n----------------------------------------------------------------------\r\nRan 3 tests in 13.699s\r\n\r\nFAILED (failures=1)\r\n```\r\n\r\n@benoitsteiner @lukeiwanski I looked at the code, and I see that in _compareBCast in cwise_ops_test we add `0.1` to one of the args.\r\nIs it by any chance a leftover?\r\nI will send a PR to fix."]}, {"number": 5809, "title": "multi-GPU training? ", "body": "We are trying to scale up one of the detection deep learning architecture written in Tensorflow for multi-GPU training. Here's the [link](https://github.com/Russell91/TensorBox) to the architecture.\r\n\r\nWe need specific help in understanding the properties of the gradients such as shape, type and more importantly ways to access the same so that it can be built on two GPUs separately and synchronize like in the cifar_multi_gpu training.. \r\n\r\nThis is the grad step in build function and we would like to access in train function:\r\n\r\nif H['clip_norm'] <= 0:\r\n                grads = tf.gradients(loss['train'], tvars)\r\n            else:\r\n                grads, norm = tf.clip_by_global_norm(tf.gradients(loss['train'], tvars), H['clip_norm'])\r\n\r\n\r\nWe have the following functions in the train and build model steps in the overall architecture.\r\n\r\ndef build(H, q):\r\n    '''\r\n    Build full model for training, including forward / backward passes,\r\n    optimizers, and summary statistics.\r\n    '''\r\n    arch = H\r\n    solver = H[\"solver\"]\r\n\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(solver.get('gpu', ''))\r\n\r\n    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\r\n    gpu_options = tf.GPUOptions()\r\n    config = tf.ConfigProto(gpu_options=gpu_options)\r\n\r\n    learning_rate = tf.placeholder(tf.float32)\r\n    if solver['opt'] == 'RMS':\r\n        opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\r\n                                        decay=0.9, epsilon=solver['epsilon'])\r\n    elif solver['opt'] == 'Adam':\r\n        opt = tf.train.AdamOptimizer(learning_rate=learning_rate,\r\n                                        epsilon=solver['epsilon'])\r\n    elif solver['opt'] == 'SGD':\r\n        opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\r\n    else:\r\n        raise ValueError('Unrecognized opt type')\r\n    loss, accuracy, confidences_loss, boxes_loss = {}, {}, {}, {}\r\n    for phase in ['train', 'test']:\r\n        # generate predictions and losses from forward pass\r\n        x, confidences, boxes = q[phase].dequeue_many(arch['batch_size'])\r\n        flags = tf.argmax(confidences, 3)\r\n\r\n\r\n        grid_size = H['grid_width'] * H['grid_height']\r\n\r\n        (pred_boxes, pred_confidences,\r\n         loss[phase], confidences_loss[phase],\r\n         boxes_loss[phase]) = build_forward_backward(H, x, phase, boxes, flags)\r\n        pred_confidences_r = tf.reshape(pred_confidences, [H['batch_size'], grid_size, H['rnn_len'], arch['num_classes']])\r\n        pred_boxes_r = tf.reshape(pred_boxes, [H['batch_size'], grid_size, H['rnn_len'], 4])\r\n\r\n\r\n        # Set up summary operations for tensorboard\r\n        a = tf.equal(tf.argmax(confidences[:, :, 0, :], 2), tf.argmax(pred_confidences_r[:, :, 0, :], 2))\r\n        accuracy[phase] = tf.reduce_mean(tf.cast(a, 'float32'), name=phase+'/accuracy')\r\n\r\n        if phase == 'train':\r\n            global_step = tf.Variable(0, trainable=False)\r\n\r\n            tvars = tf.trainable_variables()\r\n            if H['clip_norm'] <= 0:\r\n                grads = tf.gradients(loss['train'], tvars)\r\n            else:\r\n                grads, norm = tf.clip_by_global_norm(tf.gradients(loss['train'], tvars), H['clip_norm'])\r\n            train_op = opt.apply_gradients(zip(grads, tvars), global_step=global_step)\r\n        elif phase == 'test':\r\n            moving_avg = tf.train.ExponentialMovingAverage(0.95)\r\n            smooth_op = moving_avg.apply([accuracy['train'], accuracy['test'],\r\n                                          confidences_loss['train'], boxes_loss['train'],\r\n                                          confidences_loss['test'], boxes_loss['test'],\r\n                                          ])\r\n\r\n            for p in ['train', 'test']:\r\n                tf.scalar_summary('%s/accuracy' % p, accuracy[p])\r\n                tf.scalar_summary('%s/accuracy/smooth' % p, moving_avg.average(accuracy[p]))\r\n                tf.scalar_summary(\"%s/confidences_loss\" % p, confidences_loss[p])\r\n                tf.scalar_summary(\"%s/confidences_loss/smooth\" % p,\r\n                    moving_avg.average(confidences_loss[p]))\r\n                tf.scalar_summary(\"%s/regression_loss\" % p, boxes_loss[p])\r\n                tf.scalar_summary(\"%s/regression_loss/smooth\" % p,\r\n                    moving_avg.average(boxes_loss[p]))\r\n\r\n        if phase == 'test':\r\n            test_image = x\r\n            # show ground truth to verify labels are correct\r\n            test_true_confidences = confidences[0, :, :, :]\r\n            test_true_boxes = boxes[0, :, :, :]\r\n\r\n            # show predictions to visualize training progress\r\n            test_pred_confidences = pred_confidences_r[0, :, :, :]\r\n            test_pred_boxes = pred_boxes_r[0, :, :, :]\r\n\r\n            def log_image(np_img, np_confidences, np_boxes, np_global_step, pred_or_true):\r\n                \r\n                merged = train_utils.add_rectangles(H, np_img, np_confidences, np_boxes,\r\n                                                    use_stitching=True,\r\n                                                    rnn_len=H['rnn_len'])[0]\r\n                \r\n                num_images = 10\r\n                img_path = os.path.join(H['save_dir'], '%s_%s.jpg' % ((np_global_step / H['logging']['display_iter']) % num_images, pred_or_true))\r\n                misc.imsave(img_path, merged)\r\n                return merged\r\n\r\n            pred_log_img = tf.py_func(log_image,\r\n                                      [test_image, test_pred_confidences, test_pred_boxes, global_step, 'pred'],\r\n                                      [tf.float32])\r\n            true_log_img = tf.py_func(log_image,\r\n                                      [test_image, test_true_confidences, test_true_boxes, global_step, 'true'],\r\n                                      [tf.float32])\r\n            tf.image_summary(phase + '/pred_boxes', tf.pack(pred_log_img),max_images=10)\r\n            tf.image_summary(phase + '/true_boxes', tf.pack(true_log_img),max_images=10)\r\n\r\n    summary_op = tf.merge_all_summaries()\r\n\r\n    return (config, loss, accuracy, summary_op, train_op,\r\n            smooth_op, global_step, learning_rate)\r\n\r\n\r\ndef train(H, test_images):\r\n    '''\r\n    Setup computation graph, run 2 prefetch data threads, and then run the main loop\r\n    '''\r\n\r\n    if not os.path.exists(H['save_dir']): os.makedirs(H['save_dir'])\r\n\r\n    ckpt_file = H['save_dir'] + '/save.ckpt'\r\n    with open(H['save_dir'] + '/hypes.json', 'w') as f:\r\n        json.dump(H, f, indent=4)\r\n\r\n    x_in = tf.placeholder(tf.float32)\r\n    confs_in = tf.placeholder(tf.float32)\r\n    boxes_in = tf.placeholder(tf.float32)\r\n    q = {}\r\n    enqueue_op = {}\r\n    for phase in ['train', 'test']:\r\n        dtypes = [tf.float32, tf.float32, tf.float32]\r\n        grid_size = H['grid_width'] * H['grid_height']\r\n        shapes = (\r\n            [H['image_height'], H['image_width'], 3],\r\n            [grid_size, H['rnn_len'], H['num_classes']],\r\n            [grid_size, H['rnn_len'], 4],\r\n            )\r\n        q[phase] = tf.FIFOQueue(capacity=30, dtypes=dtypes, shapes=shapes)\r\n        enqueue_op[phase] = q[phase].enqueue((x_in, confs_in, boxes_in))\r\n\r\n    def make_feed(d):\r\n        return {x_in: d['image'], confs_in: d['confs'], boxes_in: d['boxes'],\r\n                learning_rate: H['solver']['learning_rate']}\r\n\r\n    def thread_loop(sess, enqueue_op, phase, gen):\r\n        for d in gen:\r\n            sess.run(enqueue_op[phase], feed_dict=make_feed(d))\r\n\r\n    (config, loss, accuracy, summary_op, train_op,\r\n     smooth_op, global_step, learning_rate) = build(H, q)\r\n\r\n    saver = tf.train.Saver(max_to_keep=None)\r\n    writer = tf.train.SummaryWriter(\r\n        logdir=H['save_dir'],\r\n        flush_secs=10\r\n    )\r\n\r\n    with tf.Session(config=config) as sess:\r\n        tf.train.start_queue_runners(sess=sess)\r\n        for phase in ['train', 'test']:\r\n            # enqueue once manually to avoid thread start delay\r\n            gen = train_utils.load_data_gen(H, phase, jitter=H['solver']['use_jitter'])\r\n            d = gen.next()\r\n            sess.run(enqueue_op[phase], feed_dict=make_feed(d))\r\n            t = tf.train.threading.Thread(target=thread_loop,\r\n                                 args=(sess, enqueue_op, phase, gen))\r\n            t.daemon = True\r\n            t.start()\r\n\r\n        tf.set_random_seed(H['solver']['rnd_seed'])\r\n        sess.run(tf.initialize_all_variables())\r\n        writer.add_graph(sess.graph)\r\n        weights_str = H['solver']['weights']\r\n        if len(weights_str) > 0:\r\n            print('Restoring from: %s' % weights_str)\r\n            saver.restore(sess, weights_str)\r\n        else:\r\n            init_fn = slim.assign_from_checkpoint_fn(\r\n                  '%s/data/inception_v1.ckpt' % os.path.dirname(os.path.realpath(__file__)),\r\n                  [x for x in tf.all_variables() if x.name.startswith('InceptionV1') and not H['solver']['opt'] in x.name])\r\n            init_fn(sess)\r\n\r\n        # train model for N iterations\r\n        start = time.time()\r\n        max_iter = H['solver'].get('max_iter', 10000000)\r\n        for i in xrange(max_iter):\r\n            display_iter = H['logging']['display_iter']\r\n            adjusted_lr = (H['solver']['learning_rate'] *\r\n                           0.5 ** max(0, (i / H['solver']['learning_rate_step']) - 2))\r\n            lr_feed = {learning_rate: adjusted_lr}\r\n\r\n            if i % display_iter != 0:\r\n                # train network\r\n                batch_loss_train, _ = sess.run([loss['train'], train_op], feed_dict=lr_feed)\r\n            else:\r\n                # test network every N iterations; log additional info\r\n                if i > 0:\r\n                    dt = (time.time() - start) / (H['batch_size'] * display_iter)\r\n                start = time.time()\r\n                (train_loss, test_accuracy, summary_str,\r\n                    _, _) = sess.run([loss['train'], accuracy['test'],\r\n                                      summary_op, train_op, smooth_op,\r\n                                     ], feed_dict=lr_feed)\r\n                writer.add_summary(summary_str, global_step=global_step.eval())\r\n                print_str = string.join([\r\n                    'Step: %d',\r\n                    'lr: %f',\r\n                    'Train Loss: %.2f',\r\n                    'Softmax Test Accuracy: %.1f%%',\r\n                    'Time/image (ms): %.1f'\r\n                ], ', ')\r\n                print(print_str %\r\n                      (i, adjusted_lr, train_loss,\r\n                       test_accuracy * 100, dt * 1000 if i > 0 else 0))\r\n\r\n            if global_step.eval() % H['logging']['save_iter'] == 0 or global_step.eval() == max_iter - 1:\r\n                saver.save(sess, ckpt_file, global_step=global_step)\r\n\r\nThanks in advance!\r\n\r\n", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html :\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n"]}, {"number": 5808, "title": "mac install fail", "body": "when excute command\r\nsudo pip install --upgrade $TF_BINARY_URLException:\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_set.py\", line 778, in install\r\n    requirement.uninstall(auto_confirm=True)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py\", line 754, in uninstall\r\n    paths_to_remove.remove(auto_confirm)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_uninstall.py\", line 115, in remove\r\n    renames(path, new_path)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/utils/__init__.py\", line 267, in renames\r\n    shutil.move(old, new)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 299, in move\r\n    copytree(src, real_dst, symlinks=True)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 208, in copytree\r\n    raise Error, errors\r\nError: [('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py', '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py', \"[Errno 1] Operation not permitted: '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc', '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc', \"[Errno 1] Operation not permitted: '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py', '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py', \"[Errno 1] Operation not permitted: '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc', '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc', \"[Errno 1] Operation not permitted: '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib', '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib', \"[Errno 1] Operation not permitted: '/tmp/pip-1gfyUU-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib'\")]\r\nlibaochengdeMacBook-Pro:~ libaocheng$ \r\n", "comments": ["--ignore-installed"]}, {"number": 5807, "title": "max_pool3d output shape is more undefined than input shape", "body": "It looks like `tf.nn.max_pool3d` returns wrong output shape:\r\n\r\n```\r\ntf.nn.max_pool3d(tf.placeholder(tf.float32, [None, None, 100, 100, 1]), [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], 'VALID')\r\n```\r\n\r\nreturns following tensor: \r\n`<tf.Tensor 'MaxPool3D_6:0' shape=(?, ?, ?, ?, 1) dtype=float32>`\r\n\r\nI believe that dimensions 2 and 3 (counting from 0) should not be undefined.\r\n\r\nIn similar situation `tf.nn.max_pool` returns correct result.\r\n\r\n```\r\ntf.nn.max_pool(tf.placeholder(tf.float32, [None, None, 100, 1]), [1, 1, 1, 1], [1, 1, 1, 1], 'VALID')\r\n```\r\nproduces: \r\n`<tf.Tensor 'MaxPool_3:0' shape=(?, ?, 100, 1) dtype=float32>`\r\n\r\n\r\n### Environment info\r\nOperating System: Linux\r\nTensorflow version: v0.11.0 (282823b877f173e6a33bbc9d4b9ad7dd8413ada6), built from sources with bazel 0.4.0\r\n", "comments": ["@vrv Shape function problem with max_pool_3d?", "Probably: https://github.com/tensorflow/tensorflow/blob/61a099fda0dbb91e99c83436da0af20ca991ed9e/tensorflow/core/framework/common_shape_fns.cc#L560  I think we can just remove that if statement and it'll work.  I'll send a PR later today.", "I made the change internally today, should be fixed on the next internal push.", "@vrv Thanks for the rapid fix!\r\n\r\n@dm0 Please reopen if this doesn't solve your problem.", "the next push should close the bug automatically when it's available.  leaving open until then."]}, {"number": 5806, "title": "What's wrong with this loss? Shape () must have rank 1", "body": "", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html: \r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n"]}, {"number": 5805, "title": "The repository named 'local_config_cuda' could not be resolved.", "body": "I'm trying to use this structure for my project (using tensorflow as a module in my project) and my_project only contains code to build an .so library.\r\n```\r\nmy_project\r\n|-- WORKSPACE\r\n|-- my_android_app\r\n|   |-- BUILD\r\n|   `-- ...\r\n|-- tensorflow\r\n|   |-- tensorflow\r\n|   |   |   |-- workspace.bzl\r\n|   |   |   |-- tensorflow.bzl\r\n|   |   |   `-- ...\r\n|   |-- WORKSPACE\r\n|   |-- BUILD\r\n.    `-- ...\r\n```\r\n\r\nThe content of `my_project/WORKSPACE` is \r\n```\r\nworkspace(name = \"my_project\")\r\n\r\nlocal_repository(\r\n  name = \"org_tensorflow\",\r\n  path = \"tensorflow/\"\r\n)\r\n```\r\n\r\nThe content of `my_project/my_android_app/BUILD` for now is:\r\n\r\n```\r\npackage(default_visibility = [\"//visibility:public\"])\r\nload(\"@org_tensorflow//tensorflow:tensorflow.bzl\", \"tf_copts\")\r\ntf_copts()\r\n```\r\n\r\nWhen I try build this project with bazel I have this error:\r\n\r\n`bazel build -c opt //my_project --crosstool_top=//external:android/crosstool  --cpu=armeabi-v7a  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain`\r\n\r\n`ERROR: error loading package 'my_project': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': error loading package 'external': The repository named 'local_config_cuda' could not be resolved.`\r\n\r\nNB: I'm successfully building the TensorFlow Android Camera Demo `tensorflow/tenssorflow/examples/android/`\r\n\r\nRelated Issue: [#2775](https://github.com/tensorflow/tensorflow/issues/2775)", "comments": ["@petewarden I can't tell if this is just a generic bazel support question (which should have been on StackOverflow), or something specific to the Android platform.  Could you please take a look?  ", "I'm not sure on this one, it looks like a more generic Bazel question rather than something specific to Android?", "Hi,\r\n  I am having a similar issue when trying to run the retraining test (following https://www.tensorflow.org/tutorials/image_retraining):\r\n\r\nERROR: error loading package 'tensorflow/examples/image_retraining': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': error loading package 'external': The repository named 'local_config_cuda' could not be resolved.\r\n\r\nHave you been able to solve this issue, and how? \r\n\r\nI'm using cuda 8.0, bazel 0.4.4, python 3.5.3 (also tried with 2.7).\r\n\r\nThanks a lot,\r\nBest,\r\nNicolas", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 5804, "title": "ffmpeg.decode_audio cannot be run in parallel", "body": "To decode the input, the function writes the content to a temporary file. Its name is generated by the function `GetTempFilename` found in `tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc`. The template for the filename is `%tmp_dir/tmp_file_%PID.%EXT`.\r\n\r\n**When using multiple decoders in parallel this causes an undetermined behaviour since all decoders want to write and afterwards delete the same file.**\r\n\r\nA possible solution would be to use the thread id instead of the process id. I.e.\r\n\r\n```\r\n#include <sys/syscall.h>\r\n#define gettid() syscall(SYS_gettid)\r\n...\r\nreturn io::JoinPath(dir, StrCat(\"tmp_file_\", gettid(), \".\", extension));\r\n```\r\nThe first two lines are necessary because [glibc does not wrap the call](http://man7.org/linux/man-pages/man2/gettid.2.html).\r\n\r\nThis solution works for me (on Linux). I'm, however, not sure if it works on all supported platforms. If that's fine, I can make a pull request.", "comments": ["@fredbertsch You are listed as owner for this contrib dir... could you please take a look?", "I also have this problem. Will you fix this?", "@fredbertsch Any updates here?\r\nIs this still a problem?", "Just keeping folks in the loop, we have a candidate fix internally that @fredbertsch authored. Hopefully it will show up in master over the next day or two!", "A test and fix were added internally, and they should propagate here soon.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 5803, "title": "The parameters are not updated using multi-gpu training. ", "body": "from __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n##------ Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n\r\n##------- Parameters\r\nlearning_rate = 0.001\r\ntraining_iters = 100000\r\nbatch_size = 32\r\ndisplay_step = 10\r\nnum_gpus = 2\r\n\r\n##--------- Network Parameters\r\nn_input = 784 # MNIST data input (img shape: 28*28)\r\nn_classes = 10 # MNIST total classes (0-9 digits)\r\ndropout = 0.75 # Dropout, probability to keep units\r\n\r\n\r\n##------- Create some wrappers for simplicity\r\n\r\ndef conv2d(x, W, b, strides=1):\r\n    # Conv2D wrapper, with bias and relu activation\r\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\r\n    x = tf.nn.bias_add(x, b)\r\n    return tf.nn.relu(x)\r\n\r\n\r\ndef maxpool2d(x, k=2):\r\n    # MaxPool2D wrapper\r\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\r\n                          padding='SAME')\r\n\r\n\r\ndef average_gradients(tower_grads):\r\n\r\n    average_grads = []\r\n    for grad_and_vars in zip(*tower_grads):\r\n    # Note that each grad_and_vars looks like the following:\r\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n        grads = []\r\n        for g, _ in grad_and_vars:\r\n      # Add 0 dimension to the gradients to represent the tower.\r\n            expanded_g = tf.expand_dims(g, 0)\r\n\r\n      # Append on a 'tower' dimension which we will average over below.\r\n            grads.append(expanded_g)\r\n\r\n    # Average over the 'tower' dimension.\r\n            grad = tf.concat(0, grads)\r\n            grad = tf.reduce_mean(grad, 0)\r\n\r\n    # Keep in mind that the Variables are redundant because they are shared\r\n    # across towers. So .. we will just return the first tower's pointer to\r\n    # the Variable.\r\n    v = grad_and_vars[0][1]\r\n    grad_and_var = (grad, v)\r\n    average_grads.append(grad_and_var)\r\n    return average_grads\r\n\r\n\r\n\r\n##--------   Create model\r\ndef conv_net(images, dropout):\r\n    # Reshape input picture\r\n    images = tf.reshape(images, shape=[-1, 28, 28, 1])\r\n\r\n    # Convolution Layer\r\n    with tf.variable_scope('conv1'):\r\n        W = tf.get_variable('weights', [5, 5, 1, 32], initializer=tf.random_normal_initializer())\r\n        b = tf.get_variable(\"biases\", [32], initializer=tf.random_normal_initializer())\r\n        conv1 = conv2d(images, W, b)\r\n        # Max Pooling (down-sampling)\r\n        conv1 = maxpool2d(conv1, k=2)\r\n\r\n    # Convolution Layer\r\n    with tf.variable_scope('conv2'):\r\n        W = tf.get_variable('weights', [5, 5, 32, 64], initializer=tf.random_normal_initializer())\r\n        b = tf.get_variable(\"biases\", [64], initializer=tf.random_normal_initializer())\r\n    # Max Pooling (down-sampling)\r\n        conv2 = conv2d(conv1, W, b)\r\n        conv2 = maxpool2d(conv2, k=2)\r\n\r\n    # Fully connected layer\r\n    # Reshape conv2 output to fit fully connected layer input\r\n    fc1 = tf.reshape(conv2, [-1, 7*7*64])\r\n\r\n    with tf.variable_scope('fully'):\r\n        weights = tf.get_variable('weights', [7*7*64, 1024], initializer=tf.random_normal_initializer())\r\n        bias = tf.get_variable('bias', [1024], initializer=tf.random_normal_initializer())\r\n        fc1 = tf.add(tf.matmul(fc1, weights), bias)\r\n        fc1 = tf.nn.relu(fc1)\r\n    # Apply Dropout\r\n        fc1 = tf.nn.dropout(fc1, dropout)\r\n\r\n    # Output, class prediction\r\n\r\n    with tf.variable_scope('softmax'):\r\n        weights = tf.get_variable('weights', [1024, 10], initializer=tf.random_normal_initializer())\r\n        bias = tf.get_variable('bias', [10], initializer=tf.random_normal_initializer())     \r\n        out = tf.add(tf.matmul(fc1, weights), bias)\r\n\r\n    return out\r\n\r\n##----------    multi-GPU\r\n\r\ndef train():\r\n\r\n    with tf.Graph().as_default():\r\n\r\n        x = tf.placeholder(tf.float32, [batch_size*num_gpus, 784])\r\n        y = tf.placeholder(tf.float32, [batch_size*num_gpus, n_classes])\r\n        keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\r\n\r\n        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\r\n\r\n\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n        tower_grads = []\r\n        for i in xrange(num_gpus):\r\n            with tf.device('/gpu:%d' % i):\r\n                with tf.name_scope('%s_%d' % ('MNIST', i)) as scope:\r\n          # Calculate the loss for one tower of the CIFAR model. This function\r\n          # constructs the entire CIFAR model but shares the variables across\r\n          # all towers\r\n          # Reuse variables for the next tower.\r\n\r\n                    next_batch = x[i*batch_size:(i+1)*batch_size, :]\r\n                    next_label_batch = y[i*batch_size:(i+1)*batch_size, :]\r\n                    pred = conv_net(next_batch, keep_prob)\r\n                    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, next_label_batch))\r\n                    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(next_label_batch, 1))\r\n                    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n                    tf.get_variable_scope().reuse_variables()\r\n\r\n          # Calculate the gradients for the batch of data on this CIFAR tower.\r\n                    grads = optimizer.compute_gradients(cost)\r\n          # Keep track of the gradients across all towers.\r\n                    tower_grads.append(grads)\r\n\r\n\r\n\r\n        grads = average_gradients(tower_grads)\r\n        apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\r\n\r\n\r\n##--------------------start training----------------------\r\n\r\n##---------- Initializing the variables\r\n        init = tf.initialize_all_variables()\r\n\r\n##----------- Launch the graph\r\n        with tf.Session() as sess:\r\n\r\n            sess = tf.Session(config=tf.ConfigProto(\r\n            allow_soft_placement=True,\r\n            log_device_placement=True))\r\n            sess.run(init)\r\n            saver = tf.train.Saver()\r\n            var = [v for v in tf.trainable_variables()][0]\r\n            aa = sess.run(var)\r\n            np.save('initial.npy', aa)\r\n\r\n            step = 1\r\n    # Keep training until reach max iterations\r\n            while step * (num_gpus*batch_size) < training_iters:\r\n\r\n                large_input = np.zeros((num_gpus*batch_size, n_input))\r\n                large_labels = np.zeros((num_gpus*batch_size, n_classes))\r\n        \r\n                for index in xrange(num_gpus):\r\n                    batch_x, batch_y = mnist.train.next_batch(batch_size)\r\n                    large_input[index*batch_size:(index+1)*batch_size, :] = batch_x\r\n                    large_labels[index*batch_size:(index+1)*batch_size, :] = batch_y\r\n\r\n        # Run optimization op (backprop)\r\n                sess.run(apply_gradient_op, feed_dict={x: large_input, y: large_labels,\r\n                                       keep_prob: dropout})\r\n                if step % display_step == 0:\r\n            # Calculate batch loss and accuracy\r\n                    loss, acc = sess.run([cost, accuracy], feed_dict={x: large_input,\r\n                                                              y: large_labels,\r\n                                                              keep_prob: 1.})\r\n                    print(\"Iter \" + str(step*num_gpus*batch_size) + \", Minibatch Loss= \" + \\\r\n                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\r\n                  \"{:.5f}\".format(acc))\r\n                step += 1\r\n            print(\"Optimization Finished!\")\r\n            var = [v for v in tf.trainable_variables()][0]\r\n            aa = sess.run(var)\r\n            np.save('final.npy', aa)\r\n\r\n##-----------main---\r\nif __name__ == '__main__':\r\n    train()\r\n\r\n\r\nI'm a beginner of Tensorflow and wrote a piece of code to test on the MNIST dataset. I find that during training my parameters doesn't change at all and cannot find the problem, so SOS!!!\r\n", "comments": ["up", "From https://www.tensorflow.org/versions/r0.10/resources/index.html:\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n"]}, {"number": 5802, "title": "A theano.clone() equivalent for Tensorflow", "body": "This an implementation of a `clone()` function for Tensorflow based on `meta_graph` utilities, inspired by discussion with @sherrym in #5477 .\r\n\r\nFeature requests on this can be found in #1070  #5479 . Previous implementations of copying graphs #557 does copy naively and doesn't have `replace` feature as in [theano.clone()](http://deeplearning.net/software/theano/library/#theano.clone).\r\n\r\nThough this implementation is almost good. It still has problems for now with control flow copies. To be specific, I'm not sure how to deal with collections of `cond_context` and `while_context`, because both of its `to_proto` functions only consider the case of stripping `export_scope`.\r\n\r\nNot copying `cond_context` collection to new operations (to their `_control_flow_context`) will cause problems described in #5477 , also demonstrated in the last test (`testCloneBatchNorm`) in this pull request, which is failed for now. I'm opening this PR early before that test pass, wondering if someone can have advice on dealing with control flow collections when cloning subgraphs. Thanks.", "comments": ["Can one of the admins verify this patch?", "@thjashin, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @ebrevdo to be potential reviewers.", "Anyone help with this PR? @sherrym , @keveman , @vrv , @ebrevdo , especially about the control flow context part.", "@yuanbyu may know about control flow parts", "Ping all! @sherrym, @yuanbyu can you look at this?", "@yaroslavvb @martinwicke Thanks for help. I'm always willing to keep working on this feature.", "How does this work related to the graph view and replace operations in contrib?  Can those tools be used to perform cloning?", "@ebrevdo Thanks for the question. **The short answer** is:\r\nThey cannot do exactly the same thing due to bugs, but to fix bugs touches the design. While using meta_graph, from my view, is the most elegant way to implement and maintain this feature.\r\n\r\n**Long answer**:\r\nI used to rely on `tf.contrib.graph_editor`. It seems to have everything. But I have to say it's still buggy, and maintenance seems to have stopped for some time. The `graph_replace` function provided in `tf.contrib.graph_editor` tries to do similar things with this PR, however, there are big cases it fails, for example, when one node in the given replacements is a descendant of another, the copied subgraph won't cover the uprooting graph of the descendant node when there are target nodes between this two nodes. I tried to fix but it turned out things were not easy under the SubgraphView framework. This is not the only problem, you could see that I had a PR ( #5451 ) merged on problems about control inputs. It's still an easy one to fix, but there exist much harder ones.\r\n\r\nThough this owes much to lack of extensive tests in `tf.contrib` modules, I'm still wondering whether `tf.contrib.graph_editor` is the right way to achieve this clone() ability. Then I found the `_control_flow_context` problem, which I raised in #5477 . This is because copying operations in `tf.contrib.graph_editor` is by directly copying members of a python objects. In that issue @sherrym pointed out to me that I could use `meta_graph` utilities to achieve what I want. So I made this trial. Though I still don't succeed in copying control flow contexts, I find the code more elegant in this way, compared to implementations in `tf.contrib.graph_editor`.\r\n\r\nI believe this is at least a right direction to implement this theano equilvalent. And I also want to point out that **this function is of great value to people building higher level libraries based on Tensorflow, it helps on operation level reuse (which cannot be achieved either by using name scope or by experimental function utilities in TF)**.", "@sherrym  ping. Can you comment if this is feasible to get merged?\r\n\r\n@thjashin note that getting added to contrib means it's a candidate to be moved into core, which means that core TensorFlow team would have to take over support and maintenance. Hence, the code has to be easy enough to transfer over (ie, design docs can help sometimes)", "@thjashin could you merge and push again?", "@yaroslavvb Thanks. Any templates for design docs?", "@drpngx Sure, I'll do it tomorrow.", "@thjashin I was thinking more of an informal thing that'll help someone understand for someone reading the code for the first time (ie, motivation, brief outline of how code is structured and why)", "@drpngx Done.", "Jenkins, test this please.", "We might have to move that to contrib first.", "@drpngx I think I've modified things in the framework, not just adding other functions...Can you give some advice on how to move them into contrib? btw, I guess if you can get some team member to review, can we directly merge? I guess I don't change the original behavior of other parts of code. Also if you need design docs, I'm very happy to write one. In fact it's just a DFS in the graph with some terminal conditions.", "@sheerym or @yuanbyu are the best to review this.", "@sherrym sorry, typo.", "@thjashin I'll try pinging sherrym and yuanbyu directly to review this. In the mean time, can you fix the merge conflict that's appeared?", "@dandelionmane Sure. I will do this tomorrow.", "cc @purpledog ", "@dandelionmane I've done the merge.", "Jenkins, test this please.", "@yuanbyu @sherrym Could you take a look? I'm not super familiar with this part of the code.", "I am not very familiar with this part of the codebase. For the control flow context, it seems to strip name scope when exporting and to add back the \"current\" name scope when importing. Why doesn't it work in your case?", "I think for now, given that we don't really have any volunteer maintainers of this code internally, we're going to close this PR;  I think the current implementation of GraphDef and state in TensorFlow's python implementation is not very amenable to analysis and correctness, and while what you've done is very neat, it will probably make it very difficult or impossible to move the existing complex python code (control flow, state management) to better implementations (not a proof, but just a gut feeling).\r\n\r\nI believe some of the work on improving functions and state management will make something equivalent to clone() possible and maintainable, and that's what we really need to make sure we do if we intend to make our users happy.", "+1 I think for now we need to reserve the right to make changes to this\npart of the tf codebase without maintaining a .clone implementation,\nespecially given the lack of internal maintainers.\n\nOn May 5, 2017 5:45 PM, \"Vijay Vasudevan\" <notifications@github.com> wrote:\n\n> I think for now, given that we don't really have any volunteer maintainers\n> of this code internally, we're going to close this PR; I think the current\n> implementation of GraphDef and state in TensorFlow's python implementation\n> is not very amenable to analysis and correctness, and while what you've\n> done is very neat, it will probably make it very difficult or impossible to\n> move the existing complex python code (control flow, state management) to\n> better implementations (not a proof, but just a gut feeling).\n>\n> I believe some of the work on improving functions and state management\n> will make something equivalent to clone() possible and maintainable, and\n> that's what we really need to make sure we do if we intend to make our\n> users happy.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/5802#issuecomment-299604392>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4hXvJfm5wuLY7Kd6F2SbPmX1GAXks5r28LFgaJpZM4K6UfI>\n> .\n>\n"]}, {"number": 5801, "title": "Hello. How can I use pre-train tensorflow's weights in my cnn library? Can you explain how conv layers work? How it use weights and input channels to get result?", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html: \r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n"]}, {"number": 5800, "title": "Fix file system regex pattern in tensorboard server", "body": "Minor fix for the uri_pattern", "comments": ["Can one of the admins verify this patch?", "@llhe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @dsmilkov and @danmane to be potential reviewers.", "@tensorflow-jenkins test this please", "PR merged. Thanks, @llhe !"]}, {"number": 5799, "title": "nvcc fatal   : Unknown option '-expt-relaxed-constexpr' while bazel build", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.4\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCUDA v7.0\r\ncuDNN 4\r\n\r\n    -rw-r--r-- 1 root root   310328  3\uc6d4  6  2015 /usr/local/cuda/lib64/libcudadevrt.a\r\n    lrwxrwxrwx 1 root root       16  3\uc6d4  6  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.0\r\n    lrwxrwxrwx 1 root root       19  3\uc6d4  6  2015 /usr/local/cuda/lib64/libcudart.so.7.0 -> libcudart.so.7.0.28\r\n    -rwxr-xr-x 1 root root   377896  3\uc6d4  6  2015 /usr/local/cuda/lib64/libcudart.so.7.0.28\r\n    -rw-r--r-- 1 root root   708938  3\uc6d4  6  2015 /usr/local/cuda/lib64/libcudart_static.a\r\n    -rwxr-xr-x 1 root root 61453024  6\uc6d4  9 18:46 /usr/local/cuda/lib64/libcudnn.so\r\n    -rwxr-xr-x 1 root root 61453024  6\uc6d4  9 18:46 /usr/local/cuda/lib64/libcudnn.so.4\r\n    -rwxr-xr-x 1 root root 61453024  6\uc6d4  9 18:46 /usr/local/cuda/lib64/libcudnn.so.4.0.7\r\n    -rwxr-xr-x 1 root root 48217000 11\uc6d4 24  2015 /usr/local/cuda/lib64/libcudnn.so.7.0\r\n    -rwxr-xr-x 1 root root 48217000 11\uc6d4 24  2015 /usr/local/cuda/lib64/libcudnn.so.7.0.64\r\n    -rw-r--r-- 1 root root 62025862  6\uc6d4  9 18:46 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n55dbc54192a378c5e685c52595f42503f037320e\r\n\r\n2. The output of `bazel version`\r\n\r\n    Build label: 0.3.2\r\n    Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI'm trying to install from source and faces following errors while build. I'm not sure what is the problem. Is this because of CUDA version?\r\n\r\n    bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n    \r\n    ...\r\n    ERROR: /home/jinhyung/tensorflow/tensorflow/core/kernels/BUILD:1128:1: error while parsing .d file: /home/jinhyung/.cache/bazel/_bazel_jinhyung/24f731f36d8cecf427d437d0326fcc3c/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/resize_nearest_neighbor_op_gpu/tensorflow/core/kernels/resize_nearest_neighbor_op_gpu.cu.pic.d (No such file or directory).\r\n    nvcc fatal   : Unknown option '-expt-relaxed-constexpr'\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build", "comments": ["Your bazel version does not appear to be very recent..\r\nCould you try installing a recent build as per https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#installing-from-sources ?\r\n", "Thanks for the answer. However, it shows same error after I changed the bazel to the latest version. I did again from configure step.", "I was able to fix this by reverting #5256. Turns out this flag isn't compatible with CUDA 7.0.", "@gunan I believe we still support CUDA 7.0?  Could you please take a look at this.", "I reverted it the same day in\nhttps://github.com/vrv/tensorflow/commit/14bc025ffb79a224d027ca29ca5a8ab128ff6646\n\n\nOn Thu, Nov 24, 2016 at 9:03 AM, Paul Barham <notifications@github.com>\nwrote:\n\n> @gunan <https://github.com/gunan> I believe we still support CUDA 7.0?\n> Could you please take a look at this.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5799#issuecomment-262818530>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAcTeSE734k0JLHKeN7IaXLql-OjUlCNks5rBcNQgaJpZM4K6Maj>\n> .\n>\n", "@vrv Thanks Vijay!  Hadn't noticed that.\r\n@kkjh0723  Could you please sync to head and confirm that the problem is already fixed?", "Interesting, I thought I merged that, but it never went through in master.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L424 :(\r\n\r\n", "https://github.com/tensorflow/tensorflow/commit/14bc025ffb79a224d027ca29ca5a8ab128ff6646 So I did merge it, and then\r\nhttps://github.com/tensorflow/tensorflow/commit/584d4a193c41918d33d0d43c0d13b055dfb5b254 added it back somehow.  Bleh :(", "argh, cant figure out git, I guess it was a bad merge?"]}, {"number": 5798, "title": "make cuDNN linux instructions copy-pastable", "body": "previously, we were trying to cp files over directories", "comments": ["Can one of the admins verify this patch?", "@cwlbraa, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 5797, "title": "Release notes for 0.12.0-rc0.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @asimshankar and @jhseu to be potential reviewers.", "The failures, do not make sense.\r\nAll latest tests on the branch have been all passing.\r\nJenkins, test this please."]}, {"number": 5796, "title": "Change Optimizer while training, after a certain number of steps?", "body": "I am working on machine translation, using seq2seq model in Tensorflow. I am aware that once the graph has been established, it cannot be modified during training.\r\nWhat if I want to change the optimizer from SGD to Adam after certain global steps?\r\nHow should the code be?\r\nThanks a lot.", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html:\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n", "I am also looking for the answer to this question. I am trying to change the var_list provided to the minimize() function after some iterations. I am trying to implement a two step finetuning, where for first 50 iterations, I will be training the last layer and after that finetuning the whole network."]}, {"number": 5795, "title": "[CMake] Add examples/tutorials/mnist to Python package.", "body": "Tutorial scripts in examples/tutorials/mnist rely on some classes to be present in the Python package, so added this directory to CMake. Also, fixed code in contrib.learn that caused the MNIST tutorials to fail on Windows.", "comments": ["Can one of the admins verify this patch?", "@vit-stepanovs, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @mrry and @danmane to be potential reviewers.", "@tensorflow-jenkins test this please."]}, {"number": 5794, "title": "Make windows gpu build project name tensorflow_gpu.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @jhseu and @mrry to be potential reviewers.", "Could you also change the script for windows gpu build?\r\nwe only uninstall (pip uninstall) tensorflow at the moment. To avoid weird behaviour, we should uninstall both tensorflow and tensorflow_gpu\r\n", "Also, we need to modify windows GPU installation instructions.", "ok, let me do that"]}, {"number": 5793, "title": "[CMake] Build Python protos after core protos", "body": "Ensures that the generated code for the core protos has been generated before compiling the generated code for the Python-specific protos.", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @danmane and @martinwicke to be potential reviewers."]}, {"number": 5792, "title": "I am having a problem when adding new op in tensorflow", "body": "I just followed the tutorial when trying to add a new op in tensorflow\r\nI would like to add a new op with which the file name is: rec_conv.cc\r\nThe code is like:(Input an tensor and just output a string)\r\n```\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\nusing namespace tensorflow;\r\n\t\r\nREGISTER_OP(\"RecConv\")\r\n    .Input(\"input: int32\")\r\n    .Output(\"output: string\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      c->set_output(0, c->input(0));\r\n      return Status::OK();\r\n    });\r\n\r\nclass RecConvOp: public OpKernel{\r\n public:\r\n\texplicit RecConvOp(OpKernelConstruction* context): OpKernel(context){}\r\n\r\n void Compute(OpKernelContext* context) override {\r\n\t//const Tensor& input_tensor = context->input(0);\r\n\tTensor* output_tensor = NULL;\r\n\tauto output = output_tensor->template scalar<string>();\r\n\toutput() = \"Output is Rec_conv layer\";\r\n }\r\n}\r\nREGISTER_KERNEL_BUILDER(Name(\"RecConv\").Device(DEVICE_CPU), RecConvOp);\r\n```\r\n-------------------------------------------------\r\nThe BUILD file is like:\r\n```\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\r\n\r\ntf_custom_op_library(\r\n    name = \"rec_conv.so\",\r\n    srcs = [\"rec_conv.cc\"],\r\n)\r\n```\r\n---------------------------------------------\r\nHowever, when I run:\r\n bazel build -c opt //tensorflow/core/user_ops:rec_conv.so\r\nError occure like:\r\n```\r\nServer finished RPC without an explicit exit code\r\n\r\nlixinyu@lixinyu-PC:~/tensorflow/tensorflow/tensorflow/core/user_ops$ bazel build -c opt //tensorflow/core/user_ops:rec_conv.so\r\n...........\r\nINFO: Found 1 target...\r\nERROR: /home/lixinyu/tensorflow/tensorflow/tensorflow/core/user_ops/BUILD:3:1: C++ compilation of rule '//tensorflow/core/user_ops:rec_conv.so' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-canonical-system-headers ... (remaining 57 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/core/user_ops/rec_conv.cc:3:0:\r\n./tensorflow/core/framework/op_kernel.h:1097:7: error: expected initializer before 'registrar__body__1__object'\r\n       registrar__body__##ctr##__object(                                 \\\r\n       ^\r\n./tensorflow/core/framework/op_kernel.h:1093:3: note: in expansion of macro 'REGISTER_KERNEL_BUILDER_UNIQ'\r\n   REGISTER_KERNEL_BUILDER_UNIQ(ctr, kernel_builder, __VA_ARGS__)\r\n   ^\r\n./tensorflow/core/framework/op_kernel.h:1090:3: note: in expansion of macro 'REGISTER_KERNEL_BUILDER_UNIQ_HELPER'\r\n   REGISTER_KERNEL_BUILDER_UNIQ_HELPER(__COUNTER__, kernel_builder, __VA_ARGS__)\r\n   ^\r\ntensorflow/core/user_ops/rec_conv.cc:32:1: note: in expansion of macro 'REGISTER_KERNEL_BUILDER'\r\n REGISTER_KERNEL_BUILDER(Name(\"RecConv\").Device(DEVICE_CPU), RecConvOp);\r\n ^\r\n./tensorflow/core/framework/op_kernel.h:1104:30: error: expected unqualified-id before ')' token\r\n                             });\r\n                              ^\r\n./tensorflow/core/framework/op_kernel.h:1093:3: note: in expansion of macro 'REGISTER_KERNEL_BUILDER_UNIQ'\r\n   REGISTER_KERNEL_BUILDER_UNIQ(ctr, kernel_builder, __VA_ARGS__)\r\n   ^\r\n./tensorflow/core/framework/op_kernel.h:1090:3: note: in expansion of macro 'REGISTER_KERNEL_BUILDER_UNIQ_HELPER'\r\n   REGISTER_KERNEL_BUILDER_UNIQ_HELPER(__COUNTER__, kernel_builder, __VA_ARGS__)\r\n   ^\r\ntensorflow/core/user_ops/rec_conv.cc:32:1: note: in expansion of macro 'REGISTER_KERNEL_BUILDER'\r\n REGISTER_KERNEL_BUILDER(Name(\"RecConv\").Device(DEVICE_CPU), RecConvOp);\r\n ^\r\nTarget //tensorflow/core/user_ops:rec_conv.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 11.505s, Critical Path: 2.99s\r\nlixinyu@lixinyu-PC:~/tensorflow/tensorflow/tensorflow/core/user_ops$ bazel build -c opt //tensorflow/core/user_ops:rec_conv.so\r\n```\r\n-----------------------------------------------------\r\nCould anyone tell me what type of error it is like and how to debug it cuz I am wondering why there are so many warnings and error promped from the included header file\r\n\r\nThanks in advance\r\n", "comments": ["Are you missing a semicolon after your class definition?", "Oh, god it is.\r\nOne more question, I did sucessfully build the op but when I am trying to use it in python,\r\nit shows nothing to reference.\r\nlike:Traceback (most recent call last):\r\n  File \"new_op_test.py\", line 3, in <module>\r\n    zero_out_module = tf.load_op_library('zero_out.so')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: zero_out.so: cannot open shared object file: No such file or directory\r\nAm I missing anymore step?\r\n", "@prb12 ", "@lixinyutfd You will need to make sure that the shared library is on your `LD_LIBRARY_PATH` \r\nPlease let me know if this fixes the problem so I can close this issue.", "@prb12 \r\nThanks in advance;\r\nI did resolve the problem by passing absolute path of zero_out.so to  tf.load_library()\r\nI am not sure if it is the same problem wrt the LD_LIBRARY_PATH but if so I hope there will be a solution to solve it cuz it is a pain to write such length path in it.\r\nOr I am so sorry that If your solution helps, I didnt quite get it when and how to set LD_LIBRARY_PATH for my self-defined library. ", "There is no standard mechanism for this.\r\n\r\nTypically you would either (manually) install your custom ops library in a well-defined place and put that directory on your LD_LIBRARY_PATH, or add `.` to your path if you want to live dangerously.\r\n\r\nI normally add the bazel-bin directory where I built the ops library to my path, \r\ne.g `export LD_LIBRARY_PATH=/path/to/tensorflow-tree/bazel-bin/tensorflow/core/user_ops`\r\n\r\nI'm closing this issue now since the reported problem was just a typo.", "@lixinyutfd : Hey Do you know how to add a header file as a dependency while compiling your new op?\r\nI am also creating a new op and I have a similar build file as yours:\r\n```\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\r\n\r\ntf_custom_op_library(\r\n    name = \"rec_conv.so\",\r\n    srcs = [\"rec_conv.cc\"],\r\n)\r\n```\r\n\r\nBut I also have a \"rec_conv.h\" file which I am unable to include successfully in the build and Bazel is complaining. \r\nIn case you have worked this out, can you help?", "Add .h files in srcs. If you have gpu_srcs and the same .h file as srcs, you'll have to add it again in gpu_srcs\r\ntf_custom_op_library(\r\n    name = \"rec_conv.so\",\r\n    srcs = [\"rec_conv.cc, rec_conv.h\"],\r\n)\r\n\r\n\r\n\r\n\r\n> @lixinyutfd : Hey Do you know how to add a header file as a dependency while compiling your new op?\r\n> I am also creating a new op and I have a similar build file as yours:\r\n> \r\n> ```\r\n> load(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\r\n> \r\n> tf_custom_op_library(\r\n>     name = \"rec_conv.so\",\r\n>     srcs = [\"rec_conv.cc\"],\r\n> )\r\n> ```\r\n> \r\n> But I also have a \"rec_conv.h\" file which I am unable to include successfully in the build and Bazel is complaining.\r\n> In case you have worked this out, can you help?\r\n"]}, {"number": 5791, "title": "Only run one GPU test at a time on windows.", "body": "", "comments": []}, {"number": 5790, "title": "Feature proposal: bilinear upsampling transposed convolution initialization.", "body": "Hello,\r\n\r\nI have been recently [implementing the FCN-32 network for image segmentation](http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/)\r\nand found out that there is no bilinear upsampling op in Tensorflow or TF-slim.\r\nThat would be great if there will be an op that will do that, because that will make it\r\npossible to bilinearly upsample blobs in a differentiable way or initialize upsampling\r\nfilter with bilinear kernel.\r\n\r\nI have already implemented that and checked correctness.\r\nYou can see more if you follow the link. \r\nIf you are interested, I can make a pull request.\r\n\r\nThank you. ", "comments": ["Feel free to submit a PR.  ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5789, "title": "Improve CUDA peer to peer access to support Amazon P2 instances", "body": "If you try to run Tensorflow on a machine that has more than 8 GPU you will receive an error or Warning saying: CUDA_ERROR_TOO_MANY_PEERS.\r\n\r\nFrom the Nvidia forums seems that this is documented behavior:\r\n\r\nhttp://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#peer-to-peer-memory-access\r\n\r\n> Peer-to-peer memory access must be enabled between two devices by calling cudaDeviceEnablePeerAccess() as illustrated in the following code sample. Each device can support a system-wide maximum of eight peer connections.\r\n\r\nTF is doing a full NxN peer access map, each 16x16 and that would explain the error on 16 gpus machines.\r\n\r\nThe challenge is figuring out which GPUs should peer with each other. \r\n We do the full NxN right now since we don't yet have a better answer about the physical topology of the devices.  (E.g., how do you know that the first 8 are all physically the first die, and the second 8 are all physically the second die?)  If such an API exists and we can query it reliably, that might be a better solution.\r\n\r\nAll the code is in one file: gpu_device.cc\r\n\r\nrelated Issues: \r\n- https://github.com/tensorflow/tensorflow/issues/5362", "comments": ["@Mistobaan \r\n> first 8 are all physically the first die, and the second 8 are all physically the second die\r\n\r\nI'm guessing you meant bus here, not die?\r\n\r\n@poxvoculi I'm pretty sure that our code handles 16 GPUs?  (8 x K80 on two PCI buses).  Could you take a look at this please?", "We are able to run TF on machines that have 8 k80 cards, which appear as 16 GPUs, 8 GPUs (4 cards) on each of two separate PCIe buses (each bus connected to one CPU socket).  In this configuration cudaDeviceCanAccessPeer returns false for GPUs on different buses, so cudaDeviceEnablePeerAccess gets called only for GPUs on the same bus.  I'm guessing that your system architecture is different, so that cudaDeviceCanAccessPeer returns true for all pairs among 16 GPUs.  In that case you're going to need to restrict visibility of the GPUs within a process to 8, as discussed in #5362.  ", "@poxvoculi \r\n\r\nThe error was discussed on the [mailing list](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/Hua1KbqqLWU)\r\n and it occurs on the new AWS p2.16xlarge\tinstances.\r\n\r\nLooking at the code then there is something strange because I suspect that in [gpu_device.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L766)\r\n\r\n`if (from->CanEnablePeerAccessTo(to)) {` returns `true` and then \r\n`auto status = from->EnablePeerAccessTo(to);` returns `false`.\r\n\r\nand that was throwing errors [before this change](https://github.com/tensorflow/tensorflow/commit/cfccd7ce1b9092eec98bb0989eb55a11e9d2b894) \r\n", "@poxvoculi Yup, we know the driver is only returning the potential peerings, not the optimal peering.\r\n\r\nThe point of opening up this bug is to eventually figure out a better algorithm for enabling peer access.  I'll mark this as contributions welcome for some enterprising developer to figure out a nice, general solution to this.", "It seems difficult to have a nice, fully-automatic solution.  The consequence of enabling peer access is slightly faster inter-GPU memory copies, with less CPU memory interface contention.  If you're using 16 GPUs that can all feasibly be peered to each other, but only 8 can be with respect to any one, the choice of which ones are most useful to peer depends very much on how your model is structured, and maybe secondarily on the underlying system topology.   So, a useful contribution might be some kind of startup option that allows explicitly specifying which GPUs to peer, overriding the default behavior of trying to make every feasible peering.", "Renamed title to reflect that this affects Amazon P2 instances specifically.", "Closing.  NCCL is the solution NVIDIA created to help navigate this issue but it is not always the right choice.  I did not do a lot of testing on the p2.16x large but in testing on a variety of sytems 4x Titan X, 8x k80 on GCE, 8x K80 on AWS and 8x P100 on DGX-1 I found the approach for how to deal with variable update ranges and is also different based on the model.  The results are [here](https://www.tensorflow.org/performance/benchmarks), which also contains links to details about different approaches of variable management.  At this point the p2.16xlarge is not as desirable as 8x P100s, which work well with NCCL or just CPU as the variable CPU.  ", "I found `successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero` in the log posted https://github.com/tensorflow/tensorflow/issues/5362#issuecomment-258298504 by @alexatknit, which is opposed to what AWS has been said [here](https://aws.amazon.com/ec2/instance-types/p2/):\r\n\r\n> P2 instances also offer GPUDirect\u2122 (peer-to-peer GPU communication) capabilities for up to 16 GPUs, so that multiple GPUs can work together within a single host.\r\n\r\nI do not have access to P2 instances at hand; do we still have this problem with the latest TF?", "According to the QEMU-devel mailing list archive [here](https://lists.gnu.org/archive/html/qemu-devel/2017-08/pdfUda5iEpgOS.pdf), the support of GPUDirect P2P in VM requires coordinated support of PEER_CLIQUE_ID from both hypervisor and Nvidia driver, and based on the date of the [patch](https://lists.gnu.org/archive/html/qemu-devel/2017-08/msg05826.html) it seems that it is yet to be merged in QEMU. \r\n\r\n@benbarsdell may I know if this feature is supported in current CUDA driver release?", "I test on these systems once a month and the peering works fine.  The messages is unfortunately misleading.  I have a test that runs VGG16 with NCCL and it would not perform well if the sync was not GPU to GPU.  ", "@tfboyd Thanks for the heads up! \n\nI'm actually quite interested in the 16 GPUs setup on AWS. All to all peering seems to be unlikely as those 16 GPUs are not connected directly through a single PCIe switch (according to Mu Li's PhD thesis). \n\nAny reason why we don't have single node 16 GPUs benchmark result included in the TF CNN benchmarks page?", "Yeah, I realized I missed the mark with my comment.  16 gpus in one server\nis rare and at this point newer hardware makes the setup less interesting.\nI thought about it when I tested but I just did not find it useful as it is\nmore of an oddity.  P100s and V100s are very unlikely to ever be configured\nlike that and I do not think nvlink supports more than 8.  I end up wrong\nalot but when I am wrong I will benchmark it.  :-).\n\nOn Sep 21, 2017 5:17 PM, \"Bairen Yi\" <notifications@github.com> wrote:\n\n> @tfboyd <https://github.com/tfboyd> Thanks for the heads up!\n>\n> I'm actually quite interested in the 16 GPUs setup on AWS. All to all\n> peering seems to be unlikely as those 16 GPUs are not connected directly\n> through a single PCIe switch (according to Mu Li's PhD thesis).\n>\n> Any reason why we don't have single node 16 GPUs benchmark result included\n> in the TF CNN benchmarks page?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5789#issuecomment-331315735>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesp6p_wUYrWXWBrorsdu2CK7Ygivcks5skvy1gaJpZM4K54NB>\n> .\n>\n", "I have not aware of any virtualization technique that exposes NVLink to the hypervisor, and if the current KVM/Xen doesn't support NVLink up to the same level of their support to PCIe, I doubt if it's  possible for us to use native NVLink interconnect on public cloud.", "@tfboyd Even with the latest P100 and V100, the problem still remains. If you take a look of the DGX-1 system bus topology (hypercube alike), it is unlikely that all-to-all peering could be achieved with uniform latency/bandwidth provision. \r\n\r\nNCCL claims to handle this case optimally, but when I tested vgg16 on my local box with 4 K40m GPUs in a PCI-passthrough VM:\r\n\r\n|Parameters|Bare Metal|VM|\r\n|:-:|:-:|:-:|\r\n|`variable_update=independent`| 150 img/sec | 150 img/sec |\r\n|`variable_update=parameter_server` <br> `local_parameter_device=cpu` | 150 img/sec | 150 img/sec |\r\n| `variable_update=replicated` <br> `local_parameter_device=cpu` <br> `use_nccl=False` | 146 img/sec | 146 img/sec |\r\n| `variable_update=parameter_server` <br> `local_parameter_device=gpu` | 130 img/sec (OOM) | 130 img/sec (OOM) |\r\n| `variable_update=replicated`<br> `local_parameter_device=gpu` <br> `use_nccl=False` | 122 img/sec (OOM) | 119 img/sec (OOM) |\r\n| `variable_update=replicated`<br> `local_parameter_device=cpu` <br> `use_nccl=True` | 94 img/sec | 107 img/sec |\r\n| `variable_update=replicated`<br> `local_parameter_device=gpu` <br> `use_nccl=True` | 94 img/sec | 98 img/sec |", "@byronyi \r\nHere are some numbers from AWS\r\n\r\nOn K80 and I would assume this is also true for K40 even if peering is turned on nccl is often slower due to the sync calls ending up being other work in the thread.  But even on a DGX-1 the best (although this is changing) approach was to put the shared parameters on the CPU for resnet and inception but for VGG16 which has a lot more parameters replicated NCCL was optimal.  \r\n\r\nYou can set an NVIDIA env variable:  ```CUDA_DEVICE_MAX_CONNECTIONS=12```.  I believe the default for K80 and K40 is 8.  For me this improved my VGG16 batch-size:32 per GPU time on p2.8xlarge to 266 images/sec with real data and 277 for synthetic (not that that matters for anything).  This also made replicated NCCL a viable option.  For resnet and inception it made replicated NCCL better and viable but still not as good as the other options.   Previously my best for VGG16 on AWS was ~242 images/sec with real data.  So a pretty good gain.  Not sure if it will help on K40s.  \r\n\r\nIf I find time I will link all of my results so someone can get value out of them.  It is really hard for me to make a simple Google Sheet public from my google employee account.  \r\n\r\nI am about to test an all reduce solution that should work in distributed mode on AWS that I am told may make VGG16 scale.  I think I remember seeing VGG scale on MPI but not on a normal network.  Also just because I have not seen it does not mean it has not happened but I am excited to try it myself.\r\n\r\nI was aware of the DGX-1 topology and I have seen P100s also setup with a ring topology.  I am far from an expert but with a movement toward all reduce, I do not think there is a need for direct 1:1 communication.  The aggregation is going to go in a ring."]}, {"number": 5788, "title": "Only run one GPU test at a time on windows.", "body": "", "comments": ["Jenkins, test this please.", "failure unrelated. Merging."]}, {"number": 5787, "title": "[Windows] Add Windows instructions to the 0.12 docs.", "body": "Added some instructions for installing/building TensorFlow on Windows, and made minor changes to other docs.", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @caisq and @martinwicke to be potential reviewers.", "@gunan @yifeif I was guessing what the eventual paths would look like, based on the PIP packages we're generating and the convention for other platforms. Did I guess right?", "Yeah, the links look right to me.\r\nIs this commit also already in master?", "@gunan This isn't in master yet - would you like me to send a backport PR?", "Either is fine.\nOnce binaries are out, we will merge this back into master, so we can wait\nfor that too.\n\nOn Tue, Nov 22, 2016 at 10:16 AM, Derek Murray notifications@github.com\nwrote:\n\n> @gunan https://github.com/gunan This isn't in master yet - would you\n> like me to send a backport PR?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5787#issuecomment-262320688,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOSHbSlP3V9zNnZVqrGjVJfsLV-mEks5rAzGSgaJpZM4K5uZh\n> .\n"]}, {"number": 5786, "title": "Gradients and variables was not shared in Adam optimizers when using bucketing", "body": "All,\r\n\r\nI used bucketing-like technology for seq2seq task:\r\n\r\n```python\r\n# For different length in encoder and decoder\r\nmodel_map = {}\r\nfor i in encoder_shape:\r\n    for j in decoder_shape:\r\n        with variable_scope.variable_scope(variable_scope.get_variable_scope(),\r\n                                 reuse=True if tt > 0 else None):\r\n            model = Seq2SeqModel()\r\n            model.build(encoder[:i], decoder[:j])\r\n            model_map[i*100+j] = model\r\n```\r\nAnd get shared model's parameters:\r\n\r\n```python\r\nfor t in tf.all_variables():\r\n    print t.name, t.get_shape() \r\n```\r\n\r\n```\r\nPrint: \r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias:0 (1600,)\r\n```\r\nModel's optimizer is like below:\r\n\r\n```python\r\n#every model have an optimizer\r\nparams = tf.trainable_variables()\r\nopt = tf.train.AdamOptimizer(1e-3)\r\ngradients = tf.gradients(self.loss, params)\r\nself.optimizer = opt.apply_gradients(zip(gradients, params))\r\n```\r\nBut I find that the optimizers don't share gradient:\r\n```\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_1:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_1:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam:0 (1600,)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_1:0 (1600,)\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_2:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_3:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_2:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_3:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_2:0 (1600,)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_3:0 (1600,)\r\n```\r\nWith the growth of the number of buckets, the GPU memory will grow too. And meanwhile I get a larger model in tf.train.Saver.save().\r\n\r\nSo is it possible to share gradient in bucketing?\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@Syndrome777   Please can you provide the information requested in the issues reporting template.  (e.s. TensorFlow version, installtion method, and exact command lines to reproduce)\r\n\r\n@lukaszkaiser Since this appears to be using Seq2Seq, can you spot anything obvious?  (presumably this is because the  `with tf.variable_scope(reuse=True)` mechanism relies on state being created using `tf.get_variable()` ?", "Yes, our optimizers, including AdamOptimizer, use tf.Variable instead of tf.get_variable, so they will not allow to share Adam variables like this. I don't know if we want to correct this in all optimizers, or find other workaround?", "@ebrevdo I seem to recall you gave a talk on related issues recently (bucketing for sequence models)... do you have any suggestions?", "@prb12 \r\nMy tensorflow's information:\r\nTensorFlow version: 0.11.0rc1\r\nInstalltion method: \r\n```\r\n# Ubuntu/Linux 64-bit, CPU only, Python 2.7\r\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n```\r\n@lukaszkaiser \r\nSorry, I'm not sure if I understand your solution.\r\n\r\n> Use tf.Variable instead of tf.get_variable\r\n\r\nDo you mean I need to change tf.Variable to tf.get_variable in Adam? \r\nBut the optimizer has been wrapped, now I can't find where Adam applies these variables. Could you give me some good ideas?", "Oh, sorry. I'm afraid it's not a solution at this point. I'm thinking that we should maybe change all optimizers in TensorFlow to use get_variable instead of just Variable. But that's a big change, needs to be discussed a bit more. I see no trivial hack around this at present, sorry. (Except if you patch your own TensorFlow and change both calls to tf.Variable in AdamOptimizer to corresponding calls of tf.get_variable.)", "@lukaszkaiser @prb12 Thanks.\r\nIn my opinion, could I fix this bug by changing:\r\n`slot = variables.Variable(val, name=scope, trainable=False)`\r\nto\r\n`slot = variables.get_variable(val, name=scope, trainable=False)`\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L50", "Indeed, that looks good. Did you test it, does it work for you? Would you care to make a PR with this change? I'm thinking that you might also need to change the betas in adam.py (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py#L111).\r\n\r\nDoes it work for you?", "@lukaszkaiser \r\nI think I have fixed this bug now in MomentumOptimizer with modifying some codes in some .py files. And fortunately the momentum.py don't need to be modified.\r\nBut I'm not sure if other optimizers should modify some codes in their .py file for correcting this bug.\r\nSo do you think I should correct all optimizers now or just MomentumOptimizer / AdamOptimizer first?\r\nAnd meanwhile I will try to make a PR. ", "I'd start small, changing what you need to solve your problem. When this PR is ready and all tests pass, one can always make another one. Meanwhile, I'll ask around to see if there was any special reason to not use get_variable in optimizers -- I'll report back if I learn about anything.", "Please feel free to make a PR.  I'll leave this open to track.", "Hi, @prb12 @lukaszkaiser \r\nI have make a pull request [PR5813](https://github.com/tensorflow/tensorflow/pull/5813). And if possible, please give me some advice in this PR. Thank you so much.", "Closing as the PR is merged.", "os = ubuntu18\r\npython3.6 \r\ntf 1.9 installed from pip3 \r\nsonnet installed from pip3\r\n\r\nim new to tf's lower level constructs, so any feedback/facts are welcome \r\n\r\nbelow are the parts of the graph that are being used inside an estimator the code below is just to test if i can get sonnet, slim and estimators working together, before using them on the actual project that I'm working on\r\nthe idea is to get image embedding using slim and then using sonnet's linear module to create logits \r\nthis block is responsible for returning image embeddings and tf.contrib.slim is used here  \r\n\r\n```\r\n\r\ndef image_em(image_batch = inp_image_batch):\r\n    with tf.variable_scope('embd') as scope:\r\n        l1 = slim.layers.conv2d( inputs= image_batch, num_outputs = 8, kernel_size  = (3,3), padding = 'VALID',stride = 2  )\r\n        l1 = slim.layers.max_pool2d(  l1 ,  [2,2] , padding= \"VALID\", stride = 2 )\r\n        flat = slim.layers.flatten( inputs= l1 )\r\n        return flat\r\n```\r\n\r\n\r\nwhen an object of the following class is called with inout the sub-graph/op ( which is being returned ) is inserted to parent graph \r\n```\r\nclass logit_mod(snt.AbstractModule):\r\n    def __init__(self, hidden_size, name=\"logit_mod\"):\r\n        super(logit_mod, self).__init__(name=name)\r\n        self._hidden_size = hidden_size\r\n        \r\n    def _build(self, \r\n        lin_mod = snt.Linear( output_size=self._hidden_size )\r\n        return tf.nn.softmax( lin_mod(inputs) )    # then connect it.\r\n\r\n```\r\n\r\nthis is the estimator part I have removed the irrelevant code from the moedel_fn \r\n\r\nthe model_fn: \r\n```\r\n## Model_fn\r\n# tf.reset_default_graph( )\r\n\r\n    \r\ndef model_fn(features, labels, mode, params):\r\n\r\n    embeddings = image_em( image_batch= features )\r\n    module = logit_mod( hidden_size= 10 )\r\n    logits = module( embeddings )\r\n    \r\n\r\n        # Define the optimizer for improving the neural network.\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.01, epsilon=0.001)\r\n\r\n        # Get the TensorFlow op for doing a single optimization step.\r\n        train_op = optimizer.minimize( loss=loss,  global_step= tf.train.get_global_step() )\r\n\r\n        # spec = Wrap all of this in an EstimatorSpec.\r\n   \r\n        \r\n    return spec\r\n\r\n```\r\n\r\n```\r\n#then i created an Estimator object\r\nest = estimator.Estimator( model_fn=model_fn,   )\r\n#then trained \r\nest = est.train( input_fn= tr_inp , steps=200)\r\n\r\n```\r\nbut when i ran \r\n``` \r\n    tf.trainable_variables() \r\n    # i got\r\n```    \r\n\r\n\r\n- ['beta1_power',\r\n-  'beta2_power',\r\n-  'embd/Conv/biases',\r\n-  'embd/Conv/biases/Adam',\r\n-  'embd/Conv/biases/Adam_1',\r\n-  'embd/Conv/weights',\r\n-  'embd/Conv/weights/Adam',\r\n-  'embd/Conv/weights/Adam_1',\r\n-  'global_step',\r\n-  'logit_mod/linear/b',\r\n-  'logit_mod/linear/b/Adam',\r\n-  'logit_mod/linear/b/Adam_1',\r\n-  'logit_mod/linear/w',\r\n-  'logit_mod/linear/w/Adam',\r\n-  'logit_mod/linear/w/Adam_1']\r\n\r\n**why are there two betas and \"Adam\" variables?, have i made any mistakes or this is because of using different modules(sonnet and slim) together ?** \r\nalso would anyone please  explain a bit about the down sides of this, \r\nmy guess is the two modules will not be able to  updaet gradients as supposed to and will learn independent of each other instead of one big composit function \r\n\r\nI've also reported it at [sonnet's repo](https://github.com/deepmind/sonnet/issues/118)\r\n if you made it till here, here's a cookie :cookie: :)\r\n\r\n", "Two Adam variables is because adam accumulates two things, a momentum and a\ngradient norm thingy. This is expected.\n\nOn Thu, Feb 7, 2019 at 1:38 AM deepakmeena635 <notifications@github.com>\nwrote:\n\n> os = ubuntu18\n> python3.6\n> tf 1.9 installed from pip3\n> sonnet installed from pip3\n>\n> im new to tf's lower level constructs, so any feedback/facts are welcome\n>\n> below are the parts of the graph that are being used inside an estimator\n> the code below is just to test if i can get sonnet, slim and estimators\n> working together, before using them on the actual project that I'm working\n> on\n> the idea is to get image embedding using slim and then using sonnet's\n> linear module to create logits\n> this block is responsible for returning image embeddings and\n> tf.contrib.slim is used here\n>\n>\n>\n> def image_em(image_batch = inp_image_batch):\n>\n>     with tf.variable_scope('embd') as scope:\n>\n>         l1 = slim.layers.conv2d( inputs= image_batch, num_outputs = 8, kernel_size  = (3,3), padding = 'VALID',stride = 2  )\n>\n>         l1 = slim.layers.max_pool2d(  l1 ,  [2,2] , padding= \"VALID\", stride = 2 )\n>\n>         flat = slim.layers.flatten( inputs= l1 )\n>\n>         return flat\n>\n>\n> when an object of the following class is called with inout the\n> sub-graph/op ( which is being returned ) is inserted to parent graph\n>\n> class logit_mod(snt.AbstractModule):\n>\n>     def __init__(self, hidden_size, name=\"logit_mod\"):\n>\n>         super(logit_mod, self).__init__(name=name)\n>\n>         self._hidden_size = hidden_size\n>\n>\n>\n>     def _build(self,\n>\n>         lin_mod = snt.Linear( output_size=self._hidden_size )\n>\n>         return tf.nn.softmax( lin_mod(inputs) )    # then connect it.\n>\n>\n>\n>\n> this is the estimator part I have removed the irrelevant code from the\n> moedel_fn\n>\n> the model_fn:\n>\n> ## Model_fn\n>\n> # tf.reset_default_graph( )\n>\n>\n>\n>\n>\n> def model_fn(features, labels, mode, params):\n>\n>\n>\n>     embeddings = image_em( image_batch= features )\n>\n>     module = logit_mod( hidden_size= 10 )\n>\n>     logits = module( embeddings )\n>\n>\n>\n>\n>\n>         # Define the optimizer for improving the neural network.\n>\n>         optimizer = tf.train.AdamOptimizer(learning_rate=0.01, epsilon=0.001)\n>\n>\n>\n>         # Get the TensorFlow op for doing a single optimization step.\n>\n>         train_op = optimizer.minimize( loss=loss,  global_step= tf.train.get_global_step() )\n>\n>\n>\n>         # spec = Wrap all of this in an EstimatorSpec.\n>\n>\n>\n>\n>\n>     return spec\n>\n>\n>\n>\n> #then i created an Estimator object\n>\n> est = estimator.Estimator( model_fn=model_fn,   )\n>\n> #then trained\n>\n> est = est.train( input_fn= tr_inp , steps=200)\n>\n>\n>\n>\n> but when i ran\n>\n>     tf.trainable_variables()\n>\n>     # i got\n>\n>\n>\n>    - ['beta1_power',\n>    - 'beta2_power',\n>    - 'embd/Conv/biases',\n>    - 'embd/Conv/biases/Adam',\n>    - 'embd/Conv/biases/Adam_1',\n>    - 'embd/Conv/weights',\n>    - 'embd/Conv/weights/Adam',\n>    - 'embd/Conv/weights/Adam_1',\n>    - 'global_step',\n>    - 'logit_mod/linear/b',\n>    - 'logit_mod/linear/b/Adam',\n>    - 'logit_mod/linear/b/Adam_1',\n>    - 'logit_mod/linear/w',\n>    - 'logit_mod/linear/w/Adam',\n>    - 'logit_mod/linear/w/Adam_1']\n>\n> *why are there two betas and \"Adam\" variables?, have i made any mistakes\n> or this is because of using different modules(sonnet and slim) together ?*\n> also would anyone please explain a bit about the down sides of this,\n> my guess is the two modules will not be able to updaet gradients as\n> supposed to and will learn independent of each other instead of one big\n> composit function\n>\n> if you made it till here, here's a cookie \ud83c\udf6a\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5786#issuecomment-461347077>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxSAahCYZRb1UlKCGVzZE25Ffkhqjks5vK_P7gaJpZM4K5fC1>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 5785, "title": "tensorflow label_image recognize so slow", "body": "I already asked [this question](http://stackoverflow.com/questions/40705203/tensorflow-label-image-recognize-so-slow?noredirect=1#comment68687966_40705203) on stackoverflow but It seems no one knows.\r\n\r\nwould you mind giving advices to make to code run faster.\r\nCause It took 5 second on laptop (30 seconds on pi 2) to recognize one picture of letter  \r\n\r\n```\r\n(tensorflow)khoa@khoa:~/tensorflow/tf_lv$ time python label_image.py photo1/9_9.jpg \r\nW tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\nt\r\n\r\nreal\t0m2.344s\r\nuser\t0m3.144s\r\nsys\t0m0.512s\r\n```\r\n\r\n", "comments": ["Please can you provide all of the information requested in the issues reporting template.  \r\n", "I already asked [this question](http://stackoverflow.com/questions/40705203/tensorflow-label-image-recognize-so-slow?noredirect=1#comment68687966_40705203) on stackoverflow but It seems no one knows.\r\n\r\n**Operating System:** \r\nLaptop : Ubuntu 14.04 LTS  \r\nRaspberry pi 2 : \"Raspbian GNU/Linux 8 (jessie)\"\r\n\r\n**Installed version of tensorflow:**   0.10.0\r\n\r\nMy project is OCR. I used **image_retraining**(v0.10.0) to recognize letters.\r\n\r\nI train it with pictures size 128x128\r\n\r\nAfter that I use my [code.py](https://github.com/shaolinkhoa/tensorflow/blob/master/OCR_image.py) to input several letter pictures (1306 pictures) which I segmented from a page of document\r\n\r\nThe code run so slow.\r\n\r\n    It took 3 seconds to recognize 1 letter and near 30 minutes to finish 1306 pictures on my laptop.\r\n\r\n    It took 38 seconds to recognize 1 letter and near 6 hours to finish 1306 pictures on pi 2\r\n\r\nI don't know why it run so slow. My C++ code use SVM on QT just took 5 seconds to do that ( It uses picture size 32x24).\r\n\r\nSo I think It's because I use picture too large or python run slower than C++ \r\n\r\nUpdate #1: The picture size is not the problem. I tried with the small picture size 20x20 but it's still slow. \r\nFollow the [time_chart](https://github.com/shaolinkhoa/tensorflow/blob/master/time_chart.txt). \r\n```\r\n  ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n     1309    0.029    0.000  990.377    0.757 /home/khoa/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py:599(run)\r\n     1309    0.134    0.000  990.333    0.757 /home/khoa/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py:835(_run)\r\n     1309    0.010    0.000  989.609    0.756 /home/khoa/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py:916(_do_run)\r\n     1309    0.004    0.000  989.599    0.756 /home/khoa/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py:963(_do_call)\r\n     1309    0.040    0.000  989.595    0.756 /home/khoa/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py:940(_run_fn)\r\n    1309 988.335    0.755  988.335    0.755 {_pywrap_tensorflow.TF_Run}\r\n     2646     56.390 0.021  56.390      0.021 {posix.system}\r\n```\r\n\r\nIt seems the code slow because of this command\r\n\r\npredictions = sess.run(softmax_tensor, \\\r\n        {'DecodeJpeg/contents:0': image_data})\r\n\r\nWould you mind giving me advices to make it run faster", "> I already asked this question on stackoverflow but It seems no one knows.\r\n\r\nI'm sorry, but this is not an appropriate venue for general support questions.\r\n\r\nFrom https://www.tensorflow.org/versions/r0.11/resources/index.html\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\nIn general, for any performance related issue I would suggest that you consider capturing a Timeline as described here #3009.  If that shows that there is a specific TensorFlow op which is running slower than expected the please feel free to open a specific issue.  \r\n\r\nI would also suggest running something like `htop` or `vmstat` to check that you do not have any virtual memory issues.\r\n\r\nOther things I would suggest is to measure how long you spend doing I/O (to read the model parameters and construct the graph, and the time spent reading the JPEG image data).  You will also incur some substantial overheads feeding the image data and fetching the results from Python.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "did you find a solution?", "any volunteer? 3s a picture. tooo slow"]}]