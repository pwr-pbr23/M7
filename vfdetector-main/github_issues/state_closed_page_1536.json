[{"number": 6844, "title": "changing number of outputs/classes", "body": "my StackOverflow thread: http://stackoverflow.com/questions/41645571/changing-number-of-outputs-classes-in-tensorflow\r\n\r\n### Environment info\r\nOperating System: OSX\r\n1. python3 installed with miniconda3, TF installation link: https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc1-py3-none-any.whl\r\n2. Version 0.11.0rc1\r\n\r\nMinimal example:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.reset_default_graph()\r\n\r\nx = tf.placeholder(tf.float32, (None, 2))\r\ny = tf.placeholder(tf.int32, (None,))\r\n\r\nw = tf.Variable(tf.truncated_normal(shape=(2, 2)))\r\nb = tf.Variable(tf.zeros(shape=(1, 2)))\r\n\r\nn_more = tf.placeholder(tf.int32, ())\r\n\r\nw_more = tf.truncated_normal((2, n_more))\r\nnew_w = tf.concat(concat_dim=1, values=[w, w_more])\r\nchange_w_op = tf.assign(w, new_w, validate_shape=False)\r\n\r\nb_more = tf.zeros(shape=(1, n_more))\r\nnew_b = tf.concat(concat_dim=1, values=[b, b_more])\r\nchange_b_op = tf.assign(b, new_b, validate_shape=False)\r\n\r\n\r\np = tf.matmul(x, w) + b\r\np1 = tf.matmul(x, w)\r\n\r\nloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(p, y))\r\nloss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(p1, y))\r\nopt = tf.train.GradientDescentOptimizer(0.1)\r\ngrads_and_vars = opt.compute_gradients(loss, var_list=[w, b])\r\ntrain_op = opt.apply_gradients(grads_and_vars)\r\n\r\ngrads_and_vars1 = opt.compute_gradients(loss1, var_list=[w])\r\ntrain_op1 = opt.apply_gradients(grads_and_vars1)\r\n\r\ninit = tf.initialize_all_variables()\r\n\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\n# add two more classes\r\nres = sess.run([change_w_op, change_b_op], feed_dict={n_more: 2})\r\nsess.run([w, b])\r\n#>[array([[-0.49880403,  0.1797405 , -0.66030115, -0.2065938 ],\r\n#>        [-1.19939673,  1.05717182,  0.03949607,  1.66543031]], dtype=float32),\r\n#> array([[-0.02882343,  0.02882343,  0.        ,  0.        ]], dtype=float32)]\r\n# Works as expected\r\nres = sess.run([p, p1], \r\n             feed_dict={\r\n                 x:np.random.randn(3, 2), \r\n                 y:np.random.randint(0, 4, 3)\r\n             })\r\n# both elements in the res are (3, 4) arrays  -- Works as expected\r\n\r\nres = sess.run([loss, loss1], \r\n             feed_dict={\r\n                 x:np.random.randn(3, 2), \r\n                 y:np.random.randint(0, 4, 3)\r\n             })\r\n# res = [2.4644723, 2.4450662]  --  Works as expected\r\n\r\nres = sess.run(grads_and_vars1, \r\n             feed_dict={\r\n                 x:np.random.randn(3, 2), \r\n                 y:np.random.randint(0, 2, 3)\r\n             })\r\n# res is a list of two (2, 4) arrays -- Works as expected\r\n\r\nres = sess.run(grads_and_vars, \r\n             feed_dict={\r\n                 x:np.random.randn(3, 2), \r\n                 y:np.random.randint(0, 2, 3)\r\n             })\r\n\r\n```\r\nThe last one produces the error below.\r\nMy main questions are:\r\nWhat am I doing wrong here? \r\nWhy changing w does not cause problems but changing b does? \r\nIs this a bug?\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n    971     try:\r\n--> 972       return fn(*args)\r\n    973     except errors.OpError as e:\r\n\r\n/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n    953                                  feed_dict, fetch_list, target_list,\r\n--> 954                                  status, run_metadata)\r\n    955 \r\n\r\n/Users/merekhinsky/.conda/envs/e1/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/errors.py in raise_exception_on_not_ok_status()\r\n    462           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 463           pywrap_tensorflow.TF_GetCode(status))\r\n    464   finally:\r\n\r\nInvalidArgumentError: Incompatible shapes: [3,4] vs. [1,2]\r\n\t [[Node: gradients/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/add_grad/Shape, gradients/add_grad/Shape_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-206-b8a2a3bd8471> in <module>()\r\n      2              feed_dict={\r\n      3                  x:np.random.randn(3, 2),\r\n----> 4                  y:np.random.randint(0, 2, 3)\r\n      5              })\r\n      6 res\r\n\r\n/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    715     try:\r\n    716       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 717                          run_metadata_ptr)\r\n    718       if run_metadata:\r\n    719         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    913     if final_fetches or final_targets:\r\n    914       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 915                              feed_dict_string, options, run_metadata)\r\n    916     else:\r\n    917       results = []\r\n\r\n/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n    963     if handle is None:\r\n    964       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n--> 965                            target_list, options, run_metadata)\r\n    966     else:\r\n    967       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n    983         except KeyError:\r\n    984           pass\r\n--> 985       raise type(e)(node_def, op, message)\r\n    986 \r\n    987   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Incompatible shapes: [3,4] vs. [1,2]\r\n\t [[Node: gradients/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/add_grad/Shape, gradients/add_grad/Shape_1)]]\r\n\r\nCaused by op 'gradients/add_grad/BroadcastGradientArgs', defined at:\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-196-ab717a15c74f>\", line 31, in <module>\r\n    grads_and_vars = opt.compute_gradients(loss, var_list=[w, b])\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\", line 469, in gradients\r\n    in_grads = _AsList(grad_fn(op, *out_grads))\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py\", line 555, in _AddGrad\r\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 388, in _broadcast_gradient_args\r\n    name=name)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n...which was originally created as op 'add', defined at:\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n[elided 18 identical lines from previous traceback]\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-196-ab717a15c74f>\", line 25, in <module>\r\n    p = tf.matmul(x, w) + b\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 751, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 71, in add\r\n    result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/merekhinsky/.conda/envs/e1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Incompatible shapes: [3,4] vs. [1,2]\r\n\t [[Node: gradients/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/add_grad/Shape, gradients/add_grad/Shape_1)]]\r\n```", "comments": ["Please don't crosspost StackOverflow threads. TensorFlow is community supported. We try our best to keep this issue tracker limited to bugs and feature requests.", "I am now more convinced that is not just support question, but a bug, there is a different behavior for tf.matmul() and tf.add() ops. Can you please look into it.", "Re-opening so this can be re-triaged.", "I don't think this is a bug, I commented on SO question"]}, {"number": 6843, "title": "Branch 144490912", "body": "Includes:\r\n\r\n144470919 (compiler opts)\r\n144485165, and 144478254 (parallel concat / parallel stack)\r\n144490912 i/o namespace in C++ API", "comments": ["@petewarden `cc1plus: error: unrecognized argument in option '-march=native'` is that new?", "@petewarden NM, I think it's 144470919.", "@martinwicke could you take a look?\r\n@drpngx Would you like to fix internally first, or should we merge and then fix forward?", "I prefer to fix internally, but this adds latency maybe an hour or two of latency. I'm fine working late, up to you. Let's see if we have other failures while we are thinking of a fix.", "however works best for you I can adjust, then let's close this, prepare a fix and prepare a new push?", "OK, closing. Not clear yet what the fix is."]}, {"number": 6842, "title": "Branch 144485165", "body": "Includes:\r\n* 144470919 (compiler opts)\r\n* 144485165, and 144478254 (parallel concat / parallel stack)", "comments": []}, {"number": 6841, "title": "A zip and/or tar for pypi", "body": "As of 0.12.1, https://pypi.python.org/pypi/tensorflow/ only has os specific wheels.  It would be very nice for packages (e.g. me with fink on the mac) to have a point release tar or zip file too.", "comments": ["I'm not familiar with Fink. Why can't it use wheel files? What are the advantages of putting a tarball on cheeseshop instead of just the wheels?", "Fink builds from source.  It needs to be able to control the entire setup process.  Looking in the wheel, I find:\r\n\r\nfile tensorflow/python/_pywrap_tensorflow.so\r\ntensorflow/python/_pywrap_tensorflow.so: Mach-O 64-bit dynamically linked shared library x86_64\r\n\r\notool -L tensorflow/python/_pywrap_tensorflow.so\r\ntensorflow/python/_pywrap_tensorflow.so:\r\n\tbazel-out/local-opt/bin/tensorflow/python/_pywrap_tensorflow.so (compatibility version 0.0.0, current version 0.0.0)\r\n\t/System/Library/Frameworks/IOKit.framework/Versions/A/IOKit (compatibility version 1.0.0, current version 275.0.0)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1)\r\n\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 120.1.0)\r\n\r\nIf for fink, I need to change anything about compile options or linking (which we do), I'm SOL.", "Have you considered configuring Fink to download the tarball from the [release page](https://github.com/tensorflow/tensorflow/releases/tag/0.12.1)?", "Sure, but the normal thing for python packages to do is to put a tar and/or zip on pypi.", "Normally Python packages are capable of being built from source by setuptools, which is not the case with TensorFlow. Uploading the GitHub tarball to cheeseshop would add complexity to our release process and I can't think of any reason why it would provide greater benefit to our users."]}, {"number": 6840, "title": "Set NDK_ROOT in android_full.sh to fix Makefile build", "body": "tensorflow/contrib/makefile/build_all_android.sh disagrees about the environment variable to use. This seems the easiest fix with the least chance of breaking anything else.", "comments": ["Didn't catch this when I tested the previous change locally because NDK_ROOT is set on my system, and the presubmits don't run android_full.sh. Can we have Jenkins make an exception and test it before or immediately after merging so that we know it works?"]}, {"number": 6839, "title": "Branch 144457649", "body": "", "comments": ["Jenkins, test this please.", "Will send another one with the AVX config. Keeping this open to check on jenkins.", "FWIW, I ran the timeout tests at master from a docker container on my box and they passed, so they are at most flaky.\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/3237/console"]}, {"number": 6838, "title": "Saver fails to restore on GCloud", "body": "You can demonstrate this with the distributed MNIST example code: https://cloud.google.com/ml/docs/quickstarts/training#train_on_the_cloud_distributed\r\n\r\nRunning it on gcloud produces:\r\n\r\n```ERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\tTraceback (most recent call last):\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    \"__main__\", fname, loader, pkg_name)\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    exec code in run_globals\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/root/.local/lib/python2.7/site-packages/trainer/task.py\", line 537, in <module>\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    tf.app.run()\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/root/.local/lib/python2.7/site-packages/trainer/task.py\", line 307, in main\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    run(model, argv)\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/root/.local/lib/python2.7/site-packages/trainer/task.py\", line 431, in run\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    dispatch(args, model, cluster, task)\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/root/.local/lib/python2.7/site-packages/trainer/task.py\", line 472, in dispatch\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    Trainer(args, model, cluster, task).run_training()\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/root/.local/lib/python2.7/site-packages/trainer/task.py\", line 243, in run_training\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    self.eval(session)\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/root/.local/lib/python2.7/site-packages/trainer/task.py\", line 284, in eval\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    self.model.format_metric_values(self.train_evaluator.evaluate()),\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/root/.local/lib/python2.7/site-packages/trainer/task.py\", line 75, in evaluate\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    self.sv.saver.restore(session, last_checkpoint)\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1388, in restore\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    {self.saver_def.filename_tensor_name: save_path})\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    run_metadata_ptr)\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    feed_dict_string, options, run_metadata)\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    target_list, options, run_metadata)\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\t    raise type(e)(node_def, op, message)\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\tInternalError: Unable to get element from the feed as bytes.\r\nINFO\t2017-01-13 10:56:52 -0500\tworker-replica-0\t\tTrain [worker/0], step 3195 (6.213 sec) 485.6 global steps/s, 170.5 local steps/s\r\nERROR\t2017-01-13 10:56:52 -0500\tmaster-replica-0\t\tModule raised an exception Command '['python', '-m', u'trainer.task', u'--train_data_paths=gs://cloud-ml-data/mnist/train.tfr.gz', u'--eval_data_paths=gs://cloud-ml-data/mnist/eval.tfr/gz', u'--output_path=<output path>']' returned non-zero exit status 1.```\r\n\r\nA fairly minimal test case follows:\r\n\r\n```#!/usr/bin/python\r\nimport argparse\r\nimport json\r\nimport os\r\nimport tensorflow as tf\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--output_path', type=str, default='/tmp/matrix_multiply')\r\nflags = parser.parse_args()\r\n\r\nmatrix1 = tf.placeholder_with_default([[3., 3.]], shape=(1, 2))\r\nmatrix2 = tf.placeholder_with_default([[2.], [2.]], shape=(2, 1))\r\nproduct = tf.Variable(tf.matmul(matrix1, matrix2))\r\n\r\ninputs = {'matrix1': matrix1.name, 'matrix2': matrix2.name}\r\ntf.add_to_collection('inputs', json.dumps(inputs))\r\n\r\noutputs = {'product': product.name}\r\ntf.add_to_collection('outputs', json.dumps(outputs))\r\n\r\nsaver = tf.train.Saver()\r\n\r\nwith tf.Session() as sess:\r\n\tsess.run(tf.global_variables_initializer())\r\n\tresult = sess.run(product)\r\n\tif not os.path.isdir(flags.output_path):\r\n\t\tos.makedirs(flags.output_path)\r\n\tsaver.save(sess, os.path.join(flags.output_path, \"export\"))\r\n\tprint result\r\n\r\nwith tf.Session() as sess:\r\n\tcheckpoint = tf.train.latest_checkpoint(flags.output_path)\r\n\tprint checkpoint\r\n\tsaver.restore(sess, checkpoint)\r\n\r\n\tfor v in tf.global_variables():\r\n\t\tprint(v.name, v.eval())```", "comments": ["Thanks for bringing this to our attention. That error message isn't helpful, which is something we could improve. See https://github.com/tensorflow/tensorflow/issues/436 which contains a possible solution. There are StackOverflow threads as well if you search that error. If you discover there's an error in the tutorial, please file an issue with https://github.com/GoogleCloudPlatform/cloudml-samples who maintain it.", "Also, you can try posting CloudML related questions on StackOverflow with the tag \"google-cloud-ml\" -- the team monitors that tag and is very helpful!"]}, {"number": 6837, "title": "occasional GraphHandle not found error", "body": "We occasionally get this error like below on universe-starter-agent. This is on a dedicated box and the code doesn't handle task restarts, so the common scenario doesn't apply . @mrry any idea if there's any way to get this error without a task restart?\r\n\r\nhttps://github.com/openai/universe/issues/112\r\n\r\n```\r\nException in thread Thread-10:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.AbortedError: Graph handle is not found: 000000000000000d\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/experiment/universe-starter-agent/a3c.py\", line 92, in run\r\n    self._run()\r\n  File \"/experiment/universe-starter-agent/a3c.py\", line 101, in _run\r\n    self.queue.put(next(rollout_provider), timeout=600.0)\r\n  File \"/experiment/universe-starter-agent/a3c.py\", line 139, in env_runner\r\n    summary_writer.add_summary(summary, policy.global_step.eval())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 515, in eval\r\n    return self._variable.eval(session=session)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 575, in eval\r\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3633, in _eval_using_default_session\r\n    return session.run(tensors, feed_dict)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.AbortedError: Graph handle is not found: 000000000000000d\r\n\r\n[2017-01-10 06:27:56,508] Received signal 15: exiting\r\nI tensorflow/core/distributed_runtime/master_session.cc:891] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000002c. Possibly, this worker just restarted.\r\n[2017-01-10 06:27:56,601] Killing and removing container: id=e8bf782d91fa3d6e3bb685e37ed4f9622f054bfb281f1e1e61021f1ed6798a06\r\n```", "comments": ["Can you describe the exact setup? I'm not sure what \"dedicated box\" and \"common scenario\" mean.\r\n\r\nIs it possible that some other  thread could have decided to close the session before that exception was raised? I think if a `close()` and a `run()` issue concurrently, it's possible to see a partially-shut-down state, which would correspond to this error message.\r\n\r\n(If no process restarted, I'd be very surprised if this happened, because the sequence of events is:\r\n\r\n1. Master calls register graph.\r\n2. Worker receives register graph call.\r\n3. Worker atomically invents a handle and registers it in its `GraphMgr` [(code)](https://github.com/tensorflow/tensorflow/blob/018583b064f40fa1f3cabb7eef0a5ac6b169275d/tensorflow/core/distributed_runtime/graph_mgr.cc#L240).\r\n4. Worker sends handle back to master.\r\n6. Master stores that handle in the `MasterSession`.\r\n7. Later, master attempts to deregister a graph (presumably when closing a session), using the handle it received from the worker.\r\n8. Worker receives deregister graph call.\r\n9. Worker atomically removes the graph associated with the given handle in its `GraphMgr` [(code)](https://github.com/tensorflow/tensorflow/blob/018583b064f40fa1f3cabb7eef0a5ac6b169275d/tensorflow/core/distributed_runtime/graph_mgr.cc#L252).\r\n\r\nI suppose it's possible that the DeregisterGraph call could have been retransmitted by gRPC, with the second response taking precedence. Perhaps we should ignore failures from the `DeregisterGraph` RPC.)\r\n", "Thanks for the explanation! We indeed had threads calling `sess.run` and `sess.close` in parallel"]}, {"number": 6836, "title": "Fix documentation sample code", "body": "Signed-off-by: Norman Heckscher <norman.heckscher@gmail.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6835, "title": "update Defun doc to clarify that definitions are frozen at first .run call", "body": "Fixes #6804 ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "We need to wait two weeks on that one, sorry about that.", "@zffchen78 @drpngx Do you have additional comments? Can we merge this or are we still in the two week waiting period mentioned above?", "changed to the wording suggested by zf", "Thanks @yaroslavvb. @zffchen78 can you take another look, please?", "Jenkins, test this please."]}, {"number": 6834, "title": "tf.strided_slice outputs tensor of incorrect shape", "body": "so for tensor x of shape (?, 41, 41, 32), using\r\n\r\n```\r\ntf.strided_slice(x, [0, 1, 1, 0], [-1, -1, -1, -1], [1, 2, 2, 1])\r\n```\r\n\r\nwill produce tensor <tf.Tensor 'StridedSlice_9:0' shape=(?, 20, 20, 31) dtype=float32>. This result is unexpected, since for last dimension, begin = 0, last = -1, and stride = 1. It should extract the entire dimension.", "comments": ["note that `a[:-1]` doesn't extract whole of `a`, it stops one element short, and  `strided_slice` copies numpy semantics\r\n\r\nIf you want to go all the way, you could do a[:] in Python subscript notation (which is syntactic sugar for `strided_slice`) or use `end_mask` argument to ignore the endpoint\r\n\r\n`y=tf.strided_slice(x, [0, 0, 0, 0], [-1, -1, -1, -1], [1, 2, 2, 1], end_mask=8)`", "@strin does @yaroslavvb's response address your issue?", "As discussed with @strin IRL, the confusion was that in `tf.slice(a, b)`, `b` specifies `size` and `-1` means \"take all elements\", whereas in `tf.striced_slice(a, b)`, `b` specifies `end` and `-1` means \"take elements up to the last one, not including the last one\". I'll close this issue for now since new users are likely to not use `slice` directly, so this should become less of an issue", "As @yaroslavvb explained, the second argument `b` has different semantic meaning in `tf.slice` and `tf.strided_slice`. this was confusing to me, because i thought `tf.strided_slice` is an extension of `tf.slice` except for the stride.", "This just bit me too. It'd be better if the [documentation](https://www.tensorflow.org/api_docs/python/tf/strided_slice) suggested to use Python's array notation instead. "]}, {"number": 6833, "title": "Go: Unable to set shape attributes on operations", "body": "Hi TensorFlow/Go team, I am attaching a code snippet that is failing due to the way shape argument is expected.\r\n\r\ncc: @asimshankar and referencing [this](https://github.com/tensorflow/tensorflow/issues/10#issuecomment-272461355) discussion\r\n\r\n```\r\n\r\npackage bugs\r\n\r\nimport (\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n\t\"testing\"\r\n)\r\n\r\nfunc TestVarHandleOpShape(t *testing.T) {\r\n\tscope := op.NewScope()\r\n\t_ = op.VarHandleOp(scope, tf.Int32, []int64{2, 3})\r\n\t_, err := scope.Finalize()\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n}\r\n\r\n/*\r\n=== RUN   TestVarHandleOpShape\r\n--- FAIL: TestVarHandleOpShape (0.00s)\r\n        bugs_test.go:14: failed to add operation \"VarHandleOp\": AttrValue had value with type 'list(int)' when 'shape' expected\r\n                         for attr 'shape'\r\n                        ; NodeDef: VarHandleOp = VarHandleOp[_class=[], container=\"\", dtype=DT_INT32, shape=[2, 3], shared_name=\"\"](); Op<name=VarHandleOp; signature= -> resource:resource; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; attr=dtype:type; attr=shape:shape; is_stateful=true> (Stacktrace: goroutine 6 [running]:\r\n                runtime/debug.Stack(0x0, 0x0, 0x0)\r\n                        /usr/lib/golang/src/runtime/debug/stack.go:24 +0x79\r\n                github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).UpdateErr(0xc420012570, 0x51d427, 0xb, 0x79fba0, 0xc420054048)\r\n                        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:113 +0x72\r\n                github.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc420012570, 0x51d427, 0xb, 0x51d427, 0xb, 0x0, 0x0, 0x0, 0xc4200125a0, 0xc420012540)\r\n                        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:78 +0xf9\r\n                github.com/tensorflow/tensorflow/tensorflow/go/op.VarHandleOp(0xc420012570, 0xc400000003, 0xc4200106e0, 0x2, 0x2, 0x0, 0x0, 0x0, 0x1123b1d4, 0xc420051f78)\r\n                        /home/sdeoras/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:14045 +0x27d\r\n                bitbucket.hgst.com/x/tensorflow.git/bugs.TestVarHandleOpShape(0xc4200b8240)\r\n                        /home/sdeoras/go/src/bitbucket.hgst.com/x/tensorflow.git/bugs/bugs_test.go:11 +0x96\r\n                testing.tRunner(0xc4200b8240, 0x52bb48)\r\n                        /usr/lib/golang/src/testing/testing.go:610 +0x81\r\n                created by testing.(*T).Run\r\n                        /usr/lib/golang/src/testing/testing.go:646 +0x2ec\r\n                )\r\nFAIL\r\nexit status 1\r\nFAIL    bitbucket.hgst.com/x/tensorflow.git/bugs        0.134s\r\n*/\r\n\r\n```", "comments": ["Thank you folks. It is working great!"]}, {"number": 6832, "title": "GridRNN cell uses tuples for output and states", "body": "In response to this (https://github.com/tensorflow/tensorflow/issues/2560) and recent changes in LSTMCell, this PR added `state_is_tuple=True` and `output_is_tuple=True` into the constructor of GridRNNCell:\r\n\r\n```\r\n> cell = tf.contrib.grid_rnn.Grid2LSTMCell(3, use_peepholes=True)\r\n> cell.state_size\r\n       (LSTMStateTuple(c=3, h=3), LSTMStateTuple(c=3, h=3))\r\n> cell.output_size\r\n       (3,)\r\n```\r\n\r\nThis means there are 2 LSTM cells in the grid, and the state of each cell has size of `(c=3, h=3)`. There is only one output dimension, whose size is 3.\r\n\r\nIn contrast:\r\n\r\n```\r\n> cell = tf.contrib.grid_rnn.Grid2BasicRNNCell(3)\r\n> cell.state_size\r\n     (3, 3)\r\n> cell.output_size\r\n     (3,)\r\n```\r\n\r\nThis means there are 2 BasicRNN cell in the grid, and the state of each cell has size of 3. There is only one output dimension, whose size is 3.\r\n\r\nPrevious behaviour is maintained by using `state_is_tuple=False, output_is_tuple=False` when creating the cell.\r\n\r\n```\r\n> cell = tf.contrib.grid_rnn.Grid2LSTMCell(3, use_peepholes=True, state_is_tuple=False, output_is_tuple=False)\r\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2LSTMCell object at 0x10de68f50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\r\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2LSTMCell** object at 0x10de68f50>: Using a concatenated output is slower and will soon be deprecated.  Use output_is_tuple=True.\r\n\r\n> cell.state_size\r\n    12\r\n> cell.output_size\r\n    3\r\n\r\n> cell = tf.contrib.grid_rnn.Grid2BasicRNNCell(3, state_is_tuple=False, output_is_tuple=False)\r\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2BasicRNNCell object at 0x10de4b950>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\r\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2BasicRNNCell object at 0x10de4b950>: Using a concatenated output is slower and will soon be deprecated.  Use output_is_tuple=True.\r\n\r\n> cell.state_size\r\n   6\r\n> cell.output_size\r\n   3\r\n```\r\n\r\nThis also fixes https://github.com/tensorflow/tensorflow/issues/4296\r\n\r\n# Backward compatibility\r\nThis change is not fully backward-compatible:\r\n- The old implementation concatenates the output and the state tensors, so when you have `g, s = cell(...)` then `g` and `s` are simply two tensors.\r\n- This implementation, by default, returns `g` and `s` as tuples of tensors. \r\n  - `g` is a tuple of length equals to the size of `output_dims` of the cell. Normally you only have one output dimension, so `g` will be a tuple of length 1.\r\n  - `s` is a tuple of length equals to the size of `recurrent_dims` of the cell, containing the states of all the recurrent cells in all recurrent dimensions.\r\nNow if you use LSTM cells for the recurrent dimensions, the state of each LSTM cell is a tuple of tensors (with `c` and `h` components). So for instance, the state of a Grid2LSTMCell will be a tuple of tuples: `((c=<tensor>, h=<tensor>), (c=<tensor>, h=<tensor>))`\r\nIf you use GRU or vanila RNN cells for the recurrent dimensions, the state of each cell is a tensor. So for instance, the state of a Grid2GRUCell will be a tuple of tensors: `(<tensor>, <tensor>)`\r\n\r\nCode that depends on this cell can use the old behaviour by setting `state_is_tuple=False` and `output_is_tuple=False` when constructing the cell. \r\n\r\n```\r\ncell = tf.contrib.grid_rnn.Grid2LSTMCell(2, use_peepholes=True,\r\n                                             state_is_tuple=False,\r\n                                             output_is_tuple=False)\r\n```\r\n\r\n**Breaking change**: `state_is_tuple` and `output_is_tuple` are `True` by default.", "comments": ["Can one of the admins verify this patch?", "@drpngx Can you please take a look? Thanks.", "@ebrevdo is more familiar with that code.", "@phvu I'm trying to finish an RNNCell refactoring this week and that needs to happen before this PR can be reviewed.  Please hold on another few days.  I'll ping this thread when it's time to rebase and rerun tests to ensure things are still working with this PR.", "@ebrevdo Can work on this continue now?", "Yes - this PR can now continue!  thanks for waiting!\n\nOn Tue, Mar 7, 2017 at 10:00 AM, Dandelion Man\u00e9 <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Can work on this continue now?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6832#issuecomment-284804115>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9JJXvVNOuwpTvO38Khyl09Zmj4-ks5rjZsmgaJpZM4LjBXK>\n> .\n>\n", "@ebrevdo: This should be ready for review.\r\n`state_is_tuple` and `output_is_tuple` are `True` by default. I also refactored the code to make it cleaner.", "@ebrevdo, friendly ping", "Jenkins, test this please.\r\n\r\n@ebrevdo if you have cycles, please take a look, otherwise LMK", "Thanks @ebrevdo!\r\nI made the changes, do let me know if anything else needs to be done.", "Jenkins, test this please."]}, {"number": 6831, "title": "running session multiple times with tf.random returns different values for conv2d", "body": "```\r\nimport tensorflow as tf \r\nimport numpy as np \r\n\r\nx_tf = tf.placeholder('float',[None, 2, 5, 1])\r\nx_np = np.random.noraml(0,1,[1,2,5,1])\r\n\r\n# ======== filter option1 and option2 ===========\r\nf_np = np.random.normal(0,1,[1,3,1,1])\r\nf_tf = tf.constant(f_np,'float') # option 1\r\nf_tf = tf.random_normal([1,3,1,1]) # option 2\r\n# ===============================================\r\n\r\nx_conv = tf.nn.conv2d(x_tf,f_tf,[1,1,1,1],'SAME')\r\n\r\nwith tf.Session() as sess:\r\n     tf.gloval_variables_initializer().run()\r\n     x_conv_np  = sess.run(x_conv, feed_dict={x_tf, x_np})\r\n     x_conv_np2 = sess.run(x_conv, feed_dict={x_tf, x_np})\r\n```\r\nIf I run the code above with option1, I get the same values for `x_conv_np` and `x_conv_np2`\r\nHowever, when I run the above with option2, I get different values for `x_conv_np` and `x_conv_np2`.\r\n\r\nI am guessing the `tf.random_normal` gets initialized every time the session is ran.\r\nIs this meant to happen or is it a bug?\r\nI found this out while I was trying to debug my code, and it took me a while to figure out the issue. \r\n\r\nDoesn't this mean that when option2 is chosen, the result of the convolution changes for each loop that iterates over mini batches?\r\n", "comments": ["Yes, this is intentional. See the docs for more details on how you can control this behavior:\r\nhttps://www.tensorflow.org/api_docs/python/constant_op/random_tensors\r\n\r\nIn the future, please use StackOverflow for usage questions like this."]}, {"number": 6830, "title": "Update einsum to check whether uncompacted_shape has multiple None values", "body": "Fixes #6824.\r\nIt checks `uncompacted_shape`, which contains the 'final' shape of `matmul` of two tensors, if it contains two or more `None` values.\r\nIf `uncompacted_shape` contains two or more `None` values, it means that it cannot be directly given to `reshape` as parameter.\r\nThis PR resolves this issue, by querying the dimension value using `tf.shape` function for that case.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@jihunchoi that's consistent with numpy behavior right?\r\n\r\n@aselle FYI", "If you mean the test results, yes:\r\n```\r\nIn [2]: m0 = np.array([[[1, 2]]])\r\n\r\nIn [3]: m1 = np.array([[3], [2]])\r\n\r\nIn [4]: np.einsum('ijk,kl->ijl', m0, m1)\r\nOut[4]: array([[[7]]])\r\n```"]}, {"number": 6829, "title": "./configure: A better way to exclude fetching //tensorflow/examples/android/...", "body": "Bazel at HEAD currently [breaks TF Windows build](http://ci.bazel.io/job/TensorFlow/BAZEL_VERSION=HEAD,PLATFORM_NAME=windows-x86_64/601/console).\r\n\r\nAfter https://github.com/bazelbuild/bazel/commit/0b9ebfee8d7f3bd5cf68f4dba4790bf3fed9349d, `//tensorflow/examples/android/...` has to be excluded during bazel fetch.\r\n\r\nUsing the workaround mentioned in https://github.com/bazelbuild/bazel/issues/2220, so that it works on both Linux and Windows.\r\n\r\n@gunan ", "comments": ["That is correct.\nOn Mon, Jan 16, 2017 at 1:42 PM drpngx <notifications@github.com> wrote:\n\n> *@drpngx* commented on this pull request.\n> ------------------------------\n>\n> In configure <https://github.com/tensorflow/tensorflow/pull/6829>:\n>\n> >    fi\n> +  bazel fetch \"//tensorflow/... -//tensorflow/examples/android/...\"\n>\n> So, to summarize, we need to exclude fetch of\n> //tensorflow/examples/android/... from all targets, not just windows?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6829>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHZavASGyn5KnXTTxJCRNJHLPlwBsyNkks5rS7oJgaJpZM4Li4Sb>\n> .\n>\n", "Jenkins, test this please.", "Thanks!", "Ignore test failure. I think that division returns a float, but also we're going to remove that test in later pushes."]}, {"number": 6828, "title": "Tracking moving persons with ID", "body": "I want to track person with IDs. I am able to run android APK for tracking, but  [C++ build](https://github.com/tensorflow/tensorflow/blob/v1.0.0-alpha/tensorflow/examples/android/jni/object_tracking/object_tracker.h) is taking a lot of time to build. Is there any way to run  python code for tracking?? ", "comments": ["@nitish11 The person tracking demo is Android-only currently. No immediate plans to port to Python, sorry."]}, {"number": 6827, "title": "HOW can I USE tf.exp(x) to back propagation Gradient?", "body": "@tf.RegisterGradient(\"FGGrad\")\r\ndef grad_fg(op, x):\r\n    return (2*tf.exp(x))/((1+tf.exp(x))**2)\r\ndef fg(x):\r\n    with G.gradient_override_map({\"Identity\": \"FGGrad\"}):\r\n         return tf.identity(x)\r\n\r\nThe code is above .\r\nBut i made a mistake.\r\nHOW can I USE  (2*tf.exp(x))/((1+tf.exp(x))**2) to back propagation Gradient?\r\n(I can use tf.pow(x,2))to back propagation Gradient)", "comments": ["This is a question for Stack Overflow. They also monitor there."]}, {"number": 6826, "title": "Why is validation_metrics and validation_monitor code twice in tutorial code iris_monitors.py?", "body": "Why is validation_metrics and validation_monitor code twice in tutorial code iris_monitors.py?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/monitors/iris_monitors.py", "comments": ["Thanks for reaching out. We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support.", "I don't know, and I can not get the correct results as the tutorial. I don't know why."]}, {"number": 6825, "title": "Enable bitcode for iOS builds by default", "body": "Closes #5611", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 6824, "title": "tf.einsum fails on evaluation in some cases", "body": "Hello,\r\nI was a user of 'reshape-matmul-reshape' approach, however it felt me a bit messy, so I am trying some alternatives that do this automatically.\r\nI had tried `tf.tensordot` before, however it did not work well when shapes are partially known (#6682).\r\nAs an alternative, I tried to use `tf.einsum`.\r\nHowever, similar to `tf.tensordot`, it cannot handle some variables with partially-known-shape, for instance, the below codes fails on evaluation.\r\n\r\n```\r\na = tf.placeholder('float32', shape=[None, None, 100])\r\nb = tf.placeholder('float32', shape=[100, 300])\r\nresult_einsum = tf.einsum('ijk,kl->ijl', a, b)\r\na_value = np.random.randn(10, 20, 100)\r\nb_value = np.random.randn(100, 300)\r\nsess.run(result_einsum, {a: a_value, b: b_value})  # failure\r\n```\r\n\r\n\r\nThis is the error message:\r\n```\r\nInvalidArgumentError (see above for traceback): only one input size may be -1, not both 0 and 1\r\n         [[Node: Reshape_5 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MatMul_3, Reshape_5/shape)]]\r\n```", "comments": ["I don't fully understand how `einsum` works, but as far as I read, the reason is that when there exist two or more `None` values in one of `input` parameters, then `uncompacted_shape` in line 339 has two or more `None` values.\r\nThen, in `_reshape_if_necessary` function, `None` values of `uncompacted_shape` are replaced with `-1` and passed to `reshape` function. Since `shape` parameter of `reshape` function should have at most one `None` value, thus the error is triggered.\r\nI think it can be resolved if we check whether `uncompacted_shape` has two or more `None` values and in that case explicitly give it a shape value by `tf.shape` function to make it have at most one `-1` value.", "Assigning this issue to @drpngx who is reviewing the pull request you were generous enough to send us.", "Hi,\r\nI just got the same error, and although it's amazing to see that it has been fixed so quickly, I think my issue coming from a different location.\r\n\r\nMy line is `tf.einsum('mckd,mc->mkd', Kmcmc, w)`, where `Kmcmc.get_shape()` evaluates to `(?, 8, ?, 8)`. This causes the same problem in [special_math_ops.py#L327](https://github.com/jihunchoi/tensorflow/blob/51b494280777d735607061e27a15b49a7b80d8e9/tensorflow/python/ops/special_math_ops.py#L327). This issue should also exist in [special_math_ops.py#L344](https://github.com/jihunchoi/tensorflow/blob/51b494280777d735607061e27a15b49a7b80d8e9/tensorflow/python/ops/special_math_ops.py#L334).\r\n\r\nCould the same fix be applied to these situations too?", "I think that is indeed the similar issue, but I didn't thought of that case. \ud83d\ude22 \r\nIt seems that the same workaround can be applied to [special_math_ops.py#L327](https://github.com/jihunchoi/tensorflow/blob/51b494280777d735607061e27a15b49a7b80d8e9/tensorflow/python/ops/special_math_ops.py#L327), and it should be applied to [special_math_ops.py#L333](https://github.com/jihunchoi/tensorflow/blob/51b494280777d735607061e27a15b49a7b80d8e9/tensorflow/python/ops/special_math_ops.py#L333).\r\n\r\nI will soon check whether there exists another part of codes that I missed and send a new PR!", "thanks!", "I created a new PR that solves the issue.", "I think `_exponential_space_einsum` faced the same problem that can't handle more than one `None` shape \r\n@jihunchoi  any chance to expect new pull request?\r\n\r\n```python\r\n    x_val = np.array([[1, 2],\r\n                     [3, 4]])\r\n    y_val = np.array([[4, 5],\r\n                     [6, 3]])\r\n    g_val = np.array([[3, 5],\r\n                      [2, 7]])\r\n    x = tf.placeholder(dtype=tf.int32, shape=[None, None])\r\n    y = tf.placeholder(dtype=tf.int32, shape=[None, None])\r\n    g = tf.placeholder(dtype=tf.int32, shape=[None, None])\r\n\r\n    z = tf.einsum('ab,ab,ab->b', x, y, g)\r\n    with tf.Session() as sess:\r\n        a, b, c = sess.run([x,y,z], feed_dict={x: x_val,\r\n                                               y: y_val,\r\n                                               g: g_val})\r\n```\r\n\r\n```shell\r\nInvalidArgumentError (see above for traceback): only one input size may be -1, not both 0 and 1\r\n\t [[Node: Reshape = Reshape[T=DT_INT32, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_5, Reshape/shape)]]\r\n\t [[Node: Sum/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3_Sum\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```"]}, {"number": 6823, "title": "Upgrade HighwayHash", "body": "The HighwayHash module which is downloaded as an external dependency in TensorFlow produces different hash results on big endian and little endian architectures. This causes the test `testStringToHashBucketsStrong` from `//tensorflow/python/kernel_tests:string_to_hash_bucket_op_test` to fail on big endian. After raising an [issue](https://github.com/google/highwayhash/issues/35) with HighwayHash community, they have added a change to make hash values consistent across architectures through [commit](https://github.com/google/highwayhash/commit/cdde139127319cb5eb3917b635c4d2b182533cb4).\r\n\r\nWill it be possible to pick this or higher commit of HighwayHash in TensorFlow?", "comments": ["This doesn't sound good. I'm going to write up a change that upgrades TensorFlow. I just hope this change won't cause problems for people with big endian CPUs who stored the result of the previous hash.", "This upgrade is now blocked by https://github.com/google/highwayhash/issues/36.", "This should now be unblocked. I'm now going to mail out a change.", "Thank you @jart and @rmlarsen for resolving this. ", "Pleasure to be of service."]}, {"number": 6822, "title": "Disable building the MatrixDiag and OneHot kernels for GPU on Windows.", "body": "This is a temporary workaround for issue #6509, while the root cause of the runtime errors is under investigation.", "comments": ["@gunan I didn't manage to get the GPU PR build started... here's the error: http://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/5/console\r\n\r\n```\r\n21:59:28 c:\\tf_jenkins\\home\\workspace\\tf-pr-win-cmake-gpu>call tensorflow/tools/ci_build/windows/gpu/cmake/run_py.bat \r\n21:59:28 'tensorflow' is not recognized as an internal or external command,\r\n21:59:28 operable program or batch file.\r\n21:59:28 \r\n21:59:28 c:\\tf_jenkins\\home\\workspace\\tf-pr-win-cmake-gpu>exit 1 \r\n```\r\n\r\nAny ideas? I'll test it on my workstation tomorrow if not....", "Got it to run now,\r\nhttp://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/7/\r\nApparently we need to provide PR number slightly differently.", "Thanks for looking into that! Trying a new run, which should fix the failure: http://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/8/", "Once more without a silly typo this time: http://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/9/", "Well the build timed out before reaching the `one_hot_op_test.py`. :(\r\n\r\n@gunan Any ideas for how to make it run longer? I also made a similar change to the MatrixDiag ops and `diag_op_test.py` has failed, but when I look in the workspace at the testing log, I don't see any errors.... Is it maybe failing because it's not using the GPU (e.g. because of some smarts put in to ensure that we're actually testing GPU implementations)?", "ah, I have upped the timeout to 5 hours and restarted the build here:\r\nhttp://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/10/", "Five hours ought to be enough for **any** test run! :P", "@gunan Looks like `one_hot_op_test.py` and `trace_op_test.py` succeeded, but `diag_op_test.py` failed for mysterious reasons. 2 out of 3 isn't bad:\r\n\r\nhttp://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/10/console\r\n\r\nI re-disabled `diag_op_test.py` for the time being. Shall we merge this, since it should at least prevent ugly crashes?", "Sure, sgtm.\r\nThe remaining issues are known to me so the tests are all OK."]}, {"number": 6821, "title": "Make tfdbg tests pass in Windows Bazel build", "body": "", "comments": ["Tomorrow, I'll test internally to make sure that moving debug_ops out of array_not_windows is okay.", "The upgrade test should be fixed by previous push, and the `seq2seq` and `rnn_core` tests seem to timeout frequently. The `sync_replicas_optimizer_test` seems unrelated:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'save/RestoreV2_3': Could not satisfy explicit device specification '/job:ps/task:1/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:ps/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:2/cpu:0\r\n```"]}, {"number": 6820, "title": "Branch 144396000", "body": "contrib/layers.py conflict: preserved both indentation (decay) and doc fix (center).", "comments": ["Jenkins, test this please.", "This tends to timeout consistently: \r\n```\r\n//tensorflow/tensorboard/backend:server_test                            TIMEOUT in 65.0s\r\n```"]}, {"number": 6819, "title": "add virtual START&END symbol to CRF and implement viterbi decoding", "body": "Two major changes.\r\n\r\n    1. add virtual START and END symbol to CRF implementation\r\n    2. reimplement viterbi decoding using tensorflow\r\n\r\nThe reason of the above modifications:\r\n\r\n    1. The original implementation of CRF use no virtual START and END symbol. \r\n        However crf can use the transition weights that starting from the virtual START \r\n        symbol to labels of the first step.\r\n    2. The original implementation of viterbi decoding uses numpy, which makes it \r\n        difficult to infer sequence labels in tensorflow computation graphs.", "comments": ["Can one of the admins verify this patch?", "Thanks! Could we add some unit tests here?", "@drpngx I've updated with several unit test", "Jenkins, test this please.", "@drpngx previously, the code haven't been validated by pylint using Google code style. Now I've fixed most of them. \r\n\r\nAnd I'm wondering if these checks can be automatically enabled with I checked in new commits. It's my first time to contribute to tensorflow. Thanks so much.", "Yes that's part of the sanity check on Jenkins\n\nOn Jan 13, 2017 9:23 PM, \"Feng Weiguo\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> previously, the code haven't been\n> validated by pylint using Google code style. Now I've fixed most of them.\n>\n> And I'm wondering if these checks can be automatically enabled with I\n> checked in new commits. It's my first time to contribute to tensorflow.\n> Thanks so much.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6819#issuecomment-272603132>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbeE6nw4cH07x72gjORicD0Z7pUfzks5rSFvrgaJpZM4LidCS>\n> .\n>\n", "Jenkins, test this please.", "you need to use `tf.unstack`\r\n\r\n```\r\nFAIL: //tensorflow/contrib/crf:crf_test (see /private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/execroot/tensorflow-pull-requests-mac/bazel-out/local-opt/testlogs/tensorflow/contrib/crf/crf_test/test.log).\r\nINFO: From Testing //tensorflow/contrib/crf:crf_test:\r\n==================== Test output for //tensorflow/contrib/crf:crf_test:\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n.E...\r\n======================================================================\r\nERROR: testCrfLogNorm (__main__.CrfTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/execroot/tensorflow-pull-requests-mac/bazel-out/local-opt/bin/tensorflow/contrib/crf/crf_test.runfiles/org_tensorflow/tensorflow/contrib/crf/python/kernel_tests/crf_test.py\", line 123, in testCrfLogNorm\r\n    transition_params=constant_op.constant(transition_params))\r\n  File \"/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/execroot/tensorflow-pull-requests-mac/bazel-out/local-opt/bin/tensorflow/contrib/crf/crf_test.runfiles/org_tensorflow/tensorflow/contrib/crf/python/ops/crf.py\", line 105, in crf_log_norm\r\n    inputs = array_ops.unpack(inputs, axis=1)\r\nAttributeError: 'module' object has no attribute 'unpack'\r\n\r\n----------------------------------------------------------------------\r\nRan 5 tests in 9.069s\r\n```", "@tensorflow-jenkins test this please", "previously, pack/unpack is used in the commitments. Now I use stack/unstack instead, and passed the unit tests in my local environment   ", "Jenkins, test this please.", "@yuanbyu is that something you might want to look at? LMK or I can find someone else.", "The failure test seems to be mvn test, should I fix this or some one can help me with this? I've modified nothing on mvn test. ", "Yes, it's an unrelated time out.", "@drpngx did you review this?", "@tensorflow-jenkins test this please", "It looks like @alextp reviewed the original version of this code. Would you mind taking a look?", "Sounds good. I didn't review. I could check the math but I'm not as confident with the API changes.", "@alextp should I rename back input parameters, and make both api and parameters compatible with the original api ? ", "That would be great, thanks!\n\nOn Feb 6, 2017 23:40, \"Feng Weiguo\" <notifications@github.com> wrote:\n\n@alextp <https://github.com/alextp> should I rename back input parameters,\nand make both api and parameters compatible with the original api ?\n\n\u2014\nYou are receiving this because you were mentioned.\n\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/6819#issuecomment-277923410>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AAATxWJDO7JpgysiKOJsAVMjbxzX-oHfks5raB_YgaJpZM4LidCS>\n.\n", "@jungle-cat any updates here?", "ping for @jungle-cat -- I'll close by end of day if I don't hear back but feel free to send a new PR once you get time to look at this again. We'll try to keep the review cycle short.", "Closing as stalled per vrv@'s last comment."]}, {"number": 6818, "title": "GPU impl. of hue adjustment op", "body": "This is related to issue #6817. Mentioning @zheng-xq since he implemented the fused CPU hue adjustment kernel.", "comments": ["Can one of the admins verify this patch?", "Thanks a lot for the contribution! I'll look into it. ", "BTW, by the [following test](https://gist.github.com/mkolod/71d899687e21f91e1e870832d4de5569), run on a machine with 32 GB RAM, a 6-core Intel Core i7-5930K CPU @ 3.50GHz and an NVIDIA GTX 1080 GPU produced a GPU-based speedup factor of 17.3, given both the use of a fused CPU and GPU kernel. This was based on a test with a 4D random image tensor with a batch size of 64 and \"image\" size of 256x256x3. The GTX 1080 throughput 31,950 images per second, while the Core i7-5930K throughput was 1,845 images per second. Longer running times, e.g. running tf.while_loop 100k times, can bring the speedup factor to about 30x on this reference hardware, especially for real images where pixels' hues are spatially correlated (which reduces GPU warp divergence in the RGB-to-HSV part of the kernel). Both 3D and 4D tensors work, but of course 4D tensors perform better due to batching. Note that for uint8 inputs (e.g. from tf.image_jpeg_decode), the results of the CPU and GPU kernel are identical, pixel for pixel. For inputs of int32, with an input numeric range of [0, 256], a small fraction of pixels differ by up to 0.3% in intensity (1/255), which is caused by the fact that the Python wrapper for adjust_hue() does scaling, and scaling a range of [0, 255] by 1E31 causes precision problems. That's a very small error for that unusual case. Numbers in range of [0, 255] represented as floats, or as uint8 (scaled by 255 for float conversion by convert_image_dtype() inside tf.image_adjust_hue()) return exactly the same results on the CPU and GPU. ", "@zheng-xq I made all the changes based on your requests. I think it's in a state that's acceptable based on your first review. The tests pass, as do my own experiments, for positive and negative delta. The code was refactored according to your spec. Please let me know if this is good enough to run the CI process.", "Jenkins, test this please.", "@drpngx @zheng-xq Everything that depends on the sanity checks step fails, because sanity checks don't pass. Looking at the log, I get the following message: \r\n\r\n> FAIL: buildifier found errors and/or warnings in above BUILD files.\r\n> buildifier suggested the following changes:\r\n> 1530a1531\r\n> >     linkstatic = 1,\r\n> 1537d1537\r\n> <     linkstatic = 1,\r\n> 1542d1541\r\n> <     prefix = \"adjust_hue_op\",\r\n> 1543a1543\r\n> >     prefix = \"adjust_hue_op\",\r\n> Please fix manually or run buildifier <file> to auto-fix.\r\n\r\nI never encountered this problem while building locally. I can make that change, but I'm wondering if that won't trigger other problems. Is there a way to run this sanity check step locally, without the entire CI infra, to see if that resolves the problem? Even in the absence of this change, I can build the _pywrap_tensorflow.so and pass  //tensorflow/python:image_ops_test, etc.\r\n\r\nPlease let me know how you would like me to proceed.", "@drpngx @zheng-xq While running ./tensorflow/tools/ci_build/ci_sanity.sh locally, I get\r\n\r\n> FAIL: buildifier found errors and/or warnings in above BUILD files.\r\n> buildifier suggested the following changes:\r\n> ./tensorflow/tools/ci_build/ci_sanity.sh: line 285: buildifier: command not found\r\n\r\nSo, I can't iterate locally to address this. \r\n\r\nMoreover, I seem to be getting another set of messages locally that the execution of the sanity check script doesn't seem to produce for you on Jenkins:\r\n\r\n> FAIL: mismatch in packaged licenses and external dependencies\r\n> Please remove the licenses for the following external dependencies:\r\n> //third_party/hadoop\r\n> @boringssl//\r\n> @curl//\r\n> @jemalloc//\r\n\r\nThese don't seem to show up in your CI build.\r\n\r\nPlease let me know what I can do to make it work.\r\n\r\n\r\n\r\n", "You can do the link check with our script.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh#L84\r\n\r\nIt's basically asking you to sort.", "@drpngx Ah ok, great! Thank you! I'll check this ASAP.", "In addition, did you include the change to flip the default? It's okay to leave it as a different CL if you want. \r\n\r\nAlso we need to enable more tests on GPU by setting this to True: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_test.py#L302\r\n\r\nYou can test the GPU version with --config=cuda, and the CPU version without --config=cuda. ", "@zheng-xq Yes, I changed the default back to the unfused kernels, so now the env var is needed again, like it used to be. I'll make the change in the test as well. Thanks!", "@zheng-xq @drpngx I'm running CI now and making a few last changes (specifically [this one](https://github.com/tensorflow/tensorflow/pull/6818#discussion_r97458403)), so please don't review or merge yet. I'll make a comment here when the code is ready for final review, build and merge.", "@zheng-xq Could you review this? Also, seems like Linux tests have failed due to some networking problem (see [here](https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/3603/console)). Could you have a look? Thanks!", "@tensorflow-jenkins test this please", "@mkolod we're swamped because of the dev summit tomorrow -- XQ will have more time afterwards :)", "@vrv No worries! See you at the summit! :)", "@zheng-xq Could you review it? Thanks! :)", "Please ping me if and when you'd like me to look at this again.", "@mkolod Ping, it looks like this is really close to merging. Also, there's a conflict to resolve now.", "Thanks @dandelionmane, will make changes, time permitting.", "@zheng-xq Any remaining concerns? Regarding formatting, I ended up using `clang-format --fallback-style=\"Google\" < input.cc` so I hope that is enough to resolve formatting. I think I addressed all the other concerns for the last review as well.", "Hm, didn't mean to approve globally.  Definitely still need XQ's sign-off.", "@zheng-xq friendly ping", "FYI as a general rule, we shouldn't expect reviewers to review code with less latency than it takes reviewees to respond to comments.  If nothing else, this encourages people to respond to comments quickly, which saves reviewer time, letting them keep reviews in cache.", "LGTM.\r\n\r\nThanks for the contribution!"]}, {"number": 6817, "title": "Add CUDA implementation of fused hue adjustment", "body": "Currently, the fused (rgb2hsv->hue_adjust->hsv2rgb) hue adjustment kernel exists in TF [for the CPU](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/adjust_hue_op.cc), but not for the GPU. This is also why the CPU path can only be enabled \"secretly\" (via [setting the TF_ADJUST_HUE_FUSED env var](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py#L1113) to true). Adding the CUDA kernel would make this op available for both CPUs and GPUs.", "comments": ["Mentioning @zheng-xq since he implemented the CPU version. I will be submitting a PR for this issue soon so feel free to assign me.", "Looks like PR #6818 has fixed this?\r\nI am closing this issue but feel free to reopen if it is still not resolved."]}, {"number": 6816, "title": "Intermittent CUDA_ERROR_ILLEGAL_ADDRESS errors during back propagation for some networks when using XLA.", "body": "### Summary\r\nIn playing around with the XLA JIT compiler I've experienced intermittent CUDA_ERROR_ILLEGAL_ADDRESS errors during back propagation for some network structures. While other structures may also fail, I can confirm that this happens with simple fully-connected networks when dropout is applied to the hidden units. I have not, however, observed errors when not using XLA or when using XLA, but no hidden dropout.\r\n\r\n### Environment info\r\n- Operating System: Ubuntu 14.04 LTS\r\n- GPU: Titan X\r\n- NVIDIA driver: 375.26\r\n- CUDA: 8.0\r\n- cuDNN: 5.1\r\n- see also: [cuda_libs.txt](https://github.com/tensorflow/tensorflow/files/703021/cuda_libs.txt)\r\n-  bazel: 4.3\r\n- TensorFlow: f8fcd85b8151d287ad09a9323fabf1c5025775b7\r\n- XLA: Yes\r\n- TensorFlow compile command:\r\n\r\n    `bazel build --copt=-march=native -c opt  --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n    \r\n\r\n### Minimal reproducible example \r\nSee https://gist.github.com/nryant/8cea9bb79d7bdb965167e917d8d5aa8b for the script `test_dropout.py`, which repeatedly builds a small network consisting of a single hidden layer of 512 units and performs 100 fprop/bprop steps. It accepts three command line arguments:\r\n\r\n- `--bprop` enables backpropagation (by default, only forward propagation is performed)\r\n- `--xla` enables XLA at the session level\r\n- `--dropout` specifies the dropout proportion\r\n\r\nAlso see log files produced from running the script with various arguments:\r\n\r\n- No XLA, fprop + bprop, and dropout: https://gist.github.com/nryant/83829ada37a9b0c221a76184c5e232fa#file-noxla_fprop_bprop_dropout-log\r\n- XLA and fprop: https://gist.github.com/nryant/83829ada37a9b0c221a76184c5e232fa#file-xla_fprop-log\r\n- XLA, fprop, and dropout: https://gist.github.com/nryant/83829ada37a9b0c221a76184c5e232fa#file-xla_fprop_dropout-log\r\n- XLA, fprop + bprop, and dropout: https://gist.github.com/nryant/83829ada37a9b0c221a76184c5e232fa#file-xla_fprop_bprop_dropout-log\r\n\r\nThe only combination for which we ever observe an error is XLA, fprop + bprop, and dropout, which pretty reliable blows up in the first few iterations.", "comments": ["@jlebar Mind taking a look at this compiler/xla issue report? ", "Of course, I'll see what I can figure out here.  Thank you for the detailed bug report, Neville.", "Sorry for the long wait; I had a fire to put out elsewhere.\r\n\r\nI was able to reproduce the failure locally at f8fcd85 and at current HEAD, 1084748efa.\r\n\r\nNothing more to report, beyond that I'm looking at it now.", "Running this through cuda-memcheck, I'm seeing a bunch of errors of the form\r\n\r\n    ========= Invalid __global__ read of size 4\r\n    =========     at 0x000000b8 in _copy\r\n    =========     by thread (13,0,0) in block (866,0,0)\r\n    =========     Address 0x3bdac0c83cce9646 is misaligned\r\n\r\nThat address is indeed misaligned.\r\n\r\nThe body of the _copy kernel (which you can get by setting the TF_CPP_MIN_VLOG_LEVEL env var to 2) is\r\n\r\n\t.visible .entry _copy(\r\n\t\t.param .u64 _copy_param_0,\r\n\t\t.param .u64 _copy_param_1,\r\n\t\t.param .u64 _copy_param_2\r\n\t)\r\n\t.maxntid 32, 1, 1\r\n\t{\r\n\t\t.reg .pred \t%p<2>;\r\n\t\t.reg .b32 \t%r<6>;\r\n\t\t.reg .b64 \t%rd<10>;\r\n\r\n\t\tmov.u32 \t%r1, %ctaid.x;\r\n\t\tmov.u32 \t%r2, %tid.x;\r\n\t\tshl.b32 \t%r3, %r1, 5;\r\n\t\tor.b32  \t%r4, %r3, %r2;\r\n\t\tsetp.lt.u32 \t%p1, %r4, 401408;\r\n\t\t@%p1 bra \tLBB1_2;\r\n\t\tbra.uni \tLBB1_1;\r\n\tLBB1_2:\r\n\t\tld.param.u64 \t%rd4, [_copy_param_0];\r\n\t\tld.param.u64 \t%rd5, [_copy_param_1];\r\n\t\tcvta.to.global.u64 \t%rd1, %rd5;\r\n\t\tcvta.to.global.u64 \t%rd2, %rd4;\r\n\t\tcvt.u64.u32 \t%rd3, %r4;\r\n\t\tld.global.u64 \t%rd6, [%rd2+8];\r\n\t\tshl.b64 \t%rd7, %rd3, 2;\r\n\t\tadd.s64 \t%rd8, %rd6, %rd7;\r\n\t\tld.u32 \t%r5, [%rd8];\r\n\t\tadd.s64 \t%rd9, %rd1, %rd7;\r\n\t\tst.global.u32 \t[%rd9], %r5;\r\n\tLBB1_1:\r\n\t\tret;\r\n\t}\r\n\r\nThere's only one 32-bit read in this kernel, `ld.u32 %r5, [%rd8]`, so presumably that's the bad one.\r\n\r\n`rd8` is the sum of `%rd3 << 2` and a value read from memory.  Clearly `%rd3 << 2` is aligned, so the problem must be that the value we're reading from memory is not.\r\n\r\nNot sure yet where this bad value in memory is coming from.\r\n\r\n(Incidentally, we should be able to figure out somehow that rd8 is in the global addr space and emit `ld.global.u32`.  But that's just an efficiency problem, not one of correctness.)", "Although _copy has three arguments, we seem to be passing only two when we call it.  (Maybe this is intentional; the kernel never uses the third arg.)  But in addition, those two args are the same pointer.  That...does not seem intentional.", "Fixed in 081758b0e5efc1a1591cda068a4866099bf8a3c5.\r\n\r\nThanks for your patience, and thanks again for the high-quality bug report.  (As you can see from the commit, we actually had this in hand six days ago, but for unrelated reasons it took until today before we had a successful uplift to github.)"]}, {"number": 6815, "title": "keep_dims vs keepdims", "body": "There are 15 results for [keepdims](https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=keepdims) and 77 results for [keep_dims](https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=keep_dims&type=Code) in TF codebase, any chance this can be made consistent in 1.0? Ideally TF would just match numpy API and use `keepdims` @martinwicke @karpathy\r\n\r\n", "comments": ["@aselle for comment on whether it's a feasible change", "Yes, it's possible, probably it would be handled like axis where we keep the old argument around, but make keepdims the defacto version. It's too late to remove the old arg  for 1.0, but we can definitely make both work and the document be keepdims.\r\n", "Cool, having alias for `keepdims` would solve the user annoyance problem", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "The name \"keepdims\" is a violation of PEP-8's naming convention [https://www.python.org/dev/peps/pep-0008/#method-names-and-instance-variables], thus a violation of TensorFlow's style guide\r\n\r\n> Method Names and Instance Variables\r\n>\r\n> Use the function naming rules: lowercase with words separated by underscores as necessary to improve readability. \r\n\r\nNow one has to write code in inconsistent style like\r\n\r\n```\r\ntf.reduce UNDERSCORE max(..., keep NO_UNDERSCORE dims=True)\r\n```\r\n\r\nto avoid a deprecation warning.\r\n\r\nGiven that keep_dims had been the majority in TF's code base, wouldn't also accepting keepdims (while issuing a warning to suggest \"keep_dims\" instead) be a better solution?\r\n  ", "I am not sure why this was modified in the first place? `keep_dims` is perfectly good.", "We modified it to conform to the numpy API, which for most users is much more relevant than PEP8. \r\n\r\nWe could talk about supporting both (without deprecation warning), or even pushing the keepdims support into a tf.numpy compatibility module. I'd much rather just have one though. This bug is about that, make sure that one can use a single version throughout.\r\n\r\n@saxenasaurabh FYI.", "Just saying that `keepdims` - although being used in numpy - is just a really bad name and `keep_dimensions` or `keep_dims` (if you insist on non-obvious abbreviations) is much better readable (which I would give the highest priority in an API). I would rather keep the good name, and try to change the numpy parameter.", "Btw, `tf.nn.moments` still uses `keep_dims` in latest master. ", "Can we fix the inconsistency? `tf.nn.moments` and many other functions in `tf.nn` still uses `keep_dims`. @martinwicke ", "The plan is to fix it in 2.0. I'm open to PRs changing to keepdims any time."]}]