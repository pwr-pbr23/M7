[{"number": 20294, "title": "[Intel MKL] Added instructions to build Tensorflow with Intel MKL DNN.", "body": "", "comments": ["Thanks!"]}, {"number": 20293, "title": "transfer learning: /scripts/label_image.py\", line 137, in <module>     print(template.format(labels[i], results[i])) IndexError: list index out of range", "body": "In Tensorflow 1.8, Ubuntu 16.04, Anaconda Python 3.6.5, I have:\r\n\r\n```\r\nmona@Mona:~/code/alternative-controllers/style_transfer$ python -m scripts.label_image     --graph=dataset/retrained_graph.pb      --image=dataset/charmander/charmander30.jpg \r\n/home/mona/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-06-25 18:51:53.814201: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-06-25 18:51:53.934034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.90GiB freeMemory: 10.57GiB\r\n2018-06-25 18:51:53.934061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-25 18:51:54.128693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-25 18:51:54.128723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-06-25 18:51:54.128728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-06-25 18:51:54.128937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10233 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-06-25 18:51:54.240643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-25 18:51:54.240673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-25 18:51:54.240678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-06-25 18:51:54.240682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-06-25 18:51:54.240766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10233 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n\r\nEvaluation time (1-image): 0.752s\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mona/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/mona/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/mona/code/alternative-controllers/style_transfer/scripts/label_image.py\", line 137, in <module>\r\n    print(template.format(labels[i], results[i]))\r\nIndexError: list index out of range\r\n```\r\n\r\n\r\nI was successfully able to follow the tutorial for transfer learning for flowers from Google Collab, but then I tried it on Pokemon images of 200*200 and set the image size to 224 (because it did not accept the 200), I was able to pretrain but inference does not work. Can you please guide how to fix?\r\n\r\nI got the pokemon images from:\r\nhttps://github.com/AravindVasudev/Pokedex\r\n\r\nand I followed this codelab tutorial for transfer learning:\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#8\r\n\r\nAs you can see the retrained_labels.txt and retrained_graph.pb are already made:\r\n\r\n```\r\nmona@Mona:~/code/alternative-controllers/style_transfer$ ls -ltra dataset/\r\ntotal 5524\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 alakazam\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 abra\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 bulbasaur\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 blastoise\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 articuno\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 magikarp\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 ivysaur\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 gengar\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 charmeleon\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 charmander\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 charizard\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 squirtle\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 pikachu\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 moltres\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 mewtwo\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 mew\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 meowth\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 zapdos\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 wartortle\r\ndrwxrwxr-x  2 mona mona    4096 Jun 25 18:41 venusaur\r\ndrwxr-xr-x  4 mona mona    4096 Jun 25 18:45 training_summaries\r\ndrwxrwxr-x  3 mona mona    4096 Jun 25 18:45 models\r\ndrwxrwxr-x 22 mona mona    4096 Jun 25 18:45 bottlenecks\r\n-rw-rw-r--  1 mona mona     168 Jun 25 18:45 retrained_labels.txt\r\n-rw-rw-r--  1 mona mona 5548864 Jun 25 18:45 retrained_graph.pb\r\ndrwxrwxr-x 25 mona mona    4096 Jun 25 18:45 .\r\ndrwxrwxr-x  7 mona mona    4096 Jun 25 18:51 ..\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@monajalal The line [73](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/label_image.py#L73) in [label_image.py](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/label_image.py) is `label_file = \"tf_files/retrained_labels.txt\"`. So, if you don't pass `--labels` flag, it refers to label file of flowers dataset. Your command should be \r\n```\r\npython -m scripts.label_image --graph=dataset/retrained_graph.pb --image=dataset/charmander/charmander30.jpg --labels=dataset/retrained_labels.txt\r\n```\r\nYou are getting `list index out of range` because the flowers dataset has 5 labels and I can see that you have 20."]}, {"number": 20292, "title": "Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel''", "body": "So, I downloaded whole repository and I tried to run the 'android' application:\r\ntensorflow-master/tensorflow/examples/android\r\nbut I get the runtime error from title.\r\n\r\nthere is this line of code in build.gradle:\r\ndef nativeBuild='bazel';\r\nand when I change it, for example, to 'none', I get problem with camera.\r\nHowever, I would like to solve this bazel buildSystem because it seems that is default.\r\n\r\n**Top-level directory of the model I am using**\r\nThis is path to application:\r\nhome/AndroidStudioProjects/tensorflow-master/tensorflow/examples/android\r\nEverything is set to default\r\n\r\n**Have I written custom code**\r\nNo\r\n\r\n**OS Platform and Distribution**\r\nUbuntu 16.04\r\n\r\n**TensorFlow installed from**\r\nI installed TensorFlow by issuing pip commands\r\n\r\n**TensorFlow version**\r\n1.5.0\r\n\r\n**Bazel version**\r\nBuild label: 0.13.0\r\n\r\n**CUDA/cuDNN version**\r\nCuda compilation tools, release 7.5, V7.5.17\r\n\r\n**GPU model and memory**\r\nIntel Corporation 4 Series Chipset Integrated Graphics Controller (rev 03) (prog-if 00 [VGA Controller])\r\nSubsystem: Dell 4 Series Chipset Integrated Graphics Controller\r\n4 GB\r\n\r\nThanks in advance", "comments": ["**Solved:**\r\nLocation of my Bazel wasn't set properly", "Nagging Assignee @rohan100jain: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20291, "title": "Cherry-pick release-notes update back to master.", "body": "PR #20289, cherry-picked to master.\r\n\r\ngit cherry-pick 2897538 56fba15 ce03a10", "comments": ["Thank you!"]}, {"number": 20290, "title": "1.9-cherry-pick-request: Docs updates for getting started and guide rename", "body": "Cherry-picks include:\r\n* Getting started index page: df2c831\r\n* Rename programmers_guide/ dir to guide/: b0f2eee\r\n* Updated TOC pages: 4e0b161, d9e006e\r\n\r\nWe need the release of tf.org to reflect the new material. Also need to make sure people are editing the right place for the guide.", "comments": []}, {"number": 20289, "title": "r1.9: release notes Update.", "body": "I haven't submitted this to master... should I?\r\n\r\n- link to new get_started.\r\n- Add keras CuDNN layers.\r\n- Links for gradient boosted estimators.\r\n- Added new contrib-estimators and string-processing.\r\n- Bumped some minor sounding things down from \"Major\" to \"Bugfix+Other\"", "comments": ["Would appreciate all changes to be submitted to master and then cherry-picked over to release branches to keep things simpler. Please submit these changes to master branch when you get the chance. ty!", ">Please submit these changes to master branch when you get the chance.\r\n\r\nDone:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/20291"]}, {"number": 20288, "title": "Revert \"[StreamExecutor] Merge StreamExecutor's and XLA's StatusOr classes.", "body": "This reverts commit e7b1ab049d22119c7b649046be853ea88120f27a.", "comments": []}, {"number": 20287, "title": "update the py/py3 toolchain config", "body": "- The python3 are installed in `/opt/python3.6` in the base toolchain\r\ncontainer:\r\nhttps://console.cloud.google.com/launcher/details/google/rbe-ubuntu16-04\r\n\r\n- Also updated the corresponding python2 `numpy` library locations. \r\n\r\n- Both py2 and py3 config tested. ", "comments": ["+ @nlopezgi on reviewing python config changes. ", "@nlopezgi I've fond some internal builds are still using the old containers. I am sending CL internally to update them to use the latest container. ", "@erain let us know when we are ready to proceed with this change. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "So sorry to miss this PR. With several CLs we submitted internally, I believe now this PR is ready to submit. ", "@yifeif this PR can now be merged if possible. Thanks!"]}, {"number": 20286, "title": "java.lang.RuntimeException: Fail to connect to camera service", "body": "So, I downloaded whole repository and I tried to run the 'android' application:\r\ntensorflow-master/tensorflow/examples/android\r\nbut I get the runtime error from title, because the old camera API is deprecated.\r\n\r\nHowever, I couldn't find elegant solution to this problem.\r\nImplementing Camera2 API seems very complicated, because modifying all existing code to match Camera2 seems like quite complicated solution.\r\nIs there a fix?\r\n\r\n**Top-level directory of the model I am using**\r\nThis is path to application:\r\nhome/AndroidStudioProjects/tensorflow-master/tensorflow/examples/android\r\nEverything is set to default\r\n\r\n**Have I written custom code**\r\nNo\r\n\r\n**OS Platform and Distribution**\r\nUbuntu 16.04\r\n\r\n**TensorFlow installed from**\r\nI installed TensorFlow by issuing pip commands\r\n\r\n**TensorFlow version**\r\n1.5.0\r\n\r\n**Bazel version**\r\nBuild label: 0.13.0\r\n\r\n**CUDA/cuDNN version**\r\nCuda compilation tools, release 7.5, V7.5.17\r\n\r\n**GPU model and memory**\r\nIntel Corporation 4 Series Chipset Integrated Graphics Controller (rev 03) (prog-if 00 [VGA Controller])\r\nSubsystem: Dell 4 Series Chipset Integrated Graphics Controller\r\n4 GB\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Hello. \r\nI am not quite sure about what do you mean by exact command to reproduce.\r\nCould you, please, explain what exactly do you mean?\r\n\r\n\r\nI didn't run any commands, I just tried to run the app from android studio.\r\nCheers", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hello. I didn't solve the problem, but I changed the approach and now I\nhave other problems  :D\nfeel free to close this.\nSorry and thank you\n\nOn Thu, Jul 26, 2018 at 9:21 PM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> Nagging Assignee @bignamehyp <https://github.com/bignamehyp>: It has been\n> 30 days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20286#issuecomment-408205254>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AROI6V7DTZv31pR1zCa7AStfKAWd8m1Oks5uKhazgaJpZM4U2oar>\n> .\n>\n"]}, {"number": 20285, "title": "Add more tests in tf_trt_integration_test.py.", "body": "Changes include:\r\n- Consolidate tf_trt_integration_test.py\r\n- Remove test_tftrt.py\r\n- Fix a resource leaking issue when converting calibration graph to inference graph", "comments": ["Very good @aaroey, Thanks for the changes and sorry for delaying it so long, forcing you do it."]}, {"number": 20284, "title": "Enable unbundling dependencies and linking to the system libraries instead.", "body": "This series adds a framework to be able to unbundle the dependencies from tensorflow. Previously, bazel rebuilds every single dep from scratch. This allows distro packages to disable bundling on a per-dep basis and link with the libraries that already exist on the system.\r\n\r\nFor more information why deps should be unbundled, see these links: https://wiki.gentoo.org/wiki/Why_not_bundle_dependencies\r\nhttps://fedoraproject.org/wiki/Bundled_Libraries\r\n\r\nThe deps I have unbundled so far work on my Gentoo machine. Thanks to dennisjenkins@google.com for testing this on Gentoo as well. There are still more that need to be unbundled but this covers enough to be useful already. This has not been tested on any other distro than Gentoo but I don't forsee any big incompatibilities.\r\n\r\nThe libs that are unbundled are configured by setting `build --action_env TF_SYSTEM_LIBS=\"comma,sep,list\"`\r\neg:\r\n```\r\nbuild --action_env TF_SYSTEM_LIBS=\"com_googlesource_code_re2,nasm,jpeg,png_archive,org_sqlite,gif_archive,six_archive,astor_archive,termcolor_archive,pcre,swig,curl,grpc,lmdb,zlib_archive,snappy,flatbuffers,cython,jemalloc\"\r\n```\r\n\r\nOnce configured, the tarball for the dep will not be downloaded or extracted, and the BUILD file will be swapped to the system_build_file instead which contains different rules to compile and link against the system package.\r\nThere are also macros if_system_lib(name, a, b) and if_not_system_lib(name, a, b) to configure other things in the build.\r\n\r\nDO NOT MERGE THIS YET. I'm opening this for comments are review here before its ready for merging.", "comments": ["@angersson so you can make sure this doesn't conflict with your changes.\r\n@meteorcloudy can you take a look at the bazel bits?", "@jart Can you have a look at this as well?\r\n\r\nThe gist of this looks good to me, but I'm not certain of the implications for other systems, etc. I'm working on updating my internal proposal to better work with this.", "I pushed an update:\r\n\r\n* disabled the syslibs_configure stuff on windows.\r\n* fixed the search path in cython.BUILD.\r\n* rebased on to latest master.\r\n* master had bumped jsoncpp too so I added a commit to unbundle that as well. \r\n* grpc.BUILD added targets for the secure libs too.\r\n\r\nThe header file paths for jsoncpp are different from the system paths so I had to symlink them. bazel complains when I tried doing includes=[\"/usr/include/jsoncpp\"]. Maybe in a future bazel version we can add some configs to allow non- hermetic builds using the system headers or searching pkg-config or something.\r\n\r\nI've run this patchset in the gentoo ebuild both bundled and unbundled and its working so far :)", "Thank you for working on this!  I've been trying to use this for Anaconda's packages, too.  I'm having some difficulty with the hardcoding of paths, such as with cython: https://github.com/tensorflow/tensorflow/pull/20284/files#diff-c8176cf1a58a40560176239b30ef7e52R8\r\n\r\nIs there a good way to have that be more dynamic?  For example, all of our build paths are relative to an environment variable, ``$PREFIX``.  For you, this is probably ``/usr`` or maybe ``/usr/local``.  For us, it can be anything, and essentially never ``/usr``.  Perhaps we could use ``$PREFIX`` in the build files, and have it fall back to a default of ``/usr`` when it is not defined?", "I have pushed sci-libs/tensorflow-1.9.0_rc1-r2 to the gentoo tree with these patches. the system-libs USE-flag enabled the system ones, disabling the flag uses the bundled stuff like before in case there are any issues. Hopefully I dont get too many bugs filed because of it :D \r\n", "@msarahan yeah, the prefixing is something that I forsaw as problematic. Normal gentoo uses $ROOT=/, but we definitely do support both cross-compiling (eg $ROOT=/usr/arm-linux-gnu/) and also installing things as a whole alongside another distro/machine/whatever (eg $PREFIX=/home/jason/root/). Currently those cases are probably also broken. \r\n\r\nI was thinking to write some simple scripts like find-binary.sh and find-python.sh and change the genrule()'s to use those instead of directly cp'ing fixed paths. Once there are scripts then it would be quite easy to add extra logic per-distro or to search pkg-config for whatever flags. Not every distro puts binaries in the same place too so scripts would be more robust.\r\n\r\nAbout $PREFIX specifically, bazel strips basically everything from the environment currently. There are issues filed with the bazel team and it will eventually more variables. In the mean time you'd need to add something like build --action_env=PREFIX.\r\n\r\nThis patch set is already getting pretty large so I think we should get this basic functionality merged in then later patchsets will be smaller and much easier to review :) Other than editing some paths in third_party/systemlibs/*.BUILD, has the rest mostly been working for you? If some of the core bits fail horribly for your case, that would probably warrant fixing first. You can look at the gentoo ebuilds for inspiration too but they aren't pretty yet: https://github.com/gentoo/gentoo/tree/master/sci-libs/tensorflow\r\n", "Thanks @perfinion.  The problem with hardcoded paths at all is that we don't really use them for conda packages.  $PREFIX points to some path that looks like /usr or /usr/local (with include, bin, and lib subdirs), but it is not a fixed path.  For example, during a package build, it will be something like  \r\n\r\n``/home/msarahan/miniconda3/conda-bld/tensorflow_(timestamp)/_h_env_(pad)``\r\n\r\nwith (pad) representing enough placeholder characters to get us to a 255 character padding for any fixed paths that get baked into binaries.  That's essential to relocatability.\r\n\r\nNot having dynamic paths is a pretty big non-starter for us.\r\n\r\nAdding --action_env=PREFIX is definitely something I knew we'd need to use - I'm just curious about how to incorporate that into the BUILD files.\r\n\r\nTo put headers and libs onto the search paths, I first symlinked my $PREFIX var in my build script to a location within the Tensforflow third_party folder, and added that folder to the include and link paths:\r\n\r\n```\r\nlicenses([\"notice\"])  # BSD/MIT-like license\r\n\r\nfilegroup(\r\n    name = \"LICENSE\",\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n\r\ncc_library(\r\n    name = \"png\",\r\n    copts = [\"-Ithird_party/systemlibs/include\"],\r\n    linkopts = [\"-lpng\", \"-Lthirdparty/systemlibs/lib\"],\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n```\r\n\r\nThat worked OK, seemingly.  Next I was hung up on the hard-coded paths as I mentioned before.  I began to try something a little different, based on what I found at https://stackoverflow.com/questions/43928653/call-llvm-config-prefix-and-use-it-in-a-build-rule/43936381#43936381.\r\n\r\nI removed the BUILD files completely, and instead tried to generate them more dynamically:\r\n\r\n```\r\ndef _curl_impl(repository_ctx):\r\n  prefix_path = repository_ctx.os.getenv(\"PREFIX\", \"/usr\")\r\n  repository_ctx.symlink(prefix_path, \"external_prefix\")\r\n  repository_ctx.file(\"curl.BUILD\", \"\"\"\r\nlicenses([\"notice\"])  # MIT/X derivative license\r\n\r\nfilegroup(\r\n    name = \"COPYING\",\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n\r\ncc_library(\r\n    name = \"curl\",\r\n    copts = [\"-Iexternal_prefix/include\"],\r\n    linkopts = [\"-lcurl\", \"-Lexternal_prefix/lib\"],\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n\"\"\")\r\n```\r\n\r\nI have not gotten this working yet, but I think it might work.  It would still require --action_env=PREFIX, but that's fine with me.  Any thoughts on the viability of this approach?  It need not be part of this PR, if it is better to get this initial work in and then adapt.", "PS: for the executables that needed to be copied, I symlinked after using which to find them:\r\n\r\n```\r\ndef _cython_impl(repository_ctx):\r\n  cython_path = repository_ctx.which(\"cython\")\r\n  repository_ctx.file(\"cython.BUILD\", \"\"\"\r\nlicenses([\"notice\"])  # Apache-2.0\r\n\r\ngenrule(\r\n    name = \"lncython\",\r\n    outs = [\"cython.py\"],\r\n    cmd = \"ln -s {} $@\",\r\n)\r\n\r\n# May not be named \"cython\", since that conflicts with Cython/ on OSX\r\nsh_binary(\r\n    name = \"cython_binary\",\r\n    srcs = [\"cython.py\"],\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n\"\"\".format(cython_path))\r\n```", "@msarahan for cython specifically, it used to prefix it with $PYTHON_BIN_PATH which made things not work on gentoo since we have a special thing to select which python version is the default. that has been reverted so looks like its really easy now.\r\n\r\nYou dont need to bother with any of the generating stuff, the cmd= and copts and linkopts can all do make and bash variable expansion.\r\nhttps://docs.bazel.build/versions/master/be/make-variables.html#make-var-substitution\r\nhttps://docs.bazel.build/versions/master/be/common-definitions.html#sh-tokenization\r\n\r\nYou should be able to have just cython.BUILD with this:\r\n```\r\ngenrule(\r\n    name = \"lncython\",\r\n    outs = [\"cython.py\"],\r\n    cmd = \"ln -s $$(which cython) $@\",\r\n)\r\n\r\nsh_binary(\r\n    name = \"cython_binary\",\r\n    srcs = [\"cython.py\"],\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n```\r\n\r\nAlternatively, in .bazelrc set `build --define PREFIX=/usr` then use `cmd = \"ln -s $(PREFIX)/bin/cython\"`\r\nThe problem is if PREFIX is not defined it fails, I havent figured out yet how to make it default to a blank value.\r\n\r\nputting an `echo $${PREFIX}` inside cmd= does not work either but I think it should. That would be the best since we could use bash defaulting to do eg: \"${PREFIX:-/usr}/bin/cython\"\r\n\r\nIn copts and linkopts you can probably do \"-I$(PREFIX)/include\".\r\n", "Seems very close to working, but I'm confused on an error with the external grpc.  I have put up my patch (I am applying it to 1.9.0rc1 right now): https://gist.github.com/msarahan/e38ccd45521617356a099d50b172f2b8\r\n\r\nThe error is:\r\n\r\n```\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (149 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (149 packages loaded)\r\nERROR: /Users/msarahan/miniconda3/conda-bld/tensorflow-base_1530276141984/work/tensorflow/core/distributed_runtime/rpc/BUILD:97:1: no such package '@grpc//': tf_http_archive rule //external:grpc must create a directory and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_channel'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@grpc//': tf_http_archive rule //external:grpc must create a directory\r\nINFO: Elapsed time: 9.950s\r\n```\r\n\r\nMy build script calls bazel with: \r\n\r\n```\r\n     bazel ${BAZEL_OPTS} build \\\r\n           --verbose_failures \\\r\n           --config=opt \\\r\n           --action_env TF_SYSTEM_LIBS=\"${TF_SYSTEM_LIBS}\" \\\r\n           --define PREFIX=\"${PREFIX}\" \\\r\n           //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nBefore I was passing in --define as you recommended, it was dying in earlier externals, such as png.  That gives me confidence in the overall approach, but it seems I have something wrong.  If I remove grpc from the list of externals, it gets further, but dies on missing the license from LMDB.  It should not be necessary to list actual files in the external stuff, right?", "@msarahan I rebased everything and also added the $(which foo) stuff\r\n\r\nabout your grpc error: its probably cuz you need to use double $$'s here:\r\ncmd = \"ln -s $(which grpc_python_plugin) $@\",\r\n\r\nI added those parts tho already so hopefully it should work if you just add the linkopts and includes.\r\nI wonder if its easier for now to just add the linkopts and copts globally in the bazelrc?", "I think I'm satisfied now with this patchset so far. It should be ready for review then merging when the workflow rejig is done.\r\n\r\nI've pushed the older version with the gentoo _rc1-r2 ebuild. The _rc2 ebuild with the latest patches will be pushed to gentoo once some of the deps are bumped too.", "Can you fix the buildifier failure?", "@perfinion Thanks for keeping up with updating this!\r\n\r\nWould you mind commenting with some commands to demonstrate how to test this on Ubuntu? It'll double as documentation for later. I'm far from an expert on builds, and as a user I wouldn't know how to go from 'jpeg is one of the packages that can come unbundled' to 'I need to `apt install libjpeg-dev` and then provide `--build=...`.", "@martinwicke I rebased and fixed up the style change buildifier wanted. can you re-trigger the tests? :)", "@angersson I'm not 100% sure on the package names on ubuntu but give this a shot.\r\n\r\nTo build tensorflow using the system libs for jpeg and zlib do:\r\n```\r\n$ apt-get install libjpeg-turbo8 libjpeg-tubo8-dev zlib1g zlib1g-dev\r\n$ bazel clean --expunge\r\n$ ./configure # answer everything normally\r\n$ echo 'build --action_env TF_SYSTEM_LIBS=\"jpeg,zlib\"' >> .tf_configure.bazelrc\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package \\\r\n//tensorflow:libtensorflow_framework.so \\\r\n//tensorflow:libtensorflow.so \\\r\n//tensorflow:libtensorflow_cc.so\r\n```\r\n\r\n----\r\n\r\nTo decide which libs can be unbundled, \r\n- Look in the `tensorflow/workspace.bzl` file.\r\n- The entries that have a `system_build_file` should work.\r\n- Add the `name` to the `TF_SYSTEM_LIBS` line.\r\n- Make sure the library and header files for that package are installed in the system. The package names will differ per-distro (eg some might need to install the -dev package as well)\r\n- If ubuntu has new enough versions of all the libs, unbundling all of them so far should work in theory but I have only tested on Gentoo so far.\r\n- PREFIX installs are not easily possible yet. I wanted the basic way working first. I also need to find a good way to default the variable if not set. I will probably add stuff to ./configure to streamline the whole process but for now the beginning steps are still manual.\r\n\r\n----\r\n\r\nTo unbundle a new library:\r\n- Look in the bundled BUILD file to see which targets must be exported so tensorflow can use\r\n- Write a new foo.BUILD file with the same targets as cc_library but linking to the system lib.\r\n- Add the foo.BUILD file in third_party/systemlibs/foo.BUILD\r\n- Add the `system_build_file=` key to `tensorflow/workspace.bzl` pointing to foo.BUILD\r\n- Add the new entry in `VALID_LIBS` in `third_party/systemlibs/syslibs_configure.bzl`\r\n- Do the above building steps to test, adjust foo.BUILD, repeat.\r\n\r\nUsing the basic cc_library rules is a bit cumbersome but it does work. I'd love for bazel to have some special rules that make system stuff easier. I realize it goes against bazels hermetic philosophy but I think its one hurdle that stops bazel from being useful in a lot of other places. The system rules could be gated behind a flag perhaps. And I can still see mostly-hermetic builds with bazel as being positive because it has all the extra header validation and dep checking that other systems don't have.\r\n\r\nI should probably write more detail about this on my blog too.\r\n", "Would you be able to put something like this in your ebuild file instead?\r\n\r\n```sh\r\nsed -i '\r\n    /tf_http_archive(/!b\r\n    :a\r\n    N\r\n    /)$/!ba\r\n    /name = \"curl\"/bb\r\n    /name = \"gif_archive\"/bb\r\n    /name = \"jpeg\"/bb\r\n    /name = \"nasm\"/bb\r\n    /name = \"png_archive\"/bb\r\n    /name = \"snappy\"/bb\r\n    /name = \"zlib_archive\"/bb\r\n    b\r\n    :b\r\n    s/\\( *\\).*\\(name = \"[^\"]*\"\\).*/\\1native.local_repository(\\2, path=\".system\")/\r\n  ' tensorflow/workspace.bzl\r\n\r\nmkdir -p .system\r\ntouch .system/WORKSPACE\r\nln -sf $(command -v curl) .system/curl_bin\r\nln -sf $(command -v nasm) .system/nasm\r\nfor f in LICENSE LICENSE.txt LICENSE.md COPYING COPYING.txt; do\r\n  echo 'dynamically linked via system' >.system/$f\r\ndone\r\n\r\ncat >.system/BUILD <<EOF\r\npackage(default_visibility=[\"//visibility:public\"])\r\nexports_files(glob([\"*\"]))\r\ncc_library(name=\"curl\", linkopts=[\"-lcurl\"])\r\ncc_library(name=\"gif\", linkopts=[\"-lgif\"])\r\ncc_library(name=\"jpeg\", linkopts=[\"-ljpeg\"])\r\ncc_library(name=\"png\", linkopts=[\"-lpng\"])\r\ncc_library(name=\"zlib\", linkopts=[\"-lz\"])\r\ncc_library(name=\"snappy\", linkopts=[\"-lsnappy\"])\r\nEOF\r\n```\r\n\r\n```sh\r\n$ cd tensorflow\r\n$ run-script-above.sh\r\n$ bazel build //tensorflow:libtensorflow_framework.so\r\n$ ldd bazel-bin/tensorflow/libtensorflow_framework.so | awk '{print $1}'\r\nlinux-vdso.so.1\r\nlibdl.so.2\r\nlibm.so.6\r\nlibsnappy.so.1\r\nlibz.so.1\r\nlibpthread.so.0\r\nlibgif.so.7\r\nlibjpeg.so.62\r\nlibpng16.so.16\r\nlibstdc++.so.6\r\nlibgcc_s.so.1\r\nlibc.so.6\r\n/lib64/ld-linux-x86-64.so.2\r\n```\r\n\r\nBazel seems to allow this hackishness, even with the sandboxing features enabled.", "@jart I don't follow. That appears to work, yeah, but I don't get why?\r\n\r\nDo you mean put it in the gentoo ebuild instead of upstreaming things? Then other distros don't benefit and everyone packaging TF has to duplicate the work.\r\n\r\nOr you mean to put all the rules in one big workspace instead of separate? That works but makes toggling each dep separately more complicated. Maybe not everyone can unbundle everything so all--or-none seems less flexible.\r\n", "@jart pkg-config isn't doable nicely yet with bazel. I'm waiting on https://github.com/bazelbuild/bazel/issues/5188 and https://github.com/bazelbuild/bazel/issues/5186\r\n\r\nI was going to expand the rules and iterate in later PRs once this patch set is merged. This one is already huge and gets merge conflicts all over the place. I have to keep rebasing the core parts every few days. Once the main parts are in, future changes will be much easier and less invasive. ", "rebased and fixed the conflicts", "Also, just to be clear, this patchset does already work on other distros than just Gentoo. The lib names and linker opts are standardized.\r\n\r\nThe only thing that does not work right now is searching a different prefix other than the base compiler paths (@msarahan's usecase). But a regular .deb or .rpm would work. That one is what is more complicated by bazel's rules. I'll fix those in a later series, either with new bazel or add things to ./configure.\r\n\r\nI'd rather get this merged before the 1.10 branch happens so other distros can get started packaging this too.\r\n", "Which distros?", "@jart your solution is nice because it avoids more ifs in the BUILD. \r\n\r\nIs there a way to do this type of thing without running a separate script? Maybe by putting the if in a single place, say tf_lib_dep? tf_http_archive_switchable_dep? \r\n\r\nThe goal of this work is for people to be able to build TF with system lib, and I don't think they should have to use sed to do it. We *could* put this thing in ./configure and ask for the libs to swap out, but that breaks another requirement, that dependent projects should be able to build TF by passing things to bazel and just run bazel.\r\n\r\n", "I've tested ubuntu 18.04 too.\r\n```\r\n$ sudo apt-get install libjpeg-turbo8 libjpeg-turbo8-dev \\\r\nzlib1g zlib1g-dev libsnappy1v5 \\\r\nlibsnappy-dev libre2-4 libre2-dev\r\n$ bazel build --action_env \\\r\nTF_SYSTEM_LIBS=\"com_googlesource_code_re2,jpeg,snappy,zlib_archive\" \\\r\n//tensorflow:libtensorflow_framework.so \\\r\n//tensorflow:libtensorflow.so \\\r\n//tensorflow:libtensorflow_cc.so \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```\r\nI'm sure unbundling more would work, but those are the ones I've tested.\r\n\r\n", "I'm in favor of merging this. If we can simplify it later, great. I have the following questions:\r\n\r\n- What happens when we add a new dependency? \r\n- What happens when we update a dependency?\r\n- Can we remove some of the redundancy? Bazel likes things to be explicit, but having both a file in systemlibs and having to list its name in a whitelist is jarring (to me).", "@martinwicke \r\nI could remove the whitelist, I thought it was safer to have but have no strong feelings either way. Could also remove it in the future if it ends up being a headache frequently. I was thinking later I could add a TF_SYSTEM_LIBS=\"all\" which would just enable the whole whitelist.\r\n\r\nWhen you add a new dep it will be bundled like things were in the past to start with. If unbundling it works (ie it has a release etc) then I or someone else from a distro can put together the unbundling rules.\r\n\r\nIf you update a dep, in most cases nothing will be needed in the systemlibs side. The libs would have the same name so the compiler would find it just the same as before. Distro packages would need to keep track of the minimum versions of deps but that goes for any package. If the dep changes significantly then the BUILD file will need updating but should be much less than the non-systemlib BUILD files\r\n", "One alternative you could consider is creating a WORKSPACE file in a subdirectory and not calling `tf_workspace()`. In that case, Bazel's default behavior is to unbundle and let you define dependencies yourself. It's the only way to be certain the the unbundling is complete.\r\n\r\nIf you partially unbundle, you'll likely want to run TF's tests after building. For example, not unbundling protobuf might get you into trouble. Yesterday I encountered an issue where `import tensorflow` fails if protoc / libproto / pyproto versions mismatch: https://github.com/google/protobuf/issues/4716. Another example is our libpng config assumes a specific version of zlib. More such subtleties likely await those who partially interchange components.", "@perfinion can you check the buildifier and --nobuild errors?", "@martinwicke @angersson I fixed the buildifier style fix and rebased. can you re-trigger the tests?\r\n\r\nI think the nobuild error was unrelated, looks like this commit fixes it. The rebase should pick that up too.\r\n\r\n```\r\ncommit b13b2c15d2e864270b28eb5c5d3ec9f53d3932a7\r\nAuthor: Yu-Cheng Ling <ycling@google.com>\r\nDate:   Fri Jul 13 08:38:45 2018\r\n\r\n    Fix the broekn BUILD rule under lite/delegates/eager.\r\n\r\nnobuild error:\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/contrib/lite/delegates/eager/BUILD:22:1: no such package 'testing/base/public': BUILD file not found on package path and referenced by '//tensorflow/contrib/lite/delegates/eager:util_test'\r\nERROR: Analysis of target '//tensorflow/contrib/lite/delegates/eager:util_test' failed; build aborted: no such package 'testing/base/public': BUILD file not found on package path\r\n```", "The changes here need manual intervention to work with the new PR import process, which I am currently working on handling.", "Done and merged! Thanks!", "Yeah!  Thank you all for vastly improving the TensorFlow experience on Gentoo Linux.", "/cc @dslomov", "@perfinion do you observe a significant change in build time on gentoo when using  system libraries? I tried to adopt this for the nix package, but to my surprise the build time barely changed (around 1h on a 12 core machine, 3.5h on a 4 core).", "@timokau Sorry, didnt see this earlier, yeah its a fair bit faster if you unbundle all of them. GRPC, protobuf and several others are decently big but you're right stuff like libpng are fairly small so are not a huge compile time. The biggest part I've noticed tho is the linking stage is a lot faster and uses a *lot* less RAM than when static linking. Also the python deps were really annoying unless unbundled.\r\n\r\nAlso feel free to email me (jason at perfinion dot com) if you have any issues with packaging, i'd love to help but I might not see this thread. :)", "Thank you for your feedback and the offer :)\r\n\r\nWe have since managed to [get the source build working](https://github.com/NixOS/nixpkgs/pull/64716) and made use of [most](https://github.com/NixOS/nixpkgs/blob/6ee979916492e68871eef1d3bcf07841d2c987df/pkgs/development/python-modules/tensorflow/default.nix#L156) of the decoupled dependencies. Bundled deps are not quite as painful with nix (as everything has its own FHS etc.), but it's still nice to be able to unbundle them. Thank you for your work on this!"]}, {"number": 20283, "title": "Truncated Distributions in TensorFlow", "body": "At this point, only [tf.truncated_normal](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) is implemented within TensorFlow. Are there any plans for implementing others aswell? ex. Truncated Gamma, Truncated Exponential etc. Truncated distributions come up often in probabilistic programming, a branch in which TensorFlow is becoming prominent. I for one do most of my probabilistic work and sampling in TensorFlow, due to the GPU support and the control it offers me.\r\n\r\nEach truncated distribution offers a different method of efficiently sampling from it, so distribution specific algorithms would be ideal.\r\n\r\nIn the mean time, I implemented a general method for personal use. I attach the specifications bellow, in case there is any interest in this general solution (the implementation, documentation and examples are available [here](https://github.com/ZigaSajovic/truncatedDistribution)). Note that I am willing to work on improving it, in case interest is present.\r\n\r\n**TruncatedDistribution**\r\n\r\nThe class [TruncatedDistribution](https://github.com/ZigaSajovic/truncatedDistribution) extends any existing TensorFlow distribution, i.e. classes inheriting from [tf.distribution](https://www.tensorflow.org/api_docs/python/tf/distributions/Distribution), to enable their truncated counterparts, with full support of broadcasting.\r\n\r\n**Methods:**\r\n\r\n* \\_\\_init\\_\\_(disttribution,left,right, n_points=1000)\r\n* sample(sample_shape=())\r\n* cdf(X)\r\n* log_cdf(X)\r\n* survival_function(X)\r\n* log_survival_function(X)\r\n* prob(X)\r\n* log_prob(X)\r\n* mean(n_samples=1000)\r\n* variance(n_samples=1000)\r\n* stddev(n_samples=1000)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: Yes\r\nOS Platform and Distribution Ubuntu: 16.0\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.8\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "@ringw Dan, it looks like some of this code was yours. Can you comment on future plans, or points us at someone who can, please? ", "Hello,\r\n\r\nI had started writing [a new truncated normal kernel for arbitrary intervals](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/parameterized_truncated_normal_op.cc), but then had another 20% project that took priority. The eventual plan was to benchmark that kernel against the current tf.truncated_normal on the same fixed interval, and once it's ready, add a new truncated_normal function (maybe initially in tf.contrib) that takes vectors of mean/stddev/low/high values. However, if no one wants to get that implementation running, I'm also fine with deleting it. I don't know of any plans for other distributions, and @ebrevdo is my point of contact within the Brain team.", "I don't think we plan to have a generic Truncated distribution object, at least unless we can add [automatic implicit reparameterization gradients](https://arxiv.org/abs/1805.08498) as part of the generic implementation.", "I'm assuming here you mean for any distribution with a scalar event shape (i.e., single variate distributions).", "Thanks, @ringw and @ebrevdo. I'll mark \"contributions welcome\". @ZigaSajovic thanks for being willing to work on this!", "If you'd like to work on this, I do suggest reading the paper I linked\nabove, especially section #4.\n\nOn Fri, Jul 13, 2018 at 9:43 AM, Cliff Young <notifications@github.com>\nwrote:\n\n> Thanks, @ringw <https://github.com/ringw> and @ebrevdo\n> <https://github.com/ebrevdo>. I'll mark \"contributions welcome\".\n> @ZigaSajovic <https://github.com/ZigaSajovic> thanks for being willing to\n> work on this!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20283#issuecomment-404887995>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxoAlPnVaH8TVH7Fzdi-a4A9z0ZVks5uGM4VgaJpZM4U2nhQ>\n> .\n>\n", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "hey folks @cy89 @ringw \r\n\r\nI happen to have stumbled upon this issue for some of my research and am wondering what would be the plan here? \r\n\r\nMy thought is quite aligned with what @ZigaSajovic 's solution: for truncated univariate distributions with simple left or right truncation, have a wrapper class that wraps around the base distribution and make adjustment on the PDF and CDF. I think this is consistent with  \r\n@ebrevdo 's paper section 4. \r\n\r\nHere I'm assuming that the underlying infrastructure could take over the `tf.distribution` instance that we have and conduct differentiation etc accordingly. As long as we have re-defined PDF and CDF in the new class. Please enlighten me if this is consistent with what ya'll are thinking? Thanks \r\n\r\n", "Would also be useful if `tf.truncated_normal` permitted custom upper/lower bounds, independent of the variance (rather than the symmetric +/- 2 SDs)\r\n\r\nEDIT: I see this is already available in TF probability! Linking in case useful: https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/TruncatedNormal", "This is a stale issue. Looks like this was resolved.\r\nPlease feel free to reopen if I am mistaken. Thanks!", "Thanks -- I don't have full context into how this is being used, but the first pass is definitely complete.\r\n\r\n[TensorFlow Probability](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/TruncatedNormal) is still intended to be the entry point for the more general (arbitrary bounds) distribution; the kernel for this was originally implemented in TensorFlow proper."]}, {"number": 20282, "title": "Branch 201975328", "body": "", "comments": []}, {"number": 20281, "title": "eigen: Add install_eigen_headers target for installing to system", "body": "Eigen provides files that are both GPL and MPL. Tensorflow uses only the\r\nMPL headers. This target collects all the headers into genfiles so they\r\ncan be easily installed to /usr/include/ later.\r\n\r\nThanks to dennisjenkins@google.com for all the help testing and figuring\r\nout what was missing. And to pcloudy@google.com for pointers to the solution.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["I force pushed a tiny update. I had forgotten to remove the \"-v\" in the cp cmd from when I was originally testing. I also don't know why the OSX test failed, but it looks like it had a bunch of problems so hopefully not related. "]}, {"number": 20279, "title": "the \"body_inputs\" variable of Whilecontext class should be std::vector<InputTensor> ", "body": "Class tensorflow::Whilecontext private body_inputs_ is in the form of std::vector< Outputvector > now, which should be std::vector< Inputvector >. Although this bug may not be effect the function.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "OutputTensor is correct in this case. body_inputs_ is specifying which _outputs_ of which operations we should use as inputs to the loop body."]}, {"number": 20278, "title": "Fix tflite_convert.py issue in Python 3", "body": "This fix tries to address the issue raised in #20276 where\r\nin python 3, the following error were thrown with tflite_convert.py:\r\n```\r\nTypeError: object of type 'zip' has no len().\r\n```\r\nThis fix converts zip output to list to fix the issue in python 3.\r\n\r\nThis fix fixes #20276.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for the fix @yongtang!"]}, {"number": 20277, "title": "[ROCm] Bazel build and continuous integration infrastructure", "body": "This pull request is to start introduce support for ROCm platform to TensorFlow. In this initial pull request, 2 components are addressed:\r\n\r\n- bazel build system\r\n- continuous integration logic\r\n\r\nAuthors:\r\n\r\n- Jack Chung: jack.chung@amd.com\r\n- Jeffrey Poznanovic: Jeffrey.Poznanovic@amd.com\r\n- Peng Sun: Peng.Sun@amd.com", "comments": ["ping?", "@gunan could you please take another look?", "@rmlarsen / @gunan please hold it for now . I haven't finished addressing all comments from reviewers yet. will ping you guys once i feel more comfortable with the PR. And I think I'll probably need to squash the commits to make commit history looks nicer.", "@rmlarsen / @gunan I believe I've already address all the comments and updated the PR. Please help review it again. Thanks!", "a gentle ping?", "@gunan / @jlebar wondering if you could provide additional inputs to this pull request? There are a few other pull requests I'm preparing depends on the infrastructure established in the PR (ex: `if_rocm_is_configured`, `TENSORFLOW_USE_ROCM`). And some others PRs already filed ( such as #20709 / #20712 / #20715 ) also depends on this PR so they can be further revised / reviewed / merged.\r\n", "Hi, I work with Jack @whchung here at AMD.    Jack suggested the command-line below - is this what is desired?    So would produce 3 PIPs at the end of the build, but the overall build would be faster since we only compile the common code once?\r\n\r\n'# bazel build //tensorflow/tools/pip_package:build_cpu_pip_package //tensorflow/tools/pip_package:build_cuda_pip_package //tensorflow/tools/pip_package:build_rocm_pip_package'\r\n\r\nWe are not bazel experts so implementing this looks fairly scary, but perhaps not for someone with proper understanding of the build architecture and access to internal test frameworks.    Can someone provide a reference implementation for the existing targets (CPU, CUDA, etc) - then we could modify this PR to fit into that template?  Or we can treat the unified build steps as a separate feature/PR ?", "OK, looks like we are ready to start merging this PR. As we also need some internal work on this, it will take some time.\r\nBefore I do my final review and approve, could you resolve the final conflicts, one final time?", "@gunan thx. i was out of town yesterday. let me revise the PR one more time so you can take a look at it.", "@gunan resolved the merge conflict. please help review it once again. thanks.", "@gunan I addressed the build errors found by CI. Would you mind re-approve and re-commence the tests? Thanks.", "Looks like all build system seems broken.\r\nDo you have a mac machine you can locally iterate on?\r\nLooks like trying just the CPU build will be useful.", "@gunan I'll try get a Mac to locally reproduce the issue. It seems some of my earlier editions to remove Darwin and Windows from ROCm build path could be related. Since ROCm doesn't support Windows & OSX yet, what would be the best practice to do so?\r\n\r\nFor Ubuntu it's weird that we are seeing this error:\r\n```\r\nImportError: libtensorflow_framework.so: cannot open shared object file: No such file or directory\r\n```\r\nIn my downstream fork we are always getting this shared lib properly built. I'll need to study it a bit better.", "@gunan I resolved one merge conflict earlier today but all previous build logs were gone. Is there a way to get them?", "All new commits invalidate older builds. I will retrigger them.", "@yifeif copybara said \"An error happened while migrating the change\". Trying again.", "@whchung  could you check the build failures?", "@gunan I\u2019m working on this. I\u2019ve already resolved Mac and windows errors on my local boxes. Still haven\u2019t quite sure what\u2019s going on with libtensorflow_framework.so on those Ubuntu targets . I haven\u2019t seen such issues on my end thus far. I\u2019ll revise this PR once I got this figured out", "@whchung looks like it is proving to be too difficult to get everything in this change to work all at once.\r\nI propose breaking this up this way:\r\n\r\n1 - only the toolchains in third_party/gpus/rocm*\r\n2 - tensorflow.bzl changes\r\n3 - BUILD file changes\r\n4 - configure and bazel.rc changes\r\n5 - Dockerfile.rocm\r\n6 - build script changes.\r\n\r\nit will be 6 PRs, but they will be much easier to work with to get our CI green for both you and us.", "@gunan many thanks for the suggestion.\r\n\r\njust yesterday I've finally figured out the weird issue that we are failing on \"some\" targets in TF CI but not others. it's related to how `$ORIGIN` is passed down to linker. i came up with a fix and is under testing on our side.\r\n\r\nmeanwhile let me try break it up so it's easier to be reviewed / tested / merged.", "@gunan Just to inform you I'm working on some final touches to this PR and should be able to revise it later today. All failures in previous run should have been addressed. I'll inform you when it's ready.", "@gunan I believe with the new commit I should have addressed all previous build errors encountered on Mac, Windows, and Linux. Could you help restart the test? Thanks.", "@gunan many thanks for executing the test. there is only 1 failure in `MacOS Contrib` target, and from the log i really don't believe it has anything to do with this PR but a timing issue. Would it be possible to re-run that target once again?", "@gunan in `Ubuntu Sanity` there is one error but its log is cryptic to me and again I don't really think it has anything related to the PR itself:\r\n\r\n```\r\n=== Sanity check step 3 of 13: do_check_futures_test (Check that python files have certain __future__ imports) ===\r\nTraceback (most recent call last):\r\n  File \"check_futures_test.py\", line 107, in <module>\r\n    main()\r\n  File \"check_futures_test.py\", line 103, in main\r\n    raise AssertionError('Error in %s: %s' % (short_path, str(e)))\r\nAssertionError: Error in contrib/lite/tools/accuracy/ilsvrc/generate_validation_labels.py: Missing futures: division absolute_import print_function\r\n```\r\n\r\nNeither in this PR nor in TensorFlow `master` branch do we see this `generate_validation_labels.py` so it leads me to believe it's a configuration issue in the CI test node. I also just executed `check_futures_test.py` on my local box and it returns fine.", "Sanity failure is a known issue, so it did pass all presubmits.\r\nReviewers, could you take one final look?\r\n@yifeif How should we approach pulling this change into google?", "cc @parallelo for awareness\r\n\r\n@gunan / @yifeif , do we expect to see new CI build / test targets once this PR gets merged?", "No, we  do not have the hardware, toolchains, license reviews or anything in place for us to be able to build with ROCm. So they definitely wont become blocking presubmits for now.\r\nThis, of course will be subject to review later.\r\n\r\nIn the meantime, we can work with you to setup community supported builds, as outlined here:\r\nhttps://github.com/tensorflow/community/blob/master/sigs/build/community-builds.md\r\n\r\n", "@gunan Thanks for sharing the community build page with me. @parallelo would look into it and adapt our CI infrastructure to accommodate that.\r\n\r\nAlso we do like to revive our other outstanding PRs, specifically those in StreamExecutor and GPU common runtime which are blocked by this particular PR. Also we'll start submitting PRs to enable operators on ROCm which condition upon `TENSORFLOW_USE_ROCM` introduced in this PR. Would it be possible to help expedite allowing this PR be merged? Thanks a lot.\r\n\r\nWith more developers working on TensorFlow ROCm port now we expect subsequent PRs be revised / maintained in timely fashion.", "This will need a manual pull for sure @gunan. @whchung do you mind resolving the latest conflicts? Reviewers, let me know if this is ready and I can give pulling a shot. ", "@yifeif @gunan, my colleague @deven-amd has debased and modified the PR", "@yifeif any update to pull this PR in?", "@aaroey @meteorcloudy any additional comments?", "Oh, it looks like we have to import this manually.", "@yifeif is working on the manual import of this, she has been working on this for 2 weeks now.", "We had to revert some \"if_cuda_is_configured\" uses to if_cuda to make all internal tests to pass, but other than that all of this change has been merged.", "We finally got this change merged! Thanks for the patience. We needed to change if_cuda_is_configured in tf_cuda_library back to if_cuda to get some internal targets to pass. Feel free to send another PR if this causes any issue and we can work out a patch.", "Woohoo! Thank you @yifeif !", "Thank you @yifeif and @gunan . We\u2019ll revise other pending PRs for ROCm , as well as submitting new ones :)"]}, {"number": 20276, "title": "tflite_convert raises TypeError: object of type 'zip' has no len()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: docker image tensorflow/tensorflow:nightly-py3\r\n- **TensorFlow installed from (source or binary)**: -\r\n- **TensorFlow version (use command below)**: latest (1.9.0 rc1)\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: `tflite_convert --output_file=out.tflite --saved_model_dir=1529923978/ --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --input_array=input`\r\n\r\n### Describe the problem\r\nTrying to run the script above in Python 3 raises a `TypeError: object of type 'zip' has no len()`.\r\nThis is caused by [line 107 of the `tflite_converter` script](https://github.com/tensorflow/tensorflow/blob/f202958ee2d5177a474e3d107fdbf0c83174d099/tensorflow/contrib/lite/python/tflite_convert.py#L107), more specifically by `len(quant_stats)`. `quant stats` is defined in [line 105](https://github.com/tensorflow/tensorflow/blob/f202958ee2d5177a474e3d107fdbf0c83174d099/tensorflow/contrib/lite/python/tflite_convert.py#L105) as `zip(mean_values, std_dev_values)`.\r\n\r\n", "comments": ["I think the issue could be resolved with `list(zip(...)`. Added a PR #20278 for the fix."]}, {"number": 20275, "title": "`tf.profiler.profile` outputs negative number of flops", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\n`tf.profiler.profile` outputs negative number of flops for `tf.nn.conv2d` ops on large inputs because of np.int32 overflow.\r\n\r\n### Source code / logs\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.ops import get_stats_for_node_def\r\n\r\ntest_conv2d = tf.nn.conv2d(tf.zeros((1, 1024, 1024, 16)), tf.zeros((4, 4, 16, 4)), strides=[1, 1, 1, 1], padding='SAME')\r\n# This function is used by tf.profiler.profile\r\nstats = get_stats_for_node_def(tf.get_default_graph(), test_conv2d.op.node_def, 'flops')\r\n# Compare the results: one is negative, the other is the actual result\r\n# The size is exactly 2^31\r\nprint(stats.value, 1 * 1024 * 1024 * 4 * 16 * 4 * 4 * 2)\r\n```\r\n\r\nThe reason is:\r\nhttps://github.com/tensorflow/tensorflow/blob/cfebbbc94f3edd1622a9a42379dd2ccc956ea52c/tensorflow/python/ops/nn_ops.py#L2169\r\nhere `np.prod` is used which returns an `np.int32`. This enforces the next product line \r\nhttps://github.com/tensorflow/tensorflow/blob/cfebbbc94f3edd1622a9a42379dd2ccc956ea52c/tensorflow/python/ops/nn_ops.py#L2172\r\nto compute everything as `np.int32`. Actually, the protobuf supports int64:\r\nhttps://github.com/tensorflow/tensorflow/blob/cf375f06747b7be998f0df329772390f545577a1/tensorflow/core/profiler/tfprof_output.proto#L105\r\nso this line could be replaced by:\r\n```py\r\nnp.prod(output_shape.as_list(), dtype=np.int64)\r\n```\r\nThere are more ops affected by this issue, as they also use `np.prod`.\r\n\r\n\r\nPS: Sorry that I cannot submit the fix straight away, everything needed to fix this issue is mentioned above.", "comments": ["Thanks for the clear problem diagnosis.  I'll push a fix internally.", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @poxvoculi: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20274, "title": "Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel''", "body": "When I download whole repository and try to run android application:\r\ntensorflow-master/tensorflow/examples/android\r\nI get the error from the title.\r\n\r\nThis error happens because of this code from build.gradle :\r\n// set to 'bazel', 'cmake', 'makefile', 'none'\r\ndef nativeBuildSystem = 'bazel'\r\n\r\nWhen I try to change to some other build system, I get different errors. For example, when I put to 'none', I get the camera problem, but I would like to solve this 'bazel' problem, because it looks like it is default build system.\r\n\r\nI use Ubuntu 16.04,  \r\nI have TensorFlow version 1.5.0\r\nBazel version: Build label: 0.13.0,\r\nCUDA version: Cuda compilation tools, release 7.5, V7.5.17\r\n\r\nGPU model and memory\r\nIntel Corporation 4 Series Chipset Integrated Graphics Controller (rev 03) (prog-if 00 [VGA Controller])\r\nSubsystem: Dell 4 Series Chipset Integrated Graphics Controller\r\n4 GB\r\n\r\nThank you in advance\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nCUDA/cuDNN version\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "No, somehow I managed to run the scripts I wasn't able at first.\nfeel free to close this.\nSorry and thank you\n\n\nOn Thu, Jul 26, 2018 at 9:19 PM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> It has been 31 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20274#issuecomment-408205230>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AROI6bIwIok1Y1EU66np8LApDSzEOPSbks5uKhY0gaJpZM4U2DgA>\n> .\n>\n", "It looks like this issue has been resolved, so I'm closing it. Thanks!"]}, {"number": 20273, "title": "Add examples to documentation for CudnnRNN", "body": "Have I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.8.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: 9.0/7.0\r\nGPU model and memory: NVIDIA GTX 1080 Ti\r\nExact command to reproduce: N/A\r\n\r\nIt would be very helpful to add usage examples on how to use `cudnn_rnn`, particularly on cross-compatibility between non-CUDA and CUDA-supporting devices. Users should be able to figure out how to save/restore weights to run their models with, say, `tf.nn.rnn_cell.LSTMCell` or `tf.contrib.cudnn_rnn.CudnnLSTM`.\r\n\r\nThere are classes that seem to do this (e.g. `tf.contrib.cudnn_rnn.CudnnLSTMSaveable`) but there are no easily accessible code examples showing how they should be used.\r\n\r\n*A possible feature request*: would it be possible to have a high-level wrapper that makes this choice automatically based on the availability of a CUDA device, saving and restoring weights accordingly? This should be possible with `tf.test.is_gpu_available(cuda_only=True)`.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I found the following markdown documentation within the source code. It's in the docstring of the private class `_CudnnRNN ` in [this file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py).\r\n\r\nCould someone add this to the website API docs so this issue can be resolved? Many thanks.\r\n\r\n\r\nAbstract class for RNN layers with Cudnn implementation.\r\n  Cudnn RNNs have two major differences from other platform-independent RNNs tf\r\n  provides:\r\n  * Cudnn LSTM and GRU are mathematically different from their tf counterparts.\r\n    (e.g. @{tf.contrib.rnn.LSTMBlockCell} and @{tf.nn.rnn_cell.GRUCell}.\r\n  * Cudnn-trained checkpoints are not directly compatible with tf RNNs:\r\n    * They use a single opaque parameter buffer for the entire (possibly)\r\n      multi-layer multi-directional RNN; Whereas tf RNN weights are per-cell and\r\n      layer.\r\n    * The size and layout of the parameter buffers may change between\r\n      CUDA/CuDNN/GPU generations. Because of that, the opaque parameter variable\r\n      does not have a static shape and is not partitionable. Instead of using\r\n      partitioning to alleviate the PS's traffic load, try building a\r\n      multi-tower model and do gradient aggregation locally within the host\r\n      before updating the PS. See https://www.tensorflow.org/performance/performance_models#parameter_server_variables\r\n      for a detailed performance guide.\r\n  Consequently, if one plans to use Cudnn trained models on both GPU and CPU\r\n  for inference and training, one needs to:\r\n  * Create a CudnnOpaqueParamsSaveable subclass object to save RNN params in\r\n    canonical format. (This is done for you automatically during layer building\r\n    process.)\r\n  * When not using a Cudnn RNN class, use CudnnCompatibleRNN classes to load the\r\n    checkpoints. These classes are platform-independent and perform the same\r\n    computation as Cudnn for training and inference.\r\n  Similarly, CudnnCompatibleRNN-trained checkpoints can be loaded by CudnnRNN\r\n  classes seamlessly.\r\n  Below is a typical workflow(using LSTM as an example):\r\n  for detailed performance guide.\r\n  # Use Cudnn-trained checkpoints with CudnnCompatibleRNNs\r\n  ```python\r\n  with tf.Graph().as_default():\r\n    lstm = CudnnLSTM(num_layers, num_units, direction, ...)\r\n    outputs, output_states = lstm(inputs, initial_states, training=True)\r\n    # If user plans to delay calling the cell with inputs, one can do\r\n    # lstm.build(input_shape)\r\n    saver = Saver()\r\n    # training subgraph\r\n    ...\r\n    # Once in a while save the model.\r\n    saver.save(save_path)\r\n  # Inference subgraph for unidirectional RNN on, e.g., CPU or mobile.\r\n  with tf.Graph().as_default():\r\n    single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleLSTM(num_units)\r\n    # NOTE: Even if there's only one layer, the cell needs to be wrapped in\r\n    # MultiRNNCell.\r\n    cell = tf.nn.rnn_cell.MultiRNNCell(\r\n      [single_cell() for _ in range(num_layers)])\r\n    # Leave the scope arg unset.\r\n    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state, ...)\r\n    saver = Saver()\r\n    # Create session\r\n    sess = ...\r\n    # Restores\r\n    saver.restore(sess, save_path)\r\n  # Inference subgraph for bidirectional RNN\r\n  with tf.Graph().as_default():\r\n    single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleLSTM(num_units)\r\n    cells_fw = [single_cell() for _ in range(num_layers)]\r\n    cells_bw = [single_cell() for _ in range(num_layers)]\r\n    # Leave the scope arg unset.\r\n    (outputs, output_state_fw,\r\n     output_state_bw) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\r\n         cells_fw, cells_bw, inputs, ...)\r\n    saver = Saver()\r\n    # Create session\r\n    sess = ...\r\n    # Restores\r\n    saver.restore(sess, save_path)\r\n  ```", "@danielwatson6 Could you point me if I'm correct?\r\n1. While training, we can use just `CudnnLSTM`\r\n2. When I want to freeze graph to pb, I should use `CudnnCompatibleLSTM` with `MultiRNNCell` and `stack_bidirectional_dynamic_rnn` (if  model was trained with such options)\r\n\r\nThen, everything can be used in GPU and CPU.\r\n\r\nIf that pipeline is ok, it should be added to documentation?\r\nAlso question about `CudnnLSTMSaveable`, so what for this class was created, it just used internally and should not be used by user?", "I have manage to find a way to restore `CudnnLSTM` model on CPU. \r\nHere are jupyters with example: https://gist.github.com/melgor/41e7d9367410b71dfddc33db34cba85f\r\n\r\nThe main problem was `variable_scope` which need to be set in proper way. ", "There is currently work underway to unify RNNs in TensorFlow.\r\ncc @qlzh727 ", "\"A possible feature request: would it be possible to have a high-level wrapper that makes this choice automatically based on the availability of a CUDA device, saving and restoring weights accordingly? This should be possible with tf.test.is_gpu_available(cuda_only=True)\"\r\n\r\nWe are actually trying to unify the API for this exact direction. Basically the new API will figure out the best choice implementation for user based on the available hardware, and user don't have to specify whether its CuDNN or CPU implementation in the code. Currently we are still in the design stage, and will post more results when the APIs are finalized. The new API will probably be available as part of the change in Tensorflow 2.0.", "@melgor  which version of tensorflow you use in your scrip?", "Closing as RNNs will now automatically select an implementation based on whether or not they are running on a GPU.", "@robieta Which specific classes implement the automatic implementation selection you talk about? Are you suggesting that `tensorflow.keras.layers.CuDNNGRU` is now no longer needed because of those classes that implement automatic implementation selection?\r\n\r\nAs of TF 1.14, `tensorflow.keras.layers.CuDNNGRU` still behaves differently than `tensorflow.keras.layers.GRU` (with selection of the variant that uses CuDNN). In my experiment, when I switch between these two in the graph, the output I get during training is different; they simply calculate different things (whether it is due to different formulas or different initial weights I cannot tell). The former also takes a shorter time to train each epoch, so there is still a strong reason to prefer it for training.", "The automatic CuDNN selection is only for the 2.x RNNs.", "@robieta Which 2.0 RNN classes have the automatic CuDNN selection? With regard to the automatic CuDNN selection, I can't tell any obvious documentation difference between 1.15 and 2.0 for `tf.keras.layers.GRU`. Both documents mention a \"second variant\" compatible with CuDNNGRU:\r\nhttps://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/GRU\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU", "@jchia, LSTM and GRU layer has auto cudnn selection in the 2.x API.", "@danielwatson6  \r\nHi, I know this is a 2018 old issue but I am kind of stuck at those `opaque kernel` in tf 1.14,  so may I talk more details with you?\r\nThe situation is , I got a trained model with cudnnBLSTM in tf 1.14. What I want is to convert this tf models to [MNN](https://github.com/alibaba/MNN) model format and then inference with that engine. I have a tf frozen graph of this model and I need to decode the weights and bias data from this frozen graph. \r\nWhat I found is that the size of the `opaque kernel` is not equal to the size I calculate of this BLSTM. Specifically, say I got a 3 layers BLSTM with input dim 143 and state size 143, so the total size of parameter should be\r\n```\r\n(143 * 143 + 143 * 143 + 143) * 4 * 2 + 2 * (286 * 143 + 143 * 143 + 143) * 4 * 2 = 1312168\r\n```\r\nBut when I try to read the size of `opaque kernel`, I got 1315600.\r\nAnd there is no suprise when I read your comment about\r\n\r\n> The size and layout of the parameter buffers may change between\r\nCUDA/CuDNN/GPU generations. Because of that, the opaque parameter variable\r\ndoes not have a static shape and is not partitionable. \r\n\r\nSo my question are, is there anyway I can decode weights and bias from `opaque kernel`? How does the `opaque kernel`organize? Is there any workaround consider my situation?\r\nThanks!"]}, {"number": 20272, "title": "Custom implementation for Dequantize when converting .pb to .tflite", "body": "I'm trying to convert tensorflow quantised .pb file to .lite using toco. The command for creating .pb file is :\r\nretrain.py is [here](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/retrain.py) and [here](https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py).\r\n```\r\npython retrain.py \\\r\n--bottleneck_dir=/mobilenet_q/bottlenecks \\\r\n--how_many_training_steps=4000 \\\r\n--output_graph=/mobilenet_q/retrained_graph_mobilenet_q_1_224.pb \\\r\n--output_labels=/mobilenet_q/retrained_labels_mobilenet_q_1_224.txt \\\r\n--image_dir=/data \\\r\n--architecture=mobilenet_1.0_224_quantized\r\n```\r\nWhen I'm trying to convert the .pb file to .tflite using toco command:\r\n```\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco \\\r\n  -- --input_file= retrained_graph_mobilenet_q_1_224.pb \\\r\n  --output_file= retrained_graph_mobilenet_q_1_224.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=1,224,224,3 \\\r\n  --input_array=input \\\r\n  --output_array=final_result \\\r\n  --inference_type=FLOAT \\\r\n  --input_data_type=FLOAT\r\n```\r\nI'm getting the error:\r\n`Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Dequantize.`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code  **`No`**\r\nOS Platform and Distribution  **`MacOS High Sierra 10.13.5`**\r\nTensorFlow installed from   **`binary`**\r\nTensorFlow version  **`v1.8.0-0-g93bc2e2072 1.8.0`**\r\nBazel version  **`0.13.1`**\r\nCUDA/cuDNN version  **`N/A`**\r\nGPU model and memory  **`Intel Iris Pro 1536 MB`**\r\nExact command to reproduce  **`already mentioned`**", "Thanks, @duplex143 . Can you clarify what exactly you want? Do you want to propose a custom implementation of the op in question? Are you requesting a new op be implemented? Or something else?", "@karmel I'm requesting for that op to be implemented. I'm trying to design an Android app that uses ML Kit for image labelling on my custom dataset. To reduce the model size, I'm using quantised mobilenet architecture and ML Kit accepts tflite files. I'm getting that error when trying to convert .pb to .tflite. Do you suggest any workaround alternatively?", "@suharshs - are there any plans for a Dequantize op as described above? Or is there an alternate way to accomplish quantization that we can recommend?", "Hi @duplex143 ,\r\n\r\nWhere are you getting your .pb file? (It doesn't seem to be a TFLite compatible quantized graph) The instructions for creating a quantized TFLite graph are available here: https://www.tensorflow.org/performance/quantization \r\n\r\nIn particular your graph should include FakeQuant operations that store the quantization range information for TOCO.\r\n\r\nIf you are doing image classification, we have released pretrained (on Imagenet) models that are already in TFLite format: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md \r\n\r\nHope that helps!", "@suharshs @karmel Thanks for replying. I'm getting the .pb file from the training script provided by [Googlecodelabs](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/retrain.py)/[TensorFlow](https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py) and is explained [here](https://www.tensorflow.org/tutorials/image_retraining). The CNN architecture I'm using is mobilenet_1.0_224_quantized. So, a quantised mobilenet model in .pb format is generated, I'm trying to convert it to .lite format and I'm running into that issue. \r\n\r\n**In particular your graph should include FakeQuant operations that store the quantization range information for TOCO.**\r\n`How do I confirm if my graph has FakeQuant operations?`", "I see. You are using the correct model, but you should use this toco command to specify quantization:\r\n\r\n```\r\nbazel build tensorflow/contrib/lite/toco:toco && \\\r\n  ./bazel-bin/third_party/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=<YOUR_INPUT> \\\r\n  --output_file=<YOUR_OUTPUT> \\\r\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shape=\"1,224, 224,3\" \\\r\n  --input_array=input \\\r\n  --output_array=final_result \\\r\n  --std_value=128 --mean_value=128\r\n```", "@suharshs I'm getting `-bash: ./bazel-bin/third_party/tensorflow/contrib/lite/toco/toco: No such file or directory` when I'm running the above command. So, I executed\r\n```\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco \\\r\n  -- --input_file=retrained_graph_mobilenet_q_1_224.pb \\\r\n :\r\n :\r\n :\r\n```\r\nand the output is \r\n```\r\n2018-06-26 14:55:18.787325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.789699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.790247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.790699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.790985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.791253: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.791542: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.791815: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.791967: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.792026: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Dequantize\r\n2018-06-26 14:55:18.795307: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 144 operators, 237 arrays (0 quantized)\r\n2018-06-26 14:55:18.798708: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 144 operators, 237 arrays (0 quantized)\r\n2018-06-26 14:55:18.801078: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 64 operators, 155 arrays (1 quantized)\r\n2018-06-26 14:55:18.802208: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 63 operators, 153 arrays (1 quantized)\r\n2018-06-26 14:55:18.803271: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 62 operators, 151 arrays (1 quantized)\r\n2018-06-26 14:55:18.804424: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 62 operators, 151 arrays (1 quantized)\r\n2018-06-26 14:55:18.805219: F tensorflow/contrib/lite/toco/tooling_util.cc:1588] Array MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\n```\r\nSo, I added `'--default_ranges_min=0' '--default_ranges_max=6'` and the output is the following in place of the last line of above output\r\n```\r\n2018-06-26 15:08:24.056119: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:456] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Dequantize) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n```\r\nSo, it boils down to implementing the custom op `Dequantize ` I suppose.\r\nSee [this](https://github.com/tensorflow/tensorflow/issues/14606). Also, @andrehentz said [here](https://github.com/tensorflow/tensorflow/issues/15122#issuecomment-363126160) that the following command works for converting .pb to .tflite\r\n```\r\ntensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \\                                           \r\n  --input_file=$TRAINING_DIR/retrained_graph.pb \\                                                  \r\n  --input_format=TENSORFLOW_GRAPHDEF \\                                                             \r\n  --output_format=TFLITE \\                                                                         \r\n  --output_file=/$TRAINING_DIR/${ARCHITECTURE}.tflite \\                                            \r\n  --inference_type=QUANTIZED_UINT8 \\                                                               \r\n  --input_array=Placeholder \\                                                                      \r\n  --output_array=final_result \\                                                                    \r\n  --input_shape=1,224,224,3 \\                                                                      \r\n  --mean_value=128 \\                                                                               \r\n  --std_value=128\r\n```\r\nBut even for above command, I'm getting the same `Dequantize` error. Maybe toco changed after Feb 5?", "We have updated the retrain tutorials to use hub which should resolved some model issues with deciding what models to use. https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py \r\n\r\nThat being said, i think the issue is that the retrain script is somehow picking up an old quantized model that uses TF dequantization operations (which doesn't actually fully quantize the model)\r\n\r\ni think you should specify `mobilenet_1.0_224_quant` instead of `mobilenet_1.0_224_quantized`\r\n\r\nApologies for the confusion.", "@suharshs Thanks for the reply. The script that uses hub doesn't have `--architecture` flag.\r\nAs you've mentioned the retraining script, I think there's an issue with that script. Could you look into this?\r\nhttps://github.com/googlecodelabs/tensorflow-for-poets-2/issues/64\r\nThe predictions when using hub script are erroneous. ", "Sorry, i meant you should try using `--architecture=mobilenet_1.0_224_quant` instead of `--architecture=mobilenet_1.0_224_quantized` for the original (non-hub) retrain script. That should work.\r\n\r\n\r\n\r\n", "Got this error. The original script accepts `ized` only. \r\n```\r\nERROR:tensorflow:Couldn't understand architecture suffix 'quant' for 'mobilenet_1.0_224_quant'\r\nERROR:tensorflow:Did not recognize architecture flag\r\n```\r\n[Also why is hub script giving wrong predictions?](https://github.com/googlecodelabs/tensorflow-for-poets-2/issues/64)\r\n", "I am not super familiar with the hub scripts, but someone will take a look at that issue soon as well.\r\n\r\nI believe you are using a version of the retrain.py script that doesn't yet have support for the proper quantized models.\r\n\r\nThis is the version of the code you should be using and the line that recognizes '_quant': https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py#L1022 \r\n\r\nPlease make sure you are using that version.\r\n\r\n", "@suharshs Yes!! The above script is working fine. There are retraining scripts at [Googlecodelabs](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/retrain.py), [TensorflowHub](https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py), [TensorFlow](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py). Also I know of a [script in TensorFlow repo at 1.4.0 branch](https://github.com/tensorflow/tensorflow/blob/v1.4.0/tensorflow/examples/image_retraining/retrain.py). It'd be convenient if all retraining scripts point to the correct script. Thanks for the help."]}, {"number": 20271, "title": "ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory", "body": "I have recently installed tensorflow-gpu using pip. But when I am importing it it is giving the following error:\r\n\r\n    ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n\r\nI have gone through all the answers of stackoverflow related to this issue but none of them worked for me.\r\n\r\nlibcudnn.so.7 is present in both the following directories /usr/local/cuda/lib64 and /usr/local/cuda-9.0/lib64 .\r\n\r\nAlso, I have added the following path in my .bashrc file:\r\n\r\n    export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\r\n    export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n    \r\n    export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\r\n    export LD_LIBRARY_PATH=/usr/local/cuda/lib64\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n\r\n\r\nPlease help me in resolving this\r\n", "comments": ["It worked after changing LD_LIBRARY_PATH to this:\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:$LD_LIBRARY_PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH", "After installing cuda, there is no libcudnn.so under /usr/local/cuda/lib64\r\n\r\nCan you tell me how you installed cuda ?\r\n\r\nThanks", "Yes exactly, it's not in there!", "Turns out it's not included, you have to download and copy them to/usr/local/cuda/lib64 separately:\r\n[https://developer.nvidia.com/rdp/cudnn-download](url)", "@tedyu @mysticaltech \r\n\r\nI installed it with the following command.\r\n```\r\nsudo dpkg -i libcudnn7_7.0.5.15-1+cuda9.0_amd64.deb\r\nsudo dpkg -i libcudnn7-dev_7.0.5.15-1+cuda9.0_amd64.deb\r\nsudo dpkg -i libcudnn7-doc_7.0.5.15-1+cuda9.0_amd64.deb\r\n```\r\nI found libcudnn.so.7 in `/usr/lib/x86_64-linux-gnu`. Do you know why it will be installed there? \r\n```\r\n\u256d\u2500keiichi.kuroyanagi@instance-1 /usr/lib/x86_64-linux-gnu  \u2039system\u203a\r\n\u2570\u2500$ pwd                                                                                                                                                              02:32:15\r\n/usr/lib/x86_64-linux-gnu\r\n\u256d\u2500keiichi.kuroyanagi@instance-1 /usr/lib/x86_64-linux-gnu  \u2039system\u203a\r\n\u2570\u2500$ ls libcudnn.so.7                                                                                                                                                 02:32:26\r\nlibcudnn.so.7@\r\n```", "I think it is the default location for libcudnn.so.7\n\nOn Sun, Sep 16, 2018 at 10:39 AM KUROYANAGI KEIICHI <\nnotifications@github.com> wrote:\n\n> @tedyu <https://github.com/tedyu> @mysticaltech\n> <https://github.com/mysticaltech> I found libcudnn.so.7 in\n> /usr/lib/x86_64-linux-gnu. Do you know why it will be installed there?\n>\n> \u256d\u2500keiichi.kuroyanagi@instance-1 /usr/lib/x86_64-linux-gnu  \u2039system\u203a\n> \u2570\u2500$ pwd                                                                                                                                                              02:32:15\n> /usr/lib/x86_64-linux-gnu\n> \u256d\u2500keiichi.kuroyanagi@instance-1 /usr/lib/x86_64-linux-gnu  \u2039system\u203a\n> \u2570\u2500$ ls libcudnn.so.7                                                                                                                                                 02:32:26\n> libcudnn.so.7@\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20271#issuecomment-421801384>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOWtNTsLo9DifdiT9ocNr2OKt42hZ0Nks5ubozmgaJpZM4U12Cl>\n> .\n>\n", "Hm, the links seem to be outdated and the install commands don't work. How to resolve this now??\r\n", "If you go to the generic linux download (Not for PowerX), which should be the 3rd one down for CUda 9.0, the tar.gz file has everything you need. The instructions are right there towards the middle of the webpage to extract it and copy the files to the correct location. Do make sure your  `LD_LIBRARY_PATH` path is set as stated by @shivank01 for libcudnn.so.7.\r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html\r\n![installationlink](https://user-images.githubusercontent.com/24778586/46674403-04310380-cbaa-11e8-88da-2e1e7716d5b6.png)\r\n", "Or if you use /etc/ld.co.conf/ to manage the library locations, you may have forgotten to run `ldconfig` after installing cuddnn.", "just:\r\nsudo ldconfig /usr/local/cuda/lib64", " you  add the following path in your .bashrc file:\r\n\r\n```\r\nexport PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:$LD_LIBRARY_PATH\r\n\r\n\r\nexport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\n```", "At first you should download cuda driver from the link https://developer.nvidia.com/rdp/cudnn-download. After you extracted tar file  you will have cuda directory , In this directory copy the contents of the include/cudnn.h to /usr/local/cuda-9.0/include/  and \r\nlib64/libcudnn* files to /usr/local/cuda-9.0/lib64 . \r\nAfter that , to make tensorflow to recognize these libraries you should edit path variable named LD_LIBRARY_PATH as the following;\r\n\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:$LD_LIBRARY_PATH\r\n", "none of the above solutions worked. The issue was with the symbolic links that got corrupted after following the official `nvidia` guide. The fix I applied is explained on this page: https://stackoverflow.com/a/61563579/1215913\r\n\r\nIn short: you have to remove the broken links and create new ones, then run the `sudo ldconfig -v`\r\n\r\n```\r\nsudo rm /usr/local/cuda/lib64/libcudnn.so\r\nsudo rm /usr/local/cuda/lib64/libcudnn.so.7\r\nsudo ln -s libcudnn.so.7.6.5 libcudnn.so.7\r\nsudo ln -s libcudnn.so.7 libcudnn.so\r\n\r\n```\r\n\r\n", "Here is what worked for me (Ubuntu):\r\n\r\n- Install CuDNN manually from https://developer.nvidia.com/rdp/cudnn-download. It will be installed in `/usr/lib/x86_64-linux-gnu/`\r\n- Create symlinks from your CUDA installation to that initial location:\r\n\r\n```\r\ncd /usr/local/cuda-10.1/lib64\r\nsudo ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5 libcudnn.so.7\r\nsudo ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5 libcudnn.so\r\n```", "@BramVanroy you are a $life saver! :) (.format{} and a few sleepless nights ;))\r\n#I had to link the libcudnn.so.8 to libcudnn.so.7 but still. \r\n", "In case it helps, here is how I set up a new conda environment from scratch to get:\r\n\r\n* Tensorflow version 2.2 (supports CPU and GPU)\r\n* Python 3.8\r\n* CUDA 10.1.243\r\n* CUDNN 7.6.5\r\n\r\n## Installation Steps\r\n\r\n```bash\r\n$ conda create -n tf_env python=3.8                            # create new conda env, with name \"tf_env\"\r\n$ conda activate tf_env\r\n$ pip install -U pip                                           # ensure new version available\r\n$ pip install -U tensorflow\r\n$ conda install -c anaconda cudatoolkit=10.1.243 cudnn=7.6.5   # the important part - versions must match\r\n$ pip install ipython\r\n```\r\n\r\n## Confirming success\r\n\r\nImport tensorflow, check it finds a GPU and also confirm the version:\r\n\r\n```python\r\n$ ipython\r\nPython 3.8.3 (default, May 19 2020, 18:47:26)\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.15.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: tf.config.list_physical_devices()\r\n\r\n2020-06-22 00:49:40.954473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-06-22 00:49:40.969881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: TITAN X (Pascal) computeCapability: 6.1\r\ncoreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s\r\n2020-06-22 00:49:40.970047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-22 00:49:40.971409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-22 00:49:40.972694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-22 00:49:40.972945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-22 00:49:40.974394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-22 00:49:40.975205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-22 00:49:40.979167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-22 00:49:40.980315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n\r\nOut[2]: \r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\r\n PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\r\n PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\r\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]           # GPU!\r\n\r\nIn [3]: tf.__version__\r\nOut[3]: '2.2.0'\r\n```\r\n\r\nThis was done on Ubuntu 18.04, but should work just as well for Windows. From what I could see, the same `cudatoolkit` and `cudnn` versions are available:\r\n\r\n* https://anaconda.org/anaconda/cudatoolkit/files?version=10.1.243\r\n* https://anaconda.org/anaconda/cudnn/files?version=7.6.5", "For what it's worth, I just followed @Nicholas-Mitchell 's instructions to get things working on WSL with python 3.7 on miniconda using the NVIDIA preview drivers to allow WSL to use GPU hardware.", "From a CUDA 10.2 install on Ubuntu 18.04, I had to :\r\n\r\n1. Create symlinks for CUDA 10.1 and export the links\r\n\r\n```console\r\nln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2 /usr/lib/x86_64-linux-gnu/libcudart.so.10.1\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\n```\r\n\r\n2. Install libcudnn for CUDA 10.1 : https://developer.nvidia.com/rdp/cudnn-archive\r\n\r\nTest with\r\n\r\n```python3.7\r\nimport tensorflow as tf\r\ntf.config.experimental.list_physical_devices('GPU')\r\n```\r\n", "Hello everyone,\r\nI solved the error by installing cuDNN 7.6. I followed these steps;\r\n\r\n1. Remove cuda tool kit from anaconda environment using command `conda remove cudatoolkit`\r\n\r\n2. Download **cuDNN v7.6.4** (September 27, 2019), for **CUDA 10.1** ([https://developer.nvidia.com/rdp/cudnn-archive](https://developer.nvidia.com/rdp/cudnn-archive))\r\n                        cuDNN _Runtime_ Library for Ubuntu18.04 (Deb)\r\n                        cuDNN _Developer_ Library for Ubuntu18.04 (Deb)\r\n                        cuDNN _Code Samples_ and User Guide for Ubuntu18.04 (Deb)\r\n\r\n3. Installed the above 3 files _in the same sequence_.", "`conda install -c anaconda cudnn ` can fix the issue. [from here](https://anaconda.org/anaconda/cudnn)", "> @BramVanroy you are a $life saver! :) (.format{} and a few sleepless nights ;))\r\n> #I had to link the libcudnn.so.8 to libcudnn.so.7 but still.\r\n\r\nHey  @xxxrokxxx @BramVanroy can you tell me how you linked libcudnn.so.8 to libcudnn.so.7", "> > @BramVanroy you are a $life saver! :) (.format{} and a few sleepless nights ;))\r\n> > #I had to link the libcudnn.so.8 to libcudnn.so.7 but still.\r\n> \r\n> Hey @xxxrokxxx @BramVanroy can you tell me how you linked libcudnn.so.8 to libcudnn.so.7\r\n\r\nIt's called symlinking or soft linking: https://askubuntu.com/questions/56339/how-to-create-a-soft-or-symbolic-link", "I tried almost all the solutions here and none of them  seemed to work. Eventually, I tried to `cat` the contents of my libcudnn.so files and turns out they were empty. Downloaded them again, copied them to the right place again, and it all worked. (It's confusing enough as is I don't need the confusion from bad downloads :( )", "> sudo ldconfig /usr/local/cuda/lib64\r\n\r\nYes Sir, it worked, Thank so much,,,can you just tell the reason for it?", "> > sudo ldconfig /usr/local/cuda/lib64\r\n> \r\n> Yes Sir, it worked, Thank so much,,,can you just tell the reason for it?\r\n\r\nYes, you need to inform the system to change these *.so files", "> In case it helps, here is how I set up a new conda environment from scratch to get:\r\n> \r\n> * Tensorflow version 2.2 (supports CPU and GPU)\r\n> * Python 3.8\r\n> * CUDA 10.1.243\r\n> * CUDNN 7.6.5\r\n> \r\n> ## Installation Steps\r\n> ```shell\r\n> $ conda create -n tf python=3.8     # create new env, with name \"tf\"\r\n> $ pip install -U pip             # ensure new version available\r\n> $ conda activate tf \r\n> $ pip install -U tensorflow\r\n> $ conda install -c anaconda cudatoolkit=10.1.243 cudnn=7.6.5   # the important part - versions must match\r\n> $ pip install ipython\r\n> ```\r\n> \r\n> ## Confirming success\r\n> Import tensorflow, check it finds a GPU and also confirm the version:\r\n> \r\n> ```python\r\n> $ ipython\r\n> Python 3.8.3 (default, May 19 2020, 18:47:26)\r\n> Type 'copyright', 'credits' or 'license' for more information\r\n> IPython 7.15.0 -- An enhanced Interactive Python. Type '?' for help.\r\n> \r\n> In [1]: import tensorflow as tf\r\n> \r\n> In [2]: tf.config.list_physical_devices()\r\n> \r\n> 2020-06-22 00:49:40.954473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n> 2020-06-22 00:49:40.969881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\n> pciBusID: 0000:01:00.0 name: TITAN X (Pascal) computeCapability: 6.1\r\n> coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s\r\n> 2020-06-22 00:49:40.970047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n> 2020-06-22 00:49:40.971409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2020-06-22 00:49:40.972694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n> 2020-06-22 00:49:40.972945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n> 2020-06-22 00:49:40.974394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-06-22 00:49:40.975205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-06-22 00:49:40.979167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-06-22 00:49:40.980315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n> \r\n> Out[2]: \r\n> [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\r\n>  PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\r\n>  PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\r\n>  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]     # GPU!\r\n> \r\n> In [3]: tf.__version__\r\n> Out[3]: '2.2.0'\r\n> ```\r\n> \r\n> This was done on Ubuntu 18.04, but should work just as well for Windows. From what I could see, the same `cudatoolkit` and `cudnn` versions are available:\r\n> \r\n> * https://anaconda.org/anaconda/cudatoolkit/files?version=10.1.243\r\n> * https://anaconda.org/anaconda/cudnn/files?version=7.6.5\r\n\r\nthanks a lot, it solve my problem\r\nwish you best", "> Or if you use /etc/ld.co.conf/ to manage the library locations, you may have forgotten to run `ldconfig` after installing cuddnn.\r\n\r\nThis one solves my issue! Thanks.", "> @BramVanroy you are a $life saver! :) (.format{} and a few sleepless nights ;)) #I had to link the libcudnn.so.8 to libcudnn.so.7 but still.\r\n\r\nFor cuda11.4 & cudnn8.2.4\r\n```bash\r\ncd /usr/local/cuda-11.4/lib64\r\nln -snf libcudnn.so.8.2.4 libcudnn.so.8\r\nln -snf libcudnn.so.8 libcudnn.so\r\n```", "> Or if you use /etc/ld.co.conf/ to manage the library locations, you may have forgotten to run `ldconfig` after installing cuddnn.\r\n\r\nThanks\uff01It works well\uff01\uff01"]}, {"number": 20270, "title": "Could the site refer change?", "body": "Hi ,in https://www.tensorflow.org/mobile/tflite/demo_android \r\nthe demo site (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app)\r\nactually is direct,but i found i could not successful build it cuz various problem\r\nAnd  occasionally i found this site:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo\r\nit is much better and i can easily deploy on my phone without any change,Great!", "comments": ["Hi @zfk513. If I understand correctly, it appears that the [Android Demo App](https://www.tensorflow.org/mobile/tflite/demo_android) example application link brings you inside the `app` directory within [tensorflow/tensorflow/contrib/lite/java/demo/](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo). \r\n\r\nYou are proposing that this link be changed from [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app) to [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo)\r\n\r\nIs that correct?", "@marshalhayes \r\nyeah,thx for translating", "Sounds fair enough, will send out a change.", "@asimshankar thank you "]}, {"number": 20269, "title": "PIP user-op tests-on-install FAILED", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.9, cpu\r\n- **Python version**: \r\nPython 3.5\r\n- **Bazel version (if compiling from source)**:\r\n0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\nci_parameterized_build.sh with TF_BUILD_IS_PIP=PIP\r\n\r\n### Describe the problem\r\nI am using ci_parameterized_build.sh to build tensorflow whl in our CI system.\r\nAfter https://github.com/tensorflow/tensorflow/commit/fdbb80f217d3a153b4eda66c766df921b3f73ab4, i found do_test_user_ops kept failing due the the following error.\r\n\r\nJust let you guys know, @case540 as well.\r\nThanks.\r\n\r\n\u001b[1m=== PIP test step 4 of 7: do_test_user_ops (User ops test) ===\u001b[0m\r\n\r\n=== Testing user ops ===\r\nPYTHON_BIN_PATH: /workspace/pip_test/venv/bin/python\r\n/tmp/tmp.o7UMG4on4E /workspace\r\nTensorFlow compile flags: -I/workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1\r\nTensorFlow link flags: -L/workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow -ltensorflow_framework\r\n\r\ng++ version:\r\nUsing built-in specs.\r\nCOLLECT_GCC=g++\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper\r\nTarget: x86_64-linux-gnu\r\nlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) \r\n\r\nExtra GPP flag: \r\nTesting user ops in CPU environment\r\nero_out_op_kernel_1.cc\r\nIn file included from /workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:20:0,\r\n\t\t\t  from /workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/framework/allocator.h:23,\r\n\t\t\t  from /workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:23,\r\nero_out_op_kernel_1.cc:17:\r\n/workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\r\ncompilation terminated.\r\nero_out_op_kernel_1.cc FAILED\r\nPIP user-op tests-on-install FAILED\r\n\r\n\r\n", "comments": ["@case540 just fixed this at head.\r\n\r\nHowever, ci_parameterized_build.sh is going away. You should write your own test script or use another one under `tensorflow/tools/ci_build/linux`"]}, {"number": 20268, "title": "HI what is the difference between examples and java folder's demo", "body": "Can anyone tell me which tensorflow lite demo is good,cuz i met problem with both\r\n1.https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app\r\nand the problem is Plugin with id 'com.android.application' not found.\r\n2.https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android\r\nand this is more weird ,BUILD\\android-profile \r\nthx\r\nas 3.0 \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20267, "title": "TensorRT - KeyError: 'shape'", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution: Linux Ubuntu 16.04**\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n> tensorflow (1.8.0)\r\n> tensorrt (4.0.1.6)\r\n\r\n- **Python version**: 2.7.14 \r\n- **Bazel version (if compiling from source)**:None\r\n- **CUDA/cuDNN version**: 9.0\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nGetting the following error while trying to optimize the predict node in from a frozen graph of an estimator-based transformer model. This is the line where I get the issue : \r\n`uff_model = uff.from_tensorflow_frozen_model(frozen_file, [\"train/enc_0\"])`\r\n\r\n### Source code / logs\r\n```\r\nWarning: No conversion function registered for layer: IteratorGetNext yet.\r\nConverting as custom op IteratorGetNext train/IteratorGetNext\r\nname: \"train/IteratorGetNext\"\r\nop: \"IteratorGetNext\"\r\ninput: \"train/Iterator\"\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: -1\r\n        }\r\n      }\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: -1\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_types\"\r\n  value {\r\n    list {\r\n      type: DT_INT32\r\n      type: DT_INT32\r\n    }\r\n  }\r\n}\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorRT_optimize.py\", line 25, in <module>\r\n    uff_model = uff.from_tensorflow(\"logs/check_tiny/frozen_model_new.pb\", ['train/pred_0'])\r\n  File \"/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/conversion_helpers.py\", line 120, in from_tensorflow\r\n    name=\"main\")\r\n  File \"/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py\", line 76, in convert_tf2uff_graph\r\n    uff_graph, input_replacements)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py\", line 63, in convert_tf2uff_node\r\n    op, name, tf_node, inputs, uff_graph, tf_nodes=tf_nodes)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py\", line 38, in convert_layer\r\n    fields = cls.parse_tf_attrs(tf_node.attr)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py\", line 209, in parse_tf_attrs\r\n    for key, val in attrs.items()}\r\n  File \"/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py\", line 209, in <dictcomp>\r\n    for key, val in attrs.items()}\r\n  File \"/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py\", line 204, in parse_tf_attr_value\r\n    return cls.convert_tf2uff_field(code, val)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py\", line 189, in convert_tf2uff_field\r\n    'type': 'dtype', 'list': 'list'}[code]\r\nKeyError: 'shape'\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nGPU model and memory", "@Aashit-Sharma ,\r\n\r\nThis is an UFF issue. Please report this to Nvidia forums for TensorRT.", "hi @Aashit-Sharma , did you solve the issue?, I got a similar error:\r\n\r\n```\r\nWarning: No conversion function registered for layer: IteratorGetNext yet.\r\nConverting as custom op IteratorGetNext IteratorGetNext\r\nname: \"IteratorGetNext\"\r\nop: \"IteratorGetNext\"\r\ninput: \"OneShotIterator\"\r\nattr {\r\n  key: \"_output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 8\r\n        }\r\n        dim {\r\n          size: 224\r\n        }\r\n        dim {\r\n          size: 224\r\n        }\r\n        dim {\r\n          size: 3\r\n        }\r\n      }\r\n      shape {\r\n        dim {\r\n          size: 8\r\n        }\r\n        dim {\r\n          size: -1\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 8\r\n        }\r\n        dim {\r\n          size: 224\r\n        }\r\n        dim {\r\n          size: 224\r\n        }\r\n        dim {\r\n          size: 3\r\n        }\r\n      }\r\n      shape {\r\n        dim {\r\n          size: 8\r\n        }\r\n        dim {\r\n          size: -1\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_types\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n      type: DT_INT64\r\n    }\r\n  }\r\n}\r\n\r\nTraceback (most recent call last):\r\n  File \"tf_to_trt_chest_bhavesh_model.py\", line 188, in <module>\r\n    main()\r\n  File \"tf_to_trt_chest_bhavesh_model.py\", line 158, in main\r\n    uff_model = uff.from_tensorflow(tf_model, [\"probs\"])\r\n  File \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py\", line 120, in from_tenso                             rflow\r\n    name=\"main\")\r\n  File \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 76, in convert_tf2uff_graph\r\n    uff_graph, input_replacements)\r\n  File \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 63, in convert_tf2uff_node\r\n    op, name, tf_node, inputs, uff_graph, tf_nodes=tf_nodes)\r\n  File \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 38, in convert_layer\r\n    fields = cls.parse_tf_attrs(tf_node.attr)\r\n  File \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in parse_tf_attrs\r\n    for key, val in attrs.items()}\r\n  File \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in <dictcomp>\r\n    for key, val in attrs.items()}\r\n  File \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 204, in parse_tf_attr_value\r\n    return cls.convert_tf2uff_field(code, val)\r\n  File \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 189, in convert_tf2uff_fiel                             d\r\n    'type': 'dtype', 'list': 'list'}[code]\r\nKeyError: 'shape'\r\n```\r\n\r\n", "@vilmara, your problem is related with uff which is pure tensorrt and not related with tensorflow. Please raise the issue in NVIDIA forums."]}, {"number": 20266, "title": "tensor flow does not install on windows 10", "body": "Windows 10, TensorFlow 1.8.1, NVIDIA 1060\r\nWas working.  Had to resinstall OS.  Now I always get error below with\r\nCUDA 8.0, CUDA 9.0, CUDA 9.2, Visual Studio 2015, Visual Studio 2017 (have tried everything I can think of).  Any pointers for how to diagnose/solve?\r\n\r\n(tensorflow) v:\\deeplearning>python -W ignore test.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\jdm\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["A bit of a shot in the dark since I don't see the same code from your stack-trace in the current state of the repo but since you ask for pointers, here goes:\r\nYou could use gflags to monitor dll loading when you run python -W ignore test.py. \r\n- Get gflags from Microsoft: https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/gflags-commands\r\n- Run and open the Image tab\r\n- Enter Python.exe as an image\r\n- Select 'show loader snaps'\r\n\r\nBrace yourself for somewhat verbose output (remember to unset this after you've solved your issue) but you should see which dlls are being loaded and from where. You'll probably see one trying to load from each directory in your path and failing right before the stack-trace.\r\n\r\nAdditionally, once you've found a couple of suspect dlls, you can use dependency walker show information regarding the dlls that they will try to load. Dependency walke has a 32b and 64b version, they don't see all dlls equally, so you might need to use both.\r\nHope this helps and good luck", "Nagging Assignee @tatianashp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thank you.", "Finally tried this out.\r\nIn windows, cd ... (the folder where the python program was).\r\nTyped gflags.  Entered on image tab Python.exe and checked \"show loader snaps\".\r\nClicked \"Launch\".\r\nPython launched.\r\nTyped \"import test_mnist\" to run test_minst.py.\r\nSame failure starting tensorflow (\"A dynamic link library (DLL) initialization routine failed\").\r\nNo additional output - nothing to help trying to find out what DLL failed.\r\nI apologize for lack of knowledge here: I probably am missing a basic primer on getting output out of gflags.  Jon\r\n", "If you are running on an older processor that has no support for AVX instructions it might be the same issue as discussed #19584.  ", "Thanks.\r\nWell, I went into Anaconda3 navigator and deleted the entire environment, created the environment again, and installed tensorflow-gpu, keras-gpu, etc etc only in Anaconda navigator (no \"pip install \u2026\") and now it all works.  Finally.  Sorry to raise this issue!"]}, {"number": 20265, "title": "dataset.map function needs better documentation", "body": "Can the documentation for dataset.map be fleshed out a bit? Most importantly, what are the input and output arguments for the mapping function? The current documentation implies there is only one return, but all the code examples show (input,output).  What is the meaning of the input and output args?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "cc @mrry "]}, {"number": 20264, "title": "Update docstring of tf.linespace for 0-D inputs", "body": "In #20258 the question was raised as tf.linespace only supports 0-D inputs of start/stop, but the docstring did not mention the 0-D restriction.\r\n\r\nThis fix updates the docstring to address the discrepancy.\r\n\r\nThis fix fixes #20258.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for the fix of document."]}]