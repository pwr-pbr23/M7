[{"number": 15833, "title": "gradient function missing for odeint", "body": "I am trying to program a optimization problem using TensorFlow. The cost function depends on the outcome of an ODE solved by TensorFlow. However I find that TensorFlow's auto differential does not support the function which contains odeint.\r\n\r\nodeint: tensorflow.org/api_docs/python/tf/contrib/integrate/odeint\r\n\r\nIt would great be of great help if we could have it.\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: Ubuntu 16.04 \r\nTensorFlow installed from: official website pip \r\nTensorFlow version: 14.01\r\nBazel version: N/A\r\nCUDA/cuDNN version: 8.0\r\nGPU model and memory: GTX1080Ti, not relevant\r\nExact command to reproduce: N/A\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I don't think this problem has been solved @tensorflowbutler ", "/CC @shoyer", "@LionSR Can you provide code that reproduces this issue and report the error message you find?\r\n\r\nAs a simple example, here how you could modify the Lorenz equation example from the docs to optimize the initial conditions for the ODE:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nrho = 28.0\r\nsigma = 10.0\r\nbeta = 8.0/3.0\r\n\r\ndef lorenz_equation(state, t):\r\n  x, y, z = tf.unstack(state)\r\n  dx = sigma * (y - x)\r\n  dy = x * (rho - z) - y\r\n  dz = x * y - beta * z\r\n  return tf.stack([dx, dy, dz])\r\n\r\ntarget = tf.constant([0, 2, 20], dtype=tf.float64)\r\ninit_state = tf.Variable(target)\r\nt = np.linspace(0, 0.5, num=100)\r\ntensor_state, tensor_info = tf.contrib.integrate.odeint(\r\n    lorenz_equation, init_state, t, full_output=True)\r\n\r\nloss = tf.reduce_sum((tensor_state[-1] - target) ** 2)\r\ntrain_step = tf.train.GradientDescentOptimizer(5e-2).minimize(loss)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nlosses = []\r\ntrajectories = []\r\nfor i in range(100):\r\n  loss_eval, trajectory, _ = sess.run([loss, tensor_state, train_step])\r\n  losses.append(loss_eval)\r\n  trajectories.append(trajectory)\r\n  if i % 10 == 0:\r\n    print(loss_eval)\r\n```\r\nIt doesn't entirely converge (this problem is probably non-convex), but the loss decreases over time. The program prints:\r\n```\r\n663.541165873\r\n4.66117861453\r\n3.95432178915\r\n3.5868347332\r\n3.37596507952\r\n3.25978177698\r\n3.19799942176\r\n3.16602229662\r\n3.14980148132\r\n3.14169093144\r\n```", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to a lack of a response.", "The issue is not resolved, at least when manually recoring the gradient on tapes.\r\nTo be precise, it is not  possible to compute the gradients of the results of odeint.\r\nThis minimal example:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef f(x, t):\r\n    return 2*x\r\n\r\nx_init = tf.ones((1))\r\nt_list = tf.range(0, 10, 0.01)\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(x_init)\r\n    run = tf.contrib.integrate.odeint(f, y0=x_init, t = t_list)\r\n\r\ndiff = tape.gradient(run, x_init)\r\n```\r\n\r\nraises the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-e630f0280360>\", line 12, in <module>\r\n    diff = tape.gradient(run, x_init)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 851, in gradient\r\n    output_gradients=output_gradients)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\", line 63, in imperative_grad\r\n    tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 116, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 158, in _TensorArrayGatherGrad\r\n    grad_source = _GetGradSource(grad)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 74, in _GetGradSource\r\n    \", got: %s\" % op_or_tensor.name)\r\nValueError: Expected op/tensor name to start with gradients (excluding scope), got: Const:0\r\n\r\nI am using tf version 1.13.1\r\n", "We are porting `odeint` over into [TensorFlow Probability](https://github.com/tensorflow/probability) (tf.contrib is being removed in TensorFlow 2.0) -- see https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/math/ode.\r\n\r\nIt would be good to check if gradients work there -- it's unlikely we will fix this here given the deprecated status of tf.contrib.", "I tried with tf-nightly and tfp-nightly (the tfp release does not yet include ode), and the ode solver is not differentiable with gradient tapes. When trying to differentiate the result of the solver, the gradient function returns None ( see below).\r\nOr maybe I am getting something wrong, and differentiation works differently in tensorflow-probability? I just used the solver from tfp, and the rest I left as ik works in pure tf.\r\n\r\n```import tensorflow as tf\r\n\r\nimport tensorflow_probability as tfp\r\n\r\ndef f(x, t):\r\n    return 2*x\r\n\r\nx_init = tf.ones((1))\r\nt_list = tf.range(0, 10, 1.)\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(x_init)\r\n    solver = tfp.math.ode.BDF()\r\n    times, states, _,_ = solver.solve(f, initial_time=t_list[0], initial_state=x_init, solution_times=t_list)\r\n\r\ndiff = tape.gradient(states, x_init)\r\n# diff is None\r\nprint(diff)```", "@seb1000 thanks for giving this a try, it looks like this should work. I just opened https://github.com/tensorflow/probability/issues/405 to discuss this in TF-probability", "thanks for opening it in TF-probability, I will follow it there."]}, {"number": 15832, "title": "Feature Request: saver.save mkdir if directory not exist", "body": "### Describe the problem\r\n`saver.save(sess, 'my-model') `\r\nreturns error if directory not exist.\r\nIt would be nice if saver can automatically create the missing directory.\r\n`Traceback (most recent call last):\r\n  File \"tf_voice_recognition.py\", line 783, in <module>\r\n    saver.save(sess, 'my-model')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1594, in save\r\n    raise exc\r\nValueError: Parent directory of my-model doesn't exist, can't save.\r\n`", "comments": ["You can use absolute path.", "@formath Worked. Thanks. What about a feature to allow relative paths as well?", "It's not ok to use relative path in distributed training such as hdfs. So, absolute path is more general.", "@formath I see. Thank you."]}, {"number": 15831, "title": "Using keras and tf.keras has different result", "body": "### System information\r\n- **TensorFlow)**:\r\n- **Linux Ubuntu 16.04**:\r\n- **TensorFlow installed from Binary**:\r\n- **TensorFlow version (1.4)**:\r\n- **Python version 3.6.3**: \r\n\r\n### Describe the problem\r\nI am using tf.keras and keras but it has a different result. I am using the code below for sentiment analysis classification using imdb dataset.\r\n### Source code / logs\r\n```\r\nmodel = Sequential()\r\nmodel.add(Embedding(n_unique_words, n_dim, input_length=max_reviw_length))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(n_dense, activation='relu'))\r\nmodel.add(Dropout(dropout))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\nmodel.summary() \r\n```\r\n\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding_1 (Embedding)      (None, 100, 64)           320000    \r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 6400)              0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 64)                409664    \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 64)                0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 1)                 65        \r\n=================================================================\r\nTotal params: 729,729\r\nTrainable params: 729,729\r\nNon-trainable params: 0\r\n__________________________\r\n\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), callbacks=[modelcheckpoint])\r\n```\r\npreprocessing all exact the same\r\n\r\ntf.keras result\r\n\r\n```\r\nTrain on 25000 samples, validate on 25000 samples\r\nEpoch 1/4\r\n25000/25000 [==============================] - 9s - loss: 0.7412 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.50000.4\r\nEpoch 2/4\r\n25000/25000 [==============================] - 7s - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5000\r\nEpoch 3/4\r\n25000/25000 [==============================] - 7s - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5000\r\nEpoch 4/4\r\n25000/25000 [==============================] - 7s - loss: 0.6932 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5000\r\n\r\n<tensorflow.python.keras._impl.keras.callbacks.History at 0x7fad685e7c18>\r\n```\r\n\r\nkeras result\r\n\r\n```\r\nTrain on 25000 samples, validate on 25000 samples\r\nEpoch 1/4\r\n25000/25000 [==============================] - 9s 344us/step - loss: 0.5607 - acc: 0.6900 - val_loss: 0.3622 - val_acc: 0.8404\r\nEpoch 2/4\r\n25000/25000 [==============================] - 8s 308us/step - loss: 0.2849 - acc: 0.8849 - val_loss: 0.3486 - val_acc: 0.8446\r\nEpoch 3/4\r\n25000/25000 [==============================] - 8s 305us/step - loss: 0.1179 - acc: 0.9642 - val_loss: 0.4183 - val_acc: 0.8339\r\nEpoch 4/4\r\n25000/25000 [==============================] - 8s 307us/step - loss: 0.0252 - acc: 0.9960 - val_loss: 0.5202 - val_acc: 0.8345\r\n\r\n<keras.callbacks.History at 0x7f560b85d940>\r\n\r\n\r\n```", "comments": ["Similar with #15026, @hhao89 ?", "Yes, I think this is the same problem as https://github.com/tensorflow/tensorflow/issues/15026 , and similarly, keras is shown to have a consistently better performance. ", "I am running into the same problem described here and in #15026. The TensorFlow implementation of keras gives very bad results in terms of loss and accuracy while the standalone keras performs as expected\r\n  \r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 137 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 152 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Similar issue. How to solve this issue\uff1f@rubiagatra", "@budach @facaiy @hhao89 ", "> Similar issue. How to solve this issue\uff1f@rubiagatra\r\n\r\nIt's there any solution to resolve this issue?@fchollet @facaiy @hhao89 @budach ", "see [https://github.com/tensorflow/tensorflow/issues/15026#issuecomment-513819963](url)", "Having the same issue with Keras version `2.3.1` versus tf version `2.1.0-dev20191228` and tf.keras version`2.2.4-tf`\r\n\r\nIn my case this is the model:\r\n```\r\n    inputlayer = Input(shape=(100, 3))\r\n    model = BatchNormalization()(inputlayer)\r\n    model = Conv1D(10, kernel_size=(3), padding='same')(model)\r\n    model = Dropout(dropout_rate)(model)\r\n    model = Conv1D(20, kernel_size=(2), padding='same')(model)\r\n    model = Dropout(dropout_rate)(model)\r\n    model = LSTM(128, return_sequences=True, input_shape=(100, 3), batch_size=1)(model)\r\n    model = Dropout(dropout_rate)(model)\r\n    model = LSTM(128, batch_size=1)(model)\r\n    model = Dense(output_size, activation='softmax')(model)\r\n    model = Model(inputs=inputlayer, outputs=model)\r\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_crossentropy', 'categorical_accuracy', 'accuracy'])\r\n```\r\n\r\n`tf.keras` does not converge while `keras` shows good results.", "@yairlev \r\nI had a same issue. And I found the reason that BatchNormalization is different in tf.keras and pure keras.\r\nMy environment: tensorflow = 1.14.0, tf.keras = 2.2.4-tf, keras = 2.2.4.\r\n\r\nThe BatchNormalization in tf.keras has the parameters 'trainable' and 'fused'. But in pure keras, there are no 'trainable' and 'fused'.\r\nI had added parameters trainable=False and fuse=False, then the loss in tf.keras went down.\r\n \r\nThe newer version pure keras already has 'trainable' and 'fused'. So this different is only in old versions.\r\nI hope this helpful."]}, {"number": 15830, "title": "tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1)", "body": "###System information\r\nHave I written custom code: Yes, \r\nOS Platform and Distribution: Ubuntu14.04\r\nTensorFlow installed from: source build w/ Bazel\r\nTensorFlow version: 1.4\r\nPython version: Anaconda 3.5.2\r\nBazel version: 0.9.0\r\nGCC/Compiler version (if compiling from source): gcc version 4.8.4\r\nCUDA/cuDNN version: Not relevant\r\nGPU model and memory: Not  relevant\r\nExact command to reproduce: Not relevant\r\n\r\n\r\n### Describe the problem\r\n\r\nI construct a network with only bilinear_resize operation (I use the tf.image.resize_bilinear) and add operation, and  convert it to a tflite model successfully. However,  when I run the tflite mode, it comes to the errors as follows:\r\n\r\njava.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1)\r\n\r\nI find the code line in resize_bilinear.cc:42 as follows:\r\nTF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\r\n\r\nIs it right to modify the code line to : \r\nTF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2); \r\n   \r\n\r\n\r\n### Source code / logs\r\ndef network():\r\n      img = tf.placeholder(name='img', dtype=tf.float32, shape=(1,100,100,3))\r\n      img = tf.layers.conv2d(img, 3, (3,3), padding='same', name='conv1')\r\n      img = tf.image.resize_bilinear(img, [200,200])\r\n      var = tf.get_variable('weights', dtype=tf.float32, shape=(1,200,200,3))\r\n      val = img + var\r\n      out = tf.identity(val, name='out')\r\n\r\n\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Fixed by https://github.com/tensorflow/tensorflow/commit/67cd3121296671d32c8150e9e57ac8f296f367ae\r\n\r\nPlease reopen if you still experience this problem."]}, {"number": 15829, "title": "[Bazel/Windows] Wrap rm -rf in Bash for Windows (and some refactoring)", "body": "`rm -rf` is not available in Windows command prompt. Run it in Bash like `patch -pl`.\r\n\r\nRefactor the wrapping logic into a new macro `_wrap_bash_cmd`.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "One CI test failed:  `//tensorflow/python:framework_importer_test`. Should have nothing to do with my change."]}, {"number": 15828, "title": "Merging 1.5.0-rc0 back to master.", "body": "", "comments": []}, {"number": 15827, "title": "ci.tensorflow.org lacks a security certificate", "body": "### The Problem\r\nhttps://ci.tensorflow.org/ now lacks a security certificate (or it expired).\r\nTo repro, visit https://ci.tensorflow.org/. Chrome will note that the connection is not private.\r\n\r\nThis is breaking TensorBoard's tests that run on travis. The tests `pip install` nightly versions of TensorFlow from these URLs:\r\n\r\n- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl \r\n- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl\r\n\r\n### Error Logs from a Failed Test Run\r\n\r\n> Collecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl\r\n> Exception:\r\n> Traceback (most recent call last):\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main\r\n>     status = self.run(options, args)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/commands/install.py\", line 335, in run\r\n>     wb.build(autobuilding=True)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/wheel.py\", line 749, in build\r\n>     self.requirement_set.prepare_files(self.finder)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py\", line 380, in prepare_files\r\n>     ignore_dependencies=self.ignore_dependencies))\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py\", line 620, in _prepare_file\r\n>     session=self.session, hashes=hashes)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py\", line 821, in unpack_url\r\n>     hashes=hashes\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py\", line 659, in unpack_http_url\r\n>     hashes)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py\", line 853, in _download_http_url\r\n>     stream=True,\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 488, in get\r\n>     return self.request('GET', url, **kwargs)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py\", line 386, in request\r\n>     return super(PipSession, self).request(method, url, *args, **kwargs)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 475, in request\r\n>     resp = self.send(prep, **send_kwargs)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 596, in send\r\n>     r = adapter.send(request, **kwargs)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py\", line 47, in send\r\n>     resp = super(CacheControlAdapter, self).send(request, **kw)\r\n>   File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py\", line 497, in send\r\n>     raise SSLError(e, request=request)\r\n> SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)\r\n\r\n  ", "comments": ["This bug seems related to #12496.", "We are going to try using `pip install tf-nightly` instead of pip installing the whl URL.", "The current certificate expired, but we are looking into killing off ci.tensorflow.org in favor of kokoro, too.\r\nIn the meantime, would it be OK if we dropped SSL from ci.tensorflow.org and used `http://ci.tensorflow.org/` ?", "Thank you @gunan! That is fine because we found that `pip install tf-nightly` works.\r\n\r\nWe are now grappling with a different issue. ", "The certificate is renewed and deployed.", "Thank you @gunan for the speedy response!"]}, {"number": 15826, "title": "Remove :0 in final result argument", "body": "Small change to line 393 - in my recent testing of retrain and the label_image workflow as of today (TF master @  136697e) I had to remove :0 from the label_image output_layer argument.\r\n\r\nThank you.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "@vade has signed the CLA. Merging the PR. Thanks."]}, {"number": 15825, "title": "Update advise.md", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 15824, "title": "Move SpeechActivity animation to XML.", "body": "Moved SpeechActivity animation to `res/animator/color_animation.xml`.\r\n\r\nSpecifying the animation in code is tougher to read and detracts from what is supposed to be a simple example of how to use TF in Android.", "comments": ["Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 15823, "title": "Fix a pessimizing-move warning in GetDeviceLapackInfo()", "body": "clang reports:\r\n<pre>\r\n./tensorflow/core/kernels/cuda_solvers.h:430:10: warning: moving a local object in a return statement prevents copy elision [-Wpessimizing-move]\r\n  return std::move(new_dev_info);\r\n         ^\r\n./tensorflow/core/kernels/cuda_solvers.h:430:10: note: remove std::move call here\r\n  return std::move(new_dev_info);\r\n         ^~~~~~~~~~            ~\r\n</pre>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "It looks like the Ubuntu CC build stalled. Can it be restarted?"]}, {"number": 15822, "title": "Updating error handling in normalize_tuple", "body": "In normalize_tuple we test to see if all values are an int (or able to be cast to an int) using `int()`\r\n\r\n`ValueError` is thrown if `int()` is called with an input like 'asdf' - this is caught and gives a helpful error, using the 'name' param to provide more context.\r\n\r\n**However**, when given an input other than a string or int, a `TypeError` is thrown. This is **not** caught - making error messages much more esoteric than the helpful one written out here, especially when coming from a very long stack trace.\r\n\r\nFor example, before this change I was getting an error:\r\n```\r\nTypeError: int() argument must be a string or a number, not 'tuple'\r\n```\r\n\r\nWith this change, I get the more useful:\r\n```\r\nValueError: The `kernel_size` argument must be a tuple of 2 integers. \r\nReceived: ((0, 3), 50) including element (0, 3) of type <type 'tuple'>\r\n```\r\n  \r\n  ", "comments": ["Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 15821, "title": "TFGAN not compatible with eager execution mode", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colaboratory Google Compute Engine backend (not sure about OS here)\r\n- **TensorFlow installed from (source or binary)**: binary (non-GPU version)\r\n- **TensorFlow version (use command below)**: 1.5.0-dev20180102\r\n- **Python version**: 3\r\n\r\n### Describe the problem\r\nWhen enabling eager execution mode\r\n```python\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n```\r\nand running \r\n```python\r\nnoise_dims = 64\r\ngan_model = tfgan.gan_model(\r\n    generator_fn,\r\n    discriminator_fn,\r\n    real_data=images,\r\n    generator_inputs=tf.random_normal([batch_size, noise_dims]))\r\n```\r\nfrom the [TFGAN tutorial](https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb) by @joel-shor, I get the following error:\r\n```bash\r\nValueError: When Eager Execution is enabled, variable collections are not supported.\r\n```\r\nbecause of the following lines:\r\nhttps://github.com/tensorflow/tensorflow/blob/8c2d6fc2b0202304885d5d6c3cba57eb2a1b3262/tensorflow/contrib/gan/python/train.py#L119-L121\r\n.\r\n\r\nAre there any plans to make TFGAN compatible with eager in the short term? Is there any help wanted in this regard? I'd be happy to contribute.", "comments": ["@iganichev should have an example of a GAN using eager execution shortly and may have more to add.", "Currently, I'm able to create a GAN using eager execution, without making use of TFGAN - _see [notebook on generating samples from 1-D Gaussian](https://github.com/jppgks/shenanigan/blob/master/1-generating-samples-from-1D-gaussian.ipynb) for context_:\r\n\r\n1. Enable eager\r\n\r\n```python\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n```\r\n\r\n2. Define the model\r\n\r\n```python\r\nclass MLP(tfe.Network):\r\n  def __init__(self):\r\n    super(MLP, self).__init__()\r\n    self.layer1 = self.track_layer(tf.layers.Dense(units=30, activation=tf.nn.tanh))\r\n    self.layer2 = self.track_layer(tf.layers.Dense(units=1, activation=tf.nn.tanh))\r\n    \r\n  def call(self, input):\r\n    \"\"\"Actually runs the model.\"\"\"\r\n    result = self.layer1(input)\r\n    result = self.layer2(result)\r\n    return result\r\n```\r\n\r\n3. Create generator and discriminator\r\n\r\n```python\r\nwith tf.variable_scope(\"G\"):\r\n  G = MLP()\r\nwith tf.variable_scope(\"D\"):\r\n  D = MLP()\r\n```\r\n\r\n4. Define optimizers, ...\r\n\r\n```python\r\n# For the generator\r\nbatch_g = tfe.Variable(0)\r\nlearning_rate = tf.train.exponential_decay(\r\n  0.001,  # Base learning rate.\r\n  batch_g,  # Current index into the dataset.\r\n  TRAIN_ITERS // 4,  # Decay step - this decays 4 times throughout training process.\r\n  0.95,  # Decay rate.\r\n  staircase=True)\r\nopt_g=tf.train.MomentumOptimizer(learning_rate, 0.6)\r\n\r\n# For the discriminator\r\nbatch = tfe.Variable(0)\r\nlearning_rate = tf.train.exponential_decay(\r\n  0.001,  # Base learning rate.\r\n  batch,  # Current index into the dataset.\r\n  TRAIN_ITERS // 4,  # Decay step - this decays 4 times throughout training process.\r\n  0.95,  # Decay rate.\r\n  staircase=True)\r\ndiscriminator_optimizer=tf.train.MomentumOptimizer(learning_rate, 0.6)\r\n```\r\n\r\n5. ...and value functions\r\n\r\n```python\r\n# Used when taking tf.log(), so we don't end up doing tf.log(0)\r\nTINY = tf.constant(1e-8, dtype=tf.float64)\r\n\r\ndef value_function(D, G, x, z):\r\n  # D(x), x sampled from data distribution\r\n  p_norm_data = D(x) # output likelihood of being normally distributed\r\n  D1 = tf.maximum(tf.minimum(p_norm_data, .99), 0.01) # clamp as a probability\r\n\r\n  # D(x), x sampled from model distribution as G(z)\r\n  generated_samples = G(z)\r\n  generated_samples = tf.multiply(5.0, generated_samples)\r\n  p_norm_gen = D(generated_samples)\r\n  D2 = tf.maximum(tf.minimum(p_norm_gen, .99), 0.01)\r\n  \r\n  # v(\\theta^g, \\theta^d)\r\n  loss = tf.reduce_mean(tf.log(D1 + TINY) + tf.log(1 - D2 + TINY))\r\n  return loss\r\n\r\ndef neg_value_function(D, G, x, z):\r\n  '''Because our optimizers will minimize the loss function passed in,\r\n  we define the _negative_ value function.\r\n  \r\n  Instead of maximizing the value function, \r\n  the discriminator will minimize this _negative_ value function.\r\n  '''\r\n  return -value_function(D, G, x, z)\r\n```\r\n\r\n6. Train\r\n\r\n```python\r\n# Algorithm 1 of Goodfellow et al. (2014)\r\n\r\n# Number of discriminator updates per epoch\r\nk = 5\r\n# Setup log of loss throughout training\r\nhistd, histg = np.zeros(TRAIN_ITERS), np.zeros(TRAIN_ITERS)\r\n\r\nfor i in range(TRAIN_ITERS):\r\n  # Train discriminator.\r\n  for j in range(k):\r\n    ## 1) Sample data,\r\n    x = sample_data(batch_size=M)\r\n    \r\n    ## 2) Sample noise prior,\r\n    z = sample_noise_prior(batch_size=M)\r\n\r\n    ## 3) Compute all partial derivatives of the loss function,\r\n    histd[i], grads_and_vars = tfe.implicit_value_and_gradients(neg_value_function)(D, G, x, z)\r\n    \r\n    ## 4) Optimize discriminator, and not generator (!).\r\n    discriminator_grads_and_vars = [gv for gv in grads_and_vars if gv[1].name.startswith('D/')]\r\n    discriminator_optimizer.apply_gradients(discriminator_grads_and_vars, global_step=batch)\r\n\r\n  # Train generator.\r\n  ## 1) Sample noise prior,\r\n  z = sample_noise_prior(batch_size=M)\r\n  \r\n  ## 2) Compute all partial derivatives of the loss function\r\n  histg[i], grads_and_vars = tfe.implicit_value_and_gradients(value_function)(D, G, x, z)\r\n  \r\n  ## 3) Optimize generator, and not discriminator (!)\r\n  generator_grads_and_vars = [gv for gv in grads_and_vars if gv[1].name.startswith('G/')]\r\n  opt_g.apply_gradients(generator_grads_and_vars, global_step=batch)\r\n```", "@jppgks, as Asim mentioned, an example eager GAN should be up shortly. It is fairly similar to your code, but uses GradientTape and trains G and D at the same time. There are no concrete plans of making TFGAN eager friendly in the very near future that I am aware of. @joel-shor, do you have some bandwidth to discuss potential approaches to making TFGAN eager friendly? @jppgks is willing to contribute. \r\n\r\nAlso, you can use the loss functions in TFGAN now. They are eager friendly with the exception of summaries. ", "@iganichev I didn't manage to get TFGAN loss functions to work with `tfe.implicit_value_and_gradients`. Do you have a minimal working example of this?", "@jppgks the eager GAN example was merged and should make it to github very soon. Though it does not use `implicit_value_and_gradients` because it wants to compute multiple gradients over the same computation.  What issue are you facing?", "@iganichev That\u2019s great news! \r\n\r\nWhen I pass `tf.contrib.gan.losses.minimax_loss_discriminator` to `implicit_value_and_gradients` and then call it with discriminator output on real and generated samples, I get an error that there are no variables to compute the gradients on.", "@jppgks The reason this does not work is because the computation involving variables is done outside of the function passed to `implicit_value_and_gradients`. In your code you call `D` and `G` inside the `value_function`. You need to do the same when using GAN losses.\r\n", "@iganichev Thanks a lot for the help with debugging, I made it work using the suggestion in your last comment \ud83d\udcaf ", "Apologies for the delay!\r\n\r\nI do have plans to make TFGAN TF-Eager compatible. I'll discuss with @iganichev and update this thread ASAP.", "That\u2019s good to hear! Let me know if I can help with anything.", "TFGAN is fundamentally functional, and a number of TF-Eager paradigms are class-based. I think making TFGAN eager-compatible would require basically a re-write of the library, so we're not going to actively pursue this.", "I understand. I will be switching back to non-eager TensorFlow for now, so I can make use of TFGAN."]}, {"number": 15820, "title": "Revert \"Remove unneeded branch check (#13495)\"", "body": "This reverts commit 3aee5f1df97f44d9c14995505895f1877d7de8ae.\r\n\r\nFix build with Python3", "comments": ["LGTM"]}, {"number": 15819, "title": "Revert \"Fix a bug: bfloat16 is unsigned on Windows (#15302)\"", "body": "This reverts commit fdf34a88bec9645473f10ba2d52df4cfcb80d582.\r\n\r\nSince the fix for #15302 is not getting merged soon, I'll revert it to fix the Windows build.\r\n@gunan \r\nFYI, @snnn ", "comments": []}, {"number": 15818, "title": "Tensorflow Object Detection using Tensorflow Mobile", "body": "I am trying to use a custom model in the\u00a0[TF-Detect Android Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android).\r\n\r\n**model: ssd_mobilenet_v1_coco**\r\n\r\nThe model is trained on 8 classes. After exporting the model I've optimised it using\u00a0Tensorflow-Mobile.\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n    --in_graph=frozen_inference_graph.pb \\\r\n    --out_graph=optimized_inf_graph.pb \\\r\n    --inputs='image_tensor' \\\r\n    --outputs='detection_boxes detection_scores detection_classes num_detections' \\\r\n    --transforms='fold_batch_norms  fold_old_batch_norms  quantize_weights'\r\n```\r\nThe optimised graph is giving proper output in my local system, but when it's integrated in the application there is no output shown in the screen. But when I'm using the unoptimised graph (`frozen_inference_graph.pb`) in the application, it's working fine. I'm getting outputs.What am I doing wrong here?\r\n\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Mac OS Sierra\r\nTensorFlow installed from: Virtualenv installation\r\nTensorFlow version: 1.4\r\nBazel version: Build label: 0.7.0-homebrew\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce:\r\n\r\n1. Trained a `ssd_mobilenet_v1_coco` model using google cloud ml for 8 classes\r\n2. Exported the frozen graph from checkpoints using the below command set:\r\n\r\n```\r\npython export_inference_graph.py --input_type image_tensor \\\r\n     --pipeline_config_path training/ssd_inception_v2_coco.config \\\r\n     --trained_checkpoint_prefix training/model.ckpt-200000 \\\r\n     --output_directory frozen_graph/\r\n```\r\n\r\n`export_inference_graph.py` is the python script provided in here https://github.com/tensorflow/models/blob/master/research/object_detection/export_inference_graph.py. Tested the `frozen_inference_graph.py` in my local system, it's working fine.\r\n\r\n3. Used the below command to convert the `frozen_inference_graph.py` to optimized graph:\r\n\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n    --in_graph=<path to frozen_inference_graph.pb> \\\r\n    --out_graph=<path where optimized_inf_graph.pb will be stored> \\\r\n    --inputs='image_tensor' \\\r\n    --outputs='detection_boxes detection_scores detection_classes num_detections' \\\r\n    --transforms='fold_batch_norms  fold_old_batch_norms  quantize_weights'\r\n```\r\nTested the `optimized_inf_graph.pb` graph in my local system. It's working fine.\r\n\r\n4. Copied the `optimized_inf_graph.pb` in `tensorflow/tensorflow/examples/android/assets/` folder. Also, copied `my_labels.txt` file in the assets folder.\r\n\r\n5. Replaced `ssd_mobilenet_v1_android_export.pb` with `optimized_inf_graph.pb` in DetectorActivity.java [file](https://github.com/tensorflow/tensorflow/blob/15b1cf025da5c6ac2bcf4d4878ee222fca3aec4a/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L65)\r\n\r\nReplaced `coco_labels_list.txt` with `my_labels.txt` in DetectorActivity.java [file](https://github.com/tensorflow/tensorflow/blob/15b1cf025da5c6ac2bcf4d4878ee222fca3aec4a/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L67)\r\n\r\nTf Detect:\r\n```\r\nprivate static final String TF_OD_API_MODEL_FILE =\r\n      \"file:///android_asset/optimized_inf_graph.pb\";\r\n  private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/coco_labels_list.txt\";\r\n```\r\n\r\nMy Version:\r\n```\r\nprivate static final String TF_OD_API_MODEL_FILE =\r\n      \"file:///android_asset/ssd_mobilenet_v1_android_export.pb\";\r\n  private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/my_labels.txt\";\r\n```\r\n\r\n6. Installed on my phone: LG G4", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Details updated above", "I am also having this issue. Same model but with 1 class instead of 8. I also used Google's ML Engine to train it", "@sekhar989 , @CorvetteCole , I encountered the same issue. Any update on this?", "@WenguoLi Nope. No update on this yet. The issue's still there.", "Kuta dela harramta  ma. De pode ala\nOn 10 Jun 2018 23:53, \"Alfred Sorten Wolf\" <notifications@github.com> wrote:\n\n> Nagging Assignee @derekjchow <https://github.com/derekjchow>: It has been\n> 36 days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15818#issuecomment-396072010>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYAqY6iLyFiIh9-xr1cLj2IIw6IF2zEpks5t7WsbgaJpZM4RR3UT>\n> .\n>\n", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!"]}, {"number": 15817, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "body": "I met the same problem as ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory , https://github.com/tensorflow/tensorflow/issues/15604 , but non of their methods work.\r\n  ", "comments": ["Same issue, but its because I have 9.1 installed and 9.0 is the version supported in tf-nightly.   Was hoping tf-nightly would include 9.1 support.", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "Just install 9 with 9.1\r\n\r\nsudo apt-get install cuda-9-0\r\n", "@KeithAymar I have tried to use that command but I get the following error:\r\n\r\n>Reading package lists... Done\r\n>Building dependency tree       \r\n>Reading state information... Done\r\n>E: Unable to locate package cuda-9-0\r\n\r\nCould you please advise what to do? Thanks in advance!\r\n", "I don\u2019t know for sure.  I\u2019m an a noob with this.  Took me a few days to get things working.  Anytime something doesn\u2019t work quite right I make sure to do before running any install commands.  \r\n\r\nsudo apt-get update \r\n\r\nSounds like you don\u2019t have the packages set up for install. \r\n\r\nThese are the instructions I followed when I installed 9.1.  Be sure all the prerequisites are complete.  I am using a minimal install of Lubuntu.   But there are instructions for other flavors.  \r\n\r\nhttp://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation\r\n\r\n", "I use Ubuntu 1604, I installed cuda 9.1 following the instructions here: http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation\r\n\r\nWhen importing tensorflow happened the same problem as OP.\r\n\r\nI used the command:\r\n```sh\r\nsudo apt-get install cuda-9-0\r\n```\r\nAnd it solved my issue, I now finally have tensorflow-gpu running\r\n\r\nThanks @KeithAymar "]}, {"number": 15816, "title": "ImportError: libcublas.so.9.0: cannot open shared object file:", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 15815, "title": "When data become large,parition variables can not initialized successfully", "body": "#15216 \r\ni have a issue, but nobody help me to solve it ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler \r\ni have the same issue,but can you not put the label on this issue, cause when you put the label, i wall receive no answer", "i can repeat the question\r\ni use tensorflow to distributed trainning models, i use the partition valriables to store an array data, when the data is not so bigger, everything looks ok,but when the array data become larger, when the session initialize, the partition variables can not initialized and the session will wait util time out.\r\n\r\nDescribe the problem\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. the feat_info can initialize successfully, but the adj_info cannot initialized. the adj_info is larger than feat_info\r\n\r\nSource code / logs\r\n\r\ni use ps_num = 4, worker_num =4 and i also try some other distributed config, like ps_num=1, worker_num=4, the result is the same\r\nsource code:\r\nwith tf.device(tf.train.replica_device_setter(\r\nworker_device=\"/job:worker/task:%d\" % task_id,\r\ncluster=cluster_spec)):\r\n\r\n  feat_info = tf.get_variable(\"feature_info\", (len(id_map),FLAGS.features_column), tf.float32, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))\r\n  adj_info = tf.get_variable(\"adj_info\", (len(id_map),FLAGS.max_degree), tf.int64, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))\r\n \r\n  with tf.device('/job:worker/task:%d' %task_id):\r\n      adj_local = tf.Variable(tf.constant(minibatch.adj, dtype=tf.int64), trainable=False, name=\"adj_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n      feat_local = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False, name=\"feat_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n \r\n  length, begin, end = split_node_by_task(len(id_map), task_id, num_workers)\r\n  adj = tf.nn.embedding_lookup(adj_info, [x for x in range(begin, end)])\r\n  adj = adj_local\r\n  \r\n  feat = tf.nn.embedding_lookup(feat_info, [x for x in range(begin, end)])\r\n  feat = feat_local\r\n\r\nlog:\r\n2017-12-08 23:54:17.377290: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session c2b3ba9b700261ba with config:\r\nINFO:tensorflow:Waiting for model to be ready. Ready_for_local_init_op: None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15\r\n2017-12-09 00:00:35.637019: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session f35fcf332e3908ec with config:\r\nINFO:tensorflow:Waiting for model to be ready. Ready_for_local_init_op: None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15\r\nand it will alway waiting adj_info to initialize\r\n\r\nHave I written custom code\r\nyes\r\nOS Platform and Distribution\r\nlinux platform\r\nTensorFlow installed from\r\nN/A\r\nTensorFlow version\r\nr1.4\r\nBazel version\r\nCUDA/cuDNN version\r\nno gpu\r\nGPU model and memory\r\nno gpu\r\nExact command to reproduce\r\nN/A\r\n i use monitored_session or supervisor to do the initialize_variables, when data is not so large, it will initialize successfully, my machine have more than 300GB memory is enough for the larger variable", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "This is a duplicate of #15835 (and of #15216, as you mentioned). Please do not file the same issue multiple times."]}, {"number": 15814, "title": "Fix git configuration on Windows", "body": "http://ci.tensorflow.org/job/tf-master-win-bzl/2188/console\r\nRunning python script on Windows require explicitly invoke python binary.\r\n  \r\n@gunan ", "comments": []}, {"number": 15813, "title": "Fix typo in UserInputError()", "body": "The phrase \"The followings are ...\" (lines 336 & 337)\r\nshould be changed to \"The following are ...\".", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15812, "title": "Closing input stream, runner session in TensorFlowInferenceInterface.java and fixing bit changes", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Changes reverted! Can we go ahead? :)\r\n  "]}, {"number": 15811, "title": "Fix typo in LinearModel() docstring", "body": "adn -> and", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15810, "title": "Calling variable property of DropoutWrapper gives Error: AttributeError: 'DropoutWrapper' object has no attribute 'trainable'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Using docker container minimaxir/keras-cntk:cpu-compiled (https://hub.docker.com/r/minimaxir/keras-cntk/tags/)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.1 also tested on 1.4\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI was trying to access the weights and biases of an BasicLSTM cell I created. The BasicLSTM cell also has a DropoutWrapper and when trying to access the variables property the below error is thrown.\r\n`AttributeError: 'DropoutWrapper' object has no attribute 'trainable'`\r\n\r\nSomeone tried to help me with the error on stackoverflow (second half of this [answer](https://stackoverflow.com/a/47643427/1169493)) and noticed that while variables is implemented in Layer. \r\n\r\nI will quote his very helpful response below:\r\n\r\n> Although variables is documented for most (all?) RNN classes, it does break for DropoutWrapper. The property has been documented since r1.2, but accessing the property causes an exception in 1.2 and 1.4 (and looks like 1.3, but untested). Specifically,\r\n> \r\n> from tensorflow.contrib import rnn\r\n> ...\r\n> lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\r\n> wrapped_cell = rnn.DropoutWrapper(lstm_cell)\r\n> outputs, states = rnn.static_rnn(wrapped_cell, x, dtype=tf.float32)\r\n> print(\"LSTM vars!\", lstm_cell.variables)\r\n> print(\"Wrapped vars!\", wrapped_cell.variables)\r\n> will throw AttributeError: 'DropoutWrapper' object has no attribute 'trainable'. From the traceback (or a long stare at the DropoutWrapper source), I noticed that variables is implemented in DropoutWrapper's super RNNCell's super Layer. Dizzy yet? Indeed, we find the documented variables property here. It returns the (documented) weights property. The weights property returns the (documented) self.trainable_weights + self.non_trainable_weights properties. And finally the root of the problem:\r\n\r\n```\r\n @property\r\n def trainable_weights(self):\r\n     return self._trainable_weights if self.trainable else []\r\n \r\n @property\r\n def non_trainable_weights(self):\r\n     if self.trainable:\r\n         return self._non_trainable_weights\r\n     else:\r\n        return self._trainable_weights + self._non_trainable_weights\r\n```\r\n\r\n> That is, variables does not work for a DropoutWrapper instance. Neither will trainable_weights or non_trainable_weights sinceself.trainable is not defined.\r\n> \r\n> One step deeper, Layer.__init__ defaults self.trainable to True. Where that property gets removed, deled, unset, whatever from a DropoutWrapper object, I cannot tell.\r\n\r\n  ", "comments": ["I am that answerer, by the way. I'm curious why ``DropoutWrapper`` doesn't have a ``variables`` property, too.", "@ebrevdo can you comment? Thanks.", "DropoutWrapper does not have variables because it does not itself store any.  It wraps a cell that may have variables; but it's not clear what the semantics should be if you access the `DropoutWrapper.variables`.  For example, all keras layers only report back the variables that they **own**; and so only one layer ever **owns** any variable.  That said, this should probably return `[]`, and the reason it doesn't is that `DropoutWrapper` never calls `super().__init__` in its constructor.  That should be an easy fix; PRs welcome.", "I have come up with a workaround while we wait for the permanent fix.\r\nInstead of calling `one_kernel, one_bias = one_lstm_cell.variables` we can use `one_kernel, one_bias = one_lstm_cell._cell.variables`. This calls variables on the original RNNCell passed in. I know it's generally not a good idea to call protected properties but it works.", "It may make sense to add a @property \"cell\" or \"wrapped_cell\" to\nDropoutWrapper, specifically for use cases like this.  Would you like to\nsend a PR?\n\nOn Tue, Jan 9, 2018 at 9:43 AM, michaelkhan3 <notifications@github.com>\nwrote:\n\n> I have come up with a workaround while we wait for the permanent fix.\n> Instead of calling one_kernel, one_bias = one_lstm_cell.variables we can\n> use one_kernel, one_bias = one_lstm_cell._cell.variables. This calls\n> variables on the original RNNCell passed in. I know it's generally not a\n> good idea to call protected properties but it works.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15810#issuecomment-356359391>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6fXKdUsg08AaJYblyPSVE5oLmMsks5tI6VVgaJpZM4RRlwr>\n> .\n>\n", "@ebrevdo Sure.  I'll send one soon", "For me, even this is returning an empty list.  I have tf 1.5\r\n\r\nlstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\r\n\r\nprint(\"LSTM vars!\", lstm_cell.variables)", "Variables aren't created until after you call lstm_cell at least once.\nThis is because it doesn't know the input shape yet, so it can't create the\nvariables, which are shaped [*input_depth*, num_units].\n\nOn Fri, Mar 16, 2018 at 10:55 AM, dmoham1476 <notifications@github.com>\nwrote:\n\n> For me, even this is returning an empty list. I have tf 1.5\n>\n> lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n>\n> print(\"LSTM vars!\", lstm_cell.variables)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15810#issuecomment-373794742>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1GadxWwq-l0I1PN5SKRFdh9lhlWks5te_yUgaJpZM4RRlwr>\n> .\n>\n", "You can also call lstm_cell.build(input_shape)  to create the variables\nwithout creating any computation ops.\n\nOn Fri, Mar 16, 2018 at 10:58 AM, Eugene Brevdo <ebrevdo@google.com> wrote:\n\n> Variables aren't created until after you call lstm_cell at least once.\n> This is because it doesn't know the input shape yet, so it can't create the\n> variables, which are shaped [*input_depth*, num_units].\n>\n> On Fri, Mar 16, 2018 at 10:55 AM, dmoham1476 <notifications@github.com>\n> wrote:\n>\n>> For me, even this is returning an empty list. I have tf 1.5\n>>\n>> lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n>>\n>> print(\"LSTM vars!\", lstm_cell.variables)\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/15810#issuecomment-373794742>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtim1GadxWwq-l0I1PN5SKRFdh9lhlWks5te_yUgaJpZM4RRlwr>\n>> .\n>>\n>\n>\n"]}, {"number": 15809, "title": "add checking for input values in GANHead constructor", "body": "", "comments": ["Can one of the admins verify this patch?", "@joel-shor Can you please review this PR?", "@joel-shor Fixed the `get_hooks_fn` condition.", "@joel-shor Please take another look\r\n@tensorflow-jenkins test this please", "what about this PR?", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is it okay to close the PR?", "Sorry, closed it by mistake.", "@joel-shor  Could you please review the PR?"]}, {"number": 15808, "title": "Remove noop python command in pyx_library rule.", "body": "It looks to me like the command produced should be:\r\n`shutil.copyfile(src.split(\".\")[0] + \".cpp\", src.split(\".\")[0] + \".cpp\")`\r\nbased on how `cpp_outs` is constructed a few lines above?\r\nAllen,  could you take a look and comment?\r\n\r\nThis file,  and the direct call to the python binary is one of the root causes of #15618", "comments": []}, {"number": 15807, "title": "Fix unstable test case for Select op", "body": "Fix #14862. CF: #15764\r\n\r\nIn the test case for Select op, the condition might switch to another value when `x1` is close to `x2`.  The PR is opened to resolve the unstable condition.\r\n\r\nTest passed for 100 times.\r\n```bash\r\n[facai@h1077922 tensorflow]$ bazel test --runs_per_test=100 -c opt //tensorflow/cc:gradients_math_grad_test\r\nHEAD is now at c642574... TST: modify unstable test case\r\nINFO: Found 1 test target...\r\nTarget //tensorflow/cc:gradients_math_grad_test up-to-date:\r\n  bazel-bin/tensorflow/cc/gradients_math_grad_test\r\nINFO: Elapsed time: 49.959s, Critical Path: 21.29s\r\n//tensorflow/cc:gradients_math_grad_test                                 PASSED in 21.3s\r\n  Stats over 100 runs: max = 21.3s, min = 0.7s, avg = 11.2s, dev = 4.4s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\n```\r\n\r\ncc @gunan @drpngx \r\n  ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Thank you, @drpngx . Let's take care. \r\n\r\nUnbutu Python3 failed:\r\n//tensorflow/python/keras:data_utils_test: Time out\r\n\r\nCould you retest it?", "Thanks for pinging! Restarted."]}, {"number": 15806, "title": "Can we install tensorflow with a package manager in Linux distro?", "body": "As we all know, Tensorflow is a major opensource deeplearning framework to the developers. \r\nBTW, How can we install tensorflow with a package manager such as apt-get (for *.deb), yum/zypper/dnf (for *.rpm)  in Linux distro?\r\n\r\n* Reference - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/builds\r\n\r\nI could not find related scripts from the above web address. Does tensorflow support 1)  manual compilation with ./tools/ci_build/build/*.sh and 2) pre-built docker-based compilation only?\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.0 and Ubuntu 16.04.3\r\n- **TensorFlow installed from (source or binary)**: Latest version of Tensorflow (form github)\r\n- **TensorFlow version (use command below)**: Latest version of Tensorflow (form github)\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: with Cmake (w/o Bazel)\r\n- **GCC/Compiler version (if compiling from source)**: GCC 5.0\r\n- **CUDA/cuDNN version**:  None (w/ CPU only)\r\n- **GPU model and memory**: None , DRAM 16GB\r\n- **Exact command to reproduce**:   Nonthing.\r\n\r\n\r\n\r\n### Describe the problem\r\nNo support\r\n\r\n### Source code / logs\r\nOmission. \r\n\r\n  ", "comments": ["pip is the most popular way to install TensorFlow for Python. \r\nhttps://www.tensorflow.org/install/\r\n\r\nThere are no current plans to support install via apt-get, rpm, or other package manager.  The Anaconda team also has builds but I would stick with pip to get the best possible support as the Anaconda builds are done by a third-party.  I am not anti-Anaconda and finding the right wording is not easy.  At some point the Anaconda builds may be very good and maybe offer some additional options in the form of more CPU optimizations.  "]}, {"number": 15805, "title": "Unable to convert LSTM model to .tflite model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.2\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A, using CPU only\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n```\r\n~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=\"$(pwd)/lstm-model.pb\" \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --output_file=\"$(pwd)/lstm-model.tflite\" --inference_type=FLOAT \\\r\n  --input_type=FLOAT --input_arrays=input \\\r\n  --output_arrays=output --input_shapes=28,28\r\n```\r\n### The Issue\r\nWhen trying to convert an LSTM from a frozen graph (.pb) file to (.tflite) using the tensorflow toco script, I get unsupported operations error.\r\n\r\n### Source code / logs\r\nThis is the source code for the mode:\r\n```\r\n'''\r\nEdited code from https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/\r\n'''\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import rnn\r\n\r\n#import mnist dataset\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)\r\n\r\n#define constants\r\n#unrolled through 28 time steps\r\ntime_steps=28\r\n#hidden LSTM units\r\nnum_units=128\r\n#rows of 28 pixels\r\nn_input=28\r\n#learning rate for adam\r\nlearning_rate=0.001\r\n#mnist is meant to be classified in 10 classes(0-9).\r\nn_classes=10\r\n#size of batch\r\nbatch_size=128\r\n\r\n#weights and biases of appropriate shape to accomplish above task\r\nout_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\r\nout_bias=tf.Variable(tf.random_normal([n_classes]))\r\n\r\n#defining placeholders\r\n#input image placeholder\r\nx=tf.placeholder(\"float\",[None,time_steps,n_input],name=\"input\")\r\n#input label placeholder\r\ny=tf.placeholder(\"float\",[None,n_classes])\r\n\r\n#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\r\ninput=tf.unstack(x ,time_steps,1)\r\n\r\n#defining the network\r\n#cell = rnn.BasicLSTMCell(num_units,forget_bias=0)\r\nlstm_layer = tf.nn.rnn_cell.MultiRNNCell([rnn.BasicLSTMCell(num_units) for _ in range(3)])\r\noutputs, _ = rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\r\n\r\n#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\r\nprediction=tf.matmul(outputs[-1],out_weights,name=\"output\")+out_bias\r\n\r\n#loss_function\r\nloss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\r\n#optimization\r\nopt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n\r\n#model evaluation\r\ncorrect_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\r\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n#initialize variables\r\ninit=tf.global_variables_initializer()\r\n\r\nsaver = tf.train.Saver()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    iter=1\r\n    while iter<800:\r\n        batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\r\n\r\n        batch_x=batch_x.reshape((batch_size,time_steps,n_input))\r\n\r\n        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\r\n\r\n        if iter %10==0:\r\n            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\r\n            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\r\n            print(\"For iter \",iter)\r\n            print(\"Accuracy \",acc)\r\n            print(\"Loss \",los)\r\n            print(\"__________________\")\r\n\r\n        filename = saver.save(sess, \"model/model.ckpt\")\r\n\r\n        iter=iter+1\r\n\r\n#calculating test accuracy\r\ntest_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))\r\ntest_label = mnist.test.labels[:128]\r\nprint(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\r\n```\r\nThis is code I used for freezing the graph:\r\n```\r\n'''\r\nCode from https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc\r\n'''\r\n\r\nimport os, argparse\r\n\r\nimport tensorflow as tf\r\n\r\n# The original freeze_graph function\r\n# from tensorflow.python.tools.freeze_graph import freeze_graph\r\n\r\ndir = os.path.dirname(os.path.realpath(__file__))\r\n\r\ndef freeze_graph(model_dir, output_node_names):\r\n    \"\"\"Extract the sub graph defined by the output nodes and convert\r\n    all its variables into constant\r\n    Args:\r\n        model_dir: the root folder containing the checkpoint state file\r\n        output_node_names: a string, containing all the output node's names,\r\n                            comma separated\r\n    \"\"\"\r\n    if not tf.gfile.Exists(model_dir):\r\n        raise AssertionError(\r\n            \"Export directory doesn't exists. Please specify an export \"\r\n            \"directory: %s\" % model_dir)\r\n\r\n    if not output_node_names:\r\n        print(\"You need to supply the name of a node to --output_node_names.\")\r\n        return -1\r\n\r\n    # We retrieve our checkpoint fullpath\r\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\r\n    input_checkpoint = checkpoint.model_checkpoint_path\r\n\r\n    # We precise the file fullname of our freezed graph\r\n    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\r\n    output_graph = absolute_model_dir + \"/frozen_model.pb\"\r\n\r\n    # We clear devices to allow TensorFlow to control on which device it will load operations\r\n    clear_devices = True\r\n\r\n    # We start a session using a temporary fresh Graph\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        # We import the meta graph in the current default Graph\r\n        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\r\n\r\n        # We restore the weights\r\n        saver.restore(sess, input_checkpoint)\r\n\r\n        # We use a built-in TF helper to export variables to constants\r\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, # The session is used to retrieve the weights\r\n            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes\r\n            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\r\n        )\r\n\r\n        # Finally we serialize and dump the output graph to the filesystem\r\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n    return output_graph_def\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model_dir\", type=str, default=\"\", help=\"Model folder to export\")\r\n    parser.add_argument(\"--output_node_names\", type=str, default=\"\", help=\"The name of the output nodes, comma separated.\")\r\n    args = parser.parse_args()\r\n\r\n    freeze_graph(args.model_dir, args.output_node_names)\r\n```\r\n\r\nThis is the output of the toco command:\r\n```\r\n2018-01-02 20:05:24.912921: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\r\n2018-01-02 20:05:24.973744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: Unpack\r\n2018-01-02 20:05:24.974315: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: StridedSlice\r\n2018-01-02 20:05:25.041459: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1209 operators, 1775 arrays (0 quantized)\r\n2018-01-02 20:05:25.118862: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1114 operators, 1672 arrays (0 quantized)\r\n2018-01-02 20:05:25.176555: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1114 operators, 1672 arrays (0 quantized)\r\n2018-01-02 20:05:25.208552: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2018-01-02 20:05:25.234811: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ExpandDims, Fill, SPLIT, StridedSlice, TensorFlowShape, Unpack.\r\npbtotflite.sh: line 8:  8277 Abort trap: 6           ~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=\"$(pwd)/lstm-model.pb\" --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=\"$(pwd)/lstm-model.tflite\" --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=28,28\r\n```", "comments": ["The LSTM cell in TF is consist of several Ops, which might not be supported by TF Lite (for now). \r\nThere are some pre-trained (and pre-converted to .tflite) LSTM models, like tts and speakerid under `tensorflow/contrib/lite/models`, and there is a implementation for LSTM Op (as a single op, not a combination of several ops) in TF Lite. However, it seems toco just cannot handle conversion between TF LSTM and TF Lite LSTM. \r\nReally need some instructions on how to convert a LSTM model. :disappointed: ", "was this problem solved? I have tensorflow 1.8.0, and I am having the same issue.", "This is my error:\r\n[https://stackoverflow.com/questions/51275360/convert-lstm-graph-with-tflite-fails](https://stackoverflow.com/questions/51275360/convert-lstm-graph-with-tflite-fails)\r\n", "F tensorflow/contrib/lite/toco/tooling_util.cc:822] Check failed: d >= 1 (0 vs. 1)", "During inference, batch size = 1, 10 inputs, each input is of length 2560", "same issue here", "So what's cracking? No solution yet?", "anyone have solution for lstm in tensorflow lite ? me has the same issuse", "Also interested for LSTM support", "I modified some code and I successed to generate tflite file for this model.\r\n```\r\n#weights and biases of appropriate shape to accomplish above task\r\nout_weights=tf.Variable(tf.random_normal([num_units,n_classes]),name=\"weights\")\r\nout_bias=tf.Variable(tf.random_normal([n_classes]),name=\"bias\")\r\n\r\n#defining placeholders\r\n#input image placeholder\r\nx=tf.placeholder(\"float\",[None,time_steps,n_input])\r\n#input label placeholder\r\ny=tf.placeholder(\"float\",[None,n_classes])\r\n\r\n#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\r\ninput=tf.unstack(x ,time_steps,1,name=\"input_tensor\")\r\n\r\n#defining the network\r\n#lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\r\n#lstm_layer=rnn.MultiRNNCell([rnn.BasicLSTMCell(num_units) for _ in range(3)])\r\n#lstm_layer=rnn.LSTMBlockCell(num_units,forget_bias=1)\r\nlstm_layer=tf.nn.rnn_cell.BasicLSTMCell(num_units)\r\n#lstm_layer=tf.nn.rnn_cell.GRUCell(num_units)\r\n#lstm_layer=tf.nn.rnn_cell.LSTMCell(num_units,forget_bias=1)\r\noutputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\r\n\r\n#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\r\nprediction=tf.add(tf.matmul(outputs[-1],out_weights), out_bias, name=\"output\")\r\n\r\n#loss_function\r\nloss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\r\n#optimization\r\nopt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n\r\n#model evaluation\r\ncorrect_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\r\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n#initialize variables\r\ninit=tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n\r\n    iter=1\r\n    while iter<800:\r\n        batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\r\n\r\n        batch_x=batch_x.reshape((batch_size,time_steps,n_input))\r\n\r\n        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\r\n\r\n        if iter %10==0:\r\n            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\r\n            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\r\n            print(\"For iter \",iter)\r\n            print(\"Accuracy \",acc)\r\n            print(\"Loss \",los)\r\n            print(\"__________________\")\r\n\r\n        # added\r\n        saver = tf.train.Saver()\r\n        filename = saver.save(sess, output_dir + '/model.ckpt')\r\n\r\n        iter=iter+1\r\n```\r\n\r\nTrain the model:\r\n```\r\n$ python rnn.py --output_dir=save\r\n```\r\n\r\nFreeze the model by using generated checkpoint and meta file.\r\n```\r\n    output_node_name = \"output\"\r\n    restore_op_name = \"save/restore_all\"\r\n    filename_tensor_name = \"save/Const:0\"\r\n    clear_devices = True\r\n\r\n    (directory, fn, ext) = splitDirFilenameExt(input_graph_path)\r\n    output_frozen_graph_path = os.path.join(directory, fn + '_frozen.pb')\r\n\r\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path, input_binary,\r\n                              checkpoint_path, output_node_name, restore_op_name,\r\n                              filename_tensor_name, output_frozen_graph_path,\r\n                              clear_devices, \"\")\r\n```\r\n\r\n```\r\ntensorflow$ bazel run tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=[...]/save/rnn_frozen.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=[...]/save/rnn_frozen.tflite \\\r\n--input_arrays=\"Placeholder\" \\\r\n--input_shapes=1,28,28 \\\r\n--output_arrays=\"output\" \\\r\n--allow_custom_ops\r\n```\r\n\r\nFinally, I get the tflite and the summarize like below:\r\n```\r\n# summarize frozen pb\r\nInputs\r\n\tname=Placeholder\r\n\ttype=float(1)\r\n\tshape=[?,28,28]\r\nOutputs\r\n\tname=output, op=Add\r\nOp Types\r\n\t99 Const\r\n\t84 Mul\r\n\t84 Sigmoid\r\n\t57 Add\r\n\t56 Tanh\r\n\t30 ConcatV2\r\n\t29 MatMul\r\n\t28 BiasAdd\r\n\t28 Split\r\n\t2 ExpandDims\r\n\t2 Fill\r\n\t1 Placeholder\r\n\t1 Shape\r\n\t1 StridedSlice\r\n\t1 Unpack\r\n```\r\n```\r\n# summarize tflite\r\nNumber of operator types: 12\r\n\tCUSTOM(Unpack)[32]                    :    1 \t (total_ops: ???)\r\n\tCUSTOM(TensorFlowShape)[32]           :    1 \t (total_ops: ???)\r\n\tSTRIDED_SLICE[45]                     :    1 \t (total_ops: ???)\r\n\tCUSTOM(ExpandDims)[32]                :    2 \t (total_ops: ???)\r\n\tCONCATENATION[2]                      :   30 \t (total_ops: ???)\r\n\tCUSTOM(Fill)[32]                      :    2 \t (total_ops: ???)\r\n\tFULLY_CONNECTED[9]                    :   29 \t (total_ops: ???)\r\n\tSPLIT[49]                             :   28 \t (total_ops: ???)\r\n\tADD[0]                                :   56 \t (total_ops: ???)\r\n\tLOGISTIC[14]                          :   84 \t (total_ops: ???)\r\n\tMUL[18]                               :   84 \t (total_ops: ???)\r\n\tTANH[28]                              :   56 \t (total_ops: ???)\r\nTotal Number of operators                     :  374 \t (total_ops: 0)\r\n```", "Awesome sauce! @jyoungyun, Imma marry you! Which tensorflow version are you using? Thank you very much.", "@jyoungyun I was also able to convert the graph. I just cannot quantize to UINT8 because Unpack operation is not supported by TOCO", "by the way, the solution provided by @jyoungyun works for me in terms of conversion from pb to tflite.\r\nYet, I have not tested the graph yet.\r\nThanks!", "I don't know why exactly yall are interested in LSTMs and RNNs.  https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0 Nevertheless, I'd say they will most certainly be lower on the list of priorities. \uc5f0\u3148\uc601\uc528's version is most likely 1.9.  toco command line support came in r1.9, according to patch notes.  ", " In my case, my GPU version works remarkably well with the project I'm working on, so I wanted to continue with it on the smartphone side. \r\nThanks for sharing the article, by the way. ", "Does anyone know how to convert https://github.com/sherjilozair/char-rnn-tensorflow this model into toco? I have generated a pb file, but I get errors freezing the weights", "@RoboEvangelist I'm using tensorflow r1.9 version. :)", "We're planning to write some tutorials on RNN conversion. This approach does still include an unsupported unstack.  I think you'd probably want to make a function that generates either a training graph that uses a dynamic rnn and a inference graph that does a 1 time step static_rnn. Then your tflite Invoke() call feeds one sequence item at a time.", "@jyoungyun , Is it ok to run this tflite model in android? I found I can't transfer to tflite correctly if I don't use --allow_custom_ops. But I guess we can't use --allow_custom_ops if we don't have custome operations in android.\r\n@aselle , is there any schedule to release the tutorials on RNN conversion? I write the training graph that use dynamic rnn and a inference graph that does several fixed timestep static_rnn. But I can't transfer to toco correctly without --allow_custome_ops, it told me some OPS like max, min,stack...are not supported.\r\n", "@zhangjinhong17 I did not try to run this tflite model in android. But as I know, it probably will not work in android. Because there is no implementation of some operators related with RNN models. And since tflite does not yet fully support RNN-related operators, you need to pass the `--allow_custom_ops` option to toco.\r\n\r\nIf you use `static_rnn` model, you can try to run it in android after removing `ExpandDims` and `Fill` operators they are not supported yet in tflite. How to remove these operators is to set the specific `initial_state` (please refer to [static_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn)) because these two operators are used to make a initail state.", "@jyoungyun ,  Actually, I assigned the initial_state to zero at network, and it still can't convert to tflite without --allow_custom_ops. According your advise, maybe it can use in android if pass the --allow_custom_ops. I will try it. Thank you.", "@jyoungyun:\r\nCould you share your Freeze the model code please? \r\n\r\n\r\npython freeze.py --model_dir=/data3/frankzhu/tmp/mnistmodel --output_node_names=save/restore_all\r\n\r\nbazel run tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=/data3/frankzhu/tmp/mnistmodel/frozen_model.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=/data3/frankzhu/tmp/mnistmodel/rnn_frozen.tflite \\\r\n--input_arrays=\"Placeholder\" \\\r\n--input_shapes=1,28,28 \\\r\n--output_arrays=\"output\" \\\r\n--allow_custom_ops\r\n\r\nI don't know why the error like this:\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/data3/frankzhu/tmp/mnistmodel/frozen_model.pb' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--output_file=/data3/frankzhu/tmp/mnistmodel/rnn_frozen.tflite' '--input_arrays=Placeholder' '--input_shapes=1,28,28' '--output_arrays=output' --allow_custINFO: Build completed successfully, 1 total action\r\n2018-09-03 12:25:39.809475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RestoreV2\r\n2018-09-03 12:25:39.809571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809595: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809613: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809660: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809690: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809706: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809722: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign\r\n2018-09-03 12:25:39.809924: F tensorflow/contrib/lite/toco/tooling_util.cc:807] Check failed: model.HasArray(output_array) Output array not found: output\r\n\r\nCopied your code above that it complained that \r\nTraceback (most recent call last):\r\n  File \"freeze.py\", line 95, in <module>\r\n    (directory, fn, ext) = splitDirFilenameExt(input_graph_path)\r\nNameError: name 'splitDirFilenameExt' is not defined\r\n\r\nI just give the direct path so pass this problem.\r\n\r\nuse \r\nfrom tensorflow.python.tools import freeze_graph \r\nSo \r\nfreeze_graph.freeze_graph problem fixed.\r\n\r\nbut what should be input_graph_path or could you share your freeze code please?\r\nbelow is my attached file:\r\n\r\n[freeze.zip](https://github.com/tensorflow/tensorflow/files/2350790/freeze.zip)\r\n\r\nThank you very much in advance.\r\nFrank\r\n\r\n\r\n\r\n", "@fzhu129 \r\n--output_node_names should be output\r\n--output_node_names=output\r\n\r\n--output_arrays in toco should equals to --output_node_names in freeze.py", "[freeze_tryout.zip](https://github.com/tensorflow/tensorflow/files/2367966/freeze_tryout.zip)\r\nHi @xiaofansong: I have tried out the with attached freeze_tryout.zip with .pbtxt file but still fail to @jyoungyun's code.\r\n\r\nUse below command to run the code:\r\npython mnist.py (result will be saved in save folder in current pwd)\r\npython simple_freeze.py (savegraph.pbtxt will be saved in the folder save)\r\n\r\nbazel run tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=/home/frankzhu/TestPrj/lstm_try/save/frozen_model_Sample_model.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=/home/frankzhu/TestPrj/lstm_try/save/lstm.tflite \\\r\n--input_arrays=\"Placeholder\" \\\r\n--input_shapes=1,28,28 \\\r\n--output_arrays=\"output\" \\\r\n--allow_custom_ops\r\nthe lstm.tflite can be created.\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/frankzhu/TestPrj/lstm_try/save/frozen_model_Sample_model.pb' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--output_file=/home/frankzhu/TestPrj/lstm_try/save/lstm.tflite' '--input_arrays=Placeholder' '--input_shapes=1,28,28' '--output_arrays=oINFO: Build completed successfully, 1 total action\r\n2018-09-10 13:12:50.300403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2018-09-10 13:12:50.307606: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 407 operators, 618 arrays (0 quantized)\r\n2018-09-10 13:12:50.318362: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 407 operators, 618 arrays (0 quantized)\r\n2018-09-10 13:12:50.348429: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 374 operators, 609 arrays (0 quantized)\r\n2018-09-10 13:12:50.364372: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 374 operators, 609 arrays (0 quantized)\r\n2018-09-10 13:12:50.373883: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2018-09-10 13:12:50.377671: W tensorflow/contrib/lite/toco/tflite/operator.cc:879] Ignoring unsupported attribute type with key 'T'\r\n\r\nbut when run below command for getting quantized int8 there is core dump.\r\nbazel run tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=/home/frankzhu/TestPrj/lstm_try/save/frozen_model_Sample_model.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=/home/frankzhu/TestPrj/lstm_try/save/lstm.tflite \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--inference_input_type=QUANTIZED_UINT8 \\\r\n--input_arrays=\"Placeholder\" \\\r\n--input_shapes=1,28,28 \\\r\n--output_arrays=\"output\" \\\r\n--allow_custom_ops\r\ncore dump for this one and unpack not supported.\r\n\r\nINFO: Build completed successfully, 1 total action\r\n2018-09-10 13:15:10.565575: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\r\n2018-09-10 13:15:10.572736: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 407 operators, 618 arrays (0 quantized)\r\n2018-09-10 13:15:10.582917: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 407 operators, 618 arrays (0 quantized)\r\n2018-09-10 13:15:10.610551: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 375 operators, 610 arrays (1 quantized)\r\n2018-09-10 13:15:10.626365: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 375 operators, 610 arrays (1 quantized)\r\n2018-09-10 13:15:10.634396: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 375 operators, 610 arrays (1 quantized)\r\n2018-09-10 13:15:10.634417: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:456] Unimplemented: this graph contains an operator of type (**Unsupported TensorFlow op: Unpack) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).**\r\nAborted (core dumped)\r\n\r\nAny ideas?\r\nAnybody can help?\r\n\r\nThanks and best regards\r\nFrank\r\n\r\n\r\n", "@fzhu129 Looking at your comments, toco does not fully support quantized model yet.", "> summarize @jyoungyun\r\n\r\nwould u tell me what script used to get your summarize. thanks.\r\n\r\n summarize frozen pb\r\nInputs\r\n\tname=Placeholder\r\n\ttype=float(1)\r\n\tshape=[?,28,28]\r\nOutputs\r\n\tname=output, op=Add\r\nOp Types\r\n\t99 Const\r\n\t84 Mul\r\n\t84 Sigmoid\r\n\t57 Add\r\n\t56 Tanh\r\n\t30 ConcatV2\r\n\t29 MatMul\r\n\t28 BiasAdd\r\n...", "@bjtommychen  I use `SummarizeGraph` function in tensorflow. The code is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/summarize_graph_main.cc#L144).", "@jyoungyun @zhangjinhong17  were any of you able to successfully port the TF Lite model to android?\r\nIf yes, could you please share the source code?", "@varunj Sorry I didn't port the tflte model in android. ", "@jyoungyun Hi, I use tensoflow r1.11 right now, when i try to convert lstm model to TF Lite model, I found that Merge and Switch are not supported. I pass --allow_custom_ops to toco, I could get TF Lite model successfully. But when i try to use this model in my project, it still get error about Merge and Switch. So my question is how to use lstm TF Lite model? Could you please share the source code of custom operation Merge and Switch in tensorflow lite? ", "> We're planning to write some tutorials on RNN conversion. This approach does still include an unsupported unstack. I think you'd probably want to make a function that generates either a training graph that uses a dynamic rnn and a inference graph that does a 1 time step static_rnn. Then your tflite Invoke() call feeds one sequence item at a time.\r\n\r\n@aselle \r\n1. Do we have the RNN conversion tutorial anywhere now? \r\n2. Your answer means that we need to get rid of control flow from the inference graph in order to make it work in TFLite, right? The efficiency might drop a lot when we use Invoke() in Python layer for handling variable length of the input, right?\r\n3. Is it possible to quantize the static_rnn graph to 8-bit in TFLite?\r\n", "> @bjtommychen I use `SummarizeGraph` function in tensorflow. The code is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/summarize_graph_main.cc#L144).\r\n\r\n@jyoungyun How did you summarize the tflite file? In your list for the tflite file, CUSTOM(Unpack)[32]  means it is not implemented for quantized 8-bit or float 32-bit?", "@jinzequn The `LSTM` operator does not support in tflite yet. So I also couldn't use the `LSTM` operator. \r\n@hhxxttxsh oh, to summarize the tflite file, I made a simple application using flatbuffer schema. I couldn't share the detail codes right now. Sorry. And `CUSTOM(Unpack)` means it is not implemented for both type.", "@jyoungyun Thanks. It seems that using flatbuffer schema can generate the list of BuiltinOperator.py. However, how did you generate the list of supported quantized ops using schema? As far as I can see, the supported list of ops can be seen in the file quantized.cc, but there are no other files (similar to the builtin_ops.h) which provide a list for the quantized ops. \r\nI assume the number of unsupported list of ops for quantized 8-bit and the float 32-bit should be different, right?", "By switching from using dynamic-rnn to using an unrolled rnn using LSTMs (I know LSTM isn't formally supported, but it does exist), I was able to produce and run a tflite model on android and ios. However, the model is extremely huge (>400MB on disk) and extremely slow (>5s). If I remove the rnn portion of my model (it is a custom model containing more than just the rnn portion), the model size drops to 13MB and runs in 2s. So this substitution with an unrolled rnn is not a viable answer. The same model is 26MB/0.8s when I run with a dynamic-rnn in tensorflow mobile on the same devices. My point is this: +1 to guidance and samples on how we can use RNNs with TFLite", "@jyoungyun \r\nI meet another problem:\r\nI add an OP in tensorflow ,the OP's input type is int8. And I can generate the model ok, and the model can be used successfully, but when I transfer the model to lite (use tflite_convert), err happened:\r\nCheck failed: array->has_shape()\r\n\r\nI wondered if int8(and uint8,short...) input type is not supported in lite ...\r\n", "Please take a look at the recent example we added for Unidirectional LSTM test case. It may help  your use case:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/unidirectional_sequence_lstm_test.py", "> Please take a look at the recent example we added for Unidirectional LSTM test case. It may help your use case:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/unidirectional_sequence_lstm_test.py\r\n\r\nHi @achowdhery, I tried to use this TFLiteLSTMCell in my model, when training it consumes much more memory than tf.nn.rnn_cell.LSTMCell, for example, I can use batch_size=32, hidden_size=256, num_proj=128 with tf.nn.rnn_cell.LSTMCell for my 12GB GPU, but I can only use batch_size=8, hidden_size=128, num_proj=64 with TFLiteLSTMCell to prevent the OOM problem.\r\n\r\nIs this the expected result? Do you have any suggestion on how to increase batch_size and hidden_size with TFLiteLSTMCell?", "Hi, similar issue here: I've got the `tf.lite.experimental.nn.TFLiteLSTMCell` running with `tf.lite.experimental.nn.dynamic_rnn`, and resulting some unsupported ops error in the `tf.lite.TFLiteConverter`.\r\nThe error log shows: `Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.`\r\n\r\nUpdate: \r\nAfter a close look to the example from @achowdhery, figured out I was missing some steps.\r\nhere's a simplified test case that reproduce my error and how to fix it: https://github.com/RichardYang40148/deep_learning_playground/blob/master/TFLite_dynamic_rnn.ipynb", "Hi, all\r\n    I watched this topic for a long time, and thank all of you for the shared informations. \r\n    Recently, I got a chance to try, refer the code of Mozilla Deepspeech project. \r\n    Finally , based on TF1.12, I can convert well-trained pb file into tflite fp32/int8 model. Both tflite model work fine in x86 machine. and almost same speed. except the int8 version has little lower accuracy.\r\n    When benchmark_model these tflite model on Android hardware. int8 model is 3x speed of the fp32 version, and almost the same speed as x86 machine.", "> Hi, all\r\n> I watched this topic for a long time, and thank all of you for the shared informations.\r\n> Recently, I got a chance to try, refer the code of Mozilla Deepspeech project.\r\n> Finally , based on TF1.12, I can convert well-trained pb file into tflite fp32/int8 model. Both tflite model work fine in x86 machine. and almost same speed. except the int8 version has little lower accuracy.\r\n> When benchmark_model these tflite model on Android hardware. int8 model is 3x speed of the fp32 version, and almost the same speed as x86 machine.\r\n\r\nHi, so it seems that you have succeeded to get a fully quantized (uint8) RNN/LSTM model? Could you please share some details about the quantization part? Would appreciate if you could share your code to us. ", "> @jyoungyun I was also able to convert the graph. I just cannot quantize to UINT8 because Unpack operation is not supported by TOCO\r\n\r\nHi @RoboEvangelist , I saw you asked this last year. Have you, by any chance, figured out how to do fully quantization(uint8) on LSTM models using TFLite toco? Thanks.", "> We're planning to write some tutorials on RNN conversion. This approach does still include an unsupported unstack. I think you'd probably want to make a function that generates either a training graph that uses a dynamic rnn and a inference graph that does a 1 time step static_rnn. Then your tflite Invoke() call feeds one sequence item at a time.\r\n\r\nHi @aselle , so your solution is kind of a walk around to the unstack problem. I have a question on this, how do I restore parameters (saved as checkpoint files) trained by dynamic rnn to a newly created static rnn graph? Could you provide some more details?", "@TF-Deve I did not continue working with TensorflowLite. I could not wait. So I moved onto using the cambricon and HUAWEI HiAI APIs, which already support LSTM for Android.", "> > Hi, all\r\n> > I watched this topic for a long time, and thank all of you for the shared informations.\r\n> > Recently, I got a chance to try, refer the code of Mozilla Deepspeech project.\r\n> > Finally , based on TF1.12, I can convert well-trained pb file into tflite fp32/int8 model. Both tflite model work fine in x86 machine. and almost same speed. except the int8 version has little lower accuracy.\r\n> > When benchmark_model these tflite model on Android hardware. int8 model is 3x speed of the fp32 version, and almost the same speed as x86 machine.\r\n> \r\n> Hi, so it seems that you have succeeded to get a fully quantized (uint8) RNN/LSTM model? Could you please share some details about the quantization part? Would appreciate if you could share your code to us.\r\n@ @RoboEvangelist @TF-Deve  \r\nI only use toco. and set quantization=True. I think it's quantized (int8) and hybrid quantization.\r\nabout the lstm part. only static_rnn support by tflite 1.12", "> @TF-Deve I did not continue working with TensorflowLite. I could not wait. So I moved onto using the cambricon and HUAWEI HiAI APIs, which already support LSTM for Android.\r\n\r\n@RoboEvangelist Could you share resource link about the cambricon and HUAWEI HiAI APIs? thanks!", "I am not sure if this is what you want but I manage to use @SullyChen 's code together with some other codes to convert to a tflite model.\r\n\r\n```\r\n#weights and biases of appropriate shape to accomplish above task\r\nout_weights=tf.Variable(tf.random_normal([num_units,n_classes]),name=\"weights\")\r\nout_bias=tf.Variable(tf.random_normal([n_classes]),name=\"bias\")\r\n\r\n#defining placeholders\r\n#input image placeholder\r\nx=tf.placeholder(\"float\",[None,time_steps,n_input], name=\"input\") \r\n#input label placeholder\r\ny=tf.placeholder(\"float\",[None,n_classes]) \r\n\r\n#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\r\ninput=tf.unstack(x ,time_steps,1,name=\"input_tensor\")\r\n\r\n#defining the network\r\n#cell = rnn.BasicLSTMCell(num_units,forget_bias=0)\r\nlstm_layer = tf.nn.rnn_cell.MultiRNNCell([rnn.BasicLSTMCell(num_units) for _ in range(3)])\r\noutputs, _ = rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\r\n\r\n#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\r\nlogits_=tf.add(tf.matmul(outputs[-1],out_weights), out_bias)\r\nprediction = tf.nn.softmax(logits_, name=\"y_\")\r\n\r\n#loss_function\r\nloss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\r\n#optimization\r\nopt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n\r\n#model evaluation\r\ncorrect_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\r\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n#initialize variables\r\nsess=tf.InteractiveSession()\r\nsess.run(tf.global_variables_initializer())\r\n\r\niter=1\r\nwhile iter<800:\r\n    batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\r\n    batch_x=batch_x.reshape((batch_size,time_steps,n_input))\r\n    sess.run(opt, feed_dict={x: batch_x, y: batch_y})\r\n\r\n    if iter %10==0:\r\n        acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\r\n        los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\r\n        print(\"For iter \",iter)\r\n        print(\"Accuracy \",acc)\r\n        print(\"Loss \",los)\r\n        print(\"__________________\")\r\n\r\n    iter=iter+1\r\n\r\ntf.train.write_graph(sess.graph_def, '/tmp/checkpoint', 'har.pbtxt') \r\nsaver = tf.train.Saver()\r\nsaver.save(sess, save_path = \"/tmp/checkpoint/har.ckpt\")\r\nsess.close()\r\n```\r\n\r\nTo freeze the graph...\r\nSource: https://github.com/tensorflow/tensorflow/issues/19268\r\n\r\n\r\n```\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\nMODEL_NAME = 'har'\r\n\r\ninput_graph_path = '/tmp/checkpoint/' + MODEL_NAME+'.pbtxt'\r\ncheckpoint_path = '/tmp/checkpoint/' +MODEL_NAME+'.ckpt'\r\nrestore_op_name = \"save/restore_all\"\r\nfilename_tensor_name = \"save/Const:0\"\r\noutput_frozen_graph_name = 'frozen_'+MODEL_NAME+'.pb'\r\n\r\nfreeze_graph.freeze_graph(input_graph_path, input_saver=\"\",\r\n                          input_binary=False, input_checkpoint=checkpoint_path, \r\n                          output_node_names=\"y_\", restore_op_name=\"save/restore_all\",\r\n                          filename_tensor_name=\"save/Const:0\", \r\n                          output_graph=output_frozen_graph_name, clear_devices=True, initializer_nodes=\"\")\r\n```\r\n\r\nAnd to convert to tflite...\r\nSource : https://stackoverflow.com/questions/50632152/tensorflow-convert-pb-file-to-tflite-using-python\r\n\r\n\r\n```\r\npath_to_frozen_graphdef_pb = '/content/frozen_har.pb'\r\ninput_tensors = [\"input\"]\r\noutput_tensors = [\"y_\"]\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(path_to_frozen_graphdef_pb, input_tensors, output_tensors)\r\n\r\ntflite_model = converter.convert()\r\nopen(\"converted_file.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nI am running it on Google colab btw\r\n\r\nDownloaded the tflite and manage to run it on the android app from : \r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/digit_classifier\r\n\r\nI enabled GPU delegate too https://www.tensorflow.org/lite/performance/gpu#trying_the_gpu_delegate_on_your_own_model\r\n\r\nFound all these solutions on stackoverflow and github issue, just collating and sharing", "pip install tf-nightly\r\nsolve my issue.", "I know this is an old issue with previous TF versions. Most of the LSTM related converter issues were resolved with recent versions. Please follow the instructions provided in [this issue](https://github.com/tensorflow/tensorflow/issues/39564#issuecomment-630097911) to convert any LSTM related tflite converter issue. Thanks!\r\n\r\nIf you still have a problem, please post a new issue with a simple standalone code to reproduce the issue. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 15804, "title": "[bug]libtensorflowlite_jni.so crash when creating interpreter with byteBuffer mode", "body": "Dear tensorflow developers\uff1a\r\n       A crash is found when using java nio to create interpreter of tensorflowLite, which make us puzzled for a long time. I hope you can help us to solve the issue, thanks & best regard. The issues will be described in detail as follows:\r\n\r\n       Naturally, we use the function provided by tensorflow below to create model & interpreter.      \r\n \r\n       NativeInterpreterWrapper(ByteBuffer modelByteBuffer) {\r\n          errorHandle = createErrorReporter(ERROR_BUFFER_SIZE);\r\n          modelHandle = createModelWithBuffer(modelByteBuffer, errorHandle);\r\n          interpreterHandle = createInterpreter(modelHandle);\r\n       }\r\n\r\n       step 1.  We create the Input parameter modelByteBuffer. The ByteBuffer as follows:\r\n\r\n          fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n          \r\n          The following conditions should be satisfied at the same time, \r\n          1)  startOffset is not zero,  for example startOffset = 200\r\n               If the startOffset is zero, the issue will not occur.\r\n          2)  the byteBuffer size is very large, for example size = 80MB\r\n               if the size is small, the issue will not occur.\r\n\r\n       Step 2. We call the funtion NativeInterpreterWrapper with byteBuffer.\r\n        \r\n       Unfortunately, after running  step 1 & 2, the crash is occured as follows:\r\n\r\n01-02 18:58:54.544 21135-21135/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n01-02 18:58:54.545 21135-21135/? A/DEBUG: Build fingerprint: 'google/marlin/marlin:8.0.0/OPR3.170623.013/4397526:user/release-keys'\r\n01-02 18:58:54.545 21135-21135/? A/DEBUG: Revision: '0'\r\n01-02 18:58:54.545 21135-21135/? A/DEBUG: ABI: 'arm'\r\n01-02 18:58:54.545 21135-21135/? A/DEBUG: pid: 20837, tid: 20837, name: fish.xxxxxx  >>> com.taobao.idlexxxx.xxxxxx <<<\r\n01-02 18:58:54.545 21135-21135/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xa733f5a6\r\n01-02 18:58:54.545 21135-21135/? A/DEBUG:     r0 a733f5a6  r1 00000004  r2 00000006  r3 00000000\r\n01-02 18:58:54.545 21135-21135/? A/DEBUG:     r4 e0f59c58  r5 de9734b0  r6 a73f5160  r7 ffc5de20\r\n01-02 18:58:54.545 21135-21135/? A/DEBUG:     r8 00000004  r9 00000000  sl ffc5de90  fp a733f57a\r\n01-02 18:58:54.545 21135-21135/? A/DEBUG:     ip edee063c  sp ffc5dd90  lr edea7741  pc a73aa208  cpsr 200f1830\r\n01-02 18:58:54.546 21135-21135/? A/DEBUG: backtrace:\r\n01-02 18:58:54.546 21135-21135/? A/DEBUG:     #00 pc 00062208  /data/app/com.taobao.idlexxxx.xxxxxx-qw202S8jC-x2xWFXznSPlw==/lib/arm/libtensorflowlite_jni.so\r\n01-02 18:58:54.546 21135-21135/? A/DEBUG:     #01 pc 00062d53  /data/app/com.taobao.idlexxxx.xxxxxx-qw202S8jC-x2xWFXznSPlw==/lib/arm/libtensorflowlite_jni.so\r\n01-02 18:58:54.546 21135-21135/? A/DEBUG:     #02 pc 00006d93  /data/app/com.taobao.idlexxxx.xxxxxx-qw202S8jC-x2xWFXznSPlw==/lib/arm/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_createInterpreter+50)\r\n01-02 18:58:54.546 21135-21135/? A/DEBUG:     #03 pc 000281ef  /data/app/com.taobao.idlexxxx.xxxxxx-qw202S8jC-x2xWFXznSPlw==/oat/arm/base.odex (offset 0x1c000)\r\n01-02 18:58:55.403 742-742/? E//system/bin/tombstoned: Tombstone written to: /data/tombstones//tombstone_03\r\n\r\n\r\n    While, If we reallocate memory, the issue will also not occur .The relative code is as follows:\r\n    \u2026\u2026.\r\n    rf = new RandomAccessFile(modelPath, \"r\");\r\n    declaredLength = rf.length() - startOffset;\r\n    rf.seek(startOffset);\r\n    ByteBuffer byteBuffer = ByteBuffer.allocateDirect((int) declaredLength);\r\n    \u2026\u2026.\r\n    byteBuffer.put(tfbyte);\r\n    \u2026\u2026.\r\n\r\n    Is there a memory allocation bug? And could you tell me how to avoid the crash if not reallocate \r\n    memory\uff1f Thanks very much.\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code N/A\r\nOS Platform and Distribution  Android7.0\r\nTensorFlow installed from Github\r\nTensorFlow version 1.4.0\r\nBazel version 0.8.1-homebrew\r\nCUDA/cuDNN version N/A\r\nGPU model and memory CPU only", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is there any process on this issue?", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Why aren't you using the Interpreter class that handled using NativeInterprererWrapper for you? \r\nMy guess is that you are not holding onto a reference to the mappedByteBuffer and when it goes out of scope and gets garbage collected, your program crashes since the pointer gets freed.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I'll close this one. Please reopen if this is still an issue."]}]