[{"number": 53085, "title": "ROCm specific fix to enable GPU unit tests to pass.", "body": "The reason this is necessary is because of recent changes to the ROCm specific convrunner which requires the workspace_size attribute to be present in the algorithm descriptor.\r\n\r\n/cc @cheshire @chsigg @jurahul ", "comments": ["This is best review by @awpr as he is working on this code.", "For more clarity, this fix is needed because of this code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/rocm/rocm_dnn.cc#L3199-L3203\r\n\r\nWithout the fix, 20+ GPU unit tests fail on ROCm.", "Thanks for finding this!\r\n\r\nI don't think the code can be correct as-is: the fields `conv` and `algorithm` are part of the same oneof group, so using them both at the same time is suspicious, since one or the other must be getting a default value.  It looks like this will always set a workspace size of 0.\r\n\r\nI think the right fix would be to make `conv_ops_gpu.cc:299` set `mutable_algorithm()` directly to the whole `AlgorithmProto` (`profile_result.algorithm().ToProto()`), and update this function to expect the full proto in `algorithm()` rather than rebuild it from its components.", "@awpr I have updated the PR based on your feedback."]}, {"number": 53084, "title": "[ROCm] Mark a few GPU tests as CUDA only", "body": "The CudaMallocAsync tests in `//tensorflow/core/common_runtime/gpu:gpu_device_test_gpu` are CUDA specific. This PR marks these as DISABLED_ on ROCM platforms. ", "comments": ["@cheshire @chsigg @deven-amd", "@cheshire gentle nudge on this one. thx!"]}, {"number": 53083, "title": "TensorFlow Lite CMake build is broken (label_image & benchmark_model) on TF 2.7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yocto Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.7.0\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): N/A\r\n- CMake version: 3.19.5\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe TensorFlow Lite build using CMake is broken in v2.7.0 for generic Arm device (aarch64). \r\nThe problem is caused by this commit https://github.com/tensorflow/tensorflow/commit/feb49693266f444d5d8ce1a439ffbe7ff6e15e8a . First problem is only the bazel build scripts were updated (the CMake does not support the Hexagon delegate yet at all). Secondly the implementation expects the Qualcomm's Hexagon DSP is available on every aarch64 platform, what is not generally true.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Configure the project via CMake: \r\n```\r\ncmake -DCMAKE_TOOLCHAIN_FILE=${OE_CMAKE_TOOLCHAIN_FILE} ../tensorflow/lite/\r\n```\r\n2. Build the label_image or benchmark_model application: \r\n```\r\nmake -j6 label_image\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\n[100%] Linking CXX executable label_image\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: CMakeFiles/label_image.dir/__/__/tools/evaluation/utils.cc.o: in function `tflite::evaluation::CreateHexagonDelegate(TfLiteHexagonDelegateOptions const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::{lambda(TfLiteDelegate*)#1}::_FUN(TfLiteDelegate*)':\r\n/home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:165: undefined reference to `TfLiteHexagonDelegateDelete'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: CMakeFiles/label_image.dir/__/__/tools/evaluation/utils.cc.o: in function `operator()':\r\n/home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:166: undefined reference to `TfLiteHexagonTearDown'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: CMakeFiles/label_image.dir/__/__/tools/evaluation/utils.cc.o: in function `tflite::evaluation::CreateHexagonDelegate(TfLiteHexagonDelegateOptions const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':\r\n/home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:154: undefined reference to `TfLiteHexagonInit'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:159: undefined reference to `TfLiteHexagonDelegateCreate'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:156: undefined reference to `TfLiteHexagonInitWithPath'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:159: undefined reference to `TfLiteHexagonDelegateCreate'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:161: undefined reference to `TfLiteHexagonTearDown'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:154: undefined reference to `TfLiteHexagonInit'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:159: undefined reference to `TfLiteHexagonDelegateCreate'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:156: undefined reference to `TfLiteHexagonInitWithPath'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:159: undefined reference to `TfLiteHexagonDelegateCreate'\r\n/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:161: undefined reference to `TfLiteHexagonTearDown'\r\ncollect2: error: ld returned 1 exit status\r\nexamples/label_image/CMakeFiles/label_image.dir/build.make:331: recipe for target 'examples/label_image/label_image' failed\r\nmake[3]: *** [examples/label_image/label_image] Error 1\r\nCMakeFiles/Makefile2:4330: recipe for target 'examples/label_image/CMakeFiles/label_image.dir/all' failed\r\nmake[2]: *** [examples/label_image/CMakeFiles/label_image.dir/all] Error 2\r\nCMakeFiles/Makefile2:4337: recipe for target 'examples/label_image/CMakeFiles/label_image.dir/rule' failed\r\nmake[1]: *** [examples/label_image/CMakeFiles/label_image.dir/rule] Error 2\r\nMakefile:1414: recipe for target 'label_image' failed\r\nmake: *** [label_image] Error 2\r\n```\r\n", "comments": ["Can you please retry from master branch. A fix was pushed.\r\n\r\nThanks", "Closing, please reopen if issue still happen.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53083\">No</a>\n", "Do we need a cherrypick on 2.7 to fix the build there, @karimnosseir ?", "@mihaimaruseac Yes, i will send cherrypicks now.\r\n\r\nThanks"]}, {"number": 53082, "title": "Extend TF:TRT C++ API to handle non-frozen models", "body": "This PR extends the C++ converter API of TF-TRT #52012 to handle models that are not frozen (i.e. contains variables).\r\n\r\nDepends on #53310 . Tracker #45481.", "comments": ["@tfeher could you please resolve conflicts ?", "Conflicts resolved, tagging @bixia1 for review.", "@bixia1 Can you please review this PR ? Thanks!", "@tfeher Can you please address Ubuntu Sanity errors? Thanks!", "Addressed sanity errors. Also rebased and fixed missing argument that was introduced in #53507. ", "@tfeher Can you please check @bixia1's comments and keep us posted ? Thanks!"]}, {"number": 53081, "title": "tensorflow-gpu-v2.3.0  C++ can not infer on Tesla P4", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.3.0\r\n- Python version: python3.6\r\n- Bazel version (if compiling from source): bazel-3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0\r\n- CUDA/cuDNN version: CUDA-10.2\uff0ccuDNN8\r\n- \r\nERROR:\r\n2021-11-16 13:46:36.936240: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n2021-11-16 13:46:36,936 ERROR [EasyEdge] [tf_edge_predictor.cpp:74] 140512694415424 NewSession failed...Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n\r\n", "comments": ["@gp1322719830 ,\r\nTensorflow v2.3  are built and tested against CUDA 10.1 and cudnn 7.6. Could you please check if you are facing the same error with compatible version.Please take a look at this link for tested build [configurations](https://www.tensorflow.org/install/source#gpu).It helps Thanks!", "Need I to specify CUDA \tCompute Capability to 7.0 for v2.3 source code compilation? I see that the compiled library Compute Capability is 6.1.", "2021-11-17 05:07:47.350129: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-11-17 05:07:47.461768: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1994875000 Hz\r\n2021-11-17 05:07:47.464991: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f409095d790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-11-17 05:07:47.465045: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-11-17 05:07:47.469552: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2021-11-17 05:07:48.217466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.422117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.427598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.428541: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f40908b62c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-11-17 05:07:48.428573: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\r\n2021-11-17 05:07:48.428597: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla P4, Compute Capability 6.1\r\n2021-11-17 05:07:48.428613: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla P4, Compute Capability 6.1\r\n2021-11-17 05:07:48.428629: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla P4, Compute Capability 6.1\r\n2021-11-17 05:07:48.447802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:3b:00.0 name: Tesla P4 computeCapability: 6.1\r\ncoreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2021-11-17 05:07:48.448055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.448715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:\r\npciBusID: 0000:86:00.0 name: Tesla P4 computeCapability: 6.1\r\ncoreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2021-11-17 05:07:48.448875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.449527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties:\r\npciBusID: 0000:af:00.0 name: Tesla P4 computeCapability: 6.1\r\ncoreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2021-11-17 05:07:48.449601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.450251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 3 with properties:\r\npciBusID: 0000:d8:00.0 name: Tesla P4 computeCapability: 6.1\r\ncoreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2021-11-17 05:07:48.450317: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-11-17 05:07:48.475294: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-11-17 05:07:48.491855: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-11-17 05:07:48.495737: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-11-17 05:07:48.526848: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-11-17 05:07:48.532839: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-11-17 05:07:48.575837: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-11-17 05:07:48.576671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.577287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.577904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.579055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.579625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.580197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-17 05:07:48.580755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3\r\n2021-11-17 05:07:48.580802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n^[2021-11-17 05:11:36.450306: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n2021-11-17 05:11:36.450354: E tensorflow/core/common_runtime/session.cc:70] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\nSegmentation fault (core dumped)\r\n", "thanks\uff0csloved", "@gp1322719830 ,\r\nPlease feel free to move this issue to closed status as the issue has resolved.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53081\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53081\">No</a>\n"]}, {"number": 53080, "title": "Get an index of each worker in `ParameterServerStrategy `", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn the current implementation of ParameterServerStrategy, the `ClusterCoordinator.schedule method` assumes workers are equivalent and thus assumes the datasets on different workers are the same except they may be shuffled differently if they contain a `Dataset.shuffle` operation.\r\n\r\nBut i want to shard the dataset across the worker.\r\nLet's assume that i want to make a dataset from generator function using `tf.data.Dataset.from_generator`. If there's a way to get the index of the environment where the function is running, it's possible for the generator to emit a different result by the task index. \r\nFor now, i couldn't find a way to do it. so I filed this issue. (if there's a way, please let me know)\r\n\r\nexample code\r\n```\r\nNUM_WORKERS = 10\r\ndef example_generator(raw_dataset): \r\n worker_idx = tf.??? # the feature i request. [0-9] should be returned as the num_worker = 10\r\n for i,d in enumerate(examples):\r\n    if (i % NUM_WORKERS) == worker_idx:\r\n      yield d\r\n```\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @sanatmpa1! Could you please look at this feature request ?", "You may have to look into [PerWorkerValues](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/PerWorkerValues) which can help you. Thanks!", "@sachinprasadhs could you elaborate more please?\r\n\r\nhere's the explanation on the `PerWorkerValues` page\r\n```Currently, the only supported path to create an object of tf.distribute.experimental.coordinator.PerWorkerValues is through calling iter on a ClusterCoordinator.create_per_worker_dataset returned distributed dataset instance. The mechanism to create a custom tf.distribute.experimental.coordinator.PerWorkerValues is not yet supported.```\r\nthe point i'm wondering about is how to get a worker index inside the function generating per-worker dataset.\r\n", "Hi, in short, we advise not to do so because 1. iterating through dataset but not using it (in this case, using 1/(#worker) of it) could be quite expensive and wasting resource 2. accessing the worker index inside the dataset creation function is very error prone and can't be achieved with public APIs. ", "@moog-moloco Could you please refer to the [comment](https://github.com/tensorflow/tensorflow/issues/53080#issuecomment-1003533848)  above and this [thread](https://stackoverflow.com/questions/68778350/do-we-need-dataset-in-each-of-the-worker-when-using-parameterserverstrategy) .Please let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53078, "title": "how to control thread number in Tensorflow1.12?", "body": "I am using [https://github.com/mlcommons/training](mlcommon/training) to train resnet50.\r\nOn my device, it is thread limitation.\r\nSo I want to control the thread number while trainging.\r\nThere are too many threads, As shown in the figure below.\r\n![image](https://user-images.githubusercontent.com/17714376/141889577-41a39540-e6b2-413f-a7d9-2c7125a5c4c0.png)\r\n", "comments": ["@warren-lei ,\r\nWe see that you are using tf version 1.12, 1.x is not actively supported, please update to 2.6 or 2.7 and let us know if you are using same issue.", "Also please take a look at this reference link [1](https://github.com/pudae/tensorflow-densenet/issues/7#issuecomment-351054685) and [2](https://github.com/tensorflow/models/issues/3176) with similar query.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53077, "title": "macOS12 tensorflow-metal error", "body": "**System information**\r\n\r\n- OS Platform and Distribution : macOS 12.1 intel\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):tensorflow-metal 2.6\r\n- Python version:3.8\r\n- GPU model and memory: AMD Radeon RX 6600\r\n\r\nErrors will be reported when training the model\r\n```\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.\r\n```\r\nFor example, the following program\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.config.list_physical_devices())\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=[28, 28]),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\nprint(model.summary())\r\n\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n2021-11-16 10:59:28.127916: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers\r\nTraceback (most recent call last):\r\n  File \"/Volumes/Workspace/PythonProject/centernet2-tf2/main.py\", line 5, in <module>\r\n    model = tf.keras.models.Sequential([\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/__init__.py\", line 25, in <module>\r\n    from keras import models\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/models.py\", line 20, in <module>\r\n    from keras import metrics as metrics_module\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/metrics.py\", line 26, in <module>\r\n    from keras import activations\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/activations.py\", line 20, in <module>\r\n    from keras.layers import advanced_activations\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/layers/__init__.py\", line 23, in <module>\r\n    from keras.engine.input_layer import Input\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/engine/input_layer.py\", line 21, in <module>\r\n    from keras.engine import base_layer\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 43, in <module>\r\n    from keras.mixed_precision import loss_scale_optimizer\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/mixed_precision/loss_scale_optimizer.py\", line 18, in <module>\r\n    from keras import optimizers\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/optimizers.py\", line 26, in <module>\r\n    from keras.optimizer_v2 import adadelta as adadelta_v2\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/optimizer_v2/adadelta.py\", line 22, in <module>\r\n    from keras.optimizer_v2 import optimizer_v2\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 36, in <module>\r\n    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py\", line 360, in __init__\r\n    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,\r\n  File \"/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py\", line 135, in __init__\r\n    self._metric = self._metric_methods[self._label_length].create(*args)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.\r\n```\r\n\r\nThese are not the issues with the CPU version. It's OK to do simple things with the GPU, but these errors will occur as soon as you train\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.config.list_physical_devices())\r\n\r\nwith tf.device('/GPU'):\r\n    a = tf.random.normal(shape=(2,), dtype=tf.float32)\r\n    b = tf.nn.relu(a)\r\n    print(a)\r\n\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\nMetal device set to: AMD Radeon RX 6600\r\n\r\nsystemMemory: 16.00 GB\r\nmaxCacheSize: 3.99 GB\r\n\r\n2021-11-16 10:53:19.473055: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-16 10:53:19.473518: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n2021-11-16 10:53:19.473738: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\ntf.Tensor([-1.0673397 -1.3157675], shape=(2,), dtype=float32)\r\n\r\n```\r\n\r\n   \r\n\r\n\r\n", "comments": ["Hi @Yuri-Su! Could you try again with TF 2.7 version ?", "> Hi @Yuri-Su! Could you try again with TF 2.7 version ?\r\n\r\nThank you for your reply, but the tensorflow-macos version only has TF2.5 and 2.6", "Agreed! @Yuri-Su . \r\n\r\n1.Please check once you are not using Tensorflow library in another environment or program. \r\n2.You can verify whether Tensorflow version and Keras version are same. if not same you can install manually same keras version as Tensorflow version to fix the issue.\r\n\r\n\r\nAttaching  relevant threads for reference. [link1](https://discuss.tensorflow.org/t/alreadyexistserror-another-metric-with-the-same-name-already-exists/3787/18),[link2](https://stackoverflow.com/questions/58012741/error-importing-tensorflow-alreadyexistserror-another-metric-with-the-same-nam),[link3](https://github.com/keras-team/keras/issues/15579),[link4](https://github.com/tensorflow/tensorflow/issues/52951).\r\n\r\nYou can post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) if the issue still persists.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . \r\n\r\nThanks!", "> Agreed! @Yuri-Su .\r\n> \r\n> 1.Please check once you are not using Tensorflow library in another environment or program. 2.You can verify whether Tensorflow version and Keras version are same. if not same you can install manually same keras version as Tensorflow version to fix the issue.\r\n> \r\n> Attaching relevant threads for reference. [link1](https://discuss.tensorflow.org/t/alreadyexistserror-another-metric-with-the-same-name-already-exists/3787/18),[link2](https://stackoverflow.com/questions/58012741/error-importing-tensorflow-alreadyexistserror-another-metric-with-the-same-nam),[link3](https://github.com/keras-team/keras/issues/15579),[link4](https://github.com/tensorflow/tensorflow/issues/52951).\r\n> \r\n> You can post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) if the issue still persists. To know more see; https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999 .\r\n> \r\n> Thanks!\r\n\r\nThanks. My problem has been solved. This is because the versions of tensorflow MacOS and keras installed by PIP do not match. Reinstall the corresponding version. Thank you very much for your method!", "Ok! @Yuri-Su ! Feel free to close this issue if it helped . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53077\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53077\">No</a>\n"]}, {"number": 53076, "title": "Worse performance after converting tf.compat.v1.nn.ctc_loss to tf.nn.ctc_loss", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 1..2\r\n- GPU model and memory: GeForce RTX 3070 8GB\r\n\r\n**Describe the current behavior**\r\nI am trying to convert parts of my code from using tf.compat.v1 to pure tf functions. Here is the previous working function, that gives me good performance.\r\n\r\n`self.loss = tf.reduce_mean(\r\n            input_tensor=tf.compat.v1.nn.ctc_loss(\r\n                labels=self.gt_texts,\r\n                inputs=self.ctc_in_3d_tbc,\r\n                sequence_length=self.seq_len,\r\n                ctc_merge_repeated=True,\r\n            )\r\n        )\r\n`\r\n\r\nThis is my attempt to rewrite the function using tf.nn.ctc_loss.\r\n\r\n`self.loss = tf.reduce_mean(\r\n            input_tensor=tf.nn.ctc_loss(\r\n                labels=self.gt_texts,  # sparse tensor\r\n                logits=self.ctc_in_3d_tbc,  \r\n                label_length=None,  \r\n                logit_length=self.seq_len,  \r\n                blank_index=-1,\r\n            ))\r\n`\r\nPlease let me know if attaching logfiles will be helpful. I figure I must simply be configuring some of the arguments wrong. ", "comments": ["@thetruejacob Could you please try to execute your code on latest stable version of TF `2.7.0` and let us know if the issue still persists? Thank you!", "@sushreebarsa Thank you for the quick reply. I'm currently running on TF2.7.0 and can say that while I used to get around 60-63% accuracies, now I only get 46-48% with the new ctc loss function. Am I passing in the right function arguments?", "@thetruejacob \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53075, "title": "I want to convert tFLite model into PB save_model, can you help me?  ", "body": "I want to convert tFLite model into PB save_model, can you help me?  \r\n\r\nAfter I transform the TFlite model of four output tensors into RKNN model, the data structure and result are completely different. Do you know why?  ", "comments": ["@LMR2018 ,\r\nCan you please take a look at this links [1](https://stackoverflow.com/questions/53664279/converting-tflite-to-pb) [2](https://coderedirect.com/questions/293517/converting-tflite-to-pb) and [3](https://www.tensorflow.org/lite/convert).It helps.Thanks", "> @LMR2018\uff0c \u4f60\u80fd\u770b\u770b\u8fd9\u4e2a\u94fe\u63a5[1 ](https://stackoverflow.com/questions/53664279/converting-tflite-to-pb) [2](https://coderedirect.com/questions/293517/converting-tflite-to-pb)\u548c[3](https://www.tensorflow.org/lite/convert)\u5417\u3002\u5b83\u6709\u5e2e\u52a9\u3002\u8c22\u8c22\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n\r\nSo this is normal, and then how do we convert to pb model \uff1f", "tensorflow_1.12/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco -- \r\noutput_file=coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.pb -- \r\noutput_format=TENSORFLOW_GRAPHDEF --input_format=TFLITE -- \r\ninput_file=coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.tflite -- \r\ninference_type=FLOAT --input_type=FLOAT --input_array=\"\" --output_array=\"\" -- \r\ninput_shape=1,450,450,3 --dump_grapHviz=./\r\n\r\nIt is successful to convert into PB model according to TOCO, but pb model cannot be converted into TFLite model or RKNN model again\uff1fSo there's something wrong with him?  ", "@LMR2018 ,\r\nWe see that you are using tf version 1.12, 1.x is not actively supported, please update to v2.6 or 2.7 and let us know if you are facing same issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53075\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53075\">No</a>\n"]}, {"number": 53074, "title": "A Node name contains invalid characters error occurs when the PB model is converted to tFLite  ", "body": "During handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"model_change.py\", line 109, in <module>\r\n    test()\r\n  File \"model_change.py\", line 79, in test\r\n    tf.import_graph_def(output_graph_def, name='')\r\n  File \"/home/ubuntu/anaconda3/envs/rknn/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/rknn/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\", line 405, in import_graph_def\r\n    producer_op_list=producer_op_list)\r\n  File \"/home/ubuntu/anaconda3/envs/rknn/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\", line 505, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Node 'model/model/tf.nn.relu6/Relu6;model/model/batch_normalization/FusedBatchNormV3;model/model/batch_normalization_1/FusedBatchNormV3;model/model/depthwise_conv2d/depthwise;model/model/conv2d_9/Conv2D;model/model/conv2d/Conv2D_unfused/conv': Node name contains invalid characters\r\n", "comments": ["Hi @LMR2018! \r\nCould you please update the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced]?.Thanks!", "I converted the Tflite model into PB model using TOCO tool, and then converted the PB model back to Tflite model, and this error occurred\r\n\r\nTflite model to PB model command \uff1a\r\nbazel-bin/tensorflow/lite/toco/toco --output_file=hand_landmark.pb --output_format=TENSORFLOW_GRAPHDEF --input_format=TFLITE --input_file=hand_landmark.tflite --inference_type=FLOAT --input_type=FLOAT --input_array=input_1 --output_array=Identity --input_shape=1,224,224,3\r\n\r\npb model to tflite model command \uff1a\r\npython tflite_convert.py --graph_def_file=hand_landmark.pb --output_file=hand.tflite --input_arrays=input_1 --output_arrays=Identity --input_shape=1,224,224,3 --allow_custom_ops", "Hi @sanatmpa1! Could you please look at this issue? ", "@LMR2018,\r\n\r\nI found a similar issue [here](https://stackoverflow.com/questions/66274357/got-a-valueerror-when-convert-pb-to-tflite) where the TF Support has recommended to try out the [lite converter](https://www.tensorflow.org/lite/convert), But in your case you're already using the tflite converter for .pb to tflite conversion and that is where you're encountering the error, Can you confirm if my understanding is right? Also can you lets us know which version of TF are you using? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53074\">No</a>\n"]}, {"number": 53073, "title": "AttributeError: module 'tensorflow' has no attribute 'variable_scope'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):2.7.0\r\n- TensorFlow version (use command below):\r\n- Python version:Python 3.8.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):GCC 10.2.0\r\n- CUDA/cuDNN version:nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Mon_Oct_12_20:09:46_PDT_2020\r\nCuda compilation tools, release 11.1, V11.1.105\r\nBuild cuda_11.1.TC455_06.29190527_0\r\n- GPU model and memory:\r\ngpu-2-0.saber\r\n     properties = batch_gpu,gpu-g3,dell,saber,x86_64,nvidia\r\n     status = opsys=linux,uname=Linux gpu-2-0.saber 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64,sessions=274505 349523 380034 397050,nsessions=4,nusers=2,idletime=8805,totmem=101384352kb,availmem=99296204kb,physmem=97190052kb,ncpus=16,loadave=0.00,gres=,netload=40652996470,state=free,varattr= ,cpuclock=Fixed,version=6.1.3,rectime=1637006909,jobs=\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen I run my script:\r\n\r\npython train.py\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 109, in <module>\r\n    nn = NeuralNetwork(F=args.num_features,           \r\n  File \"/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master/neural_network/NeuralNetwork.py\", line 53, in __init__\r\n    with tf.variable_scope(self.scope):\r\nAttributeError: module 'tensorflow' has no attribute 'variable_scope'\r\n\r\n\r\n**Describe the expected behavior**\r\nFollowing previous issues I tried replacing in my code:\r\nthis line:\r\n tf.global_variables_initializer().run()\r\nwith this one:\r\n tf.compat.v1.global_variables_initializer().run()\r\n\r\nthe error stated above remains the same in both instances.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@anamariaUIC \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Hello,\r\n\r\nPlease find the attached code.\r\n[train.txt](https://github.com/tensorflow/tensorflow/files/7546842/train.txt)\r\n\r\n\r\nThanks\r\n[Ana[](url)](url)", "Hi @sushreebarsa can you please tell me if they are any updates on this issue? I attached my code in the previous comment.\r\n\r\nThanks\r\nAna", "@anamariaUIC I tried to replicate the issue on colab using TF v2.7.0 but didn't face the error reported here. Could you please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/c4993b524d7eba838e5a9374f1b94ad6/53073.ipynb) and confirm the same ? Thanks!", "@sushreebarsa I can see from the gist that you used Python 2.7. While I am using:\r\nmodule load Python/3.8.6-GCCcore-10.2.0\r\nmodule load CUDA/11.1.1-GCC-10.2.0\r\n\r\nBecause I am trying to run this on GPU. Can you please test this with my Python and CUDA version?\r\n\r\nOnce again this is what is happening on my side:\r\nanamaria@gpu-2-0.saber:/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master $ python\r\n**Python 3.8.6** (default, Sep 30 2021, 21:09:05) \r\n[GCC 10.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> quit()\r\nanamaria@gpu-2-0.saber:/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master $ python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n**v2.7.0-rc1-69-gc256c071bb2 2.7.0**\r\nanamaria@gpu-2-0.saber:/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master $ python train.py \r\nTraceback (most recent call last):\r\n  File \"train.py\", line 109, in <module>\r\n    nn = NeuralNetwork(F=args.num_features,           \r\n  File \"/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master/neural_network/NeuralNetwork.py\", line 53, in __init__\r\n    with tf.variable_scope(self.scope):\r\n**AttributeError: module 'tensorflow' has no attribute 'variable_scope'**", "Hi, did you try using `tf.compat.v1.variables_initializer` to initialize variables.", "@sachinprasadhs do you mean replacing in the code I attached:\r\ntf.global_variables_initializer().run()\r\nwith\r\ntf.compat.v1.variables_initializer.run()\r\n\r\nI tried that and I got this:\r\n\r\npython train.py \r\nTraceback (most recent call last):\r\n  File \"train.py\", line 109, in <module>\r\n    nn = NeuralNetwork(F=args.num_features,           \r\n  File \"/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master/neural_network/NeuralNetwork.py\", line 53, in __init__\r\n    with tf.variable_scope(self.scope):\r\nAttributeError: module 'tensorflow' has no attribute 'variable_scope'\r\n\r\n\r\nIf this is not what you meant please send me precise instructions on what to do.\r\n\r\nThanks", "From your error log I could notice that you're creating a `nn` class object by calling NeuralNetwork.py, which I assume the same file [here](https://github.com/MMunibas/PhysNet/blob/master/neural_network/NeuralNetwork.py), in line 53 where it uses `tf.variable_scope` is depreciated in Tensorflow 2.x and instead you need to use `tf.compat.v1.variable_scope`.\r\nEven with this modification you may find other issues since the code is not compatible in Tensorflow 2.x, you can see [this](https://www.tensorflow.org/guide/migrate) migration documentation for code migration from 1.x to 2.x. Thanks!", "@sachinprasadhs ok I installed different version of tensorflow with:\r\n\r\nmodule load Python/3.6.6-fosscuda-2018b\r\npip3 install tensorflow-gpu==1.10.1  --user\r\n\r\nwhile I was logged on my GPU node.\r\nYes indeed I am running [this code](https://github.com/MMunibas/PhysNet/blob/master/train.py):\r\n\r\nI via:\r\nanamaria@gpu-2-0.saber:/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master $ python train.py\r\nTraceback (most recent call last):\r\n  File \"/home/anamaria/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/anamaria/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/anamaria/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/Python/3.6.6-fosscuda-2018b/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/Python/3.6.6-fosscuda-2018b/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/anamaria/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/anamaria/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/anamaria/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/anamaria/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/anamaria/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/anamaria/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/Python/3.6.6-fosscuda-2018b/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/Python/3.6.6-fosscuda-2018b/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nhere is what is in my ENV:\r\n\r\nanamaria@gpu-2-0.saber:/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master $ module list\r\n\r\nCurrently Loaded Modules:\r\n  1) GCCcore/7.3.0                  8) XZ/5.2.4-GCCcore-7.3.0           15) FFTW/3.3.8-gompic-2018b                      22) SQLite/3.24.0-GCCcore-7.3.0\r\n  2) zlib/1.2.11-GCCcore-7.3.0      9) libxml2/2.9.8-GCCcore-7.3.0      16) ScaLAPACK/2.0.2-gompic-2018b-OpenBLAS-0.3.1  23) GMP/6.1.2-GCCcore-7.3.0\r\n  3) binutils/2.30-GCCcore-7.3.0   10) libpciaccess/0.14-GCCcore-7.3.0  17) fosscuda/2018b                               24) libffi/3.2.1-GCCcore-7.3.0\r\n  4) GCC/7.3.0-2.30                11) hwloc/1.11.10-GCCcore-7.3.0      18) bzip2/1.0.6-GCCcore-7.3.0                    25) Python/3.6.6-fosscuda-2018b\r\n  5) CUDA/9.2.88-GCC-7.3.0-2.30    12) OpenMPI/3.1.1-gcccuda-2018b      19) ncurses/6.1-GCCcore-7.3.0\r\n  6) gcccuda/2018b                 13) OpenBLAS/0.3.1-GCC-7.3.0-2.30    20) libreadline/7.0-GCCcore-7.3.0\r\n  7) numactl/2.0.11-GCCcore-7.3.0  14) gompic/2018b                     21) Tcl/8.6.8-GCCcore-7.3.0\r\n\r\n \r\n\r\nanamaria@gpu-2-0.saber:/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master $ echo $PATH\r\n/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/Python/3.6.6-fosscuda-2018b/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/SQLite/3.24.0-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/Tcl/8.6.8-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/libreadline/7.0-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/ncurses/6.1-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/bzip2/1.0.6-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/FFTW/3.3.8-gompic-2018b/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/OpenBLAS/0.3.1-GCC-7.3.0-2.30/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/OpenMPI/3.1.1-gcccuda-2018b/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/hwloc/1.11.10-GCCcore-7.3.0/sbin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/hwloc/1.11.10-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/libxml2/2.9.8-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/XZ/5.2.4-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/numactl/2.0.11-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/CUDA/9.2.88-GCC-7.3.0-2.30:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/CUDA/9.2.88-GCC-7.3.0-2.30/nvvm/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/CUDA/9.2.88-GCC-7.3.0-2.30/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/binutils/2.30-GCCcore-7.3.0/bin:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/GCCcore/7.3.0/bin:/software/linux-el7-x86_64/apps/anaconda3/anaconda3/condabin:/projects/com_grassim/anamaria/anamaria/plink:/usr/local/bin:/usr/lib64/qt-3.3/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/anamaria/.local/bin:/software/linux-el7-x86_64/services/hadoop-2.8.2/sbin:/software/linux-el7-x86_64/services/hadoop-2.8.2/bin\r\nanamaria@gpu-2-0.saber:/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master $ echo $LD_LIBRARY_PATH\r\n/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/Python/3.6.6-fosscuda-2018b/lib/python3.6/site-packages/numpy-1.15.0-py3.6-linux-x86_64.egg/numpy/core/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/Python/3.6.6-fosscuda-2018b/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/libffi/3.2.1-GCCcore-7.3.0/lib64:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/libffi/3.2.1-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/GMP/6.1.2-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/SQLite/3.24.0-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/Tcl/8.6.8-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/libreadline/7.0-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/ncurses/6.1-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/bzip2/1.0.6-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/ScaLAPACK/2.0.2-gompic-2018b-OpenBLAS-0.3.1/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/FFTW/3.3.8-gompic-2018b/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/OpenBLAS/0.3.1-GCC-7.3.0-2.30/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/OpenMPI/3.1.1-gcccuda-2018b/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/hwloc/1.11.10-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/libpciaccess/0.14-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/libxml2/2.9.8-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/XZ/5.2.4-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/numactl/2.0.11-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/CUDA/9.2.88-GCC-7.3.0-2.30/nvvm/lib64:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/binutils/2.30-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/zlib/1.2.11-GCCcore-7.3.0/lib:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/GCCcore/7.3.0/lib64:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/GCCcore/7.3.0/lib\r\n\r\nI looked up this library:\r\nanamaria@gpu-2-0.saber:/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib $ ls libcublas*\r\n\r\nlibcublas_device.a  libcublas.so  libcublas.so.9.2  libcublas.so.9.2.88  libcublas_static.a\r\n\r\nPlease advise, **what should I install and how** to run the above mentioned code as it is?", "Currently we don't support Tensorflow 1.x issues, you can consider upgrading your code to Tensorflow 2.x by following migration guide.\r\nFor any different issue other than the issue reported in the template, please raise a new issue and close this, Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53073\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53073\">No</a>\n"]}, {"number": 53072, "title": "Tensorflow environment variables breaking python import", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Rocky Linux 8.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Both source and Pip\r\n- TensorFlow version: 2.7.0, 2.4.1\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 11.4/ cudnn 8.2\r\n- GPU model and memory: Nvidia A100 40 GB\r\n\r\nWhat environment variables does TF look for when being imported in Python? I have 2 installations on my system: 1 is compiled from source (2.4.1) and stored in a user directory. This installation is only for linking to the C++ libraries. The second is installed with pip for use in Python. \r\n\r\nI discovered that by setting the environment variable `TF_LIB_PATH`, the Python installation fails to import Tensorflow with this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/username/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/username/.local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb\r\n```\r\n\r\nI use various environment variables to set the paths to my source compiled C++ libraries.\r\n\r\nBut, by not setting the environment variable `TF_LIB_PATH`, I am able to import Tensorflow. Is this expected behavior?\r\n", "comments": ["I narrowed it down to `TF_LIB_PATH=<path to libtensorflow_cc.so>` being added to `LD_LIBRARY_PATH`. By removing this, I am able to import Tensorflow in Python again.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53072\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53072\">No</a>\n", "\r\nI had the same problem with TF 2.8 and [jonwittmer's answer](https://github.com/tensorflow/tensorflow/issues/53072#issuecomment-969283744) was the solution\r\n"]}, {"number": 53071, "title": "Understanding TFLite interpreter output tensor names and ordering", "body": "### 1. System information\r\n\r\nOS: Ubuntu 20.04 LTS\r\nTF v2.4.1, built from source\r\n\r\n### 2. Code\r\n\r\nCan supply code later if necessary. Mostly just wanted ask a quick question to see if this is a known issue with a known solution.\r\n\r\n### 3. Failure after conversion\r\n\r\nIt seems that TFLite model outputs are generically named and not ordered in a predictable way. If my original TF model has outputs like `[\"my_output_1\", \"my_output_2\"]`, the TFLite model's outputs (e.g. obtained from `interpreter.outputs()`, `interpreter.output_tensor(idx)` will have names like `PartitionedCall:1` and `PartitionedCall:0`. These two outputs will correspond to the two outputs of the TF model, but their order may be permuted.\r\n\r\nAfter searching the API as well as looking googling around a good amount, I was unable to find any information about\r\n* How to tell, other than manually checking shapes and verified expected output values, which output is which.\r\n* If there is a way to induce TFLite to keep the original output tensor names, or at least name the output tensors in a fashion that deterministically links them to their TF counterparts.\r\n\r\n\r\n", "comments": ["The recommended way to map Input/Output tensor information is to use SignatureDef, since SignatureDef name will be reserved between TF and TFLite conversion.", "@xhae Oh, nice, thanks. Looks like I'll need to upgrade to TF 2.5+ in order to get the signature list in the python API."]}, {"number": 53070, "title": "<spam removed>", "body": "<spam removed>", "comments": ["Please don't spam"]}, {"number": 53068, "title": "Tag vectorized_reduce_with_no_vector_registers to allow exclusion", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/53067\r\nThe tag can be used on the command line when building on or for AARCH64 platforms to exclude this test that is not applicable.", "comments": []}, {"number": 53067, "title": "Unit test //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test fails on AARCH64", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): git HEAD\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nTest fails with\r\n==================== Test output for //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test:\r\n[==========] Running 1 test from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters\r\n[ RUN      ] CodegenReduceOnArchWithNoVectorRegisters.Test\r\ntensorflow/compiler/xla/service/cpu/vectorized_reduce_with_no_vector_registers_test.cc:85: Failure\r\nValue of: _status_or_value7.status().ok()\r\n  Actual: false\r\nExpected: true\r\nINTERNAL: TargetRegistry::lookupTarget failed: No available targets are compatible with triple \"i686-none-android\"\r\n[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test (13 ms)\r\n[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters (13 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test suite ran. (13 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test\r\n\r\n 1 FAILED TEST\r\n================================================================================\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nTest passes\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing): Add tag to allow test to be excluded on AARCH64 builds\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe test has a hard coded link to an x86 platform triple. But more than that, the test simply does not make sense on AARCH64 as it wants to test the behaviour if there were no vector registers available, but that is never going to be the case on AARCH64. So rather than attempting to fix the test, which would probably bypass any content on AARCH64, just add a tag to the definition to allow it to be excluded.\r\n", "comments": ["@cfRod @nSircombe ", "Hi @elfringham! This issue will be closed once PR #53068 is merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53067\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53067\">No</a>\n"]}, {"number": 53066, "title": "On-Device Training with TensorFlow Lite cannot train on GPU.", "body": "**1.System information**\r\n**1) using colab**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab w/GPU\r\n- TensorFlow version (use command below): 2.7\r\n\r\n**2) using local machine**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow version (use command below): 2.7\r\n- Python version:3.8.8\r\n- CUDA/cuDNN version: 11.2 / 8.1\r\n- GPU model and memory: RTX3090\r\n\r\n**2. Code**\r\nI use On-Device Training with TensorFlow Lite example.\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\r\n\r\nI used to download a overview.py when I use my local machine.\r\n\r\n**3. Current behavior**\r\nWhen running on Colab with GPU backend, unexpected error occurs when executing the following Cell.\r\n```\r\nrestore(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))\r\n```\r\n```\r\nNUM_EPOCHS = 10\r\nBATCH_SIZE = 100\r\nepochs = np.arange(1, NUM_EPOCHS + 1, 1)\r\nlosses = np.zeros([NUM_EPOCHS])\r\nm = Model()\r\n\r\nfor i in range(NUM_EPOCHS):\r\n  for batch_idx in range(len(train_images) // BATCH_SIZE):\r\n    batched_images = train_images[BATCH_SIZE*(batch_idx) : BATCH_SIZE * (batch_idx + 1)]\r\n    batched_labels = train_labels[BATCH_SIZE*(batch_idx) : BATCH_SIZE * (batch_idx + 1)]\r\n    result = train(\r\n        x=tf.constant(batched_images, shape=(BATCH_SIZE, IMG_SIZE, IMG_SIZE),\r\n                      dtype=tf.float32),\r\n        y=tf.constant(batched_labels, shape=(BATCH_SIZE, 10), dtype=tf.float32))\r\n  losses[i] = result['loss']\r\n  print('Finished {0} epochs, current loss: {1}'.format(i + 1, losses[i]))\r\n\r\nplt.plot(epochs, losses)\r\nplt.show()\r\n```\r\nWhen running on a local machine with a GPU, Segmentation fault (Core dumped) occurs in the same place as Colab.\r\n```\r\n$ python overview.py \r\nTensorFlow version: 2.7.0\r\n2021-11-15 22:07:23.390165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:07:23.394521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:07:23.394982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:07:23.395740: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-15 22:07:23.396617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:07:23.397092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:07:23.397548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:07:23.701187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:07:23.701686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:07:23.702127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:07:23.702560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21492 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\r\n2021-11-15 22:07:24.482198: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\nFinished 10 epochs, current loss: 5.802193641662598\r\nFinished 20 epochs, current loss: 5.773608207702637\r\nFinished 30 epochs, current loss: 5.7503509521484375\r\nFinished 40 epochs, current loss: 5.731655120849609\r\nFinished 50 epochs, current loss: 5.665677070617676\r\nFinished 60 epochs, current loss: 5.549953460693359\r\nFinished 70 epochs, current loss: 3.9862852096557617\r\nFinished 80 epochs, current loss: 3.8360049724578857\r\nFinished 90 epochs, current loss: 3.7696609497070312\r\nFinished 100 epochs, current loss: 3.7227823734283447\r\n2021-11-15 22:09:14.922369: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:absl:Importing a function (__inference_internal_grad_fn_421422) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_internal_grad_fn_421450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\n2021-11-15 22:09:15.342590: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\r\n2021-11-15 22:09:15.342614: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\r\n2021-11-15 22:09:15.342618: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\r\n2021-11-15 22:09:15.343225: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model\r\n2021-11-15 22:09:15.344752: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\r\n2021-11-15 22:09:15.344766: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: saved_model\r\n2021-11-15 22:09:15.354429: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\r\n2021-11-15 22:09:15.376079: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: saved_model\r\n2021-11-15 22:09:15.393471: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 50247 microseconds.\r\n2021-11-15 22:09:15.416095: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2021-11-15 22:09:15.478737: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1891] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\r\nFlex ops: FlexBroadcastGradientArgs, FlexReluGrad, FlexRestore, FlexSave\r\nDetails:\r\n        tf.BroadcastGradientArgs(tensor<2xi32>, tensor<2xi32>) -> (tensor<?xi32>, tensor<?xi32>) : {device = \"\"}\r\n        tf.ReluGrad(tensor<?x128xf32>, tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = \"\"}\r\n        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<10xf32>) : {device = \"\", preferred_shard = -1 : i64}\r\n        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<128x10xf32>) : {device = \"\", preferred_shard = -1 : i64}\r\n        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<128xf32>) : {device = \"\", preferred_shard = -1 : i64}\r\n        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<784x128xf32>) : {device = \"\", preferred_shard = -1 : i64}\r\n        tf.Save(tensor<!tf_type.string>, tensor<4x!tf_type.string>, tensor<784x128xf32>, tensor<128xf32>, tensor<128x10xf32>, tensor<10xf32>) -> () : {device = \"\"}\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nWARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2021-11-15 22:09:15.503954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:09:15.504198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:09:15.504370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:09:15.504576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:09:15.504741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-11-15 22:09:15.504886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21492 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 17 nodes with 0 partitions.\r\n\r\nSegmentation fault (core dumped)\r\n```\r\n**4. Question**\r\nIs it currently possible to do On-Device Training using the GPU?", "comments": ["@jvishnuvardhan Was able to reproduce the issue on colab with  TF [v2.7.0](https://colab.research.google.com/gist/sushreebarsa/456c5c73c6989963151604a536b43e8d/overview.ipynb) and [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/2037ce20b7ef4a7946acf8e7f1885808/untitled493.ipynb),Please find the attached gists for reference.Thank you!", "Select TF ops delegate cannot be run on GPU. And for TFLite generally, GPU means the mobile GPU on the phone.\r\nOn workstation, please use CPU only.", "@thaink  Thank you for reply. I understand. I'll close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53066\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53066\">No</a>\n"]}, {"number": 53064, "title": "TF2 does not load optimizer weights when restoring a saved model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, very little\r\n- OS Platform and Distribution: Ubuntu 20.04 + Colab\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.7.0 (also happens on 2.6 and 2.5)\r\n- Python version: 3.8.10\r\n\r\n**Describe the current behavior**\r\nWhen loading a saved (Keras) model, the optimizer weights are not restored. No error or warning is printed to make the user aware of this. [The documentation](https://www.tensorflow.org/guide/keras/save_and_serialize) also does not mention this. \r\n\r\n**Describe the expected behavior**\r\nThe optimizer weights should be restored automatically when loaded a saved model that was saved with its optimizer.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n- [Notebook that shows the issue in TF2](https://colab.research.google.com/drive/1WyRoHFnNvoFocqm7jKC1CiH0KzzKh2lk?usp=sharing)\r\n- [Notebook that shows that TF1 does not have the same issue](https://colab.research.google.com/drive/1HDcX9wauIuArJpBlTmfDRlk0Yhr3ngQT?usp=sharing)\r\n\r\n**Other info / logs** \r\nThis is a TF2 bug, it does not occur when using TF1 compat mode. It is a very significant bug - it means that models cannot be trained correctly if we want the training to be able to stop and resume. Workarounds such as setting the optimizer weights manually are buggy too - see https://github.com/keras-team/keras/issues/15298.\r\n\r\nIssue https://github.com/tensorflow/tensorflow/issues/52346 is related, but not the same - that one is about using checkpoints and getting errors, while here I am using the SavedModel format and not getting any errors (where there should be one).\r\n", "comments": ["@Saduf2019 ,\r\nI was able to reproduce the issue in tf [v2.5](https://colab.research.google.com/gist/tilakrayal/adfb6c1a41fabc878c649a9ff0380cd4/2-5.ipynb), [v2.7](https://colab.research.google.com/gist/tilakrayal/b3fe36096bf7c6de3962e66118cfcedf/2-7.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/822e39371a5f040a23f89154f287c7f9/nightly.ipynb).Please find the gist [1](https://colab.research.google.com/gist/tilakrayal/09f30740c3e521e9a03bba1c54b15141/2-5-tf1_optimizer_issue.ipynb), [2](https://colab.research.google.com/gist/tilakrayal/b23fced25b7cd6a7b615f8cfc9ecab47/2-7-tf1_optimizer_issue.ipynb) and [3](https://colab.research.google.com/gist/tilakrayal/250246bbf45194f4311b71885539eb85/nightly-tf1_optimizer_issue.ipynb) here.", "`Keras Savedmodel `does not save the optimizer's weight, instead it saves the optimizer's state to continue the training from where it is left off. Refer [this](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model#usage) documentation on what does the saved model contains.\r\nAlso, you can see the optimizer config in the loaded model by `loaded.optimizer.get_config()`.\r\n", "@sachinprasadhs The docs you linked state:\r\n> The SavedModel and HDF5 file contains:\r\n>   - the model's configuration (topology)\r\n>   - the model's weights\r\n>   - the model's optimizer's state (if any)\r\n>\r\n> Thus models can be reinstantiated in the exact same state, without any of the code used for model definition or training.\r\n\r\nI would argue that the optimizer's weights are its state. This is further corroborated by the last sentence - we can reinstantiate models in the exact same state. We cannot reach the exact same state if the optimizer's weights are not restored. \r\nFurthermore, the `model.save(include_optimizer=True)` function does actually save the optimizer weights. If you compare the size of two saved models, one with SGD and the other with Adam, you'll see that the one with Adam is 3x larger due to the extra optimizer weights.\r\n\r\nAs for `optimizer.get_config()`, its docs state:\r\n> The same optimizer can be reinstantiated later (without any saved state) from this configuration.\r\n\r\nThis method only returns the optimizer's hyperparameters, which as the docs imply are not its state.", "Thanks for the details.\r\nSince, development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo, would be happy to help there.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "@sachinprasadhs Thank you for the link. I actually just found a similar issue already being discussed in the Keras repo: https://github.com/keras-team/keras/issues/15512. I will move the discussion there.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53064\">No</a>\n"]}, {"number": 53061, "title": "can not deploy docker serving, it fails", "body": "I have following saved_model dir structure. The saved_model_cli is working correctly with the path but docker command not. Note this is following the book example and apparently docker part of example not working.\r\ntest.npy test input feeds 3 instances of mnist 28x28 images to serving.\r\n\r\nDIR STRUCT:\r\n\r\nroot@nonroot-Standard-PC-i440FX-PIIX-1996:~/dev-learn/gpu/tflow/tensorflow/tflow-2nded# tree p297\r\np297\r\n\u251c\u2500\u2500 0001\r\n\u2502   \u251c\u2500\u2500 assets\r\n\u2502   \u251c\u2500\u2500 saved_model.pb\r\n\u2502   \u2514\u2500\u2500 variables\r\n\u2502       \u251c\u2500\u2500 variables.data-00000-of-00001\r\n\u2502       \u2514\u2500\u2500 variables.index\r\n\u251c\u2500\u2500 assets\r\n\u251c\u2500\u2500 keras_metadata.pb\r\n\u251c\u2500\u2500 saved_model.pb\r\n\u2514\u2500\u2500 variables\r\n    \u251c\u2500\u2500 variables.data-00000-of-00001\r\n    \u2514\u2500\u2500 variables.index\r\nCLI:\r\n\r\n    saved_model_cli run --dir p297/0001 --tag_set serve --signature_def serving_default --inputs \r\nflatten_input=test.npy\r\noutput of cli (OK)\r\n\r\n2021-11-08 15:57:11.458910: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-08 15:57:11.460906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31740 MB memory:  -> device: 0, name: Device 738c, pci bus id: 0000:00:07.0\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py:445: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nINFO:tensorflow:Restoring parameters from p297/0001/variables/variables\r\n2021-11-08 15:57:11.961912: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\r\n2021-11-08 15:57:12.671939: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\r\n2021-11-08 15:57:12.686081: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\r\nResult for output key dense_2:\r\n[[3.65022061e-05 2.47262960e-05 6.37578269e-05 2.08125566e-05\r\n  7.02261750e-05 1.18829332e-01 4.30839646e-05 2.72816449e-01\r\n  1.48763061e-02 5.93046188e-01 6.37045162e-07 3.02645799e-06\r\n  3.80635288e-06 4.22269186e-05 6.12226586e-06 4.33605646e-06\r\n  7.58367719e-07 3.06199559e-06 5.42078442e-06 2.38417056e-06\r\n  4.93106018e-06 7.25027712e-06 1.24132812e-05 1.24427579e-05\r\n  1.15528803e-06 4.87520847e-05 1.68714314e-06 8.28819338e-07\r\n  2.23448342e-06 9.11506140e-06]\r\n [1.64286757e-04 3.39760754e-06 9.66621459e-01 1.61247503e-04\r\n  1.02249524e-02 7.44288286e-07 2.27110237e-02 3.17756710e-10\r\n  1.12835020e-04 3.98790796e-08 4.89366492e-10 1.29820976e-09\r\n  9.63464334e-13 5.25168797e-10 9.53418247e-11 4.82374418e-10\r\n  2.60897762e-11 2.75338996e-12 3.59164387e-09 7.25419169e-11\r\n  5.41757861e-10 8.33503266e-10 1.25494719e-11 4.14474233e-09\r\n  3.54530544e-10 7.37128275e-11 4.25408209e-10 7.22836443e-11\r\n  6.95292546e-10 2.50320233e-11]\r\n [1.12954825e-04 9.99067128e-01 5.11940962e-05 5.15281979e-04\r\n  2.25681084e-04 7.90114484e-07 2.39875553e-05 1.08763277e-06\r\n  6.77304740e-07 8.83030324e-08 3.44932758e-08 3.36465895e-08\r\n  1.49800372e-09 2.67271858e-08 7.71939810e-08 1.00484840e-07\r\n  5.05017761e-09 4.95800823e-09 1.20519260e-07 1.41827059e-07\r\n  1.35087987e-07 3.95625591e-07 4.27236913e-08 4.71499533e-08\r\n  3.37586954e-08 1.78841599e-08 1.00948716e-08 4.40149028e-10\r\n  2.45303022e-09 2.79508638e-09]]\r\nDOCKER:\r\n\r\n    MODEL_NAME=p297\r\n    docker pull tensorflow/serving\r\n    docker run -it --rm -p 8500:8500 -p 8501:8501 \\\r\n        -v \"$MODEL_NAME:/models/$MODEL_NAME\" \\\r\n        -e MODEL_NAME=$MODEL_NAME \\\r\n        tensorflow/serving\r\noutput of docker instance (fail):\r\n\r\nUsing default tag: latest\r\nlatest: Pulling from tensorflow/serving\r\nDigest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\r\nStatus: Image is up to date for tensorflow/serving:latest\r\ndocker.io/tensorflow/serving:latest\r\n2021-11-09 00:01:43.627821: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config:  model_name: p297 model_base_path: /models/p297\r\n2021-11-09 00:01:43.628201: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\r\n2021-11-09 00:01:43.628227: I tensorflow_serving/model_servers/server_core.cc:591]  (Re-)adding model: p297\r\n2021-11-09 00:01:43.629366: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?\r\n2021-11-09 00:01:44.629551: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?\r\n2021-11-09 00:01:45.629769: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?\r\n^X2021-11-09 00:01:46.629997: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?\r\nI logged onto container using\r\ndocker-exec -it <C_ID> /bin/bash\r\nand inspected p297/0001 folder but turns out to be empty.", "comments": ["@gggh000 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbunut1804\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- binary \r\n- TensorFlow version (use command below):\r\n- 2.6\r\n- Python version:\r\n- 3.6\r\n- Bazel version (if compiling from source):\r\n- n/a\r\n- GCC/Compiler version (if compiling from source):\r\n- n/a\r\n- CUDA/cuDNN version:\r\n- n/a\r\n- GPU model and memory:\r\nMI25/MI100 or similar \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nSee above\r\n**Describe the expected behavior**\r\nCan deploy tensor serving on docker. \r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nExecute the code on next post (will save the model files), then execute following bash commands to deploy serving:\r\n```\r\nMODEL_NAME=p297\r\ndocker pull tensorflow/serving\r\ndocker run -it --rm -p 8500:8500 -p 8501:8501 \\\r\n    -v \"$MODEL_NAME:/models/$MODEL_NAME\" \\\r\n    -e MODEL_NAME=$MODEL_NAME \\\r\n    tensorflow/serving\r\n\r\n```\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nUsing default tag: latest\r\nlatest: Pulling from tensorflow/serving\r\nDigest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\r\nStatus: Image is up to date for tensorflow/serving:latest\r\ndocker.io/tensorflow/serving:latest\r\n2021-11-09 00:01:43.627821: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config: model_name: p297 model_base_path: /models/p297\r\n2021-11-09 00:01:43.628201: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\r\n2021-11-09 00:01:43.628227: I tensorflow_serving/model_servers/server_core.cc:591] (Re-)adding model: p297\r\n2021-11-09 00:01:43.629366: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?\r\n2021-11-09 00:01:44.629551: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?\r\n2021-11-09 00:01:45.629769: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?\r\n^X2021-11-09 00:01:46.629997: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?\r\nI logged onto container using\r\ndocker-exec -it <C_ID> /bin/bash\r\nand inspected p297/0001 folder but turns out to be empty.\r\n```", "```\r\n# Using neural net to do a classification task.\r\n\r\nimport tensorflow as tf\r\nimport pandas as pd \r\nimport matplotlib as plt\r\nimport sys \r\nimport time\r\nimport re\r\nimport os\r\nimport numpy as np\r\nimport helper\r\nfrom tensorflow import keras\r\n\r\nDEBUG=0\r\nCONFIG_ENABLE_PLOT=0\r\nCONFIG_EPOCHS=30\r\nCONFIG_BATCH_SIZE=32\r\nCONFIG_SAVE_MODEL=1\r\n\r\nCONFIG_SAVE_MODEL_MODE_KERAS=0\r\nCONFIG_SAVE_MODEL_MODE_KERAS_H5=1\r\n# this fill save with directory structure compatible with tf serving: p672.sh\r\nCONFIG_SAVE_MODEL_MODE_SAVED_MODEL=2\r\nCONFIG_SAVE_MODEL_MODE_CHECKPOINT=3\r\nCONFIG_SAVE_MODEL_MODE=CONFIG_SAVE_MODEL_MODE_SAVED_MODEL\r\n\r\nfor i in sys.argv:\r\n    print(\"Processing \", i)\r\n    try:\r\n        if re.search(\"epochs=\", i):\r\n            CONFIG_EPOCHS=int(i.split('=')[1])\r\n\r\n        if re.search(\"batch_size=\", i):\r\n            CONFIG_BATCH_SIZE=int(i.split('=')[1])\r\n\r\n    except Exception as msg:\r\n        print(\"No argument provided, default values will be used.\")\r\n\r\nprint(\"epochs: \", CONFIG_EPOCHS)\r\nprint(\"batch_size: \", CONFIG_BATCH_SIZE)\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\r\nprint(\"X_train_full/y_train_full/X_test/y_test: \", X_train_full.shape, y_train_full.shape, X_test.shape, y_test.shape)\r\n\r\nSEPARATOR=10000\r\nX_valid, X_train = X_train_full[:SEPARATOR] / 255.0, X_train_full[SEPARATOR:]/255.0\r\ny_valid, y_train = y_train_full[:SEPARATOR], y_train_full[SEPARATOR:]\r\nX_test = X_test / 255.0\r\n\r\nprint(\"X_valid/X_train/y_valid/y_train: \", X_valid.shape, X_train.shape, y_valid.shape, y_train.shape)\r\n\r\nclass_names = [\"T-shirt/top\",\"Trouser\", \"Pullover\", \"Dress\", \"Coat\" , \"Sandal\", \"Shirt\", \"Sneaker\",\"Bad\",\"Ankle boot\"]\r\n\r\nmodel=keras.models.Sequential()\r\nmodel.add(keras.layers.Flatten(input_shape = [28, 28]))\r\nmodel.add(keras.layers.Dense(300, activation=\"relu\"))\r\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\r\nmodel.add(keras.layers.Dense(30, activation=\"softmax\"))\r\n\r\nprint(\"model summary: \", model.summary())\r\n\r\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\r\nhistory=model.fit(X_train, y_train, epochs=CONFIG_EPOCHS, batch_size=CONFIG_BATCH_SIZE, validation_data=(X_valid, y_valid))\r\n\r\nif CONFIG_ENABLE_PLOT:\r\n    pd.DataFrame(history.history).plot(figsize=(8, 5))\r\n    plt.pyplot.grid(True)\r\n    plt.pyplot.gca().set_ylim(0, 1)\r\n    plt.pyplot.show()\r\n\r\nmodel.evaluate(X_test, y_test)\r\n\r\nif DEBUG:\r\n    print(\"model layers: \", model.layers)\r\n\r\nweights, biases  = model.layers[1].get_weights()\r\n\r\nif DEBUG:\r\n    print(\"weights, biases (shapes): \", weights, biases, weights.shape, biases.shape) \r\n\r\nif CONFIG_SAVE_MODEL:\r\n    print(\"CONFIG_SAVE_MODEL_MODE: \", CONFIG_SAVE_MODEL_MODE)\r\n    if CONFIG_SAVE_MODEL_MODE == CONFIG_SAVE_MODEL_MODE_KERAS:\r\n        print(\"CONFIG_SAVE_MODEL_MODE_KERAS...\")\r\n        model.save(\"p297.h5\")\r\n    elif CONFIG_SAVE_MODEL_MODE == CONFIG_SAVE_MODEL_MODE_SAVED_MODEL:\r\n        print(\"CONFIG_SAVE_MODEL_SAVED_MODEL...\")\r\n        model_version=\"0001\"\r\n        model_name=\"p297\"\r\n        model_path=os.path.join(model_name, model_version)\r\n        tf.saved_model.save(model, model_path)\r\n    elif CONFIG_SAVE_MODEL_MODE == CONFIG_SAVE_MODEL_MODE_KERAS_H5:\r\n        model.save(\"p297\")\r\n    elif CONFIG_SAVE_MODEL_MODE == CONFIG_SAVE_MODEL_CHECKPOINT:\r\n        print(\"Checkpoint: Unimplemented...\")\r\n    else:\r\n        print(\"Unknown option: \", CONFIG_SAVE_MODEL_MODE)\r\nelse:\r\n    print(\"Saving model is not enabled. Not saving...\")\r\nX_new = X_test[:3]\r\nprint(\"X_new type: \", type(X_new))\r\nnp.save(\"test.npy\", X_new)\r\nprint(\"X_new shape: \", X_new.shape)\r\ny_proba = model.predict(X_new)\r\nprint(\"y_proba (predict)(value): \", y_proba.round(2), \"\\ny_proba(shape)\", np.array(y_proba).shape)\r\n\r\n# Deprecated with TF1.X: y_pred = model.predict_classes(X_new)\r\n\r\ny_pred = np.argmax(y_proba,axis=1)\r\n\r\nprint(\"y_pred (predict_classes): \", y_pred)\r\nprint(\"y_test: \", y_test[:3])\r\n```", "@gggh000  I was able to run the code successfully on colab using TF **v2.7.0** ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4ceb8937f3c721db29cea0709be6f9ad/53061.ipynb).Could you please refer to the[ link](https://www.tensorflow.org/tfx/serving/docker) ? This issue is more related to `TF serving`  ,for further queries please post the issue in [TF serving ](https://github.com/tensorflow/serving/issues)repo. Thank you!\r\n\r\n", "@gggh000 We see that you have opened the same issue on TF [serving](https://github.com/tensorflow/serving/issues) repo , so could you please move this ticket to closed status  as you will get the right help there ? We can track this [issue](https://github.com/tensorflow/serving/issues/1942)  . Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53061\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53061\">No</a>\n"]}, {"number": 53060, "title": "AlreadyExistsError: File system for s3 already registered", "body": "**System information**\r\n- OS Platform and Distribution ): windows 10 \r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.50\r\n- Python version: 3.8\r\n- Installed using: Conda\r\n\r\n**Describe the problem**\r\n\r\n** Not able to verify the scripts model_builder_tf2_test.py **\r\n\r\nVERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\r\n!python {VERIFICATION_SCRIPT}\r\n\r\n\r\n**tensorflow.python.framework.errors_impl.AlreadyExistsError: File system for s3 already registeredr**\r\n\r\n2021-11-15 00:16:54.612239: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-11-15 00:16:54.636703: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"Tensorflow\\models\\research\\object_detection\\builders\\model_builder_tf2_test.py\", line 25, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"C:\\Users\\Sanjay\\Anaconda3\\envs\\tensorflowEnv\\lib\\site-packages\\object_detection-0.1-py3.8.egg\\object_detection\\builders\\model_builder.py\", line 37, in <module>\r\n    from object_detection.meta_architectures import deepmac_meta_arch\r\n  File \"C:\\Users\\sanjay\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection-0.1-py3.8.egg\\object_detection\\meta_architectures\\deepmac_meta_arch.py\", line 28, in <module>\r\n    import tensorflow_io as tfio  # pylint:disable=g-import-not-at-top\r\n  File \"C:\\Users\\sanjay\\Anaconda3\\envs\\tensorflowEnv\\lib\\site-packages\\tensorflow_io-0.22.0-py3.8-win-amd64.egg\\tensorflow_io\\__init__.py\", line 17, in <module>\r\n    from tensorflow_io.python.api import *  # pylint: disable=wildcard-import\r\n  File \"C:\\Users\\sanjay\\Anaconda3\\envs\\tensorflowEnv\\lib\\site-packages\\tensorflow_io-0.22.0-py3.8-win-amd64.egg\\tensorflow_io\\python\\api\\__init__.py\", line 19, in <module>\r\n    from tensorflow_io.python.ops.io_dataset import IODataset\r\n  File \"C:\\Users\\sanjay\\Anaconda3\\envs\\tensorflowEnv\\lib\\site-packages\\tensorflow_io-0.22.0-py3.8-win-amd64.egg\\tensorflow_io\\python\\ops\\__init__.py\", line 96, in <module>\r\n    plugin_ops = _load_library(\"libtensorflow_io_plugins.so\", \"fs\")\r\n  File \"C:\\Users\\sanjay\\Anaconda3\\envs\\tensorflowEnv\\lib\\site-packages\\tensorflow_io-0.22.0-py3.8-win-amd64.egg\\tensorflow_io\\python\\ops\\__init__.py\", line 64, in _load_library\r\n    l = load_fn(f)\r\n  File \"C:\\Users\\sanjay\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_io-0.22.0-py3.8-win-amd64.egg\\tensorflow_io\\python\\ops\\__init__.py\", line 56, in <lambda>\r\n    load_fn = lambda f: tf.experimental.register_filesystem_plugin(f) is None\r\n  File \"C:\\Users\\sanjay\\Anaconda3\\envs\\tensorflowEnv\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 218, in register_filesystem_plugin\r\n    py_tf.TF_RegisterFilesystemPlugin(plugin_location)\r\n**tensorflow.python.framework.errors_impl.AlreadyExistsError: File system for s3 already registered**\r\n", "comments": ["@Sanjaygowda66 \r\nCould you please try with cuda 11.0 and cudnn 8.0 as i see you have the error.\r\nplease refer to similar isues:[link](https://github.com/tensorflow/tensorflow/issues/48868#issuecomment-841396124),#43193,#50819,#44381.#44291", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53060\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53060\">No</a>\n", "> @Sanjaygowda66 Could you please try with cuda 11.0 and cudnn 8.0 as i see you have the error. please refer to similar isues:[link](https://github.com/tensorflow/tensorflow/issues/48868#issuecomment-841396124),#43193,#50819,#44381.#44291\r\n\r\nI don't have GPU, I'm just loading pre-trained model checkpoints.\r\n\r\n**I ran the below-mentioned code twice after restarting kernel on jupyter notebook** Since then I'm getting S3 already registered error whenever I ran verifying **model_builder_tf2_test.py** script \r\n\r\nAnd this is how I installed Tensorflow Object detection \r\n```\r\nif os.name=='nt':\r\n    url=\"https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip\"\r\n    wget.download(url)\r\n    !move protoc-3.15.6-win64.zip {paths['PROTOC_PATH']}\r\n    !cd {paths['PROTOC_PATH']} && tar -xf protoc-3.15.6-win64.zip\r\n    os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(paths['PROTOC_PATH'], 'bin'))   \r\n    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && copy object_detection\\\\packages\\\\tf2\\\\setup.py setup.py && python setup.py build && python setup.py install\r\n    !cd Tensorflow/models/research/slim && pip install -e . \r\n```", "I have similar problem.\r\nOS Platform and Distribution ): windows 10\r\nTensorFlow version: TensorFlow-gpu=2.4.1\r\nPython version: 3.8\r\nInstalled using: Conda\r\ncudatoolkit = 11.0.221\r\ncuDNN = 8.0.4 for CUDA 11.0\r\n\r\npython object_detection/builders/model_builder_tf2_test.py\r\n2021-11-16 21:55:05.642660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_tf2_test.py\", line 25, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"D:\\tf_train\\models\\research\\object_detection\\builders\\model_builder.py\", line 37, in <module>\r\n    from object_detection.meta_architectures import deepmac_meta_arch\r\n  File \"D:\\tf_train\\models\\research\\object_detection\\meta_architectures\\deepmac_meta_arch.py\", line 28, in <module>\r\n    import tensorflow_io as tfio  # pylint:disable=g-import-not-at-top\r\n  File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io\\__init__.py\", line 17, in <module>\r\n    from tensorflow_io.python.api import *  # pylint: disable=wildcard-import\r\n  File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io\\python\\api\\__init__.py\", line 19, in <module>\r\n    from tensorflow_io.python.ops.io_dataset import IODataset\r\n  File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io\\python\\ops\\__init__.py\", line 96, in <module>\r\n    plugin_ops = _load_library(\"libtensorflow_io_plugins.so\", \"fs\")\r\n  File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io\\python\\ops\\__init__.py\", line 64, in _load_library\r\n    l = load_fn(f)\r\n  File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io\\python\\ops\\__init__.py\", line 56, in <lambda>\r\n    load_fn = lambda f: tf.experimental.register_filesystem_plugin(f) is None\r\n  File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 178, in register_filesystem_plugin\r\n    py_tf.TF_RegisterFilesystemPlugin(plugin_location)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: File system for s3 already registered", "@Sanjaygowda66 ,\r\nCan you please try to test your code in latest stable v2.7 and let us know if the issue still persists.Please check this link for tested build [configurations](https://www.tensorflow.org/install/source_windows).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @Sanjaygowda66 , Can you please try to test your code in latest stable v2.7 and let us know if the issue still persists.Please check this link for tested build [configurations](https://www.tensorflow.org/install/source_windows).\r\n\r\nThanks for your help, It's working completely fine with v2.7 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53060\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53060\">No</a>\n", "> > @Sanjaygowda66 , Can you please try to test your code in latest stable v2.7 and let us know if the issue still persists.Please check this link for tested build [configurations](https://www.tensorflow.org/install/source_windows).\r\n> \r\n> Thanks for your help, It's working completely fine with v2.7\r\n\r\nHow did you upgrade it?\r\n I tried doing !pip install --upgrade tensorflow==2.7.0 but its showing version upto 2.6.2 only.\r\nERROR: No matching distribution found for tensorflow==2.7.0\r\n\r\nCurrent specifications:\r\ntensorflow              2.5.0.dev2021032900\r\nPython                  3.6.5 :: Anaconda\r\nipykernel               5.5.6\r\nprotobuf                3.19.1\r\ntensorflow-addons       0.12.1\r\ntf-estimator-nightly       2.5.0.dev2021032501\r\npip                                 21.3.1\r\nKeras-Preprocessing           1.1.2\r\njupyter-client          7.1.0\r\njupyter-core            4.9.1\r\nCython                  3.0a6", "> > > @Sanjaygowda66 , Can you please try to test your code in latest stable v2.7 and let us know if the issue still persists.Please check this link for tested build [configurations](https://www.tensorflow.org/install/source_windows).\r\n> > \r\n> > \r\n> > Thanks for your help, It's working completely fine with v2.7\r\n> \r\n> How did you upgrade it? I tried doing !pip install --upgrade tensorflow==2.7.0 but its showing version upto 2.6.2 only. ERROR: No matching distribution found for tensorflow==2.7.0\r\n> \r\n> Current specifications: tensorflow 2.5.0.dev2021032900 Python 3.6.5 :: Anaconda ipykernel 5.5.6 protobuf 3.19.1 tensorflow-addons 0.12.1 tf-estimator-nightly 2.5.0.dev2021032501 pip 21.3.1 Keras-Preprocessing 1.1.2 jupyter-client 7.1.0 jupyter-core 4.9.1 Cython 3.0a6\r\n\r\nJust use `!pip install --upgrade tensorflow`, it will automatically upgrade to a newer version. ", "> \r\n\r\nI tried that but it is upgrading to 2.6.2 not 2.7\r\n", "> > \r\n> \r\n> I tried that but it is upgrading to 2.6.2 not 2.7\r\n\r\nMaybe you should upgrade your python >3.7, mine was 3.9 or try reinstalling TensorFlow. ", "> > > \r\n> > \r\n> > \r\n> > I tried that but it is upgrading to 2.6.2 not 2.7\r\n> \r\n> Maybe you should upgrade your python >3.7, mine was 3.9 or try reinstalling TensorFlow.\r\n\r\nOk. ", "> > > > \r\n> > > \r\n> > > \r\n> > > I tried that but it is upgrading to 2.6.2 not 2.7\r\n> > \r\n> > \r\n> > Maybe you should upgrade your python >3.7, mine was 3.9 or try reinstalling TensorFlow.\r\n> \r\n> Ok.\r\n\r\n[System requirements\r\nPython 3.7\u20133.9\r\nPython 3.9 support requires TensorFlow 2.5 or later.\r\nPython 3.8 support requires TensorFlow 2.2 or later.](https://www.tensorflow.org/install/pip)", "> I have similar problem. OS Platform and Distribution ): windows 10 TensorFlow version: TensorFlow-gpu=2.4.1 Python version: 3.8 Installed using: Conda cudatoolkit = 11.0.221 cuDNN = 8.0.4 for CUDA 11.0\r\n> \r\n> python object_detection/builders/model_builder_tf2_test.py 2021-11-16 21:55:05.642660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll Traceback (most recent call last): File \"object_detection/builders/model_builder_tf2_test.py\", line 25, in from object_detection.builders import model_builder File \"D:\\tf_train\\models\\research\\object_detection\\builders\\model_builder.py\", line 37, in from object_detection.meta_architectures import deepmac_meta_arch File \"D:\\tf_train\\models\\research\\object_detection\\meta_architectures\\deepmac_meta_arch.py\", line 28, in import tensorflow_io as tfio # pylint:disable=g-import-not-at-top File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io__init__.py\", line 17, in from tensorflow_io.python.api import * # pylint: disable=wildcard-import File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io\\python\\api__init__.py\", line 19, in from tensorflow_io.python.ops.io_dataset import IODataset File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io\\python\\ops__init__.py\", line 96, in plugin_ops = _load_library(\"libtensorflow_io_plugins.so\", \"fs\") File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io\\python\\ops__init__.py\", line 64, in _load_library l = load_fn(f) File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_io\\python\\ops__init__.py\", line 56, in load_fn = lambda f: tf.experimental.register_filesystem_plugin(f) is None File \"C:\\Users\\jim\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 178, in register_filesystem_plugin py_tf.TF_RegisterFilesystemPlugin(plugin_location) tensorflow.python.framework.errors_impl.AlreadyExistsError: File system for s3 already registered\r\n\r\nuse gpu==2.5.1 i think it works."]}, {"number": 53059, "title": "Implement `compute_output_shape` for attention layers", "body": "Resolves #53058", "comments": ["It looks like your PR relates to the Keras component. Please submit it to the github.com/keras-team/keras repository instead. Thankyou.\r\n@fchollet, @qlzh727", "@gbaned thank you for explanation and sorry for inconvenience. Will move PR to the keras repository. "]}, {"number": 53058, "title": "Move parameter `return_attention_weights` of `BaseAttentionLayer.call` to the constructor ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.6\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently the method compute_output_shape is not implemented for layers derived from `BaseAttentionLayer`. Hence, it's impossible to use these layers with, for example, `TimeDistributed` wrapper (see [issue](https://github.com/keras-team/keras/issues/15515) Attention module not working with TimeDistributed layer).\r\nTo fix this, one could use eager mode or implement some workaround. Another way is to implement `compute_output_shape` method for `BaseAttentionLayer`. But `BaseAttentionLayer.call` method has a parameter `return_attention_weights` that adds attention weights tensor to values returned by attention layer, if set. It could change output shape of the layer after build.\r\nMoreover, parameter `return_attention_weights` does not change a state of the layer when provided and is not saved to the object's attributes. So, it's impossible to implement `compute_output_shape` method while `return_attention_weights` belongs to the parameters of the call method. \r\n\r\nMy suggestion is to change the API like this:\r\n\r\nfrom \r\n```python3\r\nclass BaseDenseAttention(Layer):\r\n    def __init__(self, causal=False, dropout=0.0,\r\n                 **kwargs):\r\n      ...\r\n    \r\n    def call(self,\r\n             inputs,\r\n             mask=None,\r\n             training=None,\r\n             return_attention_scores=False):\r\n      ...\r\n```\r\n\r\nto\r\n\r\n```python3\r\nclass BaseDenseAttention(Layer):\r\n    def __init__(self, causal=False, dropout=0.0, return_attention_scores=False),  # move the parameter to the constructor\r\n                 **kwargs):\r\n      self.return_attention_scores=return_attention_scores\r\n      ...\r\n    \r\n    def call(self, \r\n             inputs,\r\n             mask=None,\r\n             training=None,\r\n             return_attention_scores=False):   # <- make it deprecated and add deprecation warning, remove in future versions\r\n      ...\r\n\r\n  def compute_output_shape(self, input_shape):\r\n      # this function returns output shape using self.return_attention_scores as a conditional \r\n      ...\r\n\r\nThese changes make computation of output shape possible, but keep the possibility to return attention weights, if it's necessary for model debugging and/or analysis. For most use cases, it's enough to place `return_attention_scores` in a constructor of a class. Likewise, in other keras layers the parameters that change output shape of the layer are placed in a constructor of the layer (for example, `return_sequences` in recurrent layers). \r\n```\r\n\r\nRelated issues:\r\n- [Unable to create](https://github.com/keras-team/keras/issues/15515) TimeDistributed wrapper for Attention layers\r\n- [Feature request](https://github.com/tensorflow/tensorflow/issues/44127) for the parameter `return_attention_weights` to the `call` method. \r\n\r\n**Will this change the current api? How?**\r\nParameter return_attention_weights will be moved from call method of BaseAttentionLayer layers to constructor of BaseAttentionLayer. That allows to compute output shape of the attention layers and use them with, for example, TimeDistributed wrapper (that fixes previously mentioned issue).\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who would use attention layers.\r\n\r\n**Any Other info.**\r\nIt could break some of the tutorials/examples. Fixes may be required in the future: [this NMT tutorial](https://github.com/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb) uses `return_attention_scores` parameter.\r\n", "comments": ["Hi @mishc9! Thanks for the PR!\r\nPlease post this feature request on [keras-team/keras repo](https://github.com/keras-team/keras/issues) too.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "@mohantym thank you, it seem that I've missed these changes. Will move PR to the Keras repository."]}, {"number": 53057, "title": "Error during training of EfficientDet-lite0 on GPU using the code model_maker_object_detection.ipynb", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Code: [model_maker_object_detection.ipynb](https://github.com/tensorflow/tensorflow/blob/3beeeff5abbaf24562722c4cbe3af8614346286c/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab Pro Ubuntu 18.05\r\n- TensorFlow installed from (source or binary): commands in the notebook (!pip install --upgrade tensorflow==2.5.0)\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.12\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: P100-PCIE 16GB\r\n\r\n## Current behavior\r\nThe line:\r\n` \r\nmodel = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)\r\n`\r\nwill run into the following error:\r\n\r\nUnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n\t [[Func/cond_6/then/_3438/input/_6900/_104]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_96848]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n\r\n\r\n## Expected behavior\r\nIt should be able to train on GPU.\r\n\r\n\r\n## Full Output\r\n\r\nEpoch 1/50\r\n\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-5-187f39c1697e> in <module>()\r\n----> 1 model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py in create(cls, train_data, model_spec, validation_data, epochs, batch_size, train_whole_model, do_train)\r\n    285     if do_train:\r\n    286       tf.compat.v1.logging.info('Retraining the models...')\r\n--> 287       object_detector.train(train_data, validation_data, epochs, batch_size)\r\n    288     else:\r\n    289       object_detector.create_model()\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py in train(self, train_data, validation_data, epochs, batch_size)\r\n    156       return self.model_spec.train(self.model, train_ds, steps_per_epoch,\r\n    157                                    validation_ds, validation_steps, epochs,\r\n--> 158                                    batch_size, val_json_file)\r\n    159 \r\n    160   def evaluate(self,\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py in train(self, model, train_dataset, steps_per_epoch, val_dataset, validation_steps, epochs, batch_size, val_json_file)\r\n    270         callbacks=train_lib.get_callbacks(config.as_dict(), val_dataset),\r\n    271         validation_data=val_dataset,\r\n--> 272         validation_steps=validation_steps)\r\n    273     return model\r\n    274 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1181                 _r=1):\r\n   1182               callbacks.on_train_batch_begin(step)\r\n-> 1183               tmp_logs = self.train_function(iterator)\r\n   1184               if data_handler.should_sync:\r\n   1185                 context.async_wait()\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    887 \r\n    888       with OptionalXlaContext(self._jit_compile):\r\n--> 889         result = self._call(*args, **kwds)\r\n    890 \r\n    891       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    948         # Lifting succeeded, so variables are initialized and we can run the\r\n    949         # stateless function.\r\n--> 950         return self._stateless_fn(*args, **kwds)\r\n    951     else:\r\n    952       _, _, _, filtered_flat_args = \\\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   3022        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n   3023     return graph_function._call_flat(\r\n-> 3024         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   3025 \r\n   3026   @property\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1959       # No tape is watching; skip to running the function.\r\n   1960       return self._build_call_outputs(self._inference_function.call(\r\n-> 1961           ctx, args, cancellation_manager=cancellation_manager))\r\n   1962     forward_backward = self._select_forward_and_backward_functions(\r\n   1963         args,\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    594               inputs=args,\r\n    595               attrs=attrs,\r\n--> 596               ctx=ctx)\r\n    597         else:\r\n    598           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n\t [[Func/cond_6/then/_3438/input/_6900/_104]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_96848]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n", "comments": ["I just tested that this line can start training normally with tensorflow version 2.7.\r\nHaven't tested other lines' compatiblity with tensorflow 2.7 yet.", "@LittleDijkstraZ Could you please try to execute your code on latest stable TF version `2.7.0` and let us know if it is still an issue? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53057\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53057\">No</a>\n", "> @LittleDijkstraZ Could you please try to execute your code on latest stable TF version `2.7.0` and let us know if it is still an issue? Thanks!\r\n\r\nI had tested it on tensorflow 2.7.0 and there is no bug ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53057\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53057\">No</a>\n", "@LittleDijkstraZ Thank you for the quick update! \r\nGlad its working fine for you."]}, {"number": 53056, "title": "yolov5s-int8.tflite can not be run on the npu of I.MX8M Plus.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04)\r\n- TensorFlow installation (pip package or built from source): pip package \r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options: python export.py --int8 --weights yolov5s.pt --include tflite\r\n\r\n\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\nModel can be run on cpu of I.MX8M Plus, but can not be run on npu.\r\n\r\nroot@imx8mpevk:/usr/bin/tensorflow-lite-2.4.1/examples# ./benchmark_model --graph=/home/2tflite/yolov5s-int8.tflite --use_nnapi=true\r\n STARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [/home/yolov5test/yolov5s-int8.tflite]\r\nUse NNAPI: [1]\r\nNNAPI accelerators available: [vsi-npu]\r\nLoaded model /home/yolov5test/yolov5s-int8.tflite\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nWARNING: Operator RESIZE_NEAREST_NEIGHBOR (v3) refused by NNAPI delegate: NNAPI does not support half_pixel_centers == true.\r\nWARNING: Operator RESIZE_NEAREST_NEIGHBOR (v3) refused by NNAPI delegate: NNAPI does not support half_pixel_centers == true.\r\nExplicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 3 delegate kernels.\r\nThe input model file size (MB): 7.70178\r\nInitialized session in 13.48ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nERROR: NN API returned error ANEURALNETWORKS_OP_FAILED at line 4056 while running computation.\r\n\r\nERROR: Node number 284 (TfLiteNnapiDelegate) failed to invoke.\r\n\r\ncount=1 curr=13456030\r\n\t\r\nBenchmarking failed.\r\n\r\n", "comments": ["@weili1457355863 ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "Also Could you please update TensorFlow to the latest stable version v.2.7 and test your code and let us know if you are facing the same error. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53056\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53056\">No</a>\n"]}, {"number": 53055, "title": "Current implementation only supports equal length strides in the row and column dimensions.", "body": "When I use layers related  to DepthwiseConv2d operation, such as tf.keras.layers.SeparableConv2d, exception occurs like that: 'Current implementation only supports equal length strides in the row and column dimensions. [Op: DepthwiseConv2dNative]'. It means that i cannot use parameter 'strides' like [1, 2], however, that conflicts with the documentation, which allows 'strides'  with list format without mentioning that elements within the list should be the same.\r\nI'm using TensorFlow 2.0.0, and I guess this bug exisits in 3D scenariothe and further edition.\r\n![1](https://user-images.githubusercontent.com/94270103/141666163-3ba1d244-6efb-49b2-aa49-322d0b4d0889.png)", "comments": ["Hi @beginner401! \r\nCould you please fill the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "**System information**\r\n- Ubuntu 20.04.2 LTS (GNU/Linux 5.4.0-74-generic x86_64)\r\n- TensorFlow 2.0.0 installed by anaconda\r\n- gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- NVIDIA-SMI 460.80       Driver Version: 460.80       CUDA Version: 11.2\r\n**Describe the current behavior**\r\ntf.keras.layers.SeparableConv2d do not support different value for rows and columns, caused by depthwiseConv2dNative not supporting different stride value for rows and columns.\r\n**Describe the expected behavior**\r\ntf.keras.layers.SeparableConv2d and depthwiseConv2dNative obviously should support different value for rows and columns.\r\n\r\n**Standalone code to reproduce the issue**\r\ne.g.\r\nseparable_conv2d = tf.keras.layer.SeparableConv2D(32, 3, [1, 2], depth_multiplier=3)\r\n\r\n**Other info / logs**\r\n![image](https://user-images.githubusercontent.com/94270103/141972774-8169ba95-81c4-409f-aa1d-1106f76f6cf3.png)\r\n", "Hi @beginner401 ! I could not replicate this issue from above stand alone code in TF 2.0/TF 2.7 (Colab) . Could you try again in latest version? Attaching [Gist ](https://colab.research.google.com/gist/mohantym/f6aa82ab4a4af6f26927f89e5532d170/github_53055.ipynb#scrollTo=64I8XIFWietp)for reference. Thanks!", "Hello, it seems that when I upgrade my tensorflow version to the latest 2.7, similar exceptions still occurs, the error-info takes a bit changes, but the problem stay the same. Have you ever call your layer after defining it? Only defining the layer doesn't work, you should try to at least call it once, Thanks!\r\nI copy the same environment as 2.0, and just upgrade my tensorflow to the latest. By the way, my environment has installed tensorflow-probability, I suggest that makes no sense, but maybe that's valuable for you.\r\nBelow is the error info for the latest version:\r\n![QQ\u622a\u56fe20211118111138](https://user-images.githubusercontent.com/94270103/142345009-90608418-e267-4bcf-89ab-22b7ec33e543.png)\r\n", "Hi @beginner401 !\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) too.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Hi @mohantym !\r\nI have post this issue on keras-team/keras repo., and thanks for your guidance.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53055\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53055\">No</a>\n"]}, {"number": 53054, "title": "Errors in setting up GPU with CUDA and cuDNN", "body": "System information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\nTensorFlow installed from (source or binary): pip install tensorflow (in Anaconda)\r\nTensorFlow version: 2.5.0\r\nPython version: 3.8\r\nInstalled using virtualenv? pip? conda?: pip install tensorflow (in Anaconda)\r\nCUDA/cuDNN version: CUDA: cuda_11.0.3_451.82_win10 and cuDNN: cudnn-11.0-windows-x64-v8.0.5.39\r\nGPU model and memory: NVIDIA GeForce GTX 1660 Ti with Max-Q Design 6 GB\r\n\r\n\r\nDear All,\r\n\r\nI have tried several CUDA and cuDNN versions for setting up my GPU without luck. Finally, I have used\r\n\r\n**cuda_11.0.3_451.82_win10 and\r\ncudnn-11.0-windows-x64-v8.0.5.39**\r\n\r\nin Anaconda (Python 3.8.12). I see there are few positive results and 3 errors  (please see the full output below) as I executed this command:\r\n```\r\nimport tensorflow\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\n```\r\n\r\n\r\nError 1:\r\n**2021-11-13 22:15:44.176901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found**\r\n\r\nError 2:\r\n**2021-11-13 22:15:44.191130: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found**\r\n\r\nError 3:\r\n**2021-11-13 22:15:44.191174: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU.** \r\n\r\nI tried to delete EVERYTHING from previous CUDA & cuDNN installations; I wonder if I had missed something!?\r\n\r\nCould you please look into the following error messages and give me some suggestoins, please?\r\n\r\nI am using Anaconda 3 in a Windows 10 laptop (Lenovo legion 7 slim with NVIDIA GeForce GTX 1660 Ti with Max-Q Design ).\r\n\r\nTIA\r\nSheri\r\n\r\n> >>> import tensorflow\r\n> 2021-11-13 22:50:53.719861: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n> >>> from tensorflow.python.client import device_lib\r\n> >>> print(device_lib.list_local_devices())\r\n> 2021-11-13 22:51:32.529087: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2021-11-13 22:51:32.531546: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll\r\n> 2021-11-13 22:51:33.925024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\n> pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5\r\n> coreClock: 1.335GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\r\n> 2021-11-13 22:51:33.925223: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n> 2021-11-13 22:51:33.932212: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n> 2021-11-13 22:51:33.932294: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\n> 2021-11-13 22:51:33.935750: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll\r\n> 2021-11-13 22:51:33.936811: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll\r\n> 2021-11-13 22:51:33.937346: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\r\n> 2021-11-13 22:51:33.939861: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll\r\n> 2021-11-13 22:51:33.940487: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r\n> 2021-11-13 22:51:33.940591: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n> Skipping registering GPU devices...\r\n> 2021-11-13 22:51:34.007417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2021-11-13 22:51:34.007508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0\r\n> 2021-11-13 22:51:34.008197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N\r\n> [name: \"/device:CPU:0\"\r\n> device_type: \"CPU\"\r\n> memory_limit: 268435456\r\n> locality {\r\n> }\r\n> incarnation: 10668262798735033247\r\n> ]\r\n> >>>", "comments": ["@mshariful Could you please try using latest stable TF version **2.7.0** & refer to the [build from source ](https://www.tensorflow.org/install/source_windows).Please let us know if it helps? Thank you!", "Hi @sushreebarsa Thanks for your response. I will try to do that and inform you. ", "Hi @sushreebarsa ,\r\n\r\nI have gone through all the installations as described in [build from source](https://www.tensorflow.org/install/source_windows). Now after git cloning the tensorflow, as I execute the configure.py file\r\nI get repeatedly the following message; though I enter the correct paths of CUDA and cuDNN paths.\r\n\r\n> > C:\\Users\\endro\\tensorflow>python ./configure.py\r\n> You have bazel 3.7.2 installed.\r\n> Please specify the location of python. [Default is C:\\Users\\endro\\AppData\\Local\\Programs\\Python\\Python39\\python.exe]:\r\n> \r\n> \r\n> Found possible Python library paths:\r\n>   C:\\Users\\endro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\r\n> Please input the desired Python library path to use.  Default is [C:\\Users\\endro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages]\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n> CUDA support will be enabled for TensorFlow.\r\n> \r\n> Could not find any cudnn.h, cudnn_version.h matching version '' in any subdirectory:\r\n>         ''\r\n>         'include'\r\n>         'include/cuda'\r\n>         'include/*-linux-gnu'\r\n>         'extras/CUPTI/include'\r\n>         'include/cuda/CUPTI'\r\n>         'local/cuda/extras/CUPTI/include'\r\n> of:\r\n>         'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5'\r\n> \r\n> Asking for detailed CUDA configuration...\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: CUDA 11.5\r\n> \r\n> \r\n> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: cuDNN 8.3\r\n> \r\n> \r\n> Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\\include\r\n> \r\n> \r\n> Could not find any cuda.h matching version 'CUDA 11.5' in any subdirectory:\r\n>         ''\r\n>         'include'\r\n>         'include/cuda'\r\n>         'include/*-linux-gnu'\r\n>         'extras/CUPTI/include'\r\n>         'include/cuda/CUPTI'\r\n>         'local/cuda/extras/CUPTI/include'\r\n> of:\r\n>         'C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\\include'\r\n> \r\n> Asking for detailed CUDA configuration...\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: CUDA 11.5\r\n> \r\n> \r\n> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: cuDNN\r\n> \r\n> \r\n> Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\\\r\n> \r\n> \r\n> Could not find any cuda.h matching version 'CUDA 11.5' in any subdirectory:\r\n>         ''\r\n>         'include'\r\n>         'include/cuda'\r\n>         'include/*-linux-gnu'\r\n>         'extras/CUPTI/include'\r\n>         'include/cuda/CUPTI'\r\n>         'local/cuda/extras/CUPTI/include'\r\n> of:\r\n>         'C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\\'\r\n> \r\n> Asking for detailed CUDA configuration...\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: CUDA 11.5\r\n> \r\n> \r\n> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: cuDNN 8.3\r\n> \r\n> \r\n> Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\r\n> \r\n> \r\n> Could not find any cuda.h matching version 'CUDA 11.5' in any subdirectory:\r\n>         ''\r\n>         'include'\r\n>         'include/cuda'\r\n>         'include/*-linux-gnu'\r\n>         'extras/CUPTI/include'\r\n>         'include/cuda/CUPTI'\r\n>         'local/cuda/extras/CUPTI/include'\r\n> of:\r\n>         'C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5'\r\n> \r\n> Asking for detailed CUDA configuration...\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: CUDA 11.5\r\n> \r\n> \r\n> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: cuDNN 8.3\r\n> \r\n> \r\n> Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\r\n> \r\n> \r\n> Could not find any cuda.h matching version 'CUDA 11.5' in any subdirectory:\r\n>         ''\r\n>         'include'\r\n>         'include/cuda'\r\n>         'include/*-linux-gnu'\r\n>         'extras/CUPTI/include'\r\n>         'include/cuda/CUPTI'\r\n>         'local/cuda/extras/CUPTI/include'\r\n> of:\r\n>         'C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA'\r\n> \r\n> Asking for detailed CUDA configuration...\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: Traceback (most recent call last):\r\n>   File \"C:\\Users\\endro\\tensorflow\\configure.py\", line 96, in get_input\r\n>     answer = raw_input(question)\r\n> NameError: name 'raw_input' is not defined\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\endro\\tensorflow\\configure.py\", line 1475, in <module>\r\n>     main()\r\n>   File \"C:\\Users\\endro\\tensorflow\\configure.py\", line 1387, in main\r\n>     set_tf_cuda_version(environ_cp)\r\n>   File \"C:\\Users\\endro\\tensorflow\\configure.py\", line 898, in set_tf_cuda_version\r\n>     tf_cuda_version = get_from_env_or_user_or_default(environ_cp,\r\n>   File \"C:\\Users\\endro\\tensorflow\\configure.py\", line 600, in get_from_env_or_user_or_default\r\n>     var = get_input(ask_for_var)\r\n>   File \"C:\\Users\\endro\\tensorflow\\configure.py\", line 98, in get_input\r\n>     answer = input(question)  # pylint: disable=bad-builtin\r\n> KeyboardInterrupt\r\n> ^C\r\n> C:\\Users\\endro\\tensorflow>\r\n> \r\n> C:\\Users\\endro\\tensorflow>nano configure.py\r\n> \r\n> C:\\Users\\endro\\tensorflow>python ./configure.py\r\n> You have bazel 3.7.2 installed.\r\n> Please specify the location of python. [Default is C:\\Users\\endro\\AppData\\Local\\Programs\\Python\\Python39\\python.exe]:\r\n> \r\n> \r\n> Found possible Python library paths:\r\n>   C:\\Users\\endro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\r\n> Please input the desired Python library path to use.  Default is [C:\\Users\\endro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages]\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n> CUDA support will be enabled for TensorFlow.\r\n> \r\n> Could not find any cudnn.h, cudnn_version.h matching version '' in any subdirectory:\r\n>         ''\r\n>         'include'\r\n>         'include/cuda'\r\n>         'include/*-linux-gnu'\r\n>         'extras/CUPTI/include'\r\n>         'include/cuda/CUPTI'\r\n>         'local/cuda/extras/CUPTI/include'\r\n> of:\r\n>         'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5'\r\n> \r\n> Asking for detailed CUDA configuration...\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: CUDA 11.5\r\n> \r\n> \r\n> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: cuDNN 8.3\r\n> \r\n> \r\n> Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\\include, C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\\bin, C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\\lib\\x64\r\n> \r\n> \r\n> Could not find any cuda.h matching version 'CUDA 11.5' in any subdirectory:\r\n>         ''\r\n>         'include'\r\n>         'include/cuda'\r\n>         'include/*-linux-gnu'\r\n>         'extras/CUPTI/include'\r\n>         'include/cuda/CUPTI'\r\n>         'local/cuda/extras/CUPTI/include'\r\n> of:\r\n>         'C:\\ProgramData\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\\include'\r\n> \r\n> Asking for detailed CUDA configuration...\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:\r\n\r\nCould you please tell me what I should do?\r\n\r\nTIA\r\nSheri", "@mshariful Could you please have a look at the instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) from here.\r\n.Also, please follow the instructions  to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\nPlease, refer #36167, #42367,#42197,#45435\r\nCould you please refer to the common [errors](https://www.tensorflow.org/install/errors) and let us know if it helps ?\r\n\r\nIf the issue still persists please post it on [Anaconda repo ](https://github.com/ContinuumIO/anaconda-issues/issues)as we do not support conda environment.\r\nThis issue is more suitable on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues) repo since its related to TF installation with Anaconda.\r\nPlease post it on Continuum Anaconda.\r\nThank you!", "Hi @sushreebarsa Thanks for your response. I will do these steps and inform you accordingly. TIA", "@mshariful Could you please let us know if you have tried as per the [comment ](https://github.com/tensorflow/tensorflow/issues/53054#issuecomment-970084016)and if it is still an issue ? Thanks!", "\r\n\r\nHallo @sushreebarsa , thanks for your message. I have got installed tensorflow through 'Anaconda' way very easily. \r\nInstalling CUDA and cuDNN from Nvidia and setting up all the paths....it turned out to be a horror for me! What a frustrating experience it was!\r\n\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53054\">No</a>\n"]}, {"number": 53053, "title": "Error when Saving model with data augmentation layer on Tensorflow 2.7 ", "body": "I am getting an error when trying to save a model with data augmentation layers in last tensorflow version (2.7.0).\r\n\r\nHere is the code of data augmentation:\r\n\r\n\r\n\r\n> \r\n    input_shape_rgb = (img_height, img_width, 3)\r\n    data_augmentation_rgb = tf.keras.Sequential(\r\n      [ \r\n        layers.RandomFlip(\"horizontal\"),\r\n        layers.RandomFlip(\"vertical\"),\r\n        layers.RandomRotation(0.5),\r\n        layers.RandomZoom(0.5),\r\n        layers.RandomContrast(0.5),\r\n        RandomColorDistortion(name='random_contrast_brightness/none'),\r\n      ]\r\n    )\r\n\r\n\r\n\r\n\r\n\r\nNow I build my model like this:\r\n\r\n> \r\n     input_shape = (img_height, img_width, 3)\r\n\r\n    model = Sequential([\r\n    layers.Input(input_shape),\r\n    data_augmentation_rgb,\r\n    layers.Rescaling((1./255)),\r\n  \r\n    layers.Conv2D(16, kernel_size, padding=padding, activation='relu', strides=1, \r\n       data_format='channels_last'),\r\n    layers.MaxPooling2D(),\r\n    layers.BatchNormalization(),\r\n  \r\n    layers.Conv2D(32, kernel_size, padding=padding, activation='relu'), # best 4\r\n    layers.MaxPooling2D(),\r\n    layers.BatchNormalization(),\r\n  \r\n    layers.Conv2D(64, kernel_size, padding=padding, activation='relu'), # best 3\r\n    layers.MaxPooling2D(),\r\n    layers.BatchNormalization(),\r\n  \r\n    layers.Conv2D(128, kernel_size, padding=padding, activation='relu'), # best 3\r\n    layers.MaxPooling2D(),\r\n    layers.BatchNormalization(),\r\n  \r\n    layers.Flatten(),\r\n    layers.Dense(128, activation='relu'), # best 1\r\n    layers.Dropout(0.1),\r\n    layers.Dense(128, activation='relu'), # best 1\r\n    layers.Dropout(0.1),\r\n    layers.Dense(64, activation='relu'), # best 1\r\n    layers.Dropout(0.1),\r\n    layers.Dense(num_classes, activation = 'softmax')\r\n     ])\r\n  \r\n     model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=metrics)\r\n     model.summary()\r\n\r\n\r\n\r\nThen after the training is done I just make:\r\n\r\n> \r\n     model.save(\"./\")\r\n\r\nAnd I'm getting this error:\r\n\r\n> \r\n      \r\n      ---------------------------------------------------------------------------\r\n      KeyError                                  Traceback (most recent call last)\r\n      <ipython-input-84-87d3f09f8bee> in <module>()\r\n      ----> 1 model.save(\"./\")\r\n      \r\n      \r\n      /usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in \r\n       error_handler(*args, **kwargs)\r\n       65     except Exception as e:  # pylint: disable=broad-except\r\n       66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n       ---> 67       raise e.with_traceback(filtered_tb) from None\r\n       68     finally:\r\n       69       del filtered_tb\r\n      \r\n       /usr/local/lib/python3.7/dist- \r\n       packages/tensorflow/python/saved_model/function_serialization.py in \r\n       serialize_concrete_function(concrete_function, node_ids, coder)\r\n       66   except KeyError:\r\n       67     raise KeyError(\r\n       ---> 68         f\"Failed to add concrete function '{concrete_function.name}' to \r\n       object-\"\r\n       69         f\"based SavedModel as it captures tensor {capture!r} which is \r\n       unsupported\"\r\n       70         \" or not reachable from root. \"\r\n      \r\n       KeyError: \"Failed to add concrete function \r\n       'b'__inference_sequential_46_layer_call_fn_662953'' to object-based SavedModel as it \r\n       captures tensor <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> which \r\n       is unsupported or not reachable from root. One reason could be that a stateful \r\n       object or a variable that the function depends on is not assigned to an attribute of \r\n       the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\r\n\r\nI inspected the reason of getting this error by changing the architecture of my model and I just found that the reason came from the `data_augmentation` layer since the `RandomFlip` and `RandomRotation` and others are changed from `layers.experimental.prepocessing.RandomFlip` to `layers.RandomFlip`, but still the error appears.\r\n\r\n", "comments": ["@moumed ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "I am also having the same issue with data augmentation layers. Interestingly, no error raised after removing `data_augmentation` from `model`.\r\n```python\r\ndata_augmentation = tf.keras.Sequential(\r\n    [\r\n        tf.keras.layers.RandomFlip(),\r\n        tf.keras.layers.RandomContrast(.1),\r\n        tf.keras.layers.RandomRotation(.5, fill_mode='nearest'),\r\n    ]\r\n)\r\n\r\ninputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, channels))\r\nbase_model = tf.keras.applications.ResNet50V2(\r\n    input_shape=(IMG_SIZE, IMG_SIZE, channels),\r\n    include_top=False,\r\n)\r\nx = data_augmentation(inputs)\r\nx = tf.keras.applications.resnet_v2.preprocess_input(x)\r\nx = base_model(x, training=False)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dropout(dropout)(x)\r\noutputs = tf.keras.layers.Dense(1)(x)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\r\n```\r\nAs I encountered the same issue as @moumed without his complete code and dataset, I believe the issue is not specific to any dataset. Here's my `callbacks`.\r\n```python\r\ncallbacks = [\r\n    tf.keras.callbacks.TensorBoard(\r\n        log_dir=tb_dir,\r\n        histogram_freq=5,\r\n        write_graph=True,\r\n        write_images=True,\r\n    ),\r\n    tf.keras.callbacks.ModelCheckpoint(\r\n        model_dir,\r\n        monitor='val_accuracy',\r\n        save_best_only=True,\r\n    ),\r\n]\r\nmodel.fit(train_ds, epochs=100, callbacks=callbacks, validation_data=val_ds)\r\n```", "> @moumed , In order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n\r\nlike @ngchanway said the problem is not specific to any dataset, just creating and compiling the model with `data_augmentation_rgb ` and then saving the model raises the error, and when removing the data `data_augmentation_rgb ` the error disappears.  \r\n\r\nI tried to test the provided colab in tensorflow core documentation with the provided `tf_flowers`dataset: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb\r\nthe error appears when saving model with  the option 1 of Making the preprocessing layers part of the model.\r\n", "Posted a workaround [here](https://stackoverflow.com/questions/69955838/saving-model-on-tensorflow-2-7-0-with-data-augmentation-layer/69975495#69975495).", "@moumed ,\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Posted a workaround [here](https://stackoverflow.com/questions/69955838/saving-model-on-tensorflow-2-7-0-with-data-augmentation-layer/69975495#69975495).\r\n\r\ni am getting this error when trying to save with h5 format: \r\n\r\n> NotImplementedError: \r\nLayer RandomColorDistortion has arguments ['self', 'contrast_range', 'brightness_delta', 'gamma_correction', 'contrast_fact', 'crop_fact', 'variablity']\r\nin `__init__` and therefore must override `get_config()`.", "@moumed you obviously have to override the config of your custom layer and add the parameters you are using. It has nothing to the with the original issue that you posted.", "> @moumed you obviously have to override the config of your custom layer and add the parameters you are using. It has nothing to the with the original issue that you posted.\r\n\r\nHere is the error when trying to load the model: \r\n\r\n> ValueError: Unknown layer: RandomColorDistortion. Please ensure this object is passed to the `custom_objects` argument.", "@moumed \r\nPlease feel free to close this issue here as it has tracked in Keras repo.Thanks", "this issue is moved to keras repo. Here is the link: https://github.com/keras-team/keras/issues/15699", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53053\">No</a>\n"]}, {"number": 53052, "title": "Concat tensor of[None, 192] with tensor of [1,128]", "body": "How to concatenate tensors of shapes [None, 128] with tensor of [1,128]. Here the first tensor will some data of unknown length and the second tensor is fixed tensor not dependant on data size. The final output should be of shape[None, 328]. This is a part of a neural network concatenation.\r\n\r\nI tried \r\n`c = Concatenate(axis = -1, name = 'DQN_Input')([ a, b])`\r\nbut it gives error.\r\nHere a.shape = (None, 192) and b.shape = (1,128) But this does not work. The error is\r\n\r\n> ValueError: A Concatenate layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 192), (1, 128)]", "comments": ["Hi @gunners009! Could you provide a simple stand alone code to replicate this issue? Attaching similar issues for reference. [link1](https://stackoverflow.com/questions/57542946/valueerror-a-concatenate-layer-requires-inputs-with-matching-shapes-except-fo),[link2](https://github.com/qubvel/segmentation_models/issues/1),[link3](https://pretagteam.com/question/valueerror-concatenate-layer-requires-inputs-with-matching-shapes-except-for-the-concat-axis). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53052\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53052\">No</a>\n"]}, {"number": 53051, "title": "[TPU, keras preprocessing layer] Some Op must be a compile-time constant.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): google colab\r\n- TensorFlow version (use command below): 2.7\r\n- Python version: google colab\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: TPU issue\r\n- GPU model and memory: TPU issue\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nHi!\r\nTPU error raises especially with Kears preprocessing layers.\r\nI've tried to connect two models, augmentation model that contains preprocessing layer and segmentation model.\r\n\r\n```python3\r\ndef new_concatenated_model(\r\n    image_input_hw,\r\n    mask_input_hw,\r\n    class_n\r\n):\r\n    seg_model = create_segmentation_model(class_n)\r\n    aug_model = create_augmentation_model(\r\n        image_input_hw, mask_input_hw, class_n)\r\n    \r\n    image_input_shape = list(image_input_hw) + [3]\r\n\r\n    @auto_tpu(device=CURRENT_DEVICE) # decorator `auto_tpu` is just context manager.\r\n    def create():\r\n        im = seg_model.input\r\n        model = AugConcatedSegModel(\r\n            inputs=im,\r\n            outputs=seg_model(im),\r\n            augmentation_model=aug_model,\r\n            name='seg_model_train_with_aug'\r\n        )\r\n        return model\r\n    \r\n    model = create()\r\n    return model\r\n```\r\n\r\n<br>\r\n\r\n`train_step()` function code was mainly came from tensorflow [official tutorial document](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\r\n\r\n```python3\r\nclass AugConcatedSegModel(tf.keras.Model):\r\n    def __init__(\r\n        self,\r\n        inputs=None,\r\n        outputs=None,\r\n        augmentation_model=None, \r\n        **kwargs\r\n    ):\r\n        super().__init__(inputs=inputs, outputs=outputs, **kwargs)\r\n        self.augmentation_model = augmentation_model\r\n\r\n    def train_step(self, data):\r\n        im, ma = data\r\n        im, ma = self.augmentation_model((im, ma))\r\n\r\n        with tf.GradientTape() as tape:\r\n            ma_pred = self(im, training=True)  # Forward pass\r\n            # Compute the loss value\r\n            # (the loss function is configured in `compile()`)\r\n            loss = self.compiled_loss(ma, ma_pred, regularization_losses=self.losses)\r\n\r\n        # Compute gradients\r\n        trainable_vars = self.trainable_variables\r\n        gradients = tape.gradient(loss, trainable_vars)\r\n        # Update weights\r\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n        # Update metrics (includes the metric that tracks the loss)\r\n        self.compiled_metrics.update_state(ma, ma_pred)\r\n        # Return a dict mapping metric names to current value\r\n        return {m.name: m.result() for m in self.metrics}\r\n```\r\n\r\n<br>\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected to train successfully without error.\r\nsame code were tested on:\r\n- [x] CPU : No errors\r\n- [x] GPU : No errors\r\n- [x] TPU : Error\r\n\r\nYou could reproduce this error very fast\r\nhttps://colab.research.google.com/drive/1LhHj1FrkZE9QnFhY-NOO8mn7aiXhZgNh?usp=sharing \r\n`Runtime - Run all`.\r\n\r\n+ When I changed `augmentation model` to just plain `Conv2D` layers, the error disappeared.\r\n\r\n<br>\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1LhHj1FrkZE9QnFhY-NOO8mn7aiXhZgNh?usp=sharing \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```python3\r\nInvalidArgumentError: 9 root error(s) found.\r\n  (0) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2` with op StatelessRandomUniformV2 must be a compile-time constant.\r\n\r\nXLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.\r\n\r\n\t [[{{node sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2}}]]\r\n\t [[TPUReplicate/_compile/_1646634736830564460/_4]]\r\n  (1) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2` with op StatelessRandomUniformV2 must be a compile-time constant.\r\n\r\nXLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.\r\n\r\n\t [[{{node sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2}}]]\r\n\t [[TPUReplicate/_compile/_1646634736830564460/_4]]\r\n\t [[tpu_compile_succeeded_assert/_5094882425795608634/_5/_47]]\r\n  (2) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2` with op StatelessRandomUniformV2 must be a compile-time constant.\r\n\r\nXLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.\r\n\r\n\t [[{{node sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2}}]]\r\n\t [[TPUReplicate/_compile/_1646634736830564460/_4]]\r\n\t [[tpu_compile_succeeded_assert/_5094882425795608634/_5/_159]]\r\n  (3) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_a ... [truncated]\r\n```", "comments": ["@ProtossDragoon \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Hi @sushreebarsa, there is a [COLAB notebook](https://colab.research.google.com/drive/1LhHj1FrkZE9QnFhY-NOO8mn7aiXhZgNh?usp=sharing) instead of a code snippet. You could reproduce this issue without any other codes. Just change COLAB runtime to `TPU` device, and run `all cells`. Thanks.\r\n\r\nCore code below (also exists in the COLAB notebook).\r\n\r\n```python3\r\ndef auto_tpu(device='cpu'):\r\n    \"\"\"Automatically open context manager\r\n    If your colab environment is on 'tpu'\r\n    \"\"\"\r\n    def decorator(fn):\r\n        def wrapper(*args, **kwargs):\r\n            s = time.time()\r\n            if device == 'tpu':\r\n                with TRAINING_PARALLEL_STRATEGY.scope():\r\n                    ret = fn(*args, **kwargs)\r\n            else:\r\n                ret = fn(*args, **kwargs)\r\n            e = time.time()\r\n            print(f'device: {repr(device)}, time elapse: {e-s:.3} second(s)')\r\n            return ret\r\n        return wrapper\r\n    return decorator\r\n\r\n@auto_tpu(device=CURRENT_DEVICE)\r\ndef create_augmentation_model(\r\n    image_input_hw, \r\n    mask_input_hw, \r\n    class_n:int\r\n):\r\n    _default_channel_n = 3\r\n\r\n    # runtime augmentation pipeline\r\n    seq = tf.keras.Sequential(\r\n        [\r\n            tf.keras.layers.RandomFlip(\"horizontal\"),\r\n            tf.keras.layers.RandomRotation(0.02),\r\n        ],\r\n        name='sequential_augmentation_layers'\r\n    )\r\n\r\nimage_input_shape = list(image_input_hw) + [_default_channel_n]\r\nmask_input_shape = list(image_input_hw) + [class_n]\r\nx_im = tf.keras.Input(shape=image_input_shape)\r\nx_ma = tf.keras.Input(shape=mask_input_shape)\r\nreturn tf.keras.Model(\r\n    inputs=[x_im, x_ma], \r\n    outputs=[seq(x_im), seq(x_ma)],\r\n    name='sequential_augmentation_model'\r\n    )\r\n\r\naug_model = create_augmentation_model(\r\n    image_input_hw,\r\n    mask_input_hw,\r\n    class_n\r\n)\r\n\r\nclass AugConcatedSegModel(tf.keras.Model):\r\n    def __init__(\r\n        self,\r\n        inputs=None,\r\n        outputs=None,\r\n        augmentation_model=None, \r\n        **kwargs\r\n    ):\r\n        super().__init__(inputs=inputs, outputs=outputs, **kwargs)\r\n        self.augmentation_model = augmentation_model\r\n\r\ndef train_step(self, data):\r\n    im, ma = data\r\n    im, ma = self.augmentation_model((im, ma))\r\n\r\n    with tf.GradientTape() as tape:\r\n        ma_pred = self(im, training=True)  # Forward pass\r\n        # Compute the loss value\r\n        # (the loss function is configured in `compile()`)\r\n        loss = self.compiled_loss(ma, ma_pred, regularization_losses=self.losses)\r\n\r\n    # Compute gradients\r\n    trainable_vars = self.trainable_variables\r\n    gradients = tape.gradient(loss, trainable_vars)\r\n    # Update weights\r\n    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n    # Update metrics (includes the metric that tracks the loss)\r\n    self.compiled_metrics.update_state(ma, ma_pred)\r\n    # Return a dict mapping metric names to current value\r\n    return {m.name: m.result() for m in self.metrics}\r\n\r\ndef new_concatenated_model(\r\n    image_input_hw,\r\n    mask_input_hw,\r\n    class_n\r\n):\r\n    seg_model = create_segmentation_model(class_n)\r\n    aug_model = create_augmentation_model(\r\n        image_input_hw, mask_input_hw, class_n)\r\n    \r\n    _default_channel_n = 3\r\n    image_input_shape = list(image_input_hw) + [_default_channel_n]\r\n\r\n    @auto_tpu(device=CURRENT_DEVICE)\r\n    def create():\r\n        im = seg_model.input\r\n        model = AugConcatedSegModel(\r\n            inputs=im,\r\n            outputs=seg_model(im),\r\n            augmentation_model=aug_model,\r\n            name='seg_model_train_with_aug'\r\n        )\r\n        return model\r\n    \r\n    model = create()\r\n    return model\r\n\r\nnew_seg_model = new_concatenated_model(\r\n    image_input_hw,\r\n    mask_input_hw,\r\n    class_n\r\n)\r\n\r\n@auto_tpu(device=CURRENT_DEVICE)\r\ndef run(model):\r\n    model.compile('adam', get_loss(class_n), get_metrics())\r\n    model.fit(tf_dataset)\r\n\r\nrun(new_seg_model)\r\n```", "@ProtossDragoon We could see , the TPU error raises especially with Kears preprocessing layers.\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "@ProtossDragoon We could see that you have mentioned this issue on   [keras-team/keras repo.](https://github.com/keras-team/keras/issues) .Could you please move this ticket to closed status as we will track the issue there ? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53051\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53051\">No</a>\n"]}]