[{"number": 48767, "title": "C API Locking prevents custom gradient definition", "body": "I am trying to add custom gradient support to the Java bindings (tensorflow/java#292).  This is currently being prevented by the fact that `TF_AddGradientsWithPrefix` requires a lock on the graph, but so do any operation creation methods (i.e. `TF_NewOperation`), so it is impossible to create ops in gradient functions.\r\n\r\nIs there a way I can get around this?  I am considering calling `TF_NewOperationLocked` directly but I assume the locks are there for a reason.\r\n", "comments": ["Thanks for reporting the issue!", "Please feel free to send a PR!", "I can, but what's the preferred solution?  Providing and exposing a `TF_AddGradientsWithPrefixLocked` that doesn't lock, or exposing `TF_NewOperationLocked` and `TF_FinishOperationLocked`?  (See #48815)", "Exposing `TF_NewOperationLocked` and `TF_FinishOperationLocked` sgtm.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48767\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48767\">No</a>\n"]}, {"number": 48766, "title": "ModuleNotFoundError: No module named 'tensorflow.contrib'", "body": "from tensorflow.contrib.layers.python import layers as tf layers\r\n\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\nDo I need to change tensorflow2.41 to 1.14? Do I need to delete tensorflow2.4?\r\n", "comments": ["@geo-dingky ,\r\n\r\nPlease refer the  issues #36878, #35197, #30794, [37720](https://github.com/tensorflow/tensorflow/issues/37720#issuecomment-601624532) with similar error log.It helps.\r\n\r\nThanks", "Tensorflow layer are now at https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@geo-dingky ,\r\n\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status\r\n\r\nThanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48766\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48766\">No</a>\n"]}, {"number": 48765, "title": "\"Context not initialized\" exception when calling tf.experimental.dlpack.from_dlpack()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: Quadro M1000M (2GB) /1080 Ti (11GB)\r\n\r\n**Describe the current behavior**\r\nUnhandled Exception: \"Context must be initialized first\" when executing a tf.experimental.dlpack.from_dlpack() before any other tf commands..\r\n\r\n**Describe the expected behavior**\r\ntf.experimental.dlpack.from_dlpack() should complete when called with valid arguments before other tf calls.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\ntensor = 1 #dummy argument to produce the error\r\nOffsetData = tf.experimental.dlpack.from_dlpack(tensor)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**workaround**\r\nthis produces the correct exception of \"incompatible function arguments\"\r\n\r\n```python\r\n    import tensorflow as tf\r\n    tensor = 1\r\n    v = tf.constant([1])\r\n    OffsetData = tf.experimental.dlpack.from_dlpack(tensor)\r\n```", "comments": ["I was able to reproduce the issue in TF v2.4.v2.5.0rc1,nightly.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/d57c4ab38273df93affd2b1968f39226/48765.ipynb) here", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48765\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48765\">No</a>\n"]}, {"number": 48764, "title": "Migrate the community builds table", "body": "The Community Builds table has moved to SIG Build: https://github.com/tensorflow/build.", "comments": []}, {"number": 48763, "title": "Clarify label_weights parameter in tf.keras.metrics.AUC", "body": "### Summary of Documentation Issue in tf.keras.metrics.AUC\r\n\r\nIt is not clear whether or not the `label_weights` parameter in the [`tf.keras.metrics.AUC` documentation](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC) expects weights to  sum to 1 or not.  For example with multi-label data, you can take the sum of the occurrences of each label divided by the total number of samples to compute the following example weights:\r\n\r\n![unscaled_label_weights](https://user-images.githubusercontent.com/17085758/116154273-13b3c180-a6ae-11eb-8f2a-242181de484f.png)\r\n\r\nThe weights above reflect the prevalence of each label.  You could also divide these weights by the sum of the weights array to put the weights in the range of [0, 1] like so:\r\n\r\n![scaled_label_weights](https://user-images.githubusercontent.com/17085758/116154429-452c8d00-a6ae-11eb-8f6a-046856344dd8.png)\r\n\r\n### Description of What to Clarify:\r\n\r\nIt would be helpful if the docs could be updated to clarify what these weights represent (the prevalence of each class, the scaled prevalence of each class on the range of [0, 1], or something else).  \r\n\r\nIf the docs seem straightforward to others and I am just missing something, please let me know.  I'm interested in computing micro-average PR AUC for my multi-label model, and it's not clear to me which of the following I should use for multi-hot encoded data.\r\n\r\n##### Data Sample\r\n```\r\n>>> print(y_train)\r\n>>> array([[1, 0, 1],\r\n    [0, 1, 1],\r\n    [0, 0, 0],\r\n    [0, 1, 1]])\r\n```\r\n\r\n#### Unscaled Label Weights\r\n```\r\nn_samples = y_train.shape[0]\r\nclass_totals = y_train.sum(axis=0)\r\nlabel_weights = class_totals / n_samples\r\n```\r\n\r\n#### Scaled Label Weights\r\n```\r\nn_samples = y_train.shape[0]\r\nclass_totals = y_train.sum(axis=0)\r\nweights = class_totals / n_samples\r\nlabel_weights = weights / sum(weights)\r\n```\r\n\r\n", "comments": ["I've been scaling the label weights which seems to be working well.  @fchollet, is this the intended approach?", "The docs are updated with the tf-nightly version.\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC?version=nightly", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48761, "title": "Error when import: Failed to load the native TensorFlow runtime version GLIBC_2.28 not found", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04.1\r\n- TensorFlow version: 2.2.0\r\n- Hardware: Raspberry Pi 4 Model B Rev 1.4\r\n- Architecture: aarch64\r\n- Python version: 3.7.5 [64-bit]\r\n- Installed using: virtualenv and pip\r\n- ldd version: ldd (Ubuntu GLIBC 2.27-3ubuntu1.4) 2.27\r\n\r\n**Describe the problem**\r\nI used the wheel referred to in this tutorial [tutorial](https://qengineering.eu/install-tensorflow-2.2.0-on-raspberry-64-os.html) and installed using pip successfully. However, when I tried to import TF I get this error:\r\nImportError: /lib/aarch64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\nSome sources suggest downgrading python which is not an option for me as this wheel I'm using needs explicitly 3.7 (if there's another wheel I can use, I'm willing to use other version of python, but I prefer to stick with ubuntu 18.04)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`sudo apt-get install python3.7 python3.7-dev  python3.7-venv`\r\n`python3.7 -m venv donkey` \r\n`source donkey/bin/activate`\r\n`gdown https://drive.google.com/uc?id=1fR9lsi_bsI_npPFB-wZyvgjbO0V9FbMf`\r\n`pip install tensorflow-2.2.0-cp37-cp37m-linux_aarch64.whl`\r\n\r\n**Any other info / logs**\r\n<pre>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib/aarch64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib/aarch64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /home/ubuntu/donkey/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.</pre>\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\r\n   * For TF-GPU - See point 1\r\n   * For TF-CPU - See point 2\r\n-----------------------------------------------------------------------------------------------\r\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\r\n\r\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\r\n| TF  | CUDA |\r\n| :-------------: | :-------------: |\r\n| 2.5.0  | 11.2 |\r\n| 2.4.0  | 11.0 |\r\n| 2.1.0 - 2.3.0  | 10.1 |\r\n| 1.13.1 - 2.0  | 10.0  |\r\n| 1.5.0 - 1.12.0 | 9.0 |\r\n\r\n  * If you have above configuration and using _**Windows**_ platform -\r\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\r\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\r\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\r\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\r\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\r\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\r\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\r\n\r\n-----------------------------------------------------------------------------------------------\r\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\r\n\r\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n\r\n   * Try Google Colab to use TensorFlow.\r\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\r\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\r\n      * All you need is a good internet connection and you are all set.\r\n   * Try to build TF from sources by changing CPU optimization flags.\r\n\r\n*Please let us know if this helps.*\r\n", "See https://www.tensorflow.org/lite/guide/build_arm#cross-compilation_for_arm_with_bazel the note says that you need to have glibc >= 2.28 Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48761\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48761\">No</a>\n", "> See https://www.tensorflow.org/lite/guide/build_arm#cross-compilation_for_arm_with_bazel the note says that you need to have glibc >= 2.28 Thanks!\n\nThank you, although the note you refer to is in fact in the page of TF \"lite\"", "I'm adding this comment to help anyone reading this and having same problems. I tried different things recommended by many persons trying to help, but I always got stuck with something. I think this is because it's usually very easy to overlook some details when I ask for help for my specific setup (RPI4+ARM64+Ubuntu18) and I found out that it's very common to receive help with the assumption that I'm using Raspbian or that my OS is 32-bit or I'm asking for TF lite or even thinking that Ubuntu 18 would be the same as 20. \r\n\r\nSo what ended up working for me is building it from source following the official documentation tutorial [here](https://www.tensorflow.org/install/source) and it took 22 hours to compile. Don't be intimidated by the process; I know having a wheel file and just pip install looks way easier, but really building from source is quite straightforward. I also have the wheel file if you want (I will probably upload it on google drive or dropbox)\r\n\r\nBecause of the so many problems popping up, I don't remember them or their source. However, there were 4 errors that happened because I didn't give proper attention to some details:\r\n1. Double check the .bazelversion file in the TF folder and make sure that this version really exists for ARM64 (at one point this created a problem for me because for a reason I don't know it was 1.3 and this version doesn't exist for aarch64)\r\n2. If you are using a virtual environment: You *must* do the configuration (via the configure.py file) before building, don't forget to do this :-)\r\n3. If you didn't create it with --system-site-packages, make sure you have got everything you need (numpy, keras-preprocessing ...)\r\n4. I got an error when I tried to import TF complaining about the version of numpy (I don't remember it). I solved it by `pip install --upgrade numpy`"]}, {"number": 48760, "title": "Fix the TFLM github bazel CI.", "body": "We use only a subset of the Skylark functions to keep the TFLM bazel build quick enough to be run as part of CI. As a result, the bazel CI can be broken when additional Skylark functions are used in the BUILD files that are shared between Lite and Micro.\r\n\r\nIn the case of the current breakage, https://github.com/tensorflow/tensorflow/commit/05addf893ca52dc3ddfa0a28e7b50d46c96b09fe added `pybind_extension` to `lite/kernels/BUILD` and the current PR creates an empty stub implementation of `pybind_extension`.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48758, "title": "TFLM: Remove uint8 support for CMSIS-NN kernels", "body": "Some CMSIS-NN kernels are still having uint8 support.\r\n\r\n* Remove uint8 support for remaining CMSIS-NN kernels.\r\n* Remove uint8 support for equivalent reference kernels.\r\n* Remove uint8 unit tests for equivalent kernels.\r\n* Replace uint8 default model in network tester to a int8 model.\r\n\r\nThis is a fix for: https://github.com/tensorflow/tensorflow/issues/48757", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@mansnils Can you please resolve conflicts? Thanks!", "@njeffrie ping for review", "@mansnils  Can you please resolve conflicts? Thanks!", "@njeffrie ping for review", "Unfortunately, this PR had to be reverted because it (unexpectedly) made the detection_postprocess kernel fail on bluepill.\r\n\r\nHere is a PR that reverts the revert and we can recreate the failure: https://github.com/tensorflow/tensorflow/pull/49485\r\n\r\nIts not clear to me why the CI passed for the current PR since it is failing for me 100% of the time.", "Also, when you make a new PR, can you also remove\r\nhttps://github.com/tensorflow/tensorflow/blob/5177e09c9f302372137f28ab7798a8a413c352f7/tensorflow/lite/micro/tools/make/Makefile#L445\r\n"]}, {"number": 48757, "title": "Remove uint8 support in CMSIS-NN kernels", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): 323d8b857216f20bf4266ea0a7876363d707aceb\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nUint8 support is being removed in TFLM. CMSIS-NN kernels still have some uint8 support and it should be removed.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": []}, {"number": 48756, "title": "Build failed \"The command line is too long\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n**Win10** \r\n- TensorFlow installed from (source or binary): \r\n**source**\r\n- TensorFlow version: \r\n**2.4.0**\r\n- Python version:\r\n **3.7**\r\n- Installed using virtualenv? pip? conda?:\r\n**conda**\r\n- Bazel version (if compiling from source): \r\n**3.1.0**\r\n- GCC/Compiler version (if compiling from source): \r\n**VS2019**\r\n- CUDA/cuDNN version: \r\n**11.0/8.0**\r\n- GPU model and memory:\r\n- \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHello,\r\nI'm trying to build Tensorflow.dll from source as described below with GPU support and without ROCm.\r\nThe build fails on \"Command is too long\" I can't reduce the sizes of the paths, it's all relative. \r\nI saw previous related issues and due to that I checked the official site and it's says bazel 3.1.0 compatible with tensor 2.4.0\r\n\r\nI will appreciate any suggestions. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nIgnore the dots in the path.\r\nI tried with and without \"eigen strong inline for some C++\"\r\n\r\nset BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\r\n\r\nC:\\...\\tensorflow-2.4.0>python ./configure.py\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is C:\\ProgramData\\Anaconda3\\python.exe]: C:\\ProgramData\\Anaconda3\\python.exe\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\ProgramData\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\ProgramData\\Anaconda3\\lib\\site-packages]\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.0 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include\r\nFound cuDNN 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.0\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\nC:\\..\\tensorflow-2.4.0>bazel build --config=opt --config=cuda tensorflow:tensorflow.dll\r\n\r\n**Any other info / logs**\r\n\r\n**The error**\r\nERROR: C:/../tensorflow-2.4.0/tensorflow/compiler/xla/service/cpu/BUILD:130:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:cpu_compiler' failed (Exit 1): python.exe failed: error executing command\r\n  cd C:/users/auadmin/_bazel_auadmin/q3fiaeg3/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.28.29910\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.28.29910\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.28.29910\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.28.29910\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.28.29910\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\\\MSBuild\\Current\\Bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\Llvm\\x64\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\auadmin\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.0\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\auadmin\\AppData\\Local\\Temp\r\n  C:/ProgramData/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/llvm-project /Ibazel-out/x64_windows-opt/bin/external/llvm-project /Iexternal/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_pattern_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/canonicalize_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CopyOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512IncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512ConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AVX512IncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AsyncOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/NVVMConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUToNVVMGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ParallelLoopMapperAttrGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUToROCDLTGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ROCDLOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUToSPIRVIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpUtilsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenACCOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/QuantPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsTransformsPassIncGen /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/llvm-project/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include /Iexternal/llvm-project/llvm/include/llvm/IR /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include/llvm/IR /Iexternal/llvm-project/llvm/include/llvm/Frontend/OpenMP /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include/llvm/Frontend/OpenMP /Iexternal/llvm-project/llvm/lib/Target/AMDGPU /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/lib/Target/AMDGPU /Iexternal/mkl_dnn/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/include /Iexternal/mkl_dnn/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src /Iexternal/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/common /Iexternal/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu /Iexternal/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/gemm /Iexternal/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/xbyak /Itensorflow/compiler/mlir/xla/include /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/xla/include /Itensorflow/compiler/mlir/hlo/include /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include /Iexternal/llvm-project/mlir/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include /Iexternal/llvm-project/llvm/lib/Target/NVPTX /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/lib/Target/NVPTX /Iexternal/llvm-project/mlir/lib/Conversions/GPUToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversions/GPUToSPIRV /Iexternal/llvm-project/mlir/lib/Conversion/StandardToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/StandardToSPIRV /Iexternal/llvm-project/llvm/lib/Target/X86 /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/lib/Target/X86 /DMLIR_CUDA_CONVERSIONS_ENABLED /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLLVM_ENABLE_STATS /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DLLVM_BUILD_GLOBAL_ISEL /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /arch:AVX /std:c++14 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/cpu_compiler/cpu_compiler.obj /c tensorflow/compiler/xla/service/cpu/cpu_compiler.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\nThe command line is too long.\r\nTarget //tensorflow:tensorflow.dll failed to build\r\nINFO: Elapsed time: 4.811s, Critical Path: 0.83s\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully\r\nAlso is there a  known way yo rub the build by parts? \r\n\r\nThank you! \r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@GitUser2000 \r\n\r\nCan you please check your pip version and [numpy version](https://github.com/tensorflow/tensorflow/issues/41061#issuecomment-656017717), refer to this comment here and verify your [system requirements](https://github.com/tensorflow/tensorflow/issues/47824), also refer to [link](https://github.com/tensorflow/tensorflow/issues/44913#issuecomment-751487970).", "I'm building using cpp and not pip, any specific info I need to provide ? thanks", "Is there any suggestions? ", "@GitUser2000 Could you please try installing from [source](https://www.tensorflow.org/install/source_windows#tested_build_configurations) using TF v2.6.0 and let us know if it helps ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48756\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48756\">No</a>\n"]}, {"number": 48755, "title": "tf.keras.preprocessing.image_dataset_from_directory Error", "body": "TensorFlow version: 2.4.1\r\npython version: 3.7.10\r\nCUDA version: 11.2\r\nCUDA Driver version: 465.19.01\r\n\r\n\r\nDocument of the function\r\n  `tf.keras.preprocessing.image_dataset_from_directory`\r\nthe parameter `directory` is: \r\n`Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing images for a class. Otherwise, the directory structure is ignored.`\r\n\r\nBut the directory structure is not ignored.\r\n\r\n\r\nI read a code of  `tf.keras.preprocessing.dataset_utils.index_directory`,\r\nand I found that it always care about subdirs even if labels are feed by list already. \r\n\r\nSample code: \r\n```python\r\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n    image_path,\r\n    labels=labels,\r\n    label_mode=\"binary\",\r\n    validation_split=0.2,\r\n    subset='training',\r\n)\r\n\r\nlen(labels) == 202599\r\nlen(os.listdir(image_path)) == 202599  # Every file in image_path are JPG format.\r\n```\r\n\r\nIt raises an error: \r\n Expected the lengths of `labels` to match the number of files in the target directory. `len(labels)` is 202599 while we found 0 files in `image_path`.", "comments": ["Can you check https://github.com/tensorflow/tensorflow/issues/44752? It this is a duplicate please close this and upvote that one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48755\">No</a>\n"]}, {"number": 48754, "title": "0.6.0", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48754) for more info**.\n\n<!-- need_sender_cla -->", "@adityamohan29  Can you please sign CLA. Thanks!", "All these changes seem to be commits from other people. "]}, {"number": 48751, "title": "Support FP32 computations using INT8 weights in TensorFlow Lite XNNPACK delegate", "body": "This PR introduces FP32 computations using per-tensor quantized INT8 weights in XNNPACK delegate. It is done in the same way XNNPACK handles FP16 models. Per-channel quantization support may be supported later.\r\nThis PR addresses the issue that TF Lite models quantized in dynamic range mode are not able to employ XNNPACK for faster inference, while most inference computations are generally performed in FP32 using default TF Lite kernels which are basically stealing XNNPACK's job. \r\nSuch models can be produced using TF Lite converter with manually disabled hybrid operators (btw this is a topic for another PR since hybrid kernels seems to be slower than non-hybrid once, and there is no way to turn them off in Python API) generation with dynamic range quantization mode is used: almost all the ops store INT8 weights and use `Dequantize` ops for them at the runtime.\r\nI was successfully employed this technique to significantly speed up the EfficientNet-B5-like model on Android smartphone (Samsung Galaxy S20+, latency: ~800 ms -> ~250 ms).\r\nPlease let me know if you want me to share more details on my experiments (benchmark results, precision checking results etc.)  to justify this PR.", "comments": ["@Maratyszcza @multiverse-tf could you review this PR?", "@Maratyszcza @multiverse-tf Can you please review this PR? Thanks!", "> This PR introduces FP32 computations using per-tensor quantized INT8 weights in XNNPACK delegate. It is done in the same way XNNPACK handles FP16 models. Per-channel quantization support may be supported later.\r\n> This PR addresses the issue that TF Lite models quantized in dynamic range mode are not able to employ XNNPACK for faster inference, while most inference computations are generally performed in FP32 using default TF Lite kernels which are basically stealing XNNPACK's job.\r\n> Such models can be produced using TF Lite converter with manually disabled hybrid operators (btw this is a topic for another PR since hybrid kernels seems to be slower than non-hybrid once, and there is no way to turn them off in Python API) generation with dynamic range quantization mode is used: almost all the ops store INT8 weights and use `Dequantize` ops for them at the runtime.\r\n\r\n\"no-hybrid\" means the float32 one here? Could you file an issue regarding the performance slowness in the hybrid one? We will double check our implementation and see why some of our recently landed hybrid optimizations didn't work. Many thanks!\r\n\r\n> I was successfully employed this technique to significantly speed up the EfficientNet-B5-like model on Android smartphone (Samsung Galaxy S20+, latency: ~800 ms -> ~250 ms).\r\n\r\nThe results look really impressive!\r\n\r\n> Please let me know if you want me to share more details on my experiments (benchmark results, precision checking results etc.) to justify this PR.\r\n\r\nMany thanks for the contribution! I'll wait for @Maratyszcza comments as there're WIP efforts to support quantization in XNNPACK. At the moment, you could try it out adding \"--define=xnn_enable_qs8=true\" bazel flag when building the XNNPACK library. Admittedly, this WIP support might not work for the hybrid model.\r\n", "@multiverse-tf \r\n\r\n> \"no-hybrid\" means the float32 one here? Could you file an issue regarding the performance slowness in the hybrid one? We will double check our implementation and see why some of our recently landed hybrid optimizations didn't work. Many thanks!\r\n\r\nYes, by hybrid kernels I mean ones similar to [this one](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L1331) (hybrid convolution). I have observed that this kernel is generally slower than pure float32 computations for dynamic range quantized model obtained with TF 2.3 converter, and its performance barely depends on interpreter num threads. However, model obtained with TF 2.4 converter seems to perform faster since hybrid convolution for per-channel quantization case is faster than the one for per-tensor quantization case mentioned above (I believe that TF 2.4 TFLite converter replaces per-tensor dynamic range quantization with the per-channel one, please correct me if I am wrong). Anyway, I will file an issue with the detailed description of experiments I made.\r\n\r\n> Many thanks for the contribution! I'll wait for @Maratyszcza comments as there're WIP efforts to support quantization in XNNPACK. At the moment, you could try it out adding \"--define=xnn_enable_qs8=true\" bazel flag when building the XNNPACK library. Admittedly, this WIP support might not work for the hybrid model.\r\n\r\nThis PR introduces something different from the pure INT8 inference using XNNPACK. What `xnn_enable_qs8` option introduces is the INT8 computations for **full integer** quantized models, while this PR introduces FP32 computations for **dynamic range** quantized models. So the former speeds up full integer models and the latter speeds up dynamic range quantized models. Hope my explanation makes sense.\r\nBtw I have tested this feature a couple days ago, and seems like it speeds up my full integer quantized ResNet48-like model inference by the factor of 2 comparing to the default Ruy backend, which is really amazing!", "> @multiverse-tf\r\n> \r\n> > \"no-hybrid\" means the float32 one here? Could you file an issue regarding the performance slowness in the hybrid one? We will double check our implementation and see why some of our recently landed hybrid optimizations didn't work. Many thanks!\r\n> \r\n> Yes, by hybrid kernels I mean ones similar to [this one](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L1331) (hybrid convolution). I have observed that this kernel is generally slower than pure float32 computations for dynamic range quantized model obtained with TF 2.3 converter, and its performance barely depends on interpreter num threads. However, model obtained with TF 2.4 converter seems to perform faster since hybrid convolution for per-channel quantization case is faster than the one for per-tensor quantization case mentioned above (I believe that TF 2.4 TFLite converter replaces per-tensor dynamic range quantization with the per-channel one, please correct me if I am wrong). Anyway, I will file an issue with the detailed description of experiments I made.\r\n\r\nI'm not familiar with this converter part. @daverim, as you have been working on the dynamic range quantization, could you help with this comment? Thx!\r\n\r\n> \r\n> > Many thanks for the contribution! I'll wait for @Maratyszcza comments as there're WIP efforts to support quantization in XNNPACK. At the moment, you could try it out adding \"--define=xnn_enable_qs8=true\" bazel flag when building the XNNPACK library. Admittedly, this WIP support might not work for the hybrid model.\r\n> \r\n> This PR introduces something different from the pure INT8 inference using XNNPACK. What `xnn_enable_qs8` option introduces is the INT8 computations for **full integer** quantized models, while this PR introduces FP32 computations for **dynamic range** quantized models. So the former speeds up full integer models and the latter speeds up dynamic range quantized models. Hope my explanation makes sense.\r\n\r\nAcked. As @Maratyszcza is on vacation these days (but will be back next week), sorry for being late about the code review. I'll continue to wait for his comments about hybrid kernels regarding the integration into xnnpack delegate.\r\n\r\n> Btw I have tested this feature a couple days ago, and seems like it speeds up my full integer quantized ResNet48-like model inference by the factor of 2 comparing to the default Ruy backend, which is really amazing!\r\n\r\nThx for sharing the result :-)\r\n", "@multiverse-tf  Can you please assist on above comments from @Maratyszcza. Thanks!", "@Maratyszcza \r\n\r\nI rewrote INT8 weights generation part.\r\nNo changes for FP32 bias instead of INT8 so far due to pending @multiverse-tf review.\r\n\r\n", "> Weights should be generated in three steps:\r\n> \r\n> 1. Generate FP32 values.\r\n> 2. Calculate optimal `scale` for generated values.\r\n> 3. Quantize the FP32 weights with the calculated `scale` and zero point of 0.\r\n>    Otherwise, there's a chance that all quantized weights are 0 and the test isn't actually testing much.\r\n> \r\n> Also, IIUC when weights are quantized to INT8, bias should be left as FP32 (technically could be converted to INT32, but it wouldn't save any bytes). cc @multiverse-tf to confirm.\r\n\r\nI also think that bias should be left as FP32.\r\n", "@multiverse-tf I changed the bias part in tests. Now bias is always stored in FP32.", "@Maratyszcza @multiverse-tf \r\nCould anybody review this MR once again? All the requested changes are made and the previous approve doesn't work since there were failed CI jobs (2 most recent commits fixes those). Thank you in advance!", "@multiverse-tf Thank you! I can see that some CI jobs are still failing, however these fails seem to be unrelated to this MR changes. Is this an issue for merging?", "> @multiverse-tf Thank you! I can see that some CI jobs are still failing, however these fails seem to be unrelated to this MR changes. Is this an issue for merging?\r\n\r\nNo worries. I'll handle the merge then.", "@multiverse-tf I feel so bad bothering you again, but it seems like there have been some issues during the merge attempt. What can I do to help with this?", "> @multiverse-tf I feel so bad bothering you again, but it seems like there have been some issues during the merge attempt. What can I do to help with this?\r\n\r\nNo worries. I'm actively handling the merge internally. This is a very large PR, and it has invited more comments internally :-) But I think it should be merged by the end of the week. Anyway, I'll let you know when it's merged. Thx for the penitence!", "@multiverse-tf Thank you very much for the clarification and for working on it!", "> @multiverse-tf Thank you very much for the clarification and for working on it!\r\n\r\n@dev0x13, I'm sorry that the merge could not be achieved this week as the PR enables hybrid model support by default in XNNPACK delegate which requires much more thorough testing internally with other Google products. So to move it forward, we may guard your change with some pre-defined macros. Thanks for your patience, and hope you could understand this. Many thanks again for your contribution!", "> @multiverse-tf\r\n> \r\n> > \"no-hybrid\" means the float32 one here? Could you file an issue regarding the performance slowness in the hybrid one? We will double check our implementation and see why some of our recently landed hybrid optimizations didn't work. Many thanks!\r\n> \r\n> Yes, by hybrid kernels I mean ones similar to [this one](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L1331) (hybrid convolution). I have observed that this kernel is generally slower than pure float32 computations for dynamic range quantized model obtained with TF 2.3 converter, and its performance barely depends on interpreter num threads. However, model obtained with TF 2.4 converter seems to perform faster since hybrid convolution for per-channel quantization case is faster than the one for per-tensor quantization case mentioned above (I believe that TF 2.4 TFLite converter replaces per-tensor dynamic range quantization with the per-channel one, please correct me if I am wrong). Anyway, I will file an issue with the detailed description of experiments I made.\r\n> \r\n> > Many thanks for the contribution! I'll wait for @Maratyszcza comments as there're WIP efforts to support quantization in XNNPACK. At the moment, you could try it out adding \"--define=xnn_enable_qs8=true\" bazel flag when building the XNNPACK library. Admittedly, this WIP support might not work for the hybrid model.\r\n> \r\n> This PR introduces something different from the pure INT8 inference using XNNPACK. What `xnn_enable_qs8` option introduces is the INT8 computations for **full integer** quantized models, while this PR introduces FP32 computations for **dynamic range** quantized models. So the former speeds up full integer models and the latter speeds up dynamic range quantized models. Hope my explanation makes sense.\r\n> Btw I have tested this feature a couple days ago, and seems like it speeds up my full integer quantized ResNet48-like model inference by the factor of 2 comparing to the default Ruy backend, which is really amazing!\r\n\r\n@dev0x13, could you share this ResNet48-like tflite model? And on what platform did you get this ~2X speedup? Thx!", "@multiverse-tf\r\n\r\n> I'm sorry that the merge could not be achieved this week as the PR enables hybrid model support by default in XNNPACK delegate which requires much more thorough testing internally with other Google products. So to move it forward, we may guard your change with some pre-defined macros. Thanks for your patience, and hope you could understand this. Many thanks again for your contribution!\r\n\r\nIt's okay, thank you very much for keeping me posted on this! Am I supposed to add these macro guards and a Bazel flag or it's something your team is planning to handle?\r\n\r\n> could you share this ResNet48-like tflite model? And on what platform did you get this ~2X speedup? Thx!\r\n\r\nThe platform is Samsung Galaxy S20+ (global version, not the US one). Important thing to mention is that we are focused on getting the highest inference speed with no regard for resource consumption. So that we heavily employ multithreaded inference for both Ruy and XNNPACK. Currently we use half of the number of physical cores for all threading settings in TFLite interpreter and delegates in assumption that typical Android device chip set implements big.LITTLE architecture (this assumption is kind of statistically proven to be fair since using all physical cores leads to the lower performance).\r\nI am not quite sure about sharing the model as is since it's proprietary, so I need to discuss the sharing options with out team. Why are you asking btw? Is it an unexpected result for XNNPACK being faster than Ruy? :)", "> @multiverse-tf\r\n> \r\n> > I'm sorry that the merge could not be achieved this week as the PR enables hybrid model support by default in XNNPACK delegate which requires much more thorough testing internally with other Google products. So to move it forward, we may guard your change with some pre-defined macros. Thanks for your patience, and hope you could understand this. Many thanks again for your contribution!\r\n> \r\n> It's okay, thank you very much for keeping me posted on this! Am I supposed to add these macro guards and a Bazel flag or it's something your team is planning to handle?\r\n\r\nI'll handle this internally with the merge. Basically, your PR has been imported internally as a pending change, and I'm working on that copy to fix failures that originally blocked the merge, and address internal comments.\r\n\r\n> \r\n> > could you share this ResNet48-like tflite model? And on what platform did you get this ~2X speedup? Thx!\r\n> \r\n> The platform is Samsung Galaxy S20+ (global version, not the US one). Important thing to mention is that we are focused on getting the highest inference speed with no regard for resource consumption. So that we heavily employ multithreaded inference for both Ruy and XNNPACK. Currently we use half of the number of physical cores for all threading settings in TFLite interpreter and delegates in assumption that typical Android device chip set implements big.LITTLE architecture (this assumption is kind of statistically proven to be fair since using all physical cores leads to the lower performance).\r\n\r\nThx for sharing the testing environment!\r\n\r\n> I am not quite sure about sharing the model as is since it's proprietary, so I need to discuss the sharing options with out team. Why are you asking btw? Is it an unexpected result for XNNPACK being faster than Ruy? :)\r\n\r\nWell, in order to be accelerated by the XNNPACK delegate (w/ this PR), it looks to me the model has to have a dequantize op for the int8 weight tensor and its output being the input of computation-intensive op, like CONV2D etc.,. However, unlike the fp16 quantization case where we don't have native fp16 kernels, I think the current TFLite converter and the model optimization toolkit will not produce this additional dequantize op for int8-quantized-weights models.  So, I'm just curious how is your model produced.\r\n\r\n", "@multiverse-tf\r\n\r\nThank you!\r\n\r\n> Well, in order to be accelerated by the XNNPACK delegate (w/ this PR), it looks to me the model has to have a dequantize op for the int8 weight tensor and its output being the input of computation-intensive op, like CONV2D etc.,. However, unlike the fp16 quantization case where we don't have native fp16 kernels, I think the current TFLite converter and the model optimization toolkit will not produce this additional dequantize op for int8-quantized-weights models. So, I'm just curious how is your model produced.\r\n\r\nOh, 2x speedup is for `xnn_enable_qs8` flag, i.e. for full-integer quantized model. For dynamic range quantized models I was able to get 3-4x speed up with this PR.\r\nIn order to produce a model with dequantize ops I have built TFLite converter from source with manually disabled hybrid operators [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantize_weights.cc#L479). From my perspective, an option to enable/disable hybrid kernels in high level TFlite converter API (including Python) is a nice to have feature, so I am think of creating a separate PR for this capability. This seems to be reasonable because currently `use_hybrid_evaluation` option is present in all the functions parameters, but yet hard-coded to `true` [everywhere](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantize_weights.cc#L645) for some reason. Would such kind of PR be accepted?\r\n", "> @multiverse-tf\r\n> \r\n> Thank you!\r\n> \r\n> > Well, in order to be accelerated by the XNNPACK delegate (w/ this PR), it looks to me the model has to have a dequantize op for the int8 weight tensor and its output being the input of computation-intensive op, like CONV2D etc.,. However, unlike the fp16 quantization case where we don't have native fp16 kernels, I think the current TFLite converter and the model optimization toolkit will not produce this additional dequantize op for int8-quantized-weights models. So, I'm just curious how is your model produced.\r\n> \r\n> Oh, 2x speedup is for `xnn_enable_qs8` flag, i.e. for full-integer quantized model. For dynamic range quantized models I was able to get 3-4x speed up with this PR.\r\n> In order to produce a model with dequantize ops I have built TFLite converter from source with manually disabled hybrid operators [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantize_weights.cc#L479). From my perspective, an option to enable/disable hybrid kernels in high level TFlite converter API (including Python) is a nice to have feature, so I am think of creating a separate PR for this capability. This seems to be reasonable because currently `use_hybrid_evaluation` option is present in all the functions parameters, but yet hard-coded to `true` [everywhere](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantize_weights.cc#L645) for some reason. Would such kind of PR be accepted?\r\n\r\nI'm not sure as I didn't know the reasons behind why the option is hard-coded true everywhere (which I guess is probably because we have native int8 hybrid kernels, and those kernels don't need this extra dequantize op at all). But anyway, welcome a PR and let's discuss about it there.\r\n\r\nAs you may have noticed, this PR has been merged. Basically, to use this feature, one has to compile the library with macro \"ENABLE_TFLITE_XNNPACK_DEQUANTIZED_INT8_WEIGHTS\" defined, OR set \"enable_int8_weights_unpacking\" to true when creating the xnnpack delegate explicitly.\r\n", "@dev0x13 Hello, Georgiy! I'm interested in trying your PR with some of my models on Raspberry Pi 4. I have compiled tensorflow lite interpreter pip wheel from commit https://github.com/tensorflow/tensorflow/commit/e5a7475be711e99646fa77dbf8d014bb9d9b6786 with the following parameters:\r\n```\r\n# Build python interpreter_wrapper.\r\ncd \"${BUILD_DIR}\"\r\ncase \"${TENSORFLOW_TARGET}\" in\r\n  armhf)\r\n    BAZEL_FLAGS=\"--config=elinux_armhf\r\n      --copt=-march=armv7-a --copt=-mfpu=neon-vfpv4\r\n      --copt=-O3 --copt=-fno-tree-pre --copt=-fpermissive\r\n      --define tensorflow_mkldnn_contraction_kernel=0\r\n      --define=raspberry_pi_with_neon=true\r\n      --define=tflite_pip_with_flex=true\r\n      --define=tflite_with_xnnpack=true\"\r\n    ;;\r\n  aarch64)\r\n    BAZEL_FLAGS=\"--config=elinux_aarch64\r\n      --define tensorflow_mkldnn_contraction_kernel=0\r\n      --define=tflite_pip_with_flex=true\r\n      --define=tflite_with_xnnpack=true\r\n      --define=enable_int8_weights_unpacking=true\r\n      --copt=-O3\"\r\n    ;;\r\n  native)\r\n    BAZEL_FLAGS=\"--copt=-O3 --copt=-march=native\r\n      --define=tflite_pip_with_flex=true\r\n      --define=tflite_with_xnnpack=true\"\r\n    ;;\r\n  *)\r\n    BAZEL_FLAGS=\"--copt=-O3\r\n      --define=tflite_pip_with_flex=true\r\n      --define=tflite_with_xnnpack=true\"\r\n    ;;\r\nesac\r\n```\r\nSpecifically I plan to test on 64-bit version of Raspberry Pi OS, so I only added  --define=enable_int8_weights_unpacking=true for aarch64. The compilation was successful and I'll try the produced wheel on Monday. Are there any specific requirements for the model to take advantage of this optimization? Or the following conversion code is sufficient?\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_quant_model = converter.convert()\r\n```\r\n", "@AIWintermuteAI Hi! Unfortunately it's not that simple. This PR is useful for the restricted set of converted models. Currently there are two hard requirements for them:\r\n1. Model should not contain hybrid ops.\r\n2. Model should only employ per-tensor quantization.\r\n\r\nThe first requirement currently can be met only by manual disabling hybrid ops in TFLite converter C++ code (e.g. [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantize_weights.cc#L479)). Then you should build `tflite_convert` target: `bazel build -c opt tensorflow/lite/python:tflite_convert`.\r\n\r\nThe second requirement can be met in two ways:\r\n1. Use TensorFlow <2.4 as a base version.\r\n2. Use TensorFlow master as a base version and use [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py#L454) TFLite converter option to force per-tensor quantization instead of per-channel one.\r\nHowever, I am currently working on the other PR targeting per-channel dynamic quantization support in XNNPACK delegate, so eventually pass away."]}, {"number": 48750, "title": "_init_profile_batch docstring specifies wrong return", "body": "Existing documentation describes method as returning\r\na pair of integers, while it actually sets values of the object attributes.\r\n\r\nThis can be potentially confusing. Docstring was updated to describe\r\nan actual state of the method.\r\n\r\nAdded mention of special behavior encountered when profile_batch=0.\r\n\r\nSigned-off-by: Jiri Podivin <jpodivin@gmail.com>", "comments": []}, {"number": 48749, "title": "Tensorflow_core.combat-v2 Has no attribute __Internal", "body": "I get this error that says combat-v2 module has no __internal attribute.\r\nCan someone help me how to solve this\r\nError\r\nAttributeError: module 'tensorflow_core.compat.v2' has no attribute '__internal__'", "comments": ["@Davidelvis ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset  or colab link you are using. Thanks!", "Okay\r\nI am using TensorFlow version 2.4.1,\r\nI am training Reinforcement Learning agent, when I run the code, I get the error\r\n\r\nimport gym\r\nimport numpy as np \r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import Dense, Dropout, Input\r\nfrom keras.layers.merge import Add, Multiply\r\nfrom keras.optimizers import Adam\r\nimport keras.backend as K\r\nimport tensorflow as tf\r\n\r\n   self.actor_state_input, self.actor_model = \\\r\n            self.create_actor_model()\r\n        _, self.target_actor_model = self.create_actor_model()\r\n        self.actor_critic_grad = tf.placeholder(tf.float32, \r\n            [None, self.env.action_space.shape[0]]) \r\n        \r\n        actor_model_weights = self.actor_model.trainable_weights\r\n        self.actor_grads = tf.gradients(self.actor_model.output, \r\n            actor_model_weights, -self.actor_critic_grad)\r\n        grads = zip(self.actor_grads, actor_model_weights)\r\n        self.optimize =  tf.train.AdamOptimizer(\r\n            self.learning_rate).apply_gradients(grads)", "@Davidelvis ,\r\n\r\n  Code shared is full of [indentation](https://colab.research.google.com/gist/tilakrayal/f86d3f7afac4d3aa01913dbd23f317a0/untit48749.ipynb) errors, please share a colab gist with issue reported or simple stand alone indented code with all dependencies such that we can replicate the issue reported.", "Ths is the whole code\r\nimport gym\r\nimport numpy as np \r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import Dense, Dropout, Input\r\nfrom keras.layers.merge import Add, Multiply\r\nfrom keras.optimizers import Adam\r\nimport keras.backend as K\r\n\r\nimport tensorflow as tf\r\n\r\nimport random\r\nfrom collections import deque\r\n\r\n# determines how to assign values to each state, i.e. takes the state\r\n# and action (two-input model) and determines the corresponding value\r\nclass ActorCritic:\r\n\tdef __init__(self, env, sess):\r\n\t\tself.env  = env\r\n\t\tself.sess = sess\r\n\r\n\t\tself.learning_rate = 0.001\r\n\t\tself.epsilon = 1.0\r\n\t\tself.epsilon_decay = .995\r\n\t\tself.gamma = .95\r\n\t\tself.tau   = .125\r\n\r\n\t\t# ===================================================================== #\r\n\t\t#                               Actor Model                             #\r\n\t\t# Chain rule: find the gradient of chaging the actor network params in  #\r\n\t\t# getting closest to the final value network predictions, i.e. de/dA    #\r\n\t\t# Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\r\n\t\t# ===================================================================== #\r\n\r\n\t\tself.memory = deque(maxlen=2000)\r\n\t\tself.actor_state_input, self.actor_model = self.create_actor_model()\r\n\t\t_, self.target_actor_model = self.create_actor_model()\r\n\r\n\t\tself.actor_critic_grad = tf.placeholder(tf.float32, \r\n\t\t\t[None, self.env.action_space.shape[0]]) # where we will feed de/dC (from critic)\r\n\t\t\r\n\t\tactor_model_weights = self.actor_model.trainable_weights\r\n\t\tself.actor_grads = tf.gradients(self.actor_model.output, \r\n\t\t\tactor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\r\n\t\tgrads = zip(self.actor_grads, actor_model_weights)\r\n\t\tself.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\r\n\r\n\t\t# ===================================================================== #\r\n\t\t#                              Critic Model                             #\r\n\t\t# ===================================================================== #\t\t\r\n\r\n\t\tself.critic_state_input, self.critic_action_input, \\\r\n\t\t\tself.critic_model = self.create_critic_model()\r\n\t\t_, _, self.target_critic_model = self.create_critic_model()\r\n\r\n\t\tself.critic_grads = tf.gradients(self.critic_model.output, \r\n\t\t\tself.critic_action_input) # where we calcaulte de/dC for feeding above\r\n\t\t\r\n\t\t# Initialize for later gradient calculations\r\n\t\tself.sess.run(tf.initialize_all_variables())\r\n\r\n\t# ========================================================================= #\r\n\t#                              Model Definitions                            #\r\n\t# ========================================================================= #\r\n\r\n\tdef create_actor_model(self):\r\n\t\tstate_input = Input(shape=self.env.observation_space.shape)\r\n\t\th1 = Dense(24, activation='relu')(state_input)\r\n\t\th2 = Dense(48, activation='relu')(h1)\r\n\t\th3 = Dense(24, activation='relu')(h2)\r\n\t\toutput = Dense(self.env.action_space.shape[0], activation='relu')(h3)\r\n\t\t\r\n\t\tmodel = Model(input=state_input, output=output)\r\n\t\tadam  = Adam(lr=0.001)\r\n\t\tmodel.compile(loss=\"mse\", optimizer=adam)\r\n\t\treturn state_input, model\r\n\r\n\tdef create_critic_model(self):\r\n\t\tstate_input = Input(shape=self.env.observation_space.shape)\r\n\t\tstate_h1 = Dense(24, activation='relu')(state_input)\r\n\t\tstate_h2 = Dense(48)(state_h1)\r\n\t\t\r\n\t\taction_input = Input(shape=self.env.action_space.shape)\r\n\t\taction_h1    = Dense(48)(action_input)\r\n\t\t\r\n\t\tmerged    = Add()([state_h2, action_h1])\r\n\t\tmerged_h1 = Dense(24, activation='relu')(merged)\r\n\t\toutput = Dense(1, activation='relu')(merged_h1)\r\n\t\tmodel  = Model(input=[state_input,action_input], output=output)\r\n\t\t\r\n\t\tadam  = Adam(lr=0.001)\r\n\t\tmodel.compile(loss=\"mse\", optimizer=adam)\r\n\t\treturn state_input, action_input, model\r\n\r\n\t# ========================================================================= #\r\n\t#                               Model Training                              #\r\n\t# ========================================================================= #\r\n\r\n\tdef remember(self, cur_state, action, reward, new_state, done):\r\n\t\tself.memory.append([cur_state, action, reward, new_state, done])\r\n\r\n\tdef _train_actor(self, samples):\r\n\t\tfor sample in samples:\r\n\t\t\tcur_state, action, reward, new_state, _ = sample\r\n\t\t\tpredicted_action = self.actor_model.predict(cur_state)\r\n\t\t\tgrads = self.sess.run(self.critic_grads, feed_dict={\r\n\t\t\t\tself.critic_state_input:  cur_state,\r\n\t\t\t\tself.critic_action_input: predicted_action\r\n\t\t\t})[0]\r\n\r\n\t\t\tself.sess.run(self.optimize, feed_dict={\r\n\t\t\t\tself.actor_state_input: cur_state,\r\n\t\t\t\tself.actor_critic_grad: grads\r\n\t\t\t})\r\n            \r\n\tdef _train_critic(self, samples):\r\n\t\tfor sample in samples:\r\n\t\t\tcur_state, action, reward, new_state, done = sample\r\n\t\t\tif not done:\r\n\t\t\t\ttarget_action = self.target_actor_model.predict(new_state)\r\n\t\t\t\tfuture_reward = self.target_critic_model.predict(\r\n\t\t\t\t\t[new_state, target_action])[0][0]\r\n\t\t\t\treward += self.gamma * future_reward\r\n\t\t\tself.critic_model.fit([cur_state, action], reward, verbose=0)\r\n\t\t\r\n\tdef train(self):\r\n\t\tbatch_size = 32\r\n\t\tif len(self.memory) < batch_size:\r\n\t\t\treturn\r\n\r\n\t\trewards = []\r\n\t\tsamples = random.sample(self.memory, batch_size)\r\n\t\tself._train_critic(samples)\r\n\t\tself._train_actor(samples)\r\n\r\nAnd am using tensorflow version 2.4.1\r\n", "@Davidelvis ,\r\n\r\nThe code provided is not complete hence it would be difficult for us to pinpoint the issue. Please share complete stand alone code to replicate the issue or a colab gist with the error reported.? That will allow us to determine the source of the issue easily.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48749\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48749\">No</a>\n"]}, {"number": 48748, "title": "win10  bazel Extracting Bazel installation...", "body": "i wanna build the tf2.4 version with rtx3070,cuda11,cudnn8 on win10 system\r\n\r\naccording to the official tutorial, I have built the msys2, visual studio 2019,and all the configuration including the path set\r\n\r\nhowever when i installed the bazel, when i open the cmd window, when i type bazel ,it just stagnate Extracting Bazel installation... and quit.\r\n\r\nwhy was that?SOS", "comments": ["@rainmakerMa,\r\nCould you please run the `bazel version` command and share the output with us.\r\n\r\nAlso make sure you have installed bazel as per the [official guide](https://docs.bazel.build/versions/master/install-windows.html). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Not a TF issue. Your system might be too slow for Bazel to fully unpack. In any case, this is a Bazel issue, should be opened in the Bazel repository.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48748\">No</a>\n"]}, {"number": 48747, "title": "How to reopen eager_execution after close it ?", "body": "system: Macos X\r\nTF version: 2.4.1\r\n\r\nAfter turn off eager_execution,  I get the follow error.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/yanqing/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-4-52f64b3b4137>\", line 12, in <module>\r\n    convert_variables_to_constants_v2(func)\r\n  File \"/Users/yanqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1068, in convert_variables_to_constants_v2\r\n    converter_data = _FunctionConverterData(\r\n  File \"/Users/yanqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 806, in __init__\r\n    self._build_tensor_data()\r\n  File \"/Users/yanqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 823, in _build_tensor_data\r\n    data = map_index_to_variable[idx].numpy()\r\n  File \"/Users/yanqing/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 619, in numpy\r\n    raise NotImplementedError(\r\nNotImplementedError: numpy() is only available when eager execution is enabled.\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\n\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# do something with tf1\r\n\r\nmodule = tf.keras.layers.PReLU()\r\nfunc = tf.function(lambda x0: module(x0))\r\nfunc = func.get_concrete_function(tf.TensorSpec((10, 24, 24, 3), dtype=tf.float32))\r\nconvert_variables_to_constants_v2(func)\r\n```", "comments": ["@YanqingWu \r\nwith respect to the error reported, this has been already reported, please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/27491#issuecomment-770848466) and let us know if it helps you resolve the error reported. [you may also refer to similar issue :#37052, [link](https://analyticsindiamag.com/beginners-guide-to-tensorflow-eager-execution-machine-learning-developers/)]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48747\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48747\">No</a>\n"]}, {"number": 48746, "title": "bugNodeDef expected inputs 'float, int32' do not match 1 inputs specified", "body": "### env\r\nwin10\r\npy3.6\r\ntf-1.15.2\r\ngpu1070\r\n-------------------------------------------------------------------------------------------------------------------------\r\nThis is the code I run. The failure caused by this function-- tf.train.import_meta_graph. i want to reload 'mobilenet_v2_1.4_224.ckpt.meta'. this pre-model is from [slim pre-model](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz).\r\n\r\n```python\r\ndef copy_var_from_ckpt(ckpt_path, meta_graph_path):\r\n    with tf.Session() as sess:\r\n        sess.run(tf.initialize_all_variables())\r\n        saver = tf.train.import_meta_graph(meta_graph_path)\r\n        saver.restore(sess, ckpt_path)\r\n        graph = tf.Graph().as_default()\r\n```\r\n\r\nThe following information is obtained\r\n\r\n```python\r\n File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\importer.py\", line 501, in _import_graph_def_internal\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs 'float, int32' do not match 1 inputs specified; Op<name=CrossReplicaSum; signature=input:T, group_assignment:int32 -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT, DT_INT32, DT_UINT32]>; NodeDef: {{node TPUReplicate/loop/CrossReplicaSum}}\r\n```\r\nHow to solve it?", "comments": ["I want to load the weights from the pre training model into an extended network structure. this is full code.\r\n```python\r\ndef copy_var_from_ckpt(session, graph01, dst_var_names, ckpt_path, meta_graph_path):\r\n    '''\r\n    session    \u65b0\u56fe\u7684\u4f1a\u8bddsess\r\n    dst_var_names   \u65b0\u56fe\u7684\u53d8\u91cf\u540d\u5b57\r\n    dst_vars     # \u65b0\u56fe\u7684\u53d8\u91cf\u8282\u70b9\r\n    ckpt_path   # \u539f\u59cbckpt\u8def\u5f84\r\n    meta_graph_path # \u539f\u59cb\u7684meta\u56fe\u6587\u4ef6\r\n    '''\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        with tf.Session() as sess:\r\n            saver = tf.train.import_meta_graph(meta_graph_path)\r\n            saver.restore(sess, ckpt_path)\r\n            v_names = []\r\n            for v in tf.trainable_variables():\r\n                v_names.append(v)\r\n                print(v.name,' --> ',v)\r\n            for dst_var_name in dst_var_names:\r\n                if dst_var_name.name in v_names:\r\n                    tensor = graph.get_tensor_by_name(dst_var_name)\r\n                    vs = graph01.get_tensor_by_name(dst_var_name)\r\n                    weight = sess.run(tensor)\r\n                    session.run(vs.assign(weight))\r\n\r\n```", "@chenglong0317 ,\r\n\r\nWe see that you are using tf version 1.15, there is no support for 1.x, please update to 2.x and let us know if you are using same issue.", "> \r\n> \r\n> @chenglong0317 ,\r\n> \r\n> We see that you are using tf version 1.15, there is no support for 1.x, please update to 2.x and let us know if you are using same issue.\r\n\r\nbut tf2.0 do not support slim. this code and pre-model is about slim. from :https://github.com/tensorflow/models/tree/master/research/slim", "> \r\n> \r\n> @chenglong0317 ,\r\n> \r\n> We see that you are using tf version 1.15, there is no support for 1.x, please update to 2.x and let us know if you are using same issue.\r\n\r\nlet me try", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@chenglong0317 ,\r\n\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status \r\n\r\nThanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48746\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48746\">No</a>\n"]}, {"number": 48745, "title": "TFlite on container problem", "body": "Hello, I run inference model TFlite on this Github: https://github.com/Mjrovai/TFLite_IA_at_the_Edge \r\n\r\nIt can run on raspbian 32 bit without any problem, but after I put it on the container. I got an error: \r\n\r\nTraceback (most recent call last):\r\n  File \"TFLite_IA_at_the_Edge/mMobile.py\", line 42, in <module>\r\n    model_path='./models/mobilenet_v1_1.0_224_quant.tflite')\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 204, in __init__\r\n    model_path, self._custom_op_registerers))\r\nValueError: Could not open './models/mobilenet_v1_1.0_224_quant.tflite'.\r\n\r\nI guess the problem may be TFlite doesn't support this image or something. I use base docker image jsurf/rpi-raspbian \r\n\r\nAny idea about this?", "comments": ["Please make sure the above model file is accessible in the container. Looks like there are possibilities that the file location is wrong or the model file is not available from the container.", "I can fix it now, it was because of the path on the container. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48745\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48745\">No</a>\n"]}, {"number": 48744, "title": "[INTEL MKL] Clean up of remapper code", "body": "Removed static macro `INTEL_MKL` and changed them to `IsMKLEnabled`, now it's fully supported runtime switch.\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["@Zantares sorry there are some conflicts to resolve, can you please check.Thank you ", "@rthadur @ezhulenev conflict has been resolved, please take a new look, thanks!", "@Zantares  Can you please resolve conflicts? Thanks!", "> @Zantares Can you please resolve conflicts? Thanks!\r\n\r\nDone, please help to review, thanks!", "@ezhulenev can you help to review again since the conflict is resolved? We have some internal issues which are  blocked by this PR, thanks! "]}, {"number": 48743, "title": "loss calculation for multi-dim samples in distributed strategy", "body": "Hi, I was reading the distributed strategy and found one issue in the official doc\r\nhttps://www.tensorflow.org/tutorials/distribute/custom_training\r\n\r\n> If labels is multi-dimensional, then average the per_example_loss across the number of elements in each sample. For example, if the shape of predictions is (batch_size, H, W, n_classes) and labels is (batch_size, H, W), you will need to update per_example_loss like: per_example_loss /= tf.cast(tf.reduce_prod(tf.shape(labels)[1:]), tf.float32)\r\n\r\nSince the last dimension is automatically reduced in \"NONE\" mode (https://github.com/tensorflow/tensorflow/issues/27190), then the manual reduce formula for the distributed.strategy should be\r\nper_example_loss /= tf.cast(tf.reduce_prod(tf.shape(labels)[1:**-1**]), tf.float32) # notice the addition of -1 in shape() to remove the accumulation for the last axis\r\n\r\nI don't know which place I should put this post to, but please update the relevant people who is in charge of the doc. Thanks.", "comments": ["Hi, \r\n\r\nThe source doc for this page is here:\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/custom_training.ipynb\r\n\r\nI need a few minutes to think about that loss reduction issue.", "I did a test to verify this.\r\n[test_dist-Copy1.ipynb.zip](https://github.com/tensorflow/tensorflow/files/6387269/test_dist-Copy1.ipynb.zip)\r\n", "[Note: if you post the notebook to GitHub without zipping it then people can open it directly in google Colab without leaving the browser by replacing `github.com` with `https://colab.sandbox.google.com/github/`]\r\n\r\nI see your point from #27190 that this is how it works.  I just really don't like it \ud83d\ude05.\r\n\r\n```\r\n(batch_size, H, W, n_classes) and labels is (batch_size, H, W), you will need to update per_example_loss like: per_example_loss /= tf.cast(tf.reduce_prod(tf.shape(labels)[1:]), tf.float32)\r\n```\r\n\r\nAnyway, the problem with just changing this line is that while the loss classes are [usually] a good way to implement loss functions, any callable can used.  So we have to say something generic, and not assume that anyone doing this is using one of the builtin losses with `Reduction.NONE`. Also, for this specific example `(batch_size, H, W, n_classes)` with `tf.keras.losses.SparseCategoricalCrossentropy` (the code just below) it's the `n_classes` dimension that gets reduced making the current code correct in that case.\r\n\r\nSo I think if we want to fix this, the right approach is to add:\r\n\r\n> Caution: `losses` typically take the average across the the last axis of the input  (even if you pass `reduction=Reduction.NONE` when creating a loss object). For `losses.CategoricalCrossEntropy` this reduces the `n_classes` dimension which is typically what you want. For losses that are calculated pointwise, like mean squared error or binary crossentropy `[batch, W, H]` will be reduced to `[batch, W]`, unless  you include a dummy axis like `[batch, W, H, 1]`. ", "I see your original intention now, to cover something more generic. But please add this **caution** to warn people like me who is a TF newbie and did not realize loss functions already reduce last axis even when Reduction is NONE (the naming \"NONE\" is really confusing...) Thanks!\r\n\r\nI will make sure next time when I submit a ticket I will submit a notebook rather than zip it..."]}, {"number": 48741, "title": "mbed/STM32F7 compiling issue with AllOpsResolver: undefined reference to `__SXTB16_RORn'", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 8cae746d8449c7dda5298327353d68613f16e798\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS (ST-Discovery-STM32F746NG)\r\n- Mbed version: mbed-cli==1.10.5\r\n- GNU Arm embedded toolchain version: 10-2020-q4-major\r\n\r\n**Describe the problem**\r\nWhile running mbed compilation on the tflite micro example `hello_world`, I experienced errors relating to ``undefined reference to `__SXTB16_RORn'``. Investigating further, it seems to be a problem with the `AllOpsResolver`, specifically when it calls `AddConv2D()` (details below). \r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI followed the changes to `tensorflow/lite/micro/examples/hello_world` from the recent [pull request](https://github.com/tensorflow/tensorflow/pull/48659) for `tensorflow/lite/micro/examples/micro_speech` which updated the `make` command and `disco_f746ng/Makefile.inc` to use `TARGET` and `OPTIMIZED_KERNEL_DIR` instead of `TAGS`. See [changes here](https://github.com/tensorflow/tensorflow/pull/48740). \r\n```bash\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=disco_f746ng OPTIMIZED_KERNEL_DIR=cmsis_nn generate_hello_world_mbed_project\r\n$ cd tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed\r\n$ mbed config root .\r\n$ mbed deploy\r\n$ mbed compile -m DISCO_F746NG -t GCC_ARM\r\n```\r\nAnd here is the output of the `mbed compile` command: \r\n```bash\r\n[mbed] Working path \"/home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed\" (library)\r\n[mbed] Program path \"/home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed\"\r\n[Warning] @,: Compiler version mismatch: Have 10.2.1; expected version >= 9.0.0 and < 10.0.0\r\nBuilding project mbed (DISCO_F746NG, GCC_ARM)\r\nScan: mbed\r\nLink: mbed\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.o: in function `arm_nn_mat_mult_nt_t_s8':\r\n/home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:109: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: /home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:110: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: /home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:116: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: /home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:123: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: /home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:134: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.o:/home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:135: more undefined references to `__SXTB16_RORn' follow\r\ncollect2: error: ld returned 1 exit status\r\n[ERROR] /usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.o: in function `arm_nn_mat_mult_nt_t_s8':\r\n/home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:109: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: /home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:110: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: /home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:116: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: /home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:123: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: /home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:134: undefined reference to `__SXTB16_RORn'\r\n/usr/share/gcc-arm-none-eabi-10-2020-q4-major/bin/../lib/gcc/arm-none-eabi/10.2.1/../../../../arm-none-eabi/bin/ld: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.o:/home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:135: more undefined references to `__SXTB16_RORn' follow\r\ncollect2: error: ld returned 1 exit status\r\n\r\n[mbed] ERROR: \"/home/user/.virtualenvs/mbed/bin/python\" returned error.\r\n       Code: 1\r\n       Path: \"/home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed\"\r\n       Command: \"/home/user/.virtualenvs/mbed/bin/python -u /home/user/Git/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM\"\r\n       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n---\r\n```\r\nIf I [comment out `AddConv2D()` in `tensorflow/lite/micro/all_ops_resolver.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/all_ops_resolver.cc#L33), the problem goes away. \r\nSimilarly in `tensorflow/lite/micro/examples/micro_speech`, if I replace the Op Resolver from `MicroMutableOpResolver` to `AllOpsResolver`, the same error above appears. \r\nI'm not sure if this is a problem upstream with [Mbed CLI](https://github.com/ARMmbed/mbed-cli/), or with the Arm CMSIS library. ", "comments": ["$ git clone https://github.com/user/repo.git\r\n# Clone a repo\r\n> Cloning into 'repo'...\r\n> remote: Counting objects: 66179, done.\r\n> remote: Compressing objects: 100% (15587/15587), done.\r\n> remote: Total 66179 (delta 46985), reused 65596 (delta 46402)\r\n> Receiving objects: 100% (66179/66179), 51.66 MiB | 667 KiB/s, done.\r\n> Resolving deltas: 100% (46985/46985), done.\r\n> warning: remote HEAD refers to nonexistent", "@wdjose I believe this is related the problem described in the second half of [this example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/kernels/cmsis_nn#example-2---mbed) and the fix there should allow you to compile.", "@patriklaurell Thanks! I just tried it, and you're right, it seems that the old \"cmsis_gcc.h\" is needed. After copying \"cmsis_gcc.h\", the program successfully compiles. \r\n\r\nI'm just copying the specific lines I followed from the example you quoted (in case anyone else experiences the issue): \r\n\r\n> Note: Mbed has a dependency to an old version of arm_math.h. Therefore you need to copy the newer version as follows:\r\n> ```\r\n> cp tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/\\\r\n> arm_math.h mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h\r\n>```\r\n>\r\n> There's also a dependency to an old cmsis_gcc.h, which you can fix with the following:\r\n> ```\r\n> cp tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/\\\r\n> cmsis_gcc.h mbed-os/cmsis/TARGET_CORTEX_M/cmsis_gcc.h\r\n> ```\r\n\r\nI also added these same lines to the [pull request](https://github.com/tensorflow/tensorflow/pull/48740) I created to fix the compilation of `example/hello_world`, similar to the [pull request](https://github.com/tensorflow/tensorflow/pull/48659) (for `example/micro_speech`) that incidentally was submitted by you as well. "]}, {"number": 48740, "title": "Fix/stm32f746 examples", "body": "This PR updates the mbed project generation for the hello_world and micro_speech examples for the disco_f746ng board. Based on a [recent PR](https://github.com/tensorflow/tensorflow/pull/48659) addressing [this issue](https://github.com/tensorflow/tensorflow/issues/46721) regarding \"the TAGS command line option is no longer supported in the TFLM Makefile\". ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48740) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48740) for more info**.\r\n\r\n@googlebot I signed it!"]}, {"number": 48739, "title": "Update jsoncpp to 1.9.4", "body": "The jsoncpp version in tensorflow was 1.9.2 which was almost 2 years\r\nold. This PR updates the jsoncpp to 1.9.4 which consists of OSS-Fuzz\r\nfixes (https://github.com/open-source-parsers/jsoncpp/releases/tag/1.9.4)\r\nthat is worth updating.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 48738, "title": "tfa.seq2seq.AttentionWrapperState : TypeError: __new__() missing 3 required positional arguments: 'alignments', 'alignment_history', and 'attention_state'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**The following is my code block which uses AttentionWrapper:**\r\n\r\n```\r\ndef decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \r\n                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\r\n    '''Create the decoding cell and attention for the training and inference decoding layers'''\r\n    \r\n    for layer in range(num_layers):\r\n        with tf.variable_scope('decoder_{}'.format(layer)):\r\n            lstm = tf.compat.v1.nn.rnn_cell.LSTMCell(rnn_size,\r\n                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\r\n            dec_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, \r\n                                                     input_keep_prob = keep_prob)\r\n    \r\n    output_layer = Dense(vocab_size,\r\n                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\r\n    \r\n    attn_mech = tfa.seq2seq.BahdanauAttention(rnn_size,\r\n                                                  enc_output,\r\n                                                  text_length,\r\n                                                  normalize=False,\r\n                                                  name='BahdanauAttention')\r\n\r\n    dec_cell = tfa.seq2seq.AttentionWrapper(dec_cell,\r\n                                                          attn_mech,\r\n                                                          rnn_size)\r\n            \r\n    \r\n    initial_state = tfa.seq2seq.AttentionWrapperState(enc_state[0],_zero_state_tensors(rnn_size,batch_size,tf.float32))\r\n     \r\n    with tf.variable_scope(\"decode\"):\r\n        training_logits = training_decoding_layer(dec_embed_input, \r\n                                                  summary_length, \r\n                                                  dec_cell, \r\n                                                  initial_state,\r\n                                                  output_layer,\r\n                                                  vocab_size, \r\n                                                  max_summary_length)\r\n    with tf.variable_scope(\"decode\", reuse=True):\r\n        inference_logits = inference_decoding_layer(embeddings,  \r\n                                                    vocab_to_int['<GO>'], \r\n                                                    vocab_to_int['<EOS>'],\r\n                                                    dec_cell, \r\n                                                    initial_state, \r\n                                                    output_layer,\r\n                                                    max_summary_length,\r\n                                                    batch_size)\r\n\r\n    return training_logits, inference_logits\r\n```\r\n\r\nWhen I run the below block:\r\n\r\n```\r\n# Build the graph\r\ntrain_graph = tf.Graph()\r\n# Set the graph to default to ensure that it is ready for training\r\nwith train_graph.as_default():\r\n    \r\n    # Load the model inputs    \r\n    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\r\n\r\n    # Create the training and inference logits\r\n    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\r\n                                                      targets, \r\n                                                      keep_prob,   \r\n                                                      text_length,\r\n                                                      summary_length,\r\n                                                      max_summary_length,\r\n                                                      len(vocab_to_int)+1,\r\n                                                      rnn_size, \r\n                                                      num_layers, \r\n                                                      vocab_to_int,\r\n                                                      batch_size)\r\n    \r\n    # Create tensors for the training logits and inference logits\r\n    training_logits = tf.identity(training_logits.rnn_output, 'logits')\r\n    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\r\n    \r\n    # Create the weights for sequence_loss\r\n    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\r\n\r\n    with tf.name_scope(\"optimization\"):\r\n        # Loss function\r\n        cost = tf.contrib.seq2seq.sequence_loss(\r\n            training_logits,\r\n            targets,\r\n            masks)\r\n\r\n        # Optimizer\r\n        optimizer = tf.train.AdamOptimizer(learning_rate)\r\n\r\n        # Gradient Clipping\r\n        gradients = optimizer.compute_gradients(cost)\r\n        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\r\n        train_op = optimizer.apply_gradients(capped_gradients)\r\nprint(\"Graph is built.\")\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-111-2482e3ce02af> in <module>\r\n      8 \r\n      9     # Create the training and inference logits\r\n---> 10     training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\r\n     11                                                       targets,\r\n     12                                                       keep_prob,\r\n\r\n<ipython-input-107-605aa3e33839> in seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size)\r\n     12     dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\r\n     13 \r\n---> 14     training_logits, inference_logits  = decoding_layer(dec_embed_input, \r\n     15                                                         embeddings,\r\n     16                                                         enc_output,\r\n\r\n<ipython-input-106-b74ed3efc070> in decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers)\r\n     24 \r\n     25 \r\n---> 26     initial_state = tfa.seq2seq.AttentionWrapperState(enc_state[0],_zero_state_tensors(rnn_size,batch_size,tf.float32))\r\n     27 \r\n     28     with tf.variable_scope(\"decode\"):\r\n\r\nTypeError: __new__() missing 3 required positional arguments: 'alignments', 'alignment_history', and 'attention_state'\r\n```\r\n\r\nPlease help.\r\n", "comments": ["$ git clone https://github.com/user/repo.git\r\n# Clone a repo\r\n> Cloning into 'repo'...\r\n> remote: Counting objects: 66179, done.\r\n> remote: Compressing objects: 100% (15587/15587), done.\r\n> remote: Total 66179 (delta 46985), reused 65596 (delta 46402)\r\n> Receiving objects: 100% (66179/66179), 51.66 MiB | 667 KiB/s, done.\r\n> Resolving deltas: 100% (46985/46985), done.\r\n> warning: remote HEAD refers to nonexistent", "I don't understand. Whar are you suggesting?", "@Jeet1994 ,\r\n\r\nI ran the code shared and face a different error, please find the [gist](https://colab.research.google.com/gist/tilakrayal/76580ef8070839db0c60bbda78d4772b/untitled48738.ipynb) here and share all dependencies to replicate the issue or share a colab gist with the reported error.\r\n\r\nThanks", "Hi @tilakrayal ,\r\n\r\nSorry for the delay.\r\n\r\nPlease find the collab link for the entire [code](https://colab.research.google.com/drive/1Nm24Qn6VN_TatOpAiElmxniZJe6eO0sY?usp=sharing)\r\n\r\nPlease help.", "@Jeet1994 ,\r\n\r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!", "@tilakrayal \r\n\r\nHi, \r\n\r\nThe code is complex, understood. Let me explain you the issue once again, the problem is with the usage of function `tfa.seq2seq.AttentionWrapperState` , which is a function defined under **tensorflow addons**. \r\n\r\nPlease look at the original code snippet added with the issue.\r\n\r\nRequest you to delegate the issue to someone who can help me in this matter, if you are unable to do so. \r\n\r\n\r\n", "@Jeet1994 \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nYou may refer to simialr error issue here: [link](https://github.com/tensorflow/tensorflow/issues/20727),[link2](https://stackoverflow.com/questions/37457207/python-typeerror-new-missing-1-required-positional-argument-namespace)", "Hello @Saduf2019 I am sorry, but I expected some help on the matter. I have asked this question on Stackoverflow without any input. If you cannot help, you may assign someone who can. \r\n\r\nPlease pardon me, but I expected better. ", "> Please find the collab link for the entire [code](https://colab.research.google.com/drive/1Nm24Qn6VN_TatOpAiElmxniZJe6eO0sY?usp=sharing)\r\n\r\n@Jeet1994 Thanks for your issue but code snippet you provided upstream is not minimal. \r\n\r\n> The code is complex, understood. Let me explain you the issue once again, the problem is with the usage of function `tfa.seq2seq.AttentionWrapperState` , which is a function defined under **tensorflow addons**.\r\n\r\nI see you have pinned the problem to usage of `tfa.seq2seq.AttentionWrapperState`\r\n\r\nIdeally you want to create a toy example for `tfa.seq2seq.AttentionWrapperState` with minimal code to reproduce the reported behavior in your case. Also [tf-addon](https://github.com/tensorflow/addons/issues) repo can be a good platform for the issue.\r\n\r\nTagging maintainers /tensorflow_addons/seq2seq/  @guillaumekln to take a further look.\r\nThanks!", "@Jeet1994, you should call the method [`AttentionWrapper.get_initial_state`](https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/AttentionWrapper?hl=fr#get_initial_state) instead of manually building the initial state. For example:\r\n\r\n```python\r\n# Get the initial zero state.\r\ninitial_state = dec_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\r\n\r\n# Initialize the cell state from the encoder.\r\ninitial_state = initial_state.clone(cell_state=enc_state)\r\n``` \r\n\r\nAlso note that you are using RNN cells from `tf.compat.v1` which is not supported in TensorFlow Addons. You should update your code to TensorFlow 2.", "Thank you @guillaumekln . I solved the problem with your guidance. All the best to all who tried to help me. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48738\">No</a>\n"]}, {"number": 48737, "title": "Update sqlite to latest sqlite-amalgamation-3350500", "body": "This PR updates sqlite to the latest sqlite-amalgamation-3350500\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 48736, "title": "Allow different shapes for `y_true` and `y_pred` in graph mode", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, running Keras in graph mode means that `y_true` and `y_pred` in the `loss` function passed to `Model.fit()` must have the same shape. Although this works in eager mode, I would like to be able to train models where `y_true` and `y_pred` are different shapes while benefiting from graph mode. My use case is that `y_true` has an extra final dimension of size 2, where index 0 gives the labels and index 1 gives a sample weight which I use for weighting my loss function (e.g. multiplying some pixels by zero).\r\n\r\nThis relates to the following issue in the Keras GitHub but there wasn't a satisfying solution provided: https://github.com/keras-team/keras/issues/4781\r\n\r\nFor the time being I'm simply adding an extra dimension of size 1 to the `y_pred` from my model using `tf.expand_dims`. However, this is inconvenient for the rest of my codebase where I now have to slice out the final dimension of my model's predictions.\r\n\r\nApologies if this has already been addressed in a later version of TF or if I've missed something...\r\n", "comments": ["Keras supports sample weighting in a separate argument, why does that not cover your use case?\r\n", "@deeb02 Thank you for your reply. Does the passing `sample_weight` as a third element from a data loader in `Model.fit` work with custom metrics?", "@tom-andersson Yes, it should work with custom metrics or loss but your custom metrics or loss class must support sample weights"]}, {"number": 48735, "title": "Remove deprecated operations in the datasets", "body": "In reuters.py and imdb.py, there are ndarray creations with ragged\r\narray, but it's deprecated now. So I added parameter \"dtype\" to them.", "comments": ["@luisleee , could you please clarify what was deprecated exactly?", "@deeb02 ok, please see here: https://numpy.org/doc/stable/release/1.19.0-notes.html#deprecate-automatic-dtype-object-for-ragged-input.", "@luisleee thank you! That makes sense.", "@luisleee  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned Finished."]}, {"number": 48734, "title": "Question about Cannot Import", "body": "This are parts of my code\uff1a\r\n \r\n![image](https://user-images.githubusercontent.com/78319233/115960832-21e6ce00-a546-11eb-92c2-21a28665f3d8.png)\r\n![image](https://user-images.githubusercontent.com/78319233/115960837-2b703600-a546-11eb-9027-ca04cc2e0a12.png)\r\n\r\nfrom tensorflow.python.eager.context import get_config\r\n\r\nImportError: cannot import name 'get_config'\r\ncan someone tell me why?\r\n", "comments": ["I tried to  import all the packages and   not getting any errors that you have faced. Please check the gist [here](https://colab.research.google.com/gist/saikumarchalla/d1eb9c29d5e1b7dd91ceaa423d5900e4/untitled67.ipynb).\r\n\r\nPlease use tensorflow.keras instead of keras  and  make sure you use Tensorflow 2.x  and let us know still if that doesn't resolve the  error. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48734\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48734\">No</a>\n"]}, {"number": 48733, "title": "INSTALL ERROR: No matching distribution found for tensorflow on RASPERRY PI 4 ARM64 UBUNTU 18.04 ", "body": "**System information**\r\n- Hardware: Raspberry Pi 4 Model B Rev 1.4\r\n- Architecture: aarch64\r\n- OS Platform and Distribution: Ubuntu 18.04.1\r\n- Python version: 3.7.5 [64-bit]\r\n- Pip version: 21.0.1\r\n- Installed using: virtualenv and pip\r\n\r\n**Describe the problem**\r\nI get the following error when I try to install: (I tried other versions of python including 3.5, 3.6, 3.7 and 3.8)\r\n`ERROR: Could not find a version that satisfies the requirement tensorflow`\r\n`ERROR: No matching distribution found for tensorflow`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCreated virtual environment, activated it and then:\r\n`pip3 install tensorflow`\r\n\r\n\r\n\r\n", "comments": ["$ git clone https://github.com/user/repo.git\r\n# Clone a repo\r\n> Cloning into 'repo'...\r\n> remote: Counting objects: 66179, done.\r\n> remote: Compressing objects: 100% (15587/15587), done.\r\n> remote: Total 66179 (delta 46985), reused 65596 (delta 46402)\r\n> Receiving objects: 100% (66179/66179), 51.66 MiB | 667 KiB/s, done.\r\n> Resolving deltas: 100% (46985/46985), done.\r\n> warning: remote HEAD refers to nonexistent", "It seems that the pip package for TF2.* is not available for ARM64 yet, that is why you are getting an error message. You could try installing TF 2.3 using the wheel directly.\r\nFor example: `pip install https://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-2.3.0rc2-cp35-none-linux_armv6l.whl` . Please check this [link](https://www.tensorflow.org/install/pip#package-location) for more info.\r\nNOTE: That wheel was build for Pi3, if that doesn't work, you can try building TF from [source]( https://github.com/tensorflow/build/tree/master/raspberry_pi_builds).\r\n\r\n", "@hassan-shehawy ,\r\n\r\nPlease take a look at [DnPlas](https://github.com/tensorflow/tensorflow/issues/48733#issuecomment-827267559)   comment and let us know if you are still facing the same issue? Thanks!", "Thank you, but that wheel is for ARM32 and when I tried it, I got the error:\r\n`ERROR: tensorflow-2.3.0rc2-cp35-none-linux_armv6l.whl is not a supported wheel on this platform`\r\nI also tried another wheel made for ARM64 from [here](https://qengineering.eu/install-tensorflow-2.2.0-on-raspberry-64-os.html) and could install successfully but got an error when tried to import (described in #48761 )\r\nI will try building from source and update here.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48733\">No</a>\n", "I'm adding this comment to help anyone reading this and having same problems. I tried different things recommended by many persons trying to help, but I always got stuck with something. I think this is because it's usually very easy to overlook some details when I ask for help for my specific setup (RPI4+ARM64+Ubuntu18) and I found out that it's very common to receive help with the assumption that I'm using Raspbian or that my OS is 32-bit or I'm asking for TF lite or even thinking that Ubuntu 18 would be the same as 20. \r\n\r\nSo what ended up working for me is building it from source following the official documentation tutorial [here](https://www.tensorflow.org/install/source) and it took 22 hours to compile. Don't be intimidated by the process; I know having a wheel file and just pip install looks way easier, but really building from source is quite straightforward. I also have the wheel file if you want (I will probably upload it on google drive or dropbox)\r\n\r\nBecause of the so many problems popping up, I don't remember them or their source. However, there were 4 errors that happened because I didn't give proper attention to some details:\r\n1. Double check the .bazelversion file in the TF folder and make sure that this version really exists for ARM64 (at one point this created a problem for me because for a reason I don't know it was 1.3 and this version doesn't exist for aarch64)\r\n2. If you are using a virtual environment: You *must* do the configuration (via the configure.py file) before building, don't forget to do this :-)\r\n3. If you didn't create it with --system-site-packages, make sure you have got everything you need (numpy, keras-preprocessing ...)\r\n4. I got an error when I tried to import TF complaining about the version of numpy (I don't remember it). I solved it by `pip install --upgrade numpy`\r\n", "@hassan-shehawy Hi !\r\nI've been straggling with the exact same issues for days..\r\nCould you please share you wheels ?", "Hi, here's the wheel: (sorry to repeat it, but it's for TF 2.4 on ARM64, Ubuntu 18.04 and python 3.7 just to be 100% sure :-)\r\n\r\n https://drive.google.com/file/d/12hy4RdKPQgCImvr8pGt4nZyz2qmg3pd8/view?usp=sharing", "Hi, here's the wheel: (sorry to repeat it, but it's for TF 2.4 on ARM64, Ubuntu 18.04 and python 3.7 just to be 100% sure :-)\r\n\r\n https://drive.google.com/file/d/12hy4RdKPQgCImvr8pGt4nZyz2qmg3pd8/view?usp=sharing"]}]