[{"number": 4038, "title": "Disable export_test in OSS builds temporarily", "body": "http://b/31032996\n", "comments": []}, {"number": 4037, "title": "R0.10", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 4036, "title": "distributed tensorflow \u201csocket error, connection refused\u201d", "body": "When I run the following  distributed tensorflow code.  It always show the error:  \"socket error: connection refused.\" \n\nimport tensorflow as tf\nimport input_data\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\", \n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\", \n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\ndef main(_):\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  server = tf.train.Server(cluster.as_cluster_def(), \n                           job_name=FLAGS.job_name, \n                           task_index=FLAGS.task_index)\n\n  if FLAGS.job_name == \"ps\" :\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index, \n        cluster=cluster)):\n      x = tf.placeholder(tf.float32, [None, 28_28])\n      y_ = tf.placeholder(tf.float32, [None, 10])\n      W_h1 = weight_variable([28_28, 512])\n      b_h1 = bias_variable([512])\n      h1 = tf.nn.sigmoid(tf.matmul(x, W_h1) + b_h1)\n      W_out = weight_variable([512, 10])\n      b_out = bias_variable([10])\n      y = tf.nn.softmax(tf.matmul(h1, W_out) + b_out)\n\n```\n  loss = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y)))\n  global_step = tf.Variable(0)\n  train_op = tf.train.AdagradOptimizer(0.01).minimize(\n      loss, global_step=global_step)\n  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  saver = tf.train.Saver()\n  summary_op = tf.merge_all_summaries()\n  init_op = tf.initialize_all_variables()\n\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                         logdir=\"./train_logs\",\n                         init_op=init_op,\n                         summary_op=summary_op,\n                         saver=saver,\n                         global_step=global_step,\n                         save_model_secs=600)\n\nsess = sv.prepare_or_wait_for_session(server.target)\n\nsv.start_queue_runners(sess)\n\nmnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\nstep = 0\nwhile not sv.should_stop() and step < 20000:\n  batch_xs, batch_ys = mnist.train.next_batch(50)\n  if step % 100 == 0:\n    print \"job : %s/%s\" % (FLAGS.job_name,FLAGS.task_index), \"step : \", step, \",training accuracy :\", sess.run(accuracy, feed_dict={x: batch_xs, y_: batch_ys})\n  _, step = sess.run([train_op, global_step], feed_dict={x: batch_xs, y_: batch_ys})\n\nsaver.save(sess, \"./train_logs/mlp.ckpt\")\n```\n\nif **name** == \"**main**\":\n  tf.app.run()\n\nHere is a worker log: \n\n`I0825 17:17:56.155537454   42583 socket_utils_common_posix.c:170] Disabling AF_INET6 sockets because ::1 is not available.\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {10.51.145.204:30125, 10.51.147.155:30145, 10.51.147.206:30133, 10.51.81.205:30109, 10.51.145.207:30131, 10.51.148.205:30132, 10.50.85.231:30133, 10.51.147.208:30103, 10.51.147.195:30130, 10.51.150.71:30121}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:30129, 10.51.145.204:30109, 10.51.150.8:30138, 10.51.81.205:30121, 10.51.151.134:30111, 10.51.148.143:30135, 10.51.148.148:30103, 10.51.150.205:30140, 10.51.150.9:30113, 10.51.145.131:30116, 10.51.145.200:30105, 10.51.147.144:30138, 10.51.147.155:30102, 10.50.216.39:30147, 10.51.145.198:30111, 10.51.151.135:30133, 10.50.145.197:30130, 10.51.147.143:30135, 10.50.146.231:30143, 10.50.146.221:30100}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:30129\nE0825 17:17:56.489036676   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.145.204:30125': socket error: connection refused\nE0825 17:17:56.489142227   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.147.155:30145': socket error: connection refused\nE0825 17:17:56.489182927   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.147.206:30133': socket error: connection refused\nE0825 17:17:56.489211389   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.81.205:30109': socket error: connection refused\nE0825 17:17:56.489271981   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.145.207:30131': socket error: connection refused\nE0825 17:17:56.489308098   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.148.205:30132': socket error: connection refused\nE0825 17:17:56.489330266   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.50.85.231:30133': socket error: connection refused\nE0825 17:17:56.489361511   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.147.208:30103': socket error: connection refused\nE0825 17:17:56.489796035   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.150.71:30121': socket error: connection refused\nE0825 17:17:56.489996076   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.151.134:30111': socket error: connection refused\nE0825 17:17:56.490013575   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.148.143:30135': socket error: connection refused\nE0825 17:17:56.490021842   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.150.205:30140': socket error: connection refused\nE0825 17:17:56.490028295   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.148.148:30103': socket error: connection refused\nE0825 17:17:56.490035855   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.150.9:30113': socket error: connection refused\nE0825 17:17:56.490275270   42954 tcp_client_posix.c:173]     failed to connect to 'ipv4:10.51.145.131:30116': socket error: connection refused`\n\nAll the workers are seemed to be connected to each other.  It makes me confused. \n\nAnyone can help with this? Thank you!\n", "comments": ["@lebinhe Please could you provide at least the following information:\n- All the software versions (as requested in the TensorFlow issues template)\n- The exact command lines you are running  \n- The configuration of your cluster machines -  i.e. are they running firewalls etc\n\nAlso, could you please expand on your comment:\n\n> All the workers are seemed to be connected to each other. It makes me confused.\n\nFrom the log messages, it seems that the workers are unable to talk to each other:\n\n> tcp_client_posix.c:173] failed to connect to 'ipv4:10.51.151.134:30111': socket error: connection refused\n", "### Environment info\n\nOperating System:CentOS 3.10\n1. TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n2. tensorflow.**version**: 0.10.0rc0\n3. command line: python mlp_mnist_dist.py --ps_hosts=10.51.145.204:30125,10.51.147.155:30145,10.51.147.206:30133,10.51.81.205:30109,10.51.145.207:30131,10.51.148.205:30132,10.50.85.231:30133,10.51.147.208:30103,10.51.147.195:30130,10.51.150.71:30121 --worker_hosts=10.51.145.204:30129,10.51.145.204:30109,10.51.150.8:30138,10.51.81.205:30121,10.51.151.134:30111,10.51.148.143:30135,10.51.148.148:30103,10.51.150.205:30140,10.51.150.9:30113,10.51.145.131:30116,10.51.145.200:30105,10.51.147.144:30138,10.51.147.155:30102,10.50.216.39:30147,10.51.145.198:30111,10.51.151.135:30133,10.50.145.197:30130,10.51.147.143:30135,10.50.146.231:30143,10.50.146.221:30100 --job_name=worker --task_index=0\n\nOther command lines are similar except the job_name and task_index.  I will not enumerate all of them because there are many workers. \n\nI didn't have an accurate comment of the log. @prb12 is right. It seems that the workers are unable to talk to each other. When the worker number is small, there will be no such error. But when the number of workers is more, dozens of workers, connection refused will always appear.\n", "@lebinhe \nPlease note: officially, we only support the following OS configurations:\n\n> Operating systems: Ubuntu Linux 14 LTE to 16. Mac OS X El Capitan. CentOS 7 and above\n\nThese messages are coming from the GRPC layer as it is trying to connect to other processes.  With a large number of workers, I can imagine a number of things happening:\n1) Workers take a while to start and so connection requests are made before the other end is ready.  I would hope that gRPC has a retry loop here, and perhaps the messages are benign (@mrry ?)\n2) Perhaps your OS has some firewall/anti-DDOS mechanism which is preventing large numbers of connections being established in a short time period?   You would probably have to look at a packet level trace to see if this is happening.\n\nIf it is the first problem (namely an overly-verbose connection retry loop) then you may get lots of 'error messages', but eventually all the workers manage to get connected and training proceeds correctly.  Is this what you meant by the following:?\n\n> All the workers are seemed to be connected to each other. \n\nOr do the worker processes terminate after printing the error message?\n", "@prb12 Yes, gRPC will retry connection (after printing the \"connection refused\" error message).\n", "@mrry I also met the same problem, could give me some advice?\n", "@prb12  \nSorry for not replying in time.\nActually the OS version is CentOS 7, kernel version is 3.10. \n\nI replaced the github version(0.10rc) with the Nightly version(0.10rc).  \nThe \"connection refused\" error is gone.\n", "Sounds like the issue is resolved. Thanks for following up @lebinhe.\n"]}, {"number": 4035, "title": "build tensorflow from source sufferring from slow network", "body": "this is not a bug, but really puzzle me. When everytime I try to build tf from source, bazel keep cloning grpc, protobuf and download something else, as my network is too slow....so this takes really a long time, is there a better way to do this?\n", "comments": ["Found a solution, we can modify WORKSPACE to change the url to a local path, by using local_repository\nhttp://bazel.io/docs/be/workspace.html#new_git_repository\n", "@argman hi, can you explain how exactly you changed the WORKSPACE file please, i've got the save problem as you, thanks.\n", "@gaussic , for example\nnew_http_archive(\n  name = \"dagre\",\n  build_file = \"bower.BUILD\",\n  url = \"https://github.com/cpettitt/dagre/archive/v0.7.4.tar.gz\",\n  strip_prefix = \"dagre-0.7.4\",\n)\nyou can download dagre from the link directly and store in a local path like /home/argman/dagre, then change the http archive to\nnew_local_repository(\n  name = \"dagre\",\n  build_file = \"bower.BUILD\",\n  path = \" /home/argman/dagre\"\n)\n", "@argman thank you, that's a good example, though the WORKSPACE needs to change a lot.\n"]}, {"number": 4034, "title": "cuda_configure: Use env variables that match those used in configure script", "body": "Issue #4002 \n", "comments": []}, {"number": 4033, "title": "tensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them", "body": "I am running `distributed tensorflow` version of `deep mnist` [(link)](https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html) (`asynchronous` data parallelism on different machines with gpu). The batch_size I have taken for one epoch is 1000 images and I am running 1000 epochs. My architecture has one `ps` on machine 1 and two workers, with `worker task_index=0` as `chief` on machine 2 and machine 1 has `worker task_index=1`.\n\nI really don't understand why I get this error sometimes when I run `worker task_index=1` on machine 1. The error does not persist though if I try running it a couple of times.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\ncannot import name hashtable\ncannot import name hashtable\ncannot import name hashtable\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:118] Found device 0 with properties: \nname: GeForce GTX 750 Ti\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\npciBusID 0000:02:00.0\nTotal memory: 2.00GiB\nFree memory: 125.53MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:02:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 125.53M (131629056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {172.25.1.127:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {172.25.1.108:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:203] Started server with target: grpc://localhost:2222\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nTraceback (most recent call last):\n  File \"dist_trainer_deepMnist_v2.py\", line 205, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"dist_trainer_deepMnist_v2.py\", line 157, in main\n    with sv.managed_session(server.target) as sess:\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 797, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 958, in managed_session\n    start_standard_services=start_standard_services)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 722, in prepare_or_wait_for_session\n    max_wait_secs=max_wait_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 351, in wait_for_session\n    is_ready, not_ready_msg = self._model_ready(sess)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 437, in _model_ready\n    return self._ready(self._ready_op, sess, \"Model not ready\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 406, in _ready\n    ready_value = sess.run(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 2402 elements; but when writing their indices, saw 19 elements.\n     [[Node: report_uninitialized_variables/Where = Where[_device=\"/job:ps/replica:0/task:0/cpu:0\"](report_uninitialized_variables/Reshape_1)]]\nCaused by op u'report_uninitialized_variables/Where', defined at:\n  File \"dist_trainer_deepMnist_v2.py\", line 205, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"dist_trainer_deepMnist_v2.py\", line 153, in main\n    save_model_secs=600)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 310, in __init__\n    ready_op=ready_op, ready_for_local_init_op=ready_for_local_init_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 399, in _init_ready_op\n    ready_op = variables.report_uninitialized_variables()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 1031, in report_uninitialized_variables\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 888, in boolean_mask\n    return _apply_mask_1d(tensor, mask)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 863, in _apply_mask_1d\n    indices = squeeze(where(mask), squeeze_dims=[1])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2663, in where\n    result = _op_def_lib.apply_op(\"Where\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2333, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1252, in __init__\n    self._traceback = _extract_stack()\n```\n\nI tried looking for a reason as to why this error occurs, but couldn't find one. One clue is this part in the error\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 2402 elements; but when writing their indices, saw 19 elements.\n [[Node: report_uninitialized_variables/Where = Where[_device=\"/job:ps/replica:0/task:0/cpu:0\"](report_uninitialized_variables/Reshape_1)]]\nCaused by op u'report_uninitialized_variables/Where',\n```\n\nAnother clue is \n\n```\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 125.53M (131629056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n```\n\nthis is there when I am running the ps on the same machine which is also using the gpu. It'll be great if someone can explain why this happens?\n\n(Asked the same on [stack overflow](http://stackoverflow.com/questions/39130553/tensorflow-python-framework-errors-invalidargumenterror-whereop-race-condition?noredirect=1#comment65613229_39130553))\n", "comments": ["Yes, this does look like a bug, but we tried writing a simple test case and couldn't reproduce the error.\n\nCould you please provide the info requested in the TensorFlow issues reporting template (especially OS and software versions, installaton method)\n\nIt would also be useful to have a copy of the exact python your are running and the command lines required to reproduce the error.\n", "Apologies, I skipped the template format while raising the issue.\n\n### Environment info\n\nOperating System: Ubuntu 14.04 on both machines (Machine 1 and 2)\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nMachine 1 (for my experiment) with worker task_index=0 :\n\n```\n-rwxrwxrwx 1 root root   322936 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxrwxrwx 1 root root   383336 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so.7.5.18\n-rwxrwxrwx 1 root root   720192 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704 Aug 18 08:01 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudnn.so\n-rwxr-xr-x 1 root root 60696704 Aug 18 08:01 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudnn.so.5\n-rwxr-xr-x 1 root root 60696704 Aug 18 08:01 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Aug 18 08:01 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudnn_static.a\n```\n\nMachine 2 (for my experiment) with ps task_index=0 and worker task_index=1 :\n\n```\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Aug 22 17:36 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Aug 22 17:36 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudnn.so.5 -> libcudnn.so.5.0.5\n-rwxr-xr-x 1 root root 59909104 Aug 22 17:36 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 58775484 Aug 22 17:36 /home/username/../../usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nMachine 1\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\ncannot import name hashtable\ncannot import name hashtable\ncannot import name hashtable\n0.10.0rc0\n```\n\nMachine 2\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.10.0rc0\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\nMachine 1: ce3572a08b9ecfa5c8dd94921c2011f37b58e608\nMachine 2: 78f721a849300d21035f275cc806a617541be360\n1. The output of `bazel version`\n\nMachine 1: \n\n```\nExtracting Bazel installation...\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n```\n\nMachine 2: \n\n```\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n```\n\n### Steps to reproduce\n\nMentioned above in first comment\n\n### What have you tried?\n\nMentioned above in first comment\n\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nMentioned above in first comment\n", "@abhijayghildyal We could do with more precise information for 'steps to reproduce`.\n\nIn particular, the link you provide is to a deep mnist tutorial which isn't distributed.\nAre you modifying this according to the [distributed tensorflow tutorial](https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html)   \n\nThe distributed TF turorial uses a 4 machine configuration, but you describe something which runs 4 tasks on 2 machines:\n\n> My architecture has one ps on machine 1 and two workers, with worker task_index=0 as chief on machine 2 and machine 1 has worker task_index=1.\n\nIt would be very useful to have the exact Python code you are running and the exact command lines you invoke on each of the machines.  There is a chance this may be responsible for the 'race' being reported.\n\nHaving said that, @mrry and @keveman have looked at the errors and believe this is most likely an Eigen related bug in the `WhereOp` ... @benoitsteiner could you see if anything looks suspicious?\n", "I made a simple experiment with deepMNIST for bench marking results. I made minimum changes to the codes mentioned in the TensorFlow documentation (deepMNIST and distributed tensorflow). I used two machines for running three tasks, machine 1 had ps task_index=0 and worker task_index=1, and machine 2 had worker task_index=0.\n\n```\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\nimport tempfile\nimport time\nimport numpy as np\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"folder_no\", \"\", \"save summary in this folder\")\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_integer(\"epochs\", \"\", \"no of epochs\")\ntf.app.flags.DEFINE_string(\"weights_name\", \"\", \"weight file name\")\n\n\nFLAGS = tf.app.flags.FLAGS\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\ndef main(_):\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n\n  if FLAGS.job_name == \"ps\":\n    server.join()\n    with tf.device(\"/job:ps/task:0\"):\n      global_step = tf.Variable(0)\n      print global_step\n\n  elif FLAGS.job_name == \"worker\":\n\n    # Load data\n    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n\n      x = tf.placeholder(tf.float32, shape=[None, 784])\n      y_ = tf.placeholder(tf.float32, shape=[None, 10])\n\n      W = tf.Variable(tf.zeros([784,10]))\n      b = tf.Variable(tf.zeros([10]))\n\n      y = tf.nn.softmax(tf.matmul(x,W) + b)\n\n\n      # Build model...\n      # loss = 0.5\n\n      ##### deepMNIST code. Additions to the skeleton of distributed tensorflow code (taken from official documentation) #####\n      W_conv1 = weight_variable([5, 5, 1, 32])\n      b_conv1 = bias_variable([32])\n\n\n      x_image = tf.reshape(x, [-1,28,28,1])\n\n\n\n      h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n      h_pool1 = max_pool_2x2(h_conv1)\n\n\n      W_conv2 = weight_variable([5, 5, 32, 64])\n      b_conv2 = bias_variable([64])\n\n      h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n      h_pool2 = max_pool_2x2(h_conv2)\n\n\n      W_fc1 = weight_variable([7 * 7 * 64, 1024])\n      b_fc1 = bias_variable([1024])\n\n      h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n      h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n\n      keep_prob = tf.placeholder(tf.float32)\n      h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n\n      W_fc2 = weight_variable([1024, 10])\n      b_fc2 = bias_variable([10])\n\n      y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n\n      cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n\n      tf.scalar_summary('cross entropy', cross_entropy)\n\n      correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n      tf.scalar_summary('accuracy', accuracy)\n\n      ##################################################################################\n      global_step = tf.Variable(0)\n\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\n          cross_entropy, global_step=global_step)\n          # loss, global_step=global_step)\n\n      saver = tf.train.Saver()\n      summary_op = tf.merge_all_summaries()\n      init_op = tf.initialize_all_variables()\n      merged = summary_op\n\n\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             saver=saver,\n                             global_step=global_step,\n                             save_model_secs=600)\n\n    # The supervisor takes care of session initialization, restoring from\n    # a checkpoint, and closing when done or an error occurs.\n    with sv.managed_session(server.target) as sess:\n      # Loop until the supervisor shuts down or 1000000 steps have completed.\n\n      summaries_dir = 'summaries/'\n\n      train_writer = tf.train.SummaryWriter(summaries_dir + FLAGS.folder_no + '/train', sess.graph)\n      test_writer = tf.train.SummaryWriter(summaries_dir + FLAGS.folder_no + '/test', sess.graph)\n\n      step=0\n      time_start = time.time()\n      while not sv.should_stop() and step < FLAGS.epochs:\n        batch = mnist.train.next_batch(1000)\n        train_feed = {x: batch[0], y_: batch[1], keep_prob: 0.5}        \n\n        # Run a training step asynchronously.\n        _, step = sess.run([train_op, global_step], feed_dict=train_feed)\n        print \"step: \",step\n\n        summary, _ = sess.run([merged, train_op], train_feed)\n        train_writer.add_summary(summary, step)\n\n      time_end = time.time()\n      training_time = time_end - time_start\n      print(\"Training elapsed time: %f s\" % training_time)\n\n      np.savetxt(\"weights_dist_\"+FLAGS.weights_name+\".csv\", W_fc2.eval(session=sess), delimiter=\",\")\n\n\n      # Test\n      for i in xrange(10):\n        testSet = mnist.test.next_batch(50)\n        summary, acc = sess.run([merged, accuracy], feed_dict={ x: testSet[0], y_: testSet[1], keep_prob: 1.0})\n        test_writer.add_summary(summary, i)\n        print('Accuracy at step %s: %s' % (i, acc))\n\n\n    # Ask for all the services to stop.\n    sv.stop()\n\nif  __name__ == \"__main__\":\n  tf.app.run()\n```\n\nWhen i run the three tasks on three different machines, the error does not occur.\n", "Thanks for the info.  Could you also supply the command lines you used in the configuration where the error happens?\n", "Apologies, I did not get what you mean by 'configuration where the error happens'.\nIf you mean the commands I used to run the code, then:\nOn machine 1:\n\n```\npython dist_trainer_deepMnist.py --ps_hosts=machine1:2223 --worker_hosts=machine2:2222,machine1:2222 --folder_no=1 --job_name=ps --weights_name=weights_1 --epochs=1000 --task_index=0\npython dist_trainer_deepMnist.py --ps_hosts=machine1:2223 --worker_hosts=machine2:2222,machine1:2222 --folder_no=1 --job_name=worker --weights_name=weights_1 --epochs=1000 --task_index=1\n```\n\nOn machine 2:\n\n```\npython dist_trainer_deepMnist.py --ps_hosts=machine1:2223 --worker_hosts=machine2:2222,machine1:2222 --folder_no=1 --job_name=worker --weights_name=weights_1 --epochs=1000 --task_index=0\n```\n", "> When i run the three tasks on three different machines, the error does not occur.\n\n@mrry  I've now seen two issues where strange things happen when we are running more than one TF task on the same machine.  I can't think of any way this might affect execution other than timing?\n", "@prb12 There could be some contention between two users of the same GPU, but I can't see how that would affect the `WhereOp`, which runs on CPU, with input data that comes from the CPU. (I suppose it might take a trip via the GPU to do some of the reductions, depending on what host-memory kernels are registered, but I can't see how that would cause racy behavior unless something in the GPU->CPU transfer is very broken....)\n", "@mrry - Thanks.  I agree this is very unlikely.\n\n@abhijayghildyal  After taking a look at the training program, I'm now wondering if this is something to do with the use of `tf.train.replica_device_setter` ?\n\nIn particular, you are using parameter servers, but when you call this method you do not make any mention of `ps_tasks`.  If you read the [documentation](https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#replica_device_setter) you will see that `ps_tasks` defaults to zero, but you probably need to set it to 1 or pass in your `cluster_spec`.\n\nI can't immediately think how this would go wrong, but it seems like this is worth fixing before spending time on the other problem.\n", "Thank you for looking into this issue. For my actual work I change my code when I am running with more number of `ps_tasks` and use a different implementation with `cluster_spec`, but for that implementation I always run the ps on CPU-only configuration by masking my cuda device. I would surely post if I face the same error with it.\n\nThere are some observations, which might be useful to you:\n- This problem only occurred when ps was run on the same machine (GPU enabled configuration) as the worker, and was never seen when it was running on a different machine. \n- There was one more condition when this error was not seen i.e. if  `CUDA_VISIBLE_DEVICES=''` is added while running the code, (eg: `CUDA_VISIBLE_DEVICES='' python dist_trai...`). This way my ps did not use the CUDA device and never ran into `CUDA_ERROR_OUT_OF_MEMORY`, hence avoiding the issue.\n"]}, {"number": 4032, "title": "To be able to epoch num info from tf.train.string_input_producer", "body": "In cifar10 example, we want learning rate decay base on epoch num, the code there use NUM_INSTANCES/batch_size to get steps per epoch, but I wonder if we can directly use epoch num during training ?\nIt is usefull when you have big train data you do not know it's size pre, like on hdfs.\nI've posted one quesion on stack overflow\n\n[(http://stackoverflow.com/questions/39101150/how-to-get-epoch-num-info-from-tf-train-string-input-producer)]\n\nlooks like \ntf.get_default_graph().get_tensor_by_name('input_train/input_producer/limit_epochs/epochs:0')\ndoes not fit the need.\n\nAny suggestions, or it is not possible to get epoch num directly ?\n", "comments": ["Stack overflow is the correct venue for questions like this.\n"]}, {"number": 4031, "title": "SupervisedSession cannot run dict fetchs.", "body": "### Installed from source, 0.10.0rc0.\n### `supervised_session.SupervisedSession`  under  `contrib.learn` cannot run dict fetchs:\n\n 1.Document says that it's same as `tf.Session.run()` and confused.  \n\n```\n    def run(self, fetches, feed_dict=None, options=None, run_metadata=None):\n    \"\"\"Run ops in the supervised session.\n        This method is completely compatible with the `tf.Session.run()` method.\n\n        Args:\n        fetches: Same as `tf.Session.run()`.\n        feed_dict: Same as `tf.Session.run()`.\n        options: Same as `tf.Session.run()`.\n        run_metadata: Same as `tf.Session.run()`.\n\n        Returns:\n        Same as `tf.Session.run()`.\n        \"\"\"\n        return self._sess.run(fetches, feed_dict=feed_dict, options=options,\n                          run_metadata=run_metadata)\n```\n\n2.it is constructed with MonitoredSession, so it can only run `one` or a `list`.\n\n```\n    actual_fetches = {\n        caller': fetches,\n        self._global_step_tensor: self._global_step_tensor,\n        'monitors': [_as_graph_element(f, self.graph) for f in monitor_fetches]\n    }\n```\n\nCould you support for running a dict as tf.Session.run()? Or modify the document more clearly?\n", "comments": ["@ispirmustafa this does look like a bug in the docstring (or a bug in MonitordSession), or are we misunderstanding this?\n", "supervised_session.SupervisedSession had that restriction. SuperviseSession was removed and replaced with a MonitoredSession. MonitoredSession don't have this restriction.\n\nFYI, we don't export Supervised or Monitored session via **init** since we're still working on it's interface/implementation. It will be exported via **init** when we think it's ready for a global use.\n"]}, {"number": 4030, "title": "cuda8.0 cudnn5 binary release?", "body": "I've encountered lots of build errors and none of the solutions I found online can solve my problems. I'm just wondering when can you offer binary release for cuda 8.0 and cuDNN5? I think it's very important for those who don't want to get their hands dirty on building issues.\n", "comments": ["While a binary release with the updated libraries is certainly desirable, I might be able to help you with build instructions (or even prebuilt pip-packages for Ubuntu).\n\nIf you want, you can create a new StackOverflow question with the `tensorflow` tag, and post the link here so I can have a look.\nPlease include the following info:\n- Operating System\n- CUDA version\n- cuDNN version\n- How you installed CUDA\n- Available compilers on your system\n", "I'm using ubuntu 16.04 LTS & GTX1080, with CUDA-8.0 and cuDNN-5.105 installed, I've downgrade gcc to gcc-5.3.0. I also have installed gcc-4.9 as backup choice.\nlink to stackoverflow question:\nhttp://stackoverflow.com/questions/39095577/tensorflow-build-failure-with-gcc-error-as\nthe command I type:\nsudo bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nThe first time I got this error.\n![image](https://cloud.githubusercontent.com/assets/19640630/17969359/4b950658-6b04-11e6-8845-66510656fbec.png)\nHowever,  It returns '/usr/bin/as' when I type 'which as' in command line. I'm tried symlinks which doesn't work. Me and my friend are guessing that it could be due to that we downgrade our complier and we still haven't figured out how to solve it.\nbtw, I've tried adding cxx_buildin_include_directory and add cxx_flag according to what previous issues says but the problem was not fixed.\n\nAnd I tried cloning the the master branch and building. Still haven't figured out what happened here.\n![image](https://cloud.githubusercontent.com/assets/19640630/17969570/507a9c9a-6b05-11e6-9ed4-a26f8f2451ee.png)\nI'm having a third  kind of error message now and I will paste it later...\n\nPS: my configure script\n\n```\nPlease specify the location of python. [Default is /usr/bin/python]:\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]:\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5 \nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at...\n\uff1a 6.1\n```\n\n@akors \n", "Now I have\n\n```\nERROR: missing input file '@local_config_cuda//cuda:include/curand_discrete.h'.\nERROR:          /home/pcy/.cache/bazel/_bazel_pcy/cca4539ff6237551fb9a5b20d456f78c/external/farmhash_archive/BUILD:5:1: Executing genrule @farmhash_archive//:configure failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 15.\n/home/pcy/.cache/bazel/_bazel_pcy/cca4539ff6237551fb9a5b20d456f78c/tensorflow/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 /home/pcy/.cache/bazel/_bazel_pcy/cca4539ff6237551fb9a5b20d456f78c/tensorflow\n/tmp/tmp.wqQFr9f1IO /home/pcy/.cache/bazel/_bazel_pcy/cca4539ff6237551fb9a5b20d456f78c/tensorflow/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 /home/pcy/.cache/bazel/_bazel_pcy/cca4539ff6237551fb9a5b20d456f78c/tensorflow\nERROR: /home/pcy/tensorflow/tensorflow/python/BUILD:1101:1: //tensorflow/python:_pywrap_tensorflow.so: missing input file '@local_config_cuda//cuda:include/curand_discrete.h'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nERROR: /home/pcy/tensorflow/tensorflow/python/BUILD:1101:1 1 input file(s) do not exist.\nINFO: Elapsed time: 1.085s, Critical Path: 0.02s\npcy@BJSH-DATAGPU-134-200:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\nERROR: Loading of target '@local_config_cuda//crosstool:crosstool' failed; build aborted: no such package '@local_config_cuda//crosstool': BUILD file not found on package path.\nERROR: Loading failed; build aborted.\n```\n", "@elvinpoon I answered to your question on StackOverflow, on the missing \"as\" issue.\n\nThe last error, the \"no such package\" thing seems to be a new thing that has been introduced recently: you can only build _once_ after configuring. If you run into errors, you have to actually repeat the configure step and the build again. I don't quite know why.\n\nIn case you don't want to go through the trouble, here's my last TF build for my machine:\n\nhttps://dl.dropboxusercontent.com/u/1414175/tf/tensorflow_gpu_cuda8_cdnn5.1-0.10.0rc0-6d04d60-py3-none-any.whl\n\nTo install, you have to rename it (or symlink) to `tensorflow-0.10.0rc0-py3-none-any.whl`, otherwise pip installation will fail.\n\nIt's a few days old, but still more or less up to date.\n", "Hi I'm working on what you've suggested. Btw, do you have TF build for python2.7?\n", "Am 29. August 2016 03:26:01 MESZ, schrieb elvinpoon notifications@github.com:\n\n> Hi I'm working on what you've suggested. Btw, do you have TF build for\n> python2.7?\n> \n> ## \n> \n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub:\n> https://github.com/tensorflow/tensorflow/issues/4030#issuecomment-243013130\n\nNope, I only use Python 3\n", "After having experienced similar issues with `bazel build`ing for several frustrating days, I've decided to look for already existing wheels too; so thanks for those.", "The Mac GPU pip wheels of the latest release (0.12.0) are built with CUDA 8.0and CUDNN 5.1. \r\n\r\nSee: https://www.tensorflow.org/get_started/os_setup\r\n\r\n# Mac OS X, GPU enabled, Python 2.7:\r\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0-py2-none-any.whl\r\n\r\n# Mac OS X, GPU enabled, Python 3.4 or 3.5:\r\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0-py3-none-any.whl\r\n\r\n# Python 2\r\n$ sudo pip install --upgrade $TF_BINARY_URL\r\n\r\n# Python 3\r\n$ sudo pip3 install --upgrade $TF_BINARY_URL", "The link @caisq provided ( [tensorflow install]( https://www.tensorflow.org/get_started/os_setup)) now has instructions on installing binaries, so I think the issues is resolved and thus should be closed."]}, {"number": 4029, "title": "ptb language modeling example broken with --use_fp16 and dropout other than 0", "body": "### Environment info\n\nOperating System: Ubuntu 16.04 LTS\nGPU: GTX 1080\nNvidia driver: 370.23\nCUDA: 8.0\ncuDNN: 5\nbazel: \n\n```\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local- fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Aug 24 00:13:09 2016 (1471997589)\nBuild timestamp: 1471997589\nBuild timestamp as int: 1471997589\n```\n\ngit commit hash: cc3153a7a0a23533d14ead34db37e4ccd7892079\n### Description\n\nWhen run using the `--use_fp16` flag and either `--model medium` or `--model large`, the PTB language model example fails:\n\n```\nTraceback (most recent call last):\n  File \"ptb_word_lm.py\", line 339, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"ptb_word_lm.py\", line 316, in main\n    m = PTBModel(is_training=True, config=config)\n  File \"ptb_word_lm.py\", line 117, in __init__\n    inputs = tf.nn.dropout(inputs, config.keep_prob)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1121, in dropout\n    if tensor_util.constant_value(keep_prob) == 1:\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 634, in constant_value\n    ret = _ConstantValue(tensor)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 549, in _ConstantValue\n    return MakeNdarray(tensor.op.get_attr(\"value\"))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 517, in MakeNdarray\n    raise TypeError(\"Unsupported tensor type: %s\" % tensor.dtype)\nTypeError: Unsupported tensor type: 19\n```\n\nModifying the model configs so that keep probability is 1 eliminates the problem, while any value other than 1 results in a call to `tf.nn.dropout` and, in turn, a call to `tensor_util.constant_value`, which results in a `TypeError`.\n### What have you tried?\n\nI have another machine running Ubuntu 14.04, CUDA 7.5, and a slightly older version of TensorFlow that does not have this issue. Details for this other set up:\n\nOperating System: Ubuntu 14.04 LTS\nGPU: Titan X\nNvidia driver: 352.39 \nCUDA: 7.5\ncuDNN: 5\nbazel: \n\n```\nBuild label: 0.3.0-2016-07-25 (@e671d29)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Mon Jul 25 17:47:58 2016 (1469468878)\nBuild timestamp: 1469468878\nBuild timestamp as int: 1469468878\n```\n\ngit commit hash: 27eeb441bad8bcaa1bcba42a4b4ee49fb50ea0d3\n", "comments": ["Yea, I got this error awhile back and I had to modify MakeNdarray in /tensorflow/python/framework/tensor_util.py to support float16 in the if/elseif block. I added a case for float16 and it seems to work.\n\n elif tensor_dtype == dtypes.float16:\n    if len(tensor.half_val) == 1:\n      return np.repeat(np.array(tensor.half_val[0], dtype=dtype),\n                       num_elements).reshape(shape)\n    else:\n      return np.fromiter(tensor.half_val, dtype=dtype).reshape(shape)\n\nHope this helps.\n", "Got this same problem and solution proposed by @rieram seems to work correctly\n", "I've added support for 16 bit float to the MakeNdarray function in https://github.com/tensorflow/tensorflow/commit/b0e27a3700b4f729fdc91efe96ad0dbfcab48fc1: I'm closing the issue.\n", "Thanks @benoitsteiner. It would be nice to actually have support for float16 in actual matrix operations and kernels. See for example: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_blas.cc#L1683  \nRight now it actually converts float16 to float32 instead of using Hgemm. This a different issue though.\n"]}, {"number": 4028, "title": "Branch 131232000", "body": "", "comments": []}, {"number": 4027, "title": "[cmake] Build error in dependency re2", "body": "Somehow the re2 dependency does not get build correctly.\nI think its because of `re2_INCLUDE_DIR` in re.cmake holding two directories. Then `COMMAND ${CMAKE_COMMAND} -E make_directory ${re2_INCLUDE_DIR}` fails since `cmake -E make_directory` only takes one arg. I'm just comiping and might add a PR if it works.\n", "comments": ["Please can you provide the information requested in the TensorFlow issues reporting template.\n", "Closing this out due to inactivity.  Please comment with the requested information, so that we can attempt to reproduce, and I will reopen.\n"]}, {"number": 4026, "title": "TensorFlow SKFlow Estimators Fail when Using read_batch_examples", "body": "### Environment info\n\nOperating System: Ubuntu\n\nPackage: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\nVersion: 0.10.0rc0\n\nI'm currently trying to use `tf.contrib.learn.read_batch_examples` working while using a TensorFlow (SKFlow/tf.contrib) Estimator, specifically the `LinearClassifier`. I create a `read_batch_examples` op feeding in a CSV file with a `tf.decode_csv` for the `parse_fn` parameter with appropriate default records. I then feed that op to my `input_fn` for fitting the Estimator, but when that's run I receive the following error:\n### Error Message\n\n```\nValueError: Tensor(\"centered_bias_weight:0\", shape=(1,), dtype=float32_ref) must be from the same graph as Tensor(\"linear/linear/BiasAdd:0\", shape=(?, 1), dtype=float32).\n```\n### What I've Tried\n\nThe code works if I run the op beforehand and then feed the input instead as an array of values. While this workaround exists, it is unhelpful because I am working with large datasets in which I need to batch in my inputs. Currently going over `Estimator.fit` (currently equivalent to `Estimator.partial_fit` in iterations isn't nearly as fast as being able to feed in data as it trains, so having this working is ideal.  Additionally I've tried wrapping everything with `with tf.Graph().asdefault()`. Any ideas? The non-functioning code is below.\n### Shortened Source Code to Reproduce\n\n[read_batch_examples_fails_with_estimator.txt](https://github.com/tensorflow/tensorflow/files/435736/read_batch_examples_fails_with_estimator.txt)\n\nAny alternatives for batching would be appreciated as well!\n", "comments": ["@ilblackdragon  Could you please take a look at this? \n", "You have to call read_batch_examples from somewhere inside input_fn so that the tensors it creates are in the graph that Estimator creates in fit().\n", "Oh I feel like an idiot! I've been creating the op outside of the graph scope. It works now, can't believe I didn't think to try that. Thanks a lot! This is a non-issue and has been resolved.\n"]}, {"number": 4025, "title": "TensorForest Fails in Modified wide_n_deep SKFlow Tutorial", "body": "### Environment info\n\nOperating System: Ubuntu\n\nPackage: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\nVersion: 0.10.0rc0\n\nEdit: See update at bottom for stuff I found wrong\n\nI cannot get TensorForest to work using the input_fn parameter as input. It failed in my code, so I tried using it in the wide_n_deep tutorial which failed for the same reasons. Categorical inputs using Sparse tensors are rejected saying the wrong dtype is being used (they're all String). Without using categorical inputs, it gets further but fails saying that it cannot concat Tensors. I was able to run the tests for the TensorForest successfully with the Iris dataset ([link here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/random_forest_test.py)), but I would like to use the input_fn ability with a similar style to the tutorial if possible. I can provide the source but the only thing changed was using the `tf.contrib.learn.TensorForestEstimator` with the number of classes and features specified in `tf.contrib.tensor_forest.tensor_forest.ForestHParams` as the params input. Can someone look into this?\n### Steps to reproduce\n1. Modify wide_n_deep tutorial to use a `TensorForestEstimator`\n### What I've tried\n1. Running using input_fn as parameter to fit\n2. Running using input_fn without categorical inputs to fit\n### Output that would be helpful\n#### Output including categorical sparse tensors\n\n```\nTraceback (most recent call last):\n  File \"wide_n_deep_tutorial.py\", line 224, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"wide_n_deep_tutorial.py\", line 220, in main\n    train_and_eval()\n  File \"wide_n_deep_tutorial.py\", line 213, in train_and_eval\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 240, in fit\n    max_steps=max_steps)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 550, in _train_model\n    train_op, loss_op = self._get_train_ops(features, targets)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/random_forest.py\", line 149, in _get_train_ops\n    features, spec = data_ops.ParseDataTensorOrDict(features)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/data/data_ops.py\", line 169, in ParseDataTensorOrDict\n    return _ParseSparse(data)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/data/data_ops.py\", line 101, in _ParseSparse\n    raise ValueError('Only sparse tensors of type string are supported.')\nValueError: Only sparse tensors of type string are supported.\n```\n#### Output without categorical sparse tensors\n\n```\nTraceback (most recent call last):\n  File \"wide_n_deep_tutorial.py\", line 224, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"wide_n_deep_tutorial.py\", line 220, in main\n    train_and_eval()\n  File \"wide_n_deep_tutorial.py\", line 213, in train_and_eval\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 240, in fit\n    max_steps=max_steps)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 550, in _train_model\n    train_op, loss_op = self._get_train_ops(features, targets)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/random_forest.py\", line 149, in _get_train_ops\n    features, spec = data_ops.ParseDataTensorOrDict(features)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/data/data_ops.py\", line 171, in ParseDataTensorOrDict\n    return _ParseDense(data)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/data/data_ops.py\", line 144, in _ParseDense\n    else data[k] for k in sorted(data.keys())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 759, in concat\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 414, in _concat\n    values=values, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2312, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1704, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 815, in _ConcatShape\n    (value_shape.ndims, concat_dim))\nValueError: Expected concat_dim in range [0, 1), but got 1\n```\n### UPDATE\n\nI've fooled around with more stuff and been sifting through the source and have found the following.\n1. Tensor Forest doesn't support both categorical and continuous inputs (only Sparse OR Dense)? Why is that, and can we expect that being implemented soon?\n2. If solely sparse tensors are inputted, `_ParseSparse` fails because `constants.DATA_ALL_CATEGORICAL` is not defined (I believe it should be `constants.DATA_CATEGORICAL`)\n3. Upon changing that variable to `constants.DATA_CATEGORICAL`, everything seems ok until my terminal gets flooded with these warnings:\n\n```\nW tensorflow/contrib/tensor_forest/core/ops/sample_inputs_op.cc:164] Could not find any values for input 1 inside sparse_input_indices\nW tensorflow/contrib/tensor_forest/core/ops/sample_inputs_op.cc:164] Could not find any values for input 2 inside sparse_input_indices\nW tensorflow/contrib/tensor_forest/core/ops/sample_inputs_op.cc:164] Could not find any values for input 1 inside sparse_input_indices\nW tensorflow/contrib/tensor_forest/core/ops/sample_inputs_op.cc:164] Could not find any values for input 2 inside sparse_input_indices\n```\n\nThose warnings repeat over and over and eventually the model runs successfully.\n4. I tried messing with `_Parse_Dense` and changed the `array_ops.concat` to `array_ops.pack` with `axis=1`, which returns the expected 2-D tensor. `concat` doesn't work as it is unable to create the dimension, whereas `pack` can. This resolved the continuous input issue, but I received a negligible error\n\n```\nTraceback (most recent call last):\n  File \"wide_n_deep_tutorial.py\", line 224, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"wide_n_deep_tutorial.py\", line 220, in main\n    train_and_eval()\n  File \"wide_n_deep_tutorial.py\", line 213, in train_and_eval\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 240, in fit\n    max_steps=max_steps)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 550, in _train_model\n    train_op, loss_op = self._get_train_ops(features, targets)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/random_forest.py\", line 163, in _get_train_ops\n    **self.training_args),\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py\", line 378, in training_graph\n    **tree_kwargs))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py\", line 570, in training_graph\n    regression=self.params.regression))\n  File \"<string>\", line 219, in count_extremely_random_stats\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 463, in apply_op\n    (prefix, dtypes.as_dtype(input_arg.type).name))\nTypeError: Input 'input_data' of 'CountExtremelyRandomStats' Op has type int64 that does not match expected type of float32.\n```\n\nI easily fixed above error by changing all continuous inputs to tf.float32 from tf.int64, so that might be a feature request of interest.\n5. After these modifications to source, I was able to use `input_fn` successfully with solely the continuous features of the census data set of the wide_n_deep tutorial (obtained ~82% on the testing set with 10 training steps). Not sure why but it took a while for that to run, didn't really look into it though.\n6. I tried including the categorical inputs via tf.string dtype constant tensors, but received another error saying input to StringToFloat was not 2-D.\n\n```\n     tensorflow.python.framework.errors.InvalidArgumentError: input_data should be two-dimensional\n```\n\nI was able to resolve it by changing `convert_ops.string_to_float` to `tf.string_to_number`, but I am still unable to input via Sparse Tensors.\n7. Another issue I found: not all metrics seem to work when passed in (I passed in `streaming_precision`, `streaming_recall`, `streaming_accuracy`, and `confusion_matrix`)\n\n```\n    Traceback (most recent call last):  \n      File \"read.py\", line 336, in <module>\n        tf.app.run()\n      File \"/usr/local/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n        sys.exit(main(sys.argv))\n      File \"read.py\", line 315, in main\n        eval_steps=1\n      File \"/usr/local/tensorflow/heartwood/cross_validation/evaluate.py\", line 37, in evaluate\n        'cm': tf.contrib.metrics.confusion_matrix\n      File \"/usr/local/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 356, in evaluate\n        name=name)\n      File \"/usr/local/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 630, in _evaluate_model\n        eval_dict = self._get_eval_ops(features, targets, metrics)\n      File \"/usr/local/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/random_forest.py\", line 198, in _get_eval_ops\n        result[name] = metric(probabilities, labels)\n      File \"/usr/local/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/confusion_matrix_ops.py\", line 73, in confusion_matrix\n        predictions, name='predictions', dtype=dtypes.int64),\n      File \"/usr/local/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 621, in convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n      File \"/usr/local/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 564, in _TensorTensorConversionFunction\n        % (dtype.name, t.dtype.name, str(t)))\n    ValueError: Tensor conversion requested dtype int64 for Tensor with dtype float32: 'Tensor(\"probabilities:0\", shape=(?, 2), dtype=float32)'\n```\n", "comments": ["Sorry for all the troubles.  Long story short, TensorForestEstimator doesn't support such a heterogenous set of inputs as the wide_n_deep tutorial, partly because the TensorForest internal interfaces predate the estimator interface and a more general translation between the two was never put in place because we haven't had a driver for that yet.   What does work is:\n1. All continuous numerical inputs\n2. All sparse string inputs (that are hashed and mapped to a numerical space and treated as categorical)\n3. A mix of continuous and categorical numerical inputs.\n\nLet me try to comment on the things listed under Update:\n1. Let me clarify that continuous and categorical inputs are different than sparse and dense representation (you can have categorical dense inputs).  It's a slightly important distinction for forests, because they have the ability to implement a x[i] == T function at nodes instead of x[i] < T.  But it is true that we only support sparse or dense input right now.\n2. This was quickly fixed a while ago, but unfortunately the last Tensorflow release seems to have included that mistake.  Installing the nightly build fixes this.\n3. This is usually a warning that a given example is totally empty when using sparse inputs.  If you don't think that any examples are totally empty, then it's probably something with the shape of the tensors being input (especially if they're all input 1 and 2).\n4. _ParseDense expects 2-D tensors, which is what you get when you use e.g. tf.FixedLenFeature with tf.contrib.learn.read_batch_features (which is what we normally do).  TensorForest is an online training algorithm, so it's really made for reasonable mini-batch sizes (100-10000) that sample from a larger data set. This allows us to train on data sets that are infeasible to read into memory, but also works fine for smaller data sets in our experience. If I'm reading it right it seems like the wide_n_deep tutorial throws the whole column into a 1-D tf.constant. We've set up TensorForest with the assumption that x,y is used for smaller data sets that fit in memory, and input_fn is used for batched large data sets.  Maybe that's not a good assumption, but in any case this should at least be documented better.\n5. Again, might be wrong here, but if not using mini-batching, it isn't surprising that training might take a long time. \n6. This is maybe related to 4 and mini-batching.\n7. What is confusion_matrix expecting?  TensorForest inference outputs class probabilities, so for e.g. streaming_accuracy, we wrap it in a function that does an argmax to get class prediction.\n   tensorflow/contrib/tensor_forest/client/eval_metrics.py\n\nThanks for pointing these things out, I'll be addressing them in some combination of improved documentation, improved error conditions, patching up cases that we can make work, and maybe even some heavy refactoring.  \n", "Another update:\nI was able to get all metrics I tried to work (confusion matrix, precision, recall etc.) by changing `_get_eval_ops` in `tensorflow/contrib/learn/python/learn/estimators/random_forest.py` as follows:\n\n```\n def _get_eval_ops(self, features, targets, metrics):\n  ...\n  if metrics is None:\n    metrics = {self.accuracy_metric:\n               eval_metrics.get_metric(self.accuracy_metric)}\n\n  result = {}\n\n  predictions = math_ops.argmax(probabilities, 1)\n  # undo one-hot\n  labels = math_ops.argmax(labels, 1)\n  for name, metric in six.iteritems(metrics):\n    result[name] = metric(predictions, labels)\n\n  return result\n```\n\nThis was broken because the probabilities were being fed to the metrics instead of the predictions and same situation with the labels essentially. In the `eval_ops` file for the tensor forest, the accuracy metric won't work with these changes as it handles it in the same manner I did\n", "Thanks for the response! Please see my last comment with regards to 7. Would it be reasonable to expect the TensorForest to be implemented in such a way as the other Estimators are? The way I'm working  handles things generically for different model types, so it would be nice if the expected input for all Estimators was the same. Possibly even with features columns?\nWith regards to 3, none of the tensors I was using were empty to I'm assuming there is a discrepancy between the shapes of the inputs used for the non-tensorforest Estimators. Although those warnings flew by, it eventually fit using only the categorical sparse tensors and got reasonable accuracy for not having all features as input.\nI don't think that's a good assumption for `input_fn`, although that's what I would like to use it for but having issues (see #4026), because it's currently the only method of being able to input tensors to the model. \nI apologize for 1, I meant continuous and categorical data not sparse vs dense. A lot of data I work with contains continuous numerical data alongside String categorical data. I've found in my research that the RandomForest models work the best with my specific data over other types of models. I guess in the meantime while it's not possible I can implement the model for a sparse representation of my categorical data and another model for the continuous part of my data and look into methods of combining predictions?\n", "I will certainly look into improving the interface and making it more compatible with other Estimators, though I can't promise a timeline right now.\n\nThe two-forest method would work.  Another thing that might work is translating the categorical features into dense Tensors (if they're strings, try to hash and reinterpret the bits as floats.  If they're already numbers and the index space is large but the value space is small, treat the index space as the feature value. If the value space is large but the index space is small, see if tf.sparse_to_dense won't blow up).  Another thing that would work is train an embedding on your sparse features and concat that onto your dense ones.\n", "Thanks a lot! I really appreciate it. I think I'll try the two-forest method first and compare to the categorical to dense approach. I would like to clarify what you mean by index and value space; if I have a feature with 10 different unique values and 100 examples, the value space is 10 and the index is 100?\n", "So I've seen SparseTensors used to represent different kinds of data.  Such as:\n1. The \"typical\" meaning of sparse, where each example has a few columns out of many that are present and rest are assumed zero or \"\".  If the sparse_tensor.values are only ever 0 or 1, I consider this a small value space and possibly large feature space.\n2. A convenient way to represent a variable length list of values.  e.g. a few strings per example.  While maybe this can be made to look like 1), I would consider this to have a small index space (a \"few\" strings) and a large value space (any possible string).  \n\nGood luck with the two-forest, I'd be interested to hear how it goes.\n", "Ok thanks for that clarification! I won't be getting to the two-forest until next Monday but I'll share my findings! Forest hasn't performed bad at all on only categorical or continuous inputs. I'll probably start by averaging the prediction probabilities from each forest and compare that to some over methods. Thanks again!\n", "Alright, just figured out a working way for me to get categorical (non-numeric) and continuous inputs solely through Dense tensors. I made use of the layers I wanted for the model with the combination of `tf.contrib.layers.input_from_feature_columns`. I made use of `tf.contrib.layers.sparse_column_with_hash_bucket` with `tf.contrib.layers.embedding_column` to handle my categorical inputs. I restored `_Parse_Dense` to its original state as it was then receiving 2D inputs as expected via this method. I think if this Estimator is refactored it will be in the best interest to allow it to accept feature_column inputs. Thanks for your assistance and best of luck with your work!\n", "Is there currently a way to calculate mean decrease gini for each feature? I see that I'm able to retrieve the average impurity but I believe it's for the forest as a whole. \n", "You could calculate gini for each feature that is being considered as a split candidate for a node that is currently growing (which is how we end up picking the feature to use at a node).  The candidate_split_sums variable in each tree keeps class counts for each feature under consideration. But we don't track them on the whole-tree level, mostly because it would be expensive to keep class counts for each feature around (which becomes very important when you consider building large forests).  \n", "Finally getting around to try and implement this. When I look at the `candidate_split_features` values, I see that I have values from 0 to 86 in a vector the shape of [max_fertile_nodes, num_splits_to_consider]. I'm confused as to what the 87 values correspond to; I input and specify 41 features to the forest so I am confused as to what these numbers represent. \n\nAlso, the `candidate_split_sums` appears to have class counts for 3 classes but I only have 2 (shape of [1000, 10, 3]). That puzzles me too...\n", "The values chosen for candidate_split_features are sampled from a uniform distribution with a max value of num_features as inferred from the second dimension of the input data (assuming we're talking about dense data).  Can you verify the shape of input data, and make sure all the sparse inputs are None.\n\ncandidate_split_sums (and other sums like node_sums) store the sum of all the other entries in index 0, because we make use of the sum a lot and it was easier than recalculating it all the time.  So the shape should be num_classes + 1.\n", "Oh thanks for clarifying `candidate_split_sums`, I should've figured that one out. As to `candidate_split_features`, I made the assumption that the parameter I set would be used as the max features; counting the degrees of freedom (embeddings) for each categorical feature gave me 87 inputs, which is correct.\n\nFollow up question, how do I decipher which feature is which? I input the features in the form of a dictionary via `input_fn`, so there's no fixed order going in. Additionally, is there a straightforward way of knowing when a node is growing and the class counts/feature being split? If I have the right understanding now, I should be able to calculate the gini impurity index using the candidate class counts for each candidate feature whenever a node is growing, given I know when that node grows and which feature is which. \n\nWhat does `accumulator_sums` track? From what I've found, forests accumulate the decreases is Gini impurity from the optimal splits found.\n", "_ParseDense does a sorted(data.keys()) when concatenating dense inputs, so you'll get a consistent ordering according to python sort.  \n\nThere are two maps - node_to_accumulator_map and accumulator_to_node_map that let you translate between node id and accumulator id (a node assigned to an accumulator is growing).  The accumulator id is then used to index into e.g. candidate_split_features, candidate_split_sums, etc.  \n\naccumulator_sums track the total class counts that arrive to an accumulator, and candidate_split_sums track the class counts that take the LEFT branch.  We can then infer the right-branch-taken class counts.  \n", "Thanks so much for the help! I don't see the `accumulator_to_node_map` variable you mentioned when I look at all the model's variables. It's not in my way right now but just thought you should know.\n", "I see in the `_weighted_gini` calculation, the class counts are taken via `smoothed = 1.0 + array_ops.slice(class_counts, [0, 1], [-1, -1])`; may I ask why one is being added to every class count and whether this needs to be done for the class counts in variables such as `accumulator_sums`?\n", "That's simple Laplace smoothing, mostly to avoid div/0 when no examples have arrived to an accumulator or a leaf.\n", "Closing since it looks like we have a solution."]}, {"number": 4024, "title": "How to link the TensorFlow static library for Android using the Gradle Experimental Plugin", "body": "Hi all,\n\nContext: we want to link TensorFlow against our C++ platform-independent code and load a model to run some classification relevant for our application. We use SWIG to generate the Java bindings and in the end it builds an Android .aar file that we push to a private maven repository that our other packages (Android apps, we also have an Android SDK) can consume downstream. Moving our project to Bazel is not ideal given our packaging / distribution pipeline. That's why we are trying the Gradle integration before moving the project to Bazel if there is no way around. For now, I followed https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/README.md and I was successfully able to build the .a libs (tensorflow-core and protobufs) and ran the benchmark.\n\nThen I have a lot of undefined reference in TensorFlow / Protobuf, see [output.txt](https://github.com/tensorflow/tensorflow/files/435701/output.txt)\n\nAnd for info, here's my build.gradle\n\n``` groovy\napply plugin: 'com.android.model.library'\napply plugin: 'com.jfrog.artifactory'\napply plugin: 'com.github.dcendents.android-maven'\napply plugin: 'com.jfrog.bintray'\napply plugin: 'maven-publish'\n\ndef VERSION_NAME = '1.0.0-alpha.3-SNAPSHOT'\ndef GROUP_ID = 'io.cens'\ndef REPO_KEY = VERSION_NAME.endsWith('SNAPSHOT') ? 'libs-snapshot-local' : 'libs-release-local'\n\ndef ARTIFACT_FILENAME = ARTIFACT_ID + '-' + VERSION_NAME + '.aar'\n\nversion = VERSION_NAME\ngroup = GROUP_ID\n\nmodel {\n    android {\n        compileSdkVersion 23\n        buildToolsVersion \"23.0.3\"\n\n        defaultConfig {\n            minSdkVersion.apiLevel 14\n            targetSdkVersion.apiLevel 23\n            versionCode 1\n            versionName VERSION_NAME\n            project.archivesBaseName = ARTIFACT_FILENAME\n        }\n\n        buildTypes {\n            release {\n                minifyEnabled false\n                proguardFiles.add(file(\"proguard-rules.pro\"))\n            }\n        }\n\n        ndk {\n            moduleName = \"censio_crash\"\n            stl = \"gnustl_shared\" // see https://developer.android.com/ndk/guides/cpp-support.html#stl\n\n            // TensorFlow artifacts (it is assumed that the tensorflow repo is a peer of the crash repo)\n            cppFlags.add('-I' + file('../../../../../tensorflow').absolutePath)\n            cppFlags.add('-I' + file('../../../../../tensorflow/tensorflow/contrib/makefile/gen/proto').absolutePath)\n            cppFlags.add('-I' + file('../../../../../tensorflow/tensorflow/contrib/makefile/gen/protobuf/include').absolutePath)\n            cppFlags.add('-I' + file('../../../../../tensorflow/tensorflow/contrib/makefile/downloads/eigen-eigen-6f952374ef2b').absolutePath)\n            ldFlags.add('../../../../../tensorflow/tensorflow/contrib/makefile/gen/protobuf/lib/libprotobuf-lite.a')\n            ldFlags.add('../../../../../tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a')\n\n            // Our sources\n            cppFlags.add('-I' + file(\"../../../../src\").absolutePath)\n            cppFlags.add(\"-std=gnu++11\")\n            cppFlags.addAll([\"-fexceptions\", \"-frtti\"])\n            ldLibs.addAll(['android', 'log', 'z'])\n            abiFilters.addAll(['armeabi-v7a']) // only build for main architecture subset\n        }\n\n        sources {\n            main {\n                jni {\n                    source {\n                        srcDir '../../../../src'\n                        exclude \"**/Logger.cpp\" // Android impl is generated by SWIG instead\n                    }\n                }\n            }\n        }\n    }\n}\n\ndef siteUrl = 'https://github.com/Censio/mobile-crash-sdk'\ndef gitUrl = 'https://github.com/Censio/mobile-crash-sdk.git'\n\ninstall {\n    repositories.mavenInstaller {\n        // This generates POM.xml with proper parameters\n        pom {\n            project {\n                packaging 'aar'\n\n                name 'TrueMotion Android On-board SDK - Crash package.'\n                url siteUrl\n\n                // Set your license\n                licenses {\n                    license {\n                        name 'Copyright (C) 2016 TrueMotion - All Rights Reserved'\n                        url 'http://gotruemotion.com'\n                    }\n                }\n                developers {\n                    developer {\n                        id 'antoine-dbr'\n                        name 'Antoine-Dubois-Rande'\n                        email 'antoine@cens.io'\n                    }\n                }\n                scm {\n                    connection gitUrl\n                    developerConnection gitUrl\n                    url siteUrl\n                }\n            }\n        }\n    }\n}\n\ndependencies {\n}\n\npublishing {\n    publications {\n        aar(MavenPublication) {\n            groupId GROUP_ID\n            version = VERSION_NAME\n            artifactId ARTIFACT_ID\n\n            // Tell maven to prepare the generated \"*.aar\" file for publishing\n            artifact(\"$buildDir/outputs/aar/\" + ARTIFACT_FILENAME)\n\n            pom.withXml {\n                def dependencies = asNode().appendNode('dependencies')\n                configurations.getByName(\"_releaseCompile\").getResolvedConfiguration().getFirstLevelModuleDependencies().each {\n                    def dependency = dependencies.appendNode('dependency')\n                    dependency.appendNode('groupId', it.moduleGroup)\n                    dependency.appendNode('artifactId', it.moduleName)\n                    dependency.appendNode('version', it.moduleVersion)\n                }\n            }\n        }\n    }\n}\n\nartifactory {\n    contextUrl = 'https://censiodev.artifactoryonline.com/censiodev'\n    publish {\n        repository {\n            // The Artifactory repository key to publish to\n            repoKey = REPO_KEY\n\n            username = ARTIFACTORY_USERNAME\n            password = ARTIFACTORY_PASSWORD\n        }\n        defaults {\n            publications('aar')\n            publishArtifacts = true\n            publishPom = true\n        }\n    }\n}\n\nbintray {\n    user = BINTRAY_USER\n    key = BINTRAY_API_KEY\n\n    configurations = ['archives']\n    pkg {\n        userOrg = 'censio'\n        repo = VERSION_NAME.endsWith('SNAPSHOT') ? '' : 'maven' // This will error out if we attempt to push a snapshot build\n        name = ARTIFACT_ID\n        websiteUrl = siteUrl\n        vcsUrl = gitUrl\n        licenses = ['Censio']\n        dryRun = false\n        publish = true\n    }\n}\n\ntask rename(type: Copy) {\n    from 'build/outputs/aar'\n    into 'build/outputs/aar'\n    rename { String fileName ->\n        fileName.replace('-release.aar', '')\n    }\n}\n```\n\nSince the benchmark works, I think it's probably an issue on my end. In any case, any help appreciated. Thanks!\n", "comments": ["FWIW there is now gradle+cmake+Makefile support for the Android TF JNI inference library under [tensorflow/contrib/android/cmake](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android/cmake), which may make things easier for you.\r\n\r\nFull support for the demo should be coming at some point as well.", "Good to know, thanks for the pointer @andrewharp.", "Closing since this seems solved. Feel free to reopen if I misunderstood."]}, {"number": 4023, "title": "Problems when running compile_ios_tensorflow.sh", "body": "On running compile_ios_tensorflow.sh, I am the following problems:\n\nUndefined symbols for architecture armv7:\n  \"tensorflow::functor::ReduceAndReshape<Eigen::ThreadPoolDevice, int, 6, 1>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<int, 6, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 6, 1, int>, 16>, Eigen::DSizes<int, 1> const&, Eigen::DSizes<int, 6> const&) const\", referenced from:\n      void tensorflow::TileGradientOpEigen::ThreadPoolDevice::HandleReduce<int, 6, 1>(tensorflow::OpKernelContext_, std::__1::vector<int, std::__1::allocator<int> > const&, tensorflow::Tensor_) in libtensorflow-core-armv7.a(tile_ops.o)\n  \"tensorflow::functor::ReduceAndReshape<Eigen::ThreadPoolDevice, float, 6, 1>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 6, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<float const, 6, 1, int>, 16>, Eigen::DSizes<int, 1> const&, Eigen::DSizes<int, 6> const&) const\", referenced from:\n      void tensorflow::TileGradientOpEigen::ThreadPoolDevice::HandleReduce<float, 6, 1>(tensorflow::OpKernelContext_, std::__1::vector<int, std::__1::allocator<int> > const&, tensorflow::Tensor_) in libtensorflow-core-armv7.a(tile_ops.o)\n  \"tensorflow::functor::TileGrad<Eigen::ThreadPoolDevice, int, 6>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<int, 6, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 6, 1, int>, 16>, Eigen::DSizes<int, 6> const&, Eigen::DSizes<int, 6> const&, bool) const\", referenced from:\n      void tensorflow::TileGradientOpEigen::ThreadPoolDevice::HandleCaseImpl<(tensorflow::DataType)3, 6>(tensorflow::OpKernelContext_, std::__1::vector<int, std::__1::allocator<int> > const&, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor_) in libtensorflow-core-armv7.a(tile_ops.o)\n  \"tensorflow::functor::TileGrad<Eigen::ThreadPoolDevice, float, 6>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 6, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<float const, 6, 1, int>, 16>, Eigen::DSizes<int, 6> const&, Eigen::DSizes<int, 6> const&, bool) const\", referenced from:\n      void tensorflow::TileGradientOpEigen::ThreadPoolDevice::HandleCaseImpl<(tensorflow::DataType)1, 6>(tensorflow::OpKernelContext_, std::__1::vector<int, std::__1::allocator<int> > const&, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor_) in libtensorflow-core-armv7.a(tile_ops.o)\n  \"tensorflow::functor::Tile<Eigen::ThreadPoolDevice, int, 6>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<int, 6, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 6, 1, int>, 16>, std::__1::array<int, 6ul> const&) const\", referenced from:\n      void tensorflow::TileOpEigen::ThreadPoolDevice::HandleCaseImpl<(tensorflow::DataType)3, 6>(tensorflow::OpKernelContext_, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor_) in libtensorflow-core-armv7.a(tile_ops.o)\n  \"tensorflow::functor::Tile<Eigen::ThreadPoolDevice, float, 6>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 6, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<float const, 6, 1, int>, 16>, std::__1::array<int, 6ul> const&) const\", referenced from:\n      void tensorflow::TileOpEigen::ThreadPoolDevice::HandleCaseImpl<(tensorflow::DataType)1, 6>(tensorflow::OpKernelContext_, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor_) in libtensorflow-core-armv7.a(tile_ops.o)\nld: symbol(s) not found for architecture armv7\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake: **\\* [/Users/jiq/tensorflow/tensorflow/contrib/makefile/gen/bin/ios_ARMV7/benchmark] Error 1\n- '[' 2 -ne 0 ']'\n- echo 'armv7 compilation failed.'\n  armv7 compilation failed.\n- exit 1\n\nI have looked at the #3382 commit. But I still had this problem. Does any one can help?\n", "comments": ["Have solved this problem by going back using commit d80f22\n"]}, {"number": 4022, "title": "train.batch with dynamic_pad=True and input as list of tensors not working as expected", "body": "### Environment info\n\nOperating System: \nUbuntu 14.04.4 LTS (running in Virtual Box 5.0.22 r108108)\n\nInstalled version of CUDA and cuDNN: \nNone\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\npip 8.1.2\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n0.10.0rc0\n### Preface\n\nThis issue arose from attempting to expand on an example using dynamic padding as written in: http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\nthe necessary code is copied below.\n### Steps to reproduce\n\nRun the following minimal example:\n\n``` python\nimport tensorflow as tf\n\n# [0, 1, 2, 3, 4 ,...]\nx = tf.range(1, 10, name=\"x\")\n\n# A queue that outputs 0,1,2,3,...\nrange_q = tf.train.range_input_producer(limit=5, shuffle=False)\nslice_end = range_q.dequeue()\n\n# Slice x to variable length, i.e. [0], [0, 1], [0, 1, 2], ....\ny = tf.slice(x, [0], [slice_end], name=\"y\")\n\nbatched_data = tf.train.batch(\n    tensors=[y],\n    batch_size=5,\n    dynamic_pad=True,\n    name=\"y_batch\"\n)\n\n# Run the graph\n# tf.contrib.learn takes care of starting the queues for us\nres = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n\n# Print the result\nprint(\"Batch shape: {}\".format(res[0][\"y\"].shape))\nprint(res[0][\"y\"])\n```\n\nOutput (correct behavior):\n\n```\nBatch shape: (5, 4)\n[[0 0 0 0]\n [1 0 0 0]\n [1 2 0 0]\n [1 2 3 0]\n [1 2 3 4]]\n```\n\nWhen attempted with different input (list of tensors of different lengths)\n\n``` python\nimport tensorflow as tf\n\ny = [tf.constant(range(n)) for n in range(1,10)]\n\nbatched_data = tf.train.batch(\n    tensors=[y],\n    batch_size=5,\n    dynamic_pad=True,\n    name=\"y_batch\"\n)\n\n# Run the graph\n# tf.contrib.learn takes care of starting the queues for us\nres = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n\n# Print the result\nprint(\"Batch shape: {}\".format(res[0][\"y\"].shape))\nprint(res[0][\"y\"])\n```\n\nOutput:\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-12-8b4d52a4df68> in <module>()\n     19     batch_size=5,\n     20     dynamic_pad=True,\n---> 21     name=\"y_batch\"\n     22 )\n     23 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.pyc in batch(tensors, batch_size, num_threads, capacity, enqueue_many, shapes, dynamic_pad, allow_smaller_final_batch, shared_name, name)\n    577   tensor_list = _as_tensor_list(tensors)\n    578   with ops.op_scope(tensor_list, name, \"batch\") as name:\n--> 579     tensor_list = _validate(tensor_list)\n    580     (tensor_list, sparse_info) = _serialize_sparse_tensors(\n    581         tensor_list, enqueue_many)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.pyc in _validate(tensor_list)\n    411 \n    412 def _validate(tensor_list):\n--> 413   tensor_list = ops.convert_n_to_tensor_or_indexed_slices(tensor_list)\n    414   if not tensor_list:\n    415     raise ValueError(\"Expected at least one tensor in batch().\")\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in convert_n_to_tensor_or_indexed_slices(values, dtype, name, as_ref)\n    735       ret.append(\n    736           convert_to_tensor_or_indexed_slices(value, dtype=dtype, name=n,\n--> 737                                               as_ref=as_ref))\n    738   return ret\n    739 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor_or_indexed_slices(value, dtype, name, as_ref)\n    696     return value\n    697   else:\n--> 698     return convert_to_tensor(value, dtype=dtype, name=name, as_ref=as_ref)\n    699 \n    700 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, as_ref)\n    619     for base_type, conversion_func in funcs_at_priority:\n    620       if isinstance(value, base_type):\n--> 621         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    622         if ret is NotImplemented:\n    623           continue\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in _autopacking_conversion_function(v, dtype, name, as_ref)\n    628   if dtype is not None and dtype != inferred_dtype:\n    629     return NotImplemented\n--> 630   return _autopacking_helper(v, inferred_dtype, name or \"packed\")\n    631 # pylint: enable=invalid-name\n    632 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in _autopacking_helper(list_or_tuple, dtype, name)\n    591           elems_as_tensors.append(\n    592               constant_op.constant(elem, dtype=dtype, name=str(i)))\n--> 593       return gen_array_ops._pack(elems_as_tensors, name=scope)\n    594     else:\n    595       return converted_elems\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc in _pack(values, axis, name)\n   1452     A `Tensor`. Has the same type as `values`. The packed tensor.\n   1453   \"\"\"\n-> 1454   result = _op_def_lib.apply_op(\"Pack\", values=values, axis=axis, name=name)\n   1455   return result\n   1456 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    701           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    702                            input_types=input_types, attrs=attr_protos,\n--> 703                            op_def=op_def)\n    704           outputs = op.outputs\n    705           return _Restructure(ops.convert_n_to_tensor(outputs),\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   2310                     original_op=self._default_original_op, op_def=op_def)\n   2311     if compute_shapes:\n-> 2312       set_shapes_for_outputs(ret)\n   2313     self._add_op(ret)\n   2314     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1702       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1703                          % op.type)\n-> 1704   shapes = shape_func(op)\n   1705   if shapes is None:\n   1706     raise RuntimeError(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in _PackShape(op)\n    767 \n    768   for inp in op.inputs[1:]:\n--> 769     input_shape = input_shape.merge_with(inp.get_shape())\n    770 \n    771   input_shape = input_shape.as_list()\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self, other)\n    568       except ValueError:\n    569         raise ValueError(\"Shapes %s and %s are not compatible\" %\n--> 570                          (self, other))\n    571 \n    572   def concatenate(self, other):\n\nValueError: Shapes (1,) and (2,) are not compatible\n```\n\nExpected same result as previous script.\n### What have you tried?\n- Attempted transforming `y` into a tensor but have not found a way. Given the first example, it appears possible to construct tensors with variable size in some dimension, but given a dataset in the form of List of List of primitive (with inner lists being of varying lengths), I don't know how to transform this into a tensor without padding the entire dataset.\n\n``` python\nimport tensorflow as tf\n\n# [0, 1, 2, 3, 4 ,...]\nx = tf.range(1, 10, name=\"x\")\n\n# A queue that outputs 0,1,2,3,...\nrange_q = tf.train.range_input_producer(limit=5, shuffle=False)\nslice_end = range_q.dequeue()\n\n# Slice x to variable length, i.e. [0], [0, 1], [0, 1, 2], ....\ny = tf.slice(x, [0], [slice_end], name=\"y\")\n\nprint 'Dynamic shape of y:', tf.shape(y)\nprint 'Static shape of y:', y.get_shape()\n```\n\nOutput:\n\n```\nDynamic shape of y: Tensor(\"Shape_16:0\", shape=(1,), dtype=int32)\nStatic shape of y: (?,)\n```\n- Attempted replacing `tf.train.batch` line with the `tf.PaddingFIFOQueue` as follows\n\n``` python\n# Creating a new queue\npadding_q = tf.PaddingFIFOQueue(\n    capacity=10,\n    dtypes=tf.int32,\n    shapes=[[None]])\n\n# Enqueue the examples\nenqueue_op = padding_q.enqueue([y])\n\n# Add the queue runner to the graph\nqr = tf.train.QueueRunner(padding_q, [enqueue_op])\ntf.train.add_queue_runner(qr)\n\n# Dequeue padded data\nbatched_data = padding_q.dequeue_many(5)\n```\n\nOutput: identical to above\n- Using placeholders: tried replacing `y` with a placeholder of shape `[None]` and feeding it the data, cannot seem to get this to work either.\n- Padding entire dataset: this works, but defeats the purpose of dynamic padding\n- Todo: am going to try passing dataset through `tf.train.SequenceExample()`, constructing an example for each sequence, but I would rather this not be necessary.\n", "comments": ["@mmy - This seems to go wrong in autopacking code - which is happening before the dynamic_pad logic of batch gets to fix up the ragged input data.  Is this a problem with tf.train.batch arg validation?\n", "According to the documentation (and the implementation) the `tf.train.batch()` function expects either a list of tensors or a \"dictionary of tensors\" (i.e. a dictionary in which the values are tensors) as the `tensors` argument. If you pass a **list of lists** of tensors, it will try to interpret the inner list of tensors as a tensor, using the implicit packing code, and that fails here because the tensors aren't compatible as arguments to `tf.pack()`.\n\nIt would probably be more intuitive if the `tensors` argument to `tf.train.batch()` accept any arbitrary nesting of tensors that can be flattened using [`nest.flatten()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/nest.py#L83), but it looks like `nest.flatten()` doesn't support dictionaries, so we'd need to add that support for backwards compatibility.\n\nWhat a sad state of affairs. I'll mark this as \"contributions welcome\" for now. It looks like @ebrevdo and @kosklain have the most experience with the nesting code, so perhaps they can chime in if there's a reason not to take this route.\n", "Perhaps I am mistaken or am misunderstanding the documentation/implementation, but it I now believe that even the 'correct script' in my initial post is wrong (output not matching documentation). \n\n``` python\nimport tensorflow as tf\n\n# [0, 1, 2, 3, 4 ,...]\nx = tf.range(1, 10, name=\"x\")\n\n# A queue that outputs 0,1,2,3,...\nrange_q = tf.train.range_input_producer(limit=5, shuffle=False)\nslice_end = range_q.dequeue()\n\n# Slice x to variable length, i.e. [0], [0, 1], [0, 1, 2], ....\ny = tf.slice(x, [0], [slice_end], name=\"y\")\n\nbatched_data = tf.train.batch(\n    tensors=[y],\n    batch_size=5,\n    dynamic_pad=True,\n    name=\"y_batch\",\n    enqueue_many=False\n)\n\n# Run the graph\n# tf.contrib.learn takes care of starting the queues for us\nres = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n\n# Print the result\nprint(\"Batch shape: {}\".format(res[0][\"y\"].shape))\nprint(res[0][\"y\"])\n```\n\nOutput:\n\n```\nBatch shape: (5, 4)\n[[0 0 0 0]\n [1 0 0 0]\n [1 2 0 0]\n [1 2 3 0]\n [1 2 3 4]]\n```\n\nI believe this output should be expected if and only if the `enqueue_many` flag is set to `True`, as per the documentation, this means that the `tensors` in `tensors_list` are interpreted as having first dimension indexing the examples. Which is the case for `y` as it is defined.\n\nHowever, with `enqueue_many=True` we get the following output:\n\n```\nBatch shape: (5,)\n[1 1 2 1 2]\n```\n\nI believe passing a list of tensors of varying lengths to `tf.train.batch` should look like this (with `tensors=y` not `tensors=[y]`), however the output is not as expected.\n\n``` python\nimport tensorflow as tf\n\ny = [tf.constant(range(n)) for n in range(1,10)]\n\nbatched_data = tf.train.batch(\n    tensors=y,    #tensors=[y]\n    batch_size=5,\n    dynamic_pad=True,\n    name=\"y_batch\",\n    enqueue_many=False\n)\n\n# Run the graph\n# tf.contrib.learn takes care of starting the queues for us\nres = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n\n# Print the result\n# print(\"Batch shape: {}\".format(res[0][\"y\"].shape))\nprint(res[0][\"y\"])\n```\n\nOutput:\n\n```\n[array([[0],\n       [0],\n       [0],\n       [0],\n       [0]], dtype=int32), array([[0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1]], dtype=int32), array([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]], dtype=int32), array([[0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3]], dtype=int32), array([[0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4]], dtype=int32), array([[0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8]], dtype=int32)]\n```\n\nIt almost looks each example is being queued `batch_size` number of times, or perhaps being dequeued improperly.\n", "I'll answer regarding the behavior of your \"correct script\":\n\nWhen you call `tf.train.batch` with `enqueue_many=False`, here is the sequence of what is being added to the `PaddingFIFOQueue` underneath:\n\n```\nn=0, insert []  # the empty vector\nn=1, insert [1]\nn=2, insert [1, 2]\nn=3, insert [1, 2, 3]\nn=4, insert [1, 2, 3, 4]\n```\n\nnote in each case you are inserting a _single batch element_ which is a vector.  just that the vectors have varying lengths.  this is because you used `enqueue_many=False`, so each input Tensor is considered one single batch element.\n\nAs a result, when you `dequeue_many` with `batch_size=5` you are asking for these 5 elements back.  The `PaddingFIFOQueue` pads these entries to the length of the longest one in the minibatch (4, from `n=4`) and you get `5` rows out:\n\n```\n[       ]\n[1      ]\n[1 2    ]\n[1 2 3  ]\n[1 2 3 4]\n```\n\nnote that `PaddingFIFOQueue` pads integer valued outputs with `0`s.\n\nIn contrast, when you call `tf.train.batch` with `enqueue_many=True`, each enqueue of `y` is treated as some variable number of scalars to be added to the `PaddingFIFOQueue`.  That is, here is what gets added at each step:\n\n```\nn=0, insert nothing\nn=1, insert 1\nn=2, insert 1, then insert 2\nn=3, insert 1, then insert 2, then insert 3\nn=4, insert 1, then insert 2, then insert 3, then insert 4\n```\n\nYou then ask for `5` elements from this scalar component.  The `PaddingFIFOQueue` sees you are asking for 5 scalars and as a result, no padding is necessary.  It returns to you:\n\n```\n[1]  # the single entry from the n=1 insert\n[1]  # first entry from the n=2 inserts\n[2]  # second entry from the n=2 inserts\n[1]  # first entry from the n=3 inserts\n[2]  # second entry from the n=3 inserts\n```\n\n(except you saw it as a vector, here i've written it as a column vector to show that each row is a scalar, and is considered a minibatch entry)\n", "Thanks for the explanation @ebrevdo, I see that the output is as you have explained. My confusion came from how `y` was constructed, after looking more carefully it's clear that it has shape `(?,)` and its first dimension indexes the scalars, so the behavior is as expected.\n", "what to do then if I have a SequenceExample which is parsed, and evaluates to sequences like:\r\n[1, 2, 3, 5, 6]\r\n[1, 5, 1, 1]\r\n[9, 9, 1, 5, 6, 7, 7, 8, 4]\r\netc.\r\n\r\nHow I tested:\r\n```python\r\nfilename_queue = tf.train.string_input_producer(['cnn.TFRecord'])\r\ncontext_parsed, sequence_parsed = parse_example(filename_queue)\r\nwith tf.Session() as sess:\r\n    \r\n    coord = tf.train.Coordinator()\r\n    sess.run(tf.global_variables_initializer())\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    \r\n    batched_data = tf.train.batch(\r\n        tensors=[sequence_parsed['tokens']],\r\n        batch_size=3,\r\n        dynamic_pad=True\r\n    )\r\n\r\n    #res = sess.run(sequence_parsed['tokens'])\r\n    res = batched_data.eval()\r\n    print(res)\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n    sess.close()\r\n```\r\n\r\nbut this ends in an infinite loop, but what I was expecting is smthng like this:\r\n[1, 2, 3, 5, 6, 0, 0, 0, 0]\r\n[1, 5, 1, 1, 0, 0, 0, 0, 0]\r\n[9, 9, 1, 5, 6, 7, 7, 8, 4]", "Csaba, this issue is closed.  Please ask your question on StackOverflow.\n\nShort answer is: you can use the approach here (tf.train.batch with\ndynamic_pad=True), or you can use\ntf.contrib.training.bucket_by_sequence_length with dynamic_pad=True.\n\nOn Sat, Feb 18, 2017 at 8:15 AM, Csaba Botos <notifications@github.com>\nwrote:\n\n> what to do then if I have a SequenceExample which is parsed, and evaluates\n> to sequences like:\n> [1, 2, 3, 5, 6]\n> [1, 5, 1, 1]\n> [9, 9, 1, 5, 6, 7, 7, 8, 4]\n> etc.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4022#issuecomment-280855525>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_MuGn4HnWZq1GX91JQeO67bm0_Kks5rdxkPgaJpZM4JsVog>\n> .\n>\n"]}, {"number": 4021, "title": "Improving Google Indexing for the Documentation", "body": "Whenever I run a Google search on a TensorFlow functionality, say, `tf.reshape`, it gives me the entire documentation, not the specific documentation related to that functionality. \n\nCurrently the way I use is  to run a search with `ctrl` + `f` to find specific documentation related to what I search for. \n\nNumpy has that property, i.e. when you run a Google search on `np.reshape`, you get the specific page.\n\nIt would be a nice improvement for the documentation if someone fixes the Google indexing for the documentation page, especially for the users who frequently use Google search for the documentation.\n", "comments": ["@wolffg Is this something that might be addressed by the upcoming changes to documentation?\n", "(With delayed move to new docs) it did help\r\nWhen I search for `tf.reshape`, the first result I get is to tensorflow documentation in the new TF website.\r\n\r\n"]}, {"number": 4020, "title": "Branch 131182216", "body": "", "comments": ["tensorflow/python:file_io_test is flaky. Merging.\n"]}, {"number": 4019, "title": "./configure cannot find Python libraries", "body": "I can't get the configure step to work:\n\n```\n(py35) [david@SQUIDS tensorflow]$ ./configure \nPlease specify the location of python. [Default is /home/david/.virtualenvs/py35/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nTraceback (most recent call last):\n  File \"<stdin>\", line 14, in <module>\nAttributeError: module 'site' has no attribute 'getsitepackages'\nFound possible Python library paths:\nPlease input the desired Python library path to use.  Default is []\n\n\nln: failed to create symbolic link 'util/python/python_lib' -> '': No such file or directory\n(py35) [david@SQUIDS tensorflow]$ ./configure \nPlease specify the location of python. [Default is /home/david/.virtualenvs/py35/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\nGoogle Cloud Platform support will be enabled for TensorFlow\nTraceback (most recent call last):\n  File \"<stdin>\", line 14, in <module>\nAttributeError: module 'site' has no attribute 'getsitepackages'\nFound possible Python library paths:\nPlease input the desired Python library path to use.  Default is []\n\n\nln: failed to create symbolic link 'util/python/python_lib' -> '': No such file or directory\n```\n\nThe repo is clean, at this exact commt:\n\n```\n(py35) [david@SQUIDS tensorflow]$ git status \nOn branch master\nYour branch is up-to-date with 'origin/master'.\nnothing to commit, working directory clean\n(py35) [david@SQUIDS tensorflow]$ git log | head\ncommit 8a3107801d15bf8af36221ff5bca0b94bf44d6d3\nAuthor: Derek Murray <derek.murray@gmail.com>\nDate:   Tue Aug 23 21:58:57 2016 -0700\n\n    Disable tf_stream_executor in the CMake build. (#4000)\n\n    Temporary workaround for issue #3996.\n\ncommit cc3153a7a0a23533d14ead34db37e4ccd7892079\nAuthor: Egor-Krivov <egor.krivov@frtk.ru>\n\n```\n\nThe system is Fedora Linux with Python 3.5 installed in a virtualenv. Giving it\n`/home/david/.virtualenvs/py35/lib64/python3.5` seems to work, but I can't be sure (it is stuck trying to download http://pilotfiber.dl.sourceforge.net/project/boost/boost/1.61.0/boost_1_61_0.tar.gz, but with Firefox I can get it in 5 s).\n", "comments": ["This is not an officially supported configuration.  Please could you direct your question to StackOverflow.\n", "Sorry, what exactly is not supported? The documentation recommends using a virtualenv and Python > 3.3.\n", "We only officially support the following:\nOperating systems: Ubuntu Linux 14 LTE to 16. Mac OS X El Capitan. CentOS 7 and above\nCUDA: 7.0 - 7.5  (soon 8.0)\n\nFrom the error messages like\n`ln: failed to create symbolic link 'util/python/python_lib' -> '': No such file or directory`\nit looks like python_config.sh doesn't understand your system.\n\nThis is possibly related to #3933 ?\n"]}, {"number": 4018, "title": "GPU build configuration doesn't work with Debian-packaged cuda/cudnn packages", "body": "I'm on a Debian 'sid' linux computer and I am attempting to build a GPU-enabled version of TensorFlow using the Debian-packaged versions of the Nvidia libraries.\n\nThe CUDA packages are from Debian's non-free repository, e.g. https://packages.debian.org/sid/nvidia-cuda-toolkit and cuDNN I installed from using the .deb packages available from Nvidia's site (libcudnn5_5.1.3-1+cuda7.5_amd64.deb  and libcudnn5-dev_5.1.3-1+cuda7.5_amd64.deb).\n\nThese packages put the libraries and include files in standard system locations, so there is no CUDA_TOOLKIT_PATH or CUDNN_INSTALL_PATH to speak of, as this ldconfig output shows:\n\nforrest@makemake:~$ /sbin/ldconfig -p | grep 'cud'\n    libicudata.so.57 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libicudata.so.57\n    libicudata.so.55 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libicudata.so.55\n    libicudata.so.55 (libc6) => /usr/lib/i386-linux-gnu/libicudata.so.55\n    libicudata.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libicudata.so\n    libcudnn.so.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudnn.so.5\n    libcudnn.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudnn.so\n    libcudart.so.7.5 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so.7.5\n    libcudart.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudart.so\n    libcuda.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so.1\n    libcuda.so.1 (libc6) => /usr/lib/i386-linux-gnu/libcuda.so.1\n    libcuda.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so\n    libcuda.so (libc6) => /usr/lib/i386-linux-gnu/libcuda.so\n\nI have attempted to hack the configure file to support this default Debian configuration, but so far bazel always seems to expect CUDA_INSTALL_DIR to be set.\n", "comments": ["For starters, you should show which error messages you run into.\n", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "I'm having the same issue as @fcahoon and will try to add some more information on what the problem is.\r\n\r\nI'm on Debian 8 (Jessie), cuda toolkit 7.5 and cudNN 5.1. The cuda toolkit was, however, not installed using the installer from Nvidia, but via the system package manager (in my case from the jessie-backports repo, in @fcahoon's case using the .deb packages directly downloaded from Nvidia). This places all cuda libraries in standard system locations as shown above instead of bunching them together in, e.g. `/usr/local/cuda/`.\r\n\r\nNow, trying to build tf from source (I cloned the git repo and checked out tag `v1.2.0`), the `./configure` script asks for the cuda toolkit location:\r\n\r\n```\r\n[...]\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] N\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\r\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n```\r\n\r\nbut there is no \"installation location\" to speak of, and the build would not really need any, as `#include <cuda.h>` and `-lcuda` work just fine without any path specifications (all files being in standard locations).\r\n\r\nThe exact command to reproduce is thus just `./configure` on a system with cuda installed from the `.deb` packages.", "Same issue here. Could we support standard FHS compatible installation of CUDA in our configure script? That would be nice to a lot of people.", "Same problem here also, any solutions?", "I can confirm the issue.\r\nI hacked a little bit the `.tf_configure.bazelrc` file with the debian path:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --force_python=py3\r\nbuild --host_force_python=py3\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild --define with_jemalloc=true\r\nbuild:gcp --define with_gcp_support=true\r\nbuild:hdfs --define with_hdfs_support=true\r\nbuild:s3 --define with_s3_support=true\r\nbuild:kafka --define with_kafka_support=true\r\nbuild --define with_xla_support=true\r\nbuild --define with_gdr_support=true\r\nbuild --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/include/\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.1\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu/\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env TF_CUDA_CLANG=\"1\"\r\nbuild --action_env TF_DOWNLOAD_CLANG=\"0\"\r\nbuild --action_env CLANG_CUDA_COMPILER_PATH=\"/usr/bin/clang\"\r\nbuild --config=cuda_clang\r\ntest --config=cuda_clang\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n```\r\n\r\nI get when I try bazel:\r\n```\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"/home/mte90/Desktop/kde/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1142\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"/home/mte90/Desktop/kde/tensorflow/third_party/gpus/cuda_configure.bzl\", line 997, in _create_local_cuda_repository\r\n                _find_cuda_include_path(repository_ctx, cuda_config)\r\n        File \"/home/mte90/Desktop/kde/tensorflow/third_party/gpus/cuda_configure.bzl\", line 688, in _find_cuda_include_path\r\n                auto_configure_fail((\"Cannot find cuda.h under %s\" %...))\r\n        File \"/home/mte90/Desktop/kde/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cuda.h under /usr/include\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"/home/mte90/Desktop/kde/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1142\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"/home/mte90/Desktop/kde/tensorflow/third_party/gpus/cuda_configure.bzl\", line 997, in _create_local_cuda_repository\r\n                _find_cuda_include_path(repository_ctx, cuda_config)\r\n        File \"/home/mte90/Desktop/kde/tensorflow/third_party/gpus/cuda_configure.bzl\", line 688, in _find_cuda_include_path\r\n                auto_configure_fail((\"Cannot find cuda.h under %s\" %...))\r\n        File \"/home/mte90/Desktop/kde/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find cuda.h under /usr/include\r\nINFO: Elapsed time: 0.168s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\nUseless to say that the file `/usr/include/cuda.h` exist but for bazel seems not..."]}, {"number": 4017, "title": "Add PSGLD optimizer", "body": "I add an algorithm, Preconditioned Stochastic Gradient Langevin Dynamics (PSGLD), for training optimization, which\n- derives from Stochastic Gradient Langevin Dynamics (SGLD) and SGD,\n- requires only the gradient on mini-batches of data,\n- incorperates adaptive preconditioners from RMSProp.\n\nAnd it effectively accelerates the convergence speed compared with the existed algorithms, while basically maintaining the accuracy of prediction.\n", "comments": ["Thank you for your contribution. This is really nice work, but we think that for now it may be better for this to live outside of the official repo; we can add it later if it proves to be useful to the wider community.\n", "Thankd for your comments! @rmlarsen\n"]}, {"number": 4016, "title": "Initialize layers.convolution2d from numpy array", "body": "I can't find a way to pass a numpy tensor to layers.convolution2d weights/bias initialization arguments.\nI added support for this by modifying convolution2d, code is below. I need this feature in order  If you like this solution I can add this feature to other ops also and create a pull request.\n\nFirst I tried this:\n\n```\nimport tensorflow as tf\nimport tensorflow.contrib.layers as layers\nimport numpy as np\n\nconv1_1 = np.random.rand(3, 3, 3, 64).astype(dtype=np.float32)\ninputs = tf.placeholder(tf.float32, shape=(32, 96, 96, 3))\nnet = layers.convolution2d(inputs, 64, 3, weights_initializer=conv1_1, scope='conv1_1')\n```\n\nbut get_variable method doesn't allow redundant shape argument when initializing from constant.\n\n```\nTraceback (most recent call last):\n  File \"tf1.py\", line 16, in <module>\n    net = layers.convolution2d(inputs, 64, 3, weights_initializer=conv1_1, scope='conv1_1')\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 171, in func_with_args\n    return func(*args, **current_args)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 406, in convolution2d\n    trainable=trainable)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 171, in func_with_args\n    return func(*args, **current_args)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 266, in model_variable\n    caching_device=caching_device, device=device)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 171, in func_with_args\n    return func(*args, **current_args)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 230, in variable\n    caching_device=caching_device)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 873, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 700, in get_variable\n    custom_getter=custom_getter)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 217, in get_variable\n    validate_shape=validate_shape)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 202, in _true_getter\n    caching_device=caching_device, validate_shape=validate_shape)\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 479, in _get_single_variable\n    raise ValueError(\"If initializer is a constant, do not specify shape.\")\nValueError: If initializer is a constant, do not specify shape.\n```\n\nI removed redundant arguments but then convolution2d doesn't have default value for them. \n\n```\nnet = layers.convolution2d(inputs, weights_initializer=conv1_1, scope='conv1_1')\n```\n\n```\nTraceback (most recent call last):\n  File \"tf1.py\", line 14, in <module>\n    net = layers.convolution2d(inputs, weights_initializer=conv1_1, scope='conv1_1')\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 171, in func_with_args\n    return func(*args, **current_args)\nTypeError: convolution2d() missing 2 required positional arguments: 'num_outputs' and 'kernel_size'\n```\n\nI fixed the problem by adding support inside convolution2d code here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L322\nIt works now and I don't have to pass redundant arguments.\n\n```\n@add_arg_scope\ndef convolution2d(inputs,\n                  num_outputs=None,\n                  kernel_size=None,\n                  stride=1,\n                  padding='SAME',\n                  rate=1,\n                  activation_fn=nn.relu,\n                  normalizer_fn=None,\n                  normalizer_params=None,\n                  weights_initializer=initializers.xavier_initializer(),\n                  weights_regularizer=None,\n                  biases_initializer=init_ops.zeros_initializer,\n                  biases_regularizer=None,\n                  reuse=None,\n                  variables_collections=None,\n                  outputs_collections=None,\n                  trainable=True,\n                  scope=None):\n  with variable_scope.variable_op_scope([inputs],\n                                        scope, 'Conv', reuse=reuse) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    dtype = inputs.dtype.base_dtype\n    stride_h, stride_w = utils.two_element_tuple(stride)\n    if rate > 1 and (stride_h > 1 or stride_w > 1):\n      raise ValueError('Only one of rate or stride can be larger than one')\n    num_filters_in = utils.last_dimension(inputs.get_shape(), min_rank=4)\n\n    initializing_from_value = False\n    if weights_initializer is not None and not callable(weights_initializer):\n      initializing_from_value = True\n      weights_shape = None\n    else:\n      if kernel_size == None or num_outputs == None:\n        raise ValueError('Kernel size and number of outputs must be defined')\n      kernel_h, kernel_w = utils.two_element_tuple(kernel_size)\n      weights_shape = [kernel_h, kernel_w,\n                       num_filters_in, num_outputs]\n    if biases_initializer is not None and not callable(biases_initializer):\n      bias_shape = None\n    else:\n      if num_outputs == None:\n        raise ValueError('Number of outputs must be defined')\n      bias_shape = [num_outputs]\n\n    weights_collections = utils.get_variable_collections(\n        variables_collections, 'weights')\n    weights = variables.model_variable('weights',\n                                       shape=weights_shape,\n                                       dtype=dtype,\n                                       initializer=weights_initializer,\n                                       regularizer=weights_regularizer,\n                                       collections=weights_collections,\n                                       trainable=trainable)\n    if rate > 1:\n      outputs = nn.atrous_conv2d(inputs, weights, rate, padding=padding)\n    else:\n      outputs = nn.conv2d(inputs, weights, [1, stride_h, stride_w, 1],\n                          padding=padding)\n    if normalizer_fn:\n      normalizer_params = normalizer_params or {}\n      outputs = normalizer_fn(outputs, **normalizer_params)\n    else:\n      if biases_initializer is not None:\n        biases_collections = utils.get_variable_collections(\n            variables_collections, 'biases')\n        biases = variables.model_variable('biases',\n                                          shape=bias_shape,\n                                          dtype=dtype,\n                                          initializer=biases_initializer,\n                                          regularizer=biases_regularizer,\n                                          collections=biases_collections,\n                                          trainable=trainable)\n        outputs = nn.bias_add(outputs, biases)\n    if activation_fn:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n```\n", "comments": ["Try passing a lambda function instead.\n\n```\nconv1_1 = np.random.rand(3, 3, 3, 64).astype(dtype=np.float32)\ninputs = tf.placeholder(tf.float32, shape=(32, 96, 96, 3))\nnet = layers.convolution2d(inputs, 64, 3, weights_initializer=lambda x: conv1_1, scope='conv1_1')\n```\n\nThe solution would be to change [variables.variable](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py#L197)  to use shape to validate the initial_value instead of passing it to get_variable()\n", "With lambda function initializing_from_value flag will stay false in _get_single_variable():\n\n```\nTraceback (most recent call last):\n...\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 281, in _init_from_args\n    self._initial_value = ops.convert_to_tensor(initial_value(),\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 526, in <lambda>\n    init_val = lambda: initializer(shape.as_list(), dtype=dtype)\nTypeError: <lambda>() got an unexpected keyword argument 'dtype'\n```\n\nI did what you suggested. variables.variable() is a better place for a fix. Below is the code which works for me. Is that ok, should I create a pull request for this?\n\n```\n@contrib_add_arg_scope\ndef variable(name, shape=None, dtype=dtypes.float32, initializer=None,\n             regularizer=None, trainable=True, collections=None,\n             caching_device=None, device=None):\n  # Remove duplicates\n  collections = set(collections)\n  # If initializing from numpy array don't pass shape to get_variable()\n  if initializer is not None and not callable(initializer):\n    if shape != list(initializer.shape):\n        raise ValueError(\"Shape doesn't match initializer shape: %s != %s.\"\n                         % (shape, list(initializer.shape)))\n    shape = None\n  with ops.device(device or ''):\n    return variable_scope.get_variable(name, shape=shape, dtype=dtype,\n                                       initializer=initializer,\n                                       regularizer=regularizer,\n                                       trainable=trainable,\n                                       collections=collections,\n                                       caching_device=caching_device)\n```\n", "For small variables initializing them from a value makes sense, however you should know that those values becomes constants in the graph and make the graph bigger, specially if the variables are big.\n\nYou can also look at [assign_from_values_fn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py#L467) which allows you assign values to variables without including them in the graph.\n\nA change in variables.variable would require extra checks and verifications.\n", "Ok thanks a lot. I did not know that TF keeps a copy of the init values as a constant in the graph and I was not aware of assign_from_values as it is not in the docs yet. From now on I will use assign_from_values and save some memory.\n", "@ivankreso  how did you solve it? I faced it again. thks lot about your solvement or code!"]}, {"number": 4015, "title": "Consider that arg keep_prob to dropout might be an int", "body": "Calling\n\n```\ntf.nn.dropout(layer, dropout=0.0)\n```\n\nraises a `ValueError`, as expected. However, since the condition for the ValueError reads as\n\n```\nif isinstance(keep_prob, float) and not 0 < keep_prob <= 1:\n    raise ValueError(...)\n```\n\na call like this succeeds:\n\n```\ntf.nn.dropout(layer, dropout=0)\n```\n\nThis problem occurs whenever an int is passed that is not 1 (the only allowed int value). This PR contains a simple workaround by considering that the argument might be an int.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "Updated the commit to check if keep_prob is a real number.\n", "@tensorflow-jenkins test this please\n", "@gnebehay thanks for fixing this!\n"]}, {"number": 4014, "title": "Register missing gradient for tf.nn.max_pool_with_argmax", "body": "Registering the gradient should fix issue #1793. \n\nThe corresponding test is already present in [pooling_ops_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/pooling_ops_test.py) (correct me if I'm wrong).\n", "comments": ["Can one of the admins verify this patch?\n", "Please add corresponding unit tests.\n", "sorry, you are correct, the tests are already there.\n", "@tensorflow-jenkins test this please\n", "(if the tests were already there, why wasn't the test failing because of a lack of a gradient?)\n", "@fabianschilling could you please investigate why the tests were not failing before. Modify the unit tests or add a new one that would fail and demonstrates the need for the registration.\n", "I will look into it!\n\nBtw, the CPU tests were failing because of an assertion error in `file_io_test.py`:\n\n```\n============== BEGINS failure log content ==============\n....................F.....\n======================================================================\nFAIL: testStat (__main__.FileIoTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/workspace/pip_test/tests/file_io_test.py\", line 279, in testStat\n    self.assertEqual(33188, file_statistics.mode)\nAssertionError: 33188 != 33206\n\n----------------------------------------------------------------------\nRan 26 tests in 0.257s\n\nFAILED (failures=1)\n============== ENDS failure log content ==============\n```\n", "@fabianschilling don't worry about the file_io test. It is flaky.\n", "@fabianschilling Please also rebase, since the branch now has conflicts.\n", "@fabianschilling please rebase, then I can run a final test and possibly merge. \n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Talked with @fabianschilling, will close this PR, he'll make a new one.\n", "What is the status on this? Is there any work around for using the gradients of `tf.nn.max_pool_with_argmax` at the current `master`?"]}, {"number": 4013, "title": "different types of GPUs", "body": "I have 4 GPUS, which are\ngpu0, GTX 1080\ngpu1, old TITAN X\ngpu2, old TITAN X\ngpu3, GTX 1080\nwhen I run a CNN, it shows the information below, what does `cannot enable peer access from device ordinal 0 to device ordinal 1` mean?\nDoes it mean that I cannot use multiple GPUs since they are not identical(I noticed that I only have one gpu running during the training)?\nWhat is the consequence of this info?\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:61] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:61] cannot enable peer access from device ordinal 0 to device ordinal 2\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:61] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:61] cannot enable peer access from device ordinal 1 to device ordinal 3\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:61] cannot enable peer access from device ordinal 2 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:61] cannot enable peer access from device ordinal 2 to device ordinal 3\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:61] cannot enable peer access from device ordinal 3 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:61] cannot enable peer access from device ordinal 3 to device ordinal 2\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y N N Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 1:   N Y Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 2:   N Y Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 3:   Y N N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)\n```\n", "comments": ["This usually happens when the GPUs are on different PCI buses.  The connectivity matrix looks a little strange in this case.\n", "I think this is working as intended.  The DMA matrix shows which GPUs can talk as 'peers' to each other (e.g., via direct GPU to GPU copies using DMA, rather than going through CPU).\n\nBecause you are using two generations of cards, they can't talk to each other, it seems.  The only implication is that the bandwidth between all the GPUs is not uniform, but you can still use all 4 GPUs in the same model and TensorFlow will properly use them.\n", "So far there is no way to mix different types of gpu using some kind of load balancing? Or tensorflow does not support GPU of different types? I have 2 TitanX and 2 1080TI, very similar to the situation above.", "@chakpongchung You can use different GPU types in TensorFlow, that is well supported.  See our installation page for instructions about mixing different compute capabilities, if that's the problem.\r\n\r\nTF does not do dynamic load balancing across GPUs today; given a dataflow graph, it statically assigns work to devices.  So far this has worked well enough that dynamic GPU selection has not been needed."]}, {"number": 4012, "title": "Add favicon to TensorBoard", "body": "Adding `favicon.ico` (TensorFlow's, I imagine) would avoid the following warning on launch and beautify TensorBoard. ;)\n\n`WARNING:tensorflow:IOError [Errno 2] No such file or directory: '.../tensorflow/tensorboard/favicon.ico' on path .../tensorflow/tensorboard/favicon.ico`\n", "comments": ["That would be nice, if you could contribute one.", "Actually, the following datauri favicon was added by e42fa0069.\r\n![download](https://cloud.githubusercontent.com/assets/6004563/22270119/5030c808-e275-11e6-8fc8-e83cbbef104c.png)\r\n"]}, {"number": 4011, "title": "Is there anyway to run a tensorflow graph (.pb or .meta) generated by single node tf.sess on a distributed environments with multiple ps nodes and worker nodes?", "body": "Will future versions of tensorflow provide a way to run the tensorflow graph generated by single node tf.sess on a distributed environments with multiple ps nodes and worker nodes through python interfaces?\nOr is it supported right now?\n\nI am trying to build my tf.graph on my notebook (single node) and save then graph into a binary file, \nand then loading the binary graph into a distributed environment (with multiply ps and worker nodes) to train and verify it. It seems it is not supported now.\n\nI tried it on tensorflow-0.10 and failed.\nBy using tf.train.write_graph(sess.graph_def, path, pb_name) interface: The graph saved is not trainable as loading the .pb file through import_graph_def will only g.create_ops according to the '.bp' file but not add then into ops.collections. So the graph loaded is not trainable\nBy using tf.saver.save to save a \".meta\" file: The loaded graph cannot fit into the distributed environment as devices assignment is messy. \nI tried the tf.train.import_meta_graph('test_model.meta', clear_devices=True) interface to let the load clean the original device assignment and let the \"with tf.device(device_setter)\" reassign the device for each variable, but there is a problem as operations belonging to \"Saver\" and \"Restore\" still can not be assigned correctly. When creating operations for \"Saver\" and \"Restore\" ops through g.create_op inside import_graph_def called by import_meta_graph, the device_setter will not assign ps node to these ops as their name is not \"Variable\".\nIs there any way to do so?\n", "comments": ["This is a general usage question best suited to StackOverflow.  Please can you re-ask it there.\n"]}, {"number": 4010, "title": "Fix DFS in quantize_graph.py", "body": "Mark nodes as visited before recursing into their children to avoid infinite loops (e.g. when quantizing graphs using tf.dynamic_rnn construction).\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@ilya-edrenkin  Thanks for the fix.\n"]}, {"number": 4009, "title": "tf.image.decode_image would be nice (handling both png and jpeg)", "body": "`tf.image.decode_jpeg` crashes on a seemingly valid input image with an  `InvalidArgumentError: Invalid JPEG data, size 107746`. I've used both the current nightly from tonight, as well as TF 0.9.0, using Python 3.5.2 on CentOS 7 on two different machines.\n\nThe image  can be opened/displayed without problems in Firefox, GIMP and other image viewers, as well as with the `PIL` image library within Python. The image is part of the ILSVRC2015 dataset, `n02105855/n02105855_2933.JPEG`. I've  uploaded it to http://imgur.com/a/pblKL for your reference (i hope imgur doesn't recode the image, let me know). \n\nThis is a minimal code example:\n\n```\nimport tensorflow as tf\nfn = './imagenet/ILSVRC2015/Data/CLS-LOC/train/n02105855/n02105855_2933.JPEG'\nwith tf.Graph().as_default():\n    image_contents = tf.read_file(fn)\n    image = tf.image.decode_jpeg(image_contents, channels=3)\n    init_op = tf.initialize_all_tables()\n    with tf.Session() as sess:\n        sess.run(init_op)\n        tmp = sess.run(image)\n```\n\nWhich crashes with the following error:\n\n```\nInvalidArgumentError: Invalid JPEG data, size 107746\n [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ReadFile)]]\nCaused by op 'DecodeJpeg', defined at:\n```\n\nFor reference, here the full stack trace is here: http://pastebin.com/GZMWDjge\n", "comments": ["That is a PNG.\n", "Actually, I'll leave this open in case and renamed in case someone wants to finally make `decode_image` which handles both.  It's almost possible to do it in Python, or it could be done in C++ with a bunch of code sharing with `decode_png` and `decode_jpeg`.\n", "Cc @dave-andersen to trigger painful memories. :)\n", "Also, anyone is welcome to make the error message for `decode_png` or `decode_jpeg` include a suitably escaped version of the first few bytes, which generally give it away.\n", "@girving would using Python's [imghdr](https://docs.python.org/3.4/library/imghdr.html) library and calling the appropriate `decode_<type>` function suffice?\n", "Except for using a separate library, yes.  Just check magic numbers instead: it would be a few lines of pure Python if we only had a `tf.startswith` op for strings, so adding `tf.startswith` is one way.\n", "Cool. I whipped up a first pass with tests in #4222, but will need to hard code the `imghdr.what()` function. It's already [implemented in pure Python](https://hg.python.org/cpython/file/2.7/Lib/imghdr.py), so the core logic is already available.\n", "@girving what do you think of a `tf.substring_match` op instead of `tf.startswith`? Should be more flexible in the long-term, and allows me to keep using the lazy way of detecting JPEGs that imghdr already uses :)\n\nOn further inspection, it looks it may be difficult to pull this off without doing the entire implementation in C++, as you can't read the bytes of the image before the graph is executed. That means there can't be any pure Python if/else blocks that require knowledge of the image type/header. The best I can think of for a mostly-Python implementation involves way more `tf.cond` operations than reasonable. What are your thoughts?\n", "@samjabrahams I'm not sure I follow.  By \"way more `tf.cond` operations than reasonable\", do you mean 2?  The main advantage of doing it in C++ is nicer error reporting; I don't see any other advantages.\n", "@girving - not sure why, but last night I thought it would be three. Sorry for that! It may make more sense to just use a `tf.case()` op for the sake extensibility (easier to add GIF/bitmap support). \n\nOne final question: what is the best way to throw an error for the default case (bytes not recognized as any available image type)? The easiest thing I can think of right now is simply raising an error in a `py_func` op, but I don't know whether or not `py_func` is apropos for Op wrappers.\n", "You can use `tf.Assert` to raise errors.  As for `tf.substring_match`: what would it do, and how would it be better than `tf.startswith`?\n", "Awesome, thanks. `tf.substring_match` would check to see if a selected subsection of a string (or array of strings), `s1`, matches another passed in string, `s2`. It would return true if the substring matches, false otherwise. You can think of `tf.startswith` as a special case of `tf.substring_match`, where the selected subsection is at the beginning of `s1`. \n\nProposed api:\n\n```\nArgs:\n  s1: Tensor of type `string`. The strings that will have a substring compared against `s2`\n  s2: 0-D Tensor of type `string`. The string that the `s1` substring(s) will be compared against\n  start_idx: 0-D integer. The starting index of the substring in `s1` \n```\n", "@samjabrahams It'd be better to write `tf.substr` and then use `tf.equal`.\n", "@girving Sounds good. Opening up an issue to track `tf.substr`.\n", "This is great, guys; just check the magic I guess.\n", "Fixed long ago by db490561b3d093a4eac952be8d4607234b6ef9f1.", "@untom ,  I meet a silimar question with you when i train models on google TensorFlow Object Detection API, May i ask if the question has been solved? what's the problem and how to solved? thank you!"]}]