[{"number": 16505, "title": "Remove BOM", "body": "These two files started with \"Byte Order Mark\" `<U+FEFF>` which we don't want.", "comments": []}, {"number": 16504, "title": "Issue propagating gradients through tf.while_loop", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu wheezy\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-4-g9283868\r\ntf.COMPILER_VERSION = v1.4.0-4-g9283868\r\nSanity check: array([1], dtype=int32)\r\n\r\n- **Python version**: \r\n3.5\r\n\r\n### Describe the problem\r\n\r\nI've found a few issues when trying to propagate gradients through tf.while_loops.\r\n\r\nOne issue is ops like this inside the loop body break gradients \r\n```k = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )```\r\n\r\nbut more concerningly, conjoined conditions such as this:\r\n```tf.logical_and( k < max_its-1, chg_w > tol  )```\r\nor even this\r\n``` tf.cast( max_its-k, DTYPE) *(chg_w - tol)```\r\nbreaks the differentiablity across the while loop.\r\n\r\n\r\n### Source code / logs\r\n``` python\r\ntf.reset_default_graph()\r\nsess = tf.InteractiveSession()\r\ng = tf.Graph().as_default()\r\n\r\nmax_its = 10\r\ntol = 1e-3\r\n\r\nc = tf.constant( np.arange(100), dtype=DTYPE)\r\nw = tf.Variable( initial_value=np.ones(100),  dtype=DTYPE)/100\r\nk = tf.Variable( 0, dtype=tf.int32 )\r\nchg_w = tf.constant( np.inf, dtype=DTYPE )\r\n\r\n\r\ndef _eg_step( k, w, chg_w): \r\n    grad = tf.gradients( -tf.reduce_sum( w * c ) , w )[0]\r\n    w_n = w * tf.exp( -0.1  * grad )\r\n    w_n = w_n / tf.reduce_sum( w_n ) \r\n    chg_w = tf.reduce_sum( tf.abs( w_n - w) ) / tf.reduce_sum( tf.abs( w ) )\r\n    k = k + 1\r\n    **# !! this busts the differentiablity !!\r\n    # k = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )**\r\n    return k, w_n, chg_w\r\n\r\ndef _continue_cond( k, w, chg_w, *args ):\r\n    **# NOTE either of this conjoined conditions\r\n    #        tf.logical_and( k < max_its-1, chg_w > tol  )\r\n    # OR     tf.cast( max_its-k, DTYPE) *(chg_w - tol)\r\n    # do no propagate gradients correctly**\r\n    return  k < max_its # tf.logical_and( k < max_its-1, chg_w > tol  )\r\n\r\nk, w, chg_w = tf.while_loop(\r\n    cond=_continue_cond,  body=_eg_step,\r\n    loop_vars=[k, w, chg_w],\r\n    name='while_loop', parallel_iterations=1\r\n)\r\n\r\n# see if the gradient is propagated\r\ntf.gradients( w, c)\r\n\r\n```", "comments": ["I think I have observed the problem with `Print`. Thank you for having that simple repro case! Do you think you could submit a PR to fix this?\r\n\r\nRegarding the logical conditions, could you be more specific about what gradients gets zero?\r\n\r\nCC @zffchen78 @skye ", "@drpngx  what is a PR ? \r\n\r\nregarding the logical conditions:\r\n\r\nIf the _continue_cond is a conjoined condition , the gradient of output[round_t]/<something> becomes [None], where as if the condition is  a simple expression grad output[round_t]/<something> is correctly evaluated.\r\n\r\nFor example - in the above example if\r\n```\r\ndef _continue_cond( k, w, chg_w, *args ):\r\n    return  k < max_its\r\n```\r\nthen tensorflow can correctly evaluate `tf.gradients( w, c)`\r\n\r\nBut if I make it:\r\n```\r\ndef _continue_cond( k, w, chg_w, *args ):\r\n    return tf.logical_and( k < max_its-1, chg_w > tol  )\r\n    # OR     tf.cast( max_its-k, DTYPE) *(chg_w - tol)\r\n```\r\n\r\nthen `tf.gradients( w, c) = [None]`\r\n\r\n", "Sorry, I meant a pull request, ie a bugfix.", "I'm not sure I have a bug fix - since I don't know what the root of the problem is  :)", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have a candidate fix for this.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The candidate fix is scheduled for a design review in a couple of weeks. It's going through design review because it changes the behavior of tf.gradients.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The change passed design review and will go in soon. Someone can reassign this issue to me. (I cannot do it.)", "Unfortunately we can currently only assign to TF org members :\\", "This issue should be fixed in the master branch.\r\nSearch for testIssue16504 in [control_flow_ops_py_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/control_flow_ops_py_test.py).", "Woohoo!"]}, {"number": 16503, "title": "Enable multi-dimensional and axis support for tf.unique_with_counts", "body": "This fix tries to address the issue raised in #16499 to bring multi-dimensional and axis support for `unique_with_counts`.\r\n\r\nWhen `UniqueV2` kernel was added in #12952, it actually already implemented the multi-dimensional and axis support for `unique_with_counts` as well, just not registered.\r\n\r\nThis fix:\r\n1. Register `UniqueWithCountsV2` kernel to have axis support.\r\n2. Hide both `UniqueWithCounts` and `UniqueWithCountsV2`\r\n3. Add python unique_with_counts wrapper to call `gen_array_ops._unique_with_counts`\r\n4. If API review passes and the PR merges, `unique_with_counts` will switch to `gen_array_ops._unique_with_counts_v2` (in 3 weeks).\r\n5. Add additional test cases for `gen_array_ops._unique_with_counts_v2`.\r\n\r\nThis fix fixes #16499.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @a-dai for the review. The PR has been updated to resolve merge conflict.", "Which version is this pull on? Because I have tf 1.11 and I do not see axis in documentation, nor can I find unique of more than 1-D.\r\n![image](https://user-images.githubusercontent.com/30283914/50317322-d69cec00-046f-11e9-9ef8-b8789322f3e0.png)\r\n", "Apparently it's not public yet due to API deprecation (https://github.com/yongtang/tensorflow/blob/r1.13/tensorflow/python/ops/array_ops.py#L1444 has a comment saying as much at the time of this writing). In the meantime while we're all wondering whether something that doesn't break existing uses should count as an \"API deprecation\" and delay release for months in the first place, you can dig up the nd implementation like so:\r\n\r\n```python\r\nfrom tensorflow.python.ops import gen_array_ops\r\ngen_array_ops.unique_with_counts_v2(foo, axis=bar)\r\n```", "@cooijmanstim You import `gen_array_ops` from `tensorflow.python.ops` and then you use `unique_with_counts_v2` from `tf.gen_array_ops`? Something does not seem right in your example. In any case, your example does not work with TF 2.1.", "@nbro Whoops, removed the `tf.` prefix. Otherwise seems to still work.\r\n\r\nEdit: @nbro your solution below does something very different than `unique_with_counts_v2` with an `axis` argument would do.", "@cooijmanstim I implemented a simpler solution than this. See https://stackoverflow.com/a/60046022/3924118.", "@cooijmanstim how unique_v2 works?\r\n```\r\nfrom tensorflow.python.ops import gen_array_ops\r\nx = np.array([[[0, 0], [0, 0], [0, 0], [0, 1]], [[1,1],[1,1], [0, 2], [0, 2]]])\r\ngen_array_ops.unique_v2(x, axis=[1])\r\n```\r\nI would expect as output [[[0,0],[0,1]],  [[1,1],[0,2]] but I get\r\n```\r\nUniqueV2(y=<tf.Tensor: shape=(2, 3, 2), dtype=int64, numpy=\r\narray([[[0, 0],\r\n        [0, 0],\r\n        [0, 1]],\r\n\r\n       [[1, 1],\r\n        [0, 2],\r\n        [0, 2]]])>, idx=<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 1, 2], dtype=int32)>)\r\n\r\n```", "@mindis In your example, I think you were expecting it to treat leading axes (here axis 0) as a batch axis. The reason it doesn't work like that is probably because the output would be ragged if different batch axes had different numbers of unique slices. For example, what would you expect if you modified your example input to `np.array([[[0, 0], [0, 0], [0, 0], [0, 1]], [[1,1],[1,1], [0, 2], [0, 3]]])`?\r\n\r\nSo what does `unique_v2` do? From what I've seen, I believe this would be the equivalent (using numpy for simplicity):\r\n\r\n```python\r\ndef unique_v2(x, axis):\r\n  assert 0 <= axis < x.ndim # no support for negative or multiple axes in this example\r\n  colons = tuple(slice(None) for _ in range(x.ndim)) # akin to np.index_exp[:, :, ..., :]\r\n\r\n  unique_slices = []\r\n  for i in range(x.shape[axis]):\r\n    candidate = x[(*colons[:axis], i, *colons[axis+1:])]\r\n    if _not_in(candidate, unique_slices): # read: `candidate not in unique_slices`\r\n      unique_slices.append(candidate)\r\n\r\n  return np.stack(unique_slices, axis=axis)\r\n```\r\n\r\nWith this annoying little helper to work around [the numpy `__eq__` tragedy](https://stackoverflow.com/a/6065203/7601527):\r\n\r\n```python\r\ndef _not_in(x, ys):\r\n  for y in ys:\r\n    if np.array_equal(x, y):\r\n      return False\r\n  return True\r\n```", "Came here after stumbling across dozens of merge requests and issues.\r\n- @nbro: Your solution will not work since the `idx` that you're reshaping with `tf.reshape(idx, shape=tf.shape(t))` is still have the indices from original input and not for each segment. \r\n    - `unique_labels_idx` from below example is not-so-useful when using with for example `tf.math.segment_sum` or `tf.math.unsorted_segment_sum`. It will generate wrong output.\r\n```python\r\ninput_tensor = tf.constant(\r\n  [\r\n    [0.5 0.5 0.4 0.1]\r\n    [0.8 0.5 0.5 0.5]\r\n    [0.8 0.7 0.3 0.1]\r\n  ]\r\n)\r\n\r\ndef get_uniques(t):\r\n    t1d = tf.reshape(t, shape=(-1,))\r\n    # or tf.unique, if you don't need counts\r\n    uniques, idx, counts = tf.unique_with_counts(t1d) \r\n    return uniques, tf.reshape(idx, shape=tf.shape(t)), counts\r\n\r\n>> get_uniques(input_tensor)\r\n\r\n# unique_labels\r\ntf.Tensor([2. 3. 1. 4.], shape=(4,), dtype=float32)\r\n\r\n# unique_labels_idx\r\ntf.Tensor(\r\n[[0 0 1 2]\r\n [3 0 2 2]\r\n [3 1 1 2]], shape=(3, 4), dtype=int32)\r\n\r\n# unique_labels_counts\r\ntf.Tensor([3 3 4 2], shape=(4,), dtype=int32)\r\n```", "when will `unique_with_counts_v2` be exposed via the python API?"]}, {"number": 16502, "title": "Java Android API: No OpKernel was registered to support Op 'ListDiff'", "body": "### System information\r\nAndroid with Java API releases 1.4.0 and 1.5.0-rc1 (1.5 is not available yet)\r\n\r\n### What I did\r\nI have a graph that is applying a `tf.layers.dense` on a three dimensional tensor, which is applying a dense operation to the last dimension. Learning and execution on a Windows 10 and Ubuntu work fine.\r\n\r\nNow I froze the model and put it on Android and receive the following error.\r\n\r\n```\r\n01-27 20:06:59.628 10481-11380/de.test.local W/System.err: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'ListDiff' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n01-27 20:06:59.628 10481-11380/de.test.local W/System.err:   <no registered kernels>\r\n01-27 20:06:59.628 10481-11380/de.test.local W/System.err: \t [[Node: model/logits/Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](model/logits/Tensordot/range, model/logits/Tensordot/add_1)]]\r\n01-27 20:06:59.628 10481-11380/de.test.local W/System.err:     at java.util.concurrent.FutureTask.report(FutureTask.java:94)\r\n01-27 20:06:59.628 10481-11380/de.test.local W/System.err:     at java.util.concurrent.FutureTask.get(FutureTask.java:164)\r\n01-27 20:06:59.628 10481-11380/de.test.local W/System.err:     at de.test.service.TaskWorkerLoop$Loop.run(TaskWorkerLoop.java:71)\r\n01-27 20:06:59.628 10481-11380/de.test.local W/System.err:     at java.lang.Thread.run(Thread.java:762)\r\n01-27 20:06:59.629 10481-11380/de.test.local W/System.err: Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'ListDiff' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n01-27 20:06:59.629 10481-11380/de.test.local W/System.err:   <no registered kernels>\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err: \t [[Node: model/logits/Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](model/logits/Tensordot/range, model/logits/Tensordot/add_1)]]\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java.lang.reflect.Constructor.newInstance0(Native Method)\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java.lang.reflect.Constructor.newInstance(Constructor.java:430)\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:565)\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:646)\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:704)\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:195)\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:210)\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:459)\r\n01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:624)\r\n...\r\n```\r\n\r\nIs there a reason for this kernel to be missing on Android? If not, can you please add it? And what could I do in the mean time to replace the dense layer?\r\n\r\nMany thanks in advance!", "comments": ["any progress on this?", "@andreas-eberle The JavaCPP Presets for TensorFlow now come with the official Java API as well, and the patch you provided for Android appears to be including ListDiff, so you could try these binaries out:\r\nhttp://bytedeco.org/builds/ \r\nhttps://github.com/bytedeco/javacpp-presets/tree/master/tensorflow", "I am getting the same error when using `tf.layers.dense` or `tf.tensordot(tensor, weights, axes=[-1, 0])` on 3-dimensional tensor on Android", "for now, it is possible to achieve the same result by doing `layer = tf.einsum('abc,cd->abd', tensor, weights) + bias` as a single dense layer on higher dimensional tensors", "@bryanlimy @andreas-eberle This is now working with TF 1.12, so this issue may be closed, unless it still doesn't work for you.\r\n"]}, {"number": 16501, "title": "Define Cr, Fr, Shared, Var to resolved undefined names", "body": "flake8 testing of https://github.com/tensorflow/tensorflow\r\n\r\n$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__\r\n```\r\n./tensorflow/contrib/specs/python/specs_test.py:202:11: F821 undefined name 'Cr'\r\n      _ = Cr\r\n          ^\r\n./tensorflow/contrib/specs/python/specs_test.py:204:28: F821 undefined name 'Cr'\r\n      self.assertIsNotNone(Cr)\r\n                           ^\r\n./tensorflow/contrib/specs/python/specs_test.py:205:32: F821 undefined name 'Cr'\r\n      self.assertTrue(callable(Cr(64, [3, 3])))\r\n                               ^\r\n./tensorflow/contrib/specs/python/specs_test.py:207:11: F821 undefined name 'Cr'\r\n      _ = Cr\r\n          ^\r\n./tensorflow/contrib/specs/python/specs_test.py:215:13: F821 undefined name 'Var'\r\n        v = Var(\"test_var\",\r\n            ^\r\n./tensorflow/contrib/specs/python/specs_test.py:232:13: F821 undefined name 'Shared'\r\n        f = Shared(Fr(100))\r\n            ^\r\n./tensorflow/contrib/specs/python/specs_test.py:232:20: F821 undefined name 'Fr'\r\n        f = Shared(Fr(100))\r\n                   ^\r\n```", "comments": ["Looks like this test still doesn't pass. It probably involves a more involved fix. Closing for now, but feel free to reopen a pull request if you want to work on it."]}, {"number": 16500, "title": "contrib/learn: Typo in variable name x_exrta --> x_extra", "body": "flake8 testing of https://github.com/tensorflow/tensorflow\r\n\r\n$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__\r\n```\r\n./tensorflow/contrib/learn/python/learn/datasets/synthetic.py:156:32: F821 undefined name 'x_extra'\r\n    spir_x = np.append(spir_x, x_extra)\r\n                               ^\r\n```", "comments": ["@cclauss Thanks for the fix. Is it possible to add unit-test coverage for this?", "@caisq Thanks for your positive comment.  Above, I have already provided the test case for this issue.\r\n\r\n$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__\r\n\r\nThe issues behind all [9 of my PRs](https://github.com/tensorflow/tensorflow/pulls/cclauss) to date were found using this same [flake8](http://flake8.pycqa.org) test.  I would be happy to work with you to get this test (esp. F821) implemented across the TensorFlow codebase as there are currently __24 undefined names in Python 2 and 33 undefined names in Python 3__.\r\n\r\n[Pylint](https://github.com/PyCQA) does not currently find undefined names.\r\n", "@cclauss The `flake8` command you included was helpful. However, I was asking about whether you could add a Python unit test in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/synthetic_test.py that exercises the previously problematic line, to prevent possible regressions in the future.", "@caisq I usually use pytest instead of unittest.  Did the syntax get this right?"]}, {"number": 16499, "title": "Extend tf.unique_with_counts to multi-dimensional tensors", "body": "I'm trying to solve KNN using tensorflow. After I get the K neighbours for N vectors, I have a N by K tensor. Now, for each vector in N, I need to use [```tf.unique_with_counts```][1] to find the majority vote. However, I cannot iterate in a tensor and I cannot run [```tf.unique_with_counts```][1] with a multi-dimensional tensor. It keeps giving me ```InvalidArgumentError (see above for traceback): unique expects a 1D vector.```\r\n\r\nWhy can't tf support multi-demsional input?\r\n\r\nExample:\r\n\r\n    def knnVote():\r\n    '''\r\n    KNN using majority vote\r\n    '''\r\n    #nearest indices\r\n    A = tf.constant([1, 1, 2, 4, 4, 4, 7, 8, 8])\r\n    nearest_k_y, idx, votes = tf.unique_with_counts(A)\r\n    print(\"y\", nearest_k_y.eval())\r\n    print(\"idx\", idx.eval())\r\n    print(\"votes\", votes.eval())\r\n    majority = tf.argmax(votes)\r\n    predict_res = tf.gather(nearest_k_y, majority)\r\n    \r\n    \r\n    print(\"majority\", majority.eval())\r\n    print(\"predict\", predict_res.eval())\r\n    return predict_res\r\n\r\nResult:\r\n\r\n    y [1 2 4 7 8]\r\n    idx [0 0 1 2 2 2 3 4 4]\r\n    votes [2 1 3 1 2]\r\n    majority 2\r\n    predict 4\r\n\r\nBut how can I extend this to N by D input A, such as the case when ```A = tf.constant([[1, 1, 2, 4, 4, 4, 7, 8, 8],\r\n[2, 2, 3, 3, 3, 4, 4, 5, 6]])```\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/unique_with_counts", "comments": ["The multi-dimensional and axis support for unique_with_counts actually has already been implemented when UniqueV2 kernel was added in #12952. It is just not registered and exposed to python API.\r\n\r\nAdded a PR #16503 to address the issue."]}, {"number": 16498, "title": "Bounding box do not remove", "body": "Hi there,\r\nAfter I detected my tv, it showed up but when i move it to other place, it do not remove the bounding box even though i put my camera on the table. Is this a bug?\r\n\r\n![2018-01-28-00-34-50](https://user-images.githubusercontent.com/32919949/35474032-9eb4827e-03c3-11e8-8768-c4c81f9ad391.png)\r\n", "comments": ["Is that mobilenet?", "HI there, thanks for reply.\r\nI am using Android Studio to run the code and this is the TFDetect part.", "Hi there,\r\nI have figure out this problem and solve it. Thank you for your help.\r\n\r\nIn MultiboxTracker.java\r\nif (rectsToTrack.isEmpty()) {\r\n      trackedObjects.clear(); //add this code to remove the bounding box when nothing is detected\r\n      logger.v(\"Nothing to track, aborting.\");\r\n      return;\r\n}", "For context this should only an issue if real-time tracking is not enabled (i.e. if libtensorflow_demo.so is not found). When tracking is active the normalized cross-correlation check of the object's initial vs current appearance will catch the track loss and remove the box automatically.\r\n\r\nThis is probably not the optimal behavior, so a PR to tweak it would be appreciated (though we do not want to destroy the persistence in the tracking-enabled scenario as this edit would).", "Ok, thank you. :)", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing since it sounds like you're good to go.", "@woozhanwei  \r\nI add trackedObjects.clear(); //add this code to remove the bounding box when nothing is detected\r\nBut box tracked a few times and disappeared\u3002\r\nHave you ever met this problem\uff1f", "Hi there,\r\nSorry for the late reply. Is the boundary box disappear permanently?\r\n\r\nBest Regards,\r\nWoo Zhan Wei\r\n________________________________\r\nFrom: jackweiwang <notifications@github.com>\r\nSent: Monday, July 30, 2018 6:25 PM\r\nTo: tensorflow/tensorflow\r\nCc: woozhanwei; Mention\r\nSubject: Re: [tensorflow/tensorflow] Bounding box do not remove (#16498)\r\n\r\n\r\n@woozhanwei<https://github.com/woozhanwei>\r\nI add trackedObjects.clear(); //add this code to remove the bounding box when nothing is detected\r\nBut box tracked a few times and disappeared\u3002\r\nHave you ever met this problem\uff1f\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/16498#issuecomment-408817563>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfZRjeZNjA1NOthgBvmFBcRuB8MUSo7aks5uLt83gaJpZM4RvWlh>.\r\n", "@woozhanwei\r\nI use the latest version of tensorflow, the problem has been solved, thanks.", "Ok, you're welcome.\n\n\nBest Regards,\nWoo Zhan Wei\n________________________________\nFrom: jackweiwang <notifications@github.com>\nSent: Monday, August 13, 2018 1:31 PM\nTo: tensorflow/tensorflow\nCc: woozhanwei; Mention\nSubject: Re: [tensorflow/tensorflow] Bounding box do not remove (#16498)\n\n\n@woozhanwei<https://github.com/woozhanwei>\nI use the latest version of tensorflow, the problem has been solved, thanks\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/16498#issuecomment-412411892>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfZRjRj3bEXZhdiN0ctmEZM0fU0cAyf8ks5uQQ8wgaJpZM4RvWlh>.\n"]}, {"number": 16497, "title": "SpatialConvolution in tflite is very slow", "body": "Hi, i build tflite ,using bazel build --cxxopt='--std=c++11' //tensorflow/contrib/lite/java:tensorflowlite \r\n--crosstool_top=//external:android/crosstool \\\r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n--cpu=armeabi-v7a\r\nI test my CNN model, but it's very slow in google piexl. I analyze time cost of my CNN model. SpatialConvolution function is very slow, only 20%  of the peak flops of my hardware.\r\nmy model's data type is kTfLiteFloat32. \r\nIs build script for android right \uff1fI doubt my build script is wrong , so run the SpatialConvolution function with ARM NEON.  (I don't know how Eigen is organized in tflite, i can't add log code in the source code to prove my suspicion)", "comments": ["Can you try a few things to help us get more information:\r\n\r\n - Is your model slow when run in Android on regular TensorFlow (not TF Lite)?\r\n\r\n - If it is, can you run the steps described [here](https://www.tensorflow.org/mobile/optimizing) in \"Profiling your model\" and add the log output?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hey! \r\nSomething similar happens to me recently with a really simple convolutional encoder. The \".tflite\" file exported with this:\r\nhttps://colab.research.google.com/drive/1KMQQjuSfurog3n-Ut7BMSfv1TXGFonbX#scrollTo=WZlb35eHCchl\r\n\r\nAnd executed with \"implementation 'org.tensorflow:tensorflow-lite:+'\" in Android was almost twice slow than the exported as a usual \".pb\" and executed with \"implementation 'org.tensorflow:tensorflow-android:+'\" in Android.\r\n", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 89 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this one, since original poster did not clarify and provide reproducible test cases.\r\n\r\n@ianholing :  Please create a new bug with reproducible tests "]}, {"number": 16496, "title": "Feature Suggestion: \"Float-bit-strings\"", "body": "### System information (Not really relevant ...)\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 8 (?)\r\n- **GPU model and memory**: GTX 1070\r\n- **Exact command to reproduce**: NA\r\n\r\n### Summary\r\n\r\nThis proposes the use of what I call \"float-bit-strings\" or \"float-bits\" instead of one-hot-encoded arrays so as to greatly reduce the memory and computational usage e.g. in language models.\r\n\r\nI don't think this preliminary discussion belongs on StackOverflow so I hope it is OK to post it here. It is a new feature that could be added to TensorFlow. There's quite likely somebody on the TensorFlow dev-team or in the community who has already thought of this. But I have searched the internet and cannot find any mentioning of a similar idea.\r\n\r\n\r\n### Background\r\n\r\nI have started looking at language-models using e.g. LSTM and encoder-decoder architectures. There are some aspects that seem to be incredibly wasteful and limiting. Let me briefly describe this and please forgive me if I am ignorant, I have only spent a week or two on studying LSTM and language models so far :-)\r\n\r\nFor example in Machine Translation we typically have the text-data for the source- and target-languages as lists of integer-tokens, where each integer maps to a word in the vocabulary. There may be e.g. 100k different words so these integer-tokens can take on values between zero and 100k. This data cannot be input directly to a Neural Network so we use an embedding layer to convert these integers to n-dimensional vectors with values between zero and one, according to a mapping-function that may either be loaded from disk or trained along with the rest of the Neural Network; if I understand correctly.\r\n\r\nFor the decoder in a language model, we have a similar problem where we must somehow convert integer-tokens to data that the neural network can work on. A typical way of doing this seems to be a one-hot encoding; if I understand correctly. (This could also be done for the encoder-part, but it doesn't seem to be necessary).\r\n\r\nI can't figure out what the max-size of one-hot encodings are in TensorFlow and whether it can even handle 100k one-hot encoded tensors. But it is obviously an extremely wasteful data-mapping. For example, for a vocab of 100k words we only need 17-bits (log2(100k)) to represent each integer-token - but for a one-hot encoding using 32-bit floats we need 32 x 100k bits!\r\n\r\nI can't figure out what people normally do, but it seems like the common practice is to limit the vocab to a smaller number of words, e.g. 1k or 10k. It appears that Google Translate runs on multiple GPU's and maybe that's why they can handle extremely large vocabs with one-hot encoded tensors?\r\n\r\n\r\n### Float-bit-strings\r\n\r\nI thought it might be possible to use a bit-string-like representation inside a TensorFlow model. I have searched the internet and cannot find anyone who has proposed a similar idea.\r\n\r\nThe idea is to convert each integer-token to what I call a \"float-bit-string\" or \"float-bits\". For example, the number 123 has the bit-string 01111011. We can then make a corresponding tensor with floats [0., 1., 1., 1., 1., 0., 1., 1.] and input this to the TensorFlow model.\r\n\r\nIn a language model we would then have to input and output these \"float-bits\" instead of one-hot encoded arrays. This would dramatically reduce the memory and computational requirements of the models.\r\n\r\n\r\n### Test\r\n\r\nI have hacked together a little test using numpy and Keras / TensorFlow. The idea is to see if we can learn to map integers x with values between 0 and 10k to y = 123 * x using these \"float-bit\" encodings. And it works as you can see by running the code further below! That is perhaps not a surprise as neural networks are general function approximators, but it's not always that they work according to theory :-)\r\n\r\nHowever, the network cannot learn the arithmetic mapping of e.g. y = 123 * x when x and y are \"float-bits\". This means it cannot generalize to data it hasn't seen during training in the arithmetic manner we might expect. But I don't think that is necessary for use in e.g. language models where we merely want to be able to map some tensor from e.g. an LSTM to an integer-token from the vocabulary.\r\n\r\n\r\n### Loss Functions\r\n\r\nI have tested this with both MSE and binary cross-entropy in Keras, which unfortunately isn't documented so I'm not completely sure what it does. But in both cases it works and the model trains to get the bit-wise mapping correct.\r\n\r\nThere might be cases where you are more concerned about the MSE between the actual integer-values instead of their \"float-bit-string\" representations, in which case we would need a TensorFlow method to convert \"float-bits\" to integers and then take the MSE of the resulting integer and the true integer from the data-set. This is not relevant for language models, because the proximity of integer-keys do not correspond to words that are necessarily similar in meaning. But it could be useful in other applications.\r\n\r\n\r\n### TensorFlow Implementation\r\n\r\nIn order to make this work in TensorFlow it seems that we just need a couple of TensorFlow-methods for converting between integers and \"float-bit-strings\". I have hacked this together using numpy but I'm sure somebody on the dev-team can make a super-fast native TensorFlow implementation. Then we just need a wrapper in Keras and that might be enough to do e.g. language models with gigantic vocabs.\r\n\r\n\r\n### Test-Code\r\n\r\n    import numpy as np\r\n    from tensorflow.python.keras.models import Sequential\r\n    from tensorflow.python.keras.layers import InputLayer\r\n    from tensorflow.python.keras.layers import Dense\r\n    from tensorflow.python.keras.optimizers import RMSprop\r\n    \r\n    \r\n    # Number of bits to use in our \"float-bit-strings\".\r\n    num_bits = 32\r\n    \r\n    def int_to_floatbits(value):\r\n        \"\"\"\r\n        Convert a single integer value to an array of 0.0 and 1.0 floats\r\n        corresponding to the bit-string.\r\n    \r\n        Example: value==123 gives [0.  ... 0.  1.  1.  1.  1.  0.  1.  1.]\r\n        \"\"\"\r\n    \r\n        # Convert the integer value to a bit-string.\r\n        # NOTE: This has been fixed to 32-bit length.\r\n        bitstr = \"{0:032b}\".format(value)\r\n    \r\n        # Convert the bit-string to an array of equivalent float-values.\r\n        floatbits = np.array([1.0 if bit == '1' else 0.0 for bit in bitstr])\r\n    \r\n        return floatbits\r\n    \r\n    \r\n    def floatbits_to_strbits(floatbits):\r\n        \"\"\"\r\n        Convert an array of floats to a bit-string.\r\n        A float value greater than 0.5 results in 1.0\r\n        and a float value less or equal to 0.5 results in 0.0\r\n    \r\n        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] gives \"001110\"\r\n        \"\"\"\r\n    \r\n        # Convert the float-array to a list of bit-characters '0' or '1'.\r\n        charbits = ['1' if floatbit > 0.5 else '0' for floatbit in floatbits]\r\n    \r\n        # Convert the bit-characters to a string.\r\n        strbits = \"\".join(charbits)\r\n    \r\n        return strbits\r\n    \r\n    def floatbits_to_int(floatbits):\r\n        \"\"\"\r\n        Convert a float-array to an integer, assuming each element\r\n        of the float-array corresponds to a bit.\r\n        \r\n        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] corresponds to\r\n        the bit-string \"001110\" which is the integer 14.\r\n        \"\"\"\r\n    \r\n        # Convert the float-array to a bit-string.\r\n        strbits = floatbits_to_strbits(floatbits=floatbits)\r\n    \r\n        # Convert the bit-string to an integer value.\r\n        value = int(strbits, base=2)\r\n    \r\n        return value\r\n    \r\n    \r\n    # Various tests of the above functions.\r\n    if True:\r\n        foo = int_to_floatbits(123)\r\n        print(foo)\r\n        print(floatbits_to_strbits(foo))\r\n        print(floatbits_to_int(foo))\r\n    \r\n        bar = [0.3,  0.9,  0.8,  0.51,  0.501,  0.4999,  0.999,  1.1]\r\n        print(floatbits_to_strbits(bar))\r\n        print(floatbits_to_int(bar))\r\n    \r\n        baz = [0.1, 0.49, 0.51, 0.9, 1.1, -2.3]\r\n        print(floatbits_to_strbits(baz))\r\n        print(floatbits_to_int(baz))\r\n    \r\n    # quit()\r\n    \r\n    # We will now train a TensorFlow / Keras model\r\n    # that maps integers between 0 and 10000 to\r\n    # the same numbers multiplied by 123.\r\n    # If we were to use one-hot encoding then we would\r\n    # need 10000 inputs to the Neural Network and\r\n    # 1230000 outputs if using the full output range.\r\n    # Using \"bit-strings\" encoded as floats, we only need\r\n    # 14 bits for the input and 21 bits for the output.\r\n    # We round it up to 32-bits.\r\n    \r\n    # The dataset as integers,\r\n    # we want the Neural Network to map from x to y.\r\n    x_int = np.arange(10000, dtype=int)\r\n    y_int = 123 * x_int\r\n    \r\n    # Convert the dataset to \"float-bit-strings\" (aka. float-bits).\r\n    x = np.array(list(map(int_to_floatbits, x_int)))\r\n    y_true = np.array(list(map(int_to_floatbits, y_int)))\r\n    \r\n    # Check the mapping is correct. E.g. if the number of required bits\r\n    # exceeds num_bits then these may not create numpy matrices correctly.\r\n    if False:\r\n        print(x.shape)\r\n        print(y_true.shape)\r\n        print(x[0:10])\r\n        print(y_true[0:10])\r\n    \r\n    # Start construction of the Keras Sequential model.\r\n    model = Sequential()\r\n    \r\n    # Add an input layer to the model.\r\n    model.add(InputLayer(input_shape=(num_bits,)))\r\n    \r\n    # Fully-connected / dense layers with ReLU-activation.\r\n    model.add(Dense(512, activation='relu'))\r\n    model.add(Dense(512, activation='relu'))\r\n    \r\n    # Last fully-connected / dense layer with sigmoid-activation\r\n    # so the output is between 0.0 and 1.0\r\n    model.add(Dense(num_bits, activation='sigmoid'))\r\n    \r\n    optimizer = RMSprop(lr=1e-3)\r\n    \r\n    if True:\r\n        # Loss is MSE.\r\n        model.compile(optimizer=optimizer,\r\n                      loss='mean_squared_error')\r\n    else:\r\n        # Loss is Binary Crossentropy, but also report MSE.\r\n        model.compile(optimizer=optimizer,\r\n                      loss='binary_crossentropy',\r\n                      metrics=['mse'])\r\n    \r\n    epochs = 50\r\n    \r\n    if True:\r\n        # Fit the model using the entire data-set.\r\n        model.fit(x, y_true, epochs=epochs)\r\n    else:\r\n        # Fit the model using the data-set split into training and validation.\r\n        # You will see that the validation-error is high so the model\r\n        # has not learned the arithmetic function of the data-set.\r\n        model.fit(x, y_true, epochs=epochs, validation_split=0.2)\r\n    \r\n    # Use the model to predict the output for a part of the data-set.\r\n    y_pred = model.predict(x[0:10])\r\n    \r\n    # The true output for this part of the data-set.\r\n    y_true_subset = y_true[0:10]\r\n    \r\n    # Map the \"float-bit-strings\" to integers.\r\n    y_pred_int = list(map(floatbits_to_int, y_pred))\r\n    y_true_int = list(map(floatbits_to_int, y_true_subset))\r\n    \r\n    # Print the predicted and true integers.\r\n    print(*zip(y_pred_int, y_true_int))\r\n    \r\n    # Round the float-bit-strings to 2 decimals for pretty printing.\r\n    def rounded(numbers):\r\n        return np.array([[\"{:.2f}\".format(x) for x in row] for row in numbers])\r\n    y_pred_rounded = rounded(y_pred)\r\n    y_true_rounded = rounded(y_true_subset)\r\n    \r\n    # Print the predicted and true float-bit-strings.\r\n    # (I know it is bad to reuse the same variable-names here ...)\r\n    for y_pred_int, y_true_int, y_pred_rounded, y_true_rounded \\\r\n        in zip(y_pred_int, y_true_int, y_pred_rounded, y_true_rounded):\r\n    \r\n        print(y_true_int, \"\\t\", y_true_rounded)\r\n        print(y_pred_int, \"\\t\", y_pred_rounded)\r\n        print()\r\n\r\n\r\n### Output\r\n\r\nTrue integer and true \"float-bit-string\" (note that the numbers are all exactly 0.00 or 1.00):\r\n\r\n\t738 \t ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']\r\n\r\nPredicted integer and predicted \"float-bit-string\" (note that the numbers a **not** all exactly 0.00 or 1.00):\r\n\r\n\t738 \t ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.03' '0.00' '0.00' '0.00' '0.01' '0.00' '0.00' '0.00' '0.00' '0.99' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']\r\n", "comments": ["I think it's an interesting idea. In general, this could be useful if there are special patterns in the data, for instance, if words IDs are sorted by order of frequency, the first bit predictions would share that.\r\n\r\nUsually, though, it's better to make a global prediction (all bits at the same time) rather than individual decisions. The space is quite hard to describe. Let's say you have words like this:\r\n```\r\nID; word\r\n00; small\r\n01; big\r\n10; medium\r\n11; tiny\r\n```\r\n\r\nFurther supposing that the cost of mispredicting is according to size, e.g. big vs tiny is worse than small vs tiny. Then, when predicting bit `1`, then it really depends what bit `0` was set to.\r\n\r\nIn any case, feel free to think more about the topic and maybe find some interesting application for this. I'm going to close this for admin purposes but feel free to re-open.", "I have done a tiny bit of experimentation with this in the context of Machine Translation of human language. When it is possible to use a so-called embedding layer then that is far superior to these \"float-bits\" because the embedding layer also learns semantic similarities between words in a language.\r\n\r\nRegarding the use of \"float-bits\" instead of one-hot encoded arrays, my small experiments suggest that one-hot encoding is also much better than \"float-bits\". My guess is that it is because one-hot encoding is far more forgiving to the imprecise mappings performed by a neural network, while \"float-bits\" pack a lot more information in a very small number of floating-point values, so it is much more sensitive to tiny variations in the output of a neural network.\r\n\r\nNevertheless, perhaps someone in the future will stumble upon this thread and find the idea useful for something completely different. In that case I have added a bit more code below.\r\n\r\nThis function allows you to set the number of bits used in the \"float-bit\" encoding.\r\n\r\n    def int_to_floatbits(value, num_bits):\r\n        \"\"\"\r\n        Convert a single integer value to an array of 0.0 and 1.0 floats\r\n        corresponding to the bit-string. The length is given by num_bits.\r\n\r\n        Example: value==123 gives [0.  ... 0.  1.  1.  1.  1.  0.  1.  1.]\r\n        \"\"\"\r\n\r\n        # Formatting-string used to convert integer to binary-string.\r\n        # For example, \"016b\" for num_bits==16\r\n        format_str = \"0{}b\".format(num_bits)\r\n\r\n        bitstr = format(value, format_str)\r\n        floatbits = [1 if bit == '1' else 0 for bit in bitstr]\r\n        floatbits = np.array(floatbits)\r\n    \r\n        return floatbits\r\n\r\nThe next function is a more efficient implementation when converting huge arrays of integers to \"float-bits\". Note that for memory-efficiency we use `np.uint8` to store these 0 and 1's instead of floating-points (the name \"float-bits\" is perhaps a bit misleading now). This is because I used this with huge data-sets so it saved many GB of memory using `uint8` instead of `float32`, and `uint8` can automatically be type-cast by Keras / TensorFlow when input to a neural network.\r\n\r\n    def ints_to_floatbits(values, num_bits):\r\n        \"\"\"\r\n        Convert a 2-dim array of integer values to a 3-dim array of 0.0 and 1.0\r\n        floats corresponding to the bit-strings. The length of each encoding\r\n        is given by num_bits.\r\n        \"\"\"\r\n\r\n        # Formatting-string used to convert integer to binary-string.\r\n        # For example, \"016b\" for num_bits==16\r\n        format_str = \"0{}b\".format(num_bits)\r\n\r\n        # Shape of output array.\r\n        shape = values.shape + (num_bits,)\r\n    \r\n        # Pre-allocate output array for efficiency.\r\n        # This can be several GB!\r\n        floatbits = np.zeros(shape, dtype=np.uint8)\r\n\r\n        # Process all input values.\r\n        for i, row in enumerate(values):\r\n            for j, value in enumerate(row):\r\n                bitstr = format(value, format_str)\r\n                floatbits[i, j] = [1 if bit == '1' else 0 for bit in bitstr]\r\n\r\n                # The above is much faster than the following, even\r\n                # though we avoid the creation of the bit-string.\r\n                # floatbits[i, j] = [(value >> k) & 1 for k in reversed(range(num_bits))]\r\n            \r\n        return floatbits\r\n", "Nice! Again, if you find a dataset where the hamming distance correlates well with the distance, then you should try that."]}, {"number": 16495, "title": "Update docs for installing CUDA/CUDNN", "body": "This fix addresses the issue raised in #16479 where CUDA/CUDNN versions from the docs do not match TensorFlow `v1.5.0`.\r\n\r\nFrom the Dockerfile and from the env of docker images, the version of CUDA/CUDNN for TensorFlow `v1.5.0`:\r\n```\r\nCUDA_VERSION 9.0.176\r\nCUDNN_VERSION 7.0.5.15\r\n```\r\n\r\nThis fix updates the doc so that CUDA version is changed from `8.0` -> `9.0`, CUDNN version is changed from `6.0` -> `7.0`.\r\n\r\nThis fix fixes #16479.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16494, "title": "Enable [no]unroll for Clang on Windows", "body": "#15990", "comments": []}, {"number": 16493, "title": "Keras \"Output missing from loss dictionary\"", "body": "I think that this warning can be rephrased:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/engine/training.py#L640-L641\r\n\r\nSome outputs could be consumed as TB summary and not involved in any loss. \r\nEspecially when you use an estimator converted from a tf.keras model  you have not the explicit control of the model_fn for placing summaries. \r\n\r\nSo I suppose that one of the entry point for connecting summaries to the graph is the tf.train.SessionRunHook.\r\n\r\nI.e. you can produce the output from the Dataset api, consume it as TB summary in the SessionRunHook without using it in any loss.\r\n\r\nSee also https://github.com/tensorflow/tensorflow/issues/14879#issuecomment-351996902 \r\n\r\n/cc @fchollet ", "comments": ["Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 100 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 116 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 131 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 148 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ewilderj As you can see for something quite trivial it is impossibile to understand why a triage is missing after 148 days. I cannot reverse engineering the status of this issue.", "@bhack What do you think the warning should be and can you if possible share details and code snippet of how you are using the output that does not have a corresponding loss (may be the code you use to log summaries) ? These details would help us understand your use case better and improve the code accordingly.\r\n\r\nThank you,\r\nPavithra", "@pavithrasv  The use case it is quite simple. Suppose that one of your input from the Dataset api is just a reference index batch. You could consume in a text summary but of course not in the loss. So I think that the check and warning need to works only when the input is not consumed by anything. One endpoint could be the loss but also a summary is a valid endpoint. Its need to be the same use case as the unsued variable in a linter.", "Thank you for the explanation @bhack.\r\n\r\nDo you train using `fit` or some other way? Can you please share code snippet for how you use the additional output with summaries? I am asking because we currently do one to one mapping between the loss+metrics labels and the outputs obtained from the session as seen here:\r\nhttps://github.com/keras-team/keras/blob/master/keras/engine/training_arrays.py#L156. This would mean that any output that does not correspond to a loss/metric will be dropped.\r\n", "No I was using `model_to_estimator` and `train_and_evaluate` path.", "@pavithrasv I don't know if is clear. The problem is in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L211. \r\nWe cannot only check that  `if name not in loss:` and notify:\r\n```\r\n\"and we will not be expecting '\r\n'any data to be passed to \"' + name + '\" during training.'\"\r\n```\r\nCause this is not true and misleading. An output could be simply involved in a `tf.summary` only. So the data will pass from inputs to outputs for every batch but these outputs are not involved in a loss like \"normal\" outputs and input labes.", "@bhack How do you get a handle to the output that is not used in loss when using model_to_estimator? \r\nHere is what I have tried: https://gist.github.com/pavithrasv/2077a1e4be619cc000d573180b863ea3\r\n\r\nIn the above snippet, `dense_1` is the output that is not being used in loss. But I do not have a way to get a handle to that output from the estimator instance. \r\n\r\nAs I have mentioned before it is difficult for us to be able to reproduce the issue without a code snippet. It would be great if you can share your code snippet.\r\n\r\nThank you!", "@pavithrasv I've commented your gist directly.\r\nI understand that consuming dense_1 is not so easy and everybody want a simpler method to consume these kind of things with `model_to_estimator` but this is what we have now with this kind of high level api.\r\n\r\nI want to try to support you but the bug was opened on Jan so It could be a little bit hard to me to invest time to extract snippets as I had in Jan/Feb/March cause I am on another activity now.\r\n\r\nI hope that we can solve this interacting on your gist especially if the code will be ready to copy'n'paste in a colab notebook. \r\n\r\n ", "Changing the warning message to 'Output \"' + name + '\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"' + name + '\".'", "Thank you for reporting this. The issue has been fixed by https://github.com/tensorflow/tensorflow/commit/f836031f97d4b2ef2160f71332ee9c96a20ce84a#diff-de9b96ac2d81503324cbbbe21732031f\r\n\r\nThis will be available in the next nightly release. Please try and let us know if it works for you."]}, {"number": 16492, "title": "Clang on Windows will define __BYTE_ORDER__ etc. for us", "body": "#15990", "comments": []}, {"number": 16491, "title": "Remove all_opensource_files", "body": "Fixes #15758\r\n@gunan /cc\r\n@yifeif /cc", "comments": ["I have found a few additional internal checks that depend on this, so we are not ready to merge this PR yet.\r\nWe need to discuss how we can move those checks to not use bazel.", "Looks like we will still need to export some of the files or change their visibility so other targets can access them. How should  we go about this? @gunan any suggestion?", "I think we need some filegroup rules, and new data dependencies on these filegroup rules. Those fixes should be a part of this, as they will be \"thinner\" replacements to the all_files rules these tests are already depending on.\r\nCould you go through the broken tests and triage the build failures?", "@Androbin do you mind adding this suggested fix:\r\n`/tmpfs/src/github/tensorflow/tensorflow/core/ops/compat/BUILD:33:1: no such target '//tensorflow/core:ops/ops.pbtxt': target 'ops/ops.pbtxt' not declared in package 'tensorflow/core'; however, a source file of this name exists.  (Perhaps add 'exports_files([\"ops/ops.pbtxt\"])' to tensorflow/core/BUILD?) defined by /tmpfs/src/github/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/ops/compat:backwards_compatibility_test'`", "Good catch!", "Last build failed because of `image_retraining`.\r\nRebased on last successful build and resolved merge conflicts.", "Looks like there are a few more:\r\n`ERROR: /tmpfs/src/github/tensorflow/tensorflow/examples/image_retraining/BUILD:28:1: no such target '//tensorflow/examples/label_image:data/grace_hopper.jpg': target 'data/grace_hopper.jpg' not declared in package 'tensorflow/examples/label_image'; however, a source file of this name exists.  (Perhaps add 'exports_files([\"data/grace_hopper.jpg\"])' to tensorflow/examples/label_image/BUILD?) defined by /tmpfs/src/github/tensorflow/tensorflow/examples/label_image/BUILD and referenced by '//tensorflow/examples/image_retraining:retrain_test'`\r\n", "Okay, that made `bazel nobuild` pass locally.", "Last build succeeded (256848f993ba00ffd9849bd621acd3e60aec42fc).\r\nHad to resolve minor merge conflict (620348fb6d045dc1f644925a3828ebb12de944d7).", "Thanks @Androbin! Let's see how the builds go.", "We had some infra issue earlier. It should be fine now.", "Last build successful (99f5d4e17d360b57ba3dea54d14b91ba730910e2)\r\nHad to resolve minor merge conflict (f6bda409206dc642d7a6f02842e76b0be7234491)", "Hi @Androbin sorry for the delay, I was thinking of bringing this PR in independently and was waiting for the sync to resolve merge conflicts. Let me give it another shot.", "@Androbin I managed to create a pending internal change with your PR, and unfortunately found another internal dependency on all_files. Will let you know once we figure out how we can remove it. Thanks.", "@Androbin quick update: the internal dependency issue has been fixed. I'm working on submitting this change internally following [this process](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#contributing-code).", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "https://github.com/tensorflow/tensorflow/commit/bb582f1b6fad474bc446c78a6683247a8eb6048e has been submitted. I'm going to close this PR. Thanks @Androbin!"]}, {"number": 16490, "title": "Tflite windows", "body": "Hi,\r\n\r\nI've ported tensorflow lite to compile with msvc for my own development purposes. It was relatively easy to get it to compile with Visual Studio 2017. The main differences/issues were:\r\n\r\nI chose to use cmake since I'm not so familiar with bazel. I'm hoping the CMakeLists.txt file can be used for other purposes than just compiling for msvc.\r\nneeded latest version of gemmlowp which is not dependent on POSIX functionality\r\ncompiler errors due to narrowing conversions double -> float due to lack of \"f\" suffix on float numbers in unit tests\r\nConvolution generic optimized takes prohibitively long to compile with msvc\r\nadded an operating systems abstraction layer on top of some of the OS functions which are used such as mmap files and loading of dynamic libraries\r\nAll unit tests pass except for the ones with have the \"EXPECT_DEATH\" macro. The testdata filepaths in model_test.cc also need to be made cross platform.\r\n\r\nthe cmake command i used to compile with visual studio 2017 on my windows machine:\r\n`\r\ncmake -G\"Visual Studio 15 2017 Win64\" -DMSVC_RUNTIME=static -DGTEST_LIB_DIR=\"C:/SDKS/googletest/lib\" -DGMOCK_LIB_DIR=\"C:/SDKS/googletest/lib\" ..\\tensorflow\\contrib\\lite`\r\n", "comments": ["It's very good to build msvc.\r\nHow can I built tf lite with visual studio on windows 32bit?", "Thanks for this change, it was clearly a lot of work and it would be great to get TF Lite on Windows!\r\n\r\nI've asked @mrry for some advice on the float narrowing changes, since he would have had to deal with this for mainline TensorFlow, and without good tests it's likely that doubles will creep back into the code.", "We now support in the latest nightly bazel windows builds for tflite and toco. Hope this helps.", "@aselle , what's the command of building tflite on windows? I tried `bazel build -c opt tensorflow/contrib/lite/toco:toco`, but got below errors.\r\n```\r\nERROR: D:/src/tensorflow/tensorflow/tensorflow/contrib/lite/kernels/BUILD:57:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:eigen_support' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users/administrator/_bazel_administrator/7sou473m/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Administrator/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Administrator/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TMP=C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gemmlowp /Ibazel-out/x64_windows-opt/genfiles/external/gemmlowp /Ibazel-out/x64_windows-opt/bin/external/gemmlowp /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG -w -DFARMHASH_NO_CXX_STRING /DTF_COMPILE_LIBRARY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK /DEIGEN_HAS_C99_MATH /DEIGEN_AVOID_STL_ARRAY /Fobazel-out/x64_windows-opt/bin/tensorflow/contrib/lite/kernels/_objs/eigen_support/eigen_support.obj /c tensorflow/contrib/lite/kernels/eigen_support.cc\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(195): error C2059: syntax error: '('\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(196): error C2065: 'N': undeclared identifier\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(196): error C2975: 'n': invalid template argument for 'EigenForTFLite::array', expected compile-time constant expression\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(21): note: see declaration of 'n'\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(196): error C2143: syntax error: missing ';' before '{'\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(196): error C2447: '{': missing function header (old-style formal list?)\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(199): error C2059: syntax error: '('\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(200): error C2065: 'N': undeclared identifier\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(200): error C2975: 'n': invalid template argument for 'EigenForTFLite::array', expected compile-time constant expression\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(21): note: see declaration of 'n'\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(200): error C2143: syntax error: missing ';' before '{'\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\EmulateArray.h(200): error C2447: '{': missing function header (old-style formal list?)\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\CXX11Workarounds.h(50): error C2059: syntax error: '('\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\CXX11Workarounds.h(50): error C2143: syntax error: missing ';' before '{'\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\CXX11Workarounds.h(50): error C2447: '{': missing function header (old-style formal list?)\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\CXX11Workarounds.h(51): error C2059: syntax error: '('\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\CXX11Workarounds.h(51): error C2143: syntax error: missing ';' before '{'\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\CXX11Workarounds.h(51): error C2447: '{': missing function header (old-style formal list?)\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\CXX11Workarounds.h(52): error C2059: syntax error: '('\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\CXX11Workarounds.h(52): error C2143: syntax error: missing ';' before '{'\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\CXX11Workarounds.h(52): error C2447: '{': missing function header (old-style formal list?)\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/util/CXX11Meta.h(297): error C2246: 'EigenForTFLite::internal::sum_op::Identity': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/util/CXX11Meta.h(301): error C2246: 'EigenForTFLite::internal::product_op::Identity': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(190): error C2246: 'EigenForTFLite::EventCount::kStackBits': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(191): error C2246: 'EigenForTFLite::EventCount::kStackMask': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(192): error C2246: 'EigenForTFLite::EventCount::kWaiterBits': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(193): error C2246: 'EigenForTFLite::EventCount::kWaiterShift': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(195): error C2246: 'EigenForTFLite::EventCount::kWaiterMask': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(196): error C2246: 'EigenForTFLite::EventCount::kWaiterInc': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(197): error C2246: 'EigenForTFLite::EventCount::kEpochBits': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(198): error C2246: 'EigenForTFLite::EventCount::kEpochShift': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(199): error C2246: 'EigenForTFLite::EventCount::kEpochMask': illegal static data member in locally defined class\r\nc:\\users\\administrator\\_bazel_administrator\\7sou473m\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/EventCount.h(200): error C2246: 'EigenForTFLite::EventCount::kEpochInc': illegal static data member in locally defined class\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build\r\nINFO: Elapsed time: 53.367s, Critical Path: 10.87s\r\nINFO: 116 processes: 116 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tworuler, that problem should be fixed in head\r\n", "Closing since bazel now supports windows fully. Thank you for the pr.", "I'm looking at the cmake windows build again.  A couple of changes were necessary in the download_dependencies.sh to get that tflite-windows branch to work. I update GEMMLOWP_URL to the current one used in tensorflow/workspace.bzl.  It was necessary to get rid of the pthread references.  I also backed up the FLATBUFFERS_URL to 1.8.0, since tflite apparently is not compatible with the latest flatbuffer.\r\n\r\nGEMMLOWP_URL=\"https://mirror.bazel.build/github.com/google/gemmlowp/archive/38ebac7b059e84692f53e5938f97a9943c120d98.zip\"\r\n\r\nFLATBUFFERS_URL=\"https://github.com/google/flatbuffers/archive/v1.8.0.zip\"\r\n\r\nAlso, just as a note, you have to run this download_dependencies.sh from the tensorflow root.  I don't think I saw that mentioned anywhere.\r\n\r\nAlso  you need to build googletest somewhere with options compatible.  With these changes, the tflite-windows branch, from below link,  passed 40 of 41 tests executing in msvc environment.  The skip_gram_test fails ... haven't looked at that yet.\r\n\r\nhttps://github.com/shaurya0/tensorflow/tree/tflite-windows\r\n\r\n\r\n\r\n", "> We now support in the latest nightly bazel windows builds for tflite and toco. Hope this helps.\r\n\r\nIs this true? So if I download the nightly for Windows x64, it contains the binaries to run TFLite models on Windows?"]}, {"number": 16489, "title": "Fix document typo", "body": "Fix TFLite custom op typo", "comments": []}, {"number": 16488, "title": "Travis trusty Ubuntu 14.04.5: module compiled against API version 0xc but this version of numpy is 0xb", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Travis trusty, Ubuntu 14.04.5\r\n- **TensorFlow installed from (source or binary)**: binary, pip\r\n- **TensorFlow version (use command below)**: latest pip (I guess 1.5.0)\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: GCC 4.8.4\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: ...\r\n\r\n### Describe the problem\r\n\r\n```\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nRuntimeError: module compiled against API version 0xc but this version of numpy is 0xb\r\nImportError: numpy.core.multiarray failed to import\r\nImportError: numpy.core.umath failed to import\r\nImportError: numpy.core.umath failed to import\r\n2018-01-26 23:12:12.304782: F tensorflow/python/lib/core/bfloat16.cc:664] Check failed: PyBfloat16_Type.tp_base != nullptr \r\n/home/travis/.travis/job_stages: line 57:  2555 Aborted                 (core dumped) python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Source code / logs\r\n\r\nSee the [Travis log](https://travis-ci.org/rwth-i6/returnn/jobs/333916514).\r\n\r\n", "comments": ["@albertz I have met the same problem with you , have you solved it ?", "I had the same problem using TensorFlow 1.5.0 on Pycharm with Anaconda. I solved it by updating numpy on both Pycharm and Anaconda to 1.14.0 (I thought they share the same numpy package but somehow no). [This post](https://stackoverflow.com/questions/48054531/runtimeerror-module-compiled-against-api-version-0xc-but-this-version-of-numpy) indicates that wrong pandas version may also be a trigger. ", "@yifeif or @jart does that ring a bell?", "@yifeif @av8ramit @gunan Does the 14.04 CPU release hack build inside a virtualenv? If so, try `pip install -I numpy==1.12.1` and maybe do same for other deps in [setup.py](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/tools/pip_package/setup.py).", "Looks like the binary was not built in a virtualenv. @av8ramit @gunan can confirm.\r\n@albertz could you try upgrading your numpy to 1.14.1?", "Yes it was not. Updating to 1.13+ works for me. ", "[New Travis log](https://travis-ci.org/rwth-i6/returnn/jobs/335558010), with Python 3.6:\r\n\r\n```\r\n$ pip install tensorflow\r\nCollecting tensorflow\r\n  Downloading tensorflow-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (44.4MB)\r\n...\r\nRequirement already satisfied: numpy>=1.12.1 in \r\n...\r\n$ python -c \"import numpy; print(numpy.version.full_version)\"\r\n1.13.3\r\n```\r\n\r\nSo TensorFlow has set Numpy >= 1.12.1 as a dependency, which is the problem I guess, because it needs Numpy 1.14 here? I still have to try with a newer Numpy.\r\n", "With the upgrade to Numpy 1.14, it works, as can be seen [here](https://travis-ci.org/rwth-i6/returnn/jobs/335658910). But still, this is a bug in the TensorFlow pip package, which has set the wrong Numpy version dependency.", "Looks like 1.6+ has the numpy version bumped to 1.13.3. Closing this. Please reopen if this is still a problem in the newer versions.", "This is still happening to me on tf_nightly, with numpy 1.13.3 installed using Python 3.6. Upgrading numpy to 1.16.0 resolves the issue for me (that's just what I got from `pip install -U`).", "I can confirm what @SiegeLordEx is experiencing. I am using tf-nightly and had Numpy 1.16.1 installed. Downgrading to 1.16.0 fixed it.", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=16488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=16488\">No</a>\n"]}, {"number": 16487, "title": "Tflite windows", "body": "Hi,\r\n\r\nI've ported tensorflow lite to compile with msvc for my own development purposes. It was relatively easy to get it to compile with Visual Studio 2017. The main differences/issues were:\r\n\r\n- I chose to use cmake since I'm not so familiar with bazel. I'm hoping the CMakeLists.txt file can be used for other purposes than just compiling for msvc.\r\n- needed latest version of gemmlowp which is not dependent on POSIX functionality\r\n- compiler errors due to narrowing conversions double -> float due to lack of \"f\" suffix on float numbers in unit tests\r\n- Convolution generic optimized takes prohibitively long to compile with msvc\r\n- added an operating systems abstraction layer on top of some of the OS functions which are used such as mmap files and loading of dynamic libraries\r\n\r\nAll unit tests pass except for the ones with have the \"EXPECT_DEATH\" macro. The testdata filepaths in model_test.cc also need to be made cross platform.", "comments": []}, {"number": 16486, "title": "Change RELEASE.md to specify CUDA 9.0", "body": "PR for https://github.com/tensorflow/tensorflow/issues/16348 (tinyest PR ever?)", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 16485, "title": "resolve undefined name array_ops", "body": "flake8 testing of https://github.com/tensorflow/tensorflow on Python 2.7.14\r\n\r\n$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__\r\n```\r\n./tensorflow/contrib/framework/python/ops/accumulate_n_v2.py:94:12: F821 undefined name 'array_ops'\r\n    return array_ops.identity(inputs[0], name=name)\r\n           ^\r\n```", "comments": ["@rmlarsen Could I please get reviewers assigned to https://github.com/tensorflow/tensorflow/pulls/cclauss  They should not take long to evaluate."]}, {"number": 16484, "title": "use gather_nd to _gather_states in LSTMBlockWapper", "body": "no need for calculate mod_indices and reshape data .", "comments": ["@drpngx I have clean my commit ."]}, {"number": 16483, "title": "Test case for session_partial_run_test.py is getting failed ", "body": "I tried running file `session_partial_run_test.py`, It throws an error with following two failed exception.\r\n\r\n```\r\n======================================================================\r\nFAIL: testRunAndPartialRunDist (__main__.PartialRunTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"g:/tensorflow/tensorflow/python/client/session_partial_run_test.py\", line 258, in testRunAndPartialRunDist\r\n    self.RunTestRunAndPartialRun(session.Session(server.target))\r\n  File \"g:/tensorflow/tensorflow/python/client/session_partial_run_test.py\", line 123, in RunTestRunAndPartialRun\r\n    self.assertEqual(r1, r2)\r\nAssertionError: Lists differ: [4.0, 12.0] != [array([], dtype=float32), 12.0]\r\n\r\n- [4.0, 12.0]\r\n+ [array([], dtype=float32), 12.0]\r\n\r\n======================================================================\r\nFAIL: testRunAndPartialRunDist (__main__.PartialRunWithCApiTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"g:/tensorflow/tensorflow/python/client/session_partial_run_test.py\", line 258, in testRunAndPartialRunDist\r\n    self.RunTestRunAndPartialRun(session.Session(server.target))\r\n  File \"g:/tensorflow/tensorflow/python/client/session_partial_run_test.py\", line 123, in RunTestRunAndPartialRun\r\n    self.assertEqual(r1, r2)\r\nAssertionError: Lists differ: [4.0, 12.0] != [array([], dtype=float32), 12.0]\r\n\r\n- [4.0, 12.0]\r\n+ [array([], dtype=float32), 12.0]\r\n\r\n----------------------------------------------------------------------\r\nRan 50 tests in 2.933s\r\n\r\nFAILED (failures=2)\r\n```\r\n\r\nHowever, I tried debugging the code and found some scenarios in below function:\r\n\r\n```\r\ndef RunTestRunAndPartialRun(self, sess):\r\n    a = constant_op.constant(2.0, dtypes.float32)\r\n    b = a * 2\r\n    c = b * 3\r\n    r1 = sess.run([b, c])\r\n    h = sess.partial_run_setup([b, c], [])\r\n    r2 = sess.partial_run(h, [b, c])\r\n    self.assertEqual(r1, r2)\r\n```\r\n\r\nIn 1st scenario, when `testRunAndPartialRunDirect()` is executed, the test gets succeed with h value:\r\n\r\n`h = '->mul:0,mul_1:0//1/;0'`\r\n\r\nIn 2nd scenario, when `testRunAndPartialRunDist()` is executed, the test gets failed with h value:\r\n\r\n`h = '0'`\r\n\r\nand below following list are different since `assertEqual` is throwing an exception:\r\n\r\n```\r\nLists differ: [4.0, 12.0] != [array([], dtype=float32), 12.0]\r\n\r\n- [4.0, 12.0]\r\n+ [array([], dtype=float32), 12.0] \r\n```\r\n\r\nLooks like due to `zero` handle value, it started throwing an exception. Just need your suggestion if you are facing the same issue. Can I fix it by myself (I'd be happy to contribute)?\r\n\r\nGuidance will be appreciated.\r\n\r\n", "comments": ["/CC @suharshs", "Is this throwing an exception just because of GPU? I tried it using CPU. Just a thought.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@rajendraarora16 Sorry for the delay, but I am unable to reproduce this :(\r\n\r\nThis may be a GPU issue, but without a repro case I am unable to debug. Did you have any luck digging into this?\r\n\r\nWhat machine, TF version, and operating system are you using?", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@rajendraarora16 \r\n\r\nClosing due to inactivity. Please reopen if you can provide a repro case or make any progress. thanks!"]}, {"number": 16482, "title": "Specify CUDA 9.0 version", "body": "The newest CUDA 9.1 is not supported by TF1.5. Sepecify CUDA 9.0 in order to prevent confusion", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Looks like a similar change was merged already. Closing this PR."]}, {"number": 16481, "title": "Container localhost does not exist.", "body": "Hi,\r\n\r\nI upgraded from 1.5.0-rc1 to the current master branch and I started receiving the following error:\r\n\r\n```\r\n2018-01-27 02:48:38.928667: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at lookup_table_op.cc:656 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)\r\n2018-01-27 02:48:38.928786: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at iterator_ops.cc:855 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)\r\n\t [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]\r\nException in thread \"main\" org.platanios.tensorflow.jni.NotFoundException: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)\r\n\t [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]\r\n\t [[Node: Model/Model/Iterator/Next = IteratorGetNext[output_shapes=[[?,?], [?], [?,?], [?,?], [?]], output_types=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Model/Model/Iterator)]]\r\n```\r\n\r\nIt's hard to reproduce this error but a summary of the context is that I have a lookup table op inside a dataset map operator and I get this error when I try to execute the corresponding iterator \"GetNext\" op. I'm looking for information in how to parse and debug this error. I never explicitly set any containers for my variables or lookup tables (i.e., leave them to the default value; an empty string). Were there any changes introduced recently that could result in this error? Note that this happens with my Scala API but not with the Python API and so it may be that I haven't updated something in my code. I just don't really know where to look for this.\r\n\r\nThanks!", "comments": ["I think this could be a knock-on effect from 9f4118d00fa9eb85f81a4eb3f96a5583ae5afcdc, which modifies (and in principle simplifies) how DT_RESOURCE tensors are captured inside a `Dataset` transformation (such as `Dataset.map()`).\r\n\r\nIt sounds like there is some disagreement between the code that creates the lookup table and the op that performs the lookup itself. Could you share the code for creating the table and the `Dataset.map()`?\r\n\r\n/cc @rohan100jain ", "@mrry I'd love to share the code but it's written in Scala using my API and it might be hard to get familiar with the codebase. I can do so if you think that can help. However, in order to give you a summary, the lookup table is created right before being used in the dataset map. It is also initialized prior to being used by calling `GetNext`. What kind of disagreement could cause such an error? Is there an easy way to debug? Also, is `localhost` the default container used when not providing and container?", "Hmm, what you're doing doesn't sound like it's wrong, because that's effectively what the Python version would be doing too. And the recent change should make it *less* likely to see a `NotFound` error, because it changes the function implementation to share the same `ResourceMgr` (which defines the namespace for resource names and containers), whereas before there was an explicit copying-and-rewriting step for captured resource handles.\r\n\r\nCould you possibly create a minimal example that exhibits the problem in both Python and Scala, and share the code for these? If you could also capture the text-format `GraphDef`, it might be possible to inspect the graphs and find the discrepancy.\r\n\r\n> Also, is `localhost` the default container used when not providing and container?\r\n\r\nThat's right. Are you using the `container` attr in your code? (It should still work, but it's possible there's a bug there....)", "@mrry I can look into creating a minimal example this week, but I have found some information that might be useful. It has to do with creating functions and capturing control dependencies of these functions. Currently, functions are created in a separate graphs and tensors that are used as inputs are captured and replaced with placeholders to be fed later. What about control dependencies that some of the new ops might have on ops defined outside the function? For example, I have a dataset map op that uses a lookup table, but I want the iterator initializer op for that dataset to have a control dependency on the lookup table initializer op of the outer graph. How should I go about handling that? Currently I get an error of the form `Source ... not in the main body`, where `source` refers to the source of the control edge. I think that's what's causing the error because I cannot properly initialize the iterator.", "@mrry Actually never mind, even if I work around this initialization issue and manage to run all the initializer ops fine, I still get the same error once I get to invoke `GetNext`. I'll try to create a minimal example for this, but in the meantime, do you have any idea of what could be causing the container to not be found? Shouldn't the `localhost` container always exist given that it's the default one?", "Note also that if I put a breakpoint in my Scala code, this error: \r\n\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at lookup_table_op.cc:656 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)\r\n```\r\n\r\nkeeps being printed over and over again indefinitely as if it keeps being thrown from within some loop running in another thread.", "@mrry Given that the lookup table initializer op and the iterator initializer op run without any issues, is there any chance the resource manager is cleared/reset afterwards in between the session runs? Is there any tips you might have on how to debug this sort of error?", "@mrry I'm wondering if this has to do with me using the C API `TF_GraphToFunction` method. I'm using it to create the function provided to the dataset map. The lookup table which is used by the function is replaced with a placeholder of type `RESOURCE`. The initialized lookup table resource tensor is then passed as input to the created function op. Could this be causing the problem? It was all working fine with version 1.5.0-rc1.", "@mrry I finally resolved this. I was accidentally setting the shared name of the iterator I was creating. I'm not sure why this caused the problem, but without setting the shared name, everything works fine. Do you know why that could have happened? I'm actually curious.", "Thanks for tracking that down! I think this is a legitimate bug, introduced in https://github.com/tensorflow/tensorflow/commit/9f4118d00fa9eb85f81a4eb3f96a5583ae5afcdc. That change modifies most iterators to use the same `Device`, `FunctionLibraryRuntime`, and `ResourceMgr` as the op that created them, which enables the resource-capturing logic to be simplified, because handles are valid in both the caller and the callee.\r\n\r\nHowever, when the `shared_name` is set, the iterator might outlive the `FunctionLibraryRuntime` used by the op that created it. The new code identifies this case and creates its own private `FunctionLibrary`, `Device`, and `ResourceMgr` that will outlive the caller. Unfortunately, this means that the handles from the caller and no longer valid in the callee, which manifests as a `NotFoundError`. \r\n\r\nThanks for the promising lead on this bug! We'll try to get a fix in soon.", "@mrry Thanks for tracking this down and fixing it! :)", "Thanks for digging into it and making it much easier to fix!", "Hi @mrry @eaplatanios can you please take a look if the issue #50801 is related to this in any way?\r\nThanks in advance :)"]}, {"number": 16480, "title": "CMake adopt to latest eager runtime and reusable shared library in windows environment", "body": "Due to some error in CLA checking https://github.com/tensorflow/tensorflow/pull/16394, a new pull request is made\r\n\r\nI am currently working on the cmake files to generate shared library that allows users to develop with C++ interface.\r\n\r\nIt is easier to develop by adding export targets in tf_shared_lib.cmake, but I think in the future this function should be migrated to the main CMakeList.txt", "comments": ["Accompany with this pull request is the following CMakeLists for C++ api hello world.\r\n\r\nmain.cxx can be found from https://www.tensorflow.org/api_guides/cc/guide\r\n\r\ntested on windows 10 MSVC 2015 with CPU only, GPU version will be tested soon\r\n\r\n```cmake\r\ncmake_minimum_required (VERSION 2.6)\r\nproject (tf_hello)\r\n\r\n# Tensorflow\r\nfind_package(Tensorflow REQUIRED)\r\ninclude_directories(${TENSORFLOW_INCLUDE_DIRS})\r\n\r\n# compiler setting required by tensorflow, to be test on all compilers\r\n# currently only tested on MSVC and GCC\r\nif (${CMAKE_CXX_COMPILER_ID} STREQUAL MSVC) \r\n\tadd_definitions(-DCOMPILER_MSVC)\r\nelseif (${CMAKE_CXX_COMPILER_ID} STREQUAL GNU)\r\n\tif (${CMAKE_CXX_COMPILER_VERSION} VERSION_LESS \"3\")\r\n\t\tadd_definitions(-DCOMPILER_GCC3)\r\n\telse()\r\n\t\tadd_definitions(-D__GNUC__)\r\n\tendif()\r\nelse()\r\n\tmessage(ERROR \" compiler ${CMAKE_CXX_COMPILER_ID} not supported by this CMakeList.txt, under development\")\r\nendif()\r\n\r\nadd_executable(tf_hello main.cxx)\r\ntarget_link_libraries(tf_hello ${TENSORFLOW_LIBRARIES})\r\n```", "After updating protobuf to 3.5, it request NASM, perl and go for grpc compilation, how can I add these compilers to the check build environment?", "@jackyko1991 can you fix this conflict?", "@apattnaik0721013 conflict solved", "@mrry Code modified according to review", "Would it be possible to have a better control over the exported symbols ? Currently we have a symbol conflict with OpenCV (because of libpng and libjpeg, see https://github.com/tensorflow/tensorflow/issues/14267).\r\n\r\nCould we also work towards having GRPC optional on Linux ? The main issue would be with [tensorflow/core/debug/debug_io_utils.h](../blob/master/tensorflow/core/debug/debug_io_utils.cc)", "It would be great to compile with all symbols hidden except those that need to be public. @allenlavoie, @gunan", "@Skydes @martinwicke We already run a linker script for the C++ and C APIs on other platforms (we should do it here too). The issue with https://github.com/tensorflow/tensorflow/issues/14267 is that the C++ API doesn't include protocol buffer symbols, and to get those you need to link against something that needs to export the conflicting symbols (I've [outlined a solution](https://github.com/tensorflow/tensorflow/issues/9525#issuecomment-385475293) if you're interested, although it's probably not relevant to this PR since custom ops aren't supported).", "@jackyko1991 \r\n\r\nIt might break existing eigen dependent apps?. Shouldn't it be in include/tensorflow/xxxx/ as these are tensorflow specific?\r\n\r\n```\r\n# Eigen directory\r\ninstall(DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/eigen/src/eigen/Eigen/\r\n        DESTINATION include/Eigen)\r\n# external directory\r\ninstall(DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/external/eigen_archive/\r\n        DESTINATION include/external/eigen_archive)\r\n# third_party eigen directory\r\ninstall(DIRECTORY ${tensorflow_source_dir}/third_party/eigen3/\r\n        DESTINATION include/third_party/eigen3)\r\n# unsupported Eigen directory\r\ninstall(DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/eigen/src/eigen/unsupported/Eigen/\r\n        DESTINATION include/unsupported/Eigen)\r\n```", "It would be great to get this feature finished an merged. Having Windows CPU/GPU support could allow building the Go and Java APIs for Windows and even with GPU support. Please continue this work and hopefully the tensorflowlers can prioritize this. ", "With dropped cmake support, and bazel officially working, what is the latest on this PR?", "It contains windows API release with reusable cmake packages.\r\n\r\nI am not sure if bazel build on windows get GPU support if I want to code with MSVC.", "bazel build on windows does have GPU support, if that is what you are asking.\r\nI think about this, we may be able to accept this PR, but we wont be running any continuous tests to see if the cmake build is healthy or not.\r\nmoreover, the cmakefiles will be removed in a few months, as contrib itself will be removed from the tensorflow repository.", "@gunan I still see there are updates on cmake build, though latest release already changed to bazel build on windows.\r\n\r\nTF cc core is packed as a single .dll with cmake config. With this users can link TF with their own applications.\r\n\r\nThe C++ api solution is only provided in linux (https://www.tensorflow.org/guide/extend/cc) and also hard for users to get all the necessary dependencies. I believe cmake contrib should not be removed as this PR would give a better C++ api experience in windows.", "Cmake build is deprecated, and the cmakefiles are going to be removed together with the removal of  rest of contrib.\r\n\r\nlibtensorflow can be built on windows if you like to use the C++ API of TF. To pack things into a single DLL, you can simply use --config=monolithic with bazel build. So our bazel build is much more capable than the current cmake build. So I still do not understand what cmake build offers that bazel does not in terms of your requirements.", "So... should I close this PR?", "Btw will cmake TF build change to community supported? Most of cross platform C++ projects are on CMake, e.g. OpenCV, VTK, ITK. These are famous image processing toolkits and allow me to develop IO interfaces in specific domain application.\r\n\r\nI am actually writing offline inference tools for medical image analysis, which greatly depends on cmake toolchains. But there is no way combing bazel output with cmake projects (e.g. header and dependency\uff09. It is still worth to keep cmake.", "You can reach out to `build@tensorflow.org` but the decision on cmake is unfortunately final.\r\nThere are volunteers in the mailing list that would like to keep cmake build working, but there is no official support for cmake going forward.\r\n\r\nAs for this pr, I am OK with accepting this, but I would like to see the changes in cmake folder and all the other changes (setup.py, C++ headers, etc) split to a different PR, to have better description on those. Because those changes have the potential to affect all of TF.", "Nagging Assignee @mrry: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16479, "title": "Does 1.5.0 not suppurt CUDA 9.1? It worked with CUDA 9.0 but not 9.1", "body": "I installed 1.5.0, and tying to import tensorflow, but it said that 'cannot find cudart64_90.dll'. \r\nThen I installed the CUDA 9.0, and then everything works fine. \r\nSo I want to make sure than does 1.5.0 not support CUDA 9.1 or I have something installed wrong? \r\n", "comments": ["TF 1.5.0 supports CUDA 9.1.\r\nTF 1.5.0 (official) prebuilt binaries only supports CUDA 9.0.", "@ppwwyyxx Is installation from PyPI prebuilt binaries? I installed TF from PyPI and it shows the 'cannot find cudart64_90.dll' error. How to install TF 1.5.0 that is not prebuilt binaries? Thanks!", "Same problem, hope meaningful answers", "@zhiruiwang Yes, PyPI contains prebuilt binaries, meaning as of now there's only the version with CUDA 9.0 support.\r\n\r\nYou can build tensorflow by yourself: https://www.tensorflow.org/install/install_sources.\r\n\r\nHowever, if you're on Windows, it's not officially supported and it's not straightforward. For tensorflow 1.4.0, I found these [binaries with CUDA 9.1](https://github.com/fo40225/tensorflow-windows-wheel) but it hasn't been updated for 1.5.0 yet.", "Easier just to go to https://developer.nvidia.com/cuda-90-download-archive and install CUDA 9.0 instead of current default install of CUDA 9.1", "From the Dockerfile that TensorFlow builds against:\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.5.0/tensorflow/tools/ci_build/Dockerfile.gpu\r\n\r\nand by tracing to the related docker images, it looks like Tensorflow 1.5.0 is built with:\r\n```\r\nCUDA_VERSION 9.0.176\r\nCUDNN_VERSION 7.0.5.15\r\n```\r\n\r\nI think it makes sense to update the docs for CUDA/CUDNN install version.\r\n\r\nCreated a PR #16495 for version update.", "@richardmark Following instructions to install Cuda 9.0 on Ubuntu (apt-get) doesn't work, it will still install 9.1.", "It works for me, but you have to specify the version so that it won't pick 9.1 automatically. Steps:\r\n1. `apt-cache show cuda | grep Version`\r\n2. Write down the full 9.0.? version\r\n3. `apt-get install cuda=FULL-VERSION-CODE`\r\n\r\nAlso, make sure to run `apt-get update` before any of this, after you download the installer for 9.0.", "@rubenlg i followed the instructions on NVIDIA website:\r\n\r\n```\r\nwget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\r\ndpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\r\napt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\r\napt-get update\r\napt-get install -y cuda\r\n```\r\n\r\nThe above doesn't work. I will try your suggestion, thanks.\r\n\r\n(Update: Specifying version works `apt-get install cuda=9.0.176-1`)", "i'm using _cuda 9.1_ and _cudnn 7.1.13_ with _tensorflow 1.5.0_ building from the source ,my laptop configuration i5 4200h 8go ram and gtx950m , and it work for me,\r\nto build use Bazel and gcc 4.8 g++ 4.8 "]}, {"number": 16478, "title": "Failed install on Windows", "body": "Python 3.6.4\r\n\r\nThere is a strange error when I install tensorflow 1.5.\r\n\r\n```\r\nCollecting tensorflow\r\n  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/34/96/11f048eca7b4d6da3084ca49c636b9e720e9dd1483c0c4e9ba3cf5037564/tensorflow-1.5.0-cp36-cp36m-win_amd64.whl\r\nRequirement already up-to-date: wheel>=0.26 in d:\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: numpy>=1.12.1 in d:\\python\\python36\\lib\\site-packages (from tensorflow)\r\nCollecting absl-py>=0.1.6 (from tensorflow)\r\n  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/42/3c/1985d86a44bfe44fd060c02807336f840a509bfaa2d340860fba7d22da39/absl-py-0.1.9.tar.gz\r\nRequirement already up-to-date: protobuf>=3.4.0 in d:\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: six>=1.10.0 in d:\\python\\python36\\lib\\site-packages (from tensorflow)\r\nCollecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow)\r\n  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/43/69/82e2a368076c94edbba3cd15804103bf1f31486d69e11551b71fa1d1f384/tensorflow_tensorboard-1.5.0-py3-none-any.whl\r\nRequirement already up-to-date: setuptools in d:\\python\\python36\\lib\\site-packages (from protobuf>=3.4.0->tensorflow)\r\nRequirement already up-to-date: bleach==1.5.0 in d:\\python\\python36\\lib\\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\nRequirement already up-to-date: markdown>=2.6.8 in d:\\python\\python36\\lib\\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\nRequirement already up-to-date: werkzeug>=0.11.10 in d:\\python\\python36\\lib\\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\nRequirement already up-to-date: html5lib==0.9999999 in d:\\python\\python36\\lib\\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\nCollecting futures>=3.1.1 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\r\n  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/1f/9e/7b2ff7e965fc654592269f2906ade1c7d705f1bf25b7d469fa153f7d19eb/futures-3.2.0.tar.gz\r\nUnknown requires Python '>=2.6, <3' but the running Python is 3.6.4\r\n```\r\n\r\nWhy the dependency is *futures*? It doesn't have a verion of Python 3.6.4.", "comments": ["It is hard to understand your problem without complete information.  What installation method do you use? ", "I have a same problem on the Ubuntu 16.04. I use pip to get the Python 3.5 binary file. And it is strange that Python 3 doesn't need the futures package.", "@fpsandnoob Could you please specify what method you use to install TensorFlow?", "@tatianashp I used the native pip to install tensorflow-gpu with the version of Python 3.5. When the pip install the dependency, the problem occurred. Like this one:\r\n`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.5.0-cp35-cp35m-linux_x86_64.whl`", "Same problem here on Ubuntu 16.04 with Python 3.5.\r\nClearly, Python 3 doesn't need futures.  And with Python 2.7, using : \r\n`pip2 install tensorflow-gpu==1.5.0`\r\nit goes well.\r\n Here's a solution for Python 3:\r\nThe error is occurred when installing the version of futures 3.2.0 for Python 3. It is noticed that:\r\n> It should not be installed on Python 3, although there should be no harm in doing so, as the standard library takes precedence over third party libraries.\r\n\r\nFirst, install futures 3.1.1:\r\n\r\n` pip3 install futures==3.1.1`\r\nthen : \r\n`pip3 install tensorflow-gpu==1.5.0`\r\nDo not add the `--ignore-installed` or `-I`", "Thanks for previous solution.", "I think this issue should be reopened.  @WongChen has posted a workaround, but this is still a bug in tensorflow's dependency setup, is it not?", "agree with pokey, this is a bug", "agree with @pokey, I think it's a bug as well.", "i'd argue this is more a problem with futures, not with TF since TF's requirement with futures is 3.1.1+ which works fine on Python3.6", "I think the problem here is that futures > 3.1.1 is not allowed to be installed for py3.\r\nsee [here](https://pypi.python.org/pypi/futures).\r\nSince pip install the newest version, the error is occurred.\r\nIf you google it, you can find many errors just like this.\r\nOn the other side, I think tf for py3 package's dependency **should not include futures**.", "ah, my bad. i assumed that it was ok to install futures 3.1.1 for python 3 but seems like they just forgot to check the version.", "@chihuahua Could you please take a look at tensorboard dependency on futures?", "I think the problem is caused by tensorboard dependency, it requires futures >= 3.1.1.\r\n> Collecting futures>=3.1.1 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)  \r\n    futures requires Python '>=2.6, <3' but the running Python is 3.6.4", "@yaox12 You are correct. It is tensorboard issue. See tensorflow/tensorboard/#916. tensorflow/tensorboard/#920 PR fixes the problem.", "pip3 install futures==3.1.1\r\npip3 install tensorflow==1.14.0  not work", "Please open new issue, this is for Tf 1.5 which is no longer supported.\r\n\r\nLocking conversation since this issue has been solved in 2018."]}, {"number": 16477, "title": "Windows Installation tutorial has wrong cuda version requirement, 9.0 required for latest version.", "body": "I just ran the installation validation and it's telling me I need 9.0, the tutorial says we must use 8.0. I don't have cheap access to Internet, now I have to find 1GB+ plus of data without paying $15 to use my phone's data. Please update the page to recommend 9.0.\r\n\r\nThank you.", "comments": ["A pull request has been submitted. Pending for review and merge", "I meet the same issue. and I've reported it to Tensorflow team.\r\n\r\nAs @mtngle commented in #16513 , there are three solutions:\r\n\r\n> The prebuilt tensorflow binaries support CUDA 9.0 - https://github.com/tensorflow/tensorflow/releases/tag/v1.5.0\r\n> \r\n> You have 3 options:\r\n> \r\n> Compile from source and link against CUDA 8.0.\r\n> Upgrade your CUDA to 9.0\r\n> Continue to user TF 1.4.\r\n\r\nwaiting for response.", "Marking as contributions since there's a pull request. (@jackyko1991 can you link the pull request?)", "I see a couple pull requests to update the release notes file from \"CUDA 9\" to \"CUDA 9.0\", but the Windows gpu installation instructions on the TF website (https://www.tensorflow.org/install/install_windows) still show 8.0 and 6.0.", "Hi @reedwm  ,\r\nPR #16486 added the Cuda requirement to README.md.\r\n\r\nand Hi @tylerlekang ,\r\nI've reported it to Tensorflow team via our Google Developer Group on 28st and waiting for their updating.\r\n", "I have added c++ interface support with updated readme in PR https://github.com/tensorflow/tensorflow/pull/16480\r\n\r\nprotobuf has been updated to 3.5.0 in this version, however it will require NASM and golang to get it build pass.", "@jackyko1991 that PR does not update the documentation [here](https://github.com/tensorflow/tensorflow/blob/835a466c487c3a62852223eac6475ff4a4cc9836/tensorflow/docs_src/install/install_windows.md) and [here](https://github.com/tensorflow/tensorflow/blob/835a466c487c3a62852223eac6475ff4a4cc9836/tensorflow/docs_src/install/install_linux.md), which I believe is what this issue is about.\r\n\r\n@gunan, can you update the documentation?", "As mentioned in the other thread (which is now closed):\r\n\r\nIt might be nice to have CUDA/cuDNN version support information in one location, and have everything else point to that webpage. Then you don't have to remember to update the version numbers in multiple places (and potentially forget a few).\r\n\r\nIn that case, it might also be nice to mention a few more specifics of the version numbers that a particular tf-gpu release was tested with.\r\n\r\nBecause there are a lot of different versions of CUDA and cuDNN, beyond simply saying 8.0 or 9.0. CUDA has 8.0GA1, 8.0GA2, and 9.0, the latter two of which also have an option for a separately installed update. cuDNN has 6.0 for CUDA 8.0, but also 7.0.4 and 7.0.5 for CUDA 9.0\r\n\r\nFurthermore, it could be mentioned what version of the nVidia driver was used ... but only if that might actually make a difference (not sure).", "Only thing we can point to is the cuda and cudnn versions we built against.\r\nFrom user perspective, 8.0GA1 and 8.0GA2 should be the same, there should be only bugfixes between the two. So we will keep just announcing 8.0, or 9.0 (or 9.) version we build against.\r\nSimilarly, when we build against cuDNN 7.0.5, you should still be able to run it with cuDNN 7.0.4.\r\n\r\nThe drivers you will need to use for each CUDA version can be different, but only nvidia can help you with that. So you should reach out to them for which driver version should be used with which cuda version.\r\n\r\nWe usually include CUDA and cuDNN versions, but this time we forgot to update it.\r\nIt is currently updated in the source, but we need to update the website @av8ramit @MarkDaoust FYI the website still says CUDA8\r\nWe also include/announce CUDA updates in our release notes, but those are not followed as close as the website.\r\n\r\nOne thing I would like to do is improve our error reporting. When someone tries to import tensorflow rather than saying \".....so not found\" I want to have an error message that says \"You need cuda XX to run this version tensorflow, however you have XX installed\". I am open to contributions for this if anyone would like to take this on.", "https://github.com/tensorflow/tensorflow/pull/16702\r\nAssigning to @MarkDaoust to update the website.", "r1.5, r1.6 and master all include that change now and the website has been updated.", "@MarkDaoust the windows install webpage now shows CUDA 9.0, but shows cuDNN 6.0, and requires minimum compute capability of 3.0\r\n\r\nThe ubuntu install webpage, on the other hand, shows CUDA 9.0, cuDNN 7.0, and requires minimum compute capability of 3.5.\r\n\r\nJust want to confirm these are correct. If so it means we can't use cuDNN 7.0(.x) on windows??", "Windows also requires cuDNN 7,  looks like we missed that.", "Does Ubuntu actually require a GPU with higher computer capability than for Windows? It would wipe out a large swath of previous gen GPU cards if 3.5 is the true minimum."]}, {"number": 16476, "title": "Tflite SSD Postprocessing", "body": "Adding custom operators to implement SSD postprocessing in mobilenet-ssd.", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 109 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 124 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 139 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@mohamedadaly , After adding SSD Postprocessing ops,  how to convert mobilenet-ssd .pb model to tflite model ?\r\n\r\nlike this ?\r\nbazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=${MODEL_PATH}/frozen_inference_graph_stripped.pb \\\r\n  --output_file=${MODEL_PATH}/ssd_mobilenet_v1_float.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shapes=1,300,300,3 \\\r\n  --input_arrays=Preprocessor/sub \\\r\n  --output_arrays=**detection_boxes,detection_scores,detection_classes,num_detections** \\\r\n  --dump_graphviz=${MODEL_PATH}\r\nbut convert failed,  I don't know how you converted the model to use these SSD Postprocessing ops.\r\n\r\nThanks\r\n", "You first need to strip out a bunch of the unsupported operations from the .pb file. You could use something like this [script](https://gist.github.com/mohamedadaly/2e6f0f5daff41919de1d37d8858f63a0).\r\n\r\nThen use toco, with something like this:\r\n```bash\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n    --input_file=/home/maly/tensorflow-lite/ssd_mobilenet_coco_2017-11-17_lite.pb \\\r\n    --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n    --output_file=/home/maly/tensorflow-lite/ssd_mobilenet_coco_2017-11-17_lite.lite \\\r\n    --inference_type=FLOAT   --input_data_type=FLOAT --input_array=image_tensor   \\\r\n    --output_arrays=\"Result/NumDetections,Result/Boxes,Result/Scores,Result/Classes\" \\\r\n    --input_shape=1,300,300,3   --allow_custom_ops\r\n\r\n```", "@mohamedadaly , Thanks,\r\nHow to get \"ssd-mobilenet-anchors.pb\"  in  script? \r\n", "You can find it [here](https://www.dropbox.com/s/vanawje6zyocznt/ssd-mobilenet-anchors.pb)", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "You can find it here\n<https://www.dropbox.com/s/vanawje6zyocznt/ssd-mobilenet-anchors.pb?dl=0>\n\nOn Mon, Jul 9, 2018 at 1:40 AM Wenguo.Li <notifications@github.com> wrote:\n\n> @mohamedadaly <https://github.com/mohamedadaly> , Thanks,\n> How to get \"ssd-mobilenet-anchors.pb\" in script?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16476#issuecomment-403403522>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADUcf3GRPsGq6T_uJGbq_xnx3Pk_UHcmks5uExcDgaJpZM4RvDXG>\n> .\n>\n", "@petewarden -- what is the state of this PR? Is it intended to be reviewed and merged?", "Closing this PR as we've now pulled in code to do this elsewhere."]}]