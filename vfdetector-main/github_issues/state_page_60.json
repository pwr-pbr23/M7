[{"number": 37615, "title": "TF saved model Assertion Error ( Called a function referencing variables which have been deleted )", "body": "I am facing an issue . When returning a ```tf.saved_model.load``` object inside a function and then try to use it, it is not working. \r\n\r\nI am having a file ```sample.py``` \r\n```\r\n#### sample.py\r\n\r\nimport tensorflow as tf\r\ndef load_model(model_dir):\r\n\r\n    # Load Model\r\n    loaded = tf.saved_model.load(model_dir)\r\n    model = loaded.signatures['serving_default']\r\n    print(\"Model Loaded\")\r\n    return model\r\n\r\n```\r\n\r\nWhen I am executing ```main.py```\r\n\r\n```\r\nfrom sample import load_model\r\n\r\nmodel_dir = 'som_path of a saved model'\r\nmodel1 = load_model(model_dir)\r\n```\r\n\r\nIf I print model.variables I am getting following error\r\n\r\n```\r\nAssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.\r\n\r\n```\r\n\r\nBut. If load the model with same code inside the function, but not using the function it works fine\r\n\r\n```\r\n#### main.py\r\nloaded = tf.saved_model.load(model_dir)\r\nmodel = loaded.signatures['serving_default']\r\n```\r\nIf I print model.variables, its working as expected. ", "comments": ["@s4sarath \r\nCould you please let us know the tensor flow version for us to replicate the issue faced by you.\r\n\r\nPlease check [this link](https://stackoverflow.com/questions/57168947/variable-assignment-in-tensorflow-v2) on similar issue.", "I am using Tensorflow version 2.1", "@s4sarath \r\nwe are unable to replicate the issue as save model is not provided [example .pb or .h5 file to load it]\r\nplease share all dependencies for us to replicate the issue, if possible share a colab gist for us to analyse.", "Hi @Saduf2019 - \r\n\r\nHere is the link to reproduce . \r\n```\r\nhttps://github.com/s4sarath/Deep-Learning-Projects/blob/master/reproduce_error.ipynb\r\n```\r\nPlease have a look . :-) ", "@s4sarath \r\nAs the variable is accessed after the model is deleted the local variables cannot be accessed once model is deleted.\r\n\r\nFor the purpose of saving model and accessing it later please have a look at these [link1](https://www.tensorflow.org/guide/keras/save_and_serialize) and [link2](https://www.tensorflow.org/guide/checkpoint) for reference.", "@Saduf2019  - model is deleted just for clearing the variable name space. \r\nI have removed all cells where ```del model``` was present. Still you can reproduce it.\r\n\r\nhttps://colab.research.google.com/gist/s4sarath/c00719e2408cb25e729cd5eb5dbe0f71/reproduce_error.ipynb", "i am able to replicate the issue, please find [gist](https://colab.sandbox.google.com/gist/Saduf2019/3f5e1921442649d6d030e0a468edbba5/untitled100.ipynb) here", "I think this is related to this issue [here](https://stackoverflow.com/questions/57168947/variable-assignment-in-tensorflow-v2). Please take a look at it and let me know if it helps. ", "@gowthamkpr - I have seen that. But, this is not related to that, even though error is same. Its basically, some issue in loading the model to the current name space when we are using the load function from another script. ", "Any update on this ?", "Its so sad that no one has given much attention to this, its such a serious bug. ", "Has anyone managed to fix this?", "Tensorflow is getting disappointed day by day. It's been so many months.\n\nOn Fri, Aug 7, 2020, 9:49 PM Leoni Mota Loris <notifications@github.com>\nwrote:\n\n> Has anyone managed to fix this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/37615#issuecomment-670596761>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KDNU3L45RWIKIAOP73R7QSQZANCNFSM4LLETJ6Q>\n> .\n>\n", "Hi,\r\n\r\nfor me this was fixed when I wrote\r\n\r\n`loaded = tf.saved_model.load(model_dir)`\r\n`model = loaded.signatures[\"serving_default\"]`\r\n\r\nwhen loading the model in my main instead of wrapping the loading inside the load_model() function. Alternatively you can also go with `tf.compat.v1.disable_eager_execution()`.\r\nHowever, I don't know why TF would delete the variables when you return the model via your load_model() function. Can somebody elaborate? Is this a bug?", "I am able to replicate this issue on tf-nightly ( 2.4.0-dev20200812), please find the [gist here](https://colab.research.google.com/gist/Saduf2019/51bfd15bc3e365eea993b0214e44789d/untitled367.ipynb)", "Facing this as well - it looks like if the original trackable object is released by the Python garbage collector once it goes out of scope, and the signature returned by the function does not maintain a back-reference to the original loaded object.\r\n\r\nA quick workaround to avoid this, at the possible expense of creating a circular reference and/or leaking memory:\r\n```python\r\ndef load_model_safely(path_to_saved_model):\r\n    saved_model = tf.saved_model.load(path_to_saved_model)\r\n    model = saved_model.signatures[\"serving_default\"]\r\n    model._backref_to_saved_model = saved_model\r\n    return model\r\n```", "Do not worry . try pip install tf-tranasformers. Faster and complete\nserialisation support. GitHub is on the way.\n\nOn Wed, Jan 27, 2021, 1:53 AM Peter Sobot <notifications@github.com> wrote:\n\n> Facing this as well - it looks like if the original trackable object is\n> released by the Python garbage collector once it goes out of scope, and the\n> signature returned by the function does not maintain a back-reference to\n> the original loaded object.\n>\n> A quick workaround to avoid this, at the possible expense of creating a\n> circular reference and/or leaking memory:\n>\n> def load_model_safely(path_to_saved_model):\n>     saved_model = tf.saved_model.load(path_to_saved_model)\n>     model = saved_model.signatures[\"serving_default\"]\n>     model._backref_to_saved_model = saved_model\n>     return model\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/37615#issuecomment-767804930>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KDTWDWNUYJGF44LG2DS34QE7ANCNFSM4LLETJ6Q>\n> .\n>\n", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/42d78a05cebf9044c480caabd56c79e3/untitled7.ipynb). Thanks!", "I would suggest a bypass solution since it seems it would take long to fix this.\r\n\r\nDo not return its signature but loaded model itself in load_model function. Then, get signature where an inference is actually executed, like `main.py` for @s4sarath case.\r\n\r\n```\r\ndef load_model(path):\r\n    saved_model = tf.saved_model.load(path_to_saved_model)\r\n    return saved_model\r\n```", "Absolutely right. \ud83d\udc4d\n\nOn Tue, 22 Jun, 2021, 8:42 pm youngchan.kim, ***@***.***>\nwrote:\n\n> I would suggest a bypass solution since it seems it would take long to fix\n> this.\n>\n> Do not return its signature but loaded model itself in load_model\n> function. Then, get signature where the inference is actually executed like\n> 'main'\n>\n> def load_model(path): saved_model =\n> tf.saved_model.load(path_to_saved_model) return saved_model\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/37615#issuecomment-866069887>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KGCW6XLLDXCWNGXWF3TUCR4RANCNFSM4LLETJ6Q>\n> .\n>\n", "I am having the same problem when using MLFlow ([mlflow.tensorflow.load_model](https://www.mlflow.org/docs/latest/_modules/mlflow/tensorflow.html#load_model)) to load a previously saved model. The same code works if I run it in a notebook, but when I load the model with `mlflow.tensorflow.load_model` I get the same error as mentioned here.\r\n\r\nSince the code is defined in a 3rd party library I cannot edit the function and return the full loaded model.", "Was able to reproduce your issue in Tf 2.7.0, please find the gist [here](https://colab.research.google.com/gist/kumariko/69a6ba15b9a91f0c0b167282bdea341c/untitled7.ipynb#scrollTo=HlK5EPmqKUYn). Thanks!"]}, {"number": 37580, "title": "tf.train.FloatList() on a tensor takes too long", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: any\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 2.1.x, 1.5.x\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**: any\r\n- **GCC/Compiler version (if compiling from source)**: any\r\n- **CUDA/cuDNN version**: any\r\n- **GPU model and memory**: any\r\n- **Exact command to reproduce**: `tf.train.FloatList(value=tf.zeros(int(1e6)))`\r\n\r\n### Describe the problem\r\n\r\nCompare the execution time of those two lines:\r\n```\r\nfeature = tf.train.FloatList(value=tf.zeros(int(1e6)).numpy())\r\n```\r\n```\r\nfeature = tf.train.FloatList(value=tf.zeros(int(1e6)))\r\n```\r\n\r\nThe first one runs as it should, the second one takes ages to complete. (I guess something tries iterating over the tensor in the second case)\r\n\r\nN.B. this is not specific to `FloatList` - its rather numpy vs Tensor value, evaluation something (I don't quite grasp the internals).\r\n\r\n", "comments": ["I think numpy() `vectorizing`  it so it runs fast as compare of `tf.train.FloatList(value=tf.zeros(int(1e6)))`", "I notice the difference in execution time between input being a numpy array and a tensor. ", "I still see significant different with `numpy` array and `tensor` as input in `Tensorflow 2.6`, please find the [Gist](https://colab.research.google.com/gist/sachinprasadhs/f1c6c1f411cd6fe8d10abaefafff30ce/37580.ipynb) for reference."]}, {"number": 37577, "title": "log(diag_part(x)) vs diag_part(log(x)) difference with Cholesky gradients", "body": "\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Mac OS Mojave 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: -\r\n- TensorFlow installed from (source or\r\nbinary): binary (by pip)\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.6\r\n- Bazel\r\nversion (if compiling from source): -\r\n- GCC/Compiler version (if compiling from\r\nsource): -\r\n- CUDA/cuDNN version: - (running on CPU)\r\n- GPU model and memory: - (running on CPU)\r\n\r\n**Describe the current behavior**\r\nGradients of a variable, whose loss depends on the logarithm of the diagonal of its Cholesky decomposition (`tf.linalg.cholesky()`), can fail depending on the ordering of the `tf.math.log()` and `tf.linalg.diag_part()` operations. `tf.linalg.diag_part(tf.math.log(L))` results in NaN gradients, `tf.math.log(tf.linalg.diag_part(L))` runs as expected (where `L` is the Cholesky decomposition). \r\n\r\n**Describe the expected behavior**\r\n`tf.math.log(tf.linalg.diag_part(L))` and `tf.linalg.diag_part(tf.math.log(L))` should both give rise to the same (correct) gradients. the order of these ops shouldn't matter\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nLR = 0.01\r\nX = tf.random.normal([100, 10], 0., 1.)\r\nLOG2PI = 1.8378770664093453\r\nWITH_BUG = True\r\nN_ITERS = 100\r\n\r\n\r\ndef gaussian_pdf(samples, mean, covariance):\r\n    n = tf.cast(tf.shape(samples)[1], tf.float32)\r\n    L = tf.linalg.cholesky(covariance)\r\n    alpha = tf.linalg.cholesky_solve(L, tf.transpose(samples - mean))\r\n    data_fit = -0.5 * tf.reduce_sum(tf.transpose(alpha) * (samples - mean), -1)\r\n    regulariser_bug = -0.5 * tf.reduce_sum(tf.linalg.diag_part(tf.math.log(L)), axis=-1)\r\n    regulariser_fine = -0.5 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)), axis=-1)\r\n    regulariser = regulariser_bug if WITH_BUG else regulariser_fine\r\n    normaliser = -n * 0.5 * LOG2PI\r\n    return data_fit + regulariser + normaliser\r\n\r\n\r\ndef loss(x, mu, sigma):\r\n    ll = gaussian_pdf(x, mu, sigma)\r\n    return -tf.reduce_mean(ll)\r\n\r\n\r\ncov_np = np.random.normal(size=(10, 10), scale=0.001) + np.eye(10) * 2.\r\ncov_var = tf.Variable(cov_np, dtype=tf.float32)\r\nmean_var = tf.Variable([3.] * 10, dtype=tf.float32)\r\n\r\n\r\ndef train(x):\r\n    with tf.GradientTape() as t:\r\n        nll = loss(x, mean_var, cov_var)\r\n    dm, dsig = t.gradient(nll, [mean_var, cov_var])\r\n    dm_avg, dcov_avg = tf.reduce_mean(dm).numpy(), tf.reduce_mean(dsig).numpy()\r\n    print(f'Avg grads: mu: {dm_avg}, cov: {dcov_avg}')\r\n    mean_var.assign_sub(LR * dm)\r\n    cov_var.assign_sub(LR * dsig)\r\n\r\n\r\nfor _ in range(N_ITERS):\r\n    print(f'Negative log-likelihood: {loss(X, mean_var, cov_var).numpy()}')\r\n    train(X)\r\n```", "comments": ["Was able to reproduce the issue with [TF2.1](https://colab.research.google.com/gist/amahendrakar/a5a82eabde0cfb98d3c0ff122dbb915c/37577.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/1c85bd9b1be3006a1dff270d9f1f4254/37577-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Thanks @sethnabarro for reporting this. This seems to be an artifact of how the gradient of `tf.diag_part` is [implemented](https://github.com/tensorflow/tensorflow/blob/6bc4800a402f54703e0a6dd0a444769d61910980/tensorflow/python/ops/array_grad.py#L387). We fill the non-diagonal values with zeros and if those entries in `L` are zeros as well you would get a `0/0`. I don't see an easy way to fix this for now (unless we add some fused APIs which does not seem combinatorially feasible) but I will discuss possible solutions with other folks. ", "Hi @saxenasaurabh, thanks for your response. Ok, I understand the issue. I will also have a think about possible solutions.", "Was able to reproduce the issue with TF2.7.0 . Please find the gist [here](https://colab.research.google.com/gist/kumariko/a09c2dc30d8dbdeeefda2a7616e53ebc/37577.ipynb#scrollTo=wUIpG5oBCpJp). Thanks!"]}, {"number": 37573, "title": "tf.data.experimental.ignore_errors unintuitive behavior", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Mac OS 10.14\r\n- TensorFlow installed from (source or\r\nbinary): pip install tensorflow\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\n\r\nWhen applying tf.data.experimental.ignore_errors() to a dataset which has been limited by take(), the error ignoring is applied before the take(). I.e., dataset.map(potential_error_source).take(N).apply(ignore_errors) always returns N elements. This is unintuitive, because according to the documentation \"produce a dataset that contains the same elements as the input, but silently drops any elements that caused an error.\"\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected behavior would be that dataset.map(potential_error_source).take(N).apply(ignore_errors) takes N elements, and then skips those that caused the error, i.e., resulting in <N elements in case there were errors. If that is not intended or cannot be implemented, at least the documentation should be updated to reflect that ignore_errors takes effect already at the point where the errors are raised.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\ndef generate():\r\n         for i in range(1000):\r\n             yield i\r\n\r\ndef map_fn(x):\r\n    # raise assertion error for uneven numbers\r\n    with tf.control_dependencies([tf.debugging.assert_equal(x % 2, 0)]):\r\n         return x\r\n\r\ndataset = tf.data.Dataset.from_generator(generate,output_types=tf.dtypes.int32).take(100)\r\nlen(list(iter(dataset)))\r\nlen(list(iter(dataset)))\r\n# returns 100\r\ndataset = tf.data.Dataset.from_generator(generate,output_types=tf.dtypes.int32).map(map_fn).take(100).apply(tf.data.experimental.ignore_errors())\r\nlen(list(iter(dataset)))\r\n# returns 100\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.1.0 ,2.2.0-dev20200316 and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/364ee68cbbcfd90658d7d06514c5d64b/untitled730.ipynb) Thanks!", "This is working as intended, errors do not [count towards cardinality](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/take_dataset_op.cc#L117-L136).", "If this is intended behavior, could this be clarified in the documentation? Thanks!", "@Andreas5739738 \r\nAs many updates have been made in latest version can you please check in the latest tf version and let us know.", "Running the gist shared above (https://colab.sandbox.google.com/gist/ravikyram/364ee68cbbcfd90658d7d06514c5d64b/untitled730.ipynb) with TF 2.7 shows that the behavior of ignore_errors has not changed. The documentation (https://www.tensorflow.org/api_docs/python/tf/data/experimental/ignore_errors) has also not been updated to explain the observed behavior."]}, {"number": 37567, "title": "Cumulative decaying sum", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes (with some guidance)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nAdd a new function `cumsum_decay` which computes the cumulative sum along an axis while multiplying the previous element with a scalar decay factor `p`:\r\n\r\n    c_i = x_i + p*c_{i-1}    for i >= 1\r\n    c_i = x_i                for i == 0\r\n\r\nExample interface:\r\n\r\n    def cumsum_decay(x, p, axis=0, exclusive=False, reverse=False, name=None):\r\n\r\nwhere\r\n\r\n* `x, axis, exclusive, reverse, name` are similar to [`tf.math.cumsum`](https://www.tensorflow.org/api_docs/python/tf/math/cumsum),\r\n* `p` is a scalar tensor.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will add one function `tf.math.cumsum_decay`.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nDoing this sort of computation comes up repeatedly, for example see these StackExchange questions:\r\n\r\n* [Increasing each element of a tensor by the predecessor in Tensorflow 2.0](https://stackoverflow.com/q/60590333/3767239)\r\n* [Can the cumsum function in NumPy decay while adding?](https://stackoverflow.com/q/28915088/3767239)\r\n* [Parallel or efficient computation of value with momentum](https://cs.stackexchange.com/q/73746/23162)\r\n\r\n**Any Other info.**\r\n\r\nSince I'm not familiar with the tensorflow core, I cannot judge whether this feature is easy to implement based on the implementation of `cumsum`.", "comments": []}, {"number": 37564, "title": "Dose TensorFlow has tf.sparse.segment_max? how to code it by customer?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["@wangshang19911011 \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\nplease share simple stand alone code for us to replicate the issue.\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@wangshang19911011 \r\ni looked up for \"tf.sparse.segment_max\" in tensorflow website and do not find anything related to this, there is no such module in case you are looking to use it. [tf.math.segment_max ](https://www.tensorflow.org/api_docs/python/tf/math/segment_max)is present.", "@wangshang19911011\r\nplease update on the above comment", "@Saduf2019 Thanks for reply. I'm working with sparse input, tf.math.segment_max is not so good with sparse input. I tried code  tf.math.segment_max by myself, but it is too hard. Hope tensorflow will has this module."]}, {"number": 37563, "title": "large model parallelism on gpu causing oom", "body": "We are using v100 with gpu memory of 32 gb to train wide and deep model of size 12gb, facing the problem of gpu out of memory. \r\n\r\nSo we are thinking of the model parallelism to distribute the large model on different gpu, however, all of the current available strategies for model parallelism could not meet our demand.\r\n\r\n1. MirroredStrategy \r\n2. MultiWorkerMirroredStrategy\r\n3. CentralStorageStrategy\r\n4. ParameterServerStrategy\r\n5. TPUStrategy\r\n\r\nFor MirroredStrategy  and MultiWorkerMirroredStrategy, each variable in the model is mirrored across all the replicas, which means that gpu memory of each card is still in shortage. CentralStorageStrategy and ParameterServerStrategy have great communication cost among cpus and gpus. So my question is for our large ctr model is there any solutions to realize model parallelism", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nPlease provide colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster.Please, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!", "ubuntu 18.04 and tf 1.14 @ravikyram ", "It's noticed that the wide-and-deep model involves the usage of the DNNLinearCombinedClassifier, so the detail of the model is invisible for the user to specify which variable should be placed on the specified gpu,  i.e. with tf.device('/gpu:0'). @ymodak ", "May be you can consider [limiting gpu memory growth ](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) in this case?\r\nAlso see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/ConfigProto\r\nFor TF 1.X  you can try,\r\n```python\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n```", "@ymodak I've tried limiting memory growth as mentioned above, without any help. Because the model involves adagrad op, the internal variables of which require three times the model size, which is 3*12gb = 36gb > 32gb of the gpu memory. So it's useless to set `config.gpu_options.allow_growth = True`. Any other suggestions?", "What I desired is the parallel model training that could split large model on one machine with eight gpus. Each part of the model is trained on the single gpu, and then the results are combined, just the philosophy of map-reduce, for the reason that each gpu memory is not large enough to hold the entire model, not the MirroredStrategy that simply makes the replica of model on different gpus which still would not solve the gpu oom problem. ", "This is not supported natively in TF yet. We're certainly interested in it. Could you describe your use case more if possible? Are you using a deep and wide model?\r\n\r\nYou can look into Mesh Tensorflow.", "@crccw @ymodak The model is exactly the wide-and-deep model. Mesh Tensorflow seems to be one of the solutions that implement model parallelism, however, mesh tensorflow involves so much revise of the original model code. Maybe some other distribution strategies like MirroredStrategy are welcomed. Or would you please provide any materials describing the detail implementation and architecture of MirroredStrategy so that user can develop the strategies by themselves?", "MirroredStrategy use a custom variable_creator[1] to replica each variable on all devices. It launches computation on all devices by using multiple threads. In each thread, it's essentially:\r\n\r\nwith tf.device(\"xxx\"):\r\n   do_the_work()\r\n\r\n(The actual code is much more complicated in order to make things work with other parts of TF).\r\n\r\nAfter getting the gradient, it AllReduce them and each replica applies the gradient by itself. \r\n\r\nDeveloping your own strategy is not trivial. Optionally you can look into ZeRO (https://arxiv.org/abs/1910.02054). One of its optimization is to distribute the optimizer states. This is possible to implement by wrapping optimizers. It's still non trivial, but you don't need to touch your model code.\r\n\r\n[1] https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope\r\n[2] https://arxiv.org/abs/1910.02054", "@crccw Thanks a lot for your helpful advice. ZeRO seems to be amazing, and [here](https://github.com/microsoft/DeepSpeed) is the micorsoft's implementation of the optimizer on Pytorch. Any plan to support it on tensorflow?", "We don't have concrete plans at this moment. Please understand that we need to prioritize this among other things that are also important. I'll keep this ticket updated.", "Hello, is there any update of this thread? We are planning to train a model that may even not fit into a100 which has 40GB GPU memory. And we are seeking for model parallelism solutions. Or something similar to Pipeline parallelism[1] in Pytorch should help.\r\n\r\n[1] https://pytorch.org/docs/stable/pipeline.html"]}, {"number": 37444, "title": "InaccessibleTensorError in graph mode with for loop itteration (different sized tensors) ( tf.image.resize)", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): custom Layer, using tf.image.resize\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Linux Mint 19.3 Cinnamon (Ubuntu based)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: No\r\n- TensorFlow installed from (source or\r\nbinary): pip install\r\n - TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.6.9\r\n\r\nProblem:\r\nNot sure if its a bug, or i misused something\r\nbut i expected autograph to do a loop unrolling once, and later work with the tensors created in the unrolled loop.\r\n\r\n**Describe the current behavior**\r\nin graph mode (tf.function) InaccessibleTensorError is thrown if tf.image.resize is used in loop\r\n\r\n**Describe the expected behavior**\r\nshould work as in eager mode\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf \r\n\r\ndef main():\r\n    execute_eager = False\r\n    test_nr = 2\r\n    tf.config.experimental_run_functions_eagerly(execute_eager)\r\n    \r\n    images = tf.constant(100.,shape=[1,10,10,20])\r\n    if execute_eager or test_nr == 1:\r\n        scale = Scaler1()\r\n        out = scale(images)\r\n        print(out)\r\n    if execute_eager or test_nr == 2:\r\n        scale = Scaler2()\r\n        out = scale(images)\r\n        print(out)\r\n    if execute_eager or test_nr == 3:\r\n        scale = Scaler2()\r\n        out = scale(images)\r\n        print(out)\r\n\r\nclass Scaler1(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, count = 5, name = \"Scaler\", **kwargs):\r\n        self.count = tf.cast(count, dtype = tf.float32)\r\n        super().__init__(name = name, **kwargs)\r\n        self.sized_images=[]\r\n         \r\n    @tf.function\r\n    def call(self, inputs):\r\n        images = inputs\r\n        \r\n        image_size = tf.cast(tf.shape(images)[1:3], dtype=tf.float32)\r\n                   \r\n        for i in range(int(self.count)):\r\n            scale = image_size * (1 + tf.cast(i, dtype=tf.float32))\r\n            sized_image = tf.image.resize(images, tf.cast(scale + 0.5, dtype = tf.int32))\r\n            self.sized_images.append(sized_image)\r\n        \r\n        return self.sized_images\r\n\r\nclass Scaler2(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, count = 5, name = \"Scaler\", **kwargs):\r\n        self.count = tf.cast(count, dtype = tf.float32)\r\n        super().__init__(name = name, **kwargs)\r\n        self.sized_images=[]\r\n         \r\n    @tf.function\r\n    def call(self, inputs):\r\n        images = inputs\r\n        \r\n        image_size = tf.cast(tf.shape(images)[1:3], dtype=tf.float32)\r\n                   \r\n        for i in tf.range(self.count):\r\n            scale = image_size * (1 + tf.cast(i, dtype=tf.float32))\r\n            sized_image = tf.image.resize(images, tf.cast(scale + 0.5, dtype = tf.int32))\r\n            self.sized_images.append(sized_image)\r\n        \r\n        return self.sized_images\r\n    \r\nclass Scaler3(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, count = 5, name = \"Scaler\", **kwargs):\r\n        self.count = tf.cast(count, dtype = tf.float32)\r\n        super().__init__(name = name, **kwargs)\r\n        self.sized_images=[]\r\n         \r\n    @tf.function\r\n    def call(self, inputs):\r\n        images = inputs\r\n        \r\n        image_size = tf.cast(tf.shape(images)[1:3], dtype=tf.float32)\r\n             \r\n        self.sized_images = [tf.image.resize(images, tf.cast(image_size * (1 + i) + 0.5, dtype = tf.int32)) for i in tf.range(self.count)]     \r\n        \r\n        return self.sized_images\r\n    \r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nTraceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/bhb/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/ptvsd_launcher.py\", line 48, in <module>\r\n    main(ptvsdArgs)\r\n  File \"/home/bhb/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/old_ptvsd/ptvsd/__main__.py\", line 432, in main\r\n    run()\r\n  File \"/home/bhb/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/old_ptvsd/ptvsd/__main__.py\", line 316, in run_file\r\n    runpy.run_path(target, run_name='__main__')\r\n  File \"/usr/lib/python3.6/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/test_scaling_tensor.py\", line 83, in <module>\r\n    main()\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/test_scaling_tensor.py\", line 72, in main\r\n    out = scale(images)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 967, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 579, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 626, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    expand_composites=True)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 617, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 617, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 945, in convert\r\n    x = deps_ctx.mark_as_return(x)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/framework/auto_control_deps.py\", line 217, in mark_as_return\r\n    tensor = array_ops.identity(tensor)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 282, in identity\r\n    ret = gen_array_ops.identity(input, name=name)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3901, in identity\r\n    \"Identity\", input=input, name=name)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 591, in _create_op_internal\r\n    inp = self.capture(inp)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 641, in capture\r\n    % (tensor, tensor.graph, self))\r\ntensorflow.python.framework.errors_impl.InaccessibleTensorError: The tensor 'Tensor(\"resize/ResizeBilinear:0\", shape=(1, None, None, 20), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=while_body_32, id=139898298208720); accessed from: FuncGraph(name=call, id=139898299444136).\r\n```", "comments": ["I found a workaround, which is a bit wired because logicically does the same as the original code, but it splits the work in to two loops.\r\nSo a lot overhead in typing code, and not very intuitive.\r\n\r\nThe workaround (Test 6) shoes that tensorflow is basically able to perform the required computation.\r\nIt has nothing to do with tf.image.resize but with autograph loop handling\r\nSo i guess the solution should be a:\r\n-  autograph bug\r\n- or autograph feature request\r\n\r\nSo i here by reformulate the bug report:\r\n\r\n* Describe the current behaviour:\r\nWorks in eager mode\r\nThrows different Errors in graph mode (tf.function)\r\nProblem: autograph has problems with converting for loops\r\n\r\nTests: 1 to 5 fail in graph mode\r\nTest: 6 works like expected :)\r\nCan be used for implementing the expected behaviour\r\n\r\nTest 5 and Test 6 are very similar (no obvious clue why 6 works but 5 does not)\r\nTest 1,2 are also very similar and should be solvable\r\n\r\nTest 3 uses Generator (which is probably hard to implement in autograph)\r\n\r\nTest 4 is a manually unrolled loop, which does not work (probably autograph is try s to be \"to smart\" and try s implementing the conditions in the graph, and fails because of different length output)\r\nthe conditions don't depend on tf.function input so a conversion only of path at construction time would be preferable \r\n\r\n* Describe the expected behaviour:\r\nshould work like in eager mode\r\nor like Test 6\r\n\r\n```\r\nimport tensorflow as tf \r\n\r\ndef main():\r\n    execute_eager = False\r\n    test_nr = 6\r\n    tf.config.experimental_run_functions_eagerly(execute_eager)\r\n    \r\n    images = tf.constant(100.,shape=[1,10,10,20])\r\n    if execute_eager or test_nr == 1:\r\n        scale = Scaler1()\r\n        out = scale(images)\r\n        print(out)\r\n    if execute_eager or test_nr == 2:\r\n        scale = Scaler2()\r\n        out = scale(images)\r\n        print(out)\r\n    if execute_eager or test_nr == 3:\r\n        scale = Scaler3()\r\n        out = scale(images)\r\n        print(out)\r\n    if execute_eager or test_nr == 4:\r\n        scale = Scaler4()\r\n        out = scale(images)\r\n        print(out)\r\n    if execute_eager or test_nr == 5:\r\n        scale = Scaler5()\r\n        out = scale(images)\r\n        print(out)\r\n    if execute_eager or test_nr == 6:\r\n        scale = Scaler6()\r\n        out = scale(images)\r\n        print(out)\r\n\r\nclass Scaler1(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, count = 4, name = \"Scaler\", **kwargs):\r\n        self.count = tf.cast(count, dtype = tf.float32)\r\n        super().__init__(name = name, **kwargs)\r\n        self.sized_images=[]\r\n         \r\n    @tf.function\r\n    def call(self, inputs):\r\n        images = inputs\r\n        \r\n        image_size = tf.cast(tf.shape(images)[1:3], dtype=tf.float32)\r\n                   \r\n        for i in range(int(self.count)):\r\n            scale = image_size * (1 + tf.cast(i, dtype=tf.float32))\r\n            sized_image = tf.image.resize(images, tf.cast(scale + 0.5, dtype = tf.int32))\r\n            self.sized_images.append(sized_image)\r\n        \r\n        return self.sized_images\r\n\r\nclass Scaler2(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, count = 5, name = \"Scaler\", **kwargs):\r\n        self.count = tf.cast(count, dtype = tf.float32)\r\n        super().__init__(name = name, **kwargs)\r\n        self.sized_images=[]\r\n         \r\n    @tf.function\r\n    def call(self, inputs):\r\n        images = inputs\r\n        \r\n        image_size = tf.cast(tf.shape(images)[1:3], dtype=tf.float32)\r\n                   \r\n        for i in tf.range(self.count):\r\n            scale = image_size * (1 + tf.cast(i, dtype=tf.float32))\r\n            sized_image = tf.image.resize(images, tf.cast(scale + 0.5, dtype = tf.int32))\r\n            self.sized_images.append(sized_image)\r\n        \r\n        return self.sized_images\r\n    \r\nclass Scaler4(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, count = 5, name = \"Scaler\", **kwargs):\r\n        self.count = tf.cast(count, dtype = tf.float32)\r\n        super().__init__(name = name, **kwargs)\r\n        self.sized_images=[]\r\n         \r\n    @tf.function\r\n    def call(self, inputs):\r\n        images = inputs\r\n        \r\n        image_size = tf.cast(tf.shape(images)[1:3], dtype=tf.float32)\r\n        \r\n        i = 1\r\n        scale = image_size * (1 + tf.cast(i, dtype=tf.float32))\r\n        sized_image1 = tf.image.resize(images, tf.cast(scale + 0.5, dtype = tf.int32))\r\n        if i == self.count:\r\n            return [sized_image1]\r\n        else:\r\n            i = 2\r\n            scale = image_size * (1 + tf.cast(i, dtype=tf.float32))\r\n            sized_image2 = tf.image.resize(images, tf.cast(scale + 0.5, dtype = tf.int32))\r\n            if i == self.count:\r\n                return [sized_image1,sized_image2]\r\n            else:\r\n                i = 3\r\n                scale = image_size * (1 + tf.cast(i, dtype=tf.float32))\r\n                sized_image3 = tf.image.resize(images, tf.cast(scale + 0.5, dtype = tf.int32))\r\n                if i == self.count:\r\n                    return [sized_image1,sized_image2,sized_image3]\r\n                else:\r\n                    i = 4\r\n                    scale = image_size * (1 + tf.cast(i, dtype=tf.float32))\r\n                    sized_image4 = tf.image.resize(images, tf.cast(scale + 0.5, dtype = tf.int32))\r\n                if i == self.count:\r\n                    return [sized_image1,sized_image2,sized_image3,sized_image4]\r\n                else:\r\n                    return [sized_image1,sized_image2,sized_image3,sized_image4]\r\n    \r\nclass Scaler3(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, count = 5, name = \"Scaler\", **kwargs):\r\n        self.count = tf.cast(count, dtype = tf.float32)\r\n        super().__init__(name = name, **kwargs)\r\n        self.sized_images=[]\r\n         \r\n    @tf.function\r\n    def call(self, inputs):\r\n        images = inputs\r\n        \r\n        image_size = tf.cast(tf.shape(images)[1:3], dtype=tf.float32)\r\n             \r\n        self.sized_images = [tf.image.resize(images, tf.cast(image_size * (1 + i) + 0.5, dtype = tf.int32)) for i in tf.range(self.count)]     \r\n        \r\n        return self.sized_images\r\n    \r\nclass Scaler5(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, count = 5, name = \"Scaler\", **kwargs):\r\n        self.count = tf.cast(count, dtype = tf.float32)\r\n        super().__init__(name = name, **kwargs)\r\n         \r\n \r\n    @tf.function\r\n    def call(self, inputs):\r\n        images = inputs\r\n                \r\n        image_size = tf.cast(tf.shape(images)[1:3], dtype=tf.float32)\r\n        \r\n        sized_images = []\r\n        for i in tf.range(self.count):\r\n            scale = image_size * (1 + i)\r\n            sized_images.append(tf.image.resize(images,tf.cast(scale + 0.5, dtype = tf.int32)))\r\n        \r\n        return sized_images\r\n    \r\nclass Scaler6(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, count = 5, name = \"Scaler\", **kwargs):\r\n        self.count = tf.cast(count, dtype = tf.float32)\r\n        super().__init__(name = name, **kwargs)\r\n\r\n    @tf.function\r\n    def call(self, inputs):\r\n        images = inputs\r\n                \r\n        image_size = tf.cast(tf.shape(images)[1:3], dtype=tf.float32)\r\n        \r\n        scales_arr = tf.TensorArray(dtype =tf.float32, size=tf.cast(self.count, dtype=tf.int32),dynamic_size=False)\r\n        for i in tf.range(self.count):\r\n            scale = image_size * (1 + i)\r\n            scales_arr = scales_arr.write(tf.cast(i, dtype=tf.int32), scale)\r\n        scales = scales_arr.stack()\r\n        \r\n        scales_list = tf.unstack(scales)\r\n        sized_images = []\r\n        for scale in scales_list:\r\n            sized_images.append(tf.image.resize(images,tf.cast(scale + 0.5, dtype = tf.int32)))\r\n\r\n        return sized_images\r\n    \r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```", "Was able to reproduce the issue, code works only when `test_nr = 6`, fails in all other cases. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/8708c578c99d40f7eab22c18b2e3874d/37444.ipynb). Thanks!", "Hello,\r\nI had the same issue, with a simpler code sample:\r\n\r\n```import tensorflow as tf\r\n\r\nclass Connect4Environment(object):\r\n  def __init__(self, batch_size:int=1):\r\n    self.board = tf.zeros((batch_size,6,7), dtype=tf.int8)\r\n\r\n  def step(self, board, action_mask):\r\n    #tf.debugging.Assert(action_mask.ndim == board.ndim) # Fails with \"'Tensor' object has no attribute 'ndim'\" in graph mode\r\n    self.board = board * action_mask\r\n  \r\n@tf.function\r\ndef play_game():\r\n\r\n  batch_size = 4\r\n  history = tf.zeros([batch_size,1], dtype=tf.int32)\r\n  o = Connect4Environment(batch_size)\r\n\r\n  for current_index in tf.range(0, 0): # seems XLA doesn't drop the loop\r\n    action_mask = tf.expand_dims(tf.one_hot(history[:,current_index], 7, dtype=tf.int8), 1)\r\n    o.step(o.board, action_mask)\r\n    #o.board = o.board * action_mask # using this way avoid the crash\r\n\r\n  current_observation = tf.stack([o.board>0, o.board<0], -1) # pass if this line is commented\r\n  return tf.constant(0) # current_observation is not returned\r\n\r\nif __name__ == \"__main__\":\r\n  #tf.config.experimental_run_functions_eagerly(True) # disable graph / stop crash\r\n  print(play_game())\r\n````\r\n\r\nThe error seems to be related to tensor update/write in `step` function, inside of the loop. Moving the write outside, or skipping the function avoid the issue.\r\n  ", "@cmarlin \r\nYour Error is signifikantly different and is not an issue with Tensorflow but a Case of wrong Implementation.\r\n\r\nTensors can never be reassined once created. For that you have to use variables.\r\n\r\nIn your case in your Loop you create a new constant every time. So your Loop cant execute in Graph mode, nur is unrolled for each Loop itteration and leads to e verry big Graph.", "Thanks for the advice, I'll try to rewrite code using Variables", "Yes, list manipulation with uneven tensors is not well supported at the moment. We're looking into fixing this. For now, the easiest way to make `Scalar1` work, which I think is the most idiomatic version, is to avoid turning `self.count` into a `Tensor`.\r\n\r\nNote that if `self.count` ever becomes a truly dynamic tensor, then you won't be able to use a Python list for `self.sized_images` - Python lists can't change length in TensorFlow.\r\n\r\nAn alternative to using lists or TensorArray is to use tf.RaggedTensor, which can hold elements of uneven size, and will work with a dynamic self.count as well.", "Thanks for your reply.\r\n\r\nI already tryed the solution with RagedTensors, but the current Implementation for them is only useable for one dimentional Data. The scaled Images have two variing Dimentsons, so a RagedTensors is not working.\r\n\r\nA RagedTensors Implementation which allowed simple multible ragged dimensions would be nice.\r\n\r\nCurrently one would have to Splitt the Image in its singel rows and colums, by doing so a folowing compuation is not posible any more.\r\n\r\nSo using a Python List ist the only posible solution.\r\n\r\nThat the list cant change size is clear, in Graph mode it ther is no list in the graph.\r\n\r\nCan you explain for clearaty what differens it makes whether count is a tensor or a Python scalar? The Loop did a explicit conversion to python int so i guest it should not be translated by autograph.\r\n\r\nIm looking Forward to the fix of list itteration.\r\n\r\nThanks for your Help.", "I see. Although sometimes you can downcast a Tensor to a Python int (see `tf.get_static_value`), that only works for simple values like constants, and won't work for outputs of more complicated ops. In our case, `int(tensor)` is just a type cast, that is `tf.cast(tensor, tf.int32)` - perhaps that was the source of the confusion, and the reason why I suggested avoiding to wrap self.count into a `tf.constant` in the first place.", "thanks, indeed it was not clear to me that a explicit python cast is also converted by autograph to tf.cast(tensor, tf.int32).\r\nIt is very confusing because python ints don't need to be int32.", "Agreed, perhaps raising an error would be less confusing instead. The choice for int32 is because int32 and float32 are the default data types in TF (used e.g. when creating tf.constant).", "@bela127, from the above comments if your issue is resolved can we go ahead and close this issue? Thanks.", "I'm not sure it is resolved?\r\n\r\nI used a \"work around\" to make it work, but it is not pretty.\r\n\r\nIn the conversation are multiple solutions mentioned (throwing an error, or a warning, better ragged tensors, better autograph translator to make all examples behave the same -> they should do the same) that would resolve this issue\r\n\r\nBut i think non of them is currently implement in tensor flow. So the issue still exists.", "Hi @bela127! This issues seems to be getting resolved in TF 2.7 .Attaching [Gist ](https://colab.research.google.com/gist/mohantym/2c30b2a6856d1a4be20eac59ce1f4049/github_37444.ipynb#scrollTo=JFwkJ5fEKDUT)for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry for my late reply,\r\n\r\nso i run all the tests in the gist, and added one new.\r\nthe new [gist](https://colab.research.google.com/gist/bela127/d4b8bb3779e113202df2ebdb1ac9f206/github_37444.ipynb#scrollTo=OvGGjSOUJ_84).\r\n\r\nThe most phythonic test 0 now works! Perfect!\r\n\r\nBut there is still a big gap between eager and tf.function (this can be expected, as autographing is a difficult job).\r\n\r\nThe bigger issue is that the Exceptions do not point to the correct problems, or give confusing error messages.\r\nE.g. compare test 0 and 1, the error points to a totally different location than the actual difference in code.\r\n\r\nMaybe there are some ideas how to make it better?\r\n\r\nI'm happy to see development goes on :+1: ", "Glad to hear! Sorry I forgot to mark the bug when the fix was submitted.\r\n\r\nFor the remaining gaps (especially wrong location information in error messages), there should be some improvements in tf-nightly, and we definitely expect the error to point to the correct line - if you come across examples when that's not true, we'd appreciate filing new issues!"]}, {"number": 37428, "title": "tf.Graph.get_tensor_by_name does not work as expected in TF2", "body": "**System information** \r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Max OSX 10.15.3 (Catalina) \r\n- Mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):  N/A\r\n- CUDA/cuDNN version: - GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n`tf.Graph.get_tensor_by_name` returns 'resource' dtype tensors, which do not evaluate as expected in a session. For example, the code below will return something like:\r\n\r\n```\r\narray([( 10,), ( 44,), ( 47,), (106,), (111,), ( 98,), ( 58,), (108,),\r\n       (111,), ( 99,), ( 97,), (108,), (104,), (111,), (115,), (116,),\r\n       ( 47,), (114,), (101,), (112,), (108,), (105,), ( 99,), ( 97,),\r\n       ( 58,), ( 48,), ( 47,), (116,), ( 97,), (115,), (107,), ( 58,),\r\n       ( 48,), ( 47,), (100,), (101,), (118,), (105,), ( 99,), (101,),\r\n       ( 58,), ( 67,), ( 80,), ( 85,), ( 58,), ( 48,), ( 18,), (  9,),\r\n       (108,), (111,), ( 99,), ( 97,), (108,), (104,), (111,), (115,),\r\n       (116,), ( 26,), (  1,), (120,), ( 32,), (224,), (167,), (192,),\r\n       (135,), ( 18,), ( 42,), ( 18,), ( 78,), ( 49,), ( 48,), (116,),\r\n       (101,), (110,), (115,), (111,), (114,), (102,), (108,), (111,),\r\n       (119,), ( 51,), ( 86,), ( 97,), (114,), ( 69,), ( 50,), (  4,),\r\n       (  8,), (  3,), ( 18,), (  0,)], dtype=[('resource', 'u1')])\r\n```\r\n\r\n**Describe the expected behavior**\r\nExpect v1 compatible code to work as it does in TF1.X. In particular, the code below should return `3`.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_eager_execution()\r\nx = tf.Variable(3, name='x')\r\ny = tf.get_default_graph().get_tensor_by_name('x:0')\r\nwith tf.Session() as sess:\r\n  sess.run(y)\r\n```\r\n\r\n**Other info / logs** \r\n- When instantiating a `tf.Variable`:\r\n ```\r\nWARNING:tensorflow:From /Users/shermes/Projects/tensorflow/venv2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n```\r\n- From `sess.run` block: \r\n```\r\n/Users/shermes/Projects/tensorflow/venv2/lib/python3.7/site-packages/tensorflow_core/python/client/session.py:1445: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n```\r\n(This error shows up when using numpy 1.16 as well as 1.17 and 1.18.)\r\n\r\n\r\n", "comments": ["@stephenhermes, I tried replicating the issue but getting different error .\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/eda9478397bd2186c904344a8f9b93e1/untitled430.ipynb?authuser=2) and provide more information to replicating the reported issue. Thanks!", "The issue reported is with TensorFlow version 2.1.0 (not 1.15.0).", "@stephenhermes, The code is compatible to Tf 1.x version. Please confirm.\r\nThanks", "The code requires `tf.global_variables_initializer()` to run in TF 1.15; including it does not fix the issue in TF2.1:\r\n\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_eager_execution()\r\nx = tf.Variable(3, name='x')\r\ny = tf.get_default_graph().get_tensor_by_name('x:0')\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n  sess.run(init)\r\n  sess.run(y)\r\n```", "I could replicate the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/ebc5400e9789fdc4a185ab4ac88eeeaf/untitled439.ipynb). Thanks!", "@stephenhermes Correct me if I am wrong but I don't see any difference in 1.x and 2.x please find my gist [here](https://colab.research.google.com/gist/gowthamkpr/c6cc5083b4021931ba8d18ae68929aef/untitled3.ipynb?authuser=2)", "@gowthamkpr the issue is between 1.x and  tf.compat.v1 in 2.x. Please find the gist [here](https://colab.research.google.com/gist/stephenhermes/d609c5fd43246d4cff5c0b3246f83dff/untitled3.ipynb?authuser=2).", "@stephenhermes I get it now. Thanks. I do see the difference between 1.x and tf.compat.v1 in 2.x.", "Is there any work around of this issue, like instead of `get_tensor_by_name `to return `resource` dtype tensors, any other ways to get the same dtype tensors as tf1?", "Is there any solution? the same issue when I run freeze_graph.py convert ckpt to pb with TF 2.1", "I would also like a solution for this as am getting the same error.", "Try\r\n```\r\ntf.compat.v1.disable_v2_behavior()\r\ntf.compat.v1.disable_eager_execution()\r\n```", "\n\n\nbut I try, and error as follow:\nfile\".../tensorflow_core/python/ops/variables.py\",line 1663, in __init__\n\n   self.distribute_strategy = distribute_strategy\n\n\nNameError:global name 'distribute_strategy' is not defined\n\n\nHow can i solve this question? thank you \n\n\n\n\n\n\n\n\n\n\n\u5728 2020-05-18 23:56:23\uff0c\"pks\" <notifications@github.com> \u5199\u9053\uff1a\n\nTry\n\ntf.compat.v1.disable_v2_behavior()\ntf.compat.v1.disable_eager_execution()\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or unsubscribe.", "You can refer to https://stackoverflow.com/questions/35678883/tensorflow-getting-variable-by-name\r\n\r\n```\r\nvar = [v for v in tf.global_variables() if v.name == \"xxx_name\"][0]\r\nsess.run(var)\r\n```\r\n", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ba9357303fec1748876ad6d45069da26/untitled7.ipynb). Thanks!", "The issue is [reproducible](https://colab.research.google.com/gist/Saduf2019/fee018f83d3d4fce82b44612972f55d7/untitled638.ipynb) in tf nightly 2.7 as well."]}, {"number": 37311, "title": "Method Chaining in Tensorflow", "body": "I have posted a question about this in stackoverflow. https://stackoverflow.com/questions/60528238/why-no-method-chaining-in-tensorflow\r\n\r\nAka. named parameter idiom. I did a simple Nearest Neighbor algorithm in TF and I ended up writing the whole NN algorithm in one line. You can argue that I should take it apart, but I think this happens often anyways (in coding in general too):\r\n\r\n```\r\ntf.math.argmin(tf.math.reduce_sum(tf.math.abs(tf.math.subtract(Xtrain_set, Xtest_row)), axis=1), output_type=tf.int32)\r\n\r\n# returns the minimum train set indices for each X predictable row based on L1 (in a for cycle) but this is not important\r\n```\r\n\r\nIt could be much more readable and simple to write:\r\n\r\n**`tf.subtract(Xtrain_set, Xtest_row).abs().reduce_sum(axis=1).argmin(output_type=tf.int32)\r\n`**\r\n\r\nI think it really follows the \"Tensor-flow\" thinking too.\r\nThanks.", "comments": ["I have a similar idea for the Keras interface: instead of\r\n```\r\nmodel = vgg16.VGG16()\r\nmodel.compile(loss=\"mse\")\r\nmodel.fit(...)\r\n```\r\n\r\nit would be nice to be able to do\r\n```\r\nvgg16.VGG16().compile(loss=\"mse\").fit(...)\r\n```\r\n\r\nAnd I think all it takes in this examples is a simple `return(self)` here:\r\nhttps://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/engine/training.py#L462-L470\r\nhttps://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/engine/training.py#L555-L557"]}, {"number": 37284, "title": "Cert error for https://download.tensorflow.org/", "body": "\r\n![image](https://user-images.githubusercontent.com/10709657/75855865-232ed600-5e2e-11ea-9391-ce3f23a81fb6.png)\r\n\r\nNot sure if this domain is still used. I sent a [PR](https://github.com/tensorflow/docs/pull/1489) for #34460 , but this domain also exists in various places.\r\nFor example: `tensorflow/lite/micro/examples/magic_wand/train/README.md`", "comments": []}, {"number": 37252, "title": "Memory leak activated when setting a seed in TensorFlow 2 (2.0 & 2.1) and using eager execution.", "body": "Firstly, I would like to thank @condnsdmatters for his help in investigating this issue and tracing the problem back to the use of a random state when setting a seed for TensorFlow.\r\n\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.1.0 (also tested on 2.0.0)\r\n- Python version: Python 3.7.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: - GPU model and memory: N/A\r\n\r\n**Describe the current behaviour**\r\nWhen a random seed is set in TensorFlow 2 (2.0 or 2.1) memory usage increases with every call to usage of any random sampling in TensorFlow. We believe that this is due to the use of a Python's `random.Random` random state as a source of randomness and a lack of garbage collection for the results of random sampling. We may see that when a seed is set TensorFlow uses `random.Random` by looking at the [`_set_global_seed`](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/eager/context.py#L433) method of an eager execution context.\r\n\r\nThe reproduction example does not use `tf.random.set_seed` since we have tracked the issue down more narrowly to the sampling of random values and we therefore concentrate on a minimal focussed example in the hope of enabling a faster fix.\r\n\r\nSee the reproduction below and [our colab notebook](https://colab.research.google.com/drive/1HhVduWlY6Va_KLX2b-SqCGIWLNDy5MFT) for concrete examples of when this occurs. See the note at the end of this post before running the code.\r\n\r\n**Describe the expected behaviour**\r\nWe would not expect memory usage to grow with the number of calls to random sampling. In the example below we would expect constant memory usage.\r\n\r\n**Standalone code to reproduce the issue** \r\nThe code below will show increasing memory usage over time.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom random import Random\r\nimport os\r\nimport psutil\r\n\r\nMAXINT = 2**32 - 1\r\n\r\ndef print_memory_usage():\r\n    print(\"memory used:\", psutil.Process(os.getpid()).memory_info().rss)\r\n    return psutil.Process(os.getpid()).memory_info().rss\r\n\r\nif __name__ == \"__main__\":\r\n    random_state = Random()\r\n    for _ in range(10):\r\n        for _ in range(1000):\r\n            a = random_state.randint(0, MAXINT)\r\n            tf.Variable(a)\r\n        print_memory_usage()\r\n```\r\n\r\nFurther examples of cases with and without memory leaks are provided in [this colab notebook](https://colab.research.google.com/drive/1HhVduWlY6Va_KLX2b-SqCGIWLNDy5MFT).\r\n\r\n**Other info / logs**\r\nIf the random integer sampled in the example is cast to a float the memory issue remains. However, if it is cast to a string before being passed to `tf.Variable` then there is no memory issue. This leads us to think that the issue is related to the memory usage as laid out in the C++ code of TensorFlow [here](https://github.com/tensorflow/tensorflow/blob/b87f4a54652d72435a088e5a510ca46e124deccd/tensorflow/c/tf_tensor.cc#L109).\r\n\r\nThis is an issue with eager execution as defining the loops in a function and decorating it with `tf.function` resolves the memory issue. This then leads us to the more targetted example included below.\r\n\r\n```python\r\nfrom random import Random\r\nfrom tensorflow.python.framework.constant_op import convert_to_eager_tensor\r\nfrom tensorflow.python.eager import context\r\nimport os\r\nimport psutil\r\n\r\nMAXINT = 2**32 - 1\r\n\r\ndef print_memory_usage():\r\n    print(\"memory used:\", psutil.Process(os.getpid()).memory_info().rss)\r\n    return psutil.Process(os.getpid()).memory_info().rss\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    random_state = Random()\r\n    ctx = context.context()\r\n    for _ in range(10):\r\n        for _ in range(1000):\r\n            a = random_state.randint(0, MAXINT)\r\n            convert_to_eager_tensor(a, ctx)\r\n\r\n        print_memory_usage()\r\n```\r\n\r\n**Note:** Running the code in [the colab notebook](https://colab.research.google.com/drive/1HhVduWlY6Va_KLX2b-SqCGIWLNDy5MFT) provided can lead to inconsistent results due to the cloud execution environment. We suggest running our examples locally when debugging. At the very least the run time should be reset and each example run from a clean starting point before conclusions are drawn.", "comments": ["I believe this is caused by caching which `convert_to_eager_tensor` performs. Notably look at https://github.com/tensorflow/tensorflow/blob/7072568ed6b735e347fb87bc84a2b83daf806e3f/tensorflow/python/framework/constant_op.py#L68-L72 which mentions that some values can be cached.\r\n\r\nIn the C++ code, https://github.com/tensorflow/tensorflow/blob/7072568ed6b735e347fb87bc84a2b83daf806e3f/tensorflow/python/eager/pywrap_tensor.cc#L307-L313 performs the caching -- currently for Python scalars only.\r\n\r\nThis is why you need to generate random numbers -- each call allocates a new Eager tensor with that specific constant. If you instead use a constant, it allocates the eager tensor only once.", "Update: I reran the reproduction code with TensorFlow 2.2 and the issue is still present.", "@IanRDavies Thanks for updating us", "I just had a quick review with the latest versions of TensorFlow. The behaviour in TensorFlow versions 2.3.2 and 2.4.1 is consistent with observations in past versions. The issue still exists."]}, {"number": 37245, "title": "Warning/Error not present when not using the result of write in a TensorArray", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nWhen not using the result of a `write` method from a `TensorArray`, I am not getting any warning or error.\r\n\r\n**Describe the expected behavior**\r\nAccording to [the docs of `write`](https://www.tensorflow.org/api_docs/python/tf/TensorArray#write), I should receive a warning or error.\r\nI think it's also important to receive one given I spent quite some time realizing it was not inplace.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\n@tf.function\r\ndef use_tensor_array(x):\r\n    y = tf.TensorArray(x.dtype, tf.shape(x)[0])\r\n    for i in tf.range(tf.shape(x)[0]):\r\n        y.write(i, x[i])\r\n    y = y.stack()\r\n    return y\r\n\r\nuse_tensor_array(tf.constant([1, 3, 4, 6]))\r\n```\r\n\r\n", "comments": ["I have replicated the mentioned issue and it persist,please find the [gist](https://colab.sandbox.google.com/gist/Saduf2019/a47bad1134ac9ec7bee368439fcdbdea/untitled71.ipynb) here", "I have tried in colab with TF versions 2.2,2.3-rc1,nightly versions(`2.4.0-dev20200712`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/4a898fa9440cb0d6faeaae72748b9761/untitled115.ipynb).Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ff2301e043c5a133e9d64aa9443f152e/35650.ipynb). Thanks!", "This issue is still reproducible on tf 2.7-[nightly](https://colab.research.google.com/gist/Saduf2019/726eb43542712e0a4320a2a3cb9ae557/untitled638.ipynb) version."]}, {"number": 37041, "title": "Feature suggestion: Ability to add copyright notice to saved model", "body": "A trained TensorFlow model can represent valuable intellectual property, and may sometimes be distributed as a saved model, rather than python code etc. Is there any way of including a copyright notice in a saved model file?\r\n\r\nIf this is not yet possible, perhaps a comments= argument could be added to model.save().\r\n", "comments": []}, {"number": 37029, "title": "Hang on out of memory error", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1.0 and nightly, used `tensorflow/tensorflow:2.1.0-gpu-py3` and `tensorflow/tensorflow:nightly-gpu-py3 `\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:  P100 and V100\r\nDriver: 440.33.01 \r\nCUDA 10.1 in container\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTensorFlow hangs when it hits out of memory after it dumps the out of memory message.\r\n\r\n**Describe the expected behavior**\r\nTensorFlow should exit on non-zero return code on OOM.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\n\r\nimport numpy as np\r\n\r\n\r\ndef random_image_generator(batch_size, num_classes, input_shape):\r\n    templates = 2 * num_classes * np.random.random((num_classes,) + input_shape)\r\n    random_data = np.random.normal(loc=0, scale=1., size=input_shape)\r\n    while True:\r\n        y = np.random.randint(0, num_classes, size=(batch_size,))\r\n        x = np.zeros((batch_size,) + input_shape, dtype=np.float32)\r\n        for i in range(batch_size):\r\n            x[i] = templates[y[i]] + random_data\r\n        x_array = np.array(x)\r\n        y_array = tf.keras.utils.to_categorical(y, num_classes)\r\n        yield(x_array, y_array)\r\n\r\ndef run_model():\r\n    K.set_image_data_format('channels_first')\r\n    image_dim = 5000\r\n    input_shape = (3, image_dim, image_dim)\r\n\r\n    num_classes = 15\r\n    batch_size = 1\r\n    model_class = tf.keras.applications.ResNet50\r\n    model = model_class(weights=None, include_top=True, input_shape=input_shape,\r\n                        classes=num_classes)\r\n\r\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\n\r\n    random_generator = random_image_generator(batch_size, num_classes,\r\n                                              input_shape)\r\n    model.fit(random_generator, steps_per_epoch=10,\r\n              epochs=1)\r\n\r\nrun_model()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nThis program hangs after dumping the out of memory error on 16GB and 32GB GPUs (P100 and V100 tested). The program use to exit on TensorFlow 1.15. This happens on both the 2.1.0 and nightly containers on Intel x86 systems.\r\n\r\nI originally hit this on built-from-source TensorFlow 2.1.0 on ppc64le. On that system, I attached gdb and dumped the stacks. It seems the code is hanging on the three thread stacks noted in the attachment.\r\n[threeThreadStacks.txt](https://github.com/tensorflow/tensorflow/files/4246783/threeThreadStacks.txt)\r\n", "comments": ["Was able to reproduce the issue on colab with Tf 2.1 and Tf-nightly. ", "This hangs inside `ParallelMapIterator::~ParallelMapIterator`.  Assigning to @jsimsa for further triaging.", "@aaudiber could you please take a look?", "From the stack traces, it looks like the program hangs while waiting for the Python generator function to complete. It isn't clear whether the Python function is hanging, or if it crashes but we fail to notify RunWithBorrowedArgs that it has finished. \r\n\r\n#6  0x00007fff462e9cd4 in tensorflow::condition_variable::wait\r\n#7  0x00007fff4114079c in tensorflow::data::InstantiatedCapturedFunction::RunWithBorrowedArgs\r\n#8  0x00007fff40d88d3c in tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::GetNextInternal\r\n\r\n@smatzek can you tell from the OOM error message where the OOM is encountered?", "@gadagashwini I haven't been able to reproduce the hang. Can you share your repro methodology?", "@aaudiber, I could replicate the issue with Tf-gpu 2.1 on colab.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/b74f86d43ac19829827b59b3a149f876/untitled410.ipynb). At the end session stopped by saying `Your session crashed after using all available RAM`. Thanks!", "@aaudiber The stack after the memory region dump mentions FusedBatchNorm.\r\n\r\n```\r\n2020-02-28 16:35:54.147979: W tensorflow/core/framework/op_kernel.cc:1732] OP_REQUIRES failed at fused_batch_norm_op.cc:1181 : Resource exhausted: OOM when allocating tensor with shape[1,256,1250,1250] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n 1/10 [==>...........................] - ETA: 3:52Traceback (most recent call last):\r\n  File \"recreate.py\", line 37, in <module>\r\n    run_model()\r\n  File \"recreate.py\", line 35, in run_model\r\n    epochs=1)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 718, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v2.py\", line 341, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 576, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 640, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2414, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1660, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1741, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[1,256,1250,1250] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node resnet50/conv2_block2_3_bn/FusedBatchNormV3 (defined at recreate.py:35) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n [Op:__inference_distributed_function_19978]\r\n```", "> @aaudiber, I could replicate the issue with Tf-gpu 2.1 on colab.\r\n> Please find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/b74f86d43ac19829827b59b3a149f876/untitled410.ipynb). At the end session stopped by saying `Your session crashed after using all available RAM`. Thanks!\r\n\r\n@gadagashwini That's the behavior I get as well. In this case crashing after using all available RAM is the expected behavior, the issue is about hanging instead of crashing.\r\n\r\n\r\nI spent some time trying to reproduce on my local GPU with both tf-nightly and tensorflow-2.1-gpu, but I've never been able to get a hang - the process always exits with an OOM error.\r\n\r\nIf the reported stack traces are to be believed, the hanging c++ thread is blocked waiting for the `random_image_generator` function to generate an element. It would be useful to know whether that python function is blocked during the hang, or whether the python function completed with an error, but failed to notify the GeneratorDataset thread. I visually inspected our logic for handling errors in calling a python function, and I don't see a way for the python function to complete without notifying the GeneratorDataset thread.\r\n\r\nTo debug further, I suggest simplifying the example as much as possible so that we can zero in on what exactly causes the hang.", "I gutted the random_image_generator and it does **NOT** hang for me.\r\n\r\nCode:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\n\r\nimport numpy as np\r\n\r\nimage_dim = 5000\r\ninput_shape = (3, image_dim, image_dim)\r\n\r\nnum_classes = 15\r\nbatch_size = 1\r\n\r\ntemplates = 2 * num_classes * np.random.random((num_classes,) + input_shape)\r\nrandom_data = np.random.normal(loc=0, scale=1., size=input_shape)\r\ny = np.random.randint(0, num_classes, size=(batch_size,))\r\nx = np.zeros((batch_size,) + input_shape, dtype=np.float32)\r\nfor i in range(batch_size):\r\n    x[i] = templates[y[i]] + random_data\r\nx_array = np.array(x)\r\ny_array = tf.keras.utils.to_categorical(y, num_classes)\r\n\r\n\r\ndef random_image_generator(batch_size, num_classes, input_shape):\r\n    while True:\r\n        yield(x_array, y_array)\r\n\r\ndef run_model():\r\n    K.set_image_data_format('channels_first')\r\n\r\n    model_class = tf.keras.applications.ResNet50\r\n    model = model_class(weights=None, include_top=True, input_shape=input_shape,\r\n                        classes=num_classes)\r\n\r\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\n\r\n    random_generator = random_image_generator(batch_size, num_classes,\r\n                                              input_shape)\r\n    model.fit(random_generator, steps_per_epoch=10,\r\n              epochs=1)\r\n\r\nrun_model()\r\n```", "One idea is to add print statements to `random_image_generator` to see whether the function hangs.", "As noted in the original issue description, this hang on OOM did not occur on TF 1.15. The image generator is only calling out to tf.keras and numpy.\r\n\r\nSo this morning I tried the following container images:\r\n```\r\ntensorflow/tensorflow:1.15.2-gpu-py3 (has numpy 1.18.1)\r\ntensorflow/tensorflow:2.0.0-gpu-py3  (has numpy 1.17.2)\r\ntensorflow/tensorflow:2.1.0-gpu-py3 (has numpy 1.18.1)\r\ntensorflow/tensorflow:nightly-gpu-py3 (has numpy 1.18.1)\r\n```\r\nThe recreate program hangs on OOM in 2.1.0 and nightly. It does NOT hang in 1.15.2 or 2.0.0. The numpy levels used are noted above. I also upgraded numpy to 1.18.1 in my 2.0.0 container and it still did not hang.\r\n\r\nSince the numpy levels remain constant and I see the change in behavior between 2.0.0 and 2.1.0 I suspect that something changed between 2.0.0 and 2.1.0 which is contributing to this behavior. Later today I'll diff tensorflow::data between the 2.0.0 and 2.1.0 tag levels and see if anything stands out to me.\r\n", "In my diff I noticed that the entire ParallelMapIterator class is new between 2.0.0 and 2.1.0. I spent over an hour investigating the code and following its waits/notifies. As far as I traced the code flows it looked OK to me.\r\n\r\nI then added prints into the generator function and saw I do have a start without a yield. However, I do not think it is dependent on the actual work this function is doing. I replaced the work in the while loop with a simple sleep and still reproduced the hang on ParallelMapIterator.\r\n\r\nI understand that the hang is \"new\" in 2.1.0 because ParallelMapIterator is doing the wait for all outstanding work on deconstruction. It may not have happened in the previous way this was called.\r\n\r\nThe random generator with only the sleep in the while/yield is:\r\n```python\r\ndef random_image_generator(batch_size, num_classes, input_shape):\r\n    templates = 2 * num_classes * np.random.random((num_classes,) + input_shape)\r\n    random_data = np.random.normal(loc=0, scale=1., size=input_shape)\r\n    y = np.random.randint(0, num_classes, size=(batch_size,))\r\n    x = np.zeros((batch_size,) + input_shape, dtype=np.float32)\r\n    for i in range(batch_size):\r\n        x[i] = templates[y[i]] + random_data\r\n    x_array = np.array(x)\r\n    y_array = tf.keras.utils.to_categorical(y, num_classes)\r\n    while True:\r\n        print('random_image_generator while start')\r\n        time.sleep(1)\r\n        print('random_image_generator while yield')\r\n        yield(x_array, y_array)\r\n```\r\n\r\nIn my recreate I see about 14 \"while start\" messages and 13 yields. So it's not the first result return either.", "I don't think the issue is with `ParallelMapIterator` - it was moved between 2.0.0 and 2.1.0, but it's always had the logic of waiting for outstanding calls to finish during deconstruction: https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/core/kernels/data/parallel_map_iterator.cc#L70-L79\r\n\r\nFrom the stack trace\r\n\r\n#6 0x00007fff462e9cd4 in tensorflow::condition_variable::wait\r\n#7 0x00007fff4114079c in tensorflow::data::InstantiatedCapturedFunction::RunWithBorrowedArgs\r\n#8 0x00007fff40d88d3c in tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::GetNextInternal\r\n\r\nit looks like we're getting stuck here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/captured_function.cc#L717-L721. We call `lib_->Run` to invoke the python function, which is supposed to call `Notify()` when the python function completes (whether or not it succeeds). For some reason it looks like that callback never happens. It isn't clear whether that's because the python function itself never completes, or because `lib_->Run` fails to call `Notify` on some error-handling code path. If I could reproduce, I would add additional logging to see what happens in `lib_->Run`", "Colab crashes while reproducing the issue with TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/b6d69794a8890503affb90f658e3da4d/untitled299.ipynb#scrollTo=juXh8fO8n5in)..Thanks!", "@smatzek With the latest `tf-nightly` I believe the issue has been resolved with no crash anymore:Could you Please find the [gist ](https://colab.research.google.com/gist/sushreebarsa/93c4ba41732627df5e750ca1865ddb99/untitled299.ipynb#scrollTo=29gASIMYoGZD)here for reference and confirm the same ?Thank you!\r\n\r\n\r\n", "@sushreebarsa I see the gist failing with `InvalidArgumentError:  Default MaxPoolingOp only supports NHWC on device type CPU` rather than an out of memory error.\r\n\r\nThe [gist from June 23](https://colab.research.google.com/gist/sushreebarsa/b6d69794a8890503affb90f658e3da4d/untitled299.ipynb#scrollTo=juXh8fO8n5in), IS failing on OOM.", "Was able to reproduce the issue in tf v2.7. Please find the gist [here](https://colab.sandbox.google.com/gist/tilakrayal/f7355e559a93dd95610dcaf4d235a741/untitled162.ipynb)"]}, {"number": 37010, "title": "TopK result always sort when k==num_cols", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): CentOS Linux release 7.5.1804\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: No\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1.0 using pip install \r\n- Python version: - Bazel\r\nversion (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from\r\nsource): NA\r\n- CUDA/cuDNN version: - GPU model and memory:NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\ntop_k op always sort when k == input size\r\n\r\n**Describe the expected behavior**\r\nNo sort when sorted=False\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.constant([1, 2, 3, 4], dtype=tf.float32)\r\nb = tf.math.top_k(a, k=4, sorted=False)\r\nc = tf.math.top_k(a, k=4, sorted=True)\r\n\r\nprint(b)\r\nprint(c)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```python\r\nTopKV2(values=<tf.Tensor: shape=(4,), dtype=float32, numpy=array([4., 3., 2., 1.], dtype=float32)>, indices=<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 2, 1, 0], dtype=int32)>)\r\nTopKV2(values=<tf.Tensor: shape=(4,), dtype=float32, numpy=array([4., 3., 2., 1.], dtype=float32)>, indices=<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 2, 1, 0], dtype=int32)>)\r\n```\r\nThe result show b and c are same with the result has been sorted.", "comments": ["I have tried on colab with TF version 2.1.0, 2.2.0-dev20200218 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/6561c3e65bf02fa7009ca2cd20956dbd/untitled667.ipynb). Thanks!", "@rmlarsen do you know who works on this?", "I have tried on colab with TF version 2.2,2.3-rc1,nightly version(`2.4.0-dev20200712`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0bcc82f2eb673aef6a7fe2645d3e4377/untitled114.ipynb). Thanks!", "able to reproduce even when k!=num_col. `sorted` is never effective. \r\n\r\n```python\r\nimport tensorflow as tf\r\na = tf.reshape(tf.range(100), (10, 10))\r\na = tf.random.normal((10, 10))\r\nprint(a)\r\nprint(tf.math.top_k(a, k=4, sorted=True))\r\nprint(tf.math.top_k(a, k=4, sorted=False))\r\n```\r\n\r\n\r\n(tf 2.4.1, Driver: 460.73.01, CUDA Version: 11.2 Quadro RTX 6000)\r\nReproduced also on google colab.\r\n\r\nBasically `sorted=False` does not work at all. We have to tf.sort the indices again. \r\n", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/65569fa4dae03a295b41e9a52f61da9e/35650.ipynb). Thanks!", "@rmlarsen this is still not working for 2.6.0-dev20210607 "]}, {"number": 37003, "title": "RuntimeError: Encountered unresolved custom op: TensorListFromTensor.Node number 892 (TensorListFromTensor) failed to prepare.", "body": "System information\r\n\r\nGoogle Colaboratory   \r\nTensorflow 2.1.0     \r\n\r\nMy tf_keras model can be converted to tf.lite successfully as follows.   \r\n`\r\nefficientdet_model= keras.models.load_model('inference_ckpts/ckpts_B1_image-size- \r\n640/mbconv_head_anchor-extend_1e-6/csv_10_0.7139_0.8459.h5',      \r\n                                            custom_objects=custom_objects,     \r\n                                            compile=False,    \r\n                        )     \r\n\r\nexport_dir = \"tf_save/ckpts_B1_image-size-640/mbconv_head_anchor-extend_1e-6/\"     \r\n\r\ntf.saved_model.save(efficientdet_model, export_dir)      \r\n\r\nmodel_tfsave = tf.saved_model.load(export_dir)     \r\n\r\nconcrete_func = model_tfsave.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n     \r\nconcrete_func.inputs[1].set_shape([None,511500,4])     \r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])     \r\n\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]     \r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\n     \r\nconverter.allow_custom_ops=True     \r\n\r\ntflite_model = converter.convert()     \r\n\r\nopen(\"inference_tflite/ckpts_B1_image-size-640/mbconv_head_anchor-extend_1e-6/csv_10_0.7139_0.8459.tflite\", \"wb\").write(tflite_model)    \r\n\r\n`\r\n\r\nHowever, I encountered a RuntimeError  in inference when running \"interpreter.allocate_tensors()\" as the following.    \r\n`\r\ninterpreter = tf.lite.Interpreter(model_path=\"inference_tflite/ckpts_B1_image-size-640/mbconv_head_anchor-extend_1e-6/csv_10_0.7139_0.8459.tflite\")     \r\n\r\ninterpreter.allocate_tensors()    \r\n`  \r\nError.    \r\n`RuntimeError                              Traceback (most recent call last)\r\n<ipython-input-23-e8f284b41149> in <module>()\r\n----> 1 interpreter.allocate_tensors()\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter.py in allocate_tensors(self)\r\n    245   def allocate_tensors(self):\r\n    246     self._ensure_safe()\r\n--> 247     return self._interpreter.AllocateTensors()\r\n    248 \r\n    249   def _safe_to_run(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n    108 \r\n    109     def AllocateTensors(self):\r\n--> 110         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    111 \r\n    112     def Invoke(self):    \r\n\r\nRuntimeError: Encountered unresolved custom op: TensorListFromTensor.Node number 892 (TensorListFromTensor) failed to prepare.`    \r\n    \r\nWhat should I do for this error?", "comments": ["@2696120622 please share simple standalone code[with all dependencies] for us to replicate the issue in our environment.\r\n\r\nAlso please check this [link](https://github.com/tensorflow/tensorflow/issues/35923) if it helps", "> @2696120622 please share simple standalone code[with all dependencies] for us to replicate the issue in our environment.\r\n> \r\n> Also please check this [link](https://github.com/tensorflow/tensorflow/issues/35923) if it helps\r\n\r\nI am working with tf 2.1. Code as follows. \r\nhttps://colab.research.google.com/drive/16Px4jCUWURdISjG5d3Fv5TVK448L15MO  \r\n Should I implement the custom op \"TensorListFromTensor\" by myself ?  \r\nThanks.  \r\n", "@2696120622 Could you please provide access to the google drive or share the code directly so i could replicate it in our environment.", "> @2696120622 Could you please provide access to the google drive or share the code directly so i could replicate it in our environment.\r\nMy code based on \r\n> https://github.com/xuannianz/EfficientDet\r\nand\r\n> https://github.com/KUASWoodyLIN/EfficientDet/tree/tf2.0.\r\nHow to provide access to my google dirve or share my code directly for you?", "@2696120622 please share your code gist through a colab.", "> @2696120622 please share your code gist through a colab.\r\nThis is my google drive link.\r\n\r\n> https://drive.google.com/open?id=1aFTO2lO57pvu8noTCQfXax3PxYt1Axgq  \r\n\r\nYou can train one model with train_test.ipynb and convert it with convert_model_test.ipynb.\r\nThanks.", "@Saduf2019 Could you access my Google drive by above link and replicate my issue? ", "@2696120622 \r\nI have tried replicating the issue, there are too many dependencies,could you please provide with simple standalone code to replicate.", "@Saduf2019 \r\nThanks a lot. My project is really based on https://github.com/fizyr/keras-retinanet and https://github.com/xuannianz/EfficientDet.\r\n\r\nMay be some installation steps should be done ahead of time.\r\n\r\n> https://github.com/fizyr/keras-retinanet#installation\r\n\r\nWhat issues do you encounter when replicating the issue?\r\n\r\nCould you try the following shared link again and download all in the link.\r\n\r\n> https://drive.google.com/open?id=15i6d9yJ-OAcPROiGEs-RWmG-v6KyQauK\r\n\r\nThen,you can train one model with train_test.ipynb and convert it with convert_model_test.ipynb.\r\nAnd, should I implement the custom op \"TensorListFromTensor\" by myself according to my issue?", "@haozha111 \r\nI have update my tf to 2.1. However, I still encountered the same error:\r\n\r\n> https://github.com/tensorflow/tensorflow/issues/37003#issue-569633708\r\n\r\n`RuntimeError: Encountered unresolved custom op: TensorListFromTensor.Node number 892 (TensorListFromTensor) failed to prepare`.\r\nAccording to your comment:\r\n> https://github.com/tensorflow/tensorflow/issues/28485#issuecomment-574453673\r\nMaybe I need the new MLIR-based TF Lite converter. How to get this new MLIR-based TF Lite converter?\r\nMy project is here.\r\n\r\n> https://drive.google.com/open?id=15i6d9yJ-OAcPROiGEs-RWmG-v6KyQauK\r\nYou can train one model with train_test.ipynb and convert it with convert_model_test.ipynb.\r\nThanks a lot!"]}, {"number": 36981, "title": "`recompute_grad` does not save memory and is incompatible with graph mode", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  No.\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Linux Ubuntu 16.04 and Windows 10.\r\n- TensorFlow installed from (source or\r\nbinary): from binary (pip install)\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: CUDA10.2+CuDNN7.6.5 (Windows), CUDA10.1+CuDNN7.6.5+TensorRT 6 (Ubuntu), \r\n- GPU model and memory: GeForce GTX 1060 with Max-Q Design, 6GB (Windows) and GeForce GTX 1080 Ti, 12GB (Ubuntu)\r\n\r\n**Describe the current behavior**\r\nUsing `tf.recompute_grad` to wrap keras layers does not take any effect. I build a DenseNet model and wrap each \"-bn-relu-conv1x1-bn-relu-conv\" block by the function. But I have not seen any GPU memory reduction on both the Windows and Ubuntu platforms. When eager mode is disabled, it throws \"ValueError: Variable <tf.Variable 'batch_normalization/gamma:0' shape=(32,) dtype=float32> has `None` for gradient.\", indicating that using `compute_grad` blocks the gradient backpropagation in graph mode.\r\n\r\n**Describe the expected behavior**\r\nThe function seems to originate from OpenAI's gradient checkpointing (https://github.com/cybertronai/gradient-checkpointing) and is expected to save GPU memory during training. Recently, a tensorflow implementation of efficient DenseNets (https://github.com/joeyearsley/efficient_densenet_tensorflow) also uses this function to perform the gradient checkpointing (they used `tf.contrib.layers.recompute_grad` in tf1 graph mode, not exactly the same environment as our case.)\r\n\r\nPlease fix the incompatibility bug so that the function can still work with the graph mode. If the function is designed to perform gradient checkpointing, please verify its effectiveness. If it is not supposed to implement efficient DenseNets, please provide the correct and effective implementation.\r\n\r\n\r\n**Standalone code to reproduce the issue** \r\n```PYTHON\r\nimport os\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom absl import app, flags\r\nfrom absl.flags import FLAGS\r\nfrom tensorflow import keras\r\n\r\nflags.DEFINE_list(\"gpu\",\r\n                  default=None,\r\n                  help=\"index of GPU\")\r\nflags.DEFINE_bool(\"recompute_grad\",\r\n                  default=False,\r\n                  help=\"whether to recompute gradients to save GPU RAM\")\r\nflags.DEFINE_integer(\"batch_size\",\r\n                     default=1024,\r\n                     help=\"batch size\")\r\nflags.DEFINE_bool(\"graph\",\r\n                  default=False,\r\n                  help=\"use graph mode instead of eager mode\")\r\n\r\n\r\ndef dense_lenet(inputs):\r\n    net = keras.layers.Conv2D(32, 5, strides=2, use_bias=False, padding=\"SAME\")(inputs)\r\n\r\n    for _ in range(5):\r\n        def _block(x):\r\n            x = keras.layers.BatchNormalization()(x)\r\n            x = keras.layers.ReLU()(x)\r\n            x = keras.layers.Conv2D(16, 1, use_bias=False, padding=\"SAME\")(x)\r\n            x = keras.layers.BatchNormalization()(x)\r\n            x = keras.layers.ReLU()(x)\r\n            x = keras.layers.Conv2D(4, 3, use_bias=False, padding=\"SAME\")(x)\r\n            return x\r\n        if FLAGS.recompute_grad:\r\n            _block = tf.recompute_grad(_block)\r\n        net = keras.layers.concatenate([net, _block(net)])\r\n\r\n    net = keras.layers.BatchNormalization()(net)\r\n    net = keras.layers.ReLU()(net)\r\n    net = keras.layers.Conv2D(64, 1, use_bias=False, padding=\"SAME\")(net)\r\n    net = keras.layers.AveragePooling2D()(net)\r\n\r\n    for _ in range(10):\r\n        def _block(x):\r\n            x = keras.layers.BatchNormalization()(x)\r\n            x = keras.layers.ReLU()(x)\r\n            x = keras.layers.Conv2D(32, 1, use_bias=False, padding=\"SAME\")(x)\r\n            x = keras.layers.BatchNormalization()(x)\r\n            x = keras.layers.ReLU()(x)\r\n            x = keras.layers.Conv2D(8, 3, use_bias=False, padding=\"SAME\")(x)\r\n            return x\r\n        if FLAGS.recompute_grad:\r\n            _block = tf.recompute_grad(_block)\r\n        net = keras.layers.concatenate([net, _block(net)])\r\n\r\n    net = keras.layers.BatchNormalization()(net)\r\n    net = keras.layers.ReLU()(net)\r\n    net = keras.layers.Conv2D(128, 1, use_bias=False, padding=\"SAME\")(net)\r\n    net = keras.layers.AveragePooling2D()(net)\r\n\r\n    for _ in range(10):\r\n        def _block(x):\r\n            x = keras.layers.BatchNormalization()(x)\r\n            x = keras.layers.ReLU()(x)\r\n            x = keras.layers.Conv2D(32, 1, use_bias=False, padding=\"SAME\")(x)\r\n            x = keras.layers.BatchNormalization()(x)\r\n            x = keras.layers.ReLU()(x)\r\n            x = keras.layers.Conv2D(8, 3, use_bias=False, padding=\"SAME\")(x)\r\n            return x\r\n        if FLAGS.recompute_grad:\r\n            _block = tf.recompute_grad(_block)\r\n        net = keras.layers.concatenate([net, _block(net)])\r\n\r\n    net = keras.layers.BatchNormalization()(net)\r\n    net = keras.layers.ReLU()(net)\r\n    net = keras.layers.GlobalAveragePooling2D()(net)\r\n\r\n    net = keras.layers.Dense(10)(net)\r\n    net = keras.layers.Softmax()(net)\r\n\r\n    return net\r\n\r\n\r\ndef main(_):\r\n    if FLAGS.gpu:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, FLAGS.gpu))\r\n    if FLAGS.graph:\r\n        tf.compat.v1.disable_eager_execution()\r\n        tf.compat.v1.keras.backend.set_session(\r\n            session=tf.compat.v1.Session(\r\n                config=tf.compat.v1.ConfigProto(\r\n                    gpu_options=tf.compat.v1.GPUOptions(\r\n                        allow_growth=True\r\n                    )\r\n                )\r\n            )\r\n        )\r\n    else:\r\n        for gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n    tfds.core.constants.DATA_DIR = \"data\"\r\n    dataset_builder = tfds.image.FashionMNIST(version=\"3.*.*\")\r\n    dataset_builder.download_and_prepare()\r\n    dataset = dataset_builder.as_dataset(\r\n        split=\"train\",\r\n        shuffle_files=True,\r\n        as_supervised=True,\r\n    ).repeat().batch(FLAGS.batch_size)\r\n\r\n    inputs = keras.layers.Input((28, 28, 1), batch_size=FLAGS.batch_size)\r\n    model = keras.Model(inputs, dense_lenet(inputs))\r\n\r\n    model.compile(\r\n        optimizer='adam',\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        metrics=['accuracy']\r\n    )\r\n    model.summary()\r\n\r\n    model.fit(\r\n        x=dataset,\r\n        epochs=3,\r\n        steps_per_epoch=60000//FLAGS.batch_size,\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    app.run(main)\r\n```\r\n", "comments": ["@BinyanHu I've got a minimal example showing that it doesn't work for memory reduction over here: https://github.com/tensorflow/tensorflow/issues/30418#issuecomment-589820336", "@BinyanHu if you're interested, I've written a simple gradient checkpointing decorator [here](https://github.com/davisyoshida/tf2-gradient-checkpointing)", "@davisyoshida Thanks for doing this! I've been looking at implementing the same thing; will test out yours.", "@mathemakitten Happy to help! Do let me know if you run into any issues.", "@davisyoshida Thank you for sharing. Will test your implementation as soon as possible!", "@davisyoshida does this work with keras? if so, can you provide a small example of how to use it with keras?", "@paulter  I have a version working with Keras but sequential models only.\r\nI have create a pull request as part of TF addons github repo - https://github.com/tensorflow/addons/pull/1600.\r\nYou can find an example notebook here - https://github.com/pidajay/addons/blob/grad_checkpointing_eager/docs/tutorials/training_gradient_checkpointing.ipynb", "Thank you @pidajay ", "@pidajay thanks for the work on this.  \r\n\r\nyou say this only works on sequential models?  unfortunately, my keras model is too complex to be a sequential model, so I can't use your code.  Is there a way I can use what you've written?\r\n\r\nI don't mind manually checkpointing - in fact, it is probably preferable.  I'm writing custom keras lines for research and it would be nice to have something that specifies to recompute the gradient for a particular layer. \r\n\r\nUnfortunately, I don't really understand the documentation for `recompute_grad` here https://www.tensorflow.org/api_docs/python/tf/recompute_grad\r\n\r\nThe documentation there seems to imply that you go:\r\n\r\n`my_layer = tf.recompute_grad(keras.layers.Conv2D(...))`\r\n\r\nbut this gives no memory improvements.  \r\n\r\nAny chance that I can use what you've written, even if it's in a manual way?", "@paulter I have posted a small tutorial here https://github.com/pidajay/tf2_gradient_checkpointing/blob/master/tf_recompute_grad_tutorial.ipynb\r\nFor this to work you need to replace (or just copy the delta) the custom_gradient.py file with this version in my TF fork https://github.com/pidajay/tensorflow/blob/fix_gradient_checkpointing/tensorflow/python/ops/custom_gradient.py\r\nI plan to submit this fix as a PR soon but not sure if TF folks would be interested.\r\nUnfortunately my example demonstrates how to do this for a keras sequential model in eager mode. But splitting a functional or custom model and invoking recompute_grad should work the same way. Just that I need to check if the graph mode decorator has the same bug as the eager mode decorator (conversation at top of this thread says it has been fixed). Will dig into this week and let you know. Hope this helps.", "Any news for the Graph Mode models? I tried to use the code from @pidajay. Still, as long as I passed any keywords like `variables` to the recomputed grad function, TF raised an error 'The custom_gradient decorator currently supports keywords arguments only when eager execution is enabled\". ", "If you're looking to do gradient checkpointing in graph mode I suggest the implementation tf-slim here, which I've extracted and successfully tested on tf-nightly in graph mode on TPU: https://github.com/google-research/tf-slim/blob/a62dc893de5e46e6f2e9ec24a74b2abce026307a/tf_slim/layers/rev_block_lib.py", "> If you're looking to do gradient checkpointing in graph mode I suggest the implementation tf-slim here, which I've extracted and successfully tested on tf-nightly in graph mode on TPU: https://github.com/google-research/tf-slim/blob/a62dc893de5e46e6f2e9ec24a74b2abce026307a/tf_slim/layers/rev_block_lib.py\r\n\r\nThanks for your advice. I tried the extracted code from tf-slim. It did work to some degree, but in my case, it just reduced 5% of memory usage. Finally, I just copied the Tensorflow v1.15's contribs library's Graph Editor. With the OpenAI's Gradient Checkpointing, I got the memory reduction of 40% at the cost of 48% longer time."]}, {"number": 36968, "title": "Gradient checkpointing: Wrap `tf.keras.Model` or `tf.keras.layers.Layer` in `tf.recompute_gradients()`", "body": "I'm implementing gradient checkpointing with my Tensorflow 2.1 project, following the doc here: https://www.tensorflow.org/api_docs/python/tf/recompute_grad\r\n\r\nWhen I try\r\n```code\r\nmodel = tf.recompute_grad(model)\r\n```\r\nit fails, and likewise with \r\n```code\r\nmodel.layer = tf.recompute_grad(model.layer)\r\n```\r\n\r\nI see that Keras acts differently because tf.recompute_grad() wraps the function with an inner() call, but what should I do to get it working?", "comments": ["+1 to this question, I'm doing something similar with `tf-nightly==2.2.0.dev20200119` with custom layers inherited from Keras layers and the docs could use some clarification on \"keep a reference to the underlying object around for the purpose of accessing these variables\"", "+1 to this. would be great to know how to make use of both keras layers and the gradient checkpointing functionality. "]}, {"number": 36919, "title": "Memory leak in finalizing GeneratorDataset iterator", "body": "**System information**\r\n- OS Linux Ubuntu 18.04  \r\n- TensorFlow installed from `pipenv install tensorflow~=2.1  \r\n- TensorFlow version 2.1.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 2 x (RTX 2080 Ti 11GB) \r\n\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.preprocessing.image.ImageDataGenerator` output warning message `Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was canceled` when I am using `tf.keras.model.fit(..., worker = 10)` start to train to next epoch. The warning is gone when `worker = 1`. However, the warning does not seem to affect the result. \r\n\r\nIn #35100, people tend to mean the warning is superfluous and spurious and the warning has been removed in tf-nightly. But after my usage case, I believe the warning is correct. I have infinite models to be trained one by one (generated by a genetic algorithm). When `tf.keras.model.fit(..., worker = 10)` the error message print out and the memory does not get freed in the next epoch. After training a few models.  All the memory is occupied and the process gets killed. It does not happen when `worker=1` the memory gets freed for each epoch.\r\n\r\nI have try tf-nightly, The error message is removed but the memory problem is still there. \r\n\r\n**Describe the expected behavior**\r\nIt should free the memory for every epoch. \r\n\r\n**Code to reproduce the issue** \r\nI prepare my data in the following way.\r\n`samples` is NumPy array with shape (n, 90, 90, 3). n'th image withe 90 by 90 RGB image.\r\n`labels` is NumPy array with shape (n, 2), which is yes and no. [1, 0] is yes and [0, 1] is no.\r\n\r\nI have to make a pipenv file, data set and small script for you to demonstrate the memory problem. \r\n[Download link from google drive](https://drive.google.com/file/d/1oQ1wQkY1ZXimQNBQljlMbNB4VNYjDAp7/view?usp=sharing)\r\n(300 MB)\r\n\r\nUnzip the link above and run\r\n`pipenv install`\r\n`pipenv shell`\r\n`python main.py`\r\nThen you see the warning\r\n\r\n\r\n``` python\r\ntrain_generator = data_aug.flow(samples, labels, batch_size = 64)\r\nself.model.fit(train_generator, epochs=70, max_queue_size = 64, workers = 10)\r\n```\r\n\r\n**Other info / logs**\r\nDuring these 70 epochs, you can see the memory go up for each epoch and print the error message every time between the epoch. \r\n", "comments": ["@Mauhing \r\n\r\nCould you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!", "> @Mauhing\r\n> \r\n> Could you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!\r\n\r\nI have to make a pipenv file, data set and small script for you to demonstrate the memory problem. \r\n[Download link from google drive](https://drive.google.com/file/d/1oQ1wQkY1ZXimQNBQljlMbNB4VNYjDAp7/view?usp=sharing)\r\n(300 MB)\r\n\r\nUnzip the link above and run\r\n`pipenv install`\r\n`pipenv shell`\r\n`python main.py`\r\nThen you see the warning\r\n\r\nThe problem occurs when you train it on GPU. The warning shown twice when you train on two GPU at each epoch.", "Was able to reproduce the issue on colab. Please find the gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/3fddb14893a157806d399f7cb146a9f7/untitled19.ipynb). Thanks!", "> Was able to reproduce the issue on colab. Please find the gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/3fddb14893a157806d399f7cb146a9f7/untitled19.ipynb). Thanks!\r\n\r\nI want to make a kindly reminder that the colab code does not seem to print out the memory leak in each epoch. But the memory does not get free in each epoch.  Thanks"]}, {"number": 36911, "title": "tensorflow.python.ops.linalg.sparse.matmul crashes when inputs have different rank, but only if either is a CSRSparseMatrix", "body": "**System information** \r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04 \r\n- TensorFlow installed from: binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.2/7.6.4\r\n- GPU model and memory: Nvidia GeForce GTX 1050 4GB\r\n\r\n**Describe the current behavior**\r\n`tensorflow.python.ops.linalg.sparse.matmul` works for dense inputs of rank 2 and 3, or 3 and 2, but crashes if either of the inputs is a `CSRSparseMatrix`.\r\n\r\n**Describe the expected behavior**\r\n`tensorflow.python.ops.linalg.sparse.matmul` works for inputs of rank 2/3, or 3/2, and for any combination of dense/sparse inputs.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport numpy as np\r\nfrom tensorflow.python.ops.linalg.sparse import sparse as tfsp\r\n\r\na = np.ones((3, 3))\r\nb = np.ones((5, 3, 3))\r\na_sp = tfsp.CSRSparseMatrix(a)\r\nb_sp = tfsp.CSRSparseMatrix(b)\r\n\r\n# This works fine\r\noutput = tfsp.matmul(a, b)\r\nprint(output.shape)\r\n\r\n# These crash\r\noutput_sp = tfsp.matmul(a_sp, b)\r\noutput_sp = tfsp.matmul(a, b_sp)\r\noutput_sp = tfsp.matmul(a_sp, b_sp)\r\n```\r\n\r\n**Other info / logs** \r\n\r\nIf only one of the two inputs is sparse, the stack trace looks like this: \r\n\r\n```\r\n2020-02-19 18:51:52.547579: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at mat_mul_op.cc:507 : Invalid argument: Ranks of a and b must match, saw: 0x7ffc085957fc vs. 3.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-5-256f5bb49d4d> in <module>\r\n----> 1 output_sp = tfsp.matmul(a_sp, b)\r\n\r\n~/dev/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/linalg/sparse/sparse_csr_matrix_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, name)\r\n    220           transpose_b=transpose_b,\r\n    221           adjoint_a=adjoint_a,\r\n--> 222           adjoint_b=adjoint_b)\r\n    223     else:\r\n    224       # opA(A) . opB(B) = t(nopB(B) . nopA(A))\r\n\r\n~/dev/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/linalg/sparse/gen_sparse_csr_matrix_ops.py in sparse_matrix_mat_mul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, transpose_output, conjugate_output, name)\r\n    480         raise\r\n    481     except _core._NotOkStatusException as e:\r\n--> 482       _ops.raise_from_not_ok_status(e, name)\r\n    483   # Add nodes to the TensorFlow graph.\r\n    484   if transpose_a is None:\r\n\r\n~/dev/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6604   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6605   # pylint: disable=protected-access\r\n-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6607   # pylint: enable=protected-access\r\n   6608 \r\n\r\n~/dev/tf2/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Ranks of a and b must match, saw: 0x7ffc085957fc vs. 3. [Op:SparseMatrixMatMul]\r\n\r\n```\r\n\r\nIf both inputs are sparse:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-9a25f9aa5868> in <module>\r\n----> 1 output_sp = tfsp.matmul(a_sp, b_sp)\r\n\r\n~/dev/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/linalg/sparse/sparse_csr_matrix_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, name)\r\n    204           adjoint_a=adjoint_a,\r\n    205           adjoint_b=adjoint_b,\r\n--> 206           type=a.dtype)\r\n    207 \r\n    208       # In eager mode, shape inference functions are not called, and the output\r\n\r\n~/dev/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/linalg/sparse/gen_sparse_csr_matrix_ops.py in sparse_matrix_sparse_mat_mul(a, b, type, transpose_a, transpose_b, adjoint_a, adjoint_b, name)\r\n   1281         raise\r\n   1282     except _core._NotOkStatusException as e:\r\n-> 1283       _ops.raise_from_not_ok_status(e, name)\r\n   1284   # Add nodes to the TensorFlow graph.\r\n   1285   type = _execute.make_type(type, \"type\")\r\n\r\n~/dev/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6604   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6605   # pylint: disable=protected-access\r\n-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6607   # pylint: enable=protected-access\r\n   6608 \r\n\r\n~/dev/tf2/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Inner product dimensions of A and B do not agree.  Shapes are: [3,3] vs. [5,3,3] [Op:SparseMatrixSparseMatMul]\r\n\r\n```\r\n", "comments": ["The issue is related to `SparseMatrixMatMul` if only one of the inputs is sparse, and `SparseMatrixSparseMatMul` if both are sparse.\r\nLooking at the code it seems that `tfsp.matmul` is basically a switch to decide which op to use, so maybe this should be changed as a feature request?\r\nI opened it as a bug because the documentation does not specify any restrictions on the inputs.\r\n", "Was able to reproduce the issue with TF 2.1 and TF nightly. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/098f60ba73339d773791c538b7fdea14/36911.ipynb). Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/f50c5417320b7e0d488fc084e14cd738/35650.ipynb). Thanks!"]}, {"number": 36827, "title": "Keras with multiple outputs: cannot evaluate a metric without associated loss", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nI am training a model which has two outputs, but I only want to train it with respect to a loss function on the first output. Nervertheless, I want to calculate a metric based on the second output. So, I pass this kind of argument to `compile`: \r\n`loss={'main_output': tf.keras.losses.MeanSquaredError(),}, metrics={'second_output': tf.keras.losses.MeanSquaredError()}` or `loss={'main_output': tf.keras.losses.MeanSquaredError(), 'second_output': None}, metrics={'second_output': tf.keras.losses.MeanSquaredError()}`. This does not work as intendend as the metric on `second_output`is not computed when I call the `fit` method. The first try raises a Warning which explains why:\r\n```\r\nWARNING:tensorflow:Output second_output missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to second_output.\r\n```\r\nI don't get why the API is done that way, for what reason no data could be passed to `second_output`just because I didn't associate a loss.\r\n\r\n**Workaround**\r\nI had to write a custom loss functions returning always `0.` to make things work:\r\n```\r\ndef zero_loss(y_true, y_pred):\r\n    return 0.\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \r\n              loss={'main_output': tf.keras.losses.MeanSquaredError(), 'second_output': zero_loss}, \r\n              metrics={'second_output': tf.keras.losses.MeanSquaredError()})\r\n```\r\nBut this prints a lot of useless stuff when calling `model.fit`: this prints `second_output_loss: 0.0000e+00` on each epoch.\r\n\r\n**Describe the expected behavior**\r\nI think that metrics on outputs without losses should be able to be computed when calling `fit`or `evaluate`.\r\n\r\n**Code to reproduce the issue** \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\ninput_data = np.random.normal(size=(1024, 5))\r\ntarget_data = {'main_output' : np.random.normal(size=(1024,)), 'second_output': np.random.normal(size=(1024,))}\r\ninput_dataset = tf.data.Dataset.from_tensor_slices(input_data)\r\ntarget_dataset = tf.data.Dataset.from_tensor_slices(target_data)\r\ndataset = tf.data.Dataset.zip((input_dataset, target_dataset)).batch(16)\r\n\r\ndef make_model():\r\n    inp = tf.keras.Input(shape=(5))\r\n    hidden = tf.keras.layers.Dense(24, activation='relu')(inp)\r\n    out = tf.keras.layers.Dense(1, name='main_output')(hidden)\r\n    out2 = tf.keras.layers.Lambda(lambda x: x ** 2, name='second_output')(out)\r\n    model = tf.keras.Model(inputs=inp, outputs=[out, out2])\r\n    return model\r\n\r\nepochs = 2\r\n\r\nmodel = make_model()\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \r\n              loss={'main_output': tf.keras.losses.MeanSquaredError(),}, \r\n              metrics={'second_output': tf.keras.losses.MeanSquaredError()})\r\nmodel.fit(x=dataset, epochs=epochs)\r\n\r\nmodel = make_model()\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \r\n              loss={'main_output': tf.keras.losses.MeanSquaredError(), 'second_output': None}, \r\n              metrics={'second_output': tf.keras.losses.MeanSquaredError()})\r\nmodel.fit(x=dataset, epochs=epochs)\r\n\r\nmodel = make_model()\r\ndef zero_loss(y_true, y_pred):\r\n    return 0.\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \r\n              loss={'main_output': tf.keras.losses.MeanSquaredError(), 'second_output': zero_loss}, \r\n              metrics={'second_output': tf.keras.losses.MeanSquaredError()})\r\nmodel.fit(x=dataset, epochs=epochs)\r\n```", "comments": ["I am able to replicate the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/40449d13fb2f790a30a5595c30ef3852/untitled399.ipynb). Thanks!", "Thank you @durandg12. This is by design today but we can look into whether the behavior can be changed. Marked this as a feature request.", "Thank you @pavithrasv.\r\n\r\nTo give you more context about my request, this is my real use case: the main output of my model is the element-wise multiplication of two sub-outputs. To enforce more constraints on my model, I train it with one loss on each of the two sub-outputs. Nevertheless, to evaluate the quality of the model, I still need to compute metrics on the main output, without giving it a loss.\r\n\r\nAnother detrimental effect of my actual workaround is that the use of a custom object introduces an additional burden to load the model after saving it.", "Any news on this?", "I am having a similar issue, any update on this?"]}, {"number": 36809, "title": "import_graph_def  with cond nodes fails in eager/tf.function contexts", "body": "*Have I written custom code*: Yes\r\n*OS Platform and Distribution*: Ubuntu 16.04\r\n*Tensorflow installed from*: binary\r\n*Tensorflow version*: v2.1.0-rc2-17-ge5bf8de 2.1.0 (cpu)\r\n\r\n**Describe the current behavior**\r\n`import_graph_def` fails to rebuild graphs containing `tf.cond` nodes in eager mode/`tf.function`ed calls.\r\n\r\n**Describe the expected behavior**\r\nWork as it does using tf 1.x-style session calls, or eager mode/`tf.function`ed calls on graph defs containing similar nodes like `tf.where`.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    x = tf.keras.backend.placeholder(shape=(), dtype=tf.float32)\r\n    y = tf.keras.backend.placeholder(shape=(), dtype=tf.float32)\r\n    training = tf.keras.backend.placeholder(shape=(), dtype=tf.bool)\r\n    z = tf.cond(training, lambda: x * 10, lambda: y * 2)\r\n\r\ngraph_def = graph.as_graph_def()\r\n\r\n\r\ndef fn(*args):\r\n    x_value, y_value, training_value = args\r\n    return tf.graph_util.import_graph_def(graph_def,\r\n                                          input_map={\r\n                                              x.op.name: x_value,\r\n                                              y.op.name: y_value,\r\n                                              training.op.name: training_value,\r\n                                          },\r\n                                          return_elements=[z.name])\r\n\r\n\r\nclass ImportedCondTest(tf.test.TestCase):\r\n\r\n    def test_in_graph_mode(self):\r\n        with tf.Graph().as_default():\r\n            out = tf.graph_util.import_graph_def(graph_def,\r\n                                                 input_map={\r\n                                                     x.op.name: 0.,\r\n                                                     y.op.name: 1.0,\r\n                                                     training.op.name: True,\r\n                                                 },\r\n                                                 return_elements=[z.name])\r\n            with tf.compat.v1.Session() as sess:\r\n                sess.run(out)\r\n\r\n    def test_fn(self):\r\n        fn(0., 1., True)\r\n\r\n    def test_tf_function_fn(self):\r\n        tf.function(fn)(0., 1., True)\r\n\r\n    def test_map(self):\r\n        dataset = tf.data.Dataset.from_tensor_slices(([0.], [1.], [True]))\r\n        mapped = dataset.map(fn)\r\n        for _ in mapped:\r\n            pass\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.test.main()\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nRunning tests under Python 3.6.8: ...anaconda2/envs/tf-2.0-cpu/bin/python\r\n[ RUN      ] ImportedCondTest.test_in_graph_mode\r\n2020-02-17 15:13:40.567294: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-17 15:13:40.587856: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3491630000 Hz\r\n2020-02-17 15:13:40.588472: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f5c5ffbd30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-17 15:13:40.588510: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n[       OK ] ImportedCondTest.test_in_graph_mode\r\n[ RUN      ] ImportedCondTest.test_map\r\n2020-02-17 15:13:40.733678: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at optimize_dataset_op.cc:60 : Invalid argument: Unable to find FunctionDef for cond_true_4 in the registry.\r\n[  FAILED  ] ImportedCondTest.test_map\r\n[ RUN      ] ImportedCondTest.test_normal\r\n[  FAILED  ] ImportedCondTest.test_normal\r\n[ RUN      ] ImportedCondTest.test_session\r\n[  SKIPPED ] ImportedCondTest.test_session\r\n[ RUN      ] ImportedCondTest.test_tf_function\r\n[  FAILED  ] ImportedCondTest.test_tf_function\r\n======================================================================\r\nERROR: test_map (__main__.ImportedCondTest)\r\ntest_map (__main__.ImportedCondTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"cond_fail.py\", line 47, in test_map\r\n    for _ in mapped:\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 418, in __iter__\r\n    return iterator_ops.OwnedIterator(self)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 594, in __init__\r\n    self._create_iterator(dataset)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 600, in _create_iterator\r\n    dataset = dataset._apply_options()\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 381, in _apply_options\r\n    static_optimization_configs)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 4213, in __init__\r\n    **self._flat_structure)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 3638, in optimize_dataset\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Unable to find FunctionDef for cond_true_4 in the registry. [Op:OptimizeDataset]\r\n\r\n======================================================================\r\nERROR: test_normal (__main__.ImportedCondTest)\r\ntest_normal (__main__.ImportedCondTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"cond_fail.py\", line 39, in test_normal\r\n    fn(0., 1., True)\r\n  File \"cond_fail.py\", line 21, in fn\r\n    return_elements=[z.name])\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\", line 405, in import_graph_def\r\n    producer_op_list=producer_op_list)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\", line 487, in _import_graph_def_internal\r\n    validate_colocation_constraints)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\", line 221, in _PopulateTFImportGraphDefOptions\r\n    dst_output = input_dst._as_tf_output()  # pylint: disable=protected-access\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1125, in _as_tf_output\r\n    \"_as_tf_output not supported when eager execution is enabled.\")\r\nNotImplementedError: _as_tf_output not supported when eager execution is enabled.\r\n\r\n======================================================================\r\nERROR: test_tf_function (__main__.ImportedCondTest)\r\ntest_tf_function (__main__.ImportedCondTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"cond_fail.py\", line 42, in test_tf_function\r\n    tf.function(fn)(0., 1., True)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 638, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"...anaconda2/envs/tf-2.0-cpu/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Op type not registered 'cond_true_4' in binary running on jackd-5810. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'import/cond/then/_0' [Op:__inference_fn_61]\r\n\r\n----------------------------------------------------------------------\r\nRan 5 tests in 0.243s\r\n\r\nFAILED (errors=3, skipped=1)\r\n```", "comments": ["@jackd  I tried to reproduce the issue on colab with TF 2.1 but getting different error. Please find the [gist](https://colab.sandbox.google.com/gist/saikumarchalla/9264519582cef95421c7d15fcf6abdaa/-36809.ipynb) here.Also,tried on tf-nightly and getting same output. \r\nPlease provide more information to reproduce the issue.Thanks!", "Looks like `tf.test` doesn't like colab. Please replace the `__main__` section with\r\n\r\n```python\r\nif __name__ == '__main__':\r\n  # tf.test.main()\r\n  test = ImportedCondTest()\r\n  try:\r\n    test.test_in_graph_mode()\r\n    print('test_in_graph_mode passed')\r\n  except Exception:\r\n    print('test_in_graph_mode failed')\r\n  try:\r\n    test.test_fn()\r\n    print('test_fn passed')\r\n  except Exception:\r\n    print('test_fn failed')\r\n  try:\r\n    test.test_tf_function_fn()\r\n    print('test_tf_function_fn passed')\r\n  except Exception:\r\n    print('test_tf_function_fn failed')\r\n  try:\r\n    test.test_map()\r\n    print('test_map passed')\r\n  except Exception:\r\n    print('test_map failed')\r\n```", "Was able to reproduce the issue with TF 2.1 on colab. Please find the [gist](https://colab.sandbox.google.com/gist/saikumarchalla/a8f77b27844ee3644c29687a3afeee7f/-36809.ipynb) here.Thanks!", "Was able to reproduce the issue with TF 2.2 , nightly version (2.3.0-dev20200605) .Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/cc5fa86dcc5ffaeba3b16930df7fe051/untitled52.ipynb).Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/1e6689a8aaa80c490e3f62f7d050fbb9/35650.ipynb). Thanks!"]}, {"number": 36798, "title": "TF 2.0: Spyder auto complete and go to definition not working. Please fix it!", "body": "Due to the nature of the \"lazy import\" spyder IDE is not able to resolve the package tree and fails both \"auto complete\" and \"go to function definition\". \r\n\r\nSpyder guys told me TF should implemeny .pyi files to overcome this.\r\n\r\nhttps://github.com/spyder-ide/spyder/issues/11553", "comments": ["Which version of TF are you using? Can you please fill in the issue template?\r\n\r\n.pyi files already exist on master as @annarev is working on exactly this.", "Interesting. I added a `__init__.pyi` file under keras/ in https://github.com/tensorflow/tensorflow/commit/69f6601034942b018081715b31f6dd40111250af but looks like that doesn't help in Spyder. It still doesn't autocomplete for `from tensorflow.keras import metrics`\r\n\r\nEdit: just realized I was testing in a wrong environment. I will update when I retry it.", "Retested with correct environment now. Now autocomplete works for `from tensorflow.keras import metrics`, but jump to definition doesn't work for `from tensorflow.keras.layers import Conv2D`", "> \r\n> \r\n> Retested with correct environment now. Now autocomplete works for `from tensorflow.keras import metrics`, but jump to definition doesn't work for `from tensorflow.keras.layers import Conv2D`\r\n\r\nExactly, I also tested with Kite engine installed. I dunno if this is the beauty of the Kite engine but Autocomplete now works smooth though go to definition still fails."]}, {"number": 36797, "title": "Severe slowdown with 1 BN step in while_v2", "body": "I'm developing recurrent batch normalization for LSTM, and am seeing a best-case scenario of **8x slowdown** relative to base implementation (worst-case can **exceed 20x**). `tf.function` helps notably, but not nearly enough - and the behavior is unreasonable; applying the operation on _1 out of 150 steps_ yields a 6.7x slowdown, and on 150/150 steps, 8x. I suspect [`while_v2.while_loop()`](https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/control_flow_ops.py#L2666)'s gradient handling is responsible.\r\n\r\nThe problem's exacerbated by TF 2.1's [broken profiler](https://github.com/tensorflow/tensorboard/issues/3256) - my IDE (Spyder 4) is also unable to capture the call details in Eager or Graph, unlike in TF2. I also cannot develop in 2.0 per bug fixes and significant performance improvements. I don't think `while_v2` was designed to handle whatever I'm doing.\r\n\r\nWhat can I do to circumvent such a severe slowdown? Also, is this the best place to ask, or is there a dev forum somewhere to avoid a month-long back & forth? Help is appreciated.\r\n\r\n<hr>\r\n\r\n**DETAILS**: a part of the full code:\r\n\r\n```python\r\ndef recurrent_bn(self, inputs_t, op_name, step, training=None):\r\n    training = self._get_training_value(training)\r\n    beta, gamma, moving_mean, moving_variance = self._get_bn_vars(op_name)\r\n\r\n    @tf.function\r\n    def outs(inputs_t, step, beta, gamma, moving_mean, moving_variance, training):\r\n        if tf.math.less(step, self.max_inference_step) and (training != False):\r\n            mean_t, variance_t = self._get_stats_and_maybe_update(\r\n                inputs_t, step, moving_mean, moving_variance, training)\r\n        else:\r\n            mean_t, variance_t = moving_mean[-1], moving_variance[-1]\r\n        return nn.batch_normalization(inputs_t, mean_t, variance_t,\r\n                                      beta, gamma, self.bn_epsilon)\r\n\r\n    outputs = outs(inputs_t, step, beta, gamma, moving_mean,\r\n                   moving_variance, training)\r\n    outputs.set_shape(inputs_t.shape)\r\n    return outputs\r\n\r\ndef _get_bn_vars(self, op_name):\r\n    return [getattr(self, op_name + '_' + var_name) for var_name in\r\n            ('gamma', 'beta', 'moving_mean', 'moving_variance')]  # these were built\r\n```\r\n\r\n`recurrent_bn` is applied 9 times in `call` - 4 times for each of `i, f, c, o` of `kernel, recurrent`, and once on `c`. `_get_stats_and_maybe_update` returns `nn.moments` applied on `moving_...[step:step+1]` slice, and calls [`_assign_moving_average`](https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/normalization.py#L503) (added method to `LSTMCell`) when `training` is truthy. `LSTM` and `LSTMCell` are also inherited by `recurrent_v2` classes, which yielded a speedup.\r\n\r\n - `gamma, beta` have shape `(1, self.units)`\r\n - `moving_mean, moving_variance` have shape `(self.max_inference_steps, self.units)`\r\n - There are 9 of each -> 36 total\r\n - Example call: `x_i = self.recurrent_bn(x_i, 'kernel_i', step, training)`\r\n\r\n<hr>\r\n\r\n**Possible culprits**:\r\n\r\n 1. Gradients, **main culprit**, via T2-T5: T3 yielded significant speedup relative to every other test, including T4 - and T5 was negligible. It appears that updating the BN variables just _once_ triggers some gradient operations for _every timestep_ - hence the slowdown. \r\n 2. `_get_bn_vars`, **unknown**; the variables are being fetched from a Python dictionary in `tf.function`, and though there are no retracing warnings, it's still a suspect\r\n 3. `nn.batch_normalization` + `nn.moments`, **ruled out** via T2; only a 12% speedup is granted.\r\n 4. `step`, **ruled out** via T6 + T7; no noted performance difference\r\n 5. `CuDNNLSTM`, **ruled out**; I set `implementation = 1` and `recurrent_dropout = .05`\r\n\r\n**Tests**:\r\n\r\n - **T1**: replace `self.max_inference_steps` (`.numpy() == 150`) with `tf.constant(1, dtype='int64')\r\n - **T2**: T1 + move `nn.batch_normalization` under `if`, replace `else` with `return inputs_t`\r\n - **T3**: wrap `gamma, beta` with `K.stop_gradient()` _before_ `if`, and `mean_t, variance_t` _after_ `else` (otherwise cannot slice wrapped tensor)\r\n - **T4**: T3, but wrap everything _after_ `else`.\r\n - **T5**: `K.stop_gradient` on `inputs_t, training`\r\n - **T6**: immediately `return inputs_t` in `recurrent_bn`\r\n - **T7**: remove hidden state `step`, don't return `step + 1` in `call`\r\n\r\n**Bugs**:\r\n\r\n  1. Wrapping `self._assign_moving_average` with `self.add_update` fails [`_assert_same_graph()`](https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L5816):\r\n\r\n```\r\noriginal_item == updates[0], item == updates[1]\r\nupdates[0] == <tf.Operation 'training/Adam/Adam/AssignAddVariableOp' type=AssignAddVariableOp>\r\nupdates[1] == <tf.Operation 'AssignMovingAvg' type=ResourceStridedSliceAssign>\r\nupdates[0].graph == <tensorflow.python.framework.ops.Graph object at 0x000002A3473AF788>\r\nupdates[1].graph == <tensorflow.python.ops.control_flow_v2_func_graphs.CondBranchFuncGraph\r\n                     object at 0x000002A35C7F2548>\r\n```\r\n   -  [`updates`](https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/engine/training.py#L2059); `len(updates) == 37` = 9*4 + 1; so 4 per each of 9 calls to `recurrent_bn`, +1 default\r\n   - [`self.add_update`](https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/recurrent_v2.py#L1158) is used in `LSTM` right _after_ `K.rnn` - so it appears that `while_v2` is responsible for `updates[1].graph` being different\r\n   - `self.add_update` compiles, and yields a 10% speedup in Eager\r\n\r\n 2. As noted [here](https://github.com/tensorflow/tensorflow/issues/36840): while there aren't errors in Eager, the gradients are extremely small: `1e-9` to `1e-19`, whereas usually the same layers have `1e-6` to `1e-2`. This may or may not be a design rather than a bug problem.\r\n\r\n<hr>\r\n\r\n**IDE Profiler encapsulation**:\r\n\r\n<img src=\"https://user-images.githubusercontent.com/16495490/74617169-49255c80-5145-11ea-9aba-b5cca922e97c.png\" width=\"550\">\r\n\r\n\r\n", "comments": []}, {"number": 36626, "title": "Dataset.shuffle leads to worse training performance due to chunked processing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\n\r\nDataset.shuffle is (essentially) described as buffering N elements, then choosing 1 out of those N to return. Hence the input data is processed in chunks. In the extreme consider a shuffle buffer of size 2: In the first epoch only the first 2 elements can be returned. And the following code (using the 3rd and 4th element) produces only values of 0-4:\r\n\r\n```\r\ndataset = tf.data.Dataset.range(10)\r\ndataset = dataset.shuffle(2)\r\nfor _ in range(20):\r\n    print(list(dataset.as_numpy_iterator())[2:4])\r\n```\r\n\r\nFor good training performance (as in accuracy reaches high values fast) a complete shuffling of the dataset is required. This becomes obvious if one considers (accidentally or purposely) order sets of training data (in the MNIST example: First all zeros, then all ones, etc). There will be many batches mostly or completely consisting of only 1 label value. This does not work well with SGD approaches.\r\n\r\nSome statistics on MNIST (validation after 10 epochs by shuffle buffer size):\r\n```\r\n  100: Eval loss: 0.29262512158124876, accuracy: 0.9161659\r\n 1000: Eval loss: 0.2921471730925334,  accuracy: 0.9165665\r\n10000: Eval loss: 0.2914975070131895,  accuracy: 0.9171675\r\n60000: Eval loss: 0.29154436285488117, accuracy: 0.91696715\r\n```\r\nAs you can see the accuracy increases with the buffer size with everything else constant.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe whole dataset should be shuffled. This requires the concept of random access datasets. I believe the TFRecord format supports random access(?) So the shuffle operation can take random data from the whole dataset.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\ntfds.disable_progress_bar()\r\n\r\n\r\ndef make_datasets_unbatched():\r\n    # Scaling MNIST data from (0, 255] to (0., 1.]\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n        return image, label\r\n\r\n    datasets, info = tfds.load(name='mnist',\r\n                               with_info=True,\r\n                               as_supervised=True)\r\n\r\n    return {key: ds.map(scale).cache()\r\n            for key, ds in datasets.items()}, info.splits\r\n\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32,\r\n                               3,\r\n                               activation='relu',\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\n\r\ndatasets, infos = make_datasets_unbatched()\r\nmodel = build_and_compile_cnn_model()\r\n# Init\r\nmodel.train_on_batch(datasets['train'].batch(64))\r\nWEIGHTS_PATH = './init-weights'\r\nmodel.save_weights(WEIGHTS_PATH)\r\n\r\nfor buffer_size in (100, 1000, 5000, 10000, 60000):\r\n    print(\"Buffer size: %s\" % buffer_size)\r\n    model.load_weights(WEIGHTS_PATH)\r\n\r\n    model.fit(x=datasets['train'].shuffle(buffer_size).batch(64).repeat(),\r\n              epochs=10,\r\n              steps_per_epoch=infos['train'].num_examples // 64)\r\n    eval_loss, eval_acc = model.evaluate(datasets['test'].batch(64),\r\n                                         steps=infos['test'].num_examples //\r\n                                         64)\r\n    print(\"Eval loss: %s, accuracy: %s\" % (eval_loss, eval_acc))\r\n```\r\n\r\n", "comments": ["I have tried in colab with TF 2.1.0 and Nightly versions. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ed02f7cb1ee7c44ca2ef7111df9d2156/untitled634.ipynb). Thanks!", "I am working on a \"TFIndexedDataset\" RFC for externally shuffling dataset whose limit is beyond memory size. Let me know your preferred use case and I will consider adding it to the RFC. Thanks!", "@byronyi Basically a way to use either TFDS or TFRecord files where shuffling considers the whole dataset. To be more specific: I expect a dataset to know its size and provide random access to its elements where this is possible (and I assume for most datasets it is possible as they use images, videos or whatever files or lines which can be counted and ordered first, I even thought TFRecord files were made for that). With this 2 properties a random shuffle operation should exist, which produces every element from the dataset exactly once in a completely random order. So basically a `dataset.shuffle(dataset.size)` equivalent which doesn't preload everything in memory.", "Feature request:  \r\nAdd a `return_shuffled` argument to the `.cache()` method on the tf.data.Datasets. Whether the dataset is cached in memory or on disk it should be doable to create a pointer index while caching and sample randomly from the index when reading from the cache. Obviously, you can't do that from a TFRecord file ccurrently and implementing it would be so much work. It also makes sense that data would be stored on a magnetic drive but cached on an SSD.\r\n\r\n@Flamefire The recommended approach is to write your dataset out into multiple TFRecord files and then when loading the data doing it like so:\r\n\r\n```python\r\nfiles = tf.data.Dataset.from_tensor_slices(filenames)\r\nfiles = files.shuffle(len(filenames))\r\nds = files.interleave(lambda x: tf.data.TFRecordDataset(x, compression_type='GZIP').prefetch(1),\r\n                      num_parallel_calls=tf.data.experimental.AUTOTUNE,\r\n                      deterministic=False, cycle_length=10)\r\n```\r\n\r\nThat way you are reading from 10 files at once (still serially in each file though), feeding it to the shuffle buffer at the appropriate point in your pipeline. And each epoch you shuffle the files you are reading from and if you have saved the dataset in more than 10 files that will change the order that you get the files (i.e. if you have 20 files then the expectation is that 5 files will be the same in the first half of epochs 1 and 2).\r\n\r\n@jsimsa I really think adding a shuffle capability to the cache of TF Dataset would be a slam-dunk. It would definitely let me remove a lot of code from my projects that only exists to deal with Tensorflows limited capability in shuffling \u2764\ufe0f", "@grofte am I correct to assume that you would want the functionality of `return_shuffled` to be equivalent to `cache().shuffle(buffer_size=NUM_ELEMENTS)` while avoiding the extra buffer that `shuffle` introduces? In particular, the expected behavior during the first epoch (where the cache is constructed) would be that the first element is returned only after the entire cache has been constructed.\r\n\r\nI don't think the `return_shuffled` is general enough (e.g. you would also need to specify the RNG seed management to achieve reproducibility and deterministic reshuffling across epochs). What I can imagine though is to introduce `tf.data.Dataset.at(i)` transformation which could be used to index into \"indexable\" datasets (such as `cache` or `from_tensor_slices`). The way one could use it then would be:\r\n\r\n```\r\nds = ... # the dataset to be cached\r\nds = ds.cache()\r\n\r\nindices = tf.data.Dataset.range(NUM_ELEMENTS)\r\nindices = indices.shuffle(NUM_ELEMENTS)\r\nds = indices.flat_map(lambda index: ds.at(index))\r\n```\r\n\r\nThis is a more general API, which handles the aspect of shuffling mentioned above and could be used with any datasets that supports indexing. The main disadvantage is that it requires the number of elements of the caches dataset to be known.", "You are 100% correct that an `.at` method would be an excellent extension. Probably very useful for researchers that need their data with some kind of deterministic properties. But for both applications you are going to need to read the whole dataset once without training - analogous to how the Keras preprocessing layers have an `.adapt` method. Because you need to link the indices to memory/disk adresses either way. Say you didn't and your first three indices were 100, 3, and 2000. Then you would have to read the first 100 elements/rows from the file, then 3, and then 1997. So 2100 reads to get three elements/rows. That's very ineffiencient. \r\n\r\nAs I understand it caching currently happens during the first training epoch for efficiency reasons. However, if you expect to train your model for many epochs then performing the caching before is only a small percentage increase in time and a large potential increase in model quality (and faster convergence which reduces training time). So you wouldn't have to know the number of elements in advance.", "> I am working on a \"TFIndexedDataset\" RFC for externally shuffling dataset whose limit is beyond memory size. Let me know your preferred use case and I will consider adding it to the RFC. Thanks!\r\n\r\nThe proposed IndexedDataset is stored with the very same on-disk format as that of cached datasets, but it could be stored in the remote storage without reading it through first. The `dataset.at(index)` API looks perfect, and if people in this thread still interest, I could continue work on this proposal.", "@grofte I am a little bit confused by your example. I would assume that if the first three indices were 100, 3, and 2000, then you would need to read 100 elements to get the first results, and then either 0 (if the read elements are cached) or 3, and then either 1900 (if the read elements are cached) or 2000. So you would either need to read 2000 elements if the read elements are cached or 2103 elements if there is no caching -- neither of which matches your description.\r\n\r\nNote that `at` will generally not require reading the whole dataset. For dataset that cannot be efficiently indexed into, it simply won't be supported (i.e. it will throw an informative error). The existing `skip` transformation could be used to perform this inefficient indexing that you are alluding to.\r\n\r\nI was asking you about what would you expect the behavior of `cache(..., returned_shuffled=True)` be on the first epoch (e.g. when the data is not cached yet). In order for the method to be able to return any permutation of the input dataset, it needs to a) known the cardinality of the input dataset so that it knows the range of indices to consider and b) possibly require reading the entire input dataset if the first element to return after shuffling is the last element of its input.\r\n\r\n@byronyi it makes sense for us to chat over VC about your proposal -- we have internal WIP proposal for indexed datasets as well and it would be great if we could align the two and I would be more than happy for you to lead the effort. I will reach out to you via email."]}, {"number": 36603, "title": "TensorRT plugins", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Maybe\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIs it possible to use TensorRT plugin in TrtGraphConverterV2? Should I register the plugin in TF? I noticed there's some code for  plugins here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/tf2tensorrt/plugin, but I can't find any example.\r\nIt would be best to be able to use plugins directly from TrtGraphConverterV2\r\n\r\n**Will this change the current api? How?**\r\n\r\nNot necessary\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone who want to speed up with TensorRT. Current set od convertable layers is very limited\r\n\r\n**Any Other info.**\r\n", "comments": ["@aaroey, maybe you can help?", "@soldierofhell do you want to support plugins for custom ops, or plugins for a TF subgraph?\r\n\r\n@sanjoy In order to support the second case, I think we'll need a way for user to specify the subgraph (e.g. inputs + outputs), which is tricky since TF2 doesn't retain the node names during function instantiation.", "Hi @aaroey, thank you for your reply. To be precise I want to use custom TensorRT layers like in this [example](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#example2_add_custlay_c). But UFF/graphsurgeon approach is now depreciated. Recommended way is to use TrtGraphConverterV2 which in practice leave you with TF/TensorRT mix, because you are now longer able to replace TF ops with TensorRT plugins (https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html/index.html#tensorrt-plan). There are many complains that in practice this doesn't give you performace improvement.\r\nIn my opinion this means for most of users that TrtGraphConverterV2 is useless, at least if you really need performance boost and want to convert your model purely to TensorRT/C++ to for example deploy it on Jetson devices.\r\n\r\nRegards,", "Hi. So, there is no way to specify my own operation in tf-trt like cuda/c++ plugin?\r\nAnd will be it possible in future versions?"]}, {"number": 36539, "title": "tf.data.Dataset unusable with steps_per_epoch standard training loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\n\r\nThis is the underlying issue for https://github.com/tensorflow/tensorflow/issues/36153\r\n\r\nBasically:\r\n- `Dataset` can be used as/converted to an iterator\r\n- Once the iterator reaches the end of the dataset it does not restart -> Yields no more samples\r\n- `steps_per_epoch` for the `fit` method of a keras model can be used to specify the number of batches to run per epoch, more exactly: The number of times the iterator is advanced/dereferenced\r\n- `steps_per_epoch` is required for e.g. if not the full dataset should be traversed. Either due to external requirements or to avoid using the trailing (incomplete) batch.\r\n- `steps_per_epoch` is absolutely required to be set for `MultiWorkerMirroredStrategy`: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy\r\n- Whether to recreate an iterator for each epoch out of the dataset is determined by `DatasetAdapter.should_recreate_iterator` at https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/engine/training_v2.py#L241-L242\r\n- Note that the iterator absolutely must be recreated for all common use cases:\r\n  - Iterate over full dataset\r\n  - Iterate over full dataset except incomplete trailing batch\r\n  - Iterate over a random subset of the full dataset per epoch\r\n  - Not to be recreated for: Infinite datasets. **Maybe** If the full dataset should be consumed over multiple epochs. But why? What should happen if the dataset is exhausted after some epoch? Usually restart, right?\r\n\r\nIn the current TF (2.1.0) the implementation of `DatasetAdapter.should_recreate_iterator` is: `return self.get_size() is not None or steps_per_epoch is None`: https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/engine/data_adapter.py#L208\r\n\r\nThis is **wrong**. For datasets the size is always None (see https://github.com/tensorflow/tensorflow/issues/36531 which intends for this to be changed) and as motivated above having `steps_per_epoch` set is a common use case but the iterator should still be recreated on each epoch.\r\n\r\nThis was recently changed to ` (self._user_steps is None or cardinality.cardinality(self._dataset).numpy() == self._user_steps)` https://github.com/tensorflow/tensorflow/commit/6be131d0860559954c42685a87c63f16cebb2185#diff-f8dd40712ac721c1b363e1a1ec44c1a3R741-R747\r\n\r\nThis is also **wrong**. Again assume `_user_steps` is set (see above reasoning): First the `cardinality` might be unknown, e.g. for any TFDS dataset (and `TFRecordDataset`) it is unknown. I guess due to use of `interleave` in the dataset reader. Second even if the size was known it may not be equal to the number of steps. Common example: Skipping the last batch.\r\n\r\n**Describe the expected behavior**\r\n\r\nThis is a design issue and hence hard to resolve.\r\n\r\nIn general it would be best to eliminate the UNKNOWN size of a dataset. But when reading data line-by-line from a file it might not be known upfront. So the user has to input the size of the dataset or specify explicitly `AUTO` which would iterate over the whole dataset once to get the number of samples. This can be costly but should not be possible in general (e.g. TFDS knows the number of samples)\r\n\r\nI think the sanest approach would be to default to recreating the iterator on each epoch UNLESS the dataset is known to be infinite. This might still be wrong for cases I can't imagine right now but is correct for all cases I can think of. Maybe even allow the user to specify this, but this default is IMO way better than the current.\r\n\r\nThe other approach would be to recreate the iterator when it runs out of data after starting an epoch. This would partially solve the issue, but fails for:\r\n- Omitting the trailing batch: It would yield the incomplete batch from the last epoch first, but that should be skipped.\r\n- Using only some random samples per batch but wanting to shuffle before each batch: It would happily consume the rest of the samples and not see the samples used in an earlier epoch until it runs out of data\r\n\r\nWith the UNKNOWN size fixed, one could also fix the check at  https://github.com/tensorflow/tensorflow/commit/6be131d0860559954c42685a87c63f16cebb2185#diff-f8dd40712ac721c1b363e1a1ec44c1a3R741-R747 to check if `_user_steps` yields 1 epoch (and optional trailing batches) by using `size // steps == 1` which would be better than the previous approach (recreate iterator when out of data) as the use case with of Omitting the trailing batch is covered. But it would fail for the other.\r\n\r\nSo my suggestion would to\r\n- (optional) avoid UNKNOWN sizes\r\n- recreate iterators unless dataset is infinite by default\r\n- allow the user to overwrite this explicitly, maybe via `with_options` of the dataset\r\n\r\n\r\n**Code to reproduce the issue**\r\nSome reduced example code based on e.g. https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nfrom tensorflow.python.data.experimental.ops import cardinality\r\nimport numpy as np\r\ntfds.disable_progress_bar()\r\n\r\n# Scaling MNIST data from (0, 255] to (0., 1.]\r\ndef scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32,\r\n                               3,\r\n                               activation='relu',\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\nBATCH_SIZE = 64\r\nif False:\r\n    examples = np.ones([10*BATCH_SIZE,28,28,1])\r\n    labels = np.ones([examples.shape[0]])\r\n    dataset = tf.data.Dataset.from_tensor_slices((examples, labels))\r\n    num_examples = examples.shape[0]\r\nelse:\r\n    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n    dataset = datasets['test']\r\n    num_examples = info.splits['test'].num_examples\r\n\r\nx = dataset.map(scale).cache().shuffle(10000).batch(BATCH_SIZE)\r\nmodel = build_and_compile_cnn_model()\r\n\r\ncard = cardinality.cardinality(x)\r\nnum_batches = sum(1 for _ in x)\r\nfull_batches = num_examples // BATCH_SIZE\r\nprint(\"Samples: %s\\nBatches: %s (%s full)\\nCardinality: %s\" % (num_examples, num_batches, full_batches, card))\r\nmodel.fit(x=x, epochs=2, steps_per_epoch=full_batches)\r\n\r\n```\r\n\r\n**Other info / logs**\r\nThere is a warning:\r\n> WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches). You may need to use the repeat() function when building your dataset.\r\n\r\nIt seems that adding `repeat()` and hence creating an infinite dataset is a viable option. However the docu also states\r\n> In TF 1.X, the idiomatic way to create epochs was through the `repeat` transformation:\r\n>     In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it possible to also create epochs through Python iteration:\r\n\r\nSo it seems that it should not be required and as per above explanation the return value of `DatasetAdapter.should_recreate_iterator` is not correct.", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/62479d460ebecd024c86e405314d5979/36539.ipynb). Thanks!", "Karmel, could you please triage this to someone on the Keras team. Thank you.", "@jsimsa Why is this a documentation issue if there is no way to solve this problem using the current code? As I argued in the initial description there is no way to use the current API to work with common use cases, in particular: \"Iterate over full dataset except incomplete trailing batch\". I reviewed the code to come to this conclusion and posted the relevant links to verify this along with the arguments why that code is wrong.", "I can confirm the same issue. Waiting for a fix.", "Is there anyway, tf.data() can be used with keras?\r\n\r\n```python\r\ndef load_image(file, label):\r\n    nifti = np.asarray(nibabel.load(file.numpy().decode('utf-8')).get_fdata())\r\n\r\n    xs, ys, zs = np.where(nifti != 0)\r\n    nifti = nifti[min(xs):max(xs) + 1, min(ys):max(ys) + 1, min(zs):max(zs) + 1]\r\n    nifti = nifti[0:100, 0:100, 0:100]\r\n    nifti = np.reshape(nifti, (100, 100, 100, 1))\r\n    nifti = tf.convert_to_tensor(nifti, np.float64)\r\n    return nifti, label\r\n\r\n\r\n@tf.autograph.experimental.do_not_convert\r\ndef load_image_wrapper(file, labels):\r\n    return tf.py_function(load_image, [file, labels], [tf.float64, tf.float64])\r\n\r\n\r\ndef train_input_fn():\r\n    dataset = tf.data.Dataset.from_tensor_slices((train, labels))\r\n    dataset = dataset.map(load_image_wrapper, num_parallel_calls=6)\r\n    dataset = dataset.batch(6, drop_remainder=True).repeat()\r\n    dataset = dataset.prefetch(buffer_size=4)\r\n    # iterator = iter(dataset)\r\n    return dataset\r\n\r\n\r\n########################################################################################\r\nwith tf.device(\"/cpu:0\"):\r\n    with tf.device(\"/gpu:0\"):\r\n        model = tf.keras.Sequential()\r\n\r\n        model.add(Conv3D(64,\r\n                         input_shape=(100, 100, 100, 1),\r\n                         data_format='channels_last',\r\n                         kernel_size=(7, 7, 7),\r\n                         strides=(2, 2, 2),\r\n                         padding='valid',\r\n                         activation='relu'))\r\n\r\n    with tf.device(\"/gpu:1\"):\r\n        model.add(Conv3D(64,\r\n                         kernel_size=(3, 3, 3),\r\n                         padding='valid',\r\n                         activation='relu'))\r\n\r\n    with tf.device(\"/gpu:2\"):\r\n        model.add(Conv3D(128,\r\n                         kernel_size=(3, 3, 3),\r\n                         padding='valid',\r\n                         activation='relu'))\r\n\r\n        model.add(MaxPooling3D(pool_size=(2, 2, 2),\r\n                               padding='valid'))\r\n\r\n    with tf.device(\"/gpu:3\"):\r\n        model.add(Conv3D(128,\r\n                         kernel_size=(3, 3, 3),\r\n                         padding='valid',\r\n                         activation='relu'))\r\n\r\n        model.add(MaxPooling3D(pool_size=(2, 2, 2),\r\n                               padding='valid'))\r\n\r\n    with tf.device(\"/gpu:4\"):\r\n        model.add(Conv3D(128,\r\n                         kernel_size=(3, 3, 3),\r\n                         padding='valid',\r\n                         activation='relu'))\r\n\r\n        model.add(MaxPooling3D(pool_size=(2, 2, 2),\r\n                               padding='valid'))\r\n\r\n        model.add(Flatten())\r\n\r\n        model.add(Dense(256, activation='relu'))\r\n        model.add(Dropout(0.7))\r\n        model.add(Dense(256, activation='relu'))\r\n        model.add(Dropout(0.7))\r\n        model.add(Dense(1, activation='sigmoid'))\r\n\r\n\r\nmodel.compile(loss=tf.keras.losses.binary_crossentropy(from_logits=True),\r\n              optimizer=tf.keras.optimizers.Adagrad(0.01),\r\n              metrics=['accuracy'])\r\n\r\n########################################################################################\r\n\r\nmodel_dir = tempfile.mkdtemp()\r\nkeras_estimator = tf.keras.estimator.model_to_estimator(\r\n    keras_model=model, model_dir=model_dir)\r\n\r\nkeras_estimator.train(input_fn=train_input_fn, steps=4600)\r\n```\r\nTried every possible way of doing this, and with the above example, it outputs:\r\n`TypeError: binary_crossentropy() missing 2 required positional arguments: 'y_true' and 'y_pred'`", "This error is not related to tf.data, you are using keras loss function incorrectly. \r\n\r\nInstead of writing:\r\n```\r\nloss=tf.keras.losses.binary_crossentropy(from_logits=True)\r\n```\r\nyou should do:\r\n```\r\nloss=lambda *args: tf.keras.losses.binary_crossentropy(*args, from_logits=True)\r\n```\r\n\r\n", "[This](https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-596213250) helped in my case.", "cc @fchollet for thoughts on this usability question. ", "I solved this issue in one way, when passing the validation data into the generator, pass the **tfds.as_numpy()** generator creation function instead of passing the created generator itself. @Flamefire \r\n\r\n ```\r\nds_train,ds_val,ds_test= tfds.load('coco/2017', data_dir='/data/datasets/tensorflow_datasets/', split=['train','validation','test'], download=False)\r\n ds_train = tfds.as_numpy(ds_train)\r\n training_generator = tf.data.Dataset.from_generator(lambda:data_generation(gen_object = ds_train,\r\n                     batch_size=params[\"batch_size\"]),\r\n                     output_types=(tf.float32, tf.int32),\r\n                    output_shapes=(tf.TensorShape((params[\"batch_size\"],1,120,160)), tf.TensorShape((params[\"batch_size\"], 2))))\r\n validation_generator = tf.data.Dataset.from_generator(lambda: data_generation(gen_object = tfds.as_numpy(ds_val), \r\n                                batch_size=None, test=True),\r\n                                 output_types=(tf.float32, tf.int32),\r\n                                 output_shapes=(tf.TensorShape((4614, 1, 120,160)), tf.TensorShape((4614, 2))))\r\n```", "Sure that would work because the whole dataset will be loaded into memory as a numpy array (for which TF knows how to get the size) but as described in the documentation using numpy arrays for large datasets is discouraged. Example: Try finding enough RAM to load the whole of ImageNet ;)", "Hello everyone, \r\nCan you please help in understanding the functionality of `dataset = dataset.repeat(-1)`? In my case, when I am using it, it iterates the dataset infinitely. However, when I set it any other number than -1, it fetches only a single sample. ", "Was able to reproduce issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/bf698a00e6b8877da48ad1738498255a/35650.ipynb). Thanks!"]}, {"number": 36516, "title": "Misleading documentation for hook parameters in estimators evaluate, predict, train methods", "body": "\r\nThe documentation for pre-made estimators such as tensorflow.estimator.LinearRegressor says that each of the methods evaluate, predict and train take a parameter called \"hooks\", which is a list of tensorflow.train.SessionRunHooks. However, no such class exist in the v2.1 API; instead, the source code says that it expects instances of tensorflow.compat.v1.train.SessionRunHook.\r\n\r\nIn the example of LinearRegressor (other pre-made estimators share the same problem), this is the API doc, which I believe is generated directly from the source code (see the documentation of the parameters for the train, evaluate and predict methods):\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#evaluate\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#predict\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#train\r\n\r\nThis is the source code that says it expects a class from the compat.v1 API:\r\nhttps://github.com/tensorflow/estimator/blob/a7ba3b45d07dd517a0e6ff38e90ae3aa240f424b/tensorflow_estimator/python/estimator/estimator.py#L1947\r\n\r\n\r\n\r\n", "comments": ["@espinafre \r\nCould you please verify with the latest version if the issue still persist."]}, {"number": 36505, "title": "Loading Tensorflow models from disk is instant while loading from gcs is very slow", "body": "**System information**\r\n- OS Platform and Distribution: Both MacOS and Ubuntu 18 (Tensorflow/serving)\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nI want to use Tensorflow serving to load multiple Models. If I mount a directory containing the model, loading everything is done in an instant, while loading them from a `gs://` path takes around 10 seconds per model. \r\n\r\nWhile researching the issue I discovered this is probably a Tensorflow issue and not a Tensorflow Serving issue as loading them in Tensorflow is a huge difference as well:\r\n```\r\n    [ins] In [22]: %timeit tf.saved_model.load('test/1')\r\n    3.88 s \u00b1 719 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n    [ins] In [23]: %timeit tf.saved_model.load('gs://path/to/test/1')\r\n    30.6 s \u00b1 2.66 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\nThen it could be that downloading the model (which is very small) is slow, but I tested this as well:\r\n```\r\n    def test_load():\r\n        bucket_name = 'path'\r\n        folder='test'\r\n        delimiter='/'\r\n        file = 'to/test/1'\r\n     \r\n        bucket=storage.Client().get_bucket(bucket_name)\r\n        blobs=bucket.list_blobs(prefix=file, delimiter=delimiter)\r\n        for blob in blobs:\r\n            print(blob.name)\r\n            destination_uri = '{}/{}'.format(folder, blob.name)\r\n            blob.download_to_filename(destination_uri)\r\n\r\n    [ins] In [31]: %timeit test_load()\r\n    541 ms \u00b1 54.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\nAny idea what is happening here?\r\n\r\n**Describe the expected behavior**\r\nAround the same load time for external vs local models. The first load from gs can be slow if the auth needs to happen, but even authenticating is still way faster than the models loads.\r\n\r\n", "comments": ["@Sam-Persoon,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "This is just an untrained example model from the TF docs, but it has the issue:\r\n```\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import keras\r\nimport timeit\r\n\r\nnum_tags = 12  # Number of unique issue tags\r\nnum_words = 10000  # Size of vocabulary obtained when preprocessing text data\r\nnum_departments = 4  # Number of departments for predictions\r\n\r\ntitle_input = keras.Input(shape=(None,), name='title')  # Variable-length sequence of ints\r\nbody_input = keras.Input(shape=(None,), name='body')  # Variable-length sequence of ints\r\ntags_input = keras.Input(shape=(num_tags,), name='tags')  # Binary vectors of size `num_tags`\r\n\r\n# Embed each word in the title into a 64-dimensional vector\r\ntitle_features = layers.Embedding(num_words, 64)(title_input)\r\n# Embed each word in the text into a 64-dimensional vector\r\nbody_features = layers.Embedding(num_words, 64)(body_input)\r\n\r\n# Reduce sequence of embedded words in the title into a single 128-dimensional vector\r\ntitle_features = layers.LSTM(128)(title_features)\r\n# Reduce sequence of embedded words in the body into a single 32-dimensional vector\r\nbody_features = layers.LSTM(32)(body_features)\r\n\r\n# Merge all available features into a single large vector via concatenation\r\nx = layers.concatenate([title_features, body_features, tags_input])\r\n\r\n# Stick a logistic regression for priority prediction on top of the features\r\npriority_pred = layers.Dense(1, activation='sigmoid', name='priority')(x)\r\n# Stick a department classifier on top of the features\r\ndepartment_pred = layers.Dense(num_departments, activation='softmax', name='department')(x)\r\n\r\n# Instantiate an end-to-end model predicting both priority and department\r\nmodel = keras.Model(inputs=[title_input, body_input, tags_input],\r\n                    outputs=[priority_pred, department_pred])\r\nmodel.compile(optimizer='adam', loss='mae')\r\nmodel.save('test')\r\n\r\n# --------\r\nimport_module = \"import tensorflow as tf\"\r\ncode = \"tf.saved_model.load('test')\"\r\nprint(timeit.timeit(stmt=code, setup=import_module, number=10))\r\n\r\n# Upload the test model to a gcs bucket you can access\r\ncode_gcs = \"tf.saved_model.load('gs://your-bucket/test')\"\r\nprint(timeit.timeit(stmt=code_gcs, setup=import_module, number=10))\r\n```", "There are two different filesystems involved. Can you try to post the times for the following code\r\n\r\n```python\r\nlocal_filename = \"/path/to/some/local/file\"\r\ngcs_filename = \"gs://bucket/and/path/to/file\"\r\nsize = 1024\r\n\r\nfilename = local_filename   # or gcs_filename\r\nwith tf.io.gfile.GFile(filename) as f:\r\n  f.read(size)\r\n```\r\n\r\nEnsure that the file has at least `size` bits, keep increasing the size (1KB, 2KB, 4KB, ... 1MB, 1GB) and compare the two times (maybe plot them)?", "The following code is used for the plot\r\n```\r\n# --------\r\nimport_module = \"import tensorflow as tf\"\r\nsizes = [1024*i for i in [1, 2, 4, 8, 50, 100, 500, 1000]]\r\nlocal_times = []\r\ngcs_times = []\r\nfor size in sizes:\r\n    print(size)\r\n    code = f\"\"\"\r\nfilename = 'test/saved_model.pb'\r\nwith tf.io.gfile.GFile(filename, 'rb') as f:\r\n    f.read({size})\r\n\"\"\"\r\n    local_times.append(timeit.timeit(stmt=code, setup=import_module, number=10))\r\n    code_gcs = f\"\"\"\r\nfilename = 'gs://bucket/test/saved_model.pb'\r\nwith tf.io.gfile.GFile(filename, 'rb') as f:\r\n    f.read({size})\r\n\"\"\"\r\n    gcs_times.append(timeit.timeit(stmt=code_gcs, setup=import_module, number=10))\r\n```\r\nMind the log scales in the plot\r\n![plot](https://user-images.githubusercontent.com/54799731/74019743-68c7c280-4998-11ea-9c32-edbae525471c.png)\r\n", "This seems reasonable as the difference between local and remote access over the network.", "Loading 1 kb in 6s is really slow though. I tested the impact of sequentially loading the models to check if maybe the authentication to gcs impacts things. The following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport time\r\nfrom google.cloud import storage\r\nimport os\r\n\r\nmodel_names = ['test1', 'test2', 'test3', 'test4']\r\n\r\nstorage_client = storage.Client()\r\nbucket = storage_client.get_bucket('bucket-name')\r\nfor model in model_names:\r\n\tprint(model)\r\n\r\n\tstart_time = time.time()\r\n\ttf.saved_model.load(f'gs://bucket-name/models/{model}/1')\r\n\telapsed_time = time.time() - start_time\r\n\tprint(f'Loading from gcs: {elapsed_time}')\r\n\r\n\tstart_time = time.time()\r\n\tblobs = bucket.list_blobs(prefix=f'models/{model}/1')  # Get list of files\r\n\tfolder = f'./download_folder'\r\n\tfor blob in blobs:\r\n\t\t// create dirs if not exist, code omitted\r\n\t\tblob.download_to_filename(folder + '/' + blob.name)\r\n\r\n        // time local disk loading\r\n\tstart_time_internal = time.time()\r\n\ttf.saved_model.load(os.path.join(folder, 'models', model, '1' ))\r\n\telapsed_time_internal = time.time() - start_time_internal\r\n\tprint(f'Local disk: {elapsed_time_internal}')\r\n\r\n\telapsed_time = time.time() - start_time\r\n\tprint(f'Downloading from gcs, then loading: {elapsed_time}')\r\n```\r\nGives the following outputs:\r\n\r\n```\r\ntest1\r\nLoading from gcs: 32.72501277923584\r\nLocal disk: 2.4520139694213867\r\nDownloading from gcs, then loading: 4.770485162734985\r\n\r\ntest2\r\nLoading from gcs: 0.9281768798828125\r\nLocal disk: 0.013478279113769531\r\nDownloading from gcs, then loading: 0.45862293243408203\r\n\r\ntest3\r\nLoading from gcs: 57.323729038238525\r\nLocal disk: 1.7698051929473877\r\nDownloading from gcs, then loading: 3.2642600536346436\r\n\r\ntest4\r\nLoading from gcs: 3.6672658920288086\r\nLocal disk: 0.053382158279418945\r\nDownloading from gcs, then loading: 0.8592350482940674\r\n```\r\n\r\ntest1 and test3 are larger models, test2 and 4 are smaller. Large models really suffer from this.", "Can you compare with downloading a file from GCS from command-line? (That is, not using TensorFlow)", "Single-threaded downloading the files gives the following output:\r\n```\r\ntest1\r\nLoading from gcs: 29.73722219467163\r\nLoading from local disk: 2.546816825866699\r\nDownloading from gcs, then loading from local disk: 3.7411539554595947\r\nDownloading from gcs using cli, then loading from local disk: 9.842350006103516\r\n\r\ntest2\r\nLoading from gcs: 0.721973180770874\r\nLoading from local disk: 0.01868605613708496\r\nDownloading from gcs, then loading from local disk: 0.31568098068237305\r\nDownloading from gcs using cli, then loading from local disk: 3.2689220905303955\r\n\r\ntest3\r\nLoading from gcs: 49.419824838638306\r\nLoading from local disk: 1.8356938362121582\r\nDownloading from gcs, then loading from local disk: 2.639312982559204\r\nDownloading from gcs using cli, then loading from local disk: 11.165163040161133\r\n\r\ntest4\r\nLoading from gcs: 2.7186670303344727\r\nLoading from local disk: 0.054453134536743164\r\nDownloading from gcs, then loading from local disk: 0.8785662651062012\r\nDownloading from gcs using cli, then loading from local disk: 3.097564935684204\r\n```", "Any updates on this?\r\n", "I think the issue is that you don't factor in the time spent on network operations. To me, it looks consistent", "> I think the issue is that you don't factor in the time spent on network operations. To me, it looks consistent\r\n\r\nI don't see how I am not factoring in these. If I adapt the script posted above to:\r\n```\r\nimport tensorflow as tf\r\nimport time\r\nfrom google.cloud import storage\r\nimport os\r\nimport subprocess\r\ntotal_time = time.time()\r\nmodel_names = ['test1', 'test2', 'test3', 'test4']\r\nfor model in model_names:\r\n        start_time = time.time()\r\n        tf.saved_model.load(f'gs://bucket-name/models/{model}/1')\r\n        elapsed_time = time.time() - start_time\r\n        print(f'Loading from gcs: {elapsed_time}')\r\ntotal_elapsed_time = time.time() - total_time\r\nprint(f\"Total elapsed time: {total_elapsed_time}\")\r\n```\r\nI get the following output:\r\n```\r\nLoading from gcs: 28.524829864501953\r\nLoading from gcs: 0.7831361293792725\r\nLoading from gcs: 52.6575129032135\r\nLoading from gcs: 3.1442267894744873\r\nTotal elapsed time: 85.57314991950989\r\n```\r\nAnd downloading them first and then loading:\r\n```\r\nimport tensorflow as tf\r\nimport time\r\nfrom google.cloud import storage\r\nimport os\r\nimport subprocess\r\ntotal_time = time.time()\r\nmodel_names = ['test1', 'test2', 'test3', 'test4']\r\n\r\nstorage_client = storage.Client()\r\nbucket = storage_client.get_bucket('bucket-name')\r\nfor model in model_names:\r\n        start_time = time.time()\r\n        blobs = bucket.list_blobs(prefix=f'models/{model}/1')  # Get list of files\r\n        folder = f'./download_folder'\r\n        for blob in blobs:\r\n                # create folder if not exist\r\n                blob.download_to_filename(folder + '/' + blob.name)\r\n\r\n        start_time_internal = time.time()\r\n        tf.saved_model.load(os.path.join(folder, 'models', model, '1' ))\r\n        elapsed_time_internal = time.time() - start_time_internal\r\n        print(f'Loading from local disk: {elapsed_time_internal}')\r\n\r\n        elapsed_time = time.time() - start_time\r\n        print(f'Downloading from gcs, then loading from local disk: {elapsed_time}')\r\n\r\n)\r\n\r\ntotal_elapsed_time = time.time() - total_time\r\nprint(f\"Total elapsed time: {total_elapsed_time}\")\r\n```\r\nGives following output\r\n```\r\nLoading from local disk: 2.6329727172851562\r\nDownloading from gcs, then loading from local disk: 4.476565837860107\r\nLoading from local disk: 0.025137901306152344\r\nDownloading from gcs, then loading from local disk: 0.4631459712982178\r\nLoading from local disk: 1.6898088455200195\r\nDownloading from gcs, then loading from local disk: 3.088888168334961\r\nLoading from local disk: 0.04868316650390625\r\nDownloading from gcs, then loading from local disk: 1.0312209129333496\r\nTotal elapsed time: 9.417081117630005\r\n```\r\nI timed everything now, so network operations are in here. I can execute both scripts multiple times, the execution times are always in line with these results.", "Thanks. I'll get a look into this, though not this week or the next (loaded with other work items).", "> Thanks. I'll get a look into this, though not this week or the next (loaded with other work items).\r\n\r\nThanks.", "As a month passed since the last comment, any updates on this?", "Sorry, but unfortunately due to COVID my work schedule became more chaotic. So it will still take some time. Apologies for this", "Having a same issue here, except that I'm loading a big SavedModel and it's extremely slow. Downloading the same model with gsutil is much faster so it rules out the problem of the network.", "The filesystem glob matching [implementation](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/file_system_helper.cc;l=123-266;drc=8b5b9dc96666a3a5d27fad7179ff215e3b74b67c) has recently changed. Let's test with the 2.4 release when it gets out to see if this is made better", "TF 2.4 has been released. Is this still an issue?", "Please reopen if this is still an issue in 2.4 release.", "Hi @mihaimaruseac - I can confirm this is still an issue in TF2.4\r\n\r\nThis is the code I used to test this issue:\r\n\r\n```\r\nimport sys\r\nimport time\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef main(argv):\r\n    print(tf.__version__)\r\n    tic = time.time()\r\n    tf.saved_model.load(argv[0])\r\n    toc = time.time()\r\n    print(f'Local load time: {toc - tic:.2f}s')\r\n    tic = time.time()\r\n    tf.saved_model.load(argv[1])\r\n    toc = time.time()\r\n    print(f'Remote load time: {toc - tic:.2f}s')\r\n\r\n\r\nif __name__ == '__main__':\r\n    main(sys.argv[1:])\r\n```\r\n\r\nI copied the SavedModel directory I'm loading from GCS to local on a Google Cloud Virtual Machine. The directory is around 170MB in size and copying takes less than 2 seconds.\r\n\r\nTest result on TF 2.2.2:\r\n\r\n```\r\nhao.hu@derekhh-dev /tmp\r\ntmp_py37 \u276f python temp.py /tmp/202102051600 gs://foo/202102051600/\r\n\r\n2.2.2\r\nLocal load time: 20.78s\r\nRemote load time: 173.86s\r\n```\r\n\r\nTest result on TF 2.3.2:\r\n\r\n```\r\nhao.hu@derekhh-dev /tmp\r\ntmp_py37 \u276f python temp.py /tmp/202102051600 gs://foo/202102051600/\r\n\r\n2.3.2\r\nLocal load time: 20.98s\r\nRemote load time: 180.84s\r\n```\r\n\r\nTest result on TF 2.4.1:\r\n\r\n```\r\nhao.hu@derekhh-dev /tmp\r\ntmp_py37 \u276f python temp.py /tmp/202102051600 gs://foo/202102051600/\r\n\r\n2.4.1\r\nLocal load time: 20.63s\r\nRemote load time: 195.49s\r\n```\r\n\r\n", "Thank you for confirming.", "@mihaimaruseac Thanks for investigating. I created a quick profile with TF 2.4 and most of the time is actually spent in the following function call mainly due to the execution of the `restore_v2` operation, so I am not sure if the root cause is actually related to the TF file system implementation:\r\nhttps://github.com/tensorflow/tensorflow/blob/7a1b65b61397dbad1d400e1d3c7079246dfe309a/tensorflow/python/saved_model/load.py#L161", "@mihaimaruseac did you have any chance to look into this issue?\r\n\r\nReloading models (even small ones) from GCS seems almost unusable even with a super fast network connection so unfortunately the only workaround currently is to download the saved model (which tends to be super fast) and read it from the local disk which can be quite cumbersome for users.", "Unfortunately, I haven't :( I'm working on the security issues at the moment and putting everything else on a lower priority track as there is not enough time to work on everything :("]}]