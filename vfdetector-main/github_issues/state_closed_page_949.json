[{"number": 24963, "title": "Build tensorflow.dll on Windows properly", "body": "1. To build a DLL for TensorFlow C API, you can now build\r\n   `//tensorflow:tensorflow.dll`\r\n\r\n2. The import library for tensorflow.dll can be built by\r\n   `//tensorflow:tensorflow_dll_import_lib`\r\n\r\n3. You can also get a compressed package for both (tensorflow.dll, tensorflow.lib) and C API headers by building\r\n   `//tensorflow/tools/lib_package:libtensorflow`\r\n\r\n3. Besides symbols for the official C API, symbols for\r\n   building custom ops are also exported.\r\n\r\n4. Also updated the libtensorflow_cpu.sh and libtensorflow_gpu.sh\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/24885", "comments": ["FYI @guikarist", "A similar example to use the `tensorflow.dll` as https://www.tensorflow.org/install/lang_c\r\n\r\nAssume you already have C API headers, the import library and the DLL for tensorflow in your local base in the following structure:\r\n```\r\n$ tree\r\n.\r\n\u251c\u2500\u2500 hello_tf.c\r\n\u251c\u2500\u2500 include\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tensorflow\r\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 c\r\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 c_api.h\r\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 eager\r\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 c_api.h\r\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 LICENSE\r\n\u2514\u2500\u2500 lib\r\n    \u251c\u2500\u2500 tensorflow.dll\r\n    \u2514\u2500\u2500 tensorflow.lib\r\n```\r\n\r\n**Example program (hello_tf.c)**:\r\n```\r\n#include <stdio.h>\r\n#include <tensorflow/c/c_api.h>\r\n\r\nint main() {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n  return 0;\r\n}\r\n```\r\n\r\n**Compile**\r\nYou need to include the C API headers by `/I .\\include` and link to the import library `.\\lib\\tensorflow.lib`\r\n```\r\nC:\\Users\\pcloudy\\Desktop\\lib_package>cl.exe hello_tf.c /I .\\include .\\lib\\tensorflow.lib\r\nMicrosoft (R) C/C++ Optimizing Compiler Version 19.00.24210 for x64\r\nCopyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\nhello_tf.c\r\nMicrosoft (R) Incremental Linker Version 14.00.24210.0\r\nCopyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\n/out:hello_tf.exe\r\nhello_tf.obj\r\n.\\lib\\tensorflow.lib\r\n```\r\n\r\n**Make `tensorflow.dll` available at runtime**\r\nThere are two ways to do this on Windows:\r\n1) Copy the `tensorflow.dll` to the binary directory\r\n```\r\nC:\\Users\\pcloudy\\Desktop\\lib_package>copy lib\\tensorflow.dll .\\\r\n        1 file(s) copied.\r\n```\r\n2) Add `tensorflow.dll`'s directory to PATH:\r\n```\r\nC:\\Users\\pcloudy\\Desktop\\lib_package>set PATH=.\\lib;%PATH%\r\n```\r\n\r\n**Run**\r\n```\r\nC:\\Users\\pcloudy\\Desktop\\lib_package>.\\hello_tf.exe\r\nHello from TensorFlow C library version 1.12.0\r\n\r\n```\r\n \r\n", "@asimshankar With this change you can update https://www.tensorflow.org/install/lang_c for Windows. Let me know if you need any help.", "@asimshankar I also applied the name pattern approach on `//tensorflow:libtensorflow_cc.so`, do you know what's the difference between `//tensorflow:libtensorflow.so` and this target?", "> @asimshankar I also applied the name pattern approach on `//tensorflow:libtensorflow_cc.so`, do you know what's the difference between `//tensorflow:libtensorflow.so` and this target?\r\n\r\nIt is said that `//tensorflow:libtensorflow.so` is for C API.", "@guikarist @asimshankar c++ tensorflow header contains session and some \r\nstandard ops, which seems missing from c API:\r\n\r\nhttps://www.tensorflow.org/guide/extend/cc\r\n\r\nFrom the past Cmake DLL generation it contains c++ stuffs, can you tell if this is in c api?", "@jackyko1991 Sorry, I am not familiar with C API.", "@meteorcloudy There is a [known `eigen` bug of building on `Windows Bazel GPU`](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2) (or in #19198). But it seems that nobody wants to solve it by applying patch. Will it affect your check process?", "@asimshankar I've rebased the branch, please take another look!", "Hi everybody,\r\nThank you very much for the great work !\r\n\r\n@meteorcloudy, I pulled your latest commits today and successfully built TensorFlow C and C++ libs by running `bazel build --config=opt //tensorflow:tensorflow.dll` and `bazel build --config=opt //tensorflow:tensorflow_cc.dll`. However, when I link my C++ project against them and try to build, VS raises `unresolved external symbols` errors. I dumped all the symbols in tensorflow_cc.dll and I got ~30k symbols, but they aren't covering the ones VS complains about (listed here under). Am I missing something in the building process ? I haven't been able to dig into the related issue (tensorflow/tensorflow#23542) and check the difference but I intend to do so soon.\r\n\r\nRegarding my setup, I'm using:\r\n* [meteorcloudy/tensorflow:tensorflow_dll](https://github.com/meteorcloudy/tensorflow/tree/tensorflow_dll)\r\n* Bazel 0.19.0\r\n* VS2015 x64 Native Tools Command Prompt\r\n* Python 3.6.7 virtual environment with the required packages\r\n* All environment variables are set according to the installation steps \r\n* Include folders:\r\n    * .../tensorflow/bazel-tensorflow/external/com_google_absl\r\n    * .../tensorflow/bazel-tensorflow/external/eigen_archive\r\n    * .../C:/Libs/tensorflow/bazel-tensorflow/external/protobuf_archive/src\r\n    * .../tensorflow/bazel-genfiles\r\n    * .../tensorflow\r\n\r\nLet me know if I can do/provide anything, I'll gladly help.\r\n\r\nProblematic symbols when I tried to build [inference_cc](https://github.com/PatWie/tensorflow-cmake/blob/master/inference/cc/inference_cc.cc):\r\n\r\n```\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &,class tensorflow::Session * *)\" (?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"class tensorflow::GraphDefDefaultTypeInternal tensorflow::_GraphDef_default_instance_\" (?_GraphDef_default_instance_@tensorflow@@3VGraphDefDefaultTypeInternal@1@A) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"class tensorflow::SaverDefDefaultTypeInternal tensorflow::_SaverDef_default_instance_\" (?_SaverDef_default_instance_@tensorflow@@3VSaverDefDefaultTypeInternal@1@A) referenced in function \"class tensorflow::Status __cdecl LoadModel(class tensorflow::Session *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >)\" (?LoadModel@@YA?AVStatus@tensorflow@@PEAVSession@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z)\r\n1>D:\\workspace\\tensorflow_cc\\build\\Release\\test.exe : fatal error LNK1120: 4 unresolved externals\r\n```\r\n\r\nProblematic symbols when I try to build the example on TensorFlow C++ API page:\r\n```\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"private: class tensorflow::Scope __cdecl tensorflow::Scope::WithOpNameImpl(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?WithOpNameImpl@Scope@tensorflow@@AEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ClientSession::ClientSession(class tensorflow::Scope const &)\" (??0ClientSession@tensorflow@@QEAA@AEBVScope@1@@Z) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ClientSession::~ClientSession(void)\" (??1ClientSession@tensorflow@@QEAA@XZ) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: class tensorflow::Status __cdecl tensorflow::ClientSession::Run(class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)const \" (?Run@ClientSession@tensorflow@@QEBA?AVStatus@2@AEBV?$vector@VOutput@tensorflow@@V?$allocator@VOutput@tensorflow@@@std@@@std@@PEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@5@@Z) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"class tensorflow::Output __cdecl tensorflow::ops::Const(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z) referenced in function main\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::MatMul::MatMul(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input,struct tensorflow::ops::MatMul::Attrs const &)\" (??0MatMul@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1AEBUAttrs@012@@Z) referenced in function main\r\n1>D:\\workspace\\tensorflow_cc\\build\\Release\\test.exe : fatal error LNK1120: 9 unresolved externals\r\n```\r\n", "@kdethoor It is obviously caused by not adding all necessary symbols to `tensorflow.dll`. I can compile these two C++ files successfully after I have added these symbols into [my patch](https://github.com/guikarist/tensorflow-windows-build-script/blob/master/patches/tf_exported_symbols_msvc.lds).", "@kdethoor BTW, what are the file extensions of the built external dependencies? And how to build with them (e.g. `grpc`) ?\r\n\r\nUpdate: I found them in `bazel-source`! So, I should just include these directories. Am I right?", "@guikarist Thanks for answering and checking ! \r\nI came to the same conclusion as you. I tried to dig into the changes brought by meteorcloudy but I haven't been able to get the def_file_filter to keep the symbols I need - either I get the same as the ones that are normally generated when I try to change the filtering rules, or I get way too many symbols when I add some dependencies such as //tensorflow::ops when they're generated... (I'm not fully familiar with the source of TF and Bazel yet to be honest). \r\nI haven't tried adding the missing symbols to your patch though, so I'll give it a look today.\r\n\r\nEdit: Sorry, I misunderstood which files you were referring to. For the extensions of the dependencies, I've got .libs in bazel-bin/externals (bazel-bin/third_party for icu). \r\nI honestly can't say for sure if you need to add those libs, but looking at how the C API works and, if my understanding is correct, considering that Bazel builds collects all the code that is needed to create the tensorflow libraries (both from internal and external dependencies), I would say that they aren't needed.", "@kdethoor I meant that I wanted to compile my project with Tensorflow and external dependencies built by Bazel. See [this comment](https://github.com/tensorflow/tensorflow/issues/24885#issuecomment-454829156) but I get symbol error when I only link `liblibtensorflow_cc.so.ifso` built with Tensorflow 1.12. If I link `bazel-bin\\external\\grpc\\libgrpc++.a`, the Visual Studio cannot open it. So I have no idea how to deal with them properly.", "@guikarist Thanks for the details. So, if you also want to use the external libs generated by Bazel, using meteorcloudy's latest commits, you will get .lib files (e.g. grpc++.lib), which VS should understand. I haven't tried to use them in a project though.\r\nOn the other hand, some symbols you require from Tensorflow might be missing as I mentioned earlier.\r\n\r\nBtw, I tried your script again, by adding the symbols I was searching for into the patch. All but one work: ?WithOpNameImpl@Scope@tensorflow@@AEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z. Actually, when I add that one to the patch, Bazel can't build successfully anymore.", "@kdethoor Did you try to modify [def_file_filter.py.tpl](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/def_file_filter.py.tpl#L48) to get more symbols exported? It might work.\r\n\r\nOn the other hand, I think you are only suppose to use the [C API](https://www.tensorflow.org/install/lang_c) with `//tensorflow:tensorflow.dll`, because it doesn't contain ops and some other dependencies. To use the TensorFlow [C++ AP](https://www.tensorflow.org/guide/extend/cc)I, it's better to include TensorFlow as an external dependency and build it with Bazel.", "@meteorcloudy Yes, but I would usually get way too many symbols, which would make the build crash.\r\nSince //tensorflow:tensorflow.dll is supposed to be used with C API, can //tensorflow_cc.dll be used with the C++ API (outside of Bazel) ? This is what I understood/inferred, sorry if I was wrong.\r\nAlso, if I include TensorFlow as an external library using Bazel, can I rely on the C++ API to write a library that I would use later on in another program ?\r\n\r\nThanks a lot for your help !", "Take a look at the legacy cmake shared library build https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_shared_lib.cmake\r\n\r\ntensorflow_cc.dll seems miss quite a lot of other targets if we want to run C++ tensorflow on windows properly\r\n\r\nIn cmake build grpc in windows was not supported", "> 1. To build a DLL for TensorFlow C++ API, you can now build\r\n>    `//tensorflow:tensorflow.dll`\r\n> 2. The import library for tensorflow.dll can be built by\r\n>    `//tensorflow:tensorflow_dll_import_lib`\r\n> 3. You can also get a compressed package for both (tensorflow.dll, tensorflow.lib) and C API headers by building\r\n>    `//tensorflow/tools/lib_package:libtensorflow`\r\n> 4. Besides symbols for the official C API, symbols for\r\n>    building custom ops are also exported.\r\n> 5. Also updated the libtensorflow_cpu.sh and libtensorflow_gpu.sh\r\n> \r\n> Fixes #24885\r\n\r\n@meteorcloudy Didn't you denote that `//tensorflow:tensorflow.dll` is for C++ API? Why did you advise @kdethoor to use it as C API?\r\n\r\nAnd on the other hand, most of projects are not built with Bazel. So there is no way to include Tensorflow as external library, otherwise they rewrite the project.", "@guikarist Oh, sorry! I wasn't having a clear understanding of the differences between C and C++ API. Just updated the PR description. Looks like they are quite different things.\r\n\r\n@asimshankar Do you know which target should be used as C++ API? Or there isn't one?", "@meteorcloudy It is said that `libtensorflow_cc.so` is for C++ API and `libtensorflow.so` is for C API.", "This is the guide I have used in the past:\r\nhttps://joe-antognini.github.io/machine-learning/build-windows-tf\r\n\r\nWhich makes use of contrib/cmake\r\n\r\nI am not moving onto TF 1.12 and CUDA 10 because clients need support for RTX cards so I am currently halfway through running\r\n\r\nbazel build --config=opt //tensorflow:libtensorflow_cc.so\r\n\r\nAfter configuring for CUDA 10.0 and CUDNN 7.4.2.24\r\n\r\nBut this will have the symbol missing problems as outlined here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/23542\r\n\r\nthe workaround:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/22047#issuecomment-421452033\r\n\r\nSeems a little bit hacky.\r\n\r\nOn Linux and Windows my product also uses libtensorflow_framework.so for loading models\r\n\r\nI might have to go back to using the the old cmake way.\r\n\r\nI really like building with bazel and doing things the supported way that tensorflow reccomends, but deploying tensorflow models and three operating systems should be a little easier than it is\r\n\r\nsee the product here\r\n\r\nhttps://kognat.com", "@gunan Can you help reimporting this change? cl/230909152 no longer exists.\r\n", "the PR is still pending after one month...hopefully this will be merged asap, I want to try if cmake is compatible with this dll build on windows", "@meteorcloudy  Could you rebase to avoid branch conflicts ?\r\n", "@hgadig Done, PTAL", "Merged at https://github.com/tensorflow/tensorflow/commit/0c8deb2f9178feef562320688198885b23ffd647", "Bazel: 0.19.0\r\nCUDA: 9.0\r\ncuDNN: 7.4\r\nBuild GPU version: yes\r\nBuild parameters: `bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/tools/lib_package:libtensorflow --verbose_failures`\r\n\r\nI have tried `//tensorflow/tools/lib_package:libtensorflow`, but get following errors:\r\n\r\n```\r\nERROR: C:/users/guikarist/_bazel_guikarist/ejkbgqyt/external/local_config_cc/BUILD:103:1: in cc_toolchain rule @local_config_cc//:cc-compiler-x64_windows: Error while selecting cc_toolchain: No toolchain found for cpu 'x64_windows'. Valid cpus from default_toolchain entries are: [\r\n]. Valid toolchains are: [\r\n  local_linux: --cpu='local' --compiler='compiler',\r\n  local_darwin: --cpu='darwin' --compiler='compiler',\r\n  local_windows: --cpu='x64_windows' --compiler='msvc-cl',\r\n]\r\nAnalyzing: target //tensorflow/tools/lib_package:libtensorflow (18 packages loaded, 79 targets configured)\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed; build aborted\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed; build aborted\r\nINFO: Elapsed time: 45.066s\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed; build aborted\r\nINFO: Elapsed time: 45.066s\r\nINFO: 0 processes.\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed; build aborted\r\nINFO: Elapsed time: 45.066s\r\nINFO: 0 processes.\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed; build aborted\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed; build aborted\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-x64_windows' failed; build aborted\r\nINFO: Elapsed time: 45.066s\r\nINFO: 0 processes.\r\n```", "@guikarist Can you try Bazel 0.20.0? The GPU build is green with this Bazel version.", "@meteorcloudy It succeeded. But is it just a C API, isn't it?", "@guikarist I think so", "@meteorcloudy Is there any issue or PR on C++ API?", "@guikarist I have make one PR for C++ https://github.com/tensorflow/tensorflow/pull/26152\r\n\r\nbut current C++ API seems a bit outdated, especially for header locations\r\n\r\nTo use C++ API seems you still need to include Eigen and some third party libraries for tf core stuff", "@meteorcloudy @jackyko1991 Thanks for you two's work.\r\n\r\n@jackyko1991 Will this PR solve symbol problems in C++ library?", "I have exported all .h in the PR, but still missing the protobuf generated\nheaders.\n\nfor symbolic link, you should try tensorflow_cc.lib if you are on windows\n\nGuikarist <notifications@github.com> \u65bc 2019\u5e743\u67085\u65e5\u9031\u4e8c \u4e0b\u53489:54\u5beb\u9053\uff1a\n\n> @meteorcloudy <https://github.com/meteorcloudy> @jackyko1991\n> <https://github.com/jackyko1991> Thanks for you two's work.\n>\n> @jackyko1991 <https://github.com/jackyko1991> Will this PR solve symbol\n> problems in C++ library?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24963#issuecomment-469685220>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALgaB-q_g184Kuojk7PzOmgmGpL02wnyks5vTncNgaJpZM4aDJNy>\n> .\n>\n"]}, {"number": 24962, "title": "Support model.fit using targets in a dictionary", "body": "This feature relates to the TensorFlow implementation of the Keras API, specifically relating to the use of `model.fit`\r\nSupporting this feature will improve the simplicity of the use case with TFRecord and multi-task problems (or multiple output streams)\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes (if my suggested implementation is sufficient)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently `model.fit` supports tensor/list/dict for the inputs `x`, but a dictionary cannot be used for the labels `y`.\r\n\r\n**Will this change the current api? How?**\r\nThe API will not change, and it will more closely represent the underlying Keras API https://keras.io/models/model/#fit\r\n\"y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays\"\r\n\r\n**Who will benefit with this feature?**\r\nUsers with multi-task learning problems (or problems with multiple output steams)\r\nImplementations using protos (tf.Example or tf.SequenceExample) and/or TFRecord\r\nImplementations using tf.data.Dataset\r\n\r\n**Any Other info.**\r\nAs per the handling of `inputs` in [`training_arrays._prepare_feed_values`](https://github.com/tensorflow/tensorflow/blob/aeddcb55fadc0ad9085d71d736ba49c6b69620bf/tensorflow/python/keras/engine/training_arrays.py#L389), we can change [line 429](https://github.com/tensorflow/tensorflow/blob/aeddcb55fadc0ad9085d71d736ba49c6b69620bf/tensorflow/python/keras/engine/training_arrays.py#L429) to the following\r\n```\r\ntargets = training_utils.ModelInputs(targets).as_list()\r\n```\r\n\r\nIf this solution is valid, it may be appropriate to modify [`training_utils.ModelInputs`](https://github.com/tensorflow/tensorflow/blob/aeddcb55fadc0ad9085d71d736ba49c6b69620bf/tensorflow/python/keras/engine/training_utils.py#L1306), either with an abstract `ModelTensors` class, or with a refactored `ModelInputs` class that does not use the word \"inputs\".", "comments": ["Once addressed this issue would allow this kind of code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(dict(x=inputs, y=outputs))\r\nmodel = tf.keras.Model()\r\nmodel.compile()\r\nmodel.fit(dataset)\r\n```\r\n\r\nCurrently, the above fails with:\r\n\r\n```\r\nValueError: Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: {'x': <tf.Tensor: id=3235, shape=(2, 1), dtype=float32, numpy=\r\narray([[6.],\r\n       [7.]], dtype=float32)>, 'y': <tf.Tensor: id=3236, shape=(2, 5), dtype=float32, numpy=\r\narray([[30., 31., 32., 33., 34.],\r\n       [35., 36., 37., 38., 39.]], dtype=float32)>}\r\n```", "There would be ambiguity in doing this, as it's perfectly valid for users to have inputs named `y`.\r\n\r\nInstead, `targets` should be the second element of a tuple when using datasets. I believe that element can be a dict", "So the only way to pass Dataset to `fit()` would be with a tuple of length 2. The first element will **always** be inputs and the second element will **always** be outputs (targets). Both can be Tensor objects or dict of tensors (or list of tensors)?\r\n\r\nIf it's already what TF 2.0 assume then I guess I am all good :-)", "I just tested the following on TF2.0:\r\n\r\n```python\r\ndef make_dataset(images, labels, batch_size=64, buffer_size=1024, shuffle=True):\r\n    inputs = dict(images=images)\r\n    outputs = dict(labels=labels)\r\n    dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\r\n    if shuffle:\r\n        dataset = dataset.shuffle(buffer_size=buffer_size)\r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.map(map_func=normalize_fn, num_parallel_calls=8)\r\n    dataset = dataset.batch(batch_size)\r\n    return dataset\r\n```\r\n\r\n(I use single Tensor input/output here but it's only for illustration)\r\n\r\nAnd after adding the correct names in my Keras model, everything works like a charm:\r\n\r\n```python\r\nmodel.add(tf.keras.Input(shape=(28, 28, 1), name='images'))\r\n# ...\r\nmodel.add(tf.keras.layers.Dense(10, activation='softmax', name='labels'))\r\n```", "(I guess you can close here)"]}, {"number": 24961, "title": "TF Keras layer_utils_test introducing test cases", "body": "Test case introducing for layer_utils.py file under keras package.", "comments": ["@fchollet \r\n\r\nPlease help to review the test case", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 24960, "title": "tf.dataset + tf.estimator  slow, starving CPU/GPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution  Linux centos\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.9.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cudatoolkit   8.0    and cudnn 7.1.3  \r\n- GPU model and memory:  Tesla M40\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI used the gpu to train the wide deep model with tf.dataset and tf.estimator .In the case that I have opened 4 gpus with os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\", but only one gpu utilization is 15%, the others are 0.Is it because the network structure of the deep network part cannot be parallel. So you can only use one gpu. Is there any other way to speed up the training of the model?\r\n**Describe the expected behavior**\r\n\r\n\r\n", "comments": ["@mathlf2015 Could you provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the issue. Thanks!\r\n", "thank you very much for your response. i read data from a csv file with 15488184 rows  and 20 columns.\r\nthis problem somewhat like somebody descirbe in this [issue](https://github.com/tensorflow/tensorflow/issues/13530) \r\n@Goorman,It seems that the wide deep model can't use multiple gpus on a single multi-gpu server, and the usage of gpu is very low, and the training takes too much time.\r\npart of code \uff1a\r\n```\r\ndef input_fn(filenames, num_epochs, batch_size=1):\r\n    def parse_csv(line):\r\n        print('Parsing', filenames)\r\n        columns = tf.decode_csv(line, record_defaults=CSV_COLUMN_DEFAULTS)\r\n        features = dict(zip(CSV_COLUMNS, columns))\r\n        labels = features.pop(LABEL_COLUMN)\r\n\r\n        return features, labels\r\n\r\n    # Extract lines from input files using the Dataset API.\r\n    dataset = tf.data.TextLineDataset(filenames) # can pass one filename or filename list\r\n\r\n    # multi-thread pre-process then prefetch\r\n    dataset = dataset.map(parse_csv, num_parallel_calls=50)\r\n    # dataset.map(lambda features, target: (process_features(features), target))\r\n\r\n    # We call repeat after shuffling, rather than before, to prevent separate epochs from blending together.\r\n    # Shuffle, repeat, and batch the examples.\r\n    if FLAGS.task_type == \"train\":\r\n        dataset = dataset.shuffle(500)\r\n    dataset = dataset.repeat(num_epochs)\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.prefetch(5)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    features, labels = iterator.get_next()\r\n    # features = process_features(features)\r\n\r\n    return features, labels\r\n```\r\nthe config \r\n```\r\nsession_config = tf.ConfigProto(log_device_placement=True,\r\n                                    inter_op_parallelism_threads=FLAGS.inter_op,\r\n                                    intra_op_parallelism_threads=FLAGS.intra_op\r\n                                    )\r\nconfig = tf.estimator.RunConfig().replace(session_config=session_config,\r\n            save_checkpoints_steps=FLAGS.log_steps, log_step_count_steps=FLAGS.log_steps, save_summary_steps=FLAGS.log_steps)\r\n    #bulid model\r\n```", "Could you say why you set ```device_count={'GPU': 0}``` in session_config?\r\n\r\nThis method prevents the Tensorflow Graph from using GPU.\r\nI think deleting that parameter and running the model would work.", "> Could you say why you set `device_count={'GPU': 0}` in session_config?\r\n> \r\n> This method prevents the Tensorflow Graph from using GPU.\r\n> I think deleting that parameter and running the model would work.\r\n\r\nthank you .and I have tried deleting and the results didn't improve.", "@mathlf2015 I have rewritten a year ago my DL pipeline in keras and it fixed all cpu starvation problems, training speed improved 10-fold. So my impression is that tf.data has some serious performance related bugs inside of it.\r\n\r\nSo use Keras or PyTorch, lol.", "@Goorman Could you describe how you fixed CPU starvation problem? This will immensely help entire community if that is not still implemented in Tensorflow. Thanks!", "> @mathlf2015 I have rewritten a year ago my DL pipeline in keras and it fixed all cpu starvation problems, training speed improved 10-fold. So my impression is that tf.data has some serious performance related bugs inside of it.\r\n> \r\n> So use Keras or PyTorch, lol.\r\n\r\n@Goorman Thank you very much for your answer. I have two questions. First, when I open 4 gpus, only one is in use, and secondly, the gpu used is at 25% utilisation. The utilization rate is very low. Can you use multiple gpus when using wide deep, have you encountered both of these problems? I really want to know how you solved it. Thank you", "Any updates on this?", "For these kind of performance issues, it is most productive if you poke at the [TensorBoard Profile](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) to see if can spot any bottlenecks.", "Same question.", "@mathlf2015,\r\n\r\nCan you try updating Tensorflow to latest stable version i.e `2.6.0` and let us know if the issue persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24960\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24960\">No</a>\n"]}, {"number": 24959, "title": "cannot open tensorflow as non-sudor", "body": ">>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nI can import tensorflow as sudo, but it fails when I import tensorflow as non-sudo.", "comments": ["@Adopteruf Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. Thanks!", "> @Adopteruf Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\nThanks!\r\nThe issues has been solved by adding cuda, cudnn path in ~/.bashrc for each user.", "@Adopteruf Thanks for posting solution also. I am closing this issue. Thanks!"]}, {"number": 24958, "title": "Why compiling tensorflow from source failed which every step was done successfully as in the documentation, in both Windows 10 and Ubuntu", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64Bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 0.18.0, 0.16. 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.4, 5.4\r\n- CUDA/cuDNN version: 10.0, 7.4.2\r\n- GPU model and memory: GTX 1080 16GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI have tried the steps on the official site, and many more tutorials, but it was unable to build successfully.\r\n\r\nevery time almost throw new kind of error. this is the last error:\r\n\r\n> 1 error detected in the compilation of \"C:/Users/adil/AppData/Local/Temp/nvcc_inter_files_tmp_dir/eye_functor_gpu.cu.cpp1.ii\".\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 171.890s, Critical Path: 58.48s\r\n> INFO: 698 processes: 698 local.\r\n> FAILED: Build did NOT complete successfully\r\n\r\neverything installed correctly. \r\n\r\nI think there are a lot of bugs with tensorflow which let me dislike this framework, very hard to compile and also very hard to get the environment ready, it relays on a lot of third party tools which leads sometimes the versions miss match, and also it is long time which Cuda 10 has been come out, but the tf still using 9.0, the same with visual studio 2015 which now it is a long time that people using 2017. Everything that is used in tf is very old, which some user must uninstall their new version and install the older one. Tensorflow must consider this problem to be compatible with new tools and systems.\r\n\r\nI am waiting when tf start to support cuda 10?\r\n", "comments": ["I guess the documentation needs to be updated. Try building version 1.11 with CUDA 10.", "@Bahramudin We appreciate your interest in using TF framework. TF 1.13.0-rc1 has released and it supports cuda 10. Please give it a try and let us know if you have any problems. Thanks!", "@ymodak TF 1.13 not released. When I try \r\n`pip install --upgrade tensorflow-gpu`\r\nIt is the output:\r\n\r\n> Requirement already up-to-date: tensorflow-gpu in c:\\users\\adil\\anaconda3\\envs\\tf\\lib\\site-packages (1.12.0)", "The official TF 1.13 is not released. However you can try,\r\n> pip install tensorflow-gpu==1.13.0-rc2", "@ymodak OK, then I will wait until it officially released. For now, I am using 1.12", "Closing this issue for now. Thanks!"]}, {"number": 24957, "title": "native tf and tf.keras optimizer and gradient calculation problem", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWin 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\npip install \r\n- TensorFlow version (use command below):\r\n1.6 and 1.10\r\n- Python version:\r\n3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n9.0/7.05\r\n- GPU model and memory:\r\nGTX 1080 Ti\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\ntrain model using tf and tf.keras\r\n**weights updates** and **gradients** are different\r\n\r\n**test result as following chart:**\r\n![gradients](https://user-images.githubusercontent.com/39117820/51234690-7a62b480-19a8-11e9-8f46-94b247d91273.JPG)\r\n\r\n\r\n**Describe the expected behavior**\r\nSince tf.keras using tf as backend operation, weights updates and gradient calculation should be identical.\r\n**Code to reproduce the issue**\r\n\r\n```\r\n###imports and default settings\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import keras\r\nimport numpy as np\r\nimport sys,os\r\nimport keras.backend as K\r\nslim = tf.contrib.slim\r\nimg_size=10\r\ndatas = np.array([np.ones((100)),np.zeros((100))])\r\nlabels = np.array([[1.0,0.0],[0.0,1.0]])\r\nlr = 1e-3\r\nbeta_1=0.9\r\nbeta_2=0.999\r\nepsilon=1e-08\r\nseed=10\r\nrelu_function = tf.nn.relu\r\n```\r\n```\r\n##keras model\r\n\r\ntf.reset_default_graph()\r\nwith tf.get_default_graph().as_default() as keras_graph:\r\n    with tf.Session(graph=keras_graph).as_default() as keras_sess:\r\n        init = keras.initializers.RandomNormal(seed=seed)\r\n        inputs = keras.layers.Input(shape=(100,))\r\n        x = keras.layers.Dense(units=2,kernel_initializer=init,bias_initializer=init)(inputs)\r\n        pred = keras.layers.Activation(activation='softmax')(x)\r\n        model = keras.models.Model(inputs,pred)\r\n        \r\n        #tf optimizer\r\n#         optimizer = tf.train.AdamOptimizer(lr,beta1=beta_1,beta2=beta_2,epsilon=epsilon)\r\n\r\n        ###keras optimizer\r\n        optimizer = keras.optimizers.Adam(lr=lr,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon)\r\n\r\n        #use differnet loss \r\n        model.compile(optimizer=optimizer,loss=keras.losses.categorical_crossentropy)\r\n\r\n        #get first cnn layer weights\r\n        keras_variable_name = [x.name for x in tf.trainable_variables()[:2]]\r\n        keras_each_layer_tensors = tf.trainable_variables()[:2]\r\n        keras_weights_list=[]\r\n        keras_gradients_list = []\r\n        ####gradient\r\n        #get gradient tensor\r\n        keras_gradients_tensor = model.optimizer.get_gradients(loss=model.loss(model.targets[0],model.outputs[0]),\r\n                                                        params=keras_each_layer_tensors)\r\n        \r\n        keras_sess = keras.backend.get_session()\r\n        #get layer weights before training\r\n        keras_weights_list.append(keras_sess.run(keras_each_layer_tensors))\r\n        #get gradients before training\r\n        keras_gradients_list.append(keras_sess.run(keras_gradients_tensor,feed_dict={model.inputs[0]:datas,model.targets[0]:labels}))\r\n        #### update weights\r\n        model.fit(x=datas,y=labels,epochs=1,verbose=2)\r\n        #get layer weights after 1 epoch training\r\n        keras_weights_list.append(keras_sess.run(keras_each_layer_tensors))\r\n        #get gradients after 1 epoch training\r\n        keras_gradients_list.append(keras_sess.run(keras_gradients_tensor,feed_dict={model.input:datas,model.targets[0]:labels}))\r\n\r\n        keras_weights_list = np.array(keras_weights_list)\r\n        keras_gradients_list = np.array(keras_gradients_list)\r\n        keras_weights_update = keras_weights_list[1]-keras_weights_list[0]\r\n  ```\r\n```\r\n###### tf model\r\ntf.reset_default_graph()\r\nwith tf.get_default_graph().as_default() as tf_graph:\r\n    with tf.Session(graph=tf_graph).as_default() as tf_sess:\r\n        initializer = tf.initializers.random_normal(seed=seed)\r\n        \r\n        ##placeholders\r\n        tf_input = tf.placeholder(tf.float32, [None, 100],\r\n                    name='input')\r\n        tf_label = tf.placeholder(tf.float32,[None,2],name='label')\r\n        tf_lr = tf.placeholder(tf.float32,[],name='lr')\r\n        #model\r\n        x = tf.layers.dense(inputs=tf_input,units=2,kernel_initializer=initializer,bias_initializer=initializer)\r\n        pred = tf.nn.softmax(x)\r\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_label,logits=x)\r\n#         loss = keras.losses.categorical_crossentropy(y_pred=pred,y_true=tf_label)\r\n        tf_loss = tf.reduce_mean(loss)\r\n#         tf_optimizer = tf.keras.optimizers.Adam(lr=lr)\r\n        tf_optimizer = tf.train.AdamOptimizer(learning_rate=lr,beta1=beta_1,beta2=beta_2,epsilon=epsilon,name='optimizer')\r\n        train = tf_optimizer.minimize(loss=tf_loss)\r\n        \r\n        #get tensors\r\n        tf_each_layer_tensors = tf.trainable_variables()\r\n        tf_variable_name = [x.name for x in tf.trainable_variables()]\r\n        #get gradient tensor\r\n#         tf_gradient_tensor  = tf.gradients(tf_loss,tf_each_layer_tensors)\r\n        tf_gradient_tensor  = keras.backend.gradients(tf_loss,tf_each_layer_tensors)\r\n        #init\r\n        tf_sess.run(tf.global_variables_initializer())\r\n        \r\n        tf_weights_list=[]\r\n        tf_gradients_list=[]\r\n        #get init weight before training\r\n        tf_weights_list.append(tf_sess.run(tf_each_layer_tensors))\r\n        \r\n        tf_gradients_list.append(tf_sess.run(tf_gradient_tensor,feed_dict={tf_label:labels,tf_input:datas}))\r\n\r\n        loss_value,_ = tf_sess.run([tf_loss,train],feed_dict={tf_input:datas,tf_label:labels})\r\n        print('loss:',loss_value)\r\n        \r\n        tf_weights_list.append(tf_sess.run(tf_each_layer_tensors))\r\n        \r\n        tf_gradients_list.append(tf_sess.run(tf_gradient_tensor,feed_dict={tf_label:labels,tf_input:datas}))\r\n        \r\n        tf_weights_list = np.array(tf_weights_list)\r\n        tf_gradients_list = np.array(tf_gradients_list)\r\n        \r\n        tf_weights_update = tf_weights_list[1]-tf_weights_list[0]\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24957\">No</a>\n", "@cwhuang119,\r\nSorry for the delayed response. As the **`Tensorflow 2.x version`** predominantly uses [**`tf.keras`**](https://www.tensorflow.org/api_docs/python/tf/keras), can you please confirm if this issue is still relevant? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24957\">No</a>\n", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. "]}, {"number": 24956, "title": "Cherry pick windows build fix and tflite test fixes.", "body": "", "comments": []}, {"number": 24955, "title": "fatal error LNK1120, LNK2019, LNK2001", "body": "\r\n**System information**\r\n- OS Platform and Distribution :    windows 10\r\n- TensorFlow installed from :          source\r\n- TensorFlow version:                     r1.12\r\n- Python version:                            3.5.4\r\n- Installed using :                            exe\r\n- Bazel version :                               0.15.0\r\n- CUDA/cuDNN version:                 10.0, 7.3\r\n- GPU model and memory:             gtx1080ti\r\n\r\nafter i change the patch from [Aleksandr Sokolovskii's blog](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2) for modify eigen's error.\r\nand add three lines code from this [#23655](https://github.com/tensorflow/tensorflow/issues/23655) issues of cielavenir's description ,\r\nlast ,i run \" bazel build --config=opt --config=cuda --config=monolithic //tensorflow:libtensorflow_cc.so\" ,at last,i get the below error:\r\n\r\nINFO: From Linking tensorflow/core/liblib_internal_impl.a:\r\nandroid_armv7a_cpu_utils_helper.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nrandom_distributions.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From Linking tensorflow/libtensorflow_framework.so:\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/liblibtensorflow_framework.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/liblibtensorflow_framework.so.exp\r\nlibcuda_platform.lo(cuda_dnn.o) : warning LNK4217: locally defined symbol ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z (public: class stream_executor::Stream & __cdecl stream_executor::Stream::ThenBlasGemm(enum stream_executor::blas::Transpose,enum stream_executor::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class stream_executor::DeviceMemory<float> const &,int,class stream_executor::DeviceMemory<float> const &,int,float,class stream_executor::DeviceMemory<float> *,int)) imported in function \"public: virtual bool __cdecl stream_executor::cuda::CudnnSupport::DoMatMul(class stream_executor::Stream *,class stream_executor::DeviceMemory<float> const &,class stream_executor::DeviceMemory<float> const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::DeviceMemory<float> *)\" (?DoMatMul@CudnnSupport@cuda@stream_executor@@UEAA_NPEAVStream@3@AEBV?$DeviceMemory@M@3@1AEBVBatchDescriptor@dnn@3@2PEAV53@@Z)\r\nERROR: D:/tf3/tensorflow-r1.12/tensorflow/cc/BUILD:456:1: Linking of rule '//tensorflow/cc:ops/math_ops_gen_cc' failed (Exit 1120): link.exe failed: error executing command\r\n  cd C:/users/swls/_bazel_swls/5dz6uozl/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python35/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python35/libs/python35.lib\r\n    SET TEMP=C:\\Users\\swls\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\swls\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/math_ops_gen_cc /SUBSYSTEM:CONSOLE -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/math_ops_gen_cc-2.params\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored\r\nlibmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)\" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z) referenced in function \"public: static class tensorflow::shape_inference::DimensionHandle __cdecl tensorflow::shape_inference::InferenceContext::DimKnownRank(class tensorflow::shape_inference::ShapeHandle,__int64)\" (?DimKnownRank@InferenceContext@shape_inference@tensorflow@@SA?AVDimensionHandle@23@VShapeHandle@23@_J@Z)\r\nlibcc_op_gen_main.a(cc_op_gen_main.o) : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)\" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z)\r\nlibcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)\" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z)\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)\" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z)\r\nlibmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol \"public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)\" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ) referenced in function \"public: void __cdecl tensorflow::internal::LogMessageFatal::`vbase destructor'(void)\" (??_DLogMessageFatal@internal@tensorflow@@QEAAXXZ)\r\nlibcc_op_gen_main.a(cc_op_gen_main.o) : error LNK2001: unresolved external symbol \"public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)\" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ)\r\nlibcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol \"public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)\" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ)\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2001: unresolved external symbol \"public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)\" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ)\r\nlibmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const *)\" (??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z) referenced in function \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * __cdecl tensorflow::internal::MakeCheckOpString<int,int>(int const &,int const &,char const *)\" (??$MakeCheckOpString@HH@internal@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBH0PEBD@Z)\r\nlibcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const *)\" (??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z)\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const *)\" (??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z)\r\nlibmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder(void)\" (??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ) referenced in function \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * __cdecl tensorflow::internal::MakeCheckOpString<int,int>(int const &,int const &,char const *)\" (??$MakeCheckOpString@HH@internal@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBH0PEBD@Z)\r\nlibcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder(void)\" (??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ)\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder(void)\" (??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ)\r\nlibmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol \"public: class std::basic_ostream<char,struct std::char_traits<char> > * __cdecl tensorflow::internal::CheckOpMessageBuilder::ForVar2(void)\" (?ForVar2@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ) referenced in function \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * __cdecl tensorflow::internal::MakeCheckOpString<int,int>(int const &,int const &,char const *)\" (??$MakeCheckOpString@HH@internal@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBH0PEBD@Z)\r\nlibcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol \"public: class std::basic_ostream<char,struct std::char_traits<char> > * __cdecl tensorflow::internal::CheckOpMessageBuilder::ForVar2(void)\" (?ForVar2@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ)\r\n......\r\n\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol \"bool __cdecl tensorflow::str_util::CUnescape(class absl::string_view,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)\" (?CUnescape@str_util@tensorflow@@YA_NVstring_view@absl@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z) referenced in function \"bool __cdecl tensorflow::ConvertLine(class absl::string_view,class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)\" (?ConvertLine@tensorflow@@YA_NVstring_view@absl@@AEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@5@@Z)\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol \"bool __cdecl tensorflow::str_util::ConsumePrefix(class absl::string_view *,class absl::string_view)\" (?ConsumePrefix@str_util@tensorflow@@YA_NPEAVstring_view@absl@@V34@@Z) referenced in function \"bool __cdecl tensorflow::ConsumeEquals(class absl::string_view *)\" (?ConsumeEquals@tensorflow@@YA_NPEAVstring_view@absl@@@Z)\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol \"bool __cdecl tensorflow::str_util::EndsWith(class absl::string_view,class absl::string_view)\" (?EndsWith@str_util@tensorflow@@YA_NVstring_view@absl@@0@Z) referenced in function \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::WordWrap(class absl::string_view,class absl::string_view,int)\" (?WordWrap@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@Vstring_view@absl@@0H@Z)\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol \"public: static bool __cdecl google::protobuf::TextFormat::ParseFromString(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class google::protobuf::Message *)\" (?ParseFromString@TextFormat@protobuf@google@@SA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVMessage@23@@Z) referenced in function \"public: class tensorflow::Status __cdecl tensorflow::ApiDefMap::LoadApiDef(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)\" (?LoadApiDef@ApiDefMap@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::ReadFileToString(class tensorflow::Env *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)\" (?ReadFileToString@tensorflow@@YA?AVStatus@1@PEAVEnv@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV45@@Z) referenced in function \"public: class tensorflow::Status __cdecl tensorflow::ApiDefMap::LoadFile(class tensorflow::Env *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)\" (?LoadFile@ApiDefMap@tensorflow@@QEAA?AVStatus@2@PEAVEnv@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\nlibop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol \"class tensorflow::OpDef_AttrDefDefaultTypeInternal tensorflow::_OpDef_AttrDef_default_instance_\" (?_OpDef_AttrDef_default_instance_@tensorflow@@3VOpDef_AttrDefDefaultTypeInternal@1@A) referenced in function \"void __cdecl tensorflow::`anonymous namespace'::InitApiDefFromOpDef(class tensorflow::OpDef const &,class tensorflow::ApiDef *)\" (?InitApiDefFromOpDef@?A0x86251cb0@tensorflow@@YAXAEBVOpDef@2@PEAVApiDef@2@@Z)\r\nbazel-out/x64_windows-opt/bin/tensorflow/cc/ops/math_ops_gen_cc : fatal error LNK1120: 119 unresolved externals\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 2672.865s, Critical Path: 259.43s\r\nINFO: 3296 processes: 3296 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nthere is \"fatal error LNK1120: 119 unresolved externals\" errors,i think it must missing something,,,\r\nis there anyone can help me? any response will appreciate!\r\n", "comments": ["does there any one can help me?", "@jesen8 Apologies for the delay in response. (Correct me if I am wrong) I see that you are trying to install tf-gpu version with cuda 10. In that case you should try installing TF 1.13.0-rc0 as it comes with pre-built binaries for cuda 10 and you don't have to install from source to get cuda 10 working. Let me know if that helps. Thanks!\r\n", "@ymodak ,thanks,i will take a try and will response you", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24954, "title": "AttributeError: module 'tensorflow' has no attribute 'keras'(Tensorflow 1.4.0 has problem with new Model directory)", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): not custom code / object_detection\r\n\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04**\r\n\r\n**TensorFlow installed from (source or binary): binary**\r\n\r\n**TensorFlow version (use command below): 1.4.0**\r\n\r\n**Python version: 3.5**\r\n\r\n**CUDA/cuDNN version: 8.0/6.0**\r\n\r\nArchitecture        :   x86_64\r\nCPU op-mode(s):   32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s)               :                6\r\nOn-line CPU(s) list:   0-5\r\nThread(s) per core:    1\r\nCore(s) per socket:    6\r\nSocket(s)              :    1\r\nNUMA node(s)     :    1\r\nVendor ID            :  GenuineIntel\r\nCPU family           :     6\r\nModel:                 63\r\nModel name:            Intel(R) Xeon(R) CPU E5-2603 v3 @ 1.60GHz\r\nStepping              :              2\r\nCPU MHz            :               1596.347\r\nCPU max MHz    :           1600.0000\r\nCPU min MHz     :           1200.0000\r\nBogoMIPS          :              3192.69\r\nVirtualization      :        VT-x \r\nL1d cache          :             32K\r\nL1i cache           :             32K\r\nL2 cache            :              256K\r\nL3 cache            :              15360K\r\nNUMA node0 CPU(s):     0-5\r\n\r\nRunning `nvidia-smi` command :\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro K2200        Off  | 00000000:02:00.0  On |                  N/A |\r\n| 42%   43C    P8     1W /  39W |    309MiB /  4040MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       967      G   /usr/lib/xorg/Xorg                           178MiB |\r\n|    0      1829      G   compiz                                       127MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\n\r\nExact command to reproduce:\r\npython object_detection/builders/model_builder_test.py\r\nWhen i run the test program with tensorflow 1.4 i get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 23, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/home/models/research/object_detection/builders/model_builder.py\", line 22, in <module>\r\n    from object_detection.builders import box_predictor_builder\r\n  File \"/home/models/research/object_detection/builders/box_predictor_builder.py\", line 21, in <module>\r\n    from object_detection.predictors import convolutional_box_predictor\r\n  File \"/home/models/research/object_detection/predictors/convolutional_box_predictor.py\", line 19, in <module>\r\n    from object_detection.core import box_predictor\r\n  File \"/home/models/research/object_detection/core/box_predictor.py\", line 137, in <module>\r\n    class KerasBoxPredictor(tf.keras.Model):\r\nAttributeError: module 'tensorflow.python.keras' has no attribute 'Model'\r\n```\r\n\r\nThe solution i found when i searched was to update the tensorflow version.\r\n\r\nBut when I update tensorflow to the latest version(1.12) i get another error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/i.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 20, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/.virtualenvs/basic/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n```\r\n\r\nError seems like the higher tensorflow version is expecting CUDA 9.0.\r\n\r\nSo my inference is like the new tensorflow version doesnot support CUDA 8.0.\r\nDowngrading the tensorflow version has problems with the new Models problem. \r\n\r\nWhat could be the solution?\r\nIs this because  the CUDA runtime version is 9.0 (driver version 384.130) and the installed CUDA version is 8.0 and updating the CUDA version is the only solution?\r\n\r\n\r\nNB: Is there any necessity for GPUs used in distributed training to have the same CUDA, tensorflow version?\r\n", "comments": ["You are right you need to update to cuda 9.0 since you have switched to TF 1.12", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24953, "title": "Multiple parameter servers are not sharing the load when running TensorFlow distributed", "body": "This is a performance issue question. \r\nI have one, two, and four parameter servers experiments to test performance.\r\nIn the two and four parameter server networks utilization, only one parameter server has a highly utilized network and the rest is having low utilization. \r\nFor example:\r\nin 4 parameter server tasks with 4 worker tasks. Each task is on one machine.\r\n\r\nAfter running the experiments, I collected the network utlization:\r\n\r\nfirst ps == 693.7K\r\nsecond ps == 93.7 M\r\nthird ps == 15.5 M\r\nfourth ps == 4.1\r\nall four workers have 28.5 M utilization of the network. \r\nif we take 28.5 * 4 = 114 M. \r\nThen 114 M = (93.7+15.5+4.1+693.7K).\r\n\r\n**System information**\r\n- I wrote my own example with MNIST dataset and fully connected layers\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Pip3 installed \r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 3.6\r\n- CPU\r\n- Bandwidth is 108 M for every connection. \r\n\r\nMy question here is why ps did not share tensor size equally. \r\n\r\nThis is the part of my code that relates to this question. In Tensorflow, there is a Round-Robin fashion for tf.Variables(...) to place variables on parameter servers.\r\n\r\n\r\nwith tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % (FLAGS.task_index),cluster=cluster)):\r\n\r\nI am using MNIST dataset with minibatch 100. In my case, I will run 600 iterations to go through all dataset( 60000 samples in the data / 100 ) = 600 times. That means the worker will push and pull the model from each parameter server 600 times. \r\n\r\n      with tf.name_scope('input'):\r\n        X = tf.placeholder(dtype=tf.float32,shape=[None, 784])\r\n        Y = tf.placeholder(dtype=tf.float32,shape=[None, 10])\r\n        keep_prob = tf.placeholder(tf.float32, name = 'keep_prob') # dropout (keep probability)\r\n\r\n      with tf.name_scope(\"weights\"):\r\n        W1 = tf.Variable(tf.truncated_normal([784, 256], stddev=0.1)) \r\n        W2 = tf.Variable(tf.truncated_normal([256, 128], stddev=0.1))\r\n        W3 = tf.Variable(tf.truncated_normal([128, 64], stddev=0.1)) \r\n        W4 = tf.Variable(tf.truncated_normal([64, 10], stddev=0.1)) \r\n\r\n      with tf.name_scope(\"biases\"):\r\n        b1 = tf.Variable(tf.zeros([256]))\r\n        b2 = tf.Variable(tf.zeros([128]))\r\n        b3 = tf.Variable(tf.zeros([64]))\r\n        b4 = tf.Variable(tf.zeros([10]))\r\n\r\n      with tf.name_scope(\"softmax\"):\r\n        XX = tf.reshape(X, [-1, 784])\r\n        z2 = tf.nn.sigmoid(tf.matmul(XX, W1) + b1)\r\n        z3 = tf.nn.sigmoid(tf.matmul(z2, W2) + b2)\r\n        z4 = tf.nn.sigmoid(tf.matmul(z3, W3) + b3)\r\n        z5 = tf.matmul(z4,W4) + b4\r\n      \r\n      # define the loss function ...\r\n      with tf.name_scope('cross_entropy'):\r\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=z5, labels=Y))\r\n\r\n      # create an optimizer then wrap it with SynceReplicasOptimizer\r\n      with tf.name_scope(\"train\"):\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\r\n\r\n        # global_step tells the graph where it is in training\r\n        global_step = tf.train.get_or_create_global_step()\r\n        \r\n        #Synchronization\r\n        optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers, total_num_replicas=num_workers)\r\n        \r\n        # averages gradients\r\n        opt = optimizer.minimize(loss, global_step=global_step)\r\n\r\n\r\n\r\n", "comments": ["> Round-Robin fashion for Variable to place variables on parameter servers\r\n\r\nAs tensors are of different size, this does not always mean perfect load balancing PS which divides the number of bytes equally. You could check out two alternatives:\r\n\r\n1. Bin packing of tensors\r\n2. Partitioned variables\r\n\r\nLet me know if you need more details than that.\r\n", "+1 to @byronyi 's suggestion. For a quick workaround, you can also try odd number of parameter servers.", "I think @byronyi has captured the explanation nicely: in this particular case, each of the four weight matrices will be placed on a different PS, and the imbalance in their sizes directly corresponds to the imbalance in network traffic.\r\n\r\nIn terms of better APIs for this problem, you might be interested in the [partitioned variable RFC](https://github.com/tensorflow/community/pull/55) that @yuefengz wrote, which will give a way to divide large variables across multiple PSs (primarily for embeddings, but you could make it work for weights like these as well)."]}, {"number": 24952, "title": "[Intel MKL] enable depthwise convolution backward side code", "body": "", "comments": ["@penpornk seem these there UT failures are not related to my code and also failed in others PR, can you help to check it?", "I agree they are not related. Tagging this ready-to-pull now.", "@penpornk thank you very much!"]}, {"number": 24951, "title": "embedding_lookup return a SparseTensor", "body": "Neither embedding_lookup nor embedding_lookup_sparse can return a sparse tensor. Could you add a feature to support SparseTensor returned?\r\n\r\nI don't mean just convert the dense to sparse. It's **sparse lookup**.", "comments": ["@1049451037 Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Hi @jvishnuvardhan , it's a recent work about graph convolutional networks. https://arxiv.org/abs/1811.04441\r\n\r\nThe idea is that we need a sparse matrix that represents the adjacency matrix of the graph. However, each non-zero element of the matrix is not a constant, but a variable, which we want to lookup from a tensor.", "@ebrevdo @martinwicke who owns embedding ops?", "I'm trying to understand the use case.  the whole point of embedding_lookup is to take a sparse tensor, and get a dense tensor back.  embedding_lookup_sparse refers to the sparsity of the input arguments, not the outputs (those are still embeddings...dense tensors...)", "Yes, the feature I request here is taking dense in and giving sparse back, which is different from both embedding_lookup and embedding_lookup_sparse.", "I don't think that's an embedding lookup. Do you have a paper or citation,\nor reference code for what you want to do?\n\nOn Thu, Jan 24, 2019, 10:50 PM Qingsong Lv <notifications@github.com> wrote:\n\n> Yes, the feature I request here is taking dense in and giving sparse back.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24951#issuecomment-457473615>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim34MC80fAq7xo4n_VlbJ2I4siE1-ks5vGqk8gaJpZM4aCWYo>\n> .\n>\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24950, "title": "Typo error correction context.py", "body": "", "comments": []}, {"number": 24949, "title": "Tensorflow random categorical", "body": "Hi,\r\n\r\nI'm running \r\nhttps://www.tensorflow.org/tutorials/sequences/text_generation\r\n\r\nWhen I arrive at the line the following error is produced. Is there an import missing?\r\nsampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-23-60a341594c52> in <module>\r\n----> 1 sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n      2 sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\r\n\r\nAttributeError: module 'tensorflow._api.v1.random' has no attribute 'categorical'\r\n\r\n**System information**\r\n- TensorFlow version: 1.12 Jupyter NoteBooks on Ubuntu\r\n- Doc Link:\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["I would recommend you to install tf-nightly to execute the tutorial locally. You can also try running the tutorial in [google colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/sequences/text_generation.ipynb) to avoid new tf installation. The tutorial works as expected in the google colab thus should not have issues in nightly version as well.", "I also encounter this issue, so I use:\r\nsampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1)\r\nto bypass the problem instead of categorical.", "I had asked on stackoverflow. https://stackoverflow.com/questions/54210128/tensorflow-text-generation/54210177#54210177 Multinomial was renamed to categorical in the newer builds of tensorflow (ergo need to tweak that answer?)?", "> I also encounter this issue, so I use:\r\n> sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1)\r\n> to bypass the problem instead of categorical.\r\n\r\nThis worked for me (but i don't know for sure the difference between the two functions..If there're any..).\r\n\r\nThank You!", "[tf.random.multinomial](https://github.com/tensorflow/tensorflow/blob/9f30ec59fda6e9134dff2c7d9cc42b3fb455f8b7/tensorflow/python/ops/random_ops.py#L330) will be deprecated starting from TF 1.13rc0\r\nClosing this issue since the work around will be to use tf.random.multinomial if using TF 1.12 or lower. Thanks!"]}, {"number": 24948, "title": "Linking of rule '//tensorflow/contrib/nccl:python/ops/_nccl_ops.so' failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: macOS 10.13.5\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.15.2\r\n- GCC/Compiler version (if compiling from source): XCODE 8.3.2\r\n- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.3.2 / NCCL 2.3.5\r\n- GPU model and memory: GTX 1080 Ti 11GB\r\n\r\nTrying to build tensorflow 1.11 from source and the following error occurred:\r\n\r\ntensorflow/tensorflow/contrib/nccl/BUILD:24:1: Linking of rule '//tensorflow/contrib/nccl:python/ops/_nccl_ops.so' failed (Exit 1)\r\nld: library not found for -l:libnccl.so.2.3.5\r\n\r\nThanks a lot\r\n\r\n\r\n", "comments": ["Could you try the latest tensorflow, i.e. master branch?", "> Could you try the latest tensorflow, i.e. master branch?\r\n\r\nWith latest version new errors emerged:\r\nexternal/curl/lib/vtls/darwinssl.c:848:10: error: use of undeclared identifier 'TLS_AES_128_GCM_SHA256'\r\n    case TLS_AES_128_GCM_SHA256:\r\n         ^\r\nexternal/curl/lib/vtls/darwinssl.c:851:10: error: use of undeclared identifier 'TLS_AES_256_GCM_SHA384'\r\n    case TLS_AES_256_GCM_SHA384:\r\n         ^\r\nexternal/curl/lib/vtls/darwinssl.c:854:10: error: use of undeclared identifier 'TLS_CHACHA20_POLY1305_SHA256'\r\n    case TLS_CHACHA20_POLY1305_SHA256:\r\n         ^\r\nexternal/curl/lib/vtls/darwinssl.c:857:10: error: use of undeclared identifier 'TLS_AES_128_CCM_SHA256'\r\n    case TLS_AES_128_CCM_SHA256:\r\n         ^\r\nexternal/curl/lib/vtls/darwinssl.c:860:10: error: use of undeclared identifier 'TLS_AES_128_CCM_8_SHA256'\r\n    case TLS_AES_128_CCM_8_SHA256:\r\n         ^\r\nexternal/curl/lib/vtls/darwinssl.c:863:10: error: use of undeclared identifier 'TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256'\r\n    case TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256:\r\n         ^\r\nexternal/curl/lib/vtls/darwinssl.c:866:10: error: use of undeclared identifier 'TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256'\r\n    case TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256:\r\n", "Did you bazel clean before building on master?", "> Did you bazel clean before building on master?\r\n\r\nYes, bazel clean --expunge before every build.\r\nDowngrade from Xcode 9.4 to 9.0 successfully eliminated the 'darwinssl' error but still failed on NCCL.\r\n\r\nERROR: undeclared inclusion(s) in rule '//tensorflow/core/kernels:nccl_kernels':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/nccl_ops.cc':\r\n  'third_party/nccl/nccl.h'\r\ntensorflow/core/kernels/nccl_ops.cc:189:15: warning: private field 'reduction_op_' is not used [-Wunused-private-field]\r\n  ncclRedOp_t reduction_op_;\r\n              ^\r\n1 warning generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build", "Sorry, I did not notice that you were on macOS. NCCL is specific to CUDA, and I do not believe that TF supports macOS+CUDA officially.", "> Sorry, I did not notice that you were on macOS. NCCL is specific to CUDA, and I do not believe that TF supports macOS+CUDA officially.\r\n\r\nThanks.\r\n\r\nFollowed some guides and now I'm stuck at \r\nSymbol not found: _ncclAllReduce\r\n\r\nTrying to find a solution.", "Using 1.12 build from https://github.com/TomHeaven/tensorflow-osx-build. Seems to work now.\r\nClosing"]}, {"number": 24947, "title": "fix spell mistake", "body": "", "comments": []}, {"number": 24946, "title": "tensorflow/third_party/toolchains/gpus/cuda/BUILD", "body": "tensorflow/third_party/toolchains/gpus/cuda/BUILD file contains hard coded paths which would not necessarily true for all systems. Anyway I thought these files were generated through repository gen rules in `third_party/gpus/cuda`. Isn't it the case anymore?", "comments": ["@r4nt are these the files we added for new GPU builds we are testing?", "@chsigg This is the issue isas suspecting you may know something about. ", "I think we should be able to get rid of those again, everything we do for remote configs is under the preconfig directory now, and I believe all uses are switched. Once I'm finished with the fires we have I plan having a large clean-up session and hunting down everything we can delete, but I'll want to make sure I don't find any left-over unconverted use cases :)", "@r4nt how are the fires?", "Thanks for pinging - there are still a couple of things on my stack, but this one is small enough to drop it in, code review coming your way.", "This has landed in 2e846b14e15d9bf04892e935e293dad1f2ec037a."]}, {"number": 24945, "title": "How to eliminate duplicate codes for shape inference", "body": "**System information**\r\n- TensorFlow version (you are using): tf 2.0-preview\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen we are looking into issue: https://github.com/tensorflow/addons/issues/12, as @jaingaurav said,  SetShapeFn only works in graph constructing stage. So for eager mode, we have to add the duplicate logic (codes) in the `Compute` method of kernel. \r\n\r\nMy question is, can we find a solution to get rid of the duplicate codes for shape inference? Say, SetShapeFn also works in eager mode?\r\n\r\ncc @jaingaurav @karmel @seanpmorgan ", "comments": ["@facaiy: The understanding is that kernels should be written in an resilient manner such that they have the appropriate checks and don't crash on bad input. We could have chosen to run the shape function in eager mode. However, this would incur an unnecessary overhead. It is a bit unfortunate this results in a bit of code duplication at the moment.", "I agree that shape checking can incur an unnecessary overhead. Thanks for your explanation. I understand. Perhaps we can address the problem in the future."]}, {"number": 24944, "title": "Fix to restore desired blocks-per-core behavior to XLA", "body": "-Fixes unintentional threads per block change introduced in PR #21958\r\n-Calls CUDA occupancy calculator function on dummy PTX kernel so the maximum number of blocks per core is returned.  (A dummy kernel is used here as a workaround because we don't have access to the actual kernel at this point, and the parameters to the occupancy function result in the max blocks per SM as the return value)", "comments": ["@jlebar Here is a proposed fix to restore previous behavior", "@jlebar Friendly ping, just let me know if there's something I should do for this PR", "I was really confused by your ping, because I had a long review all written up...\r\n\r\nTurns out I never clicked \"submit\".  Really sorry about that.  And in trying to figure out why you weren't seeing what I was seeing, I accidentally deleted my review!\r\n\r\nLet me try to get the state back.", "@pragyaak  Please help proceeding with the next steps as I've some access issues(I'm trying to resolve).", "I'm trying to submit this, but I'm hitting memory leaks detected by our leakchecker.  I *think* those would have shown up in our CI if we'd looked closely.\r\n\r\nIt seems like the leak is coming from libcudart.  :(  But I need to look more closely.", "OK, the leaks were a known problem, just not properly annotated.  So that's good.\r\n\r\nIn debugging that I discovered some other bugs, namely that we were returning -1 here but storing it in a uint64.  So later when we checked within XLA that the value returned is nonnegative...that didn't do anything.\r\n\r\nLooking better now, I'm rerunning tests and want to run some benchmarks just to double-check.", "Excitingly, this change improves some benchmarks and regresses some others.\r\n\r\nI think my belief that this choice doesn't make a big difference in practice is broadly true -- on Pascal.  On Volta I'm seeing up to a 20% difference on macrobenchmarks between block size 64 and block size 1024.\r\n\r\nSorry for the delay getting this in.  I'm actively working on it...", "Interesting, and good catch with the uint.  No worries about the delay, thanks for working on it, and let me know if I can help"]}, {"number": 24943, "title": "Refer to cuda-bin genrule in crosstool file to instantiate cuda binary links", "body": "This PR adds reference to cuda-bin in crosstool so that softlink for the cuda binaries are instantiated for compilation. These links were broken by a recent merge leading compilation failures for cuda builds.", "comments": ["@martinwicke @gunan Pinging for your attention. This should also fix the failing CI tests.", "Closing this since it apparently didn't solve it. I will open a new one once it is fixed."]}, {"number": 24942, "title": "[CUDA] Fix invalid nvcc configuration", "body": "Commit https://github.com/tensorflow/tensorflow/commit/5bb89f16124ebbcff38a7bd6ec404cfa54ec9b91#diff-15230e5dad94ffaefa7b526c0c1e89e5 fixates nvcc to a certain location which is invalid for various build targets. Restore to original logic.", "comments": ["Analysis of 4 failed build targets:\r\n\r\n- MacOS Contrib : 2 failed unit tests are irrelevant to this PR\r\n- Ubuntu Python3 PIP : 2 failed unit tests are irrelevant to this PR\r\n- Windows Bazel : failed unit tests don't seem to be relevant to this PR\r\n- XLA : failed unit tests seem to possess syntax errors in themselves, not relevant to this PR\r\n\r\nThe most important goal for this PR is to fix certain build targets claiming \"cuda/bin/nvcc\" can't be found. Reviewing the codes it seems the commit mentioned in the description failed to use \"cuda_config.cuda_toolkit_path\" to form the proper path to nvcc.", "@whchung I agree with your analysis. I've also seen some other PRs' checks failed because `nvcc` couldn't be found today. The revert looks good to me. I just wanted to confirm with @chsigg if this aligns with what he has in mind for CUDA-related builds. He is in Europe which is at nighttime right now, so it will probably take a while before he sees this PR."]}, {"number": 24941, "title": "Increase the tolerance value for quantization_mnist_test.", "body": "This fixes quantization_mnist_test in GPU+TensorRT build.\r\n\r\nPiperOrigin-RevId: 228980452", "comments": []}, {"number": 24940, "title": "NNAPI doesn't support tensors with rank 0 - Mobilenet", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10 for retraining, Linux Ubuntu 16.04 for converting to tflite model\r\n- Mobile device: Sony Xperia Z1 - Android 9 (AICP ROM)\r\n- TensorFlow installed from : binary\r\n- TensorFlow version: b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nI retrained \"https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/quantops/classification/1\" using this script: https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py.\r\n\r\nThen I converted the output to tflite format using the following command:\r\n```\r\ntflite_convert \\\r\n  --output_file=graph.tflite \\\r\n  --graph_def_file=retrained_graph.pb \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n --inference_input_type=QUANTIZED_UINT8 \\\r\n  --input_arrays=Placeholder \\\r\n  --output_arrays=final_result \\\r\n  --input_shapes=1,160,160,3 \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=127 \r\n```\r\nI moved graph.tflite (the output of the above command) to assets folder of this project: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo\r\n\r\nI did all the necessary changes (such as changing the graph name, input size, etc.) in the ImageClassifierQuantizedMobileNet class.\r\n\r\nEverything is right as long as CPU is selected from the device list, but as soon as I select NNAPI, the app crashes.\r\n\r\nI think the problem is from the pre-trained models that are available on tfhub. Since when I convert the stock mobilenet_v1_1.0_224.pb from \"http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz \"\r\nthe problem is gone.\r\n\r\nLog:\r\n```\r\n1975-04-19 08:19:20.212 14862-15282/android.example.com.tflitecamerademo E/tflite: NNAPI doesn't support tensors with rank 0 (index 91 name module_apply_default/hub_input/Mul/y)\r\n1975-04-19 08:19:20.212 14862-15282/android.example.com.tflitecamerademo E/tflite: Returning error since TFLite returned failure nnapi_delegate.cc:742.\r\n1975-04-19 08:19:20.212 14862-15282/android.example.com.tflitecamerademo E/tflite: Failed to build graph for NNAPI\r\n    \r\n    --------- beginning of crash\r\n1975-04-19 08:19:20.217 14862-15282/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 14862\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:140)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:195)\r\n        at com.example.android.tflitecamerademo.ImageClassifierQuantizedMobileNet.runInference(ImageClassifierQuantizedMobileNet.java:95)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:125)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:804)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.access$1200(Camera2BasicFragment.java:78)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment$8.run(Camera2BasicFragment.java:699)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n```\r\n", "comments": ["related #24676", "The problem is solved using @impjdi solution in #25131", "@ramtin2080 Could you explain shortly what operation caused the issue in your case and how you fixed it?\r\nI read the #25131, however I don't use any normalization (despite BatchNorm) and still get an issue.", "@wosiu In this case the problem was from module_apply_default/hub_input/Mul/y and module_apply_default/hub_input/Sub/y , which are for normalizing the image. these ops have an empty dimension and NNAPI & GPU delegate currently do not support support dynamic tensor sizes. So in my case, we chop off these ops which cause the failure and do the normalization with Java inside the Android app. \r\nI think an alternative way is to specify the tensor dimensions for the ops."]}, {"number": 24939, "title": "Cherrypick 20190115 (fixes some tflite buidls)", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 24938, "title": "tf.keras.backend.zeros implementation ends up tracking tensors as well in graph mode ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.6\r\n\r\n**Describe the current behavior**\r\n\r\nHere is my custom layer:\r\n\r\n```bash\r\nclass ReshapeLayer(Layer):\r\n    def __init__(self, **kwargs):\r\n        super(ReshapeLayer, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        assert len(input_shape) >= 2\r\n        super(ReshapeLayer, self).build(input_shape)\r\n\r\n    def call(self, x):\r\n        s = K.shape(x)\r\n        # zeros_w = K.zeros((s[0], 1, s[2], s[3]), tf.float32) # does not work\r\n        zeros_w = tf.zeros((s[0], 1, s[2], s[3]), tf.float32)\r\n        r = K.concatenate([x, zeros_w], 1)\r\n\r\n        s = K.shape(r)\r\n       #  zeros_h = K.zeros((s[0], s[1], 1, s[3]), tf.float32)  # does not work\r\n        zeros_h = tf.zeros((s[0], s[1], 1, s[3]), tf.float32)\r\n        r = K.concatenate([r, zeros_h], 2)\r\n        return r\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        shape = tf.TensorShape(input_shape).as_list()\r\n        shape[1] = shape[1] + 1\r\n        shape[2] = shape[2] + 1\r\n        return tf.TensorShape(shape)\r\n```\r\n\r\nPlease note the commented lines i.e. K.zeros vs tf.zeros\r\n\r\nIn graph mode, if I use K.zeros even though the graph gets built, later on I get an exception with long stack trace (probably because this layer gets used many times in my network) that Tensor object does not have is_initialized property\r\n\r\n```\r\nAttributeError: 'Tensor' object has no attribute 'is_initialized'\r\n```\r\n\r\nK.zeros works in eager mode.\r\n\r\nUsage of tf.zeros work fine in both graph and eager mode.\r\n\r\nAfter debugging the tensorflow code I figured that towards the very end when keras tries to initialize the **variables** it sees some entries that are of type **Tensor**\r\n\r\nThose entries are the ones generated by K.zeros.\r\n\r\nI then looked at the implementation of K.zeros https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/keras/backend.py#L1010\r\n\r\nIn that it clear says that it could return either a variable or tensor based on the input (i.e. shape). This is correct however it **seems** like that irrespective of the return value being a variable or tensor it still ends up tracking it (via track_variable) in graph mode.\r\n\r\n```bash\r\n# code of zeros in tf.keras.backend.py\r\nwith ops.init_scope():\r\n    if dtype is None:\r\n      dtype = floatx()\r\n    tf_dtype = dtypes_module.as_dtype(dtype)\r\n    v = array_ops.zeros(shape=shape, dtype=tf_dtype, name=name)\r\n    if py_all(v.shape.as_list()):\r\n      return variable(v, dtype=dtype, name=name)\r\n    track_variable(v)\r\n    return v\r\n```\r\n\r\n```bash\r\n# code of track_variable in tf.keras.backend.py\r\ndef track_variable(v):\r\n  \"\"\"Tracks the given variable for initialization.\"\"\"\r\n  if context.executing_eagerly():\r\n    return\r\n  graph = v.graph if hasattr(v, 'graph') else ops.get_default_graph()\r\n  if graph not in _GRAPH_VARIABLES:\r\n    _GRAPH_VARIABLES[graph] = weakref.WeakSet()\r\n  _GRAPH_VARIABLES[graph].add(v)\r\n```\r\n\r\nDuring debugging I can see that since the tensor is part of the collection how invoking is_initialized on it would result in error. \r\n\r\nBased on the code flow I would think that if `K.zeros` is going to return a tensor then it should not track it (i.e add it to the variables collection).\r\n\r\n**Part of the stack trace**\r\n\r\n```\r\nreturn get_session().run(tensors)  File \"/Users/ksachdeva/mlenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 469, in get_session\r\n    _initialize_variables(session)  File \"/Users/ksachdeva/mlenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 731, in _initialize_variables\r\n    [variables_module.is_variable_initialized(v) for v in candidate_vars])  File \"/Users/ksachdeva/mlenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 731, in <listcomp>\r\n    [variables_module.is_variable_initialized(v) for v in candidate_vars])  File \"/Users/ksachdeva/mlenv/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 189, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n```", "comments": ["Same error is there with `K.ones()` also.", "Similar behavior is seen when using tf.zeros_like() too.", "Pavithra, looks like you added the tracking. Should it just be made conditional?", "Yes, the fix for this is in and should be available in the next nightly. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24938\">No</a>\n"]}, {"number": 24937, "title": "Try to fix gen_git_source.py some more.", "body": "", "comments": []}, {"number": 24936, "title": "High memory usage of tensorflow.python.ops.lookup_ops.index_table_from_file compared to vocabulary_file size on disc", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n    - OSX Mojave 10.14.2\r\n    - 2.3 GHz Intel Core i5\r\n    - 8 GB 2133 MHz LPDDR3\r\n    - Number of Processors: 1\r\n    - Total Number of Cores: 2\r\n- TensorFlow installed from (source or binary):\r\n    - pip installed into anaconda virtual env \r\n- TensorFlow version (use command below): v1.11.0-rc2-4-gc19e29306c 1.11.0\r\n- Python version: Python 3.6.6 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source): N/A \r\n- GCC/Compiler version (if compiling from source): c++ (GCC) 4.8.5\r\n- CUDA/cuDNN version: N/A \r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nNote: while the example code is python, this problem also exists once the graph is deserialised and appears to be language independent.\r\n\r\nThis issue originates from trying to serve a Tensorflow model containing tensorflow.python.ops.lookup_ops.index_table_from_file to check the occurrence of a word in a vocabulary. The memory usage of the model when making predictions increases to ~8x the size of the vocabulary on disc (350MB vocab on disc, 3GB+ total memory usage when running).\r\n\r\nThe cause of this seems to be the size of the hash table created when initialising index_table_from_file. Using the debugger, the memory increase occurs on initialisation. \r\n\r\n**Describe the expected behavior**\r\nMemory usage on the same order as it's on disc usage. \r\n\r\n**Code to reproduce the issue**\r\n*This generates and saves an 84MB file*\r\nRunning the following opens a Tensorflow debug session.\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    import string\r\n    import random\r\n    from tensorflow.python.ops.lookup_ops import index_table_from_file\r\n    from tensorflow.python import debug as tf_debug\r\n\r\n    lookup_table_filename = \"./lookup_table.csv\"\r\n    data_filename = './vocab_feature.npz'\r\n\r\n    def generate_data():\r\n        \"\"\"creates the required datafiles\"\"\"\r\n        letters = [i for i in string.ascii_letters]\r\n        vocab = set([\r\n            i + i + j + j + k + k + l + l\r\n            for i in letters\r\n            for j in letters\r\n            for k in letters\r\n            for l in letters\r\n        ])\r\n        \r\n        positive_vocab = set(random.sample(vocab, 1000000))\r\n        with open(lookup_table_filename, 'w') as f:\r\n            f.write('\\n'.join(positive_vocab))\r\n            f.write('\\n')\r\n            f.write('\\n'.join(map(str, range(10**7))))\r\n            f.write('\\n')\r\n\r\n        vocab_feature = np.random.choice(list(vocab), size=1000, replace=True)\r\n        np.savez(file=data_filename, vocab_feature=vocab_feature)\r\n\r\n    # Run the first time\r\n    generate_data()\r\n\r\n    vocab_feature = np.load(data_filename)['vocab_feature']\r\n    lookup_table = index_table_from_file(\r\n        vocabulary_file=lookup_table_filename,\r\n        default_value=-1,\r\n        num_oov_buckets=0,\r\n        vocab_size=None,\r\n        name='vocab_table',\r\n        key_dtype=tf.string  \r\n    )\r\n    text = tf.placeholder(dtype=tf.string, shape=[None, ])\r\n    index = lookup_table.lookup(text)\r\n    sess = tf.Session()\r\n    sess = tf_debug.LocalCLIDebugWrapperSession(sess)\r\n    sess.run(tf.tables_initializer())\r\n    # sess.run(tf.global_variables_initializer())\r\n    np_index = sess.run(index, feed_dict={text: vocab_feature})\r\n    sess.close()\r\n    print(np_index)\r\n\r\n**Other info / logs**\r\n**Things I've tried**\r\nI have attempted to change the parallelism threads via ConfigProto to see if this reduces the memory usage but without improvement.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24936\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24936\">No</a>\n"]}, {"number": 24935, "title": "Lite: Tile operator test case updated", "body": "2 test case added to verify the correct flow of data post tiling.\r\n\r\nAlso some style check warnings are fixed.", "comments": ["Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@aselle  Could you PTAL and approve. "]}, {"number": 24934, "title": "Building from source the shared library for Raspberry-Pi", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Stretch 9.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.9.0\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: none\r\n- Bazel version (if compiling from source): 0.11.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n\r\n\r\n**Describe the problem**\r\nI've been trying to compile tensorflow from source on my raspberry-pi for quite some time and did not succeeded yet.\r\nI followed many different tutorials such as [this](https://www.youtube.com/watch?v=WqCnW_2XDw8) or [this](https://gist.github.com/EKami/9869ae6347f68c592c5b5cd181a3b205) but none of them allowed me to build it successfully.\r\nI arrived to the point that I decided to ask for help. My latest state is trying to build tensorflow 1.9.0 using bazel 0.11.0 (as stated on tensorflow's website that these two versions are compatible). The bazel build goes flawlessly but for the tensorflow build, after ~20'000 seconds, shows the final error : \r\ntensorflow/tensorflow/BUILD:506:1: Linking of rule '//tensorflow:libtensorflow_cc.so' failed (Exit 1).\r\n\r\nThe goal would be to load and run a pre-trained model on my own c++ project. The pre-trained model was trained using keras with tensorflow backend version 1.10.1.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe steps are : \r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ngit checkout v1.9.0\r\ngrep -Rl 'lib64' | xargs sed -i 's/lib64/lib/g'\r\n./configure\r\nbazel build -c opt --config=monolithic --local_resources 1024,1.0,1.0 --verbose_failures //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nERROR: /home/pi/cleanTF/tensorflow/tensorflow/BUILD:506:1: Linking of rule '//tensorflow:libtensorflow_cc.so' failed (Exit 1): gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/280abdd112ab916e808922e4ee91ed95/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /usr/bin/gcc -shared -o bazel-out/arm-opt/bin/tensorflow/libtensorflow_cc.so -z defs -Wl,--version-script tensorflow/tf_version_script.lds '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_cc.so -pthread-pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread-pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/arm-opt/bin/tensorflow/libtensorflow_cc.so-2.params)\r\nbazel-out/arm-opt/bin/tensorflow/core/kernels/_objs/list_kernels/tensorflow/core/kernels/list_kernels.pic.o:list_kernels.cc:function tensorflow::TensorListStack<Eigen::ThreadPoolDevice, tensorflow::bfloat16>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'void tensorflow::ConcatCPU<tensorflow::bfloat16>(tensorflow::DeviceBase*, std::vector<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> > > > const&, tensorflow::TTypes<tensorflow::bfloat16, 2, int>::Matrix*)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 18838.252s, Critical Path: 14426.26s\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Try\r\nhttps://stackoverflow.com/questions/53874017/error-while-building-libtensorflow-so-on-raspberry-pi/54202653#54202653", "@anilknayak The tensorflow build still give me a huge amount of warnings, apparently mostly due to my pip version even if when checking my pip version, I have the most recent one (18.1 as of today)\r\n\r\nI do not know why it doesn't detect the right pip version.\r\n\r\nThe final error is : \r\nCommand \"/usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-NJz6ti/scipy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-HXLX0R-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-NJz6ti/scipy/\r\nYou are using pip version 9.0.3, however version 18.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\nThe command '/bin/sh -c /install/install_pip_packages.sh' returned a non-zero code: 1", "@AntoineWeber \r\nAre you building on raspberry pi or cross-compiling it?\r\n", "I am building it directly on the raspberry pi", "@AntoineWeber \r\nIf you build in raspberry pi, there are lot of things to be done. Anything this will take very long time because of limited memory.\r\n\r\nI will suggest to cross compile it. This is very easy, if you follow the stackoverflow page, I have summarized the point. Try once, we will see what is happening then.", "@anilknayak \r\nIn your post you mention that you also cross-compiled it for the raspberry pi.\r\n\r\nMay I ask more informations on which shell script did you run for the bazel build ?\r\nAnd also how did you cross-compiled protobuf exactly ?", "@AntoineWeber \r\n**Protobuf:**\r\ngit clone https://github.com/google/protobuf.git\r\ncd protobuf\r\ngit checkout v3.1.0\r\n./autogen.sh\r\n./configure\r\nmake -j 4\r\nsudo make install\r\nsudo ldconfig\r\n**Output**\r\nprotoc --version\r\nlibprotoc 3.1.0\r\n**Bazel**\r\nwget https://github.com/bazelbuild/bazel/releases/download/0.15.2/bazel-0.15.2-dist.zip\r\nunzip -d bazel bazel-0.15.2-dist.zip\r\ncd bazel\r\nchmod u+w ./* -R\r\n./compile.sh\r\nsudo cp output/bazel /usr/local/bin/", "Thank you @anilknayak, I actually figured them out myself.\r\nI succesfully built libtensorflow.so. But there are still some problems. For my project I also need most of the headers normally generated by the build. For example, ```tensorflow/tensorflow/contrib/makefile/downloads``` has not been generated, and I need the absl headers which are inside. Moreover, the eigen headers in `tensorflow/third_party/eigen3/` give me the \"#include nested too deeply\" error which is related to https://github.com/tensorflow/tensorflow/issues/13705.\r\n\r\nAnyway, I can hack these two errors by simply copying the downloads folder that I built for my local use on the folder for the rpi cross-build, as these are only headers. and for the eigen3 headers, I can also link my local ones which work (however, the ones in my local tensorflow build folder give the same errors, hence I have to link the ones in `python3.6/dist-packages/tensorflow`)\r\n\r\n\r\nSo the final problem is, I tried to add the \"libtensorflow_cc.so\" to the target libraries to be built inside the `build_raspberry_pi.sh` however this target was not built, and I need it.(seems like when I try to link libtensorflow.so, I have tons of undefined reference, and when I link libtensorflow_cc.so, everything build succesfully.)\r\n\r\n**EDIT :** \r\nThe libtensorflow_cc.so was actually built, I just forgot to copy it in output-artifacts/\r\n\r\nI am however still encountering undefined reference errors : \r\n```\r\nundefined reference to `tensorflow::ReadBinaryProto(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::MessageLite*)'\r\nCMakeFiles/main.exe.dir/classifier.cpp.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > tensorflow::io::JoinPath<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':\r\n\r\nundefined reference to `tensorflow::io::internal::JoinPathImpl[abi:cxx11](std::initializer_list<absl::string_view>)'\r\nCMakeFiles/main.exe.dir/classifier.cpp.o: In function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':\r\n\r\nundefined reference to `tensorflow::strings::StrCat[abi:cxx11](tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nCMakeFiles/main.exe.dir/classifier.cpp.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*)':\r\n\r\nundefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n**2ND EDIT :** \r\nI tried running the following command : \r\n```\r\nnm bazel-bin/tensorflow/libtensorflow_cc.so|c++filt|grep tensorflow::ReadBinaryProto\r\n```\r\nwhich gave : \r\n```\r\n0437d37c T tensorflow::ReadBinaryProto(tensorflow::Env*, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::MessageLite*)\r\n```\r\nNow if you compare with my first undefined reference, it looks like the error comes from `::__cxx11::`\r\nMaybe because of the -std=gnu11 to the compilation flag inside the `build_raspberry_pi.sh` script ? \r\n", "@AntoineWeber good to hear you found the solution and Thanks for providing solution here. @anilknayak Thanks for supporting the community. I am closing this issue. "]}]