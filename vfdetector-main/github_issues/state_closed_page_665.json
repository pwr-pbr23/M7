[{"number": 33652, "title": "Release TF Java 1.15", "body": "", "comments": []}, {"number": 33651, "title": "An issue with the code in the  Custom training with tf.distribute.Strategy tutorial", "body": "##  URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/distribute/custom_training\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI am not sure if this is a bug or an issue with the tutorial, but when I apply the [Custom training with tf.distribute.Strategy][1] tutorial to the [Image segmentation][2] (I just copy-pasted the code of the two tutorials), the scale of the training loss is not good:\r\n\r\n    Epoch 1, Loss: 35478.49609375, Accuracy: 48.560935974121094, Test Loss: 0.8764073252677917, Test Accuracy: 57.97665023803711\r\n    Epoch 2, Loss: 20161.634765625, Accuracy: 74.82583618164062, Test Loss: 0.6519305109977722, Test Accuracy: 77.33595275878906\r\n    Epoch 3, Loss: 15657.2880859375, Accuracy: 81.60499572753906, Test Loss: 0.5801540017127991, Test Accuracy: 79.94847106933594\r\n    Epoch 4, Loss: 13322.1689453125, Accuracy: 84.52685546875, Test Loss: 0.5113006830215454, Test Accuracy: 82.27192687988281\r\n    Epoch 5, Loss: 11845.38671875, Accuracy: 85.9767837524414, Test Loss: 0.4614977538585663, Test Accuracy: 83.19354248046875\r\n    Epoch 6, Loss: 10827.380859375, Accuracy: 86.9468002319336, Test Loss: 0.43975135684013367, Test Accuracy: 83.65667724609375\r\n    Epoch 7, Loss: 10006.4892578125, Accuracy: 87.75154113769531, Test Loss: 0.4181833863258362, Test Accuracy: 83.8880386352539\r\n    Epoch 8, Loss: 9534.9345703125, Accuracy: 88.15916442871094, Test Loss: 0.40620285272598267, Test Accuracy: 84.22107696533203\r\n    Epoch 9, Loss: 8993.767578125, Accuracy: 88.73575592041016, Test Loss: 0.3957768976688385, Test Accuracy: 84.42972564697266\r\n    Epoch 10, Loss: 8425.7080078125, Accuracy: 89.38662719726562, Test Loss: 0.37987643480300903, Test Accuracy: 84.94923400878906\r\n\r\nThe full code to reproduce is appended below. It looks like the training loss is scaled by `image_height*image_width`.\r\n\r\n**Code:**\r\n\r\n    import sys, os\r\n    import tensorflow as tf\r\n    from tensorflow_examples.models.pix2pix import pix2pix\r\n    \r\n    import tensorflow_datasets as tfds\r\n    \r\n    import time\r\n    \r\n    \r\n    def normalize(input_image, input_mask):\r\n      input_image = tf.cast(input_image, tf.float32) / 255.0\r\n      input_mask -= 1\r\n      return input_image, input_mask\r\n    \r\n    @tf.function\r\n    def load_image_train(datapoint):\r\n      input_image = tf.image.resize(datapoint['image'], (128, 128))\r\n      input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\r\n    \r\n      if tf.random.uniform(()) > 0.5:\r\n        input_image = tf.image.flip_left_right(input_image)\r\n        input_mask = tf.image.flip_left_right(input_mask)\r\n    \r\n      input_image, input_mask = normalize(input_image, input_mask)\r\n    \r\n      return input_image, input_mask\r\n    \r\n    def load_image_test(datapoint):\r\n      input_image = tf.image.resize(datapoint['image'], (128, 128))\r\n      input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\r\n    \r\n      input_image, input_mask = normalize(input_image, input_mask)\r\n    \r\n      return input_image, input_mask\r\n    \r\n    \r\n    def main():\r\n        dataset, info = tfds.load('oxford_iiit_pet:3.0.0', with_info=True)\r\n        TRAIN_LENGTH = info.splits['train'].num_examples\r\n        BATCH_SIZE = 192\r\n        BUFFER_SIZE = 1000\r\n        STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\r\n        train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        test = dataset['test'].map(load_image_test)\r\n        train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\r\n        train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n        test_dataset = test.batch(BATCH_SIZE)\r\n    \r\n    \r\n    \r\n        OUTPUT_CHANNELS = 3\r\n    \r\n        strategy = tf.distribute.MirroredStrategy()\r\n    \r\n        with strategy.scope():\r\n    \r\n            base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\r\n    \r\n            # Use the activations of these layers\r\n            layer_names = [\r\n                'block_1_expand_relu',   # 64x64\r\n                'block_3_expand_relu',   # 32x32\r\n                'block_6_expand_relu',   # 16x16\r\n                'block_13_expand_relu',  # 8x8\r\n                'block_16_project',      # 4x4\r\n            ]\r\n            layers = [base_model.get_layer(name).output for name in layer_names]\r\n    \r\n            # Create the feature extraction model\r\n            down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\r\n    \r\n            down_stack.trainable = False\r\n    \r\n            up_stack = [\r\n                pix2pix.upsample(512, 3),  # 4x4 -> 8x8\r\n                pix2pix.upsample(256, 3),  # 8x8 -> 16x16\r\n                pix2pix.upsample(128, 3),  # 16x16 -> 32x32\r\n                pix2pix.upsample(64, 3),   # 32x32 -> 64x64\r\n            ]\r\n            def unet_model(output_channels):\r\n    \r\n                # This is the last layer of the model\r\n                last = tf.keras.layers.Conv2DTranspose(\r\n                    output_channels, 3, strides=2,\r\n                    padding='same', activation='softmax')  #64x64 -> 128x128\r\n    \r\n                inputs = tf.keras.layers.Input(shape=[128, 128, 3])\r\n                x = inputs\r\n    \r\n                # Downsampling through the model\r\n                skips = down_stack(x)\r\n                x = skips[-1]\r\n                skips = reversed(skips[:-1])\r\n    \r\n                # Upsampling and establishing the skip connections\r\n                for up, skip in zip(up_stack, skips):\r\n                    x = up(x)\r\n                    concat = tf.keras.layers.Concatenate()\r\n                    x = concat([x, skip])\r\n    \r\n                x = last(x)\r\n    \r\n                return tf.keras.Model(inputs=inputs, outputs=x)\r\n    \r\n        \r\n            model = unet_model(OUTPUT_CHANNELS)\r\n            # model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\r\n            #             metrics=['accuracy'])\r\n    \r\n            # model.compile(optimizer=tf.keras.optimizers.Adam(3e-4),\r\n            #             loss=tf.keras.losses.SparseCategoricalCrossentropy(tf.keras.losses.Reduction.NONE),\r\n            #             metrics=['sparse_categorical_accuracy'])\r\n    \r\n            optimizer=tf.keras.optimizers.Adam(3e-4)\r\n            metrics = [tf.keras.metrics.MeanIoU(num_classes=3)]\r\n    \r\n        EPOCHS = 2\r\n        VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE\r\n    \r\n        print('Start training')\r\n        start = time.time()\r\n    \r\n        epochs = EPOCHS; steps = STEPS_PER_EPOCH; GLOBAL_BATCH_SIZE = BATCH_SIZE\r\n    \r\n        # Distribute the datasets\r\n    \r\n        train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\n        test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\r\n    \r\n        with strategy.scope():\r\n            # Set reduction to `none` so we can do the reduction afterwards and divide by\r\n            # global batch size.\r\n            loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n                reduction=tf.keras.losses.Reduction.NONE)\r\n            # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy\r\n            def compute_loss(labels, predictions):\r\n                per_example_loss = loss_object(labels, predictions)\r\n                return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\r\n    \r\n        with strategy.scope():\r\n            test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n    \r\n            train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n                name='train_accuracy')\r\n            test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n                name='test_accuracy')\r\n    \r\n    \r\n        with strategy.scope():\r\n            def train_step(inputs):\r\n                images, labels = inputs\r\n    \r\n                with tf.GradientTape() as tape:\r\n                    predictions = model(images, training=True)\r\n                    loss = compute_loss(labels, predictions)\r\n    \r\n                gradients = tape.gradient(loss, model.trainable_variables)\r\n                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n    \r\n                train_accuracy.update_state(labels, predictions)\r\n                return loss \r\n    \r\n            def test_step(inputs):\r\n                images, labels = inputs\r\n    \r\n                predictions = model(images, training=False)\r\n                t_loss = loss_object(labels, predictions)\r\n    \r\n                test_loss.update_state(t_loss)\r\n                test_accuracy.update_state(labels, predictions)\r\n    \r\n        with strategy.scope():\r\n            # `experimental_run_v2` replicates the provided computation and runs it\r\n            # with the distributed input.\r\n            @tf.function\r\n            def distributed_train_step(dataset_inputs):\r\n                per_replica_losses = strategy.experimental_run_v2(train_step,\r\n                                                                args=(dataset_inputs,))\r\n                return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                                    axis=None)\r\n            \r\n            @tf.function\r\n            def distributed_test_step(dataset_inputs):\r\n                return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))\r\n    \r\n            train_iter = iter(train_dataset)\r\n            for epoch in range(EPOCHS):\r\n                # TRAIN LOOP\r\n                total_loss = 0.0\r\n                num_batches = 0\r\n                while True:\r\n                    x = next(train_iter)\r\n                    total_loss += distributed_train_step(x)\r\n                    num_batches += 1\r\n                    if num_batches >= steps:\r\n                        break\r\n                train_loss = total_loss / num_batches\r\n    \r\n                # TEST LOOP\r\n                for x in test_dist_dataset:\r\n                    distributed_test_step(x)\r\n    \r\n                # if epoch % 2 == 0:\r\n                #     checkpoint.save(checkpoint_prefix)\r\n    \r\n                template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\r\n                            \"Test Accuracy: {}\")\r\n                print (template.format(epoch+1, train_loss,\r\n                                    train_accuracy.result()*100, test_loss.result(),\r\n                                    test_accuracy.result()*100))\r\n    \r\n                test_loss.reset_states()\r\n                train_accuracy.reset_states()\r\n                test_accuracy.reset_states()\r\n    \r\n    if __name__ == \"__main__\":\r\n        main()\r\n\r\n  [1]: https://www.tensorflow.org/tutorials/distribute/custom_training\r\n  [2]: https://www.tensorflow.org/tutorials/images/segmentation\r\n", "comments": ["I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/3bd5f3e75ac2cd79dec74d3bc9619062/untitled295.ipynb).Thanks!", "To fix it, there are 2 steps:\r\n\r\n1. Iter over train_dist_dataset instead of train_dataset.\r\n2. Reduce over batch_size instead of the entire prediction tensor because we need to predict an entire image/mask here.\r\n\r\nThe code changes are:\r\n\r\n```\r\ndef compute_loss(labels, predictions):\r\n  per_example_loss = loss_object(labels, predictions)\r\n  # Reduce over batch_size\r\n  return tf.reduce_sum(per_example_loss, 0) / GLOBAL_BATCH_SIZE\r\n```\r\nand then the changing the training loop to reduce_mean over the train_loss fixes the issue.\r\n\r\n```\r\ntrain_iter = iter(train_dist_dataset)\r\nfor epoch in range(EPOCHS):\r\n  # TRAIN LOOP\r\n  total_loss = 0.0\r\n  num_batches = 0\r\n  while True:\r\n    x = next(train_iter)\r\n    # total_loss will contain the loss reduced over all the replica losses.\r\n    total_loss += distributed_train_step(x)\r\n    num_batches += 1\r\n    if num_batches >= steps:\r\n      break\r\n  # Divide the total_loss by the number of batches since we want to report the loss over the entire epoch.\r\n  # train_loss.shape == (128, 128)\r\n  train_loss = total_loss / num_batches\r\n \r\n  # Reduce over the entire array to get the loss that is report able.\r\n  train_loss = tf.reduce_mean(train_loss)\r\n```\r\nThe output I get after this is\r\n\r\n```\r\nEpoch 1, Loss: 1.0484683513641357, Accuracy: 50.26182556152344, Test Loss: 0.9157692790031433, Test Accuracy: 58.35002136230469\r\nEpoch 2, Loss: 0.692024827003479, Accuracy: 71.80204010009766, Test Loss: 0.7471278309822083, Test Accuracy: 73.24569702148438\r\nEpoch 3, Loss: 0.5559802055358887, Accuracy: 78.87828063964844, Test Loss: 0.6854224801063538, Test Accuracy: 75.4207992553711\r\nEpoch 4, Loss: 0.46647387742996216, Accuracy: 82.48332214355469, Test Loss: 0.6156165599822998, Test Accuracy: 77.92351531982422\r\nEpoch 5, Loss: 0.41117799282073975, Accuracy: 84.09635162353516, Test Loss: 0.5281449556350708, Test Accuracy: 81.19449615478516\r\nEpoch 6, Loss: 0.368894100189209, Accuracy: 85.38777160644531, Test Loss: 0.47796040773391724, Test Accuracy: 82.69153594970703\r\nEpoch 7, Loss: 0.34240323305130005, Accuracy: 86.14852142333984, Test Loss: 0.4544333815574646, Test Accuracy: 83.27204895019531\r\n```", "@yashk2810 Looks good. Thanks a lot!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33651\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33651\">No</a>\n", "@yashk2810 @ravikyram It seems that this issue is related to [this bug](https://github.com/tensorflow/tensorflow/issues/32099), which has not been fixed yet."]}, {"number": 33650, "title": "padding_shapes -> padded_shapes to match parameter", "body": "I believe there's been an error here: the docstring mentions `padding_shapes`, while the parameter it seems to be referring to is `padded_shapes`.", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac ", "It's better to submit multiple typo fixes in a single PR as the CPU/GPU hours wasted to CI test the typo are too many in comparison with the seconds it takes to submit a single typo fix. \r\n\r\nI see you have created 4 extremely short typo fix PRs, is this to get the Hacktoberfest T-Shirt?\r\n\r\nPlease try to get multiple typos in one PR. For example, fix all typos in this file/directory?", "Haha sorry, didn't realise the implications!\r\nAnd no, I have no idea about t-shirts - I've just been reading a lot of documentation lately for my experiments, and try to correct where I can :)\r\n\r\nI'll see if I can get multiple typos together, if I notice something again. Thanks!"]}, {"number": 33649, "title": "Simple custom metric caused tf.function retracing when training on multiple GPUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v2.0.0-rc2-26-g64c3d38 2.0.0`\r\n- Python version: `3.5.2`\r\n- CUDA/cuDNN version: `9.2`\r\n- GPU model and memory: GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nUsing the following custom metric (event without the commented part):\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MeanIoUIgnoreLabel(tf.keras.metrics.MeanIoU):\r\n    \"\"\"Mean Intersection-over-Union with an ignored label\r\n    \"\"\"\r\n    def __init__(self, num_classes, ignore_label=None, name='mIoU_ignore_label', **kwargs):\r\n      super(MeanIoUIgnoreLabel, self).__init__(num_classes, name=name, **kwargs)\r\n      self.ignore_label = ignore_label\r\n\r\n    def update_state(self, y_true, y_pred, sample_weight=None):\r\n        y_pred = tf.argmax(y_pred, axis=-1)\r\n        # if self.ignore_label is not None:\r\n        #     if sample_weight is not None:\r\n        #         # sample_weight = tf.where(y_true == self.ignore_label, 0, sample_weight)\r\n        #         sample_weight = tf.where(tf.math.equal(y_true, self.ignore_label), 0, sample_weight)\r\n        #     else:\r\n        #         # sample_weight = y_true != self.ignore_label\r\n        #         sample_weight = tf.math.not_equal(y_true, self.ignore_label)\r\n        return super(MeanIoUIgnoreLabel, self).update_state(y_true, y_pred, sample_weight)   \r\n```\r\n\r\nI obtained the following warning:\r\n\r\n> 2019-10-23 22:27:58.600906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> WARNING:tensorflow:5 out of the last 9 calls to <bound method MeanIoUIgnoreLabel.update_state of <metrics.MeanIoUIgnoreLabel object at 0x7fc0ac1f86a0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n\r\nI am using `tf.distribute.MirroredStrategy()` to train on multiple GPUs.\r\n\r\nI guess this is a bug?\r\n\r\nThanks.\r\n\r\n", "comments": ["@netw0rkf10w, Please provide the complete standalone code to reproduce the reported issue. Thanks!", "Hi @gadagashwini! Sorry for the delay, because this issue occurred randomly, I have been trying to investigate further to make sure it is reproducible.\r\n\r\nFirst of all, the code can be found in [this Colab notebook](https://colab.research.google.com/drive/1qpCxBeF-2GEjPkvbcQDITC7Gi_9X-oTc). Note, however, that **the problem does not occur when running the notebook, because it only occurs when training on more than one GPU**.\r\n\r\nBelow are some additional findings.\r\n\r\nIn the code, I have a function `distributed_training(datasets, model, optimizer, loss, metrics, strategy)` that performs distributed training. The issue occurs only if the same metric objects are used in both the `train_step` and `val_step` functions. If I separate the metrics, for example:\r\n```\r\ntrain_metrics = [MeanIoUIgnoreLabel(num_classes=3)]\r\nval_metrics = [MeanIoUIgnoreLabel(num_classes=3)]\r\n```\r\nthen no issue.\r\n\r\nIn addition, the issue occurs only for the custom metric that I defined: \r\n`metrics = [MeanIoUIgnoreLabel(num_classes=3)]`\r\n\r\nIf \r\n`metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]`\r\nthen no issue.\r\n\r\nIn conclusion, the issue occurs only if the following 3 things happen simultaneously:\r\n1. More than 1 GPU are used.\r\n2. The custom metric `MeanIoUIgnoreLabel` is part of the input argument `metrics` of the `distributed_training()` function.\r\n3. The same `metrics` object is used in both the `train_step()` and `val_step()` function. (I also tried to create a copy for `val_step` like this: `val_metrics = metrics.copy()` but it did not work, I guess `metrics` and `metrics.copy()` still share the same metric objects).\r\n\r\nPlease let me know if you need additional information. Thanks.", "I have encountered a similar problem when I use Keras custom metrics.\r\n\r\ninfo\uff1a\r\n- tensorflow 2.0.0\r\n- running on cpu\r\n- warning log\r\n```\r\nW1106 10:49:14.171694 4745115072 def_function.py:474] 6 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x14a3f9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n```\r\n- custom code\r\n```python\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\nclass Metrics(Callback):\r\n    def __init__(self, dev_data, classifier, dataloader):\r\n        self.best_f1_score = 0.0\r\n        self.dev_data = dev_data\r\n        self.classifier = classifier\r\n        self.predictor = Predictor(classifier, dataloader)\r\n        self.dataloader = dataloader\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        print(\"start to evaluate....\")\r\n        _, preds = self.predictor(self.dev_data)\r\n        y_trues, y_preds = [self.dataloader.label_vector(v[\"label\"]) for v in self.dev_data], preds\r\n        f1 = f1_score(y_trues, y_preds, average=\"weighted\")\r\n        print(classification_report(y_trues, y_preds,\r\n                                    target_names=self.dataloader.vocab.labels))\r\n        if f1 > self.best_f1_score:\r\n            self.best_f1_score = f1\r\n            self.classifier.save_model()\r\n            print(\"best metrics, save model...\")\r\n```\r\n", "Hi @ymodak. Any updates on this please? Thanks.", "@SeanLee97 Would you be able to share standalone code for reproducing the issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33649\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33649\">No</a>\n", "@ymodak @pavithrasv The bug can be reproduced using [this Colab notebook](https://colab.research.google.com/drive/1qpCxBeF-2GEjPkvbcQDITC7Gi_9X-oTc). It's the same notebook as in the first message, but with 2 differences:\r\n\r\n- I added a creation of 2 virtual GPUs to simulate multi-gpu training.\r\n- I changed the TF version to tf-nightly to show that the bug still exists in the latest version.\r\n\r\nSimply running the notebook, one will obtain this kind of messages:\r\n\r\n> WARNING:tensorflow:5 out of the last 9 calls to <bound method MeanIoUIgnoreLabel.update_state of <__main__.MeanIoUIgnoreLabel object at 0x7fa5403782e8>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n\r\nLet me know if you guys need additional information.\r\n", "i think\r\n#34025 \r\nhas the same problem", "I may have found the \"Problem\"\r\nI implemented a simple function with a `print` statement to see when it is traced.\r\nsimilar to :\r\nhttps://www.tensorflow.org/tutorials/customization/performance#tracing_and_polymorphism\r\nand ran it on a multi gpu setup.\r\n\r\nOutput is:\r\n```\r\nTracing with Tensor(\"inputs:0\", dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 15, 3), dtype=float32)\r\nTracing with Tensor(\"inputs:0\", dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0) Tensor(\"inputs_1:0\", shape=(None, 15, 3), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\r\n\r\nTracing with Tensor(\"inputs:0\", dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1) Tensor(\"inputs_1:0\", shape=(None, 15, 3), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\r\nTracing with Tensor(\"inputs:0\", dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:2) Tensor(\"inputs_1:0\", shape=(None, 15, 3), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:2)\r\nTracing with Tensor(\"inputs:0\", dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:3) Tensor(\"inputs_1:0\", shape=(None, 15, 3), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:3)\r\nWARNING:tensorflow:5 out of the last 5 calls to <function LocationToIndex.call at 0x7f151ab16620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n```\r\n\r\nso the graph is traced on every used cpu/gpu one time, thats why the warning is shown\r\nthe warning code counts how often a trace is made... and warns if a trace happens multiple times\r\na simple solution would be:\r\ncount how often a trace PER Device is made\r\nonly warn if trace is made on same Device multiple times", "@netw0rkf10w  I just ran the notebook you linked, and I don't see those warnings. Can you check if you're still seeing them?\r\n\r\n(Note that you should install tf-nightly-gpu instead of tf-nightly if you're planning to GPUs - but this is orthogonal point).", "@bela127 Nice finding, thanks.\r\n\r\n@guptapriya Thanks. It seems that the bug has been fixed in the latest `tf-nightly-gpu`. Let me close this issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33649\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33649\">No</a>\n"]}, {"number": 33648, "title": "Can save but not load custom metrics with a variable named 'weights' in the tf saved model format", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nAttributeError occurs when trying to load a tf saved model using  tf.keras.models.load_model with a custom metric with a variable named 'weights'.\r\n\r\n**Describe the expected behavior**\r\nEither an error gets thrown during assignment or saving that you are not allowed to save a variable with the name 'weights', or no attribute error occurs and load_model loads the metric successfully.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.metrics import Metric\r\nimport numpy as np\r\n\r\nclass CustomMetric(Metric):\r\n  def __init__(self,\r\n               name='score',\r\n               dtype=tf.float32):\r\n    super(CustomMetric, self).__init__(name=name)\r\n    self.true_positives = self.add_weight(\r\n        'true_positives',\r\n        shape=[10],\r\n        initializer='zeros',\r\n        dtype=self.dtype)\r\n    self.weights_intermediate = self.add_weight(\r\n        'weights',\r\n        shape=[10],\r\n        initializer='zeros',\r\n        dtype=self.dtype)\r\n\r\n  def update_state(self, y_true, y_pred, sample_weight=None):\r\n    pass\r\n\r\n  def result(self):\r\n    return 0\r\n\r\n  def get_config(self):\r\n    \"\"\"Returns the serializable config of the metric.\"\"\"\r\n    config = {}\r\n    base_config = super(CustomMetric, self).get_config()\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n\r\n  def reset_states(self):\r\n    self.true_positives.assign(np.zeros(10), np.float32)\r\n    self.weights_intermediate.assign(\r\n        np.zeros(10), np.float32)\r\n            \r\ninputs = keras.Input(shape=(784,), name='digits')\r\nx = layers.Dense(64, activation='relu', name='dense_1')(inputs)\r\nx = layers.Dense(64, activation='relu', name='dense_2')(x)\r\noutputs = layers.Dense(10, activation='softmax', name='predictions')(x)\r\nmodel = keras.Model(inputs=inputs, outputs=outputs, name='3_layer_mlp')\r\n\r\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=.001), metrics=[CustomMetric()])\r\n\r\nmodel.save(\"model/\", save_format='tf')\r\n\r\nnew_model = keras.models.load_model('model/', custom_objects={'score': CustomMetric})\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2256, in __setattr__\r\n    super(tracking.AutoTrackable, self).__setattr__(name, value)\r\nAttributeError: can't set attribute\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/sentim/Website/model_prediction/test_load_saved_model.py\", line 50, in <module>\r\n    new_model = keras.models.load_model('model/', custom_objects={'score': CustomMetric})\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 86, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 541, in load_internal\r\n    export_dir)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 102, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 121, in __init__\r\n    self._load_all()\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 265, in _load_all\r\n    setter(obj, reference.local_name, nodes[reference.node_id])\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 234, in _revive_setter\r\n    setattr(self, name, value)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2261, in __setattr__\r\n    'different name.').format(name))\r\nAttributeError: Can't set the attribute \"weights\", likely because it conflicts with an existing read-only @property of the object. Please choose a different name.\r\n```\r\n\r\n", "comments": ["Issue replicating for the given code TF-2.0,please find the [gist](https://colab.sandbox.google.com/gist/oanush/a6c1d66f14b79a9f45a39412ecc52c1c/33648.ipynb) of colab.Thanks!", "@AndersonHappens I agree. May be it is better to add some checks before saving the model but it is not as simple as it looks. However, the error clearly describe what needs to changed. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/e3ac0c1f35bb4c9cf73afaedde6d0975/33648.ipynb) is a gist for your reference. Thanks!", "The issue is that the custom_objects are not passed to the compile in case of `save_fromt='tf'`.\r\nI belive this PR should fix this issue: https://github.com/tensorflow/tensorflow/pull/34048", "This issue has been fixed in the latest nightly , please see the [gist here](https://colab.sandbox.google.com/gist/goldiegadde/7ed551183f76afd08d3dc36bb7ad6ce0/33648.ipynb)\r\n\r\nthere is one small change to the code, load models line should be changed to \r\nnew_model = keras.models.load_model('model/', custom_objects={'CustomMetric': CustomMetric})", "@AndersonHappens marking this as fixed, please re-open if you run into any further issues.\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33648\">No</a>\n"]}, {"number": 33647, "title": "mlir: tweak function input types", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33647) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33647) for more info**.\n\n<!-- ok -->", "This has been fixed internally , thanks for your contribution."]}, {"number": 33646, "title": "ValueError: Unknown metric function: CustomMetric using custom metrics when loading tf saved model type with tf.keras.models.load_model ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nValueError: Unknown metric function: CustomMetric occurs when trying to load a tf saved model using  tf.keras.models.load_model with a custom metric. If you look at the code for load_model, it is clear the load_model currently ignores the custom_objects dict for the tf saved model format.\r\n\r\n**Describe the expected behavior**\r\nload_model loads the custom metric successfully either just implicitly or through the custom_objects dict. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.metrics import Metric\r\nimport numpy as np\r\n\r\nclass CustomMetric(Metric):\r\n  def __init__(self,\r\n               name='score',\r\n               dtype=tf.float32):\r\n    super(CustomMetric, self).__init__(name=name)\r\n    self.true_positives = self.add_weight(\r\n        'true_positives',\r\n        shape=[10],\r\n        initializer='zeros',\r\n        dtype=self.dtype)\r\n\r\n\r\n  def update_state(self, y_true, y_pred, sample_weight=None):\r\n    pass\r\n\r\n  def result(self):\r\n    return 0\r\n\r\n  def get_config(self):\r\n    \"\"\"Returns the serializable config of the metric.\"\"\"\r\n    config = {}\r\n    base_config = super(CustomMetric, self).get_config()\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n\r\n  def reset_states(self):\r\n    self.true_positives.assign(np.zeros(self.num_classes), np.float32)\r\n    self.weights_intermediate.assign(\r\n        np.zeros(self.num_classes), np.float32)\r\n            \r\ninputs = keras.Input(shape=(784,), name='digits')\r\nx = layers.Dense(64, activation='relu', name='dense_1')(inputs)\r\nx = layers.Dense(64, activation='relu', name='dense_2')(x)\r\noutputs = layers.Dense(10, activation='softmax', name='predictions')(x)\r\nmodel = keras.Model(inputs=inputs, outputs=outputs, name='3_layer_mlp')\r\n\r\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=.001), metrics=[CustomMetric()])\r\n\r\nmodel.save(\"model/\", save_format='tf')\r\n\r\nnew_model = keras.models.load_model('model/',  tf.keras.models.load_model ={'score': CustomMetric})\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/sentim/Website/model_prediction/test_load_saved_model.py\", line 46, in <module>\r\n    new_model = keras.models.load_model('model/', custom_objects={'score': CustomMetric})\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 93, in load\r\n    model._training_config))  # pylint: disable=protected-access\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 356, in compile\r\n    self._cache_output_metric_attributes(metrics, weighted_metrics)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1901, in _cache_output_metric_attributes\r\n    metrics, self.output_names, output_shapes, self.loss_functions)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 813, in collect_per_output_metric_info\r\n    metric_name = get_metric_name(metric, is_weighted)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 987, in get_metric_name\r\n    metric = metrics_module.get(metric)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\", line 2857, in get\r\n    return deserialize(identifier)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\", line 2851, in deserialize\r\n    printable_module_name='metric function')\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 180, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"/home/sentim/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 165, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown metric function: CustomMetric\r\n```\r\n\r\n", "comments": ["I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/1c9190b3588867fa05dd7103b212bccb/untitled296.ipynb). Thanks!", "@AndersonHappens I think there is an issue with saving a model in *.tf version when the model has custom metrics. I have saved the model in  *.h5 format and everything works as expected. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/5cd85f60f1d6e975ce65675227422b3e/untitled296.ipynb). Thanks!\r\n\r\nPlease close the issue if it was resolved for you. Thanks!", "@jvishnuvardhan While it does work in the h5 format, if I have saved a model to the tf format, I cannot load the model to resave it to the h5 format later (since I can't load the model in the first place), so ultimately this is still an issue that needs to be addressed.", "Here is a workaround for the meantime:\r\n```python\r\nfrom tensorflow.python.saved_model import loader_impl\r\nfrom tensorflow.python.keras.saving.saved_model import load as saved_model_load\r\n\r\nloader_impl.parse_saved_model(load_path)\r\nmodel = saved_model_load.load(load_path, custom_objects={\"custom_metric\": custom_metric})\r\n```", "same issue here, when you save the model in tf format, you can't re-load the model with custom_objects, this should be fixed.", "> Here is a workaround for the meantime:\r\n> \r\n> ```python\r\n> from tensorflow.python.saved_model import loader_impl\r\n> from tensorflow.python.keras.saving.saved_model import load as saved_model_load\r\n> \r\n> loader_impl.parse_saved_model(load_path)\r\n> model = saved_model_load.load(load_path, custom_objects={\"custom_metric\": custom_metric})\r\n> ```\r\n\r\nnot working at keras 2.3.1, tf 2.0.0\r\n```\r\nTypeError: load() got an unexpected keyword argument 'custom_objects'\r\n```", "> @jvishnuvardhan While it does work in the h5 format, if I have saved a model to the tf format, I cannot load the model to resave it to the h5 format later (since I can't load the model in the first place), so ultimately this is still an issue that needs to be addressed.\r\n\r\n@AndersonHappens Can you please check with the `tf-nightly`. I saved model in \"tf\" format, then loaded model and saved in \"h5\" format without any issues. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/9f3c879cdb48821e6b7554e4fe700e0b/untitled702.ipynb).\r\n\r\nPlease let us know what you think. Thanks.\r\n@JustinhoCHN can you please try `tf-nightly`. If you still have an issue, please open a new issue with a standalone code to reproduce the error.\r\n\r\nI am closing this issue as it was resolved in recent `tf-nightly`. Please feel free to open if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33646\">No</a>\n", "@jvishnuvardhan This issue should not be closed. The loading as in your gist works, but once you use the model, e.g. to further train it you will get an error that the custom object is unkown.", "Here is a new workaround, not sure what changed that the old one does not work anymore:\r\n\r\n```python\r\nfrom tensorflow.python.saved_model import load as tf_load\r\nfrom tensorflow.python.keras.saving.saved_model.load import KerasObjectLoader, RevivedModel\r\nfrom tensorflow.python.keras.saving import saving_utils\r\n\r\ncustom_objects = {\"compute_loss\": loss}\r\nmodel = tf_load.load_internal(PATH_TO_MODEL, loader_cls=KerasObjectLoader)\r\nif isinstance(model, RevivedModel) and compile:\r\n  if model._training_config is not None:\r\n    model.compile(**saving_utils.compile_args_from_training_config(\r\n    model._training_config, custom_objects))\r\n```", "@j-o-d-o Can you try adding one more line as follows and train the model (`loaded_my_new_model_saved_in_h5`).\r\n\r\n`loaded_my_new_model_saved_in_h5.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=.001), metrics=[CustomMetric()])`\r\n\r\nThanks!", "The models saved in h5 format seem to work fine, the issue is about models saved with SavedModel format (as explained here https://www.tensorflow.org/guide/saved_model)\r\n\r\nWhat is working is setting the compile flag to False and then compiling it on its own e.g.:\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Save Keras Model as SavedModel (Keras model has some custom objects e.g. custom loss function)\r\ntf.saved_model.save(my_keras_model, EXPORT_DIR)\r\n...\r\n# Load the model and compile on its own (working)\r\nloaded_model = tf.keras.model.load(EXPORT_DIR, custom_objects={\"custom_loss\": my_custom_loss}, compile=False)\r\nmodel.compile(optimizer=my_optimizer, loss=my_custom_loss)\r\n# Load the model while also loading optimizer and compiling (failing with \"Unkown loss function: my_custom_loss\")\r\nloaded_model = tf.keras.model.load(EXPORT_DIR, custom_objects={\"custom_loss\": my_custom_loss}) # compile is set to True by default\r\n```\r\n\r\nThe point is:\r\n1) The default way of loading models fails if there are custom objects involved.\r\n2) By compiling yourself you are setting up a new optimizer instead of loading the previously trained models optimizer weights.\r\n\r\nMoreover I already submited a PR that would fix this: https://github.com/tensorflow/tensorflow/pull/34048. But it seems nobody bothers about it : /", "@j-o-d-o Can you please check using `model.save` after compile and the use `keras.models.load_model` to load the model. I tried it without any issue. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/0f8c9294cfd1e8ae945f9edae36831ad/untitled702.ipynb) is the gist. Please run it with `tf-nightly`. Thanks!", "I am closing this issue as it was resolved. Please feel free to reopen if the issue didn't resolve for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33646\">No</a>\n", "@jvishnuvardhan `tf-nightly` works, but doesn't run on the GPU.\r\nPs.: regular `tensorflow` does run on GPU as expected.\r\n\r\nAlso, isn't nightly an unstable build?\r\nIs there a stable solution to the problem?\r\nOr when is the regular `tensorflow` expected to be fixed?", "@rodrigoruiz Can you please open a new issue with details and a simple standalone code to reproduce the issue? Currently `TF2.2.0rc2` is the latest release candidate. I expect there will be TF2.2 stable version will be released in the near future. Thanks!", "@jvishnuvardhan I think i figured it out, `tf-nightly` does not run on GPU, `tf-nightly-gpu` does... It's just that this is not specified in the docs.\r\nI'll just wait for the stable version I guess.", "Just tried this on 2.2.0. While it doesn't run into error, it seems to load an empty model. I'm using Feature Column API.", "@timatim Please create a new issue with a simple standalone to reproduce the issue. Thanks!", "I have this problem loading an .h5 model on TF 2.3.0.", "I am using tensorflow v 2.3  in R, saving and loading the model with save_model_tf() , load_model_tf() and I get the same error because of my custom metric balanced accuracy.\r\n\r\nI can't  compile it afterwards because I am running a grid search for the optimizer learning rate, so it wont be practical.\r\n\r\nmy issue was resolved by adding my custom metric in the custom_objects:\r\nload_model_tf(path, custom_objects=list(\"CustomLayer\" = CustomLayer))"]}, {"number": 33645, "title": "Allow device groups / CPU in MirroredStrategy", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nMy neural network is too large to be trained on a single GPU. In this specific case I have one layer that needs a lot of memory but does not perform expensive computations.\r\nI can train the network if I place this layer on the CPU (so that it uses main memory) or on another GPU.\r\n\r\nBut I still would like to use MirroredStrategy to distribute the training across multiple GPUs.\r\nCurrently this combination is not possible.\r\n\r\nTherefore I would like to request a feature to place parts of the computations inside a cross-replica context on the CPU. In a more generic case MirroredStrategy could distribute computations across device groups (instead of devices) and allow the user to place operations on different devices inside the group.\r\n\r\nThis could look like follows:\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n        x = f1()\r\n    with tf.device('/cpu:0'):\r\n        y = f2(x)\r\n```\r\n\r\nOr like this in the general case:\r\n```\r\nstrategy = tf.distribute.MirroredStrategy(devices=[['/gpu:0', '/gpu:1'], ['/gpu:3', '/gpu:4']])\r\nwith strategy.scope():\r\n    with tf.device('/vgpu:0'):\r\n        x = f1()\r\n    with tf.device('/vgpu:1'):\r\n        y = f2(x)\r\n```\r\n\r\n**Will this change the current api? How?**\r\nThe general case (device groups) needs a way to specify which devices are inside which group. See above for an example.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wants to train larger models that do not fit into the VRAM of a single GPU but who still wants to use MirroredStrategy.", "comments": ["The model parallelism project is on our radar. We've actively been working on it: https://github.com/tensorflow/tensorflow/issues/27724", "@olesalscheider,\r\nCan you please confirm if we can close this issue as it has been tracked in #27724. Thanks! ", "Yes, I think both issues request the same feature. You can close this issue if you want.", "@olesalscheider,\r\nClosing this issue as discussed above. Thanks! "]}, {"number": 33644, "title": "update tutorial for tensorflow object detection api , generate_tf_record is giving a tough time", "body": "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\r\n\r\nupdate generate_tf_record file according to running version in tensor flow.", "comments": ["I would like to work on this. Please assign!", "The GitHub repository you are referring to is not maintained by offfical TensorFlow team.\r\nPerhaps your issue is more suitable on https://github.com/sglvladi/TensorFlowObjectDetectionTutorial/issues.\r\nPlease try posting it there. Thanks!"]}, {"number": 33643, "title": "Remove note about `tf.keras.experimental.export_saved_model`.", "body": "`tf.keras.experimental.export_saved_model` is deprecated. And model.save with 'tf' option works fine.", "comments": ["We will not be encouraging one liner changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac", "@rthadur @mihaimaruseac \r\nI think this line is very important and misleading us. Why don't you reconsider about this change?\r\nThank you.", "The issue is that minor changes that can be done in <1 minute still require hours of CPU/GPU time for CI, so we are trying to reduce that, especially in October when there is Hacktoberfest and we receive a lot of PRs.", "On the other hand, since this is already after 2.0 was released, let's merge this.", "@rthadur @mihaimaruseac Thank you!\r\nAnd I apologize that I didn't consider about CI cost.\r\n\r\nHow can we contribute about documentation changes? (e.g. PullRequest to `documentation` branch)", "The best approach is to fix multiple documentation issues in a file, adding docstrings, documenting all parameters, etc."]}, {"number": 33642, "title": "portable deep feed forward neural network for regression and classification with less than 100 lines of code.", "body": "Dear All: \r\n This message is just a \"ad\" to my new project at\r\nhttps://github.com/wangyi-fudan/wymlp\r\nSorry for the trouble since I have no idea to post it elsewhere. :-)", "comments": ["The lines of code have been reduced to 50!"]}, {"number": 33641, "title": "Output names lost when loading Keras model in SavedModel format", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-5.0.0-25-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\nWhen loading a Keras model saved in the `SavedModel` format the output names are lost. Losing the output names causes loading to fail if using dictionaries for configuring loss or metrics.\r\n\r\n**Describe the expected behavior**\r\nOutput names to be restored when loading the model and dictionaries for losses to be working when loading the model.\r\n\r\n**Code to reproduce the issue**\r\n`output_names` is not restored and outputs are given new auto-generated names:\r\n```py\r\nimport tensorflow as tf\r\n\r\ni = tf.keras.layers.Input((1,))\r\nx = tf.keras.layers.Dense(1, name='my-output')(i)\r\nm = tf.keras.Model(i, x)\r\nm.compile(loss='mse')\r\n\r\nm.save('my-saved-model')\r\nm2 = tf.keras.models.load_model('my-saved-model')\r\nassert m2.output_names[0] == 'my-output'  # AssertionError\r\n```\r\n\r\nModel fails to load when using dictionaries for losses:\r\n```py\r\nimport tensorflow as tf\r\n\r\ni = tf.keras.layers.Input((1,))\r\nx = tf.keras.layers.Dense(1, name='my-output')(i)\r\nm = tf.keras.Model(i, x)\r\nm.compile(loss={'my-output': 'mse'})\r\nassert m.output_names[0] == 'my-output'\r\n\r\nm.save('my-saved-model')\r\nm2 = tf.keras.models.load_model('my-saved-model') # ValueError: Unknown entries in loss dictionary: ['my-output']. Only expected following keys: ['output_1']\r\n```", "comments": ["Issue is replicating with TF 2.0.0. Please take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/529b06c9cac349064ad90177da793da9/untitled223.ipynb). Thanks!", "```model.save``` method fails with precondition error in TF 2.0 nightly version '2.1.0-dev20191024'\r\n```python\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1785: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-1-edade35c87e6> in <module>()\r\n      6 m.compile(loss='mse')\r\n      7 \r\n----> 8 m.save('my-saved-model')\r\n      9 m2 = tf.keras.models.load_model('my-saved-model')\r\n     10 assert m2.output_names[0] == 'my-output'  # AssertionError\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py in recursive_create_dir_v2(path)\r\n    453     errors.OpError: If the operation fails.\r\n    454   \"\"\"\r\n--> 455   pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(path))\r\n    456 \r\n    457 \r\n\r\nFailedPreconditionError: my-saved-model/variables; Not a directory\r\n```", "This is fixed with latest tf version 2.1.0.-rc0. Thanks!", "Confirming, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33641\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33641\">No</a>\n"]}, {"number": 33640, "title": "Feature_column  to surport  weighted_categorical_column  in sequence;  and   sequence or normal categorical_column in shared_embeddings.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nfor example:\r\n\r\nweighted_categorical_column  in sequence:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.feature_column.feature_column_v2 import SequenceCategoricalColumn\r\n\r\nbatch_tensor_dict = {'dog': [[-1,-1],[1,1],[2,-1],[0,1]],\r\n                     'dog_weight': [[0.0,0.0],[1.0,2.0],[3.0,0.0],[4.0,5.0]]\r\n                     }\r\n\r\nfc_cat_dog = tf.feature_column.categorical_column_with_identity(key='dog',num_buckets=3)\r\nfc_wei_dog = tf.feature_column.weighted_categorical_column(fc_cat_dog, 'dog_weight')\r\n\r\nseq_dog = SequenceCategoricalColumn(fc_wei_dog)\r\nseq_dog2 = tf.feature_column.sequence_categorical_column_with_identity('dog', 3)\r\n\r\nind_dog = tf.feature_column.indicator_column(seq_dog)\r\ninput_layer = tf.keras.experimental.SequenceFeatures([ind_dog])\r\nseq_input, seq_len = input_layer(batch_tensor_dict)\r\nseq_len_mask = tf.sequence_mask(seq_len)\r\n\r\nprint(seq_input)\r\npass\r\n```\r\n\r\n\r\nsequence or normal categorical_column in shared_embeddings: \r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.feature_column as fc\r\n\r\n\r\nbatch_tensor_dict = {'item_id': [-1,1,2,0],\r\n                     'history_item_id_list': [[-1,-1],[1,1],[2,-1],[0,1]]\r\n                     }\r\n\r\nfc_cat_item_id = fc.categorical_column_with_identity('item_id', 3)\r\nfc_seq_cat_history_item_id_list = fc.sequence_categorical_column_with_identity('history_item_id_list', 3)\r\n\r\nfc_shared_emb_cols = fc.shared_embeddings(categorical_columns=[fc_cat_item_id, fc_seq_cat_history_item_id_list])\r\n```\r\ntf.keras.experimental.SequenceFeatures and tf.keras.layers.DenseFeatures  both are not suitable for fc_shared_emb_cols.\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["Feature column support is currently not in our roadmap. We have been putting every resource on migrating to [Keras preprocessing layers](https://github.com/tensorflow/community/pull/188). I think your use case can be supported by tf.RaggedTensor + `IndexLookup` layer + `Embedding` layer."]}, {"number": 33639, "title": "tf.io.write_file truncates the file everytime", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.io.write_file is not able to append to the file. I have a scenario where I want to write the output of the network (text generation) to a file. However I use sharded execution of tf.function which writes to different files (suffixed by the shard range) so that I can handle the total billion range. This is not possible through current available API\r\n\r\n**Will this change the current api? How?** A parameter to tf.io.write_file to append to the file.\r\n\r\n**Who will benefit with this feature?** All users\r\n\r\n**Any Other info.**\r\n", "comments": ["Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.", "Please post a code sample showcasing the bug", "Tested on `tf-nightly`, there is no bug:\r\n\r\n```console\r\nPython 3.7.3 (default, Apr  3 2019, 05:39:12) \r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.1.0-dev20191024'\r\n>>> tf.io.gfile.GFile('x', 'w').write('xyz')\r\n>>> tf.io.gfile.GFile('x', 'w').write('asdf')\r\n>>> tf.io.gfile.GFile('x', 'r').read()\r\n'asdf'\r\n>>> tf.io.gfile.GFile('x', 'a').write('xyz')\r\n>>> tf.io.gfile.GFile('x', 'a').write('asdf')\r\n>>> tf.io.gfile.GFile('x', 'r').read()\r\n'asdfxyzasdf'\r\n>>> \r\n```", "> Tested on `tf-nightly`, there is no bug:\r\n> \r\n> ```\r\n> Python 3.7.3 (default, Apr  3 2019, 05:39:12) \r\n> [GCC 8.3.0] on linux\r\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n> >>> import tensorflow as tf\r\n> >>> tf.__version__\r\n> '2.1.0-dev20191024'\r\n> >>> tf.io.gfile.GFile('x', 'w').write('xyz')\r\n> >>> tf.io.gfile.GFile('x', 'w').write('asdf')\r\n> >>> tf.io.gfile.GFile('x', 'r').read()\r\n> 'asdf'\r\n> >>> tf.io.gfile.GFile('x', 'a').write('xyz')\r\n> >>> tf.io.gfile.GFile('x', 'a').write('asdf')\r\n> >>> tf.io.gfile.GFile('x', 'r').read()\r\n> 'asdfxyzasdf'\r\n> >>> \r\n> ```\r\n\r\nhowever, \r\ntf.io.gfile.GFile could not open file when filename stored in tf.tensor while tf.function could\r\nthis is especially important inside tf.function "]}, {"number": 33638, "title": "Fix Crash When input_size Is an int", "body": "", "comments": ["@abduelhamit please include description,what issue or bug this will be resolving.", "Sure. Sorry, my mistake.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 x64 1809 (17763.805)\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.0.130/7.6.3.30\r\n- GPU model and memory: NVIDIA GeForce GTX 980 4GB\r\n\r\n**Code to reproduce the issue**\r\nI followed [tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb), added `tf.compat.v1` where necessary, and added `tf.compat.v1.disable_eager_execution()`.\r\n\r\n**Other info**\r\nWith this patch, I've followed the usage of `isinstance` in [this part of the code](https://github.com/tensorflow/tensorflow/blob/1059514f7c87fc4d7afe211349fa9d0d4ac30e3d/tensorflow/lite/experimental/examples/lstm/rnn_cell.py#L301):\r\n```python\r\n    if len(inputs_shape) != 2:\r\n      raise ValueError(\r\n          \"inputs_shape must be 2-dimensional, saw shape: %s\" % inputs_shape)\r\n    input_depth = (\r\n        inputs_shape[1]\r\n        if isinstance(inputs_shape[1], int) else inputs_shape[1].value)\r\n    if input_depth is None:\r\n      raise ValueError(\"Invalid inputs_shape, saw shape: %s\" % inputs_shape)\r\n```\r\n\r\n**Log**\r\n```\r\n2019-10-24 14:54:34.023448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\nWARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTraceback (most recent call last):\r\n  File \".\\tf_lite_rnn_lstm.py\", line 42, in <module>\r\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 114, in __init__\r\n    self.add(layer)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 196, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 847, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\", line 795, in call\r\n    return self.function(inputs, **arguments)\r\n  File \".\\tf_lite_rnn_lstm.py\", line 33, in buildLstmLayer\r\n    time_major=True)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\lite\\experimental\\examples\\lstm\\rnn.py\", line 266, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py\", line 916, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py\", line 2675, in while_loop\r\n    back_prop=back_prop)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py\", line 198, in while_loop\r\n    add_control_dependencies=add_control_dependencies)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py\", line 176, in wrapped_body\r\n    outputs = body(*_pack_sequence_as(orig_loop_vars, args))\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py\", line 884, in _time_step\r\n    (output, new_state) = call_cell()\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py\", line 870, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 847, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\", line 137, in call\r\n    inputs, states = cell.call(inputs, states, **kwargs)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\lite\\experimental\\examples\\lstm\\rnn_cell.py\", line 440, in call\r\n    if input_size.value is None:\r\nAttributeError: 'int' object has no attribute 'value'\r\n```", "Changes are merged internally waiting for auto-merge to happen."]}, {"number": 33637, "title": "TFLite C++ example for Raspberry Pi", "body": "Having followed this guide: https://www.tensorflow.org/lite/guide/build_rpi I end up with a static library called `libtensorflow-lite.a`. \r\n\r\nThere doesn't seem to be any instruction, documentation or examples available on how to actually use this for C++ on ARM. Some says to have a look at the `minimal.cc` example, but this does not include the static library nor does it include any compile instruction for ARM.\r\n\r\nCan someone provide a working example?", "comments": ["If you're looking for Makefile of minimal.cc, you can find it on https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/make/Makefile#L280"]}, {"number": 33636, "title": "MultiLabel Metric Support", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.15+ \r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n# Issue\r\nIn my comments on [issue](https://github.com/tensorflow/tensorflow/issues/28074) #28074 I outline that although it was suggested multi-label metric support exists, it does not. The reasons stands as\r\n\r\n1. The utility function `update_confusion_matrix_variables` is called without a user-exposed option resulting in the default argument `multi_label=False` to be used.\r\n\r\n2. `tf.keras.metrics.Metric` generally take in `y_pred` and `y_true`. Multi-label classification might use a loss like `tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)`, which applies a sigmoid during the loss calculation. This suggests that the user should not have the output (logits) of their model undergo a sigmoid transformation, as the loss would then be calculated using `sigmoid(sigmoid(...))`.  While a putative solution to is to make a multi-tailed model (one with and one without the sigmoid transformation), a collective guide on these more nuanced use cases does not exist.\r\n\r\npseudo-code of the solution\r\n\r\n```python\r\n\r\ninputs = ...\r\ny = tf.keras.layer.SomeLayer(...)(inputs)\r\ny = tf.keras.layer.SomeLayer(...)(y)\r\noutput = tf.sigmoid(y)\r\n\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs={\r\n    'logits': output,\r\n    'raw_out_for_loss': y\r\n})\r\n\r\nmodel.compile(\r\n    ...\r\n    metrics={\r\n        'logits': [MultiLabelMetric(...), ...], \r\n        'raw_out_for_loss': [],\r\n    }\r\n```\r\n\r\nThis issue does not arise in the [image segmentation guide](https://www.tensorflow.org/tutorials/images/segmentation#define_the_model) as the labels are no independent, so `sparse_categorical_crossentropy` loss is used. As reflected in the corresponding [`tf.keras.losses.SparseCategoricalCrossentropy` documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy), while multiple labels exists, the input is expect to confirm to a single label.\r\n\r\nMini-rant aside, even if a user does use a multi-tail model to address this, it does not get around that the current `tf.keras.metrics` do not expose the `multi_label` option, despite it existing in the underlying utility code.\r\n\r\n# Feature\r\n\r\nAt bare minimum expose `multi_label` option to relevant metrics (e.g. `Precision`). \r\n\r\nIt would also be nice if a transformation could be applied to the automatically feed in `y_pred` (e.g. sigmoid transformation), so that a multi-tailed model is not required. This might be done with an optional `transformation` argument which will accept a label.\r\n\r\n## other option\r\nmove multilabel metrics to separate classes, e.g. \r\n\r\n```python\r\nclass MultiLabelMacroSpecificity(tf.keras.metrics.Metric):\r\n    \r\n    def __init__(self, name='multi_label_macro_specificity', threshold=0.5, **kwargs):        \r\n        super(MultiLabelMacroSpecificity, self).__init__(name=name, **kwargs)\r\n        self.specificity = self.add_weight(name='mlm_spec', initializer='zeros')        \r\n        self.threshold       = tf.constant(threshold)\r\n\r\n        # replace this with tf confusion_matrix utils\r\n        self.true_negatives  = self.add_weight(name='tn', initializer='zeros')\r\n        self.false_positives = self.add_weight(name='fp', initializer='zeros')\r\n    \r\n    def update_state(self, y_true, y_pred):\r\n        \r\n        # Compare predictions and threshold.        \r\n        pred_is_pos  = tf.greater(tf.cast(y_pred, tf.float32), self.threshold)            \r\n        pred_is_neg  = tf.logical_not(tf.cast(pred_is_pos, tf.bool))\r\n        # |-- in case of soft labeling        \r\n        label_is_pos = tf.greater(tf.cast(y_true, tf.float32), self.threshold)                \r\n        label_is_neg = tf.logical_not(tf.cast(label_is_pos, tf.bool))\r\n        \r\n        self.true_negatives.assign_add(tf.reduce_sum(tf.cast(tf.logical_and(pred_is_neg, label_is_neg), tf.float32)))\r\n        self.false_positives.assign_add(\r\n            tf.reduce_sum(tf.cast(tf.logical_and(pred_is_pos, label_is_neg), tf.float32))\r\n        )\r\n        \r\n        tn = self.true_negatives\r\n        fp = self.false_positives\r\n        specificity = tf.div_no_nan(tn, tf.add(tn, fp))\r\n        self.specificity.assign(specificity)\r\n        return specificity\r\n    \r\n    def result(self):\r\n        return self.specificity        \r\n\r\n```\r\n\r\n\r\n**Will this change the current api? How?**\r\nYes, but barely.  Expose at least `multi_label` on relevant metrics and maybe add / expose a `transformation` argument.\r\n\r\n\r\n\r\n**Who will benefit with this feature?**\r\nIf bare minimum is done, those requiring multi-label metrics but do not which to write their own function / subclass `tf.keras.metric.Metric`.\r\n\r\nIf `transformation` is also added to all metrics, then anyone using tf metrics could make existing metrics work with minimum overhead instead of redesigning their model to meet metric requirements.\r\n\r\n**Any Other info.**\r\n\r\n\r\n", "comments": ["Adding this to contributions welcome list. Please feel free to add the `multi-label` option to the APIs and send me a PR.", "@pavithrasv  see above\r\n", "@SumNeuron  @pavithrasv Are the functionality for multi-label precison, recall, f1-score, etc are added in tensorflow main code base. \r\nThis is also a issue in tensorflow addon repo. \r\nPlease confirm.\r\nThank you", "@Saqhas the code is on a branch in the add-on repo, there is an issue with readme?\r\nhttps://github.com/tensorflow/addons/pull/976", "@SumNeuron Got it.\r\nSo it is still pending to be merged in the main code in the addons repo. But is yet to be merged.\r\nIs there any development still pending in this feature? I want to help\r\nAlso why is this feature only going in the addons repo because addons dont have precision, recall and f1score in the first place. Precision and recall are present here. So shouldn't it this feature bee added to both the repos. So that all the metrics present in tensorflow and addons can behave appropriately in both single and multi label scenarios. Like the multilabel class idea you suggested. ", "@Saqhas honestly, no idea. I also just wanted to help but the process is... convoluted. As you can see here (https://github.com/tensorflow/tensorflow/pull/36315#issuecomment-579859684) following the contributing guideline I tried to first add to main repo, but was the later instructed to go to add ons, where it has been stuck. ", "Ya i saw that but this is not addition of a new metrics. It is adding feature in the existing metrics. And it is used extensively.\r\nCurrently for multilabel there is hardly any metrics that can show the true performance of the model.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 33635, "title": "Adding regularization losses to models after they are built does not show up in model.losses", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nFor a prebuilt model, for example, keras_applications models and other models if they are already built, manually adding regularization to layers does not show up in model.losses\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint('TensorFlow', tf.__version__)\r\n\r\n'''\r\nCase 1: Simple model without any regularization\r\n'''\r\ninput_layer = tf.keras.Input(shape=[10])\r\nx = tf.keras.layers.Dense(units=16, activation='relu')(input_layer)\r\nx = tf.keras.layers.Dense(units=16, activation='relu')(x)\r\noutput_layer = tf.keras.layers.Dense(units=4, activation=None)(x)\r\nmodel_a = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\r\n\r\nassert model_a.losses == []\r\n\r\n\r\n'''\r\nCase 2: Simple model with regularization added\r\n        during layer creation\r\n'''\r\ninput_layer = tf.keras.Input(shape=[10])\r\nx = tf.keras.layers.Dense(units=16, activation='relu',\r\n                          kernel_regularizer=tf.keras.regularizers.l2(l=1e-5))(input_layer)\r\nx = tf.keras.layers.Dense(units=16, activation='relu', \r\n                          kernel_regularizer=tf.keras.regularizers.l2(l=1e-5))(x)\r\noutput_layer = tf.keras.layers.Dense(units=4, activation=None, \r\n                                     kernel_regularizer=tf.keras.regularizers.l2(l=1e-5))(x)\r\nmodel_b = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\r\n\r\nassert model_b.losses != []\r\n\r\n\r\n'''\r\nCase 3: For a prebuilt model, for example, keras_applications models\r\n        and other models if they are already built, manually adding regularization\r\n        to layers does not show up in model.losses\r\n'''\r\ninput_layer = tf.keras.Input(shape=[10])\r\nx = tf.keras.layers.Dense(units=16, activation='relu')(input_layer)\r\nx = tf.keras.layers.Dense(units=16, activation='relu')(x)\r\noutput_layer = tf.keras.layers.Dense(units=4, activation=None)(x)\r\nmodel_c = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\r\n\r\nfor layer in model_c.layers:\r\n    if hasattr(layer, 'kernel_regularizer'):\r\n        setattr(layer, 'kernel_regularizer', tf.keras.regularizers.l2(l=1e-5))\r\n        \r\nfor layer in model_c.layers:\r\n    if hasattr(layer, 'kernel_regularizer'):\r\n        assert getattr(layer, 'kernel_regularizer').l2 == np.array([1e-5], dtype='float32')\r\n        \r\nassert model_c.losses == []\r\n```", "comments": ["The only workaround i found was this \r\n```\r\nmodel = tf.keras.applications.ResNet50()\r\nfor layer in model.layers:\r\n    if hasattr(layer, 'kernel_regularizer'):\r\n        setattr(layer, 'kernel_regularizer', tf.keras.regularizers.l2(l=1e-5))\r\n        \r\nfor layer in model.layers:\r\n    if hasattr(layer, 'kernel_regularizer'):\r\n        assert getattr(layer, 'kernel_regularizer').l2 == np.array([1e-5], dtype='float32')\r\n        \r\nassert model.losses == []\r\nmodel.save_weights('resnet50.h5', save_format='h5')\r\n\r\nnew_model = tf.keras.models.model_from_json(model.to_json())\r\n# new_model would have random weights\r\nnew_model.load_weights('resnet50.h5')\r\nassert new_model.losses != [] #works fine\r\n```\r\nIs there a cleaner way to do this?\r\n@martinwicke ", "I could reproduce the issue with Tf 2.0. \r\nPlease see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/c48b5c0f69ac9428879296a0ffe73440/untitled222.ipynb). Thanks!", "@gowthamkpr any updates?\r\n", "In general, we wouldn't advise checking and setting the regularizer manually. Please use the add_loss API instead: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_loss", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33635\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33635\">No</a>\n"]}, {"number": 33634, "title": "Tensorflow lite dll build failed on Windows", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- TensorFlow installed from (source or binary): source (master)\r\n- TensorFlow version: 2.0 (master)\r\n- Python version: 3.8.0\r\n- Installed using virtualenv? pip? conda?: VirtualBoxVM\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): Visual Studio Build Tools 201\r\n\r\n**Describe the problem**\r\n\r\nI have a code base successfully running on Linux/MacOS/Android & iOS.\r\n\r\nNow I need to run tflite model inference under Windows system.\r\nFor this, I need to compile a tensorflowlite dll for Windows (I've not found any precompiled version)\r\n\r\nUnfortunately, the build fails at the link step... (see logs below).\r\n\r\nApparently, even if the build fails, a libtensorflowlite.so file is stil generated (and is indeed a dll), but it contains no exported symbols, so I cannot use it.\r\nAfter checking at the tensorflow/lite/BUILD file it appears that no .def file is generated for the tflite Windows link step, as it is done for tensorflow.dll (seen in tensorflow/BUILD), which explains why no symbols are exported.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nIn Windows command prompt (Run as Administrator):\r\n\r\npython configure.py\r\nbazel build -s tensorflow/lite:libtensorflowlite.so\r\n\r\n**Any other info / logs**\r\n\r\nC:\\Users\\soule\\Downloads\\tensorflow> bazel build -s tensorflow/lite:libtensorflowlite.so\r\n[...]\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/BuildTools/VC/Tools/MSVC/14.23.28105/bin/HostX64/x64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -s -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params /OPT:ICF /OPT:REF\r\nINFO: From Linking tensorflow/lite/libtensorflowlite.so:\r\nLINK : warning LNK4044: unrecognized option '/s'; ignored\r\nERROR: C:/users/soule/downloads/tensorflow/tensorflow/lite/BUILD:442:1: output 'tensorflow/lite/libtensorflowlite.so.if.lib' was not created\r\nERROR: C:/users/soule/downloads/tensorflow/tensorflow/lite/BUILD:442:1: not all outputs were created or valid\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nINFO: Elapsed time: 920.482s, Critical Path: 84.10s\r\nINFO: 235 processes: 235 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nC:\\Users\\soule\\Downloads\\tensorflow> dumpbin /exports bazel-out\\x64_windows-opt\\bin\\tensorflow\\lite\\libtensorflowlite.so\r\nMicrosoft (R) COFF/PE Dumper Version 14.23.28106.4\r\nCopyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\n\r\nDump of file bazel-out\\x64_windows-opt\\bin\\tensorflow\\lite\\libtensorflowlite.so\r\n\r\nFile Type: DLL\r\n\r\n  Summary\r\n\r\n        1000 .data\r\n        1000 .pdata\r\n        1000 .rdata\r\n        1000 .reloc\r\n        2000 .text\r\n        1000 _RDATA\r\n", "comments": ["I confirm the issue is caused by the fact that no .def file is provided in the case of tflite build for Windows.\r\n\r\nI successfully build a tflite dll for Windows by calling link.exe by myself and providing a custom .def file I created.\r\n\r\nA block similar to the following one (found in tensorflow/BUILD) should be added to tensorflow/lite:libtensorflowlite.so target:\r\n\r\n    # add win_def_file for tensorflow_cc\r\n    win_def_file = select({\r\n        # We need this DEF file to properly export symbols on Windows\r\n        \"//tensorflow:windows\": \":tensorflow_filtered_def_file\",\r\n        \"//conditions:default\": None,\r\n    }),\r\n", "Good catch! Feel free to propose a PR, I'd be happy to approve.", "@jdduke Can you please elaborate on the required change? when I add what @twkx mentioned in `tensorflow/lite/BUILD` I get the error:\r\n```\r\nunexpected keyword 'win_def_file' in call to tflite_cc_shared_object\r\n```\r\nWhat am I doing wrong?\r\nThx.", "@ValYouW This is normal that you have this error.\r\n\r\nIn order to make it fully work you must copy all the tensorflow BUILD parts that generate the .def file, and customize it for tensorflow lite dll. I'm not an expert in Bazel, so I could not make it myself, this is why I didn't push a proper PR.\r\n\r\nWhat I did is to generate manually the .def file based on the missing symbols I had in my own project, and used link.exe to build the dll from command line.\r\n\r\nBy the way, @jdduke the dll I obtained is not SSE optimized, so it is very slow. I have started to check why NEON2SSE.h is not used in windows build (as in MacOS build) but not yet found. I move on other tasks right now, but I have planned to make it work some day.", "> By the way, @jdduke the dll I obtained is not SSE optimized, so it is very slow. I have started to check why NEON2SSE.h is not used in windows build (as in MacOS build) but not yet found. I move on other tasks right now, but I have planned to make it work some day.\r\n\r\nOK, thanks for checking, there may be some tweaks we need to make in our build config to get this working (sadly we don't have exhaustive internal tests for Windows internally).\r\n\r\n", "@twkx Thanks. Too complicated, I'll skip windows for now... :)", "I think this might be resolved with https://github.com/bazelbuild/bazel/pull/9976. I'm curious if adding `        features = [\"windows_export_all_symbols\"],` to [this build rule](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/build_def.bzl#L148) resolves the problem temporarily?", "@jdduke I confirm adding this fixes the issue and creates a working dll (just rename the .so as .dll)! Thanks a lot. \r\n\r\nI'll check for neon asap.", "Btw, I'm landing a change shortly that should automatically generate the appropriate lib name based on the target platform (e.g., tensorflowlite.dll, libtensorflowlite.so, libtensorflowlite.dylib). I'll also go ahead and land the `features = [\"windows_export_all_symbols\"]` for now. Thanks for your patience!", "Am I understanding correctly that if I build bazel from master to get that latest fix, then add `features = [\"windows_export_all_symbols\"],` to tensorflow/lite/build_def.bzl like:\r\n\r\ndef tflite_cc_shared_object(\r\n        name,\r\n        copts = tflite_copts(),\r\n        linkopts = [],\r\n        linkstatic = 1,\r\n        deps = [],\r\n        visibility = None,\r\n        per_os_targets = False,\r\n        tags = None):\r\n    \"\"\"Builds a shared object for TFLite.\"\"\"\r\n    tf_cc_shared_object(\r\n        name = name,\r\n        copts = copts,\r\n        linkstatic = linkstatic,\r\n        linkopts = linkopts + tflite_jni_linkopts(),\r\n        framework_so = [],\r\n        deps = deps,\r\n        visibility = visibility,\r\n        tags = tags,\r\n        per_os_targets = per_os_targets,\r\n\t\tfeatures = [\"windows_export_all_symbols\"],\r\n    )\r\n\r\nI should then be able to compile tflite on windows using the bazel version built from source with: bazel build //tensorflow/lite:libtensorflowlite.so\r\n\r\nIs this correct? Or will I need to do other steps to get it working?\r\nThanks", "For anyone who is interested I got it built on windows using the 2.1.0 release of bazel, adding in the features = [\"windows_export_all_symbols\"], to tensorflow/lite/build_def.bzl as before and compiling with bazel-2.1.0.exe build //tensorflow/lite:libtensorflowlite.so", "From a master checkout, could you try doing:\r\n\r\n```\r\nbazel build -c opt //tensorflow/lite:tensorflowlite\r\n```\r\n\r\nThat should produce a proper .dll, though I want to make sure it's properly symbolized. Thanks!", "I pulled from master on tensorflow. Then tried:\r\n`bazel build -c opt //tensorflow/lite:tensorflowlite`\r\n\r\nThis built the .lib and .dll, however the files were tiny ~2KB and when I tried to use them I got error with unresolved symbols. I then added the following after line 168 in build_def.bzl and rebuilt in the same way:\r\n`features = [\"windows_export_all_symbols\"],`\r\n\r\nThis produced the .lib that I could link against. However my code failed at runtime. (I'm not sure why yet).", "The same code worked fine when using an older commit on tensorflow:master ~15 Jan, and adding the extra line to build_def.bzl then compiling with bazel build //tensorflow/lite:libtensorflowlite.so (2.1.0 release of bazel). However I did have to keep the generated library named as .so.", "Is it possible to build TFLite for Windows and ARM(32/64)? I am asking because it would be useful to run it on the HoloLens 2 which is based on Universal Windows Platform (UWP) and has an ARM chip set.", "@chrisagblake \r\n\r\nI built tensorflowlite.dll.if.lib and tensorflowlite.dll from master branch with modified build_def.bzl.\r\nThe generated  library files work in my project  on Visual Studio 2017.\r\nDo you happen to forget to copy tensorflowlite.dll?\r\n\r\nI just arranged the way to generate tensorflowlite.dll which actually works at the moment.\r\nhttps://gist.github.com/iwatake2222/3017d9ac3112b27cc9f5011ece993009\r\n", "I'm fairly sure I copied the .dll across into the runtime folder correctly. However I could have made some other mistake. I'll give it another go later when I get a chance.", "I think I might be experiencing the same problem as @chrisagblake\r\n\r\n- Pull nightly (TF 2.2.0-dev20200308)\r\n- Add `features = [\"windows_export_all_symbols\"],` to `tensorflow/lite/build_def.bzl`\r\n- `bazel build -c opt //tensorflow/lite:tensorflowlite`\r\n- This gives me `tensorflowlite.dll.if.lib` (56.4 MB) and `tensorflowlite.dll` (13.6 MB)\r\n- I can successfully link the `.dll` using the `.dll.if.lib` in CMake as proposed by @iwatake2222\r\n- No errors (not during build, not during runtime)\r\n- Code crashes at runtime. More specifically, at `float* input_tensor = interpreter->typed_input_tensor<float>(0);`\r\n(the FlatBufferModel is able to load properly before this)\r\n\r\nThe same code works perfectly on Ubuntu 18.04 using `libtensorflowlite.so` compiled on a Ubuntu system. Any updates on this issue?", "In general, if you're using the C++ API, you need to be extremely careful that your client library (referencing TFLite code) is built with identical build/link options as the TFLite library itself.\r\n\r\nIn general, prefer using the tensorflow/lite/c shared library to avoid ABI and stdlib incompatibilities.", "@Lotte1990 double check you use the exact same .h files in your project that the ones used to compile the library itself (extract those .h from the cloned tflite repository to be sure).\r\n\r\nAlso, as @jdduke said, be carefull to use the same stdlib implementation in your project than the one used in the tensorflowlite library.\r\n", "Thanks for the quick reply. I'm using the same git checkout for (1) compiling tensorflowlite.dll (2) the header files (3) exporting the .tflite file. I'm no expert regarding the library stuff. I tried adding different lines in different combinations to CMakeLists.txt, such as `set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -stdlib=libc++\")` and `set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -stdlib=libc++ -lc++abi\")`, but without any success.\r\nI'm not getting any error, the program just crashes, which makes it quite hard to figure out what the problem is. It would be great if you could point me in the right direction.\r\nAs I mentioned before, the same code works perfectly on Ubuntu 18.04 using libtensorflowlite.so compiled on a Ubuntu system, so I have a very strong feeling that this is indeed a library issue...", "@Lotte1990 \r\nDo you use MSVC and run your program with Debug configuration?\r\nIf so, you can try Release configuration. I actually experienced unexpected crash with Debug configuration on Visual Studio 2017. With Release configuration, my program worked well.", "I use CMake to generate a .sln file that I open in MSVC. Then I press `right-click --> build` on the solution. Nothing else. I tried `set(CMAKE_BUILD_TYPE \"Debug\")` and `set(CMAKE_BUILD_TYPE \"Release\")` in CMakeLists.txt, but the result is the same. Any other suggestions?", "You can select that configuration using UI in Visual Studio (I'm not sure CMAKE_BUILD_TYPE makes an effect to configuration for Visual Studio). Please find a drop box just under a menu bar in Visual Studio, and select Release. Also make sure you use x64 architecture (not x86). You can also find a drop box to select architecture next to the drop box to select configuration. It's also located under the menu bar in Visual Studio.", "Debug config was indeed the problem. When I compile everything with release config, it works great. As you suspected, Visual Studio completely ignores CMAKE_BUILD_TYPE.", "> \r\n> \r\n> You can select that configuration using UI in Visual Studio (I'm not sure CMAKE_BUILD_TYPE makes an effect to configuration for Visual Studio). Please find a drop box just under a menu bar in Visual Studio, and select Release. Also make sure you use x64 architecture (not x86). You can also find a drop box to select architecture next to the drop box to select configuration. It's also located under the menu bar in Visual Studio.\r\n\r\nIs there any way to build tensorflowlite.dll for windows X86 support?", "I don't think 32-bit (x86) Windows build are supported directly. There is some additional [information here](https://stackoverflow.com/questions/53291191/compile-a-c-project-wit-bazel-for-x86-32-bit).", "@Lotte1990 How exactly did you use CMAKE to create an .sln file?", "I used CMake to create an .sln file for my own project that uses TensorFlow Lite, not for TensorFlow Lite itself. For TensorFlow Lite, unfortunately, you still need to use Bazel.", "I see. Thank you.", "@jdduke I see that `build_def.bzl` on `master` still don't have `features = [\"windows_export_all_symbols\"]` is there any problem with this \"fix\"? it seems to be working. Also any idea why the dll is quite big (~15MB) while `so` for Android are fairly small? thx", "Thanks for flagging, I'll go ahead and land this for the C++ .dll target.\r\n\r\nI'm curious if you're able to use the C API/dll in lite/c:tensorflowlite_c? That should have finer grained symbol export, has a simpler header export story and also avoids many of the C++ ABI issues with compatibility.", "@jdduke Thanks, I see this fix hasn't made it to v2.2.0 :(\r\nYes, I was able to build the C dll and use it in my object detector, conversion was pretty much straight forward and the dll size is only ~1.4MB.\r\nThx!", "> @jdduke Thanks, I see this fix hasn't made it to v2.2.0 :(\r\n\r\nYeah, unfortunately the 2.2 branch was only accepting P0/P1 issues at that point. Should be in the next release.", "Hello, I got the latest version of TF and tensorflow/lite/build_def.bzl seems to be slightly different than listed above.  I try to build and it says there are multiple values for argument features.\r\n\r\nERROR: C:/temp/tensorflow/tensorflow/lite/build_def.bzl:157:24: Traceback (most recent call last):\r\n        File \"C:/temp/tensorflow/tensorflow/lite/BUILD\", line 610\r\n                tflite_cc_shared_object(<5 more arguments>)\r\n        File \"C:/temp/tensorflow/tensorflow/lite/build_def.bzl\", line 157, in tflite_cc_shared_object\r\n                tf_cc_shared_object(name = name, <7 more arguments>)\r\ntf_cc_shared_object() got multiple values for keyword argument 'features'\r\n\r\nAnything obviously wrong?  Thanks.\r\n\r\ndef tflite_cc_shared_object(\r\n        name,\r\n        copts = tflite_copts(),\r\n        linkopts = [],\r\n        linkstatic = 1,\r\n        per_os_targets = False,\r\n        **kwargs):\r\n    \"\"\"Builds a shared object for TFLite.\"\"\"\r\n    tf_cc_shared_object(\r\n        name = name,\r\n        copts = copts,\r\n        linkstatic = linkstatic,\r\n        linkopts = linkopts + tflite_jni_linkopts(),\r\n        framework_so = [],\r\n        per_os_targets = per_os_targets,\r\nfeatures = [\"windows_export_all_symbols\"],        \r\n        **kwargs,\r\n\r\n    )", "@OmarJay1 Check commit fa32891 to see the correct files, it should work on \"master\".\r\nRegardless, as jdduke recommended above, you can try and use the C library, it worked pretty well for me (made a [youtube](https://youtu.be/dox1ZkFP-f4) video showing how to build and use)", "Thanks ValYouW, I watched your video and I was doing something really dumb.  I wasn't running python configure.py.  "]}, {"number": 33633, "title": "Build Tensorflow with Bazel not working", "body": "I would like build tensorflow with Bazel to use the compiled libary in c++. I run the following commands in cmd.\r\n```\r\nC:\\Users\\Furkan\\Desktop\\tensorflow-1.14.0>bazel build --config=opt\r\nWARNING: Usage: bazel build <options> <targets>.\r\nInvoke `bazel help build` for full description of usage and options.\r\nYour request is correct, but requested an empty set of targets. Nothing will be built.\r\nINFO: Build option --define has changed, discarding analysis cache.\r\nINFO: Analyzed 0 targets (0 packages loaded, 0 targets configured).\r\nINFO: Found 0 targets...\r\nINFO: Elapsed time: 0.124s, Critical Path: 0.01s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\n```\r\nAfter that they create the folders in tensorflow, but the folders are empty.. What I do wrong??\r\n\r\nbazel: 0.25.2\r\ntensorflow: 1.14.0", "comments": ["You are not passing a target to build. Please consult Bazel documentation and the [TF Install from source page](https://www.tensorflow.org/install/source_windows)\r\n\r\nClosing as this is not a TF issue"]}, {"number": 33632, "title": "[TF 2.0 API docs] expanding the tf.keras.activations.relu", "body": "Addressing issue #26211. Feedback would be greatly appreciated, and will make any revisions necessary.", "comments": ["```\r\nFAIL: Found 1 non-whitelisted pylint errors:\r\ntensorflow/python/keras/activations.py:220: [C0301(line-too-long), ] Line too long (82/80)\r\n```\r\n\r\nPlease fix.", "@Williscool13 can you please check this failures here https://source.cloud.google.com/results/invocations/d5139b8a-2820-46ed-8563-f604013d9fda/log", "```\r\nFile \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/keras/activations.py\", line 204, in tensorflow.python.keras.activations.relu\r\nFailed example:\r\n    tf.keras.activaions.relu(foo, max_value=5).numpy()\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"/usr/lib/python3.6/doctest.py\", line 1330, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest tensorflow.python.keras.activations.relu[3]>\", line 1, in <module>\r\n        tf.keras.activaions.relu(foo, max_value=5).numpy()\r\n    AttributeError: module 'tensorflow.python.keras.api._v2.keras' has no attribute 'activaions'\r\n```\r\n\r\nplease fix"]}, {"number": 33631, "title": "tf.data.Dataset.from_tensor_slices creates infinite loop in https://www.tensorflow.org/tutorials/load_data/pandas_dataframe ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow tutorial \r\n- TensorFlow installed from: Source\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behaviour**\r\n\r\nAttempting to create a batched dataset to train an RNN-LSTM using the logic from the [TensorFlow Time Series Forecasting tutorial ](https://www.tensorflow.org/tutorials/structured_data/time_series) and the [load Pandas.DataFrame logic](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe).\r\n\r\nWhen I execute these lines on my Pandas Dataframe of shape (1661, 5):\r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\r\nfor feat, targ in dataset.take(5):\r\n  print ('Features: {}, Target: {}'.format(feat, targ))\r\n\r\n```\r\nThe loop runs as an infinite loop and does not produce the same out as that in the tutorial. The only difference with my dataframe is that it has a DateTimeIndex.\r\n\r\nThe output \r\n**Describe the expected behaviour**\r\n\r\nI would expect a batch of size 5 as per dataset.take(5) in the format below:\r\n\r\nFeatures: [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\r\n   2. ], Target: 0\r\n\r\n\r\n**Code to reproduce the issue**\r\n`df.head(10)`\r\n\r\nOut:[1]\r\n\tindex\tcol_1         col_2        col_3       col_4        col_5\r\n2016-10-03\t0.017647\t0.000000\t0.048040\t0.334455\t0.132643\r\n2016-10-04\t0.017647\t0.000925\t0.047372\t0.334455\t0.174480\r\n2016-10-05\t0.017647\t0.001849\t0.046696\t0.334455\t0.142559\r\n2016-10-06\t0.017647\t0.002774\t0.046011\t0.334455\t0.148387\r\n2016-10-07\t0.017647\t0.003699\t0.045317\t0.334455\t0.142472\r\n2016-10-10\t0.017647\t0.004624\t0.044615\t0.333301\t0.129686\r\n2016-10-11\t0.017647\t0.005548\t0.043905\t0.331875\t0.120031\r\n2016-10-12\t0.017647\t0.006473\t0.043185\t0.324570\t0.120031\r\n2016-10-13\t0.017647\t0.007398\t0.042458\t0.324327\t0.105506\r\n2016-10-14\t0.017647\t0.008323\t0.041721\t0.341778\t0.129425\r\n\r\n`\r\ntarget = final_df.pop('col_1')\r\ndataset = tf.data.Dataset.from_tensor_slices((final_df.values, final_df.values))\r\nfor feat, targ in dataset.take(5):\r\n  print ('Features: {}, Target: {}'.format(feat, targ))\r\n`\r\nOut[2]:\r\n\r\nFeatures: Tensor(\"IteratorGetNext:0\", shape=(4,), dtype=float64), Target: Tensor(\"IteratorGetNext:1\", shape=(4,), dtype=float64)\r\nFeatures: Tensor(\"IteratorGetNext_1:0\", shape=(4,), dtype=float64), Target: Tensor(\"IteratorGetNext_1:1\", shape=(4,), dtype=float64)\r\ntarget: Tensor(\"IteratorGetNext_9:1\", shape=(4,), dtype=float64)\r\n\r\nOutput in an infinite loop.\r\n", "comments": ["@cmp1 \r\n\r\nCan you please share sample data and simple standalone code to reproduce the issue in our environment.It will help us localizing the issue faster.Thanks!", "[df.xlsx](https://github.com/tensorflow/tensorflow/files/3769609/df.xlsx)\r\n\r\nHi - thank you for getting back to me. I have attached a sample from the dataframe.  The code I followed is exactly that from the RNN tutorial. Previously I have used generator functions to train RNNs however this function looks like it could substantially reduce the time creating these:\r\n\r\n```\r\n\r\n# separate label from df\r\ntarget = df.pop('label')\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\r\n\r\n# loops over entire dataset and does not produce tensors of size n features, target \r\nfor feat, targ in dataset.take(5):\r\n  print ('Features: {}, Target: {}'.format(feat, targ))\r\n\r\n# runs ok\r\ntrain_dataset = dataset.shuffle(len(df)).batch(1)\r\n\r\n# define model\r\ndef get_compiled_model():\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(10, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='relu'),\r\n    tf.keras.layers.Dense(1, activation='sigmoid')\r\n  ])\r\n\r\n  model.compile(optimizer='adam',\r\n                loss='mean_squared_error',\r\n                metrics=['mae', 'acc'])\r\n  return model\r\n\r\nmodel = get_compiled_mo\r\n\r\nmodel = get_compiled_model()\r\n\r\n# returns error\r\nmodel.fit(train_dataset, epochs=15)\r\n\r\n```\r\n\r\nThank you!\r\n\r\n\r\n\r\n\r\n", "@cmp1 \r\nCan you please enable the eager execution by `tf. enable_eager_execution()`at the start of the code and check. After enabling eager execution i am not seeing infinite loop in the output. Thanks!\r\n", "Ah I forgot this - that has fixed it! thank you so much!\n\nOn Friday, 25 October 2019, ravikyram <notifications@github.com> wrote:\n\n> @cmp1 <https://github.com/cmp1>\n> Can you please enable the eager execution by tf. enable_eager_execution()at\n> the start of the code and check. After enabling eager execution i am not\n> seeing infinite loop in the output. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33631?email_source=notifications&email_token=AG4N7QEOC2DU7X2RYTR4NJDQQK6QJA5CNFSM4JD5PHZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECHZZ7Q#issuecomment-546282750>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AG4N7QEPJ5KQW7RIIMS2SUDQQK6QJANCNFSM4JD5PHZQ>\n> .\n>\n", "I am closing this issue since the query is been resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33631\">No</a>\n"]}, {"number": 33630, "title": "MKL is not enabled after building from source", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n\r\n**Describe the problem**\r\nMKL is not enabled after building from source with -config=mkl. I have checked with the function tensorflow.pywrap_tensorflow.IsMklEnabled(). It returns \"False\".\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed the installation guide on https://www.tensorflow.org/install/source. I build with:\r\n`bazel build -c opt --copt=-march=native --copt=-mfpmath=both --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nHow can I build correct with mkl enabled. Thank you in advance", "comments": ["TensorFlow version: 1.15\r\n```\r\nyes  \"\" | python configure.py\r\nbazel build -c opt --copt=-march=native --copt=-mfpmath=both --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nTest Code\r\n```\r\nimport tensorflow\r\nprint(tensorflow.pywrap_tensorflow.IsMklEnabled())\r\n```\r\nwhich returns true.\r\n", "@leslie-fang-intel Thanks, after several tries I finally built with MKL.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33630\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33630\">No</a>\n"]}, {"number": 33629, "title": "Is it possible to provide a demo of the uvccamera object detection in android by tensorflow lite ? ", "body": "this demo \"https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\"\r\nis android camera , i  need to use in uvccamera in android to detection object , i copy code to my uvccamera in android project , it is not detection right , so Is it possible to provide a demo of the uvccamera object detection in android by tensorflow lite ? ", "comments": ["This is really more of a StackOverflow question, can you please post there? As long as you can convert your camera image into a Bitmap, the code should largely remain the same. You'll just want to make sure you're doing the right cropping/scaling, and that you convert to RGB format.", "thanks you "]}, {"number": 33628, "title": "neural network", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33628) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 33627, "title": "TF 1.14 Python CPU single thread configuration", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution**: Ubuntu 18.04\r\n- **TensorFlow installed from**: pip2 install intel-tensorflow==1.14.0\r\n- **TensorFlow version (use command below)**: 1.14\r\n- **Python version**: 2.7\r\n\r\n### Describe the problem\r\nI'm going to participate in submissions limited to use CPU single thread. My implementation is in Python and I found that when construct tf.Session, a threadpool is created. However, I can't set it to single threaded even with the following suggested configuration:\r\n```python\r\nsess = tf.Session(config=tf.ConfigProto(\r\n    intra_op_parallelism_threads=1,\r\n    inter_op_parallelism_threads=1,\r\n    device_count = {'CPU': 1}\r\n))\r\n```\r\nHow can I reach single threaded?  \r\nThank you !!\r\n\r\n", "comments": ["Are you using Intel MKL optimized TF version? Also can you try with following snippet;\r\n```python\r\nimport tensorflow as tf\r\nconfig = tf.ConfigProto(intra_op_parallelism_threads=1, \r\n                        inter_op_parallelism_threads=1, \r\n                        allow_soft_placement=True,\r\n                        device_count = {'CPU': 1})\r\nsession = tf.Session(config=config)\r\n```", "Yes, I am using Intel MKL optimized TF version with pip2 install intel-tensorflow==1.14.0.\r\nThe suggestion above doesn't work either.", "Could you please try export OMP_NUM_THREADS=1 to restrict the MKL threadpool to 1", "I had tried and it didn't work either. ", "to test single thread please use \"numactl\" or \"taskset\" to confine the application (TF) to one core. And then set inter/intra op/OMP_NUM_THREADS to 1 ", "I also encountered the same problem, did you solve it?", "No.", "@wei-v-wang Can you please explain your answer with a simple example? \"to test single thread please use \"numactl\" or \"taskset\" to confine the application (TF) to one core. And then set inter/intra op/OMP_NUM_THREADS to 1\"", "Yes @fisakhan please see below for an example: \r\nexport OMP_NUM_THREADS=1; numactl --physcpubind=2 --membind=0  python app.py --inter_op_parallelism_threads=1 --intra_op_parallelism_threads=1 \r\nThe above runs the app on physical CPU 2 (i.e. just the 3rd core). \r\n", "@deHsien Did you solve the problem? I'm facing the same problem.", "Thanks @wei-v-wang but I need to control the threads from within a c++ program that will be called by another party (NIST FRVT). In such a setting, the solution you provided doesn't work.\r\nhttps://github.com/tensorflow/tensorflow/issues/42510\r\nhttps://stackoverflow.com/questions/60206113/how-to-stop-tensorflow-from-multi-threading", "@fisakhan  Ok, it seems a complex task. I have two more comments:\r\n1) Build TF from source but with --config=mkl_threadpool command, with this you do not need to worry about setting OMP_NUM_THREADS at all. *NOTE this needs to be with relatively newer version of TF*  \r\n2) consider invoking in your C++ program the thread affinity APIs, https://man7.org/linux/man-pages//man3/pthread_setaffinity_np.3.html and somehow confine the TF to the specified set of cores? ", "@wei-v-wang Once I Build TF 2.3 (latest version) from source with --config=mkl_threadpool command, what/how should I change my following piece of code to restrict the number of threads to 1? \r\nAs soon as I load the model the number of threads reaches 8 ((using \"top -H -b -n1 | grep example | wc -l\" command on linux).\r\n\r\nSome interesting issues related to the same problem (links to more related issue on that page): https://github.com/tensorflow/tensorflow/issues/35387\r\n```\r\nStatus LoadGraph(string graph_file_name, std::unique_ptr<tensorflow::Session>* session) {\r\n  tensorflow::GraphDef graph_def;\r\n  Status load_graph_status =\r\n      ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);\r\n  if (!load_graph_status.ok()) {\r\n    return tensorflow::errors::NotFound(\"Failed to load compute graph at '\",\r\n                                        graph_file_name, \"'\");\r\n  }\r\n  \r\n  // initialize the number of worker threads\r\n    tensorflow::SessionOptions options;\r\n    tensorflow::ConfigProto & config = options.config;\r\n    config.set_inter_op_parallelism_threads(1);\r\n    config.set_intra_op_parallelism_threads(1);\r\n    config.set_use_per_session_threads(false); \r\n\r\n  session->reset(tensorflow::NewSession(options));\r\n  Status session_create_status = (*session)->Create(graph_def);\r\n  if (!session_create_status.ok()) {\r\n    return session_create_status;\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n```", "You have already set inter/intra to 1, so I don't expect any other changes.  Adding @Srini511 for help. \r\n@Srini511  Please see the unexpected behaviour with mkl_threadpool above. @fisakhan  wanted to use single thread TF but are getting 8 threads even though inter/intra ops are set to 1.  ", "@fisakhan I have the same problem, have you found a solution?", "@aliqing0122 Not yet. I'm still struggling to find its solution. I also tried suggestions provided at the following link but all in vain. Can you please share the solution if you find?\r\n#42510\r\n#33627\r\nusnistgov/frvt#12\r\ntensorflow/models#3176\r\n#13853\r\nhttps://stackoverflow.com/questions/47548145/understanding-tensorflow-inter-intra-parallelism-threads\r\nhttps://stackoverflow.com/questions/60206113/how-to-stop-tensorflow-from-multi-threading\r\nhttps://stackoverflow.com/questions/34389945/changing-the-number-of-threads-in-tensorflow-on-cifar10\r\nhttps://stackoverflow.com/questions/39035639/importing-tensorflow-spawns-threads", "@aliqing0122  @wei-v-wang @Srini511  Do you agree with the reason given at the following link? It explains that TensorFlow cannot be a single threaded. \r\nhttps://stackoverflow.com/questions/48696900/why-tensorflow-creates-so-many-cpu-threads", "I had migrated my models to mxnet for the single threaded constraint.", "@deHsien Thanks for your response. I'm also migrating my code to either mxnet or caffe. It is not possible to have a single threaded TensorFlow.", "@fisakhan\r\n`config.set_inter_op_parallelism_threads(1); config.set_intra_op_parallelism_threads(1);` can't work well because there would be many python libs to be executed in a AI script. It's possible other libs will enable multiple threads.\r\n\r\nAnd above two parameters only limit the tensroflow train/infer. They can't limit other functions in Tensorflow like load model/dataset.\r\n\r\nThere is simple method to limit one thread, like:\r\nnumactl -C 1 python train.py\r\n\r\nIt sets the python running on the No.1 CPU core only.\r\nIf you use 'htop', you will see only No.1 CPU core is busy.", "@NeoZhangJianyu There are no other python libs that are under execution. I have verified that session creation and session run in TensorFlow are generating all the threads. We have already discussed your second suggestion https://github.com/tensorflow/tensorflow/issues/33627#issuecomment-680025939", "If I build TensorFlow from source then what code should I change to disable threading by TensorFlow?", "@fisakhan \r\n\r\nCurrently, the method to control thread number by external is only for process, instead of library (your case).\r\nThe current env variables/parameters are only impact the AI operation part. Like OMP_NUM_THREADS and inter_op_parallelism_threads, intra_op_parallelism_threads.\r\n\r\nI think they are working for AI operation of Tensorflow. But they won't impact the other part of tensorflow to use single thread or multiple threads. For example, load dataset.\r\n\r\nYour case need whole tensorflow use single thread.\r\nIt's a little hard: even if the source code of tensorflow is changed to use single thread, but it's hard to make sure the 3rd party libs used by tensorflow to use single thread.\r\n\r\nI have two alternatives:\r\n\r\n1. Modify the source code of tensorflow to set the thread number variables to 1 by hard code.\r\nIt's big work, and would be fault, because it can't control the 3rd party lib.\r\n\r\n2. Refactor your software structure:\r\nSperate the software into two processes. The function call is changed to IPC.\r\nYour code is running as child process and receive the parameters/return result by IPC.\r\nSo the thread number can be controlled by 'numactrl'.", "@deHsien \r\n\r\nCould you close this issue?  Since your issue is not present.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33627\">No</a>\n", "@deHsien \r\n\r\nThank you very much!", "Hi guys, please, did anyone manage to solve this problem, I still have the problem of multi-threads running every time, and I can't find a solution.  ?? ", "\r\n\r\nFor using 1 thread for your TF session, you could follow below comment.\r\nhttps://github.com/tensorflow/tensorflow/issues/33627#issuecomment-675661637\r\n\r\nOverall, threadpool will be created, so there should have 2 threads at last (master process + 1 thread from threadpool).\r\n", "@Koussailakadi why do you need it? I couldn't reduce the number of threads to 1 or 2 but solved the problem for which I required the reduce the threads.", "@fisakhan   I want to use tensorflow with MPI on a cluster of CPUs ( in C++), so I need to run a single thread at runtime. \r\n\r\n@louie-tsai  , thank you very much for your answer, I'm looking for how to run a single thread instead of two to use MPI afterwards. ", "    TF_Graph* Graph = TF_NewGraph();\r\n    TF_Status* Status = TF_NewStatus();\r\n\r\n    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();\r\n    TF_Buffer* RunOpts = NULL;\r\n\r\n    const char* saved_model_dir = \"../../models/LR_model.pb\";\r\n    const char* tags = \"serve\"; \r\n    int ntags = 1;\r\n\r\n    //solution :///////////////////////////////////////////////////////////////////////////////////////////////\r\n    uint8_t intra_op_parallelism_threads = 1;\r\n    uint8_t inter_op_parallelism_threads = 1;\r\n    uint8_t buf[]={0x10,intra_op_parallelism_threads,0x28,inter_op_parallelism_threads};\r\n    TF_SetConfig(SessionOpts, buf,sizeof(buf),Status); \r\n    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////\r\n\r\n    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);\r\n    if(TF_GetCode(Status) == TF_OK)\r\n    {\r\n        printf(\"TF_LoadSessionFromSavedModel OK\\n\");\r\n    }\r\n    else\r\n    {\r\n        printf(\"%s\",TF_Message(Status));\r\n    }\r\n\r\n![proc2TF_thr](https://user-images.githubusercontent.com/66485079/154969173-bbaeddb7-5666-428e-8cdd-a3be68024062.png)\r\n\r\nthis solution works, but always with 2 theads, a master that starts automatically, then a thread that does the task. as you see on the screenshot "]}, {"number": 33626, "title": "tf-lite 2.0 python", "body": "I'd like to have TensorFlow-lite python module. So is it and how to build from source?", "comments": ["@peter197321 ,\r\nCan you please refer this [link](https://www.tensorflow.org/lite/guide/python) and let us know if it was helpful ?Thanks!", "I would like to know how to prepare (build from source) such a lean wheel\n*tflite_runtime*.whl*\nA page you shared is more on user experience-oriented information - am I\nright?\n\nOn Thu, Oct 24, 2019 at 10:47 AM oanush <notifications@github.com> wrote:\n\n> @peter197321 <https://github.com/peter197321> ,\n> Can you please refer this link\n> <https://www.tensorflow.org/lite/guide/python> and let us know if it was\n> helpful ?Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33626?email_source=notifications&email_token=ALGLVQTRIE3GNCQDNCYB4BDQQFOLBA5CNFSM4JD4ND22YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECEFKNI#issuecomment-545805621>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALGLVQR2QUPGMTQ2FECX6RDQQFOLBANCNFSM4JD4ND2Q>\n> .\n>\n", "@gargn should be able to advise.", "This [link](https://www.tensorflow.org/install/source) has information on how to build the TensorFlow pip from source.", "Ok. How about tf-lite only for inference but not full fledged tf?\n\nDne p\u00e1 25. 10. 2019 22:52 u\u017eivatel Nupur Garg <notifications@github.com>\nnapsal:\n\n> This link <https://www.tensorflow.org/install/source> has information on\n> how to build the TensorFlow pip from source.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33626?email_source=notifications&email_token=ALGLVQRKIJ23E7OSMKRMNO3QQNL7TA5CNFSM4JD4ND22YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECJQS4Q#issuecomment-546507122>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALGLVQTCWMKSHOIYDUCC6ITQQNL7TANCNFSM4JD4ND2Q>\n> .\n>\n", "Hi,\r\n\r\nI agree with @peter197321 : having more detailed instructions on how to build Tensorflow Lite Runtimes would be helpfull (Adding official Tensorflow support to Jetson Nano in addition to the Raspberry Pi would also be great, but that's another topic)\r\n\r\nI have recently built with success TF2.0 for Nvidia Jetson Nano (aarch64 with python 3.6) . This allows to use tf.lite API but having to load the whole tensorflow library is rather heavy on the memory.\r\n\r\nThus I would also like to have the TF 2.0 Lite Runtime for Python 3.6 . As there is [no wheel package provided for python 3.6 on aarch64](https://www.tensorflow.org/lite/guide/python#install_just_the_tensorflow_lite_interpreter), I tried to built (natively or via cross compilation) TF 2.0 Lite by myself but without success so far. In order to do so I am using the scripts from this directory: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package), but unfortunately apart from the README, there is no specific Tensorflow documentation describing how to build tensorflow lite runtimes\r\n\r\nWhen building natively, there seems to be a first issue with \"subprocess.check_call([\"make\" ...]) ,at first sight this looks like some kind of incompatibility with a  Python 3.6 module\r\n\r\nI tried to replace in setup.py the \"subprocess.check_call([\"make\" ...]) calls by \"os.system('make ...')], which allows to bypass the 1st issue : the compilation apparently starts well but it then fails with in the distutils lib with a \"TypeError: must be str, not int\" (More details on https://devtalk.nvidia.com/default/topic/1065109/jetson-nano/building-tensorflow-2-0-and-tensorflow-lite-python-runtime/post/5396093/#5396093 )\r\n", "@dmitriykovalev might be able to provide additional help", "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package\r\nThis page has the way to build Python wheel of *tflite_runtime*.whl*", "You can also see https://pypi.org/project/tflite/#files", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33626\">No</a>\n"]}, {"number": 33625, "title": "fix strings.split", "body": "fix #33623", "comments": ["> Can I get a unit test which fails before and passes after this change?\r\n\r\nI will add unit test later", "The test case has been added. @alextp.", "According to document, strings_split_v1 should return ragged tensor or sparse tensor", "@fsx950223 can you please fix the build failures ?\r\n\r\n`third_party/tensorflow/python/framework/test_util.py\", line 2892, in _assertRaggedEqual\r\n    self.assertEqual(a_list, b_list, msg)\r\nAssertionError: Lists differ:`", "`AIL: Found 8 non-whitelisted pylint errors:\r\ntensorflow/python/kernel_tests/string_split_op_test.py:282: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\n\r\ntensorflow/python/kernel_tests/string_split_op_test.py:284: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\n\r\ntensorflow/python/kernel_tests/string_split_op_test.py:286: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\n\r\ntensorflow/python/kernel_tests/string_split_op_test.py:375: [C0330(bad-continuation), ] Wrong hanging indentation (add 4 spaces).\r\n\r\ntensorflow/python/kernel_tests/string_split_op_test.py:390: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\n\r\ntensorflow/python/kernel_tests/string_split_op_test.py:392: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\n\r\ntensorflow/python/kernel_tests/string_split_op_test.py:394: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\n\r\ntensorflow/python/kernel_tests/string_split_op_test.py:362: [W0622(redefined-builtin), StringSplitV2OpTest.testSplitV1] Redefining built-in 'input'`\r\n\r\n@fsx950223  can you please fix this sanity errors ?"]}, {"number": 33624, "title": "TF Inverse operations fail on 2080ti GPU with a cublas error", "body": "I'm trying to use a loss function which needs an inverse op, but fails on tf 2.0 with the following error:\r\n\r\n![image](https://user-images.githubusercontent.com/1873401/67357639-c08a0d00-f512-11e9-960b-ed54e7378ebc.png)\r\n\r\nOther models/loss functions that don't use inverse ops train as expected using the GPU. Any pointers would be most appreciated.", "comments": ["@krsna6 ,\r\nThank you for reporting the issue, Can you share a simple and standalone code to reproduce the issue?\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.\r\n", "@krsna6 ,\r\nHi, any update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 33623, "title": "tf.strings.split bug", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.15.0\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'to_sparse'\r\n**Describe the expected behavior**\r\nThe op should return SparseTensor or RaggedTensor\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\ntf.strings.split('a b')\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[colab](https://colab.research.google.com/drive/1PrFuL7hC25yRGmFfbwwRK1if9N8d1M9A)", "comments": ["You need brackets for the input. Try this:\r\n```\r\nc = tf.strings.split(['a b'])\r\nc.values\r\n```\r\n\r\n", "> You need brackets for the input. Try this:\r\n> \r\n> ```\r\n> c = tf.strings.split(['a b'])\r\n> c.values\r\n> ```\r\n\r\ntf.strings.split('a b',result_type='RaggedTensor') works and returns a Tensor.In fact, it should return a RaggedTensor", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33623\">No</a>\n"]}]