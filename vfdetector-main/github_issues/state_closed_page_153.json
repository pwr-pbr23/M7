[{"number": 50243, "title": "`tf.TensorArray`  can't work in `tf.while_loop`", "body": "**Describe the current behavior**\r\nI tried to implement a model like an auto-regressive model which can recurrently generate samples, one of the ways is using `tf.TensorArray` and `tf.while_loop`, but I found it would go wrong. [Here](https://colab.research.google.com/gist/gdhy9064/9b59b7f9097b506446d68f1b656316db/test.ipynb) is a gist for reference. I have simplified the source, this gist is just for illustration.\r\n\r\n\r\n**Describe the expected behavior**\r\nI can use another way with `tf.keras.backend.rnn` to reach my goal, but I just wonder what the wrong is in this way. \r\n\r\n", "comments": ["@gdhy9064 ,\r\n\r\nCan you please refer the links with similar error.It helps.[Link1](https://stackoverflow.com/questions/43849693/how-tensorarray-and-while-loop-work-together-in-tensorflow/43853600),[Link2](https://stackoverflow.com/questions/43701631/tensorflow-while-loop-with-tensorarray),[Link3](https://www.tensorflow.org/api_docs/python/tf/while_loop).\r\nThanks", "@tilakrayal ,\r\nMany thanks to you, these links help me a lot. It is my first time to use `tf.TensorArray`, and I have made a careless mistake in the wrong usage of ` tf.TensorArray.write`, I should have received its return value as a new `tf.TensorArray`, but I didn't.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50243\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50243\">No</a>\n"]}, {"number": 50242, "title": "to_categorical() doc fixes.", "body": "1. For consistency with the definition of `num_classes`, the input `y` contains values up to `num_classes`-1 (included) only.\r\n2. `y` is more general than a vector: it can by any array-like (`y = numpy.arange(12).reshape((3, 4))` works as expected).\r\n3. Clearer definition of `dtype` (it's the type of the output of this function, not necessarily of some \"input\").", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50242) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Thanks for the PR! This has been fixed separately. Please note that Keras development has moved to the repo keras-team/keras; going forward any PR should be directed there."]}, {"number": 50240, "title": "[TFLite]: Add missing cc file to Makefile", "body": "This PR adds a missing file whose code is used in libedgetpu/libcoral\nto the Tensorflow Lite `Makefile`.\n", "comments": ["It looks like CI is failing because the host is running out of space:\r\n\r\n```\r\n10:49:57  ERROR: /workspace/tensorflow/core/BUILD:1883:11: Couldn't build file tensorflow/core/lib_strings_ordered_code_test/test.log:  failed due to unexpected I/O exception: write (No space left on device)\r\n```", "@terryheo could you review this PR?", "Is there anything else I need to do to get this merged?"]}, {"number": 50239, "title": "How to convert .pkl file to tflite model", "body": "", "comments": ["Please stop!", "To convert the given model to the corresponding TensorFlow Lite model, a pkl model should be converted to the TensorFlow Saved Model and then, the saved model converter (`tf.lite.TFLiteConverter.from_saved_model`) can be used for converting from the saved model to the corresponding TensorFlow Lite model.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50239\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50239\">No</a>\n"]}, {"number": 50238, "title": "Tensorflow object detection api Windows fatal exception: access violation, step stayed at 0", "body": "Trying to train custom object detection model following this tutorial:\r\nhttps://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html\r\nI have also tested a different dataset with different model from here:\r\nhttps://github.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model\r\nbut both return the same problem. \r\nAll my file structures follows the first tutorial. \r\n\r\n\r\nOS: Windows 10\r\nCUDA: 11.3\r\nTensorflow: 2.5.0\r\nPython: 3.9\r\n\r\nLog:\r\n```\r\n2021-06-12 20:17:32.813643: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n2021-06-12 20:17:35.974727: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll\r\n2021-06-12 20:17:36.003520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1650 with Max-Q Design computeCapability: 7.5\r\ncoreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s\r\n2021-06-12 20:17:36.003577: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n2021-06-12 20:17:36.034868: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n2021-06-12 20:17:36.034898: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-06-12 20:17:36.051579: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll\r\n2021-06-12 20:17:36.055829: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll\r\n2021-06-12 20:17:36.073808: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll\r\n2021-06-12 20:17:36.077387: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll\r\n2021-06-12 20:17:36.078442: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\r\n2021-06-12 20:17:36.078540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-06-12 20:17:36.079006: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-06-12 20:17:36.106947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1650 with Max-Q Design computeCapability: 7.5\r\ncoreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s\r\n2021-06-12 20:17:36.107008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-06-12 20:17:36.619265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-06-12 20:17:36.619301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n2021-06-12 20:17:36.619309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\n2021-06-12 20:17:36.619452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2145 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nW0612 20:17:36.634464 11076 mirrored_strategy.py:379] Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nI0612 20:17:36.728192 11076 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nINFO:tensorflow:Maybe overwriting train_steps: None\r\nI0612 20:17:36.728192 11076 config_util.py:552] Maybe overwriting train_steps: None\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0612 20:17:36.728192 11076 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nWARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nW0612 20:17:36.744960 11076 deprecation.py:330] From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nINFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']\r\nI0612 20:17:36.760623 11076 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']\r\nINFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']\r\nI0612 20:17:36.760623 11076 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']\r\nINFO:tensorflow:Number of filenames to read: 1\r\nI0612 20:17:36.760623 11076 dataset_builder.py:81] Number of filenames to read: 1\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nW0612 20:17:36.760623 11076 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nW0612 20:17:36.760623 11076 deprecation.py:330] From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nWARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nW0612 20:17:36.776248 11076 deprecation.py:330] From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nWARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nW0612 20:17:42.585761 11076 deprecation.py:330] From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nWARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nW0612 20:17:45.209203 11076 deprecation.py:330] From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nWARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nW0612 20:17:46.765527 11076 deprecation.py:330] From C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\n2021-06-12 20:17:49.246970: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\nC:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:435: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\r\n  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\r\n2021-06-12 20:18:20.053992: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\r\n2021-06-12 20:18:20.578938: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201\r\n2021-06-12 20:18:21.387817: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 934.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-06-12 20:18:21.387940: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 934.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-06-12 20:18:21.605687: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n2021-06-12 20:18:22.316327: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-06-12 20:18:22.544715: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-06-12 20:18:22.544804: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-06-12 20:18:22.680473: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 916.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-06-12 20:18:22.680578: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 916.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-06-12 20:18:22.680686: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-06-12 20:18:22.680753: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-06-12 20:18:33.129542: W tensorflow/core/common_runtime/bfc_allocator.cc:456] Allocator (GPU_0_bfc) ran out of memory trying to allocate 400.00MiB (rounded to 419430400)requested by op ResNet50V1_FPN/model/conv2_block2_3_bn/FusedBatchNormV3\r\nIf the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \r\nCurrent allocation summary follows.\r\nCurrent allocation summary follows.\r\n2021-06-12 20:18:33.129852: I tensorflow/core/common_runtime/bfc_allocator.cc:991] BFCAllocator dump for GPU_0_bfc\r\n2021-06-12 20:18:33.130006: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (256): \tTotal Chunks: 77, Chunks in use: 77. 19.2KiB allocated for chunks. 19.2KiB in use in bin. 8.7KiB client-requested in use in bin.\r\n2021-06-12 20:18:33.130118: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (512): \tTotal Chunks: 94, Chunks in use: 81. 47.0KiB allocated for chunks. 40.5KiB in use in bin. 35.3KiB client-requested in use in bin.\r\n2021-06-12 20:18:33.130229: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (1024): \tTotal Chunks: 263, Chunks in use: 261. 276.2KiB allocated for chunks. 274.0KiB in use in bin. 271.0KiB client-requested in use in bin.\r\n2021-06-12 20:18:33.130329: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (2048): \tTotal Chunks: 65, Chunks in use: 61. 134.5KiB allocated for chunks. 123.2KiB in use in bin. 121.5KiB client-requested in use in bin.\r\n2021-06-12 20:18:33.130427: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (4096): \tTotal Chunks: 32, Chunks in use: 31. 135.5KiB allocated for chunks. 130.8KiB in use in bin. 130.8KiB client-requested in use in bin.\r\n2021-06-12 20:18:33.130529: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (8192): \tTotal Chunks: 16, Chunks in use: 16. 134.0KiB allocated for chunks. 134.0KiB in use in bin. 128.0KiB client-requested in use in bin.\r\n2021-06-12 20:18:33.130631: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (16384): \tTotal Chunks: 5, Chunks in use: 4. 131.0KiB allocated for chunks. 103.0KiB in use in bin. 91.0KiB client-requested in use in bin.\r\n2021-06-12 20:18:33.130732: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (32768): \tTotal Chunks: 1, Chunks in use: 1. 36.8KiB allocated for chunks. 36.8KiB in use in bin. 36.8KiB client-requested in use in bin.\r\n2021-06-12 20:18:33.130829: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (65536): \tTotal Chunks: 8, Chunks in use: 6. 561.0KiB allocated for chunks. 416.0KiB in use in bin. 384.0KiB client-requested in use in bin.\r\n2021-06-12 20:18:33.130929: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (131072): \tTotal Chunks: 7, Chunks in use: 7. 1.13MiB allocated for chunks. 1.13MiB in use in bin. 1.13MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.131026: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (262144): \tTotal Chunks: 11, Chunks in use: 9. 3.10MiB allocated for chunks. 2.44MiB in use in bin. 2.16MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.131123: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (524288): \tTotal Chunks: 11, Chunks in use: 10. 6.41MiB allocated for chunks. 5.91MiB in use in bin. 5.57MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.131222: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (1048576): \tTotal Chunks: 13, Chunks in use: 12. 14.22MiB allocated for chunks. 13.00MiB in use in bin. 12.00MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.131321: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (2097152): \tTotal Chunks: 24, Chunks in use: 21. 55.75MiB allocated for chunks. 48.75MiB in use in bin. 46.50MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.131698: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (4194304): \tTotal Chunks: 21, Chunks in use: 21. 98.05MiB allocated for chunks. 98.05MiB in use in bin. 98.05MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.131835: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (8388608): \tTotal Chunks: 5, Chunks in use: 5. 50.61MiB allocated for chunks. 50.61MiB in use in bin. 50.61MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.131939: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (16777216): \tTotal Chunks: 18, Chunks in use: 18. 348.91MiB allocated for chunks. 348.91MiB in use in bin. 344.19MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.132030: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-06-12 20:18:33.132129: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (67108864): \tTotal Chunks: 3, Chunks in use: 1. 270.09MiB allocated for chunks. 75.00MiB in use in bin. 75.00MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.132223: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 136.58MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-06-12 20:18:33.132325: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 2. 1.13GiB allocated for chunks. 821.95MiB in use in bin. 800.00MiB client-requested in use in bin.\r\n2021-06-12 20:18:33.132408: I tensorflow/core/common_runtime/bfc_allocator.cc:1014] Bin for 400.00MiB was 256.00MiB, Chunk State: \r\n2021-06-12 20:18:33.132526: I tensorflow/core/common_runtime/bfc_allocator.cc:1020]   Size: 337.16MiB | Requested Size: 19.51MiB | in_use: 0 | bin_num: 20, prev:   Size: 4.88MiB | Requested Size: 4.88MiB | in_use: 1 | bin_num: -1, for: Loss/Compare_10/IOU/Equal, stepid: 16762585343613708217, last_action: 126448115942167, for: UNUSED, stepid: 16762585343613708217, last_action: 126448115942071\r\n2021-06-12 20:18:33.132604: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Next region of size 2249614848\r\n2021-06-12 20:18:33.132724: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000000 of size 1280 by op ScratchBuffer action_count 126448115939866 step 0 next 1\r\n2021-06-12 20:18:33.132802: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000500 of size 256 by op AssignVariableOp action_count 126448115939867 step 0 next 2\r\n2021-06-12 20:18:33.132875: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000600 of size 256 by op AssignVariableOp action_count 126448115939868 step 0 next 3\r\n2021-06-12 20:18:33.132945: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000700 of size 256 by op Mul action_count 126448115939941 step 0 next 4\r\n2021-06-12 20:18:33.133013: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000800 of size 256 by op SameWorkerRecvDone action_count 126448115939890 step 0 next 5\r\n2021-06-12 20:18:33.133081: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000900 of size 78643200 by op SameWorkerRecvDone action_count 126448115939871 step 0 next 6\r\n2021-06-12 20:18:33.133147: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb00900 of size 256 by op SameWorkerRecvDone action_count 126448115939892 step 0 next 7\r\n2021-06-12 20:18:33.133212: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb00a00 of size 256 by op SameWorkerRecvDone action_count 126448115939893 step 0 next 8\r\n2021-06-12 20:18:33.133278: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb00b00 of size 6400 by op SameWorkerRecvDone action_count 126448115939874 step 0 next 9\r\n2021-06-12 20:18:33.133346: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb02400 of size 25600 by op SameWorkerRecvDone action_count 126448115939875 step 0 next 10\r\n2021-06-12 20:18:33.133427: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb08800 of size 25600 by op SameWorkerRecvDone action_count 126448115939876 step 0 next 11\r\n2021-06-12 20:18:33.133505: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb0ec00 of size 25600 by op SameWorkerRecvDone action_count 126448115939877 step 0 next 12\r\n2021-06-12 20:18:33.133577: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb15000 of size 6400 by op SameWorkerRecvDone action_count 126448115939889 step 0 next 13\r\n2021-06-12 20:18:33.133647: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb16900 of size 1792 by op SameWorkerRecvDone action_count 126448115939879 step 0 next 14\r\n2021-06-12 20:18:33.133744: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb17000 of size 1792 by op SameWorkerRecvDone action_count 126448115939880 step 0 next 15\r\n2021-06-12 20:18:33.133813: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb17700 of size 256 by op SameWorkerRecvDone action_count 126448115939881 step 0 next 16\r\n2021-06-12 20:18:33.133879: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb17800 of size 256 by op SameWorkerRecvDone action_count 126448115939882 step 0 next 17\r\n2021-06-12 20:18:33.133947: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb17900 of size 6400 by op SameWorkerRecvDone action_count 126448115939883 step 0 next 18\r\n2021-06-12 20:18:33.134016: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19200 of size 256 by op SameWorkerRecvDone action_count 126448115939894 step 0 next 19\r\n2021-06-12 20:18:33.134086: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19300 of size 256 by op Fill action_count 126448115939947 step 0 next 33\r\n2021-06-12 20:18:33.134155: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19400 of size 256 by op Fill action_count 126448115939948 step 0 next 30\r\n2021-06-12 20:18:33.134221: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19500 of size 256 by op Fill action_count 126448115939949 step 0 next 29\r\n2021-06-12 20:18:33.134286: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19600 of size 256 by op Fill action_count 126448115939950 step 0 next 28\r\n2021-06-12 20:18:33.134350: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19700 of size 256 by op Fill action_count 126448115939951 step 0 next 27\r\n2021-06-12 20:18:33.134414: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19800 of size 256 by op Fill action_count 126448115939952 step 0 next 26\r\n2021-06-12 20:18:33.134485: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19900 of size 1024 by op Fill action_count 126448115940185 step 0 next 167\r\n2021-06-12 20:18:33.134553: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19d00 of size 1024 by op Fill action_count 126448115940186 step 0 next 168\r\n2021-06-12 20:18:33.134607: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1a100 of size 4096 by op Fill action_count 126448115940192 step 0 next 169\r\n2021-06-12 20:18:33.134616: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1b100 of size 4096 by op Fill action_count 126448115940193 step 0 next 171\r\n2021-06-12 20:18:33.134624: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1c100 of size 4096 by op Fill action_count 126448115940194 step 0 next 172\r\n2021-06-12 20:18:33.134632: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1d100 of size 4096 by op Fill action_count 126448115940195 step 0 next 173\r\n2021-06-12 20:18:33.134639: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1e100 of size 1024 by op Fill action_count 126448115940201 step 0 next 174\r\n2021-06-12 20:18:33.134648: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1e500 of size 1024 by op Fill action_count 126448115940202 step 0 next 176\r\n2021-06-12 20:18:33.134657: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1e900 of size 1024 by op Fill action_count 126448115940203 step 0 next 177\r\n2021-06-12 20:18:33.134666: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1ed00 of size 1024 by op Fill action_count 126448115940204 step 0 next 178\r\n2021-06-12 20:18:33.134675: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1f100 of size 1024 by op Fill action_count 126448115940210 step 0 next 179\r\n2021-06-12 20:18:33.134683: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1f500 of size 1024 by op Fill action_count 126448115940211 step 0 next 182\r\n2021-06-12 20:18:33.134691: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1f900 of size 1024 by op Fill action_count 126448115940212 step 0 next 183\r\n2021-06-12 20:18:33.134699: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1fd00 of size 1024 by op Fill action_count 126448115940213 step 0 next 184\r\n2021-06-12 20:18:33.134707: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb20100 of size 4096 by op Fill action_count 126448115940219 step 0 next 186\r\n2021-06-12 20:18:33.134715: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb21100 of size 4096 by op Fill action_count 126448115940220 step 0 next 187\r\n2021-06-12 20:18:33.134723: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb22100 of size 4096 by op Fill action_count 126448115940221 step 0 next 188\r\n2021-06-12 20:18:33.134731: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb23100 of size 4096 by op Fill action_count 126448115940222 step 0 next 189\r\n2021-06-12 20:18:33.134740: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb24100 of size 1024 by op Fill action_count 126448115940228 step 0 next 191\r\n2021-06-12 20:18:33.134748: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb24500 of size 1024 by op Fill action_count 126448115940229 step 0 next 192\r\n2021-06-12 20:18:33.134756: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb24900 of size 1024 by op Fill action_count 126448115940230 step 0 next 193\r\n2021-06-12 20:18:33.134763: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb24d00 of size 1024 by op Fill action_count 126448115940231 step 0 next 194\r\n2021-06-12 20:18:33.134771: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb25100 of size 1024 by op Fill action_count 126448115940237 step 0 next 197\r\n2021-06-12 20:18:33.134779: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb25500 of size 1024 by op Fill action_count 126448115940238 step 0 next 198\r\n2021-06-12 20:18:33.134788: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb25900 of size 1024 by op Fill action_count 126448115940239 step 0 next 199\r\n2021-06-12 20:18:33.134796: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb25d00 of size 1024 by op Fill action_count 126448115940240 step 0 next 200\r\n2021-06-12 20:18:33.134804: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb26100 of size 4096 by op Fill action_count 126448115940246 step 0 next 202\r\n2021-06-12 20:18:33.134812: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb27100 of size 4096 by op Fill action_count 126448115940247 step 0 next 203\r\n2021-06-12 20:18:33.134820: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb28100 of size 4096 by op Fill action_count 126448115940248 step 0 next 204\r\n2021-06-12 20:18:33.134828: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb29100 of size 4096 by op Fill action_count 126448115940249 step 0 next 205\r\n2021-06-12 20:18:33.134836: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2a100 of size 1024 by op Fill action_count 126448115940255 step 0 next 206\r\n2021-06-12 20:18:33.134844: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2a500 of size 1024 by op Fill action_count 126448115940256 step 0 next 207\r\n2021-06-12 20:18:33.134853: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2a900 of size 1024 by op Fill action_count 126448115940257 step 0 next 208\r\n2021-06-12 20:18:33.134862: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2ad00 of size 1024 by op Fill action_count 126448115940258 step 0 next 209\r\n2021-06-12 20:18:33.134871: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2b100 of size 1024 by op Fill action_count 126448115940264 step 0 next 212\r\n2021-06-12 20:18:33.134879: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2b500 of size 1024 by op Fill action_count 126448115940265 step 0 next 34\r\n2021-06-12 20:18:33.134887: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2b900 of size 256 by op Add action_count 126448115939943 step 0 next 32\r\n2021-06-12 20:18:33.134895: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2ba00 of size 37632 by op Add action_count 126448115939944 step 0 next 31\r\n2021-06-12 20:18:33.134903: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb34d00 of size 1024 by op Fill action_count 126448115939958 step 0 next 23\r\n2021-06-12 20:18:33.134911: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35100 of size 1024 by op Fill action_count 126448115939959 step 0 next 22\r\n2021-06-12 20:18:33.134919: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35500 of size 1024 by op Fill action_count 126448115939960 step 0 next 21\r\n2021-06-12 20:18:33.134927: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35900 of size 1024 by op Fill action_count 126448115939961 step 0 next 20\r\n2021-06-12 20:18:33.134935: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35d00 of size 256 by op Fill action_count 126448115939967 step 0 next 35\r\n2021-06-12 20:18:33.134943: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35e00 of size 256 by op Fill action_count 126448115939968 step 0 next 37\r\n2021-06-12 20:18:33.134951: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35f00 of size 256 by op Fill action_count 126448115939969 step 0 next 38\r\n2021-06-12 20:18:33.134959: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36000 of size 256 by op Fill action_count 126448115939970 step 0 next 39\r\n2021-06-12 20:18:33.134967: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36100 of size 256 by op Fill action_count 126448115939976 step 0 next 40\r\n2021-06-12 20:18:33.134975: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36200 of size 256 by op Fill action_count 126448115939977 step 0 next 43\r\n2021-06-12 20:18:33.134983: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36300 of size 256 by op Fill action_count 126448115939978 step 0 next 44\r\n2021-06-12 20:18:33.134991: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36400 of size 256 by op Fill action_count 126448115939979 step 0 next 45\r\n2021-06-12 20:18:33.135000: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36500 of size 1024 by op Fill action_count 126448115939985 step 0 next 48\r\n2021-06-12 20:18:33.135008: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36900 of size 1024 by op Fill action_count 126448115939986 step 0 next 49\r\n2021-06-12 20:18:33.135016: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36d00 of size 1024 by op Fill action_count 126448115939987 step 0 next 50\r\n2021-06-12 20:18:33.135024: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37100 of size 1024 by op Fill action_count 126448115939988 step 0 next 51\r\n2021-06-12 20:18:33.135032: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37500 of size 256 by op Fill action_count 126448115939994 step 0 next 53\r\n2021-06-12 20:18:33.135040: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37600 of size 256 by op Fill action_count 126448115939995 step 0 next 54\r\n2021-06-12 20:18:33.135076: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37700 of size 256 by op Fill action_count 126448115939996 step 0 next 55\r\n2021-06-12 20:18:33.135088: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37800 of size 256 by op Fill action_count 126448115939997 step 0 next 56\r\n2021-06-12 20:18:33.135096: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37900 of size 256 by op Fill action_count 126448115940003 step 0 next 57\r\n2021-06-12 20:18:33.135105: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37a00 of size 256 by op Fill action_count 126448115940004 step 0 next 60\r\n2021-06-12 20:18:33.135113: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37b00 of size 256 by op Fill action_count 126448115940005 step 0 next 61\r\n2021-06-12 20:18:33.135121: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37c00 of size 256 by op Fill action_count 126448115940006 step 0 next 62\r\n2021-06-12 20:18:33.135129: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37d00 of size 1024 by op Fill action_count 126448115940012 step 0 next 63\r\n2021-06-12 20:18:33.135137: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38100 of size 1024 by op Fill action_count 126448115940013 step 0 next 64\r\n2021-06-12 20:18:33.135146: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38500 of size 1024 by op Fill action_count 126448115940014 step 0 next 65\r\n2021-06-12 20:18:33.135154: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38900 of size 1024 by op Fill action_count 126448115940015 step 0 next 66\r\n2021-06-12 20:18:33.135162: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38d00 of size 256 by op Fill action_count 126448115940021 step 0 next 68\r\n2021-06-12 20:18:33.135170: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38e00 of size 256 by op Fill action_count 126448115940022 step 0 next 69\r\n2021-06-12 20:18:33.135178: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38f00 of size 256 by op Fill action_count 126448115940023 step 0 next 70\r\n2021-06-12 20:18:33.135186: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39000 of size 256 by op Fill action_count 126448115940024 step 0 next 71\r\n2021-06-12 20:18:33.135194: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39100 of size 256 by op Fill action_count 126448115940030 step 0 next 74\r\n2021-06-12 20:18:33.135202: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39200 of size 256 by op Fill action_count 126448115940031 step 0 next 75\r\n2021-06-12 20:18:33.135210: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39300 of size 256 by op Fill action_count 126448115940032 step 0 next 76\r\n2021-06-12 20:18:33.135218: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39400 of size 256 by op Fill action_count 126448115940033 step 0 next 77\r\n2021-06-12 20:18:33.135227: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39500 of size 1024 by op Fill action_count 126448115940039 step 0 next 79\r\n2021-06-12 20:18:33.135235: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39900 of size 1024 by op Fill action_count 126448115940040 step 0 next 80\r\n2021-06-12 20:18:33.135243: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39d00 of size 1024 by op Fill action_count 126448115940041 step 0 next 81\r\n2021-06-12 20:18:33.135251: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3a100 of size 1024 by op Fill action_count 126448115940042 step 0 next 82\r\n2021-06-12 20:18:33.135259: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3a500 of size 2048 by op Fill action_count 126448115940048 step 0 next 83\r\n2021-06-12 20:18:33.135268: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3ad00 of size 2048 by op Fill action_count 126448115940049 step 0 next 86\r\n2021-06-12 20:18:33.135306: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3b500 of size 2048 by op Fill action_count 126448115940050 step 0 next 87\r\n2021-06-12 20:18:33.135317: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3bd00 of size 2048 by op Fill action_count 126448115940051 step 0 next 88\r\n2021-06-12 20:18:33.135326: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3c500 of size 512 by op Fill action_count 126448115940057 step 0 next 91\r\n2021-06-12 20:18:33.135334: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3c700 of size 512 by op Fill action_count 126448115940058 step 0 next 92\r\n2021-06-12 20:18:33.135342: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3c900 of size 512 by op Fill action_count 126448115940059 step 0 next 93\r\n2021-06-12 20:18:33.135350: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3cb00 of size 512 by op Fill action_count 126448115940060 step 0 next 94\r\n2021-06-12 20:18:33.135358: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3cd00 of size 512 by op Fill action_count 126448115940066 step 0 next 97\r\n2021-06-12 20:18:33.135367: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3cf00 of size 512 by op Fill action_count 126448115940067 step 0 next 98\r\n2021-06-12 20:18:33.135375: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3d100 of size 512 by op Fill action_count 126448115940068 step 0 next 99\r\n2021-06-12 20:18:33.135383: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3d300 of size 512 by op Fill action_count 126448115940069 step 0 next 100\r\n2021-06-12 20:18:33.135391: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3d500 of size 2048 by op Fill action_count 126448115940075 step 0 next 36\r\n2021-06-12 20:18:33.135400: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3dd00 of size 28672 by op Add action_count 126448115939964 step 0 next 25\r\n2021-06-12 20:18:33.135408: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb44d00 of size 65536 by op Add action_count 126448115939955 step 0 next 24\r\n2021-06-12 20:18:33.135416: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb54d00 of size 2048 by op Fill action_count 126448115940076 step 0 next 101\r\n2021-06-12 20:18:33.135424: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb55500 of size 2048 by op Fill action_count 126448115940077 step 0 next 103\r\n2021-06-12 20:18:33.135432: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb55d00 of size 2048 by op Fill action_count 126448115940078 step 0 next 104\r\n2021-06-12 20:18:33.135441: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56500 of size 512 by op Fill action_count 126448115940084 step 0 next 105\r\n2021-06-12 20:18:33.135449: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56700 of size 512 by op Fill action_count 126448115940085 step 0 next 107\r\n2021-06-12 20:18:33.135457: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56900 of size 512 by op Fill action_count 126448115940086 step 0 next 108\r\n2021-06-12 20:18:33.135465: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56b00 of size 512 by op Fill action_count 126448115940087 step 0 next 109\r\n2021-06-12 20:18:33.135473: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56d00 of size 512 by op Fill action_count 126448115940093 step 0 next 110\r\n2021-06-12 20:18:33.135481: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56f00 of size 512 by op Fill action_count 126448115940094 step 0 next 113\r\n2021-06-12 20:18:33.135489: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb57100 of size 512 by op Fill action_count 126448115940095 step 0 next 114\r\n2021-06-12 20:18:33.135497: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb57300 of size 512 by op Fill action_count 126448115940096 step 0 next 115\r\n2021-06-12 20:18:33.135507: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb57500 of size 2048 by op Fill action_count 126448115940102 step 0 next 117\r\n2021-06-12 20:18:33.135516: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb57d00 of size 2048 by op Fill action_count 126448115940103 step 0 next 118\r\n2021-06-12 20:18:33.135525: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb58500 of size 2048 by op Fill action_count 126448115940104 step 0 next 119\r\n2021-06-12 20:18:33.135533: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb58d00 of size 2048 by op Fill action_count 126448115940105 step 0 next 120\r\n2021-06-12 20:18:33.135541: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59500 of size 512 by op Fill action_count 126448115940111 step 0 next 122\r\n2021-06-12 20:18:33.135549: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59700 of size 512 by op Fill action_count 126448115940112 step 0 next 123\r\n2021-06-12 20:18:33.135557: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59900 of size 512 by op Fill action_count 126448115940113 step 0 next 124\r\n2021-06-12 20:18:33.135565: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59b00 of size 512 by op Fill action_count 126448115940114 step 0 next 125\r\n2021-06-12 20:18:33.135573: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59d00 of size 512 by op Fill action_count 126448115940120 step 0 next 128\r\n2021-06-12 20:18:33.135582: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59f00 of size 512 by op Fill action_count 126448115940121 step 0 next 129\r\n2021-06-12 20:18:33.135590: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5a100 of size 512 by op Fill action_count 126448115940122 step 0 next 130\r\n2021-06-12 20:18:33.135598: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5a300 of size 512 by op Fill action_count 126448115940123 step 0 next 131\r\n2021-06-12 20:18:33.135606: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5a500 of size 2048 by op Fill action_count 126448115940129 step 0 next 133\r\n2021-06-12 20:18:33.135614: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5ad00 of size 2048 by op Fill action_count 126448115940130 step 0 next 134\r\n2021-06-12 20:18:33.135622: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5b500 of size 2048 by op Fill action_count 126448115940131 step 0 next 135\r\n2021-06-12 20:18:33.135630: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5bd00 of size 2048 by op Fill action_count 126448115940132 step 0 next 136\r\n2021-06-12 20:18:33.135639: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5c500 of size 512 by op Fill action_count 126448115940138 step 0 next 137\r\n2021-06-12 20:18:33.135647: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5c700 of size 512 by op Fill action_count 126448115940139 step 0 next 138\r\n2021-06-12 20:18:33.135655: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5c900 of size 512 by op Fill action_count 126448115940140 step 0 next 139\r\n2021-06-12 20:18:33.135662: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5cb00 of size 512 by op Fill action_count 126448115940141 step 0 next 140\r\n2021-06-12 20:18:33.135670: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5cd00 of size 512 by op Fill action_count 126448115940147 step 0 next 143\r\n2021-06-12 20:18:33.135678: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5cf00 of size 512 by op Fill action_count 126448115940148 step 0 next 144\r\n2021-06-12 20:18:33.135686: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5d100 of size 512 by op Fill action_count 126448115940149 step 0 next 145\r\n2021-06-12 20:18:33.135695: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5d300 of size 512 by op Fill action_count 126448115940150 step 0 next 146\r\n2021-06-12 20:18:33.135704: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5d500 of size 2048 by op Fill action_count 126448115940156 step 0 next 148\r\n2021-06-12 20:18:33.135713: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5dd00 of size 2048 by op Fill action_count 126448115940157 step 0 next 149\r\n2021-06-12 20:18:33.135722: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5e500 of size 2048 by op Fill action_count 126448115940158 step 0 next 150\r\n2021-06-12 20:18:33.135730: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5ed00 of size 2048 by op Fill action_count 126448115940159 step 0 next 151\r\n2021-06-12 20:18:33.135738: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5f500 of size 4096 by op Fill action_count 126448115940165 step 0 next 152\r\n2021-06-12 20:18:33.135745: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb60500 of size 4096 by op Fill action_count 126448115940166 step 0 next 155\r\n2021-06-12 20:18:33.135753: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb61500 of size 4096 by op Fill action_count 126448115940167 step 0 next 156\r\n2021-06-12 20:18:33.135762: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb62500 of size 4096 by op Fill action_count 126448115940168 step 0 next 157\r\n2021-06-12 20:18:33.135770: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb63500 of size 1024 by op Fill action_count 126448115940174 step 0 next 160\r\n2021-06-12 20:18:33.135778: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb63900 of size 1024 by op Fill action_count 126448115940175 step 0 next 161\r\n2021-06-12 20:18:33.135786: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb63d00 of size 1024 by op Fill action_count 126448115940176 step 0 next 162\r\n2021-06-12 20:18:33.135794: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb64100 of size 1024 by op Fill action_count 126448115940177 step 0 next 163\r\n2021-06-12 20:18:33.135802: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb64500 of size 1024 by op Fill action_count 126448115940183 step 0 next 166\r\n2021-06-12 20:18:33.135810: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb64900 of size 1024 by op Fill action_count 126448115940184 step 0 next 46\r\n2021-06-12 20:18:33.135818: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb64d00 of size 65536 by op Add action_count 126448115939982 step 0 next 47\r\n2021-06-12 20:18:33.135826: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb74d00 of size 65536 by op Add action_count 126448115939991 step 0 next 52\r\n2021-06-12 20:18:33.135835: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb84d00 of size 98304 by op Add action_count 126448115940009 step 0 next 41\r\n2021-06-12 20:18:33.135843: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb9cd00 of size 147456 by op Add action_count 126448115939973 step 0 next 42\r\n2021-06-12 20:18:33.135852: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbc0d00 of size 65536 by op Add action_count 126448115940018 step 0 next 67\r\n2021-06-12 20:18:33.135860: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd0d00 of size 2048 by op Fill action_count 126448115940408 step 0 next 294\r\n2021-06-12 20:18:33.135868: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd1500 of size 2048 by op Fill action_count 126448115940409 step 0 next 295\r\n2021-06-12 20:18:33.135876: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd1d00 of size 2048 by op Fill action_count 126448115940410 step 0 next 296\r\n2021-06-12 20:18:33.135884: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd2500 of size 2048 by op Fill action_count 126448115940411 step 0 next 297\r\n2021-06-12 20:18:33.135893: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd2d00 of size 8192 by op Fill action_count 126448115940417 step 0 next 299\r\n2021-06-12 20:18:33.135903: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd4d00 of size 8192 by op Fill action_count 126448115940418 step 0 next 300\r\n2021-06-12 20:18:33.135911: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd6d00 of size 8192 by op Fill action_count 126448115940419 step 0 next 301\r\n2021-06-12 20:18:33.135919: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd8d00 of size 8192 by op Fill action_count 126448115940420 step 0 next 302\r\n2021-06-12 20:18:33.135927: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdad00 of size 1024 by op Fill action_count 126448115940426 step 0 next 304\r\n2021-06-12 20:18:33.135935: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdb100 of size 1024 by op Fill action_count 126448115940432 step 0 next 305\r\n2021-06-12 20:18:33.135943: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdb500 of size 1024 by op Fill action_count 126448115940438 step 0 next 307\r\n2021-06-12 20:18:33.135951: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdb900 of size 1024 by op Fill action_count 126448115940439 step 0 next 308\r\n2021-06-12 20:18:33.135959: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdbd00 of size 1024 by op Fill action_count 126448115940440 step 0 next 309\r\n2021-06-12 20:18:33.135967: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdc100 of size 1024 by op Fill action_count 126448115940441 step 0 next 310\r\n2021-06-12 20:18:33.135976: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdc500 of size 1024 by op Fill action_count 126448115940447 step 0 next 312\r\n2021-06-12 20:18:33.135983: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdc900 of size 1024 by op Fill action_count 126448115940453 step 0 next 314\r\n2021-06-12 20:18:33.135991: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdcd00 of size 1024 by op Fill action_count 126448115940454 step 0 next 315\r\n2021-06-12 20:18:33.136000: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdd100 of size 1024 by op Fill action_count 126448115940455 step 0 next 316\r\n2021-06-12 20:18:33.136007: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdd500 of size 1024 by op Fill action_count 126448115940456 step 0 next 317\r\n2021-06-12 20:18:33.136015: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdd900 of size 1024 by op Fill action_count 126448115940462 step 0 next 318\r\n2021-06-12 20:18:33.136024: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbddd00 of size 1024 by op Fill action_count 126448115940463 step 0 next 319\r\n2021-06-12 20:18:33.136032: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbde100 of size 1024 by op Fill action_count 126448115940464 step 0 next 320\r\n2021-06-12 20:18:33.136040: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbde500 of size 1024 by op Fill action_count 126448115940465 step 0 next 321\r\n2021-06-12 20:18:33.136048: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbde900 of size 1024 by op Fill action_count 126448115940471 step 0 next 323\r\n2021-06-12 20:18:33.136056: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbded00 of size 1024 by op Fill action_count 126448115940472 step 0 next 324\r\n2021-06-12 20:18:33.136064: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdf100 of size 1024 by op Fill action_count 126448115940473 step 0 next 325\r\n2021-06-12 20:18:33.136072: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdf500 of size 1024 by op Fill action_count 126448115940474 step 0 next 326\r\n2021-06-12 20:18:33.136080: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdf900 of size 256 by op Mul action_count 126448115940491 step 0 next 327\r\n2021-06-12 20:18:33.136090: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfa00 of size 256 by op Cast action_count 126448115940476 step 0 next 328\r\n2021-06-12 20:18:33.136099: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfb00 of size 256 by op Cast action_count 126448115940479 step 0 next 329\r\n2021-06-12 20:18:33.136107: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfc00 of size 256 by op Cast action_count 126448115940482 step 0 next 330\r\n2021-06-12 20:18:33.136115: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfd00 of size 256 by op Cast action_count 126448115940485 step 0 next 331\r\n2021-06-12 20:18:33.136123: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfe00 of size 256 by op Cast action_count 126448115940488 step 0 next 332\r\n2021-06-12 20:18:33.136131: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdff00 of size 1024 by op Fill action_count 126448115940496 step 0 next 334\r\n2021-06-12 20:18:33.136140: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe0300 of size 1024 by op Fill action_count 126448115940497 step 0 next 335\r\n2021-06-12 20:18:33.136147: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe0700 of size 1024 by op Fill action_count 126448115940498 step 0 next 336\r\n2021-06-12 20:18:33.136156: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe0b00 of size 1024 by op Fill action_count 126448115940499 step 0 next 337\r\n2021-06-12 20:18:33.136164: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe0f00 of size 1024 by op Fill action_count 126448115940505 step 0 next 339\r\n2021-06-12 20:18:33.136172: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe1300 of size 1024 by op Fill action_count 126448115940506 step 0 next 340\r\n2021-06-12 20:18:33.136180: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe1700 of size 1024 by op Fill action_count 126448115940507 step 0 next 341\r\n2021-06-12 20:18:33.136188: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe1b00 of size 1024 by op Fill action_count 126448115940508 step 0 next 342\r\n2021-06-12 20:18:33.136196: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe1f00 of size 1024 by op Fill action_count 126448115940514 step 0 next 344\r\n2021-06-12 20:18:33.136204: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe2300 of size 1024 by op Fill action_count 126448115940515 step 0 next 345\r\n2021-06-12 20:18:33.136212: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe2700 of size 1024 by op Fill action_count 126448115940516 step 0 next 346\r\n2021-06-12 20:18:33.136220: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe2b00 of size 1024 by op Fill action_count 126448115940517 step 0 next 347\r\n2021-06-12 20:18:33.136228: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe2f00 of size 1024 by op Fill action_count 126448115940523 step 0 next 349\r\n2021-06-12 20:18:33.136237: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe3300 of size 1024 by op Fill action_count 126448115940524 step 0 next 350\r\n2021-06-12 20:18:33.136245: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe3700 of size 1024 by op Fill action_count 126448115940525 step 0 next 351\r\n2021-06-12 20:18:33.136253: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe3b00 of size 1024 by op Fill action_count 126448115940526 step 0 next 352\r\n2021-06-12 20:18:33.136261: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe3f00 of size 256 by op Fill action_count 126448115940532 step 0 next 353\r\n2021-06-12 20:18:33.136268: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe4000 of size 1024 by op Fill action_count 126448115940538 step 0 next 354\r\n2021-06-12 20:18:33.136277: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe4400 of size 1024 by op Fill action_count 126448115940539 step 0 next 355\r\n2021-06-12 20:18:33.136286: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe4800 of size 1024 by op Fill action_count 126448115940540 step 0 next 356\r\n2021-06-12 20:18:33.136295: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe4c00 of size 1024 by op Fill action_count 126448115940541 step 0 next 357\r\n2021-06-12 20:18:33.136304: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe5000 of size 1024 by op Fill action_count 126448115940547 step 0 next 359\r\n2021-06-12 20:18:33.136312: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe5400 of size 1024 by op Fill action_count 126448115940548 step 0 next 360\r\n2021-06-12 20:18:33.136320: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe5800 of size 1024 by op Fill action_count 126448115940549 step 0 next 361\r\n2021-06-12 20:18:33.136328: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe5c00 of size 1024 by op Fill action_count 126448115940550 step 0 next 362\r\n2021-06-12 20:18:33.136336: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe6000 of size 1024 by op Fill action_count 126448115940556 step 0 next 364\r\n2021-06-12 20:18:33.136344: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe6400 of size 1024 by op Fill action_count 126448115940557 step 0 next 365\r\n2021-06-12 20:18:33.136352: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe6800 of size 1024 by op Fill action_count 126448115940558 step 0 next 366\r\n2021-06-12 20:18:33.136360: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe6c00 of size 1024 by op Fill action_count 126448115940559 step 0 next 367\r\n2021-06-12 20:18:33.136368: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe7000 of size 1024 by op Fill action_count 126448115940565 step 0 next 369\r\n2021-06-12 20:18:33.136377: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe7400 of size 1024 by op Fill action_count 126448115940566 step 0 next 370\r\n2021-06-12 20:18:33.136385: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe7800 of size 1024 by op Fill action_count 126448115940567 step 0 next 371\r\n2021-06-12 20:18:33.136393: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe7c00 of size 1024 by op Fill action_count 126448115940568 step 0 next 372\r\nWindows fatal exception: access violation\r\n\r\nThread 0x000056c4 (most recent call first):\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\", line 576 in _handle_results\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 892 in run\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 954 in _bootstrap_inner\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 912 in _bootstrap\r\n\r\nThread 0x000038f4 (most recent call first):\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\", line 528 in _handle_tasks\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 892 in run\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 954 in _bootstrap_inner\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 912 in _bootstrap\r\n\r\nThread 0x0000275c (most recent call first):\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\connection.py\", line 816 in _exhaustive_wait\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\connection.py\", line 884 in wait\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\", line 499 in _wait_for_updates\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\", line 519 in _handle_workers\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 892 in run\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 954 in _bootstrap_inner\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 912 in _bootstrap\r\n\r\nThread 0x00004788 (most recent call first):\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\", line 114 in worker\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 892 in run\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 954 in _bootstrap_inner\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 912 in _bootstrap\r\n\r\nThread 0x00002b44 (most recent call first):\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59 in quick_execute\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 591 in call\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1960 in _call_flat\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3023 in __call__\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 950 in _call\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 889 in __call__\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py\", line 86 in call_for_each_replica\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_strategy.py\", line 678 in _call_for_each_replica\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 2833 in call_for_each_replica\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1285 in run\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 173 in _ensure_model_is_built\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 394 in load_fine_tune_checkpoint\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 599 in train_loop\r\n  File \"C:\\Users\\Edward\\Projects\\Tensorflow\\workspace\\training_demo\\model_main_tf2.py\", line 106 in main\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 251 in _run_main\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 303 in run\r\n  File \"C:\\Users\\Edward\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40 in run\r\n  File \"C:\\Users\\Edward\\Projects\\Tensorflow\\workspace\\training_demo\\model_main_tf2.py\", line 115 in <module>\r\n```\r\n", "comments": ["@edwardyen724-g \r\nCan you please try with cuda 11.0 and cudnn 8.0 and let us know if you still face the issue.\r\nyou may also refer to similar error issues:#42867, [link](https://github.com/tensorflow/tensorboard/issues/2399)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I have been working on a custom object detection problem with the following specifications:\r\n\r\nOS: Windows 10\r\nCUDA: 11.2\r\ncuDNN: 8.1\r\nTensorflow: 2.5.0\r\nPython: 3.8.10\r\n\r\nand have encountered an identical error. Has anyone discovered a fix yet?"]}, {"number": 50237, "title": "tf.lite.OpsSet.SELECT_TF_OPS does not work properly in tf.math.erf", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 and Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): pip install and build from source\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly==2.6.0-dev20210612 and \r\n tensorflow==2.5.0 and tensorflow==2.4.1\r\n\r\n### 2. Code\r\n- Minimal reproducible Colabo URLs.\r\nhttps://colab.research.google.com/drive/1raok6fu6uUR_VC64cg_5nRfcRFyNLzPJ?usp=sharing\r\n\r\n### 3. Any other info / logs\r\nThe problem is simple: only **`tf.math.erf`** is not properly recognized as a target of **`tf.lite.OpsSet.SELECT_TF_OPS`**. The results are the same for **`tf-nightly`**, **`v2.5.0`**, and **`v2.4.1`**.\r\n- from_keras_model\r\n```python\r\nx = Input(shape=(128,128,3), batch_size=1, dtype=tf.float32, name='input')\r\ny = tf.math.erf(x=x)\r\n\r\nmodel = Model(inputs=x, outputs=y)\r\nmodel.summary()\r\nmodel.save('saved_model')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"test.tflite\", \"wb\").write(tflite_model)\r\n```\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput (InputLayer)           [(1, 128, 128, 3)]        0         \r\n_________________________________________________________________\r\ntf.math.erf (TFOpLambda)     (1, 128, 128, 3)          0         \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nINFO:tensorflow:Assets written to: saved_model/assets\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nINFO:tensorflow:Assets written to: /tmp/tmpk9q0o_st/assets\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    293                                                  debug_info_str,\r\n--> 294                                                  enable_mlir_converter)\r\n    295       return model_str\r\n\r\n5 frames\r\nException: /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:421:0: error: 'tf.Erf' op is neither a custom op nor a flex op\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1030:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/tflite_keras_util.py:184:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:672:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:999:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3289:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3444:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3050:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:764:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1273:0: note: called from\r\n<unknown>:0: error: failed while converting: 'main': \r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: Erf\r\nDetails:\r\n\ttf.Erf {device = \"\"}\r\n\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    295       return model_str\r\n    296     except Exception as e:\r\n--> 297       raise ConverterError(str(e))\r\n    298 \r\n    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:421:0: error: 'tf.Erf' op is neither a custom op nor a flex op\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1030:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/tflite_keras_util.py:184:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:672:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:999:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3289:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3444:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3050:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:764:0: note: called from\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1273:0: note: called from\r\n<unknown>:0: error: failed while converting: 'main': \r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: Erf\r\nDetails:\r\n\ttf.Erf {device = \"\"}\r\n```\r\n- from_saved_model\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"test.tflite\", \"wb\").write(tflite_model)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    293                                                  debug_info_str,\r\n--> 294                                                  enable_mlir_converter)\r\n    295       return model_str\r\n\r\n4 frames\r\nException: <unknown>:0: error: loc(callsite(callsite(\"model/tf.math.erf/Erf@__inference__wrapped_model_20\" at \"PartitionedCall@__inference_signature_wrapper_69\") at \"PartitionedCall\")): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n<unknown>:0: error: failed while converting: 'main': \r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: Erf\r\nDetails:\r\n\ttf.Erf {device = \"\"}\r\n\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    295       return model_str\r\n    296     except Exception as e:\r\n--> 297       raise ConverterError(str(e))\r\n    298 \r\n    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(callsite(callsite(\"model/tf.math.erf/Erf@__inference__wrapped_model_20\" at \"PartitionedCall@__inference_signature_wrapper_69\") at \"PartitionedCall\")): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n<unknown>:0: error: failed while converting: 'main': \r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: Erf\r\nDetails:\r\n\ttf.Erf {device = \"\"}\r\n```", "comments": ["Use the following code snippet instead of `converter.target_ops`.\r\n```\r\nconverter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\n```\r\n\r\nVerified working with the CoLab.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50237\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50237\">No</a>\n", "Hello,\r\n\r\nI am facing similr issue with tf.Roll function. Although I see the exhaustive list of tf.ops support by tf.Lite includes the Roll operation. My code looks like this for tfLite:\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n\r\nError:\r\nException: /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/signal/fft_ops.py:459:0: error: 'tf.Roll' op is neither a custom op nor a flex op\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n/global/cscratch1/sd/jmunshi/4DCrystal/CNNTrainingNetwork/Crystal4d/utils/conv_utils.py:26:0: note: called from\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/layers/core.py:917:0: note: called from\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012:0: note: called from\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/functional.py:560:0: note: called from\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/functional.py:424:0: note: called from\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012:0: note: called from\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saving_utils.py:135:0: note: called from\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.Roll {device = \"\"}", "TensorFlow v2.5.0\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n\r\n# Create a model\r\ni = tf.keras.layers.Input(shape=[5], batch_size=1)\r\no = tf.roll(i, shift=2, axis=0)\r\nmodel = tf.keras.models.Model(inputs=i, outputs=o)\r\nmodel.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = False\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\ntflite_model = converter.convert()\r\nopen(\"test.tflite\", \"wb\").write(tflite_model)\r\n```\r\n```\r\n$ python3 test.py \r\n2021-08-13 00:02:24.777874: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-08-13 00:02:25.560136: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-08-13 00:02:25.757922: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-08-13 00:02:25.778394: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-08-13 00:02:25.778411: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ubuntu2004\r\n2021-08-13 00:02:25.778416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ubuntu2004\r\n2021-08-13 00:02:25.778450: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\r\n2021-08-13 00:02:25.778465: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.57.2\r\n2021-08-13 00:02:25.778470: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.57.2\r\n2021-08-13 00:02:25.778633: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(1, 5)]                  0         \r\n_________________________________________________________________\r\ntf.roll (TFOpLambda)         (1, 5)                    0         \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\n2021-08-13 00:02:25.853928: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2021-08-13 00:02:25.919016: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2021-08-13 00:02:25.919092: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\r\n2021-08-13 00:02:25.939759: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3699850000 Hz\r\n2021-08-13 00:02:25.939995: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: function_optimizer did nothing. time = 0.005ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n\r\n2021-08-13 00:02:25.944413: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2021-08-13 00:02:25.944452: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\r\n2021-08-13 00:02:25.945278: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\r\n  constant_folding: Graph size after: 5 nodes (0), 4 edges (0), time = 0.157ms.\r\n  constant_folding: Graph size after: 5 nodes (0), 4 edges (0), time = 0.055ms.\r\n\r\nWARNING:absl:Please consider switching to the new converter by setting experimental_new_converter=True. The old converter (TOCO) is deprecated.\r\n```\r\n![Screenshot 2021-08-13 00:03:38](https://user-images.githubusercontent.com/33194443/129220504-867bdd54-ff64-4df4-b5df-b1ae9e6c7c63.png)\r\n", "We don:t recommend using the `converter.experimental_new_converter = False` because it is a pretty old converter one.\r\n\r\nPlease upload a new post if you experience some problems to keep each post simple and isolated.", "I'm having a similar situation. I'm trying to convert mobilebert model into tflite from [here](https://github.com/tensorflow/models/blob/master/official/projects/edgetpu/nlp/serving/export_tflite_squad.py).\r\nI modified the flag of converter in the export_tflite_squad.py like the following:\r\n```\r\nif FLAGS.quantization_method in ['full-integer']:\r\n  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\r\n  converter.allow_custom_ops=True\r\n  converter.experimental_new_converter=True\r\n  converter.inference_input_type = tf.int8\r\n  converter.inference_output_type = tf.int8 #tf.float32\r\n  converter.representative_dataset = _representative_dataset\r\n```\r\nAnd I still got the following error:\r\n```\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_0/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_1/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_2/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_3/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_4/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_5/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_6/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_7/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_8/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_9/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_10/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_11/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_12/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_13/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_14/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_15/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_16/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_17/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_18/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_19/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_20/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_21/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_22/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_23/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.Erf' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main':\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nTF Select ops: Erf\r\nDetails:\r\n\ttf.Erf(tensor<1x384x4096xf32>) -> (tensor<1x384x4096xf32>) : {device = \"\"}\r\n\r\nTraceback (most recent call last):\r\n  File \"./official/projects/edgetpu/nlp/serving/export_tflite_squad.py\", line 184, in <module>\r\n    app.run(main)\r\n  File \"/home/khsu4/GPTPU/tfnightly/lib/python3.7/site-packages/absl/app.py\", line 312, in run\r\n    _run_main(main, args)\r\n  File \"/home/khsu4/GPTPU/tfnightly/lib/python3.7/site-packages/absl/app.py\", line 258, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"./official/projects/edgetpu/nlp/serving/export_tflite_squad.py\", line 175, in main\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/khsu4/GPTPU/tfnightly/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 785, in wrapper\r\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n  File \"/home/khsu4/GPTPU/tfnightly/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 771, in _convert_and_export_metrics\r\n    result = convert_func(self, *args, **kwargs)\r\n  File \"/home/khsu4/GPTPU/tfnightly/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1052, in convert\r\n    result = _convert_saved_model(**converter_kwargs)\r\n  File \"/home/khsu4/GPTPU/tfnightly/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py\", line 213, in wrapper\r\n    raise converter_error from None  # Re-throws the exception.\r\n  File \"/home/khsu4/GPTPU/tfnightly/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py\", line 206, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/khsu4/GPTPU/tfnightly/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 861, in convert_saved_model\r\n    enable_mlir_converter=True)\r\n  File \"/home/khsu4/GPTPU/tfnightly/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 311, in toco_convert_protos\r\n    raise converter_error\r\ntensorflow.lite.python.convert_phase.ConverterError: <unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_0/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_0/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_1/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_1/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_2/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_2/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_3/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_3/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_4/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_4/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_5/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_5/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_6/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_6/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_7/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_7/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_8/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_8/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_9/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_9/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_10/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_10/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_11/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_11/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_12/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_12/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_13/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_13/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_14/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_14/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_15/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_15/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_16/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_16/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_17/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_17/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_18/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_18/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_19/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_19/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_20/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_20/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_21/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_21/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_22/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_22/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_23/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.Erf' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"Erf:\", \"model/bert_span_labeler/mobile_bert_encoder/transformer_layer_23/ffn_layer_0/intermediate_dense/Gelu/Erf@__inference__wrapped_model_21884\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_73223\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: failed while converting: 'main':\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nTF Select ops: Erf\r\nDetails:\r\n\ttf.Erf(tensor<1x384x4096xf32>) -> (tensor<1x384x4096xf32>) : {device = \"\"}\r\n```\r\nBoth tf2.4.0 and tf-nightly=2.8.0-dev20211206 are having the same issue here. "]}, {"number": 50236, "title": "[MLIR][DISC] Bufferize TransposeOp and ConcatenateOp", "body": "support hlo-to-lhlo conversion for TransposeOp and ConcatenateOp", "comments": []}, {"number": 50235, "title": "Updated call method in base layer", "body": "Added a description about enabling eager mode for debugging purpose.\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/36757", "comments": ["Please send this to keras-team/keras, which is the source of truth now."]}, {"number": 50234, "title": "[TF-TRT] Throwing error if compiling with NVIDIA TensorRT < 7", "body": "This PR blocks TF compilation with TRT_VERSION < 7, effectively removing support for TRT 5 and 6.", "comments": ["I had to force push a few times to fix some PyLint issues ", "@bixia1 any comment on the PR ? Or we are good to merge as soon as we get approval from Pankaj & Sanjoy ?", "I put some detail on our weekly meeting doc. It was decided that we don't need to issue such a warning as TF nightly build have been switch to TRT 7 a while back; and we will do the code change for removing the support at the end of June.", "@bixia1 modifications executed as requested.\r\n\r\nFeel free to rapidly test: https://www.onlinegdb.com/online_c++_compiler\r\n\r\n```cpp\r\n#include <iostream>\r\n\r\n#define NV_TENSORRT_MAJOR 6\r\n#define NV_TENSORRT_MINOR 0\r\n#define NV_TENSORRT_PATCH 0\r\n#define NV_TENSORRT_BUILD 0\r\n\r\n#define IS_TRT_VERSION_GE(major, minor, patch, build)           \\\r\n  ((NV_TENSORRT_MAJOR > major) ||                               \\\r\n   (NV_TENSORRT_MAJOR == major && NV_TENSORRT_MINOR > minor) || \\\r\n   (NV_TENSORRT_MAJOR == major && NV_TENSORRT_MINOR == minor && \\\r\n    NV_TENSORRT_PATCH > patch) ||                               \\\r\n   (NV_TENSORRT_MAJOR == major && NV_TENSORRT_MINOR == minor && \\\r\n    NV_TENSORRT_PATCH == patch && NV_TENSORRT_BUILD >= build))\r\n\r\n#if ! IS_TRT_VERSION_GE(7, 0, 0, 0)    \r\n    #error From version 2.6, we only support NVIDIA TensorRT version 7 or newer.\r\n    #error Please update your environment and relaunch the compilation.\r\n#endif\r\n\r\nusing namespace std;\r\n\r\nint main()\r\n{\r\n    cout<<\"Hello World\";\r\n\r\n    return 0;\r\n}\r\n```", "@bixia1 comments addressed"]}, {"number": 50233, "title": "ValueError: numpy.ndarray size changed with numpy 1.19.5", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Windows 10 \r\n- TensorFlow installed from package: python-tensorflow\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.7\r\n- Numpy: 1.19.5\r\n- Installed using virtualenv? pip? conda?: Conda\r\n\r\nWhen I try to train my model I get the error \r\n`ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject`\r\nThe solutions I found online all tell me to upgrade numpy to 1.20; however, as far as I am aware that has compatibility issues with Tensorflow. \r\nI run the command \r\n`python model_main_tf2.py --pipeline_config_path=training/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.config --model_dir=training --alsologtostderr`\r\n\r\n\r\n**Logs*\r\n\r\n> 2021-06-11 19:10:57.108835: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-06-11 19:10:57.110211: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 32, in <module>\r\n    from object_detection import model_lib_v2\r\n  File \"C:\\Users\\Mohidul Efaz\\anaconda3\\envs\\trash\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 29, in <module>\r\n    from object_detection import eval_util\r\n  File \"C:\\Users\\Mohidul Efaz\\anaconda3\\envs\\trash\\lib\\site-packages\\object_detection\\eval_util.py\", line 35, in <module>\r\n    from object_detection.metrics import coco_evaluation\r\n  File \"C:\\Users\\Mohidul Efaz\\anaconda3\\envs\\trash\\lib\\site-packages\\object_detection\\metrics\\coco_evaluation.py\", line 25, in <module>\r\n    from object_detection.metrics import coco_tools\r\n  File \"C:\\Users\\Mohidul Efaz\\anaconda3\\envs\\trash\\lib\\site-packages\\object_detection\\metrics\\coco_tools.py\", line 51, in <module>\r\n    from pycocotools import coco\r\n  File \"C:\\Users\\Mohidul Efaz\\anaconda3\\envs\\trash\\lib\\site-packages\\pycocotools\\coco.py\", line 55, in <module>\r\n    from . import mask as maskUtils\r\n  File \"C:\\Users\\Mohidul Efaz\\anaconda3\\envs\\trash\\lib\\site-packages\\pycocotools\\mask.py\", line 3, in <module>\r\n    import pycocotools._mask as _mask\r\n  File \"pycocotools\\_mask.pyx\", line 1, in init pycocotools._mask\r\nValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject", "comments": ["@Mabedin00 \r\nplease confirm if you followed as per this [link](https://stackoverflow.com/questions/66060487/valueerror-numpy-ndarray-size-changed-may-indicate-binary-incompatibility-exp) and restarted your system. [pip install pycocotools==2.0.0]\r\n\r\nFor the error logs shared please follow:[link](https://github.com/tensorflow/tensorflow/issues/48868#issuecomment-841396124)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50233\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50233\">No</a>\n", "Is Tensorflow 2.6.0 compatible with numpy 1.20?", "@Zethson \r\nPlease follow [this pr](https://github.com/tensorflow/tensorflow/pull/48935) for numpy 1.20 compatibility"]}, {"number": 50232, "title": "Why is cudaFree(nullptr) needed?", "body": "Per code here: https://github.com/tensorflow/tensorflow/commit/4535bd5df4d077072a8f207146bf4cd051971237#diff-b0331d3a83e4697c835bec57bc002586de8a3f93b520cad43edf07218dc0ea0eR748, it is calling `cudaFree(nullptr)` as part of cuda initialization. However based on CUDA doc: https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094, cudaFree(nullptr) performs no operation. Wonder why this is needed here? CCed @yzhwang ", "comments": ["We're likely relying on `cudaFree(nullptr)` to implicitly initialize the GPU.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50232\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50232\">No</a>\n", "@sanjoy thanks for the answer. Can you elaborate a bit? Like why would `cudaFree(nullptr)` initialize GPU? Based on the cuda doc cudaFree with null pointer performs no operation.  The reason I'm asking about the necessity of it is we are seeing error: \r\n`CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory` when calling this API.", "@sanjoy Based on CUDA doc, `cudaSetDevice()` called before `cudaFree` (https://github.com/tensorflow/tensorflow/commit/4535bd5df4d077072a8f207146bf4cd051971237#diff-b0331d3a83e4697c835bec57bc002586de8a3f93b520cad43edf07218dc0ea0eR743)\r\nshould already initialize the CUDA context, so do you think this is still needed? Happy to contribute a PR if this is the case.\r\nhttps://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DRIVER.html\r\n```\r\nPrimary Contexts\r\n\r\nThere exists a one to one relationship between CUDA devices in the CUDA Runtime API and CUcontext s in the CUDA Driver API within a process. The specific context which the CUDA Runtime API uses for a device is called the device's primary context. From the perspective of the CUDA Runtime API, a device and its primary context are synonymous.\r\n\r\nInitialization and Tear-Down\r\n\r\nCUDA Runtime API calls operate on the CUDA Driver API CUcontext which is current to to the calling host thread.\r\n\r\nThe function cudaSetDevice() makes the primary context for the specified device current to the calling thread by calling cuCtxSetCurrent().\r\n\r\nThe CUDA Runtime API will automatically initialize the primary context for a device at the first CUDA Runtime API call which requires an active context. If no CUcontext is current to the calling thread when a CUDA Runtime API call which requires an active context is made, then the primary context for a device will be selected, made current to the calling thread, and initialized.\r\n\r\n```  ", "Thanks for the info, a PR would be great!"]}, {"number": 50231, "title": "Basic image classification tutorial missing minor step", "body": "URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/keras/classification\r\n\r\nThe step where the last plot is being drawn (following code) is missing `plt.show()` to display the plot, despite the plot being shown as displayed in the doc:\r\n\r\n```\r\nplot_value_array(1, predictions_single[0], test_labels)\r\n_ = plt.xticks(range(10), class_names, rotation=45)\r\n# missing plt.show() here\r\n```", "comments": ["You may want to try running the colab and check. All the plots are displayed as per the website example.", "The plot *is* displayed when I run the colab, but I was running the tutorial snippets in a terminal and the plot isn't displayed without `plt.show()`", "hi @mrpdaemon I did a quick check about this. In section \"Notes\" of [1] it says:\r\n\r\n> The jupyter backends (activated via %matplotlib inline, %matplotlib notebook, or %matplotlib widget), call show() at the end of every cell by default. Thus, you usually don't have to call it explicitly there.\r\n\r\n[1] https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html", "> plot_value_array(1, predictions_single[0], test_labels)\r\n> _ = plt.xticks(range(10), class_names, rotation=45)\r\n> # missing plt.show() here\r\n\r\n@mrpdaemon Thanks, SGTM. Will add this!", "Created pr [#1909](https://github.com/tensorflow/docs/pull/1909) request , the issue will be closed once the pr will be merged.", "A fix is on its way - you'll see the commit eventually. Thank you @Saduf2019 @mrpdaemon ", "Done https://github.com/tensorflow/docs/commit/5b21901487f0ed10083676f44a1bc5e1b0862c8b\r\n\r\nThanks again", "Moving this to closed status as the changes are implemented."]}, {"number": 50230, "title": "IRFFT much slower than RFFT on GPU", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: NVIDIA Geforce 970 M (but I've observed similar on other NVIDIA GPUs)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nIRFFT takes much longer than RFFT (orders of magnitude) when running on the GPU. On the CPU, they take about the same amount of time.\r\n\r\n**Describe the expected behavior**\r\n\r\nIRFFT may be somewhat longer than RFFT, but hopefully at most a few times longer.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndims = 1024\r\nseq_len = 1024\r\n\r\nuse_gpu = True\r\n\r\ntf.profiler.experimental.start(\r\n    logdir=\"test_fft_convolution_speed_log\" + (\"\" if use_gpu else \"_cpu\")\r\n)\r\n\r\nwith tf.device(\"/GPU:0\" if use_gpu else \"/CPU:0\"):\r\n    u = tf.random.uniform(shape=(dims, seq_len), minval=-1, maxval=1, seed=0)\r\n    v = tf.random.uniform(shape=(dims, seq_len), minval=-1, maxval=1, seed=0)\r\n\r\n    fu = tf.signal.rfft(u, fft_length=[2 * seq_len], name=\"fu\")\r\n    fv = tf.signal.rfft(v, fft_length=[2 * seq_len], name=\"fv\")\r\n\r\n    fw = fu * fv\r\n    w = tf.signal.irfft(fw, fft_length=[2 * seq_len], name=\"iw\")[..., :seq_len]\r\n\r\n    print(w.numpy()[0, 0])\r\n\r\ntf.profiler.experimental.stop()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Here's a picture of the trace viewer for the GPU, as well as the whole profile dir. In the picture, I've selected the ops on the GPU starting at about 650 ms and extending to the end of the IRFFT. You can see that the IRFFT is much longer than the RFFTs.\r\n\r\n![tensorflow-irfft-profile](https://user-images.githubusercontent.com/1952250/121748991-e98c5680-cad7-11eb-8b75-339469e77fcc.png)\r\n\r\n[test_fft_convolution_speed_log.zip](https://github.com/tensorflow/tensorflow/files/6640992/test_fft_convolution_speed_log.zip)\r\n", "I was able to reproduce the code.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/44069a362db59c2f59ddd05b0c80a915/50230.ipynb).", "Looking at this again, the GPU portion of the RFFT is very fast, but there's a CPU portion that appears to take much longer. Why is this the case?", "![ifft-profile](https://user-images.githubusercontent.com/1952250/121915581-559dd300-cd01-11eb-94e2-bfe9840b248d.png)\r\n\r\nZooming in on the part around 650 ms, you can see what's actually happening in the \"compute\" stream. There, the RFFTs, mul, and IRFFT all happen quite quickly. But then the IRFFT tensorflow op takes forever afterwards, and it's not clear what it's doing (it appears to be doing nothing). ", "You can find the similar issue discussion here #6541.", "@hunse I tried running your code and noticed similar computational time. Please check [this gist](https://colab.research.google.com/gist/jvishnuvardhan/ae40b14b21fd85dfb09fdf113d1571e1/50230.ipynb) and let me know if I am missing anything here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50229, "title": "Custom tflite writer for pre-quantized model", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nHello,\r\n\r\nI am looking for some help in the area of tflite write for conversion of a .pb file to a tflite(flatbuffer file).\r\n1. I currently have a custom pre quantized optimized model, and I want to write this pretrained file to tflite format.\r\n2. Being a custom quantized model, stock converter provided by tensorflow is not an ideal match for now.\r\n3. As I have already optimized the pre-trained model, I want my conv layers to stored as int8 instead of float32, as the weights are in integer format.\r\n4. Can we tweak the converter and make the datatype of conv layers as int8.\r\n\r\nThanks\r\n", "comments": ["@mukul74 ,\r\n\r\nPlease go through the links which provide more information of TFlite models.[Link1](https://www.tensorflow.org/lite/guide#development_workflow),[Link2](https://www.tensorflow.org/lite/examples),[Link3](https://stackoverflow.com/questions/50632152/tensorflow-convert-pb-file-to-tflite-using-python).\r\n\r\nAlso, this is not a TensorFlow bug or feature request, it is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a larger community that reads questions there. Thanks!", "Thanks, for the response."]}, {"number": 50228, "title": " Error while Installing tensorflow nightly ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-windows 10:\r\n\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\nTensorFlow installed from (source or binary): pip install tf-nightly\r\n\r\nTensorFlow version:2.5.0\r\n\r\nPython version:3.9\r\n\r\nInstalled using virtualenv? pip? conda?: pip\r\n\r\nBazel version (if compiling from source):\r\n\r\nGCC/Compiler version (if compiling from source):\r\n\r\nCUDA/cuDNN version:\r\n\r\nGPU model and memory: 6 gb ram and gt71\r\n\r\n**Describe the problem**\r\nThrowing an exception error\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFile \"c:\\users\\ts\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pip_vendor\\resolvelib\\resolvers.py\", line 171, in _merge_into_criterion\r\ncrit = self.state.criteria[name]\r\nKeyError: 'tf-nightly-gpu'\r\n\r\nDuring handling of the above exception, another exception occurred:below\r\n\r\n**Any other info / logs**\r\n`raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\r\npip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out`\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@ajinkeya007 \r\n\r\nIt looks like a timeout issue. Please refer similar issue [#48034](https://github.com/tensorflow/tensorflow/issues/48034). Please let us know if it helps.Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50228\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50228\">No</a>\n"]}, {"number": 50227, "title": "Using tf.map_fn but getting error: 'Iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.'", "body": "I am trying to implement a custom layer by wrapping around a function that reshapes my data. I got the following error `Iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.'`\r\n\r\nThis happened upon trying to use my custom class, i.e: \r\n```\r\ninput = keras.Input(shape=3)\r\nx = SparseConv2D(10)(input)\r\nx = DenseFromSparse()(x)\r\nx = layers.Flatten()(x)\r\nx = layers.Dense(128, activation='relu')(x)\r\nx = layers.Dense(NUM_CLASSES, activation='softmax')(x)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(sparse_x, y_train, epochs=5)\r\n```\r\nHere is my code for the custom class that is being problematic:\r\n\r\n```\r\n@tf.function\r\ndef dense_from_sparse(sparse_data_batch):\r\n  \"\"\"Transforms a sparse batch of data into the conventional data batch format\r\n  used in CV applications.\r\n    Args:\r\n      padded_coordinates: A `Tensor` of type `int32`.\r\n        [batch_size, max_num_coords_per_batch, 2], the padded 2D\r\n        coordinates. max_num_coords_per_batch is the max number of coordinates in\r\n        each batch item.\r\n      num_valid_coordinates: A `Tensor` of type `int32`.\r\n        [batch_size], the number of valid coordinates per batch\r\n        item. Only the top num_valid_coordinates[i] entries in coordinates[i],\r\n        padded_features[i] are valid. The rest of the entries\r\n        are paddings.\r\n      padded_features: A `Tensor` of type `float32`.\r\n        [batch_size, max_num_coords_per_batch, in_channels] where\r\n        in_channels is the channel size of the input feature.\r\n    Returns: \r\n      data_batch: A 'Tensor' of type float32,\r\n      [batch_size, image_width, image_height]\"\"\"\r\n  \r\n  indices, num_valid_coordinates, padded_features = sparse_data_batch\r\n  batch_size = padded_features.shape[0]\r\n  sparse_tensors = tf.map_fn(sparse_tensor_fn, tf.range(batch_size))\r\n  dense_batch = tf.map_fn(tf.sparse.to_dense, sparse_tensors)\r\n  return dense_batch\r\n\r\n\r\nclass DenseFromSparse(tf.keras.layers.Layer):\r\n  \r\n  def call(self, inputs):\r\n    return dense_from_sparse(inputs)\r\n\r\n\r\n@tf.function\r\ndef sparse_tensor_fn(i):\r\n  sparse_tensor = tf.sparse.SparseTensor(indices=indices[i][:num_valid_coordinates[i]], \r\n                    values=tf.squeeze(padded_features[i][:num_valid_coordinates[i]]),dense_shape=dense_shape)\r\n  return sparse_tensor\r\n```\r\n\r\nAlso any best practices for implementing and debugging custom classes would be greatly appreciated.", "comments": ["@radiradev ", "Please provide a concise complete code to reproduce the issue faced. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50227\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50227\">No</a>\n"]}, {"number": 50226, "title": "[RNN] Rolled SimpleRNN and GRU TFLite conversion", "body": "### 1. System information\r\nlocal server\r\n- OS Platform and Distribution: Ubuntu 16.04.4 LTS\r\n- TensorFlow installation: pip package\r\n- TensorFlow library version: 2.6.0-dev20210603\r\n\r\ncolab\r\n- OS Platform and Distribution: Ubuntu 18.04.5 LTS\r\n- TensorFlow installation: pip package\r\n- TensorFlow library version: 2.6.0-dev20210611\r\n\r\n### 2. Code\r\n\r\nTo reproduce the issue, an end-to-end example is in: https://colab.research.google.com/drive/1_e_pcvIjeuUA_OdqpAYeJhedAAjP2UbL?usp=sharing\r\n\r\n### 3. Question\r\nIn the colab above, we can see using the fused or unfused way of converting rolled GRU/SimpleRNN layer will result in the same error in TFLite converter: `\"ValueError: Failed to parse the model: Only models with a single subgraph are supported, model had 3 subgraphs.\"`\r\nIf the conversion code in the example is correct, we are wondering whether there is any plan to support the rolled GRU/SimpleRNN conversion?\r\n@daverim @jianlijianli @wwwind @akarmi for visibility", "comments": ["The above error shows that it is failed at the Model optimization toolkit. Model optimization toolkit has a limitation on supporting control flow ops.", "Hi, this is a known issue, and I'm actively working on this. Going to update the thread once it's resolved.", "Thank you both @abattery @xhae ", "Multiple subgraph support is now enabled with https://github.com/tensorflow/tensorflow/commit/cd5a9ad1338b01f5fd8ce64ed86ad09bf26468ee", "Hi, I tried the stabel tf 2.6.0 version which was released on 11 August on pypi (installed 14.Sept). And the error is still there. Is _cd5a9ad_ added to this 2.6.0 version or the nightly build? \r\nThanks", "Please try out the tf-nightly version instead.", "> Please try out the tf-nightly version instead.\r\n\r\nThanks for your quick answer. The nightly gives some other errors which I hoped to circumvent with the stable one. Anyways, onwards to new debugging adventures :-D"]}, {"number": 50225, "title": ".predict() method when using saved & loaded subclassing model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Windows/Linux as well)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0  (Google Colab default one)\r\n- Python version:  3.7.10 (Google Colab default one)\r\n\r\n**Describe the current behavior**\r\nWhen loading a specific custom subclassing model (overriding the train_step and multiple inputs), the `.predict()` method doesn't work. Where as it's working when the model is defined (and not loaded) and it's also working by using the `.call()` method on the loaded model (see [gist](https://colab.research.google.com/gist/quetil/2b48f95c3a18c0dd6e5fc39e83109a12/save-and-predict-with-custom-class.ipynb?authuser=1#scrollTo=ngyMn-qeuhov)).\r\n\r\nThe following error message:\r\n```\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n      Positional arguments (2 total):\r\n        * (<tf.Tensor 'inputs:0' shape=(32, 2) dtype=float32>, <tf.Tensor 'inputs_1:0' shape=(32, 2) dtype=float32>)\r\n        * False\r\n      Keyword arguments: {}\r\n    \r\n    Expected these arguments to match one of the following 4 option(s):\r\n    \r\n    Option 1:\r\n      Positional arguments (2 total):\r\n        * [TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs/1')]\r\n        * False\r\n      Keyword arguments: {}\r\n```\r\nThe option 1 is really matching with the inputs received though...\r\n\r\n**Describe the expected behavior**\r\n The `.predict()` method should work on loaded model.\r\n\r\n**Standalone code to reproduce the issue**\r\nI tried to do a simple example to show you the issue.\r\n[Gist](https://colab.research.google.com/gist/quetil/2b48f95c3a18c0dd6e5fc39e83109a12/save-and-predict-with-custom-class.ipynb?authuser=1#scrollTo=ngyMn-qeuhov)\r\n\r\nThank you in advance for you help, I imagine something is missing in the definition of the class to be able to use the `predict()` method on saved & loaded model.\r\n", "comments": ["Ok, I may find the issue. It sounds to be a mix between list and tuples. \r\nSee [the following gist](https://colab.research.google.com/gist/quetil/957428b50901fa4020fd9c60fbeed5e1/save-and-predict-with-custom-class-tuple.ipynb?authuser=1) where all is working well.\r\n\r\nIf someone can confirm that the error made, I'd by happy.\r\nThen, would it be worth to add a warning message, when the user is using list where tuple should be recommended? :)", "Was able to reproduce the issue with TF 2.5 with list but when using tuple predict method is working fine. Please find the [gist1](https://colab.research.google.com/gist/saikumarchalla/f3a50f6290b02777830985fe1c640d66/save-and-predict-with-custom-class.ipynb?authuser=1#scrollTo=cdwieu1bvCOn) and  [gist2](https://colab.research.google.com/gist/saikumarchalla/e16d40cf5e0031156b1ecd7d402855a5/save-and-predict-with-custom-class-tuple.ipynb?authuser=1#scrollTo=cdwieu1bvCOn). Thanks!", "Hi, Please try the solution mentioned [here](https://github.com/tensorflow/tensorflow/issues/37973#issuecomment-605448707).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50225\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50225\">No</a>\n"]}, {"number": 50224, "title": "[Feature request] Ability to specify TensorSpec.name in tf.Dataset", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5.0 (Colab)\r\n- Are you willing to contribute it (Yes/No): \u274c\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nPlease see an example Colab notebook [here](https://colab.research.google.com/drive/1CAPt7TqQrwaHA2WoHqx0VB-ybUCxqtTk?usp=sharing).\r\n\r\nConsider a `tf.Dataset` of the following form:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef gen():\r\n    # some data generator\r\n    for x in range(10):\r\n        yield {\r\n            'x': np.arange(x + 1),\r\n            'y': np.arange(15).reshape(3, 5).astype(np.float32),\r\n        }\r\n\r\ntf_dataset = tf.data.Dataset.from_generator(\r\n    gen,\r\n    output_shapes={\r\n        'x': (None,),\r\n        'y': (None, 5)\r\n    },\r\n    output_types={\r\n        'x': tf.int32,\r\n        'y': tf.float32,\r\n    },\r\n)\r\n\r\n# tf_dataset.element_spec contains the following:\r\n# {'x': TensorSpec(shape=(None,), dtype=tf.int32, name=None),\r\n#  'y': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)}\r\n# There seems to be no API to specify the name fields.\r\n\r\n# An attempt to set the name directly fails with AttributeError: can't set attribute\r\ntf_dataset.element_spec['x'].name = 'x'\r\n```\r\n\r\nIt would be desirable to have an API to set `TensorSpec.name`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThere could be an extra option for `from_generator`, such as `output_names`, that would specify tensor names for the generator output.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who needs to debug `keras.Model.fit()`. When there is a mismatch between what the model expects and what the dataset provides, one can see in the debugger exactly which tensors are yielded from the dataset. Currently, since Keras models can only receive tuples as inputs, there is no way to annotate these tensors other than via `Tensor.name` (if Keras models could receive dicts, there would have been no such problem).", "comments": ["@dniku you can try the following approach:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef gen():\r\n    # some data generator\r\n    for x in range(10):\r\n        yield {\r\n            'x': np.arange(x + 1),\r\n            'y': np.arange(15).reshape(3, 5).astype(np.float32),\r\n        }\r\n\r\ntf_dataset = tf.data.Dataset.from_generator(\r\n             gen,\r\n             output_signature={\r\n                 'x': tf.TensorSpec(shape=(None,), dtype=tf.int32, name='x'),\r\n                 'y': tf.TensorSpec(shape=(None, 5), dtype=tf.float32, name='y')\r\n             }\r\n         )\r\n\r\nprint(tf_dataset.element_spec)\r\n# {'x': TensorSpec(shape=(None,), dtype=tf.int32, name='x'),\r\n# 'y': TensorSpec(shape=(None, 5), dtype=tf.float32, name='y')}\r\n```", "I seem to have consulted the documentation for an old TF version. Thanks for pointing out the `output_signature` parameter!"]}, {"number": 50223, "title": "tensorflow lite on armv7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  Debian package repo\r\n- TensorFlow version: \r\n- Python version:3.6.9\r\n- Installed using virtualenv? pip? conda?:apt-get\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nInstalled from Debian repo as my OS is ubuntu 16.04 on ARMv7l.\r\nOn apt-get update i see following message:\r\nN: Skipping acquire of configured file 'main/binary-armhf/Packages' as repository 'https://apt.corretto.aws stable InRelease' doesn't support architecture 'armhf'\r\nN: Skipping acquire of configured file 'main/binary-armhf/Packages' as repository 'https://apt.kitware.com/ubuntu bionic InRelease' doesn't support architecture 'armhf'\r\nSubsequently installed : sudo apt-get install python3-tflite-runtime\r\nBut tflite_runtime import failed.\r\nDoes it mean the repo does not support current target hardware\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFollowing are the commands:\r\necho \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\r\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\r\nsudo apt-get update\r\nsudo apt-get install python3-tflite-runtime\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["@yvkrishna91 ,\r\n\r\nCan you please provide the tensorflow version you are using? Thanks!", "I have not installed tensorflow. Intended to use tensorflow lite as specified in tensorflow python quick start guide.\n________________________________\nFrom: tilakrayal ***@***.***>\nSent: Friday, June 11, 2021 9:44:07 PM\nTo: tensorflow/tensorflow ***@***.***>\nCc: VamsiKrishna Yesu ***@***.***>; Mention ***@***.***>\nSubject: Re: [tensorflow/tensorflow] tensorflow lite on armv7 (#50223)\n\n\nATTENTION! This email originates from outside of the organisation. Do not open attachments or click links unless you are sure this email comes from a known sender and you know the content is safe.\n\n\n\n@yvkrishna91<https://apc01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fyvkrishna91&data=04%7C01%7Cvamsikrishna.yesu%40yti.yokogawa.com%7C1394a181d4ce4b9797f008d92cf3f58c%7C05493cda785f4f49b345c2cce5a0e1ef%7C0%7C0%7C637590248588797771%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=nhMOqHAAGTTiFKAEjhNkaPfDPXuReJETF10YIf11pL4%3D&reserved=0> ,\n\nCan you please provide the tensorflow version you are using? Thanks!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://apc01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F50223%23issuecomment-859689779&data=04%7C01%7Cvamsikrishna.yesu%40yti.yokogawa.com%7C1394a181d4ce4b9797f008d92cf3f58c%7C05493cda785f4f49b345c2cce5a0e1ef%7C0%7C0%7C637590248588807770%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=HXzSJuNvhAoH436Vg%2FfnqBD%2BWdYh7clqIGb63qJf7lk%3D&reserved=0>, or unsubscribe<https://apc01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FATUOVPLFEWPBFGYFW326UYDTSIY47ANCNFSM46QQTDOQ&data=04%7C01%7Cvamsikrishna.yesu%40yti.yokogawa.com%7C1394a181d4ce4b9797f008d92cf3f58c%7C05493cda785f4f49b345c2cce5a0e1ef%7C0%7C0%7C637590248588807770%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=gdZasV3%2F4%2BMt9xJwYjo%2B5TQSkysLh1vmKeyO%2BWVdngY%3D&reserved=0>.\n", "@yvkrishna91 \r\nIf you are okay with unofficial, there is an installer that I created.\r\n- TensorFlow Lite stand alone installer\r\nhttps://github.com/PINTO0309/TensorflowLite-bin/tree/main/2.4.1\r\n- TensorFlow installer\r\nhttps://github.com/PINTO0309/Tensorflow-bin", "Thank you\r\nBut the installation failed. Please find the attached error log.\r\n[instalationfailure.txt](https://github.com/tensorflow/tensorflow/files/6642299/instalationfailure.txt)\r\n", "@yvkrishna91 \r\n```\r\n$ sudo pip3 install Cython\r\n```\r\nBecause,\r\n```\r\n  : \r\n ModuleNotFoundError: No module named 'Cython'\r\n  :\r\n```", "@PINTO0309 \r\nCython installation we successful. numpy,pybing11,tflite-runtime fails.\r\n[instalationfailure.txt](https://github.com/tensorflow/tensorflow/files/6642356/instalationfailure.txt)\r\n", "@yvkrishna91 \r\n```\r\n$ sudo apt install swig libjpeg-dev zlib1g-dev python3-dev unzip wget python3-pip curl git cmake make\r\n$ sudo pip3 install pip wheel --upgrade\r\n```\r\nhttps://github.com/PINTO0309/TensorflowLite-bin#usage\r\nor\r\nhttps://github.com/PINTO0309/Tensorflow-bin#usage", "Installation is successful\r\nTried label_image.py and resulted in following error. Tflite module runtime is failing.\r\n[instalationfailure.txt](https://github.com/tensorflow/tensorflow/files/6642404/instalationfailure.txt)\r\n[label_image.txt](https://github.com/tensorflow/tensorflow/files/6642405/label_image.txt)\r\n\r\n\r\n\r\n", "Your environment is strange. Xenial or Bionic? Buster or Stretch? What is the result of the following command?\r\n```\r\n$ uname -a\r\n$ ldd --version\r\n```\r\n![Screenshot 2021-06-13 00:22:36](https://user-images.githubusercontent.com/33194443/121780932-7bca5280-cbdd-11eb-8e63-e9b60e782794.png)", "Linux ubuntu 4.14.164-rt73-ampxlnx #10 SMP PREEMPT RT Wed Feb 19 09:49:00 JST 2020 armv7l armv7l armv7l GNU/Linux\r\nldd --version\r\nldd (Ubuntu GLIBC 2.27-3ubuntu1.4) 2.27\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\nWritten by Roland McGrath and Ulrich Drepper.\r\n\r\nIts Bionic", "I see. Ubuntu 18.04 had not yet created an installer for the armv7l version. :cry: ", "Does it mean it is not possible to have tensorflow lite or mediapipe on this hardware, until Ubuntu creates an installer for armv7l?\r\n", "I just didn't target armv7l (32bit) for my build because I thought the demand for it was low. \r\nDid you use either of the following?\r\n\r\nhttps://github.com/PINTO0309/TensorflowLite-bin/blob/main/2.3.1/download_tflite_runtime-2.3.1-py3-none-linux_armv7l.whl.sh\r\nor\r\nhttps://github.com/PINTO0309/TensorflowLite-bin/blob/main/2.4.0/download_tflite_runtime-2.4.0-py3-none-linux_armv7l.whl.sh", "This is a work-around that you can try in your environment for now. I don't know if it will work.\r\n```\r\n$ wget https://github.com/lhelontra/tensorflow-on-arm/releases/download/v2.4.0/tensorflow-2.4.0-cp35-none-linux_armv7l.whl\r\n$ mv tensorflow-2.4.0-cp35-none-linux_armv7l.whl tensorflow-2.4.0-cp36-none-linux_armv7l.whl\r\n$ sudo pip3 install tensorflow-2.4.0-cp36-none-linux_armv7l.whl\r\n```", "I will try..Do i need to clean up previous environment?", "I'm sorry, but I think you should probably clean it up and try it.", "I have not cleaned it up. But got following error.\r\n[instalationfailure.txt](https://github.com/tensorflow/tensorflow/files/6642485/instalationfailure.txt)\r\nDo you think clean up would solve this error?", "Try\r\n```\r\n$ sudo apt-get install libhdf5-dev\r\n```", "It did not work after installation of libhdf5-dev. Error is still the same,\r\nLoading library to get version: libhdf5.so\r\n  error: libhdf5.so: cannot open shared object file: No such file or directory\r\nAttaching the libhdf5-dev installation log.\r\n[libhdf5-installation_succesful.txt](https://github.com/tensorflow/tensorflow/files/6642519/libhdf5-installation_succesful.txt)\r\n", "```\r\n$ sudo pip3 uninstall h5py\r\n$ sudo apt-get install libhdf5-dev \r\n```", "Please check the attached logs.\r\n[instalationfailure.txt](https://github.com/tensorflow/tensorflow/files/6642534/instalationfailure.txt)\r\n", "Can you please suggest next steps soon?", "Do you want to build a TFLite Python wheel for your target armv7 OS?\r\nDid you try https://www.tensorflow.org/lite/guide/build_cmake_pip ?", "Yes i did, but get following error:\r\ndownloading 'https://storage.googleapis.com/mirror.tensorflow.org/www.kurims.kyoto-u.ac.jp/~ooura/fft2d.tgz' failed\r\n          status_code: 1\r\n          status_string: \"Unsupported protocol\"\r\n          log:\r\n          --- LOG BEGIN ---\r\n          Protocol \"https\" not supported or disabled in libcurl\r\nBut i am able to download other packages\r\nBelow is my curl version:\r\ncurl 7.77.0 (armv7l-unknown-linux-gnueabihf) libcurl/7.77.0 OpenSSL/1.1.1 zlib/1.2.11\r\nRelease-Date: 2021-05-26\r\nProtocols: dict file ftp ftps gopher gophers http https imap imaps mqtt pop3 pop3s rtsp smb smbs smtp smtps telnet tftp\r\nFeatures: alt-svc AsynchDNS HSTS HTTPS-proxy IPv6 Largefile libz NTLM NTLM_WB SSL TLS-SRP UnixSockets\r\n\r\nI have also tried building python wheel:\r\ntensorflow/tools/ci_build/ci_build.sh pi-python37 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh armhf  \r\nbut was struck installing scipy:\r\nDownloading scipy-1.6.3.tar.gz (27.2 MB)\r\nInstalling build dependencies: started\r\n  Installing build dependencies: still running...\r\n", "Is Glib version the root cause? Does upgrading Glib would solve the issue?", "FYI, https://github.com/tensorflow/tensorflow/issues/38642#issuecomment-988572129", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50223\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50223\">No</a>\n"]}, {"number": 50222, "title": "model question", "body": "my problem:\r\nTypeError: in user code:\r\n\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:800 train_step\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:460 update_state\r\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:86 decorated\r\n        update_op = update_state_fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:177 update_state_fn\r\n        return ag_update_state(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:666 update_state  **\r\n        matches, sample_weight=sample_weight)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:424 update_state\r\n        with ops.control_dependencies([value_sum]):\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:5390 control_dependencies\r\n        return get_default_graph().control_dependencies(control_inputs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:363 control_dependencies\r\n        self.control_captures.add(graph_element)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/keras_tensor.py:242 __hash__\r\n        'Instead, use tensor.ref() as the key.' % self)\r\n\r\n    TypeError: Tensors are unhashable. (KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum_1/Sum:0', description=\"created by layer 'tf.math.reduce_sum_1'\"))Instead, use tensor.ref() as the key.\r\nand my model:vae\r\nbatch_size=6\r\nx = Input(shape=(120,120,3))\r\nconv1 = Conv2d_BN(x, 8, (3, 3))\r\nconv1 = Conv2d_BN(conv1, 8, (3, 3))\r\npool1 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv1)\r\nconv2 = Conv2d_BN(pool1, 16, (3, 3))\r\nconv2 = Conv2d_BN(conv2, 16, (3, 3))\r\npool2 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv2)\r\nconv3 = Conv2d_BN(pool2, 32, (3, 3))\r\nconv3 = Conv2d_BN(conv3, 32, (3, 3))\r\npool3 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv3)\r\nconv4 = Conv2d_BN(pool3, 64, (3, 3))\r\nconv4 = Conv2d_BN(conv4, 64, (3, 3))\r\npool4 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv4)\r\nconv5 = Conv2d_BN(pool4, 128, (3, 3))\r\nconv5 = Dropout(0.5)(conv5)\r\nconv5 = Conv2d_BN(conv5, 128, (3, 3))\r\nconv5 = Dropout(0.5)(conv5)\r\nF=Flatten()(conv5)\r\nh = Dense(256, activation='relu')(F)\r\nz_mean = Dense(2)(h)\r\nz_log_var = Dense(2)(h)\r\ndef sampling(args):\r\n    z_mean, z_log_var = args\r\n    #print(type(args))\r\n    #print(type(z_log_var))\r\n    #print(type(z_mean))\r\n    epsilon = K.random_normal(shape=(2,), mean=0.,\r\n                              stddev=1.0)\r\n    #print(type(epsilon))\r\n    return z_mean + K.exp(z_log_var / 2) * epsilon\r\nz = Lambda(sampling,name = 'sampling', output_shape=(2))([z_mean, z_log_var])\r\ndecoder_h = Dense(256, activation='relu')\r\ndecoder_mean = Dense(256, activation='sigmoid')\r\nh_decoded = decoder_h(z)\r\nx_decoded_mean = decoder_mean(h_decoded)\r\ndecoder=Dense(8*8*128,activation='relu')(x_decoded_mean)\r\ndecoder=Reshape((8,8,128))(decoder)\r\ndecoder = Conv2DTranspose(128, kernel_size=(3,3), strides=2)(decoder)\r\ndecoder = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='sigmoid')(decoder)\r\nconvt1 = Conv2dT_BN(decoder, 64, (3, 3))\r\nconvt1 = Dropout(0.5)(convt1)\r\nconv6 = Conv2d_BN(convt1, 64, (3, 3))\r\nconv6 = Conv2d_BN(conv6, 64, (3, 3))\r\nconvt2 = Conv2dT_BN(conv6, 32, (3, 3))\r\nconvt2  = Dropout(0.5)(convt2)\r\nconv7 = Conv2d_BN(convt2 , 32, (3, 3))\r\nconv7 = Conv2d_BN(conv7, 32, (3, 3))\r\nconvt3 = Conv2dT_BN(conv7, 16, (3, 3))\r\nconvt3 = Dropout(0.5)(convt3)\r\nconv8 = Conv2d_BN(convt3, 16, (3, 3))\r\nconv8 = Conv2d_BN(conv8, 16, (3, 3))\r\noutpt = Conv2D(filters=3, kernel_size=(1,1), strides=(1,1), padding='same', activation='sigmoid')(conv8)\r\nvae = Model(inputs=x, outputs=outpt)\r\nvae.compile(optimizer='adam',loss='mean_squared_error',metrics = [KL_loss, recon_loss])\r\nvae.summary()\r\nhow can i solve this question", "comments": ["Does this https://stackoverflow.com/questions/61056781/typeerror-tensor-is-unhashable-instead-use-tensor-ref-as-the-key-in-keras help in your case?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50222\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50222\">No</a>\n"]}, {"number": 50219, "title": "Function Signature of `_resource_apply_gradients` and sparse equivalents in abstract `Optimizer`", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1237-L1298\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIt appears that the function signature is out of date: The second argument is called `handle` and the description indicates that the function expects a `resource` variable pointing to the variable that needs updating, but if we look at the [SGD Optimizer](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/optimizer_v2/gradient_descent.py#L129-L149) for example it appears that in reality the variable itself is passed to this function. This problem is something I could fix myself in a pull request.\r\n\r\nThe second part I am not clear on: What is the return value supposed to be? [cf. StackOverflow Question](https://stackoverflow.com/questions/67902325/what-type-should-the-method-resource-apply-gradient-from-tensorflow-keras-op). In particular I do not know how to construct an operation without using the inbuilt functions used in the provided implementations. Which is an issue if I want to create a custom optimizer.\r\n\r\nI am thinking that \r\n\r\n```python\r\nreturn tf.raw_ops.ResourceApplyGradientDescent(\r\n        var=var.handle,\r\n        alpha=coefficients[\"learning_rate\"],\r\n        delta=grad,\r\n        use_locking=self._use_locking,\r\n)\r\n```\r\n\r\nmight be equivalent to\r\n\r\n```python\r\nreturn var.assign_sub(\r\n    delta=coefficients[\"learning_rate\"]*grad,\r\n    use_locking=self._use_locking,\r\n    read_value=False\r\n)\r\n```\r\nbut I am not sure and I can not find any documentation on that", "comments": ["@FelixBenning ,\r\n\r\nWe see that the[ issue template ](https://github.com/tensorflow/tensorflow/issues/new/choose)has not been filled, could you please do so as it helps us analyse the issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50218, "title": "The method layers.experimental.preprocessing.RandomRotation is not available in M1 chip and macOS", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary from miniconda3\r\n- TensorFlow version (use command below): 2.5\r\n- Python version: 3.9.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: M1 chip\r\n\r\nI have installed Tensorflow 2.5 according to [Tensorflow Installation Instructions for Apple M1](https://developer.apple.com/metal/tensorflow-plugin/)\r\nAnd I wanna run the code from Keras example which is the [Image classification from scratch](https://keras.io/examples/vision/image_classification_from_scratch/)\r\n\r\nWhen I run this code:\r\n```Python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport os\r\n\r\nnum_skipped = 0\r\ndir_path = \"/Users/MyMac/PetImages\"\r\nfor folder_name in (\"Cat\",\"Dog\"):\r\n    folder_path = os.path.join(dir_path,folder_name)\r\n    for fname in os.listdir(folder_path):\r\n        fpath = os.path.join(folder_path, fname)\r\n        try:\r\n            fobj = open(fpath, \"rb\")\r\n            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\r\n        finally:\r\n            fobj.close()\r\n\r\n        if not is_jfif:\r\n            num_skipped += 1\r\n            # Delete corrupted image\r\n            os.remove(fpath)\r\n\r\nprint(\"Deleted %d images\" % num_skipped)\r\n\r\nimage_size = (180, 180)\r\nbatch_size = 32\r\n\r\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n    dir_path,\r\n    validation_split=0.2,\r\n    subset=\"training\",\r\n    seed=1337,\r\n    image_size=image_size,\r\n    batch_size=batch_size,\r\n)\r\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n    dir_path,\r\n    validation_split=0.2,\r\n    subset=\"validation\",\r\n    seed=1337,\r\n    image_size=image_size,\r\n    batch_size=batch_size,\r\n)\r\n\r\ndata_augmentation = keras.Sequential(\r\n    [\r\n        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\r\n        layers.experimental.preprocessing.RandomRotation(0.1),\r\n    ]\r\n)\r\n\r\nplt.figure(figsize=(10, 10))\r\nfor images, _ in train_ds.take(1):\r\n    for i in range(9):\r\n        augmented_images = data_augmentation(images)\r\n        ax = plt.subplot(3, 3, i + 1)\r\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\r\n        plt.axis(\"off\")\r\n```\r\nThe output is:\r\n```Python\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-6-2d5292b3c876> in <module>\r\n      2 for images, _ in train_ds.take(1):\r\n      3     for i in range(9):\r\n----> 4         augmented_images = data_augmentation(images)\r\n      5         ax = plt.subplot(3, 3, i + 1)\r\n      6         plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1028         with autocast_variable.enable_auto_cast_variables(\r\n   1029             self._compute_dtype_object):\r\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\r\n   1031 \r\n   1032         if self._activity_regularizer:\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)\r\n    378       if not self.built:\r\n    379         self._init_graph_network(self.inputs, self.outputs)\r\n--> 380       return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n    381 \r\n    382     outputs = inputs  # handle the corner case where self.layers is empty\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)\r\n    418         a list of tensors if there are more than one outputs.\r\n    419     \"\"\"\r\n--> 420     return self._run_internal_graph(\r\n    421         inputs, training=training, mask=mask)\r\n    422 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)\r\n    554 \r\n    555         args, kwargs = node.map_arguments(tensor_dict)\r\n--> 556         outputs = node.layer(*args, **kwargs)\r\n    557 \r\n    558         # Update tensor_dict.\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1028         with autocast_variable.enable_auto_cast_variables(\r\n   1029             self._compute_dtype_object):\r\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\r\n   1031 \r\n   1032         if self._activity_regularizer:\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in call(self, inputs, training)\r\n    825           interpolation=self.interpolation)\r\n    826 \r\n--> 827     output = control_flow_util.smart_cond(training, random_rotated_inputs,\r\n    828                                           lambda: inputs)\r\n    829     output.set_shape(inputs.shape)\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)\r\n    107     return control_flow_ops.cond(\r\n    108         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n--> 109   return smart_module.smart_cond(\r\n    110       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n    111 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     52   if pred_value is not None:\r\n     53     if pred_value:\r\n---> 54       return true_fn()\r\n     55     else:\r\n     56       return false_fn()\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in random_rotated_inputs()\r\n    816       min_angle = self.lower * 2. * np.pi\r\n    817       max_angle = self.upper * 2. * np.pi\r\n--> 818       angles = self._rng.uniform(\r\n    819           shape=[batch_size], minval=min_angle, maxval=max_angle)\r\n    820       return transform(\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in uniform(self, shape, minval, maxval, dtype, name)\r\n    811             minval=minval, maxval=maxval, name=name)\r\n    812       else:\r\n--> 813         rnd = self._uniform(shape=shape, dtype=dtype)\r\n    814         return math_ops.add(rnd * (maxval - minval), minval, name=name)\r\n    815 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _uniform(self, shape, dtype)\r\n    714   def _uniform(self, shape, dtype):\r\n    715     if compat.forward_compatible(2020, 10, 25):\r\n--> 716       key, counter = self._prepare_key_counter(shape)\r\n    717       return gen_stateless_random_ops_v2.stateless_random_uniform_v2(\r\n    718           shape=shape,\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _prepare_key_counter(self, shape)\r\n    632   def _prepare_key_counter(self, shape):\r\n    633     delta = math_ops.reduce_prod(shape)\r\n--> 634     counter_key = self.skip(delta)\r\n    635     counter_size = _get_counter_size(self.algorithm)\r\n    636     counter = array_ops.bitcast(counter_key[:counter_size], dtypes.uint64)\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in skip(self, delta)\r\n    584     \"\"\"\r\n    585     if compat.forward_compatible(2020, 10, 25):\r\n--> 586       return self._skip(delta)\r\n    587     gen_stateful_random_ops.rng_skip(\r\n    588         self.state.handle, math_ops.cast(self.algorithm, dtypes.int64),\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _skip(self, delta)\r\n    614           return ds_context.get_strategy().extended.update(\r\n    615               self.state, update_fn)\r\n--> 616     return update_fn(self.state)\r\n    617 \r\n    618   def _preprocess_key(self, key):\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in update_fn(v)\r\n    598   def _skip(self, delta):\r\n    599     def update_fn(v):\r\n--> 600       return self._skip_single_var(v, delta)\r\n    601     # TODO(b/170515001): Always call strategy.extended.update after calling it\r\n    602     #   from both replica context and cross-replica context is supported.\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _skip_single_var(self, var, delta)\r\n    592   def _skip_single_var(self, var, delta):\r\n    593     # TODO(wangpeng): Cache the cast algorithm instead of casting everytime.\r\n--> 594     return gen_stateful_random_ops.rng_read_and_skip(\r\n    595         var.handle, alg=math_ops.cast(self.algorithm, dtypes.int32),\r\n    596         delta=math_ops.cast(delta, dtypes.uint64))\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/gen_stateful_random_ops.py in rng_read_and_skip(resource, alg, delta, name)\r\n    113       return _result\r\n    114     except _core._NotOkStatusException as e:\r\n--> 115       _ops.raise_from_not_ok_status(e, name)\r\n    116     except _core._FallbackException:\r\n    117       pass\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6895   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6896   # pylint: disable=protected-access\r\n-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6898   # pylint: enable=protected-access\r\n   6899 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: No registered 'RngReadAndSkip' OpKernel for 'GPU' devices compatible with node {{node RngReadAndSkip}}\r\n\t.  Registered:  device='XLA_CPU_JIT'\r\n  device='CPU'\r\n [Op:RngReadAndSkip]\r\n```\r\n\r\nBut I modify the code like this:\r\n```Python\r\ndata_augmentation = keras.Sequential(\r\n    [\r\n        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\r\n        # layers.experimental.preprocessing.RandomRotation(0.1),\r\n    ]\r\n)\r\n```\r\nIt's running successfully!\r\nI check the Tensorflow document, the procedure to call RandomRotation method is correct.\r\nPlease advise me how to use RandomRotation.\r\nThanks!", "comments": ["@Betai18n \r\nCould you please try using this`tf.keras.layers.experimental.preprocessing.RandomRotation`  instead keras (keras.Sequential(layers.experimental.preprocessing.RandomFlip(\"horizontal\").\r\nFor more info refer this [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomRotation?version=nightly)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50218\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50218\">No</a>\n", "Same situation here. I am using [Tensorflow Plugin](https://developer.apple.com/metal/tensorflow-plugin/) on a Apple M1 MacBook Air. When I try to do the data augmentation like this\u2193 (following the [Tensorflow official tutorial](https://www.tensorflow.org/tutorials/images/classification))\r\n```\r\ndata_augmentation = keras.models.Sequential(\r\n  [\r\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3)),\r\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\r\n    tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\r\n  ]\r\n)\r\n\r\nplt.figure(figsize=(10, 10))\r\nfor images, _ in train_ds.take(1):\r\n  for i in range(9):\r\n    augmented_images = data_augmentation(images)\r\n    ax = plt.subplot(3, 3, i + 1)\r\n    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\r\n    plt.axis(\"off\")\r\n```\r\nIt got the same error message:\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n/var/folders/cs/l7w_c8nd1zsgr8qxl13xbssr0000gn/T/ipykernel_18021/2251583730.py in <module>\r\n     10 for images, _ in train_ds.take(1):\r\n     11   for i in range(9):\r\n---> 12     augmented_images = data_augmentation(images)\r\n     13     ax = plt.subplot(3, 3, i + 1)\r\n     14     plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1028         with autocast_variable.enable_auto_cast_variables(\r\n   1029             self._compute_dtype_object):\r\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\r\n   1031 \r\n   1032         if self._activity_regularizer:\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)\r\n    378       if not self.built:\r\n    379         self._init_graph_network(self.inputs, self.outputs)\r\n--> 380       return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n    381 \r\n    382     outputs = inputs  # handle the corner case where self.layers is empty\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)\r\n    418         a list of tensors if there are more than one outputs.\r\n    419     \"\"\"\r\n--> 420     return self._run_internal_graph(\r\n    421         inputs, training=training, mask=mask)\r\n    422 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)\r\n    554 \r\n    555         args, kwargs = node.map_arguments(tensor_dict)\r\n--> 556         outputs = node.layer(*args, **kwargs)\r\n    557 \r\n    558         # Update tensor_dict.\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1028         with autocast_variable.enable_auto_cast_variables(\r\n   1029             self._compute_dtype_object):\r\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\r\n   1031 \r\n   1032         if self._activity_regularizer:\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in call(self, inputs, training)\r\n    825           interpolation=self.interpolation)\r\n    826 \r\n--> 827     output = control_flow_util.smart_cond(training, random_rotated_inputs,\r\n    828                                           lambda: inputs)\r\n    829     output.set_shape(inputs.shape)\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)\r\n    107     return control_flow_ops.cond(\r\n    108         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n--> 109   return smart_module.smart_cond(\r\n    110       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n    111 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     52   if pred_value is not None:\r\n     53     if pred_value:\r\n---> 54       return true_fn()\r\n     55     else:\r\n     56       return false_fn()\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in random_rotated_inputs()\r\n    816       min_angle = self.lower * 2. * np.pi\r\n    817       max_angle = self.upper * 2. * np.pi\r\n--> 818       angles = self._rng.uniform(\r\n    819           shape=[batch_size], minval=min_angle, maxval=max_angle)\r\n    820       return transform(\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in uniform(self, shape, minval, maxval, dtype, name)\r\n    811             minval=minval, maxval=maxval, name=name)\r\n    812       else:\r\n--> 813         rnd = self._uniform(shape=shape, dtype=dtype)\r\n    814         return math_ops.add(rnd * (maxval - minval), minval, name=name)\r\n    815 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _uniform(self, shape, dtype)\r\n    714   def _uniform(self, shape, dtype):\r\n    715     if compat.forward_compatible(2020, 10, 25):\r\n--> 716       key, counter = self._prepare_key_counter(shape)\r\n    717       return gen_stateless_random_ops_v2.stateless_random_uniform_v2(\r\n    718           shape=shape,\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _prepare_key_counter(self, shape)\r\n    632   def _prepare_key_counter(self, shape):\r\n    633     delta = math_ops.reduce_prod(shape)\r\n--> 634     counter_key = self.skip(delta)\r\n    635     counter_size = _get_counter_size(self.algorithm)\r\n    636     counter = array_ops.bitcast(counter_key[:counter_size], dtypes.uint64)\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in skip(self, delta)\r\n    584     \"\"\"\r\n    585     if compat.forward_compatible(2020, 10, 25):\r\n--> 586       return self._skip(delta)\r\n    587     gen_stateful_random_ops.rng_skip(\r\n    588         self.state.handle, math_ops.cast(self.algorithm, dtypes.int64),\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _skip(self, delta)\r\n    614           return ds_context.get_strategy().extended.update(\r\n    615               self.state, update_fn)\r\n--> 616     return update_fn(self.state)\r\n    617 \r\n    618   def _preprocess_key(self, key):\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in update_fn(v)\r\n    598   def _skip(self, delta):\r\n    599     def update_fn(v):\r\n--> 600       return self._skip_single_var(v, delta)\r\n    601     # TODO(b/170515001): Always call strategy.extended.update after calling it\r\n    602     #   from both replica context and cross-replica context is supported.\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _skip_single_var(self, var, delta)\r\n    592   def _skip_single_var(self, var, delta):\r\n    593     # TODO(wangpeng): Cache the cast algorithm instead of casting everytime.\r\n--> 594     return gen_stateful_random_ops.rng_read_and_skip(\r\n    595         var.handle, alg=math_ops.cast(self.algorithm, dtypes.int32),\r\n    596         delta=math_ops.cast(delta, dtypes.uint64))\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/gen_stateful_random_ops.py in rng_read_and_skip(resource, alg, delta, name)\r\n    113       return _result\r\n    114     except _core._NotOkStatusException as e:\r\n--> 115       _ops.raise_from_not_ok_status(e, name)\r\n    116     except _core._FallbackException:\r\n    117       pass\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6895   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6896   # pylint: disable=protected-access\r\n-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6898   # pylint: enable=protected-access\r\n   6899 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: No registered 'RngReadAndSkip' OpKernel for 'GPU' devices compatible with node {{node RngReadAndSkip}}\r\n\t.  Registered:  device='XLA_CPU_JIT'\r\n  device='CPU'\r\n [Op:RngReadAndSkip]\r\n```\r\nIf I comment the two lines of RandomRotation&RandomZoom \u2193\r\n```\r\ndata_augmentation = keras.models.Sequential(\r\n  [\r\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3)),\r\n    #tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\r\n    #tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\r\n  ]\r\n)\r\n```\r\nThere is no error message. But without RandomRotation&RandomZoom it is difficult to call such a thing as a \"data augmentation.\" I am not sure whether it is a Apple issue or a Tensorflow issue."]}, {"number": 50217, "title": "Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.", "body": "OS: Windows 10\r\nPython: 3.9.1\r\nTensorflow:  2.5.0\r\nCUDA: 11.3\r\ncudnn: 8.2.0\r\n\r\nOutput of `nvcc --version`\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Mar_21_19:24:09_Pacific_Daylight_Time_2021\r\nCuda compilation tools, release 11.3, V11.3.58\r\nBuild cuda_11.3.r11.3/compiler.29745058_0\r\n```\r\n```python\r\nbuild['cuda_version']  = 64_112\r\nbuild['cudnn_version'] = 64_8\r\n```\r\n\r\n\r\nI encountered problem stated in title when testing a model which I trained on the same machine.\r\nDuring training, a log message stated that `cudnn version 8200` was loaded\r\n```\r\n2021-06-11 15:49:39.798483: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\r\n```\r\n\r\nBut then during testing, the message below appeared.\r\n```\r\n2021-06-11 15:49:10.188342: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0. \r\nCuDNN library needs to have matching major version and equal or higher minor version. \r\nIf using a binary install, upgrade your CuDNN library. \r\nIf building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n```\r\n\r\nHow should I solve the problem?\r\nThank you.", "comments": ["@dianchia \r\nCan you please follow the [tested build configurations](https://www.tensorflow.org/install/source_windows), as per which could you please tru with cuda 11.0 or 11.2 and cudnn 8.0/8.1 and let us know. [as this as been tested and works fine, please use the same]", "@Saduf2019 \r\nHi, I have three version of CUDA on my machine, 11.0, 11.1, and 11.3. I have changed the path in environment to point to v11.0 but when I get the build info it still shows cuda version being 64_112. Can I know how to specify which version of cuda to use in tensorflow or which environment variable to change?", "@dianchia \r\nPlease check at this path \"Control Panel ->System and Security->System->Advanced System settings\" , can you check with this [link](https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781) as well.", "@Saduf2019 \r\nI have changed the path variable and now when importing tensorflow, it shows `2021-06-11 19:47:30.501214: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll`. But then in the build info the cuda version is still 64_112, is that normal?\r\n\r\n```python\r\nimport tensorflow as tf\r\nbuild = tf.sysconfig.get_build_info()\r\nprint(build['cuda_version'])\r\nprint(build['cudnn_version'])\r\n```\r\nThis is how I get the build info. Not sure if it is correct?\r\n", "@dianchia \r\nCan you run some simple Tf code and see if you face any errors.", "@Saduf2019 \r\nHi, I tried running some simple code and it works fine. I have pinned point the issue to be one of the library that I have imported. The library uses PyTorch and I suspect that by importing torch it changes the environment variables of where cudnn was imported from. Once I remove the import statement, tensorflow loaded cudnn version 8100.", "@dianchia \r\nI see you have pythorch and tensorflow in same evn, can you have them separated to avoid the issue your facing, do you want to try tensorflow on virtual env and see if you still face the issue.", "@Saduf2019 \r\nI'm trying to move from pytorch to tensorflow currently. Since I know the problem is on that particular library, I will just rewrite it using tensorflow. Thanks for the help and I think this issue can be close now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50217\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50217\">No</a>\n"]}, {"number": 50216, "title": "Fix the alpha/beta datatype when using Cudnn Frontend", "body": "This fix an issue of setting alpha/beta when using the Cudnn Frontend.\r\n\r\n\r\n@nluehr @timshen91 ", "comments": ["Since @timshen91 has fixed the issue by reverting https://github.com/tensorflow/tensorflow/pull/48382# on their side, closing this PR."]}, {"number": 50215, "title": "Added reduce_variance and reduce_std_example docstring for axis=0", "body": "Adding example for `axis=0` after merge of #49609 .\r\n\r\ncc @mihaimaruseac , @edloper ", "comments": []}, {"number": 50214, "title": "Backpropagation operators get blocked by NCCL operators when TF Profiler is enabled", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster).\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.3\r\n- Nvidia Driver: 418.116.00\r\n- CUDA version: 11.0\r\n- cuDNN: 8.0.5\r\n- GPU model and memory: Tesla V100-SXM2-32GB\r\n- NCCL version: 2.8.3\r\n- Horovod version: 0.21.0\r\n- Connection: 2 physical machines, each with 2 V100 GPUs\r\n- Network: 100 Gbps RDMA\r\n\r\n**Describe the current behavior**\r\nWhen using TensorFlow's profiler `tf.profiler.experimental` API to see the timeline of a distributed training job with Horovod+NCCL, it seems that backpropagation (**BP** in short) operators are blocked by NCCL operators. The following figure is the timeline of rank 0 GPU, the row of `Stream #30` shows NCCL traces and the row of `TensorFlow Ops` shows the computation operators . Here we observe two cases of blocking:\r\n1. When the `NCCL 1` operator starts to run, one **BP** operator is still running. The execution time of this **BP** operator is always larger than that of `NCCL 1`, saying that this **BP** operator is blocked and can only finishes after `NCCL 1` is finished.\r\n2. Another case is that during the execution of `NCCL 2`, no `BP` operator is scheduled to run.\r\n![image](https://user-images.githubusercontent.com/17765864/121631175-26912280-cab1-11eb-8c09-ec36c36e95f1.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nHowever, these two kinds of blocking will not occur when TensorFlow Profiler is disabled. The following figure shows the corresponding timeline (rank 0 GPU) profiled with `nvprof`. We can see that there is no obvious gap between **BP** operators or extremely long **BP** operators.\r\n![image](https://user-images.githubusercontent.com/17765864/121632493-a6b88780-cab3-11eb-84d8-b26d3ba05ada.png)\r\n\r\nActually, we also didn't observe these two kinds of blocking in Tensorflow 1.x, **it seems that it is a problem of the profiler of  TensorFlow 2.x**. Is that expected or a bug of TF 2' profiler ?\r\n\r\n**Standalone code to reproduce the issue**\r\nThe python Script to reproduce:\r\nhttps://github.com/joapolarbear/horovod/blob/b_v0.21.0_tfissue/examples/tensorflow2/tensorflow2_synthetic_benchmark.py\r\n### Profile with TensorFlow Profiler\r\n```\r\nexport TRACE_DIR= /path/to/dir\r\nmpirun -np ${TOTAL_GPU_NUM} -H ${HOST_LIST} \\\r\n    -bind-to none -map-by slot -mca plm_rsh_args '-p 12345' \\\r\n    -mca pml ob1 -mca btl ^openib --allow-run-as-root \\\r\n    python3 tensorflow2_synthetic_benchmark.py --profile_range 10,20 --trace_dir ${TRACE_DIR}\r\n```\r\n### Profile with nvprof\r\n```\r\nexport TRACE_DIR=/path/to/dir\r\nexport NVPROF_CMD=\"nvprof -o $TRACE_DIR/simple.%q{OMPI_COMM_WORLD_RANK}.nvprof \"\r\nmpirun -np ${TOTAL_GPU_NUM} -H ${HOST_LIST} \\\r\n    -bind-to none -map-by slot -mca plm_rsh_args '-p 12345' \\\r\n    -mca pml ob1 -mca btl ^openib --allow-run-as-root \\ \r\n    ${NVPROF_CMD} python3 tensorflow2_synthetic_benchmark.py\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nThe trace files we used in above two examples:\r\n[tf_profiler](https://github.com/tensorflow/tensorflow/files/6635952/trace.json.gz)\r\n[nvprof](https://github.com/tensorflow/tensorflow/files/6635957/rank0.json.zip)\r\n\r\n", "comments": ["TF Profiler is in realm of https://github.com/tensorflow/profiler. Assigned it to its primary. ", "Hi, could you try to enable concurrent kernel profiling and see if that helps? i.e. `TF_GPU_CUPTI_FORCE_CONCURRENT_KERNEL=1` https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/profiler/internal/gpu/device_tracer_cuda.cc;drc=4d618cb1fe451d5397f821fb2021c9028cec4f5c;l=133\r\n\r\nThere is a limitation of CUPTI profiling library that serializes kernel execution by default :\r\n\r\n>**1.11.1.1. Execution overhead**\r\nFactors affecting the execution overhead under tracing are:\r\n>* Enabling serial kernel activity kind CUPTI_ACTIVITY_KIND_KERNEL can significantly change the overall performance characteristics of the application because all kernel executions are serialized on the GPU. For applications which use only a single CUDA stream and therefore cannot have concurrent kernel execution, this mode can be useful as it incurs less profiling overhead compared to the concurrent kernel mode.\r\n>* Enabling concurrent kernel activity kind CUPTI_ACTIVITY_KIND_CONCURRENT_KERNEL doesn't affect the concurrency of the kernels in the application. In this mode, CUPTI instruments the kernel code to collect the timing information. Since every kernel in the CUDA module is instrumented, the overhead is propotional to the number of kernels in the module. This is a static overhead which happens at the time of loading the CUDA module. The overhead is attributed as CUPTI_ACTIVITY_OVERHEAD_CUPTI_INSTRUMENTATION in the activity record CUpti_ActivityOverhead. Starting with the CUDA 11.2, kernels are not instrumented at the module load time, instead a single instrumentation code is used for each kernel thus avoiding the static overhead at the CUDA module load time.\r\n>* Due to the code instrumentation, concurrent kernel mode can add significant runtime overhead if used on kernels that execute a large number of blocks and that have short execution durations.\r\n>* Collection of the kernel latency timestamps i.e. queued and submitted timestamps is a high overhead activity. These are not collected by default. One can enable the collection of these timestamps using the API cuptiActivityEnableLatencyTimestamps().\r\n\r\nhttps://docs.nvidia.com/cupti/Cupti/r_main.html#r_overhead_tracing_execution\r\n\r\n", "@yisitu It works. Thanks for your reply.", "You are welcome!"]}, {"number": 50213, "title": "tensorflow-gpu 2.2 works with CUDA 10.2 but requires cuDNN 7.6.4 which doesn't have a download file in NVIDIA archive for CUDA 10.2", "body": "also documented here https://stackoverflow.com/questions/67931031/tensorflow-gpu-2-2-works-with-cuda-10-2-but-requires-cudnn-7-6-4-which-doesnt-h\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n```\r\n$ lsb_release -a\r\nLSB Version:\t:core-4.1-amd64:core-4.1-noarch\r\nDistributor ID:\tCentOS\r\nDescription:\tCentOS Linux release 7.9.2009 (Core)\r\nRelease:\t7.9.2009\r\nCodename:\tCore\r\n```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): `pip`\r\n- TensorFlow version (use command below): tensorflow-gpu 2.2\r\n- Python version: `Python 3.8.5 (default, Mar 31 2021, 02:37:07)` \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): `[GCC 7.3.1 20180303 (Red Hat 7.3.1-5)] on linux`\r\n- CUDA/cuDNN version:\r\n```\r\n$ stat /usr/local/cuda\r\n  File: \u2018/usr/local/cuda\u2019 -> \u2018/usr/local/cuda-10.2\u2019\r\n  Size: 20        \tBlocks: 0          IO Block: 4096   symbolic link\r\nDevice: fd00h/64768d\tInode: 67157410    Links: 1\r\nAccess: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)\r\nContext: unconfined_u:object_r:usr_t:s0\r\nAccess: 2021-06-10 22:12:20.673080083 -0400\r\nModify: 2020-09-21 09:39:18.559883390 -0400\r\nChange: 2020-09-21 09:39:18.559883390 -0400\r\n Birth: -\r\n```\r\n**I cannot directly figure what the cuDNN version is but the error states that cuDNN 7.4.2 is loaded.**\r\n- GPU model and memory:\r\nGeForce 1080 Ti (2x) each 12GB memory\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n`v2.2.0-rc4-8-g2b96f3662b 2.2.0`\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n-----------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThese are what I see from nvidia archive:\r\n```\r\nhttps://developer.nvidia.com/rdp/cudnn-archive\r\n\r\nDownload cuDNN v7.6.4 (September 27, 2019), for CUDA 10.1\r\nDownload cuDNN v7.6.4 (September 27, 2019), for CUDA 10.0\r\nDownload cuDNN v7.6.4 (September 27, 2019), for CUDA 9.2\r\nDownload cuDNN v7.6.4 (September 27, 2019), for CUDA 9.0\r\n```\r\nAs you see there is no cuDNN for CUDA 10.2 however, I need to use CUDA 10.2 for the rest of my framework. tensorflow-gpu 2.2 works with CUDA 10.2 but I get this error which implies I need to use cuDNN 7.6.4 instead of 7.4.2\r\n\r\n```\r\n2021-06-10 22:03:04.201770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-06-10 22:03:04.420481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-06-10 22:03:05.034154: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.4.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2021-06-10 22:03:05.038684: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.4.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n```\r\n\r\nFull log can be found here: https://pastebin.com/raw/0WQw8ktB", "comments": ["As mentioned in the document [here](https://www.tensorflow.org/install/source_windows#gpu), for Tensorflow 2.2 you can install CUDA 10.1 and cuDNN 7.6 which is the compatible versions with Python version 3.5-3.8.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50213\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50213\">No</a>\n"]}, {"number": 50212, "title": " Rebuild TensorFlow with the appropriate compiler flags getting error", "body": "2021-06-11 08:55:40.955563: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n * Serving Flask app 'app' (lazy loading)\r\n * Environment: production\r\n   WARNING: This is a development server. Do not use it in a production deployment.\r\n   Use a production WSGI server instead.\r\n * Debug mode: on\r\n * Restarting with stat\r\n2021-06-11 08:55:44.741287: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.", "comments": ["@skjha1 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50212\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50212\">No</a>\n"]}, {"number": 50211, "title": "[MLIR][DISC] Bufferize RealDynamicSliceOp and ReduceOp", "body": "support hlo-to-lhlo conversion for RealDynamicSliceOp and ReduceOp", "comments": []}]