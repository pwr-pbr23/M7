[{"number": 23916, "title": "tensorflow1.2/1.4 gpu with dbg version Eigen::GpuDevice::memory fail", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 14.04 or 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source code build with gpu and dbg\r\n- TensorFlow version (use command below): 1.2 or 1.4\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.45 or 0.54\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- CUDA/cuDNN version: 8.0/5.1 or 8.0/6.0\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI build the tensorflow1.4 with gpu and -c dbg version, but when run test code or some example, meet cuda memcpy failed, but if you build with -c opt and work normally and no issue found.\r\nI have already test with ubu14+ tensorflow1.4, or ubu16 with tensorflow1.2/1.4, both of them are failed, the faild log as below, can you give me some help.\r\n------\r\njensen @ubu16 alexnet_imagenet$ python myalexnet_forward.py \r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\n2018-11-22 13:01:42.430751: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-11-22 13:01:42.430780: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-11-22 13:01:42.430787: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-11-22 13:01:42.851315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-22 13:01:42.851638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 750 Ti\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.1105\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.96GiB\r\nFree memory: 1.92GiB\r\n2018-11-22 13:01:42.851683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2018-11-22 13:01:42.851698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2018-11-22 13:01:42.851725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)\r\ntenpython: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceCuda.h:218: void Eigen::GpuDevice::memcpy(void*, const void*, std::size_t) const: Assertion `err == cudaSuccess' failed.\r\nAborted (core dumped)\r\n\r\n------\r\n\r\n**Describe the expected behavior**\r\n\r\nwork normally with bazel -c dbg and bazel -c opt.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["While I am late to answer this, even when this was filed TF 1.2 and 1.4 were well over a year old, outside our support window."]}, {"number": 23915, "title": "tensorflow gpu with dbg version Eigen::GpuDevice::memcpy assert faile ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["bad destription"]}, {"number": 23914, "title": "Weird behavior of tf.math.reduce_max", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 9.0/7\r\n- GPU model and memory: GeForce GTX 1080 Ti, 11G\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# tf.enable_eager_execution()\r\n\r\n\r\ndef test_bug():\r\n    input = tf.convert_to_tensor(np.ones((1, 1, 1, 1), dtype=np.float32))\r\n    x = tf.layers.conv2d(input, 1, (1, 1), activation=\"tanh\",\r\n                         kernel_initializer=tf.initializers.constant(2),\r\n                         use_bias=False)\r\n    max_out = tf.math.reduce_max(x, [1, 2])\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        m = sess.run(max_out)\r\n        m1, _ = sess.run([max_out, x])\r\n        print('m =', m)\r\n        print('m1 =', m1)\r\n        assert np.allclose(m, m1)\r\n\r\ntest_bug()\r\n```\r\nI'd suppose that the assertion should pass given above code. However, `tf.math.reduce_max` looks very strange:\r\n```\r\nm = [[2.]]\r\nm1 = [[0.9640276]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n`m` and `m1` should have the same value and the assertion should pass.\r\n```\r\nm = [[0.9640276]]\r\nm1 = [[0.9640276]]\r\n```\r\n\r\n**Code to reproduce the issue**\r\nListed above.\r\n\r\n**Other info / logs**\r\nIf eager execution is enabled, then the value of `x` and `max_out` is the same.\r\n", "comments": ["@rmlarsen this looks like another bug like the earlier one we fixed where grappler is rewriting a node which is being fetched. Can you take a look?", "I think this bug has been fixed in nightly. Please reopen / comment if I'm wrong."]}, {"number": 23913, "title": "Updated the clarity of specific comments in function.py 11212018", "body": "I reworked and updated the clarity of specific comments in function.py 11212018", "comments": ["Can you fix the linter errors? Thanks!", "> Can you fix the linter errors? Thanks!\r\n\r\n@fullarray gentle ping", "@rthadur yes indeed. I was reviewing the code for a different project back then and thought I add a little contribution. I didn't touch the function and code structure. I would think that it'll be best to assign a different ticket for the linter errors. Unless someone got to them already..."]}, {"number": 23912, "title": "Building TensorFlow 1.12 with CUDA 10", "body": "I get the error below when I try to build TensorFlow on a Windows 2016 server.\r\nI followed this guide step-by-step: https://mc.ai/install-tensorflow-gpu-with-cuda-10-0-and-cudnn-7-4-for-python-on-windows-10/\r\n\r\nIt is my first time trying to something like this - so please elaborate your answers :-)\r\n\r\n# ERROR: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:68:1: undeclared inclusion(s) in rule '//tensorflow/contrib/seq2seq:python/ops/_beam_search_ops_gpu':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc':\r\n  'C:/users/administrator/appdata/local/temp/2/nvcc_inter_files_tmp_dir/beam_search_ops_gpu.cu.compute_70.cudafe1.stub.c'\r\n  'C:/users/administrator/appdata/local/temp/2/nvcc_inter_files_tmp_dir/beam_search_ops_gpu.cu.fatbin.c'\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/util/Memory.h(164): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/util/Memory.h(179): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/products/Parallelizer.h(20): warning: variable \"m_maxThreads\" was set but never used\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/ArrayWrapper.h(94): warning: __declspec attributes ignored\r\n\r\nexternal/com_google_absl\\absl/strings/string_view.h(496): warning: expression has no effect\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/map.h(1025): warning: invalid friend declaration\r\n\r\n.\\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/util/Memory.h(164): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/util/Memory.h(179): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/products/Parallelizer.h(20): warning: variable \"m_maxThreads\" was set but never used\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/ArrayWrapper.h(94): warning: __declspec attributes ignored\r\n\r\nexternal/com_google_absl\\absl/strings/string_view.h(496): warning: expression has no effect\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/map.h(1025): warning: invalid friend declaration\r\n\r\n.\\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here\r\n\r\nc:\\users\\administrator\\_bazel_administrator\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/ArrayWrapper.h(94): warning: __declspec attributes ignored\r\n\r\nexternal/com_google_absl\\absl/strings/string_view.h(496): warning: expression has no effect\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/map.h(1025): warning: invalid friend declaration\r\n\r\n.\\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration\r\n\r\nhost_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 860.118s, Critical Path: 96.34s\r\nINFO: 2217 processes: 2217 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["RESOLVED!\r\n\r\nGPU support\r\nTo make the TensorFlow package builder with GPU support:\r\n\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nBazel build options\r\n**Use this option when building to avoid issue with package creation: https://github.com/tensorflow/tensorflow/issues/22390\r\n\r\n--define=no_tensorflow_py_deps=true**\r\n\r\nBuilding TensorFlow from source can use a lot of RAM. If your system is memory-constrained, limit Bazel's RAM usage with: --local_resources 2048,.5,1.0.\r\n\r\nIf building with GPU support, add --copt=-nvcc_options=disable-warnings to suppress nvcc warning messages."]}, {"number": 23911, "title": "Fix typo in session.py method docstring", "body": "", "comments": ["Thanks for the contribution! While the new text is valid, I don't think the original text is actually a typo (\"a fed value\" parses fine to me as \"a value that has been fed\"). I'm going to close this PR, because I don't think it's worth the energy consumption that would be involved in running the test suite."]}, {"number": 23910, "title": "TFTRT: Add square op and unit tests", "body": "Add Square op and unit tests.", "comments": ["Hi @trevor-m, would you help to fix the clang-format errors?\r\nThanks.", "> Hi @trevor-m, would you help to fix the clang-format errors?\r\n> Thanks.\r\n\r\nFixed, thanks."]}, {"number": 23909, "title": "TFTRT: Add ExpandDims, Squeeze ops and unit tests.", "body": "Add conversion for ExpandDims, Squeeze ops, and unit tests for both.\r\n\r\nThese ops will allow 1D convolutions to be converted.", "comments": ["@azaks2 @smit-hinsu\r\nCould you please review.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Can you rebase this PR wrt master? It's a little hard to review as-is", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi @trevor-m, would you help to fix this error:\r\n\r\n```\r\nerror: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  TensorShapeUtils::MakeShape(shape, &tensor_shape);\r\n```\r\n\r\nThanks.", "> Hi @trevor-m, would you help to fix this error:\r\n> \r\n> ```\r\n> error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n>   TensorShapeUtils::MakeShape(shape, &tensor_shape);\r\n> ```\r\n> Thanks.\r\n\r\nDone. Thanks for reviewing!", "Thanks @trevor-m. I found that rank_two_test and unary_test are failing, probably because they're using squeeze ops, would you help to double check? If you want to add an op that is unsupported you may use `self.trt_incompatible_op`."]}, {"number": 23908, "title": "TFTRT: Add Sigmoid, Tanh ops and unit tests.", "body": "Add conversion for all three activation types supported by TensorRT's IActivationlayer: Relu, Tanh, Sigmoid. Also adds unit tests for all three ops.", "comments": ["@trevor-m thanks for the fix. Would you please fix the clang-format problem as well?", "Hey Lambda, thanks for the review.\r\nUnfortunately we can't use ASSERT_TRUE(false) because the lambda function has a non-void return value. See https://groups.google.com/a/chromium.org/forum/#!topic/chromium-dev/7clymoTb3A0\r\nLet me know if you know a work around or better alternative than EXPECT_TRUE().\r\n\r\nAlso, is there a way I can run the clang-format check locally?\r\n\r\nThanks again!", "@trevor-m Thanks for the fixes. For formatting we can just run the clang-format tool, but that will change the untouched lines as well, and I don't think we have good solution to that.  ", "> @trevor-m Thanks for the fixes. For formatting we can just run the clang-format tool, but that will change the untouched lines as well, and I don't think we have good solution to that.\r\n\r\n@trevor-m  Any update please ?", "> > @trevor-m Thanks for the fixes. For formatting we can just run the clang-format tool, but that will change the untouched lines as well, and I don't think we have good solution to that.\r\n> \r\n> @trevor-m Any update please ?\r\n\r\nUpdate on what?", "> > > @trevor-m Thanks for the fixes. For formatting we can just run the clang-format tool, but that will change the untouched lines as well, and I don't think we have good solution to that.\r\n> > \r\n> > \r\n> > @trevor-m Any update please ?\r\n> \r\n> Update on what?\r\n\r\nI see this PR has not been approved by @aaroey yet. So wanted to check if there are any changes pending from your side.", "> > > > @trevor-m Thanks for the fixes. For formatting we can just run the clang-format tool, but that will change the untouched lines as well, and I don't think we have good solution to that.\r\n> > > \r\n> > > \r\n> > > @trevor-m Any update please ?\r\n> > \r\n> > \r\n> > Update on what?\r\n> \r\n> I see this PR has not been approved by @aaroey yet. So wanted to check if there are any changes pending from your side.\r\n\r\nAh I see, I believe I have addressed all of @aaroey's requested changes.", "Hi @trevor-m, it looks like `binary_tensor_weight_broadcast_test` and `multi_connection_neighbor_engine_test` are failing, would you help to run them to make sure they are not broken?\r\n\r\nThanks.", "> Hi @trevor-m, it looks like `binary_tensor_weight_broadcast_test` and `multi_connection_neighbor_engine_test` are failing, would you help to run them to make sure they are not broken?\r\n> \r\n> Thanks.\r\n\r\nHey Lambda, it looks like those tests were using sigmoid to purposely break the graph into separate engines. I've updated them to use `sin` instead which we use in other tests for the same purpose.", "Thanks @trevor-m for the fix, I'll retry.", "Hi @trevor-m, sorry but we got one more errors:\r\n\r\n```\r\ntensorflow/contrib/tensorrt/test/binary_tensor_weight_broadcast_test.py:29: [W0611(unused-import), ] Unused math_ops imported from tensorflow.python.ops\r\n\r\ntensorflow/contrib/tensorrt/test/multi_connection_neighbor_engine_test.py:28: [W0611(unused-import), ] Unused gen_math_ops imported from tensorflow.python.ops\r\n\r\ntensorflow/contrib/tensorrt/test/multi_connection_neighbor_engine_test.py:29: [W0611(unused-import), ] Unused math_ops imported from tensorflow.python.ops\r\n```\r\n\r\nWould you help to fix them as well?\r\nThanks."]}, {"number": 23907, "title": "Revert TFLite examples to use tensorflow/contrib/lite", "body": "The CocoaPod didn't catch up with the changes which moves\r\nTFLite out of contrib/ directory. Reverting for now.", "comments": []}, {"number": 23906, "title": "Copy value of trainable variable to another trainable variable.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.4.1\r\n- Are you willing to contribute it (Yes/No): No.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nNow Tensorflow doesn't allow to copy value of trainable variable to another one. Operations like tf.assight, tf.identity, tf.Variable(source_variable.initialized_value()) or tf.contrib.copy_graph.copy_variable_to_graph(var, self.graph) DO NOT copy just value. After such operations the destination variable referes to the source variable and I can't pass this variable to optimizer for computing gradients: \"ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, ... \"\r\n**Will this change the current api? How?**\r\nThis change won't change the existing functions, just will add new opportunity for trainable variables.\r\n**Who will benefit with this feature?**\r\nIt will make more flexible structure that can be good for GANs, or when we have two networks and it is neccessary to copy values of one network parameters to second network parameters, and still train two networks independently.\r\n**Any Other info.**\r\n", "comments": ["Please provide a **small** reproducible snippet of the problem.", "Hi @ebrevdo !\r\n\r\nI started working on several GANs architecture that based on [cvpr paper 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chavdarova_SGAN_An_Alternative_CVPR_2018_paper.pdf). \r\n1. I have _Discriminator_A_ and _Generator_A_. I train them.\r\n2. Then I have to copy value of trainable variables of _Discriminator_A_ to trainable variable of _Discriminator_B_. Then I train _Discriminator_B_ and _Generator_B_.\r\nIn Tensorflow If I use **tf.assign**, **tf.Varaible** or even use two different **graphs** (tf.contrib.copy_graph.copy_variable_to_graph(var, self.graph)) I see that when I train _Discriminator_B_ and _Generator_B_ the _Discriminator_A_ is also training and vice versa. So what I\u2019ve got: actually _Discriminator_B_ refers to _Discriminator_A_ parameters, but I just need after every epoch take the values from _Discriminator_A_ and initialize _Discriminator_B_.\r\n\r\nExample of code:\r\n```\r\n# original network from which I take values of trainable variables\r\nD_vars = [var for var in self.train_vars if 'Discriminator_A' in var.name]\r\n\r\n# The destination trainable variables\r\nmsg_D_vars = [var for var in self.train_vars if 'Discriminator_B' in var.name]\r\n\r\nfor var_idx, var in enumerate(D_vars):\r\n     1)  tf.assign(msg_D_vars[var_idx], var)  -- DOESNT' WORK\r\n     2) var = tf.contrib.copy_graph.copy_variable_to_graph(var, self.graph) -- DOESN'T WORK\r\n    3) msg_D_vars[var_idx] = tf.Variable(var.initialized_value(), trainable=True) -- DOESN'T WORK\r\n```\r\n\r\nAlso I tried to remove trainable variables before training after 1) or 2) or 3) step and then add them again for training. But this tricky approach doesn't work too as I need.\r\n```\r\nwith self.graph.as_default():\r\n        trainable_collection = tf.get_collection_ref(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n        variables_to_remove = list()\r\n        for vari in trainable_collection:\r\n             # uses the attribute 'name' of the variable\r\n               if 'discriminator_B' % in vari.name:\r\n                     variables_to_remove.append(vari)\r\n       for rem in variables_to_remove:\r\n           trainable_collection.remove(rem)\r\n```\r\n\r\nThanks!", "Hello @Firyuza ! You might need to initialize a temp for exchange.\r\n```\r\ndef hello(scopeName='sc1'):\r\n    with tf.variable_scope(scopeName):\r\n        a1 = tf.get_variable(name='test_var1', initializer=0.)\r\n        b1 = tf.get_variable(name='test_var2', initializer=0.)\r\n\r\n\r\n\r\nsess = tf.Session()\r\ng=hello('sc1')\r\ng2=hello('new')\r\ng3=hello('old')\r\nsess.run(tf.global_variables_initializer())\r\n# params=tf.trainable_variables(scope='sc1')\r\nt_vars=tf.trainable_variables()\r\nvar_in_sc1= [var for var in t_vars if 'sc1' in var.name]\r\nvar_in_new= [var for var in t_vars if 'new' in var.name]\r\nvar_in_old= [var for var in t_vars if 'old' in var.name]\r\nprint('save 0')\r\nfor var_idx, var in enumerate(var_in_sc1):\r\n    sess.run(tf.assign(var_in_old[var_idx], var))\r\n\r\nprint('add 1')\r\nfor var in var_in_sc1:\r\n    sess.run(tf.assign_add(var,1.))\r\n\r\nprint('save 1')\r\nfor var_idx, var in enumerate(var_in_sc1):\r\n    sess.run(tf.assign(var_in_new[var_idx], var))\r\n\r\nprint('add 1')\r\nfor var in var_in_sc1:\r\n    sess.run(tf.assign_add(var,1.))\r\nprint(sess.run(var_in_sc1))\r\n\r\nprint('load old')\r\nfor var_idx, var in enumerate(var_in_old):\r\n    sess.run(tf.assign(var_in_sc1[var_idx], var))\r\nprint(sess.run(var_in_sc1))\r\n\r\n\r\nprint('load new')\r\nfor var_idx, var in enumerate(var_in_new):\r\n    sess.run(tf.assign(var_in_sc1[var_idx], var))\r\nprint(sess.run(var_in_sc1))\r\n```\r\nHope that this demo can help you!", "Hello @zlichen!\r\n\r\nThank you for feedback!\r\n\r\nYes, this demo works. But I have to notice that it's not so good and intuitive way of making such kind of copy. I hope [Tensorflow](https://github.com/tensorflow) community will consider more flexible and obvious way of doing it. Otherwise I need to write this code on PyTorch, because it has such ability.\r\n\r\nAlso, we have to keep in memory temp variables and it's not effective. E.g. if I'm working with ResNet it will be really expensive memory storing trainable variables values from 50 or 100 layers.\r\n\r\nThanks!", "Hello @Firyuza !\r\n\r\nAfter testing the demo, I also found that is really time-consuming while copying the weight from a huge Neural Network. The demo below is a better and more effective way which is no need for a tmp.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n    def hello(scopeName='sc1'):\r\n        with tf.variable_scope(scopeName):\r\n            a1 = tf.get_variable(name='test_var1', initializer=0.)\r\n            b1 = tf.get_variable(name='test_var2', initializer=0.)\r\n\r\n    g=hello('sc1')\r\n    g2=hello('new')\r\n    g3=hello('old')\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    t_vars=tf.trainable_variables()\r\n    var_in_sc1= [var for var in t_vars if 'sc1' in var.name]\r\n    var_in_new= [var for var in t_vars if 'new' in var.name]\r\n    var_in_old= [var for var in t_vars if 'old' in var.name]\r\n    \r\n    print('save 0')\r\n    for var_idx, var in enumerate(var_in_sc1):\r\n        var_in_old[var_idx].load(var_in_sc1[var_idx].eval(),sess)\r\n    print('add 1')\r\n    for var in var_in_sc1:\r\n        sess.run(tf.assign_add(var,1.))\r\n\r\n    print('save 1')\r\n    for var_idx, var in enumerate(var_in_sc1):\r\n        var_in_new[var_idx].load(var_in_sc1[var_idx].eval(),sess)\r\n    print('add 1')\r\n    for var in var_in_sc1:\r\n        sess.run(tf.assign_add(var,1.))\r\n    print(sess.run(var_in_sc1))\r\n\r\n    print('load old')\r\n    for var_idx, var in enumerate(var_in_old):\r\n        var_in_sc1[var_idx].load(var_in_old[var_idx].eval(),sess)\r\n    print(sess.run(var_in_sc1))\r\n\r\n    print('load new')\r\n    for var_idx, var in enumerate(var_in_new):\r\n        var_in_sc1[var_idx].load(var_in_new[var_idx].eval(),sess)\r\n    print(sess.run(var_in_sc1))\r\n```\r\nMaybe still need a few lines of code. As far as I know, Keras (base on tensorflow) can also easily get or set the weights for the models or layers with only one line of code.", "Hi @zlichen !\r\n\r\nThanks again for feedback! Your example is working, but now I noticed that If I create two networks in the same Graph and start to train first network I see that the second one is training too. They do not have sharable variables. I think maybe one of the my first approaches is working too, just the problem is on another plane...\r\n\r\nThanks!", "> Hi @zlichen !\r\n> \r\n> Thanks again for feedback! Your example is working, but now I noticed that If I create two networks in the same Graph and start to train first network I see that the second one is training too. They do not have sharable variables. I think maybe one of the my first approaches is working too, just the problem is on another plane...\r\n> \r\n> Thanks!\r\n\r\nHi! I guess you have to set different scope names for your two networks in order to make distinction.", "Hi @zlichen !\r\n\r\nYes, I gave different scope names for two networks, but it doesn't work. I think it's a Tensorflow paradigm, we cannot create two networks in the **same** graph when you want to train it **independently**. And  all copy operations that I discussed above do not make sense. \r\n\r\nSo if we wanna make copy value of variable -- your solution is right. But if we wanna make a copy of variable's value of one network to varaible of another one, there is another solution:\r\n\r\n1) for training two different  networks **independently**, define two **different** **graphs**;\r\n2) copying values of varables of one network to variables of another one:\r\n`for var_idx, var in enumerate(D_vars):\r\n        msg_D_vars[var_idx].load(D_vars[var_idx].eval(session=self.sess), self.sess_msg)`\r\n\r\nThanks!", "@Firyuza Is this still an issue ? We see that you are using old version of tensorflow  1.x which is not actively supported, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please refer [this](https://www.tensorflow.org/guide/migrate) guide to migrate from TF v1.x to 2.x . Please open a new issue in case you face any errors using TF v2, we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23905, "title": "Error after correct (or so i suppose) installation, but nothing seems to work when i try to run \"import tensorflow as tf\"", "body": "**System information**\r\n- Windows 7 64bits (service pack 1) on AMD FX-8320E\r\n- TensorFlow 1.12.0 (https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.12.0-cp36-cp36m-win_amd64.whl)\r\n- Python 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 14:57:15) [MSC v.1915 64 bit (AMD64)] on win32\r\n- Installed using pip\r\n- GTX 750 ti 2GB (not relevant cause i'm using the CPU only version)\r\n\r\n\r\nAfter a fresh install of tensorflow after getting other problems for a few times, i get this log when running a simple.py with just the line \"import tensorflow as tf\", the console log is the following:\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Impossibile trovare il modulo specificato.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\marco\\Desktop\\This is a test!.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Impossibile trovare il modulo specificato.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`", "comments": ["@EMJzero Hi, this could be due to Python 3.7.\r\nWe are maintaining a single issue i.e #20517 to track tensorflow compatibility and import errors with Python 3.7. Request you to refer it and post your comments there. Thank you !", "@harshini-gadige Is there an ETA?  What are the issues with compatibility?", "> @harshini-gadige Is there an ETA? What are the issues with compatibility?\r\n\r\nI mean, tensorflow with Python 3.7. Did you get a chance to go through #20517 ?  ", "@harshini-gadige Ran through it.  It's probably above my knowledge and/or I'm not seeing what the problems are.", "I got the point that it is probably due to the 3.7 Python version, but can you anyway confirm me that i used the right tensorflow version (atleast the latest) and that the installazione path is the one it should be? Because if only the version is the overall problema that should work with Python 3.6 right?", "One more thing....the error which came out in the #20517 isn\u2019t the same of mine...i think because of Windows behaving differently than MAC OS, but that still seems to me a different thing! Maybe caused by something which isn\u2019t just due to Python 3.7!", "@gunan ", "It can be confusing on windows, as python can be installed on so many different directories.\r\nCould you share the exact commands you used to install and run this script?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23904, "title": "New instances of iterator.make_initializer do not release previously allocated memory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 9.0.176 / 7.3.0.29\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\nConsider the following setup (see the code below)\r\n1. Create a dataset using `tf.data.Dataset.from_tensor_slices`\r\n2. Use it to initialize a `tf.data.Iterator.from_structure` iterator\r\n3. Repeat the previous step several times in a single session\r\n\r\nObserved behavior: memory consumption grows linearly with number of iterations\r\n\r\n**Describe the expected behavior**\r\nMemory consumption should be bounded for any number of iterations.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport psutil\r\n\r\nn_samples = 1000*1000\r\ndim = 100\r\nbatch_size = 50\r\n\r\nraw_data = np.zeros((n_samples, dim)).astype(np.float32)\r\ndataset = tf.data.Dataset.from_tensor_slices(raw_data).batch(batch_size)\r\niterator = tf.data.Iterator.from_structure(tf.float32, [None, dim])\r\n\r\nprocess = psutil.Process(os.getpid())\r\ndef mem():\r\n    return process.memory_info().rss / 1024 ** 3.\r\n\r\nsess = tf.Session()\r\nfor i in range(20):\r\n    sess.run(iterator.make_initializer(dataset))\r\n    print('Epoch {}, mem.: {:.2f}Gb'.format( i, mem() ))\r\n```\r\n\r\nOutput:\r\n```\r\nEpoch 0, mem.: 1.34Gb\r\nEpoch 1, mem.: 1.71Gb\r\nEpoch 2, mem.: 2.09Gb\r\nEpoch 3, mem.: 2.46Gb\r\nEpoch 4, mem.: 2.83Gb\r\nEpoch 5, mem.: 3.20Gb\r\nEpoch 6, mem.: 3.58Gb\r\nEpoch 7, mem.: 3.95Gb\r\nEpoch 8, mem.: 4.32Gb\r\nEpoch 9, mem.: 4.69Gb\r\nEpoch 10, mem.: 5.07Gb\r\nEpoch 11, mem.: 5.44Gb\r\nEpoch 12, mem.: 5.81Gb\r\nEpoch 13, mem.: 6.18Gb\r\nEpoch 14, mem.: 6.56Gb\r\nEpoch 15, mem.: 6.93Gb\r\nEpoch 16, mem.: 7.30Gb\r\nEpoch 17, mem.: 7.67Gb\r\nEpoch 18, mem.: 8.05Gb\r\nEpoch 19, mem.: 8.42Gb\r\n```", "comments": ["@artsobolev I think the root cause here is lower-level than the `Iterator.make_initializer()` implementation. For example, the following simpler program exhibits similar growth:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport psutil\r\n\r\nn_samples = 1000*1000\r\ndim = 100\r\nbatch_size = 50\r\n\r\nraw_data = tf.constant(np.zeros((n_samples, dim)).astype(np.float32))\r\n\r\nprocess = psutil.Process(os.getpid())\r\ndef mem():\r\n    return process.memory_info().rss / 1024 ** 3.\r\n\r\nsess = tf.Session()\r\nfor i in range(20):\r\n    # Creates a new \"Add\" op in a loop and run a \"new\" subgraph each iteration.\r\n    sess.run(tf.add(raw_data, raw_data))\r\n    print('Epoch {}, mem.: {:.2f}Gb'.format( i, mem() ))\r\n```\r\n\r\n...and gives me this output:\r\n\r\n```\r\nEpoch 0, mem.: 0.95Gb\r\nEpoch 1, mem.: 1.32Gb\r\nEpoch 2, mem.: 1.69Gb\r\nEpoch 3, mem.: 2.06Gb\r\nEpoch 4, mem.: 2.44Gb\r\nEpoch 5, mem.: 2.81Gb\r\nEpoch 6, mem.: 3.18Gb\r\nEpoch 7, mem.: 3.56Gb\r\nEpoch 8, mem.: 3.93Gb\r\nEpoch 9, mem.: 4.30Gb\r\nEpoch 10, mem.: 4.67Gb\r\nEpoch 11, mem.: 5.05Gb\r\nEpoch 12, mem.: 5.42Gb\r\nEpoch 13, mem.: 5.79Gb\r\nEpoch 14, mem.: 6.16Gb\r\nEpoch 15, mem.: 6.54Gb\r\nEpoch 16, mem.: 6.91Gb\r\nEpoch 17, mem.: 7.28Gb\r\nEpoch 18, mem.: 7.65Gb\r\nEpoch 19, mem.: 8.03Gb\r\n```\r\n\r\nThere are two issues that conspire to create the problem:\r\n\r\n1. Each time we call `iterator.make_initializer(dataset)` (or `tf.add()`) in the loop, some new ops are created. These new ops aren't found in the session's cache of previously executed subgraphs, so it \"reoptimizes\" the computation and stores a new \"optimized\" subgraph inside the session. This new subgraph accounts for the memory growth.\r\n2. Every time we use the same large constant in a different subgraph, a copy of that constant's data is made and stored in the subgraph. That accounts for why the memory growth is so severe in this case, because the `raw_data` tensor is approximately 400MB in size.\r\n\r\nIssue (1) is pretty fundamental, and with the shift to eager execution and away from sessions in TF 2.0, it's unlikely to be fixed for existing code (new eager code won't have this problem, since there's no \"graph\" that will accumulate this state). Issue (2) is potentially fixable, but will require some internal changes to the `Session` code to intern the values of (large) constants. I'm assigning this to @asimshankar to make the call on whether the latter is worth doing.", "Thanks for the report @artsobolev \r\n\r\nAs @mrry pointed out, here we are adding operations to the graph in each iteration of the loop, which is causing memory growth. Interning the constants as @mrry suggested would help (significantly in this case due to the large constants), but we'll still be constantly adding nodes to the graph so there will still be an increase in memory footprint.\r\n\r\nFor the example in https://github.com/tensorflow/tensorflow/issues/23904#issue-383237718, I think what you want is to create the initializer operation outside the loop, so:\r\n\r\n```python\r\ninit = iterator.make_initializer(dataset)\r\nsess = tf.Session()\r\nfor i in range(20):\r\n    sess.run(init)\r\n    print('Epoch {}, mem.: {:.2f}Gb'.format( i, mem() ))\r\n```\r\n\r\nThat way there is no growth in the graph.\r\n\r\nI'm tempted to close this out since adding nodes to a graph is bound to increase memory.\r\nWith TF 2.0, it won't be easy to run into this situation.\r\nInterning constants will help and is something worth looking into in general, but not specific to this issue.\r\n\r\nPlease feel free to reopen if I have misunderstood."]}, {"number": 23903, "title": "Poor performance when scaling up data pipeline in multi CPU environments", "body": "**System information**\r\nAWS Deep Learning AMI (ubuntu 16.04, tensorflow 12) on AWS p3.8 and p3.16 instances.\r\n\r\n**Describe the current behavior**\r\nI am trying to optimize the speed of my code when training on Imagenet and started by looking at the data pipeline. I am using TFRecordDataset(), and use both prefetch() and num_parallel_calls for mappings. Here I just load in the batches without performing any action on them.\r\n\r\nI seem to get reasonably good results on AWS p3.8 (32 vCPU, 4 Nvidia Tesla V100 GPUs), although it's still a bit below the ~3000 images/second that I've seen mentioned elsewhere. However, if I run exactly the same code on AWS p3.16 (64 vCPU, 8 GPUs), throughput *decreases* significantly, even while CPU use is higher. \r\n\r\n*The two systems perform as expected on other tasks.*  I did some very elementary benchmarking of both systems:\r\n- calculating the number of primes, spawning a number of threads using multiprocessing.Process(). As expected, performance between p3.8 and p3.16 appears to be identical when n_threads < 32, and 3.16 performs better when n_threads = 128.  \r\n- reading a file with 1,000,000 lines (and repeat 100 times): identical performance\r\n\r\nThis suggests to me the problem is with the TFRecordDataset, but if there is any other benchmarking that makes sense, please let me know.\r\n\r\nBoth systems allow two threads for each CPU. I launched another p3.16 instance with a single thread per CPU, so that the number of virtual CPUs is identical to the p3.8 instance. This did not appear to make any difference.\r\n\r\n**Describe the expected behavior**\r\nPerformance should improve, or at least remain the same, on p3.16 as compared to p3.8, as there are more CPUs available.\r\n\r\n**Code to reproduce the issue**\r\nTaking only the data pipeline part and skipping all shuffling, preprocessing, etcetera I am left with this as a minimal working example.\r\n\r\n(Note that I use multiple prefetch statements; this significantly improves performance on both systems for me, but removing them does not change their relative performance. Similarly, using 128 parallel calls in the mapping function gives better results than 32 or 64, but changing that or making it dependent on the actual number of vCPUs doesn't solve the observed behaviour.)\r\n\r\n    import tensorflow as tf\r\n    import os\r\n    import time\r\n\r\n    DATA_FOLDER = '/home/ubuntu/datasets/imagenet-data'\r\n    BATCH_SIZE = 256\r\n    N_TRIALS = 10\r\n    STEPS_PER_TRIAL = 100\r\n\r\n    def decode_imagenet(serialized_example):\r\n        feature_map = {\r\n            'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\r\n            'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1)\r\n        }\r\n        features = tf.parse_single_example(serialized_example, feature_map)\r\n\r\n        image = features['image/encoded']\r\n        image = tf.image.decode_jpeg(image, channels=3)\r\n        image = tf.image.convert_image_dtype(image, tf.float32)\r\n        image = tf.image.resize_images(image, [224, 224])\r\n\r\n        label = tf.squeeze(tf.cast(features['image/class/label'], tf.int32))\r\n        \r\n        return image, label\r\n\r\n    training_files = [os.path.join(DATA_FOLDER, fn) for fn in os.listdir(DATA_FOLDER) if 'train' in fn]\r\n\r\n    dataset = tf.data.TFRecordDataset(training_files)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.prefetch(BATCH_SIZE*16)\r\n    dataset = dataset.map(decode_imagenet, num_parallel_calls=128)\r\n    dataset = dataset.prefetch(BATCH_SIZE*4)\r\n    dataset = dataset.batch(BATCH_SIZE)\r\n    dataset = dataset.prefetch(1)\r\n\r\n\r\n    iterator = dataset.make_initializable_iterator()\r\n\r\n    batch_x, batch_y = iterator.get_next()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(iterator.initializer)\r\n\r\n        res = []\r\n        for trial in range(N_TRIALS):\r\n            t0 = time.time()\r\n            for _ in range(STEPS_PER_TRIAL):\r\n                sess.run([batch_x, batch_y])\r\n            t1 = time.time()\r\n            images_per_second = BATCH_SIZE*STEPS_PER_TRIAL/(t1-t0)\r\n            res.append(images_per_second)\r\n            print('Trial %i: %f ips' % (trial, images_per_second))\r\n        print('Overall average: %f ips' % (sum(res)/N_TRIALS))\r\n\r\n\r\n\r\n**Other info / logs**\r\n\r\nOutput of above code on p3.8:\r\n\r\n    Trial 0: 2687.172262 ips\r\n    Trial 1: 2861.731855 ips\r\n    Trial 2: 2832.192681 ips\r\n    Trial 3: 2826.211587 ips\r\n    Trial 4: 2851.585431 ips\r\n    Trial 5: 2809.069081 ips\r\n    Trial 6: 2869.325024 ips\r\n    Trial 7: 2858.402548 ips\r\n    Trial 8: 2833.678433 ips\r\n    Trial 9: 2859.654147 ips\r\n    Overall average: 2828.902305 ips\r\n\r\nMeanwhile, %CPU (observed using the top command) is 2300-2400.\r\n\r\nOn p3.16:\r\n\r\n    Trial 0: 1585.289254 ips\r\n    Trial 1: 2077.237178 ips\r\n    Trial 2: 2165.624391 ips\r\n    Trial 3: 2183.211334 ips\r\n    Trial 4: 2226.638648 ips\r\n    Trial 5: 2217.900892 ips\r\n    Trial 6: 2217.662143 ips\r\n    Trial 7: 2232.781854 ips\r\n    Trial 8: 2178.928816 ips\r\n    Trial 9: 2222.856626 ips\r\n    Overall average: 2130.813114 ips\r\n\r\n%CPU is between 3700-4000.\r\n\r\n", "comments": ["@koenhelwegen\r\n\r\nBased on the information available on [AWS website](https://aws.amazon.com/ec2/instance-types/p3/), the machines use Intel Skylake architecture which generally has [multiple sockets](https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview). What likely happens when you go from 32vCPUs to 64vCPUs, you end up using more sockets, copying large amounts of data between them. This would explain the reduced throughput and increased CPU usage (in fact, I have seen similar issue on Google Cloud).\r\n\r\nHere are my suggestions:\r\n\r\n0) Compare NUMA setting between p3.8 and p3.16 using `numactl --hardware`\r\n\r\n1) Simplify your pipeline to:\r\n\r\n```\r\n  dataset = tf.data.TFRecordDataset(training_files)\r\n  dataset = dataset.map(decode_imagenet, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n  dataset = dataset.batch(BATCH_SIZE)\r\n  dataset = dataset.prefetch(8)\r\n```\r\n\r\nI expect this to boost performance for both p3.8 and p3.16 and reduce the cross-NUMA traffic for p3.16. This is because the tf.data runtime will detect the `map` + `batch` sequence and apply a more efficient fused implementation of the two (discussed [here](https://www.tensorflow.org/guide/performance/datasets)).\r\n\r\n2) Consider pinning your program to a NUMA socket, discussed [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-os.html).", "Thanks for taking this up!\r\n\r\nWhen I run `numactl --hardware` I get:\r\n\r\n```\r\n# p3.8\r\navailable: 1 nodes (0)\r\nnode 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\r\nnode 0 size: 245854 MB\r\nnode 0 free: 215328 MB\r\nnode distances:\r\nnode   0 \r\n  0:  10\r\n\r\n# p3.16\r\navailable: 2 nodes (0-1)\r\nnode 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\r\nnode 0 size: 245864 MB\r\nnode 0 free: 243849 MB\r\nnode 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63\r\nnode 1 size: 245941 MB\r\nnode 1 free: 245151 MB\r\nnode distances:\r\nnode   0   1 \r\n  0:  10  20 \r\n  1:  20  10  \r\n```\r\n\r\nSo we indeed move from 1 to 2 nodes, which explains the observed behaviour. On p3.8, if I switch to the nightly build (was using tf-1.12) and use the pipeline you suggest performance improves to about 3200 ips; if I fix the num_parallel_calls to anything between 64 and 512 the performance increases further to ~3500 ips. On p3.16 performance is ~4200 ips; here too the AUTOTUNE option slightly underperforms as compared to fixing some value manually.\r\n\r\nThe main difference is in the nightly build vs tf 1.12: running the same code with tensorflow==1.12 (not tensorflow-gpu) gon p3.16 gives ~2400 ips.\r\n\r\nBehaviour is now as expected, thanks a lot! Now I just need to get the nightly build working with gpu support on AWS ;)\r\n\r\n\r\n"]}, {"number": 23902, "title": "tensor.op.outputs is self-referential", "body": "To be honest, I don't know if this is a \"verified\" bug. But I've tried many things, narrowed it down to a really simple reproducible case, read the documentation, [asked on SO](https://stackoverflow.com/questions/53409455/how-do-i-find-the-output-of-a-tensor-and-or-op-in-tensorflow-or-tensorflow-op), and at this point think it's either a bug, or something so unapparent that it might as well be fixed as though it _were_ a bug...So I'm thinking this belongs here as well, even if I do get an answer on SO.  \r\nThe issue is as follows (practically replicated from the SO version of my question):\r\nI have a tensor, and I'd like to find \"where it leads\" for various reasons. The way to theoretically do this would be to just look at my_tensor.op.outputs, as per documentation and such, but this always seems to point back to my_tensor itself!\r\nI've easily gone the other way before, meaning I can get the input tensor by using my_tensor.op.inputs, but for some reason \"outputs\" isn't doing the expected.\r\nHere's a simple example:\r\n\r\n    import tensorflow as tf\r\n    \r\n    a = tf.placeholder(tf.uint8, name='a')\r\n    b = tf.placeholder(tf.uint8, name='b')\r\n    my_sum = tf.identity(a + b, name='my_sum')\r\n    graph = tf.get_default_graph()\r\n    \r\n    # I should have 4 ops at this point, as validated by:\r\n    print(graph.get_operations())\r\n    >> [<tf.Operation 'a' type=Placeholder>, <tf.Operation 'b' type=Placeholder>, <tf.Operation 'add' type=Add>, <tf.Operation 'my_sum' type=Identity>]\r\n    \r\n    # So let's try get the output of 'a':\r\n    print(list(a.op.outputs))\r\n    >> [<tf.Tensor 'a:0' shape=<unknown> dtype=uint8>]\r\nIf you tried the above, you'll see you got back to 'a'...\r\nAgain, running my_sum.op.inputs gives the 'add' op, and running even further back get's us to 'a' and 'b' as expected:\r\n\r\n    input_to_my_sum = list(my_sum.op.inputs)[0]\r\n    print(input_to_my_sum)\r\n    >> Tensor(\"add:0\", dtype=uint8)\r\n    \r\n    print(list(input_to_my_sum.op.inputs))\r\n    >> [<tf.Tensor 'a:0' shape=<unknown> dtype=uint8>, <tf.Tensor 'b:0' shape=<unknown> dtype=uint8>]\r\nBut the other way round? No such luck:\r\n\r\n    print(list(input_to_my_sum.op.outputs))\r\n    >> [<tf.Tensor 'add:0' shape=<unknown> dtype=uint8>]\r\n    \r\n    print('This is no fun at all')\r\n    >> This is no fun at all\r\nSo what am I doing wrong?\r\nI've also tried using the (deprecated) op.values() with no success, and I'm confused because the documentation explicitly states that this should give me the outputs of the op (from [https://www.tensorflow.org/api_docs/python/tf/Operation]()):\r\n\r\n>outputs\r\n>The list of Tensor objects representing the outputs of this op.\r\n\r\n(I checked that a.op.__class__ is the right class and that I'm reading the correct documentation).\r\n\r\nAnd just to wrap things up, the node_def of the ops also shows no signs of an output field....\r\nThanks in advance for any advice!\r\n \r\nBTW, this is my first post and I might be doing something wrong. Let me know if that's the case and I'll try and fix it.\r\n\r\nP.S: Using tf 1.8.0 for what it's worth.\r\n", "comments": ["Got an answer on SO. Turns out I was being dumb after all. Just goes to show, um, I dunno, something or other. Sorry folks and thanks all the same!"]}, {"number": 23901, "title": "reduce_prod EXTREMELY slow", "body": "I have a custom operation running tf.reduce_sum over an axis of a Rank 5 tensor, doing an NN training.\r\nAs soon as i switch to tf.reduce_prod I get a performance drop of about 20x slower training time.\r\n\r\nThis was somewhat unexpected since reduce_max or other reductions run at the same speed of reduce_sum.\r\n\r\nIs this expected behaviour?\r\n\r\nTested on windows & linux, tensorflow versions tested from 1.5 to 1.11", "comments": ["A workaround: maybe you can try `tf.exp(tf.reduce_sum(tf.log(x)))`.", "@sjeps \r\n\r\nIs this still an issue. Please, close this thread if your issue was resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23899, "title": "tflite_simple_example  VerifyVector too few arguments to function call", "body": "\r\n\r\n**System information**\r\n- iOS 12.1 :\r\n- iPhone7Plus\r\n- TensorFlowLite 1.10.1(pod install):\r\n- 1.10.1:\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nPod install in the lite/examples/ios/simple . Pod automatically installed tensorflow lite 1.10.1.  error occurred, say \"tensorflow/lite/kernels/register.h\" file not found. After I add paths to those header files, I encounter issue \"verifier.VerifyVector(min()) too few arguments to function call, expeced 3, have 1\".\r\n\r\nAny guys have any clue ? Thanks in advance", "comments": ["What\u2019s the function definition? You could also try namespace to ensure you are calling the right function.", "> What\u2019s the function definition? You could also try namespace to ensure you are calling the right function.\r\n\r\nHi jefhai, It's the \"simple\" example in folder lite/examples/ios/. I didn't add any my code except the path for  non-founded header files. Then I run the example and error occurred.  I found the **declaration of Verifier:VerifyVector has 3 parameters in flatbuffers.h of tensorflow_lite.frame(v1.10.1) as follows:**\r\n// Common code between vectors and strings.\r\n  bool VerifyVector(const uint8_t *vec, size_t elem_size,\r\n                    const uint8_t **end) const {\r\n    // Check we can read the size field.\r\n    if (!Verify<uoffset_t>(vec)) return false;\r\n    // Check the whole array. If this is a string, the byte past the array\r\n    // must be 0.\r\n    auto size = ReadScalar<uoffset_t>(vec);\r\n    auto max_elems = FLATBUFFERS_MAX_BUFFER_SIZE / elem_size;\r\n    if (!Check(size < max_elems))\r\n      return false;  // Protect against byte_size overflowing.\r\n    auto byte_size = sizeof(size) + elem_size * size;\r\n    *end = vec + byte_size;\r\n    return Verify(vec, byte_size);\r\n  }\r\n\r\n**But the caller in schema_generated.h pass only one param:**\r\n const flatbuffers::Vector<int64_t> ***zero_point()** const {\r\n    return GetPointer<const flatbuffers::Vector<int64_t> *>(VT_ZERO_POINT);\r\n  }\r\n  bool Verify(flatbuffers::Verifier &verifier) const {\r\n    return VerifyTableStart(verifier) &&\r\n           VerifyOffset(verifier, VT_MIN) &&\r\n           verifier.VerifyVector(min()) &&\r\n           VerifyOffset(verifier, VT_MAX) &&\r\n           verifier.VerifyVector(max()) &&\r\n           VerifyOffset(verifier, VT_SCALE) &&\r\n           verifier.VerifyVector(scale()) &&\r\n           VerifyOffset(verifier, VT_ZERO_POINT) &&\r\n           verifier.VerifyVector(**zero_point()**) &&\r\n           verifier.EndTable();\r\n  }", "@miaout17 , do you have any ideas?", "The newest example is using TensorFlowLite 1.12.0 pod and I don't see this happening.\r\nPlease reopen if you can still reproduce it. Thanks!"]}, {"number": 23897, "title": " 'LSTMBlockCell' error when loading a model using saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)", "body": "in the issue below they had the same problem,but there's no much information. btw this is the only issue I found about this.\r\nI hope I can receive more  help by opening this issue. \r\n\r\n_Originally posted by @AbdelsalamHaa in https://github.com/tensorflow/tensorflow/issues/20732#issuecomment-440546353_", "comments": ["Apologies for the delay in response. Is this still an issue for you? Can you please provide the information asked by the [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md)? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23896, "title": "Looking for Tensorflow lite C++ Android NDK install instructions", "body": "Can someone provide me documentation or examples of using Tensorflow lite + Android NDK C++.\r\n\r\nThe only project example is in Java, but I need to use Tensorflow lite with the Android NDK in C++.\r\n\r\nI'm also using a Windows 10 computer and have the latest version of Bazel installed.\r\n\r\nAny steps, documentation, and example projects for Tensorflow lite + Android NDK C++ would be very beneficial to me and the community..\r\n\r\nThank you for any support", "comments": ["I\u2019d also be interested in setup instructions.\r\n\r\nI got android studio to install tflite on the java side pretty easily. How can I get it to work in c++?", "See this: https://stackoverflow.com/questions/49834875/problems-with-using-tensorflow-lite-c-api-in-android-studio-project\r\n\r\nYou must use NDK 18. Don't use dropout in your model.", "@Nimitz14 Should I go to NDK 18? or use what this warning suggests it supports?\r\n![image 11-25-18 at 5 59 pm](https://user-images.githubusercontent.com/22269225/48986040-77042880-f0dd-11e8-9ba1-173136bb20c0.jpeg)\r\n", "You can ignore that. See the last comment [here](https://github.com/bazelbuild/bazel/issues/4742).", "@cathybgyz Is this still an issue?", "@ymodak I got it working on mac and I wrote a [blog] to introduce this process. Hope it would be helpful to others. [here](https://zimenglyu.com/android/tensorflow/machinelearning/mobile/english/2018/11/27/tflite-android-ndk-eng.html)", "@cathybgyz looking forward to your blog! other resources seem more scatterbrained on the subject... Thank you \ud83d\udc4d ", "Awesome. Thanks for your contribution. Can I close this issue now since you have found the workaround solution?", "Yes.\r\nThanks", "@cathybgyz Your blog is not online anymore. Can you point to a different link where the documentation is available?", "@rribani Sorry about that, you can find it [here](https://zimenglyu.com/en/ml/android/tensorflow/2018/11/27/tflite-android-ndk-eng.html)", "Perfect! Nice tutorial.. Thanks!", "Thank you so much!", "Is there any easier way to do that, for example, do not build it by oneself? Thanks!", "> @ymodak I got it working on mac and I wrote a [blog] to introduce this process. Hope it would be helpful to others. [here](https://zimenglyu.com/android/tensorflow/machinelearning/mobile/english/2018/11/27/tflite-android-ndk-eng.html)\r\n\r\nI see the link doesn't work. Can you please help?", "Here's the correct link:\r\nhttps://zimenglyu.com/en/ml/android/tensorflow/2018/11/27/tflite-android-ndk-eng.html", "For future reference, I had a ton of trouble and that guide helped me set up TF Lite on C++ for Android NDK. @zimenglyu's personal website got changed around a little so the new link is: https://zimenglyu.com/posts/2018-11-27-tflite-android-ndk-eng.\r\n\r\nJust dropping it here in case anyone else needs to reference it. Thank you @zimenglyu !"]}, {"number": 23895, "title": "fix return type error in python3", "body": "The function `_export_output_to_tensors` in [tpu_estimator.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py) is expected to return a list, but the third conditional branch returns a dict_values in python3 which could potentially cause type error in some situations.\r\nAn example is when calling tf.contrib.tpu.TPUEstimator.export_savedmodel in python3.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it! @googlebot", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 23894, "title": "Anaconda jupyter noteboob  'Kernel Restarting(The kenel appears to have died.It will restart automatically)'when i used tensorflow-gpu", "body": "this is the code\r\nimport os\r\nimport tensorflow as tf \r\nfrom PIL import Image\r\nfrom nets import nets_factory\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt  \r\nCHAR_SET_LEN = 10\r\n\r\nIMAGE_HEIGHT = 60 \r\n\r\nIMAGE_WIDTH = 160  \r\n\r\nBATCH_SIZE = 1\r\n\r\nTFRECORD_FILE = \"D:/Tensorflow/captcha/test.tfrecords\"\r\n\r\n# placeholder\r\nx = tf.placeholder(tf.float32, [None, 224, 224])  \r\n\r\n\r\ndef read_and_decode(filename):\r\n    \r\n    filename_queue = tf.train.string_input_producer([filename])\r\n    reader = tf.TFRecordReader()\r\n   \r\n    _, serialized_example = reader.read(filename_queue)   \r\n    features = tf.parse_single_example(serialized_example,\r\n                                       features={\r\n                                           'image' : tf.FixedLenFeature([], tf.string),\r\n                                           'label0': tf.FixedLenFeature([], tf.int64),\r\n                                           'label1': tf.FixedLenFeature([], tf.int64),\r\n                                           'label2': tf.FixedLenFeature([], tf.int64),\r\n                                           'label3': tf.FixedLenFeature([], tf.int64),\r\n                                       })\r\n\r\n    image = tf.decode_raw(features['image'], tf.uint8)\r\n\r\n    image_raw = tf.reshape(image, [224, 224])\r\n    # tf.train.shuffle_batch\u5fc5\u987b\u786e\u5b9ashape\r\n    image = tf.reshape(image, [224, 224])\r\n\r\n    image = tf.cast(image, tf.float32) / 255.0\r\n    image = tf.subtract(image, 0.5)\r\n    image = tf.multiply(image, 2.0)\r\n\r\n    label0 = tf.cast(features['label0'], tf.int32)\r\n    label1 = tf.cast(features['label1'], tf.int32)\r\n    label2 = tf.cast(features['label2'], tf.int32)\r\n    label3 = tf.cast(features['label3'], tf.int32)\r\n\r\n    return image, image_raw, label0, label1, label2, label3\r\n\r\nmage, image_raw, label0, label1, label2, label3 = read_and_decode(TFRECORD_FILE)\r\n\r\n#\u4f7f\u7528shuffle_batch\u53ef\u4ee5\u968f\u673a\u6253\u4e71\r\nimage_batch, image_raw_batch, label_batch0, label_batch1, label_batch2, label_batch3 = tf.train.shuffle_batch(\r\n        [image, image_raw, label0, label1, label2, label3], batch_size = BATCH_SIZE,\r\n        capacity = 50000, min_after_dequeue=10000, num_threads=1)\r\n\r\n\r\ntrain_network_fn = nets_factory.get_network_fn(\r\n    'alexnet_v2',\r\n    num_classes=CHAR_SET_LEN,\r\n    weight_decay=0.0005,\r\n    is_training=False)\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)  \r\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\r\n    # inputs: a tensor of size [batch_size, height, width, channels]\r\n    X = tf.reshape(x, [BATCH_SIZE, 224, 224, 1])\r\n    \r\n    logits0,logits1,logits2,logits3,end_points = train_network_fn(X)\r\n    \r\n    \r\n    predict0 = tf.reshape(logits0, [-1, CHAR_SET_LEN])  \r\n    predict0 = tf.argmax(predict0, 1)  \r\n\r\n    predict1 = tf.reshape(logits1, [-1, CHAR_SET_LEN])  \r\n    predict1 = tf.argmax(predict1, 1)  \r\n\r\n    predict2 = tf.reshape(logits2, [-1, CHAR_SET_LEN])  \r\n    predict2 = tf.argmax(predict2, 1)  \r\n\r\n    predict3 = tf.reshape(logits3, [-1, CHAR_SET_LEN])  \r\n    predict3 = tf.argmax(predict3, 1)  \r\n\r\n    \r\n    sess.run(tf.global_variables_initializer())\r\n    \r\n    saver = tf.train.Saver()\r\n    saver.restore(sess,'./captcha/models/crack_captcha.model-6000')\r\n\r\n    \r\n    coord = tf.train.Coordinator()\r\n    \r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n    for i in range(10):\r\n        \r\n        b_image, b_image_raw, b_label0, b_label1 ,b_label2 ,b_label3 = sess.run([image_batch, \r\n                                                                    image_raw_batch, \r\n                                                                    label_batch0, \r\n                                                                    label_batch1, \r\n                                                                    label_batch2, \r\n                                                                    label_batch3])\r\n       \r\n        img=Image.fromarray(b_image_raw[0],'L')\r\n        plt.imshow(img)\r\n        plt.axis('off')\r\n        plt.show()\r\n        \r\n        print('label:',b_label0, b_label1 ,b_label2 ,b_label3)\r\n       \r\n        label0,label1,label2,label3 = sess.run([predict0,predict1,predict2,predict3], feed_dict={x: b_image})\r\n        \r\n        print('predict:',label0,label1,label2,label3) \r\n                \r\n    \r\n    coord.request_stop()\r\n    \r\n    coord.join(threads)", "comments": ["I would recommend you to use python script instead of jupyter notebook in this case. This will help in fetching more information related to warnings/error messages that may incur.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23893, "title": "StructuralEnsembleRegressor gives warning: Converting sparse IndexedSlices to a dense Tensor of unknown shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI break down the stock example and run it on debug mode. I used my own sample data which has 1000 rows x 15 cols.  here's the code:\r\n#----\r\n    reader = tf.contrib.timeseries.CSVReader(csv_file_name,column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)+ (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * N_COLS))\r\n    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=N_COLS, window_size = 15)\r\n    training_steps=150\r\n    structural = tf.contrib.timeseries.StructuralEnsembleRegressor(periodicities=[7,100], num_features=N_COLS, cycle_num_latent_values=3)\r\n    ff = structural.train(input_fn=train_input_fn, steps=training_steps)\r\n#----\r\n- OS Platform and Distribution: Windows 10, Anaconda, Python3.6\r\n- TensorFlow installed from (source or binary): 1.12.0\r\n- GPU model and memory: Quadro K2200\r\n\r\nChecked related SO posts and tf source code (gradients_impl.py), looks to me this was due to that TensorFlow automatically densifies the tf.IndexedSlices for unknown shape.  This also makes it throw OOM error for slightly larger dataset (1000rows x 25cols). I also saw a note from gradient_impl.py that reads:\r\n  # TODO(mrry): Consider adding static shape information to\r\n  # IndexedSlices, to avoid using numpy here.\r\n\r\n", "comments": ["Reassigning to @agarwal-ashish, since the fix here is in the application code in `tf.contrib.timeseries`.", "@wliangaz,\r\nSorry for the delayed response. In the **`Tensorflow Version 2.x`**, since we use [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras/) and [tf.data](https://www.tensorflow.org/guide/data) predominantly and don't use [Estimators](https://www.tensorflow.org/guide/estimator) much, can you please let us know if this Feature is still relevant? \r\n\r\nAlso, please refer [this Tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series) which demonstrates the usage of [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras/) for **`Time Series Data`**.\r\n\r\nThanks!  ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23892, "title": "TensorBoard Callback write_images", "body": "I want to use the TensorBoard callback to visualize my conv layer kernels. But i can only see the first conv layer kernel in TensorBoard and my Dense layers at the end. For the other conv layers i can just see the bias values and not the kernels.\r\n\r\nHere is my sample code for the Keras model.\r\n\r\n# Imports\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nfrom os import makedirs\r\nfrom os.path import exists, join\r\nfrom keras.datasets import mnist\r\nimport time\r\n\r\nfrom keras.layers import *\r\nfrom keras.activations import *\r\nfrom keras.models import *\r\nfrom keras.optimizers import *\r\nfrom keras.initializers import *\r\nfrom keras.callbacks import TensorBoard\r\nfrom keras.callbacks import ModelCheckpoint\r\nfrom keras.utils.np_utils import to_categorical\r\n\r\nfrom plotting import *\r\n\r\nlog_dir = '\"./\"\r\n\r\n# Load MNIST dataset\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nbatch_size = 128\r\nepochs = 10\r\nwidth = 28\r\nheight = 28\r\ndepth = 1\r\nnum_classes = 10\r\ntrain_size = x_train.shape[0]\r\ntest_size = x_test.shape[0]\r\n\r\nx_train = x_train.reshape(train_size, width, height, depth)\r\ny_train = to_categorical(y_train, num_classes=num_classes)\r\nx_test = x_test.reshape(test_size, width, height, depth)\r\ny_test = to_categorical(y_test, num_classes=num_classes)\r\n\r\ntb = TensorBoard(\r\n    log_dir=log_dir, \r\n    histogram_freq=1, \r\n    write_graph=True, \r\n    write_images=True)\r\n\r\n# Define the DNN\r\nmodel = Sequential()\r\nmodel.add(Conv2D(filters=16, kernel_size=3, input_shape=(width, height, depth), name=\"conv1\"))\r\nmodel.add(Activation(\"relu\"))\r\nmodel.add(Conv2D(filters=20, kernel_size=3, name=\"conv2\"))\r\nmodel.add(Activation(\"relu\"))\r\nmodel.add(MaxPool2D())\r\n\r\nmodel.add(Conv2D(filters=24, kernel_size=3, name=\"conv3\"))\r\nmodel.add(Activation(\"relu\"))\r\nmodel.add(Conv2D(filters=28, kernel_size=3, name=\"conv4\"))\r\nmodel.add(Activation(\"relu\"))\r\nmodel.add(MaxPool2D())\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128))\r\nmodel.add(Activation(\"relu\"))\r\nmodel.add(Dense(num_classes, name=\"features\"))\r\nmodel.add(Activation(\"softmax\"))\r\n\r\n# Print the DNN layers\r\nmodel.summary()\r\n\r\n# Train the DNN\r\nlr = 1e-3\r\noptimizer = Adam(lr=lr)\r\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\r\nmodel.fit(x_train, y_train, verbose=1, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=[tb])\r\n\r\n# Test the DNN\r\nscore = model.evaluate(x_test, y_test, batch_size=batch_size)\r\nprint(\"Test performance: \", score)\r\n\r\n![ktb](https://user-images.githubusercontent.com/42785357/48809367-29508000-ecd9-11e8-93e8-dd73713c4515.png)\r\n", "comments": []}, {"number": 23891, "title": "Can we have working example with assets in the documentation for the simple_save?", "body": "**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/saved_model/simple_save \r\n\r\nSimple_save seems to be a newer and more standardized saving approach. Since it can also use assets to store vocabularies, I wonder if it is possible to have a working example in document based on keras sentiment analysis system's data? So there is place to look up how to really integrate the entire model (including vocabulary, word embedding, etc) in a single simple_save.", "comments": ["@allenlavoie Allen, what is the current best way to save a model and do we have a documentation page for it?\r\n\r\nsimple_save seems to be deprecated.", "simple_save won't appear in v2. We do have a SavedModel API: https://www.tensorflow.org/alpha/guide/saved_model\r\n\r\nI don't have plans to write a sentiment analysis example, but it does sound useful.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23890, "title": "Failed to allocate tensors : Xcode 10 ", "body": "\r\n**System information**\r\n- OS Platform and Distribution:  macOS Mojave 10.14.2 Beta\r\n- Mobile device : iPhone 8P / IPhone XR\r\n- TensorFlow installed from (source or binary):  SOURCE\r\n- TensorFlow version:  Newest \r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: Unknown\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\nXcode Crashed when I import my models into the TF-Lite example project. I don't know what to do or try to get he models to work.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Import the TFlite and txt files \r\n2. copy the path if needle ( Xcode asks this )\r\n3. rename model_file_name to imported tflite model\r\n4. rename label_file_name to imported txt file\r\n\r\n**Any other info / logs**\r\n Loaded model 1resolved reportertensorflow/contrib/lite/kernels/squeeze.cc:64 current >= 0 && current < input_num_dims && input_dims->data[current] == 1 was not true.\r\nNode 66 failed to prepare.\r\n\r\nFailed to allocate tensors!2018-11-20 13:08:26.664470-0600 tflite_camera_example[9610:2085000] [MC] System group container for systemgroup.com.apple.configurationprofiles path is /private/var/containers/Shared/SystemGroup/systemgroup.com.apple.configurationprofiles\r\n2018-11-20 13:08:26.664721-0600 tflite_camera_example[9610:2085000] [MC] Reading from public effective user settings.\r\n(lldb) \r\n", "comments": ["I am still lost on what the error exactly means? The TFLite models were given to me to get in the demo iOS app. but that's it. ", "What models are you using can you provde links? It sounds like the conversion didn't work properly. Did the person that gave them you test them with the tflite interpreter? You can dump a graphviz representation of the model using TocoConvert if you have the original savedmodel or .pb tensorflow model.\r\n\r\n", "Closing the bug since it has been few months waiting on reply. @michael-stram please feel free to open new one or reopen this one if the problem is still happening with the details in @aselle  reply above.\r\n\r\nThanks"]}, {"number": 23889, "title": "[WIP] Add tensor forest classification train in core ", "body": "According to https://github.com/tensorflow/community/blob/master/rfcs/20180626-tensor-forest.md\r\n\r\nThis is a first step. And the whole process is tracked by #21830\r\n\r\nWe are adding an canned TensorForestClassifier and TensorForestRegressor into core.\r\n\r\nThe Pr is only a third step implementing TensorForestClassifier with train only functionality.", "comments": ["Nagging Reviewer @yifeif: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "close this and add a new one https://github.com/tensorflow/tensorflow/pull/24656"]}, {"number": 23888, "title": "Error: ...tensorflow/core/common_runtime/bfc_allocator.cc:373] tried to deallocate nullptr", "body": "**System information**\r\nUbuntu 18.04 (bionic);\r\ng++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0;\r\nbazel release 0.18.1\r\nPython 2.7.15rc1\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2601035/tf_env.txt)\r\n\r\n**Current behavior**\r\nI've built TensorFlow from commit\r\n`git log -1`\r\n`commit a6d8ffae097d0132989ae4688d224121ec6d8f35 (HEAD, tag: v1.12.0, origin/r1.12)`\r\n`Author: Todd Wang <toddwang@gmail.com>`\r\n`Date:   Thu Nov 1 18:35:10 2018 -0700`\r\n`Fix a bug in tpu.py and xla.py that while creating an identity node for control input edges under rewrite context, the parent control flow context is lost. (#23446)\r\nPiperOrigin-RevId: 219724472`\r\nwith command line\r\n`bazel build --config=mkl -c opt --copt=-mavx --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package`\r\nand got the error, repeatedly appearing in terminal:\r\n`E tensorflow/core/common_runtime/bfc_allocator.cc:373] tried to deallocate nullptr`\r\nwhile attempting to run the example [Text generation using a RNN with eager execution](https://www.tensorflow.org/tutorials/sequences/text_generation).\r\nAlso can be reproduced with the package, installed with `pip install https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.11.0-cp27-cp27mu-linux_x86_64.whl `.\r\n\r\n\r\n**Expected behavior**\r\nRunning without error messages.\r\n", "comments": ["@muarvyn  Please try with Bazel 0.15, TF version 1.12 and see if you are running into the same error. Please post with your updates. ", ">  Please try with Bazel 0.15, TF version 1.12\r\n\r\nThe same result - \"...bfc_allocator.cc:373] tried to deallocate nullptr\"", "I'm seeing the same issue running TensorFlow benchmarks with `--xla=True --allow_growth=True`.", "I see the same error, \"...bfc_allocator.cc:373] tried to deallocate nullptr\"", "I see this as well, also using mkl.", "the same issue. Tensorflow installed using miniconda:\r\n  - tensorflow=1.12.0=mkl_py36h69b6ba0_0\r\n  - tensorflow-base=1.12.0=mkl_py36h3c3e929_0\r\n", "I meet with the same issue, any solutions?", "linux\r\ntensorflow 1.13.1\r\n--config=mkl\r\n--config=xla\r\nsame issue", "the same issue\r\ntensorflow 1.13.1 cpu \r\n", "We are investigating this bug and will update as soon as we have the fix", "@muarvyn \r\nIs this issue present?\r\nIf yes, could you update the latest status? so that we could continue supporting.", "@muarvyn \r\n\r\nIf your issue is disappear, could you close it?", "@muarvyn We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is out of support window. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. please check [**`link`**](https://stackoverflow.com/questions/54694022/warning-tried-to-deallocate-nullptr-when-using-tensorflow-eager-execution-with) Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23888\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23888\">No</a>\n"]}, {"number": 23887, "title": "Hessian diagonal computation", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI'm computing the full Hessian before extracting the diagonal.\r\n\r\n**Describe the expected behavior**\r\n\r\nI want to only compute the diagonal to avoid the overhead of computing the full Hessian.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ny_true = np.array([\r\n    [1, 0, 0, 0, 0],\r\n    [0, 1, 0, 0, 0],\r\n    [0, 0, 0, 1, 0],\r\n    [0, 0, 1, 0, 0],\r\n    [0, 0, 0, 0, 1],\r\n    [0, 0, 0, 0, 1],\r\n    [0, 0, 0, 0, 1]\r\n], dtype=float)\r\n\r\ny_pred = np.array([\r\n    [1, 0, 0, 0, 0],\r\n    [0, 1, 0, 0, 0],\r\n    [0, 0, 0, 1, 0],\r\n    [0, 0, 1, 0, 0],\r\n    [0, 0, 0, 0, 1],\r\n    [0, 0, 0, 0, 1],\r\n    [0, 0, 0, 0, 1]\r\n], dtype=float)\r\n\r\nweights = np.array([1, 1, 1, 1, 1], dtype=float)\r\n\r\nwith tf.Session():\r\n\r\n    # We first convert the numpy arrays to Tensorflow tensors\r\n    y_true = tf.convert_to_tensor(y_true)\r\n    y_pred = tf.convert_to_tensor(y_pred)\r\n    weights = tf.convert_to_tensor(weights)\r\n\r\n    # The following code block is a custom loss \r\n    ys = tf.reduce_sum(y_true, axis=0)\r\n    y_true = y_true / ys\r\n    ln_p = tf.nn.log_softmax(y_pred)\r\n    wll = tf.reduce_sum(y_true * ln_p, axis=0)\r\n    loss = -tf.tensordot(weights, wll, axes=1)\r\n\r\n    grad = tf.gradients(loss, y_pred)[0]\r\n\r\n    hess = tf.hessians(loss, y_pred)[0]\r\n    hess = tf.diag_part(hess)\r\n\r\n    print(hess.eval())\r\n```\r\n\r\nThis outputs:\r\n\r\n```python\r\n[[0.24090069 0.12669198 0.12669198 0.12669198 0.12669198]\r\n [0.12669198 0.24090069 0.12669198 0.12669198 0.12669198]\r\n [0.12669198 0.12669198 0.12669198 0.24090069 0.12669198]\r\n [0.12669198 0.12669198 0.24090069 0.12669198 0.12669198]\r\n [0.04223066 0.04223066 0.04223066 0.04223066 0.08030023]\r\n [0.04223066 0.04223066 0.04223066 0.04223066 0.08030023]\r\n [0.04223066 0.04223066 0.04223066 0.04223066 0.08030023]]\r\n```\r\n\r\nwhich is exactly what I want. The problem is that the full Hessian is computed before actually `tf.diag_part`.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Thanks for the report, though I don't think anything is likely to be done with this unless compiler optimizations become pretty sophisticated.  For example, if you were to replace\r\n    hess = tf.diag_part(hess)\r\nwith\r\n   hess = hess[1, 2]\r\n\r\nOne might argue that we similarly shouldn't need to compute the full hessian just for a single element.  However, doing this optimization in practice I think is beyond what most frameworks can do today, though it is a good aspirational goal.  I'm going to close this issue since it's unlikely that any specific action will be done to address this issue; a more general compiler approach would be needed, or a very specific hessian_diag op would need to be implemented."]}, {"number": 23886, "title": "Add PYTHONPATH to bazelrc when necessary", "body": "When the user choosen python_lib_path was retreived from the\r\nPYTHONPATH environment variable, need to set PYTHONPATH in the\r\nbazelrc file so bazel includes it in all the build operations.\r\n\r\nFixes tensorflow#23695", "comments": ["@angersson  Could you please look into the 2 failed Windows Bazel checks. Please let us know if we can bypass them to proceed further ?"]}, {"number": 23885, "title": "tensorflow/core/framework/op_kernel.cc:1261] Invalid argument: ValueError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, I also tested the code in Examples\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: TITAN Xp (12196MiB)\r\n--\r\n\r\n\r\n**Describe the current behavior**\r\nI have been testing your MirroredStrategy() function with your example script and my own script. \r\n- It worked well on the sample script. \r\n- When I adapted my own script, it kept emitting the following error I don't know how to interpret/search.\r\n\r\n```\r\nWARNING:tensorflow:Not all devices in distribute strategy are visible by TensorFlow sessions.\r\nWARNING:tensorflow:You are accessing attribute optimizerof the \r\nDistributedCallbackModel that may not have been set correctly.\r\nWARNING:tensorflow:You are accessing attribute \r\n_unconditional_checkpoint_dependenciesof the DistributedCallbackModel \r\nthat may not have been set correctly.\r\nEpoch 1/1\r\n2018-11-20 16:41:15.195119: W tensorflow/core/framework/op_kernel.cc:1261] \r\nInvalid argument: ValueError: `generator` yielded an element of shape (1, 1) \r\nwhere an element of shape (1,) was expected.\r\nTraceback (most recent call last):\r\n\r\n  File \"xxxxx/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py\", \r\nline 206, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"xxxxx/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\", \r\nline 487, in generator_py_func\r\n    \"of shape %s was expected.\" % (ret_array.shape, expected_shape))\r\n\r\nValueError: `generator` yielded an element of shape (1, 1) \r\nwhere an element of shape (1,) was expected.\r\n\r\n\r\n[[{{node PyFunc}} = PyFunc[Tin=[DT_INT64], Tout=[DT_INT32, DT_INT64], \r\ntoken=\"pyfunc_1\"](arg0)]]                                                   \r\n[[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,500], [?,1]], \r\noutput_types=[DT_INT32, DT_INT64]](IteratorFromStringHandle V2)]] \r\n[[{{node ExperimentalFunctionBufferingResourceGetNext}} = \r\nExperimentalFunctionBufferingResourceGetNext[output_types=[DT_INT32, DT_INT64] , \r\n_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]\r\n(ExperimentalFunctionBufferingResource)]]\r\nException ignored in:  6e80>>\r\n--\r\n\r\n\r\n\r\n```\r\nThe script without \t```distribution = tf.contrib.distribute.MirroredStrategy()``` ran well. Since I need to populate the network with a huge set, I plan to parallize the work on various GPUs. \r\n\r\n\r\n**Describe the expected behavior**\r\nPlease kindly let me know how should I interpret the errors and what the solutions are. Thank you.\r\n\r\n**Code to reproduce the issue**\r\nMy network architecture looks like this (a hierarchical one): \r\n```\r\nconfig = tf.ConfigProto(allow_soft_placement = True, log_device_placement= False)\r\n\tsess = tf.Session(config = config)\r\n\twith tf.Session(config = config) as sess:\r\n\t\tsess.run(tf.global_variables_initializer())\r\n                # load in data\r\n\t         #######################DNN Level 1########################\r\n\t        if options.L1_model == 0:\r\n\t\t        model = BuildModel()\r\n\t\t        model.fit(X_train, y_train[:, 0],\r\n\t\t\t\t  validation_data=(X_test, y_test[:, 0]),\r\n\t\t\t\t  epochs=options.epochs,\r\n\t\t\t\t  verbose=2,\r\n\t\t\t\t  batch_size=options.batch_size_L1)\r\n          #######################DNN Level 1########################\r\n\t         if options.L2_model == 0:\r\n \r\n\t\t         model = BuildModel()\r\n\t\t          model.fit(..., batch_size=options.batch_size_L2)\r\n\r\n```\r\n\r\nThe model stopped already at the first level. And I don't think the failure has something to do with a hierarchical structure. \r\n\r\n`BuildModel()` is defined this way:\r\n```\r\ndef buildModel_DNN(word_index, embeddings_index, nClasses, \r\nMAX_SEQUENCE_LENGTH, EMBEDDING_DIM, gpusno, nLayers=3,Number_Node=100, dropout=0.5):\r\n\t\r\n\tembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\r\n\tfor word, i in word_index.items():\r\n\t\tembedding_vector = embeddings_index.get(word)\r\n\t\tif embedding_vector is not None:\r\n\t\t\tembedding_matrix[i] = embedding_vector\r\n\t\t\t\r\n\tmodel = tf.keras.models.Sequential()\r\n\t\r\n\tmodel.add(tf.keras.layers.Embedding(len(word_index) + 1,\r\n\t\t\t\t\t\t\t\tEMBEDDING_DIM,\r\n\t\t\t\t\t\t\t\tweights=[embedding_matrix],\r\n\t\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\r\n\t\t\t\t\t\t\t\ttrainable=True))\r\n\tmodel.add(tf.keras.layers.Flatten())\r\n\t\r\n\tfor i in range(0,nLayers):\r\n\t\tmodel.add(tf.keras.layers.Dense(Number_Node, activation='relu'))\r\n\t\tmodel.add(tf.keras.layers.Dropout(dropout))\r\n\t\r\n\tmodel.add(tf.keras.layers.Dense(nClasses, activation='softmax'))\r\n\r\n\t\r\n\tdistribution = tf.contrib.distribute.MirroredStrategy(\r\n                               ['/device:GPU:0', '/device:GPU:1'], prefetch_on_device=True)\r\n\t\r\n\tmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n\t\t\t\t  optimizer=tf.train.AdamOptimizer(),\r\n\t\t\t\t  metrics=['accuracy'], \r\n\t\t\t\t  # distribute=distribution,\r\n\t\t\t\t  options = run_opts)\r\n\t\t\t\t  \r\n\tprint('model summary:') \r\n\tmodel.summary()\r\n\r\n\treturn model\r\n```", "comments": ["Reassigning this to @josh11b, because this is about the distribution strategy component.", "Ok, I solved this problem on my own after a tedious period of debugging. I realized the input of X_train and X_test is not in np.ndarray() but in `<class 'numpy.matrixlib.defmatrix.matrix'>`, which generates the error `ValueError: `generator` yielded an element of shape (1, 1) \r\nwhere an element of shape (1,) was expected.`\r\n\r\nPlease close this case. \r\n\r\nThank you."]}]