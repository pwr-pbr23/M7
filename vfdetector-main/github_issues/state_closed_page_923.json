[{"number": 25764, "title": "Different behavior between Keras and tensorflow.keras in v1.13.0rc1 on placeholder and variable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n - TensorFlow installed from (source or binary): Binary I suppose\r\n- TensorFlow version (use command below): 1.13.0rc1\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nSo I'm teaching a course and decide to use `tensorflow.keras` instead of Keras.\r\nI then noticed how the two systems handle the `placeholder` and `variable` are different even though\r\nthey are using the same backend. I was trying to compile a simple adding function:\r\n\r\n```python\r\n# variable can be initialized with a value like this\r\ninit_variable_1 = np.zeros(shape=(2, 2))\r\nvariable_1 = K.variable(value=init_variable_1)\r\n\r\n# variable can also be initialized with particular functions like this\r\nvariable_2 = K.ones(shape=(2, 2))\r\n\r\nadd_tensor = variable_1+variable_2\r\nadd_function = K.function(inputs=[variable_1, variable_2],\r\n                          outputs=(add_tensor,))\r\n```\r\n\r\nUntil this line, both Keras and `tensorflow.keras` allow me to run without errors or warning.\r\n\r\nHowever, for the following code, Keras can pass while `tensorflow.keras features an error:\r\n\r\n```\r\na = np.array([[1, 3], [2, 4]], dtype=np.float32)\r\nb = np.array([[3, 2], [5, 6]], dtype=np.float32)\r\n\r\nprint(add_function((a,b)))\r\n```\r\n\r\nThe error is:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-8-4df03c3d618b> in <module>()\r\n     10 print(a, b)\r\n     11 \r\n---> 12 print(add_function((a,b)))\r\n     13 \r\n     14 # notice that the add_function created is independent of the variables\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3074 \r\n   3075     fetched = self._callable_fn(*array_vals,\r\n-> 3076                                 run_metadata=self.run_metadata)\r\n   3077     self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n   3078     return nest.pack_sequence_as(self._outputs_structure,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)\r\n   1437           ret = tf_session.TF_SessionRunCallable(\r\n   1438               self._session._session, self._handle, args, status,\r\n-> 1439               run_metadata_ptr)\r\n   1440         if run_metadata:\r\n   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    526             None, None,\r\n    527             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 528             c_api.TF_GetCode(self.status.status))\r\n    529     # Delete the underlying status object from memory otherwise it stays alive\r\n    530     # as there is a reference to status from this from the traceback due to\r\n\r\nInvalidArgumentError: ResourceHandleToInputTensor() received non-DT_RESOURCE Tensor: 1\r\n\t [[{{node _arg_Variable_0_0}}]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nSo the API `function` defines the input should be a list of `placeholders`.\r\nSo either someone (maybe @fchollet) need to change the definition in the API so both `placeholder` and `variable` are fine to be valid inputs (looks like they can be compiled fine in both systems). And make `tensorflow.keras` correct also. Or limit the input tensors to be `placeholder`s where some type checks need to be done.\r\n\r\nBy the way, I'm not sure this os a specific bug in v1.13.0rc1 and v1.12 or it happens to other releases as well since I was using Colab.\r\n\r\nRegards,\r\nYuhuang.", "comments": ["@duguyue100 For me both are giving same output. Could you provide the code you are using to reproduce the bug? Thanks!", "Hi @jvishnuvardhan, here it is in Colab notebook:\r\n\r\nhttps://colab.research.google.com/drive/1W4KtanniPfgX4OoTTADsO-g_QFvQ1CSM", "Just tested on my local machine which has the specs: Ubuntu 16.04+Python 3.6+TensorFlow 1.12+CUDA 9, it reports the same error as in the Colab notebook:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/dgywork/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 2986, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/dgywork/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n  File \"/home/dgywork/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: ResourceHandleToInputTensor() received non-DT_RESOURCE Tensor: 1\r\n\t [[{{node _arg_Variable_1_0_0}} = _Arg[T=DT_RESOURCE, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\t [[{{node _arg_Variable_2_0_1/_13}} = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_9__arg_Variable_2_0_1\", tensor_type=DT_RESOURCE, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n```", "any feedback on this? @jvishnuvardhan ", "@ymodak, any update on this?", "I think it should be able to receive any symbolic tensor, can you try converting you variable via tf.convert_to_tensor before feeding into the function?", "@tanzhenyu Apply `tf.convert_to_tensor` on the variables works. Then there must be an issue in the Keras implementation then because the program still doesn't work applying the `convert_to_tensor` function. In the meantime, the backend of standalone Keras can use variables as placeholders apparently.", "@duguyue100,\r\nSorry for the delayed response. Can you please confirm if this issue is resolved, as you stated:\r\n\r\n> Apply tf.convert_to_tensor on the variables works.  \r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25764\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25764\">No</a>\n", "> @duguyue100,\r\n> Sorry for the delayed response. Can you please confirm if this issue is resolved, as you stated:\r\n> \r\n> > Apply tf.convert_to_tensor on the variables works.\r\n> \r\n> Thanks!\r\n\r\nThanks!"]}, {"number": 25763, "title": "TFTRT: Support new unary ops for TRT 5.1+", "body": "TRT 5.1 adds the following unary ops:\r\nSin, Cos, Tan, Sinh, Cosh, Asin, Acos, Atan, Asinh, Acosh, Atanh, Ceil, Floor\r\n\r\nThis PR:\r\n* Adds support for converting these new ops\r\n* Adds validators for all unary ops\r\n* Adds unit tests for all unary ops", "comments": ["Also, consider rebasing to pick up the following commit.\r\nhttps://github.com/tensorflow/tensorflow/commit/14abf581047502dc17b8fcbbc5e5ab8bac5311ec", "We will also need to update the following. The test is failing after this PR.\r\nhttps://github.com/tensorflow/tensorflow/blob/95147fbbf7b62b421c3a4da7f770249abc3f1287/tensorflow/compiler/tf2tensorrt/convert/convert_graph_test.cc#L111"]}, {"number": 25762, "title": "Missing clean_dep call in tf_custom_op_library_additional_deps", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.0-rc0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 10\r\n\r\n**Describe the problem**\r\n\r\nI believe the 'if_windows' part of \r\nhttps://github.com/tensorflow/tensorflow/blob/248016edd1555315c4db540e668d42b95292c2c7/tensorflow/tensorflow.bzl#L1469\r\nis missing a call to clean_deps() and should be:\r\n\r\n    ] + if_windows([clean_dep(\"//tensorflow/python:pywrap_tensorflow_import_lib\")])\r\n\r\n\r\nNote that I am not actually using windows, and my custom op builds and works fine. However, when I call:\r\n\r\n    bazel query \"deps(//learning/tensorflow_ops:custom_op_tf.so)\"\r\n\r\nI get the error\r\n\r\n    ERROR: learning/tensorflow_ops/BUILD:9:1: no such package 'tensorflow/python': BUILD file not found on package path and referenced by '//learning/tensorflow_ops:custom_op_tf.so_check_deps'\r\n    ERROR: Evaluation of query \"deps(//learning/tensorflow_ops:custom_op_tf.so)\" failed: errors were encountered while computing transitive closure", "comments": ["@Alfus Sorry for the delay. Could you check whether this issue is persisting with latest version of TF? Thanks!", "The issue still remains at the current master. Note that this problem will only show up when including the tf source as an external repo (this is the case that clean_dep is designed to fix)", "I completely missed this issue.\r\n@yifeif you were working with custom ops on windows.\r\nCould you patch the suggested fix here?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25762\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25762\">No</a>\n"]}, {"number": 25761, "title": "XLA makes incorrect calls into cuBLAS", "body": "XLA GPU will sometimes make what looks like illegal calls into cuBLAS.\r\n\r\n```\r\n $ cat /tmp/bad-dot.hlo\r\nHloModule Main\r\n\r\nENTRY Main {\r\n  p0 = f32[6656,96] parameter(0)\r\n  p1 = f32[6656,384] parameter(1)\r\n  dot = f32[96,384] dot(p0, p1), lhs_contracting_dims={0}, rhs_contracting_dims={0}\r\n  ROOT tr = f32[384,96] transpose(dot), dimensions={1,0}\r\n}\r\n```\r\n\r\n```\r\n $ bazel run -c opt tensorflow/compiler/xla/tools:replay_computation_gpu -- /tmp/bad-dot.hlo --use_fake_data\r\n```\r\n\r\n```\r\n...\r\n2019-02-14 09:17:41.060396: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcudart.so.9.0\r\n2019-02-14 09:17:41.062661: E tensorflow/compiler/xla/tools/replay_computation.cc:389] iteration complete.\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ** On entry to GEMM_EX  parameter number 9 had an illegal value\r\n ...\r\n```", "comments": ["And to add a little more context: my current guess is that maybe the `lda` and `ldb` parameters are wrong during autotuning if the output has a `{0, 1}` layout.", "@jlebar Could you please take a look at this issue? Thanks!", "ltrace gives \r\n```\r\n[pid 35759] cublasGemmEx@libcublas.so.10.0(0x559288d7aae0, 0, 1, 384, 96, 6656, 1.000000, 0x7f1b2c800000, 0, 96, 0x7f1b2cc00000, 0, 384, 0.000000, 0x7f1b2c600000, 0, 384, 0, -1) = 7\r\n```\r\nso transa is 0/`n`, this means that `lda` (parameter 9) should be at least `m`. `m` in this case is 384, and `lda` is 96. \r\n"]}, {"number": 25760, "title": "Sudden drop in conv3d_transpose GPU performance with large input sizes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): _Yes_\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _Ubuntu 18.10_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /\r\n- TensorFlow installed from (source or binary): _Source_\r\n- TensorFlow version (use command below): _'v1.12.0-0-ga6d8ffae09' 1.12.0_\r\n- Python version: _3.6.7_\r\n- Bazel version (if compiling from source): _0.18.1_\r\n- GCC/Compiler version (if compiling from source): _4.8_\r\n- CUDA/cuDNN version: _10.0, 7.3.1_\r\n- CPU model: _Intel(R) Xeon(R) Bronze 3106 CPU @ 1.70GHz_\r\n- GPU model and memory: _Nvidia GeForce GTX 1080 Ti 11GB_\r\n\r\n**Describe the current behavior**\r\nOn GPU, the _conv3d_transpose_ operation shows a huge spike in terms of execution time when increasing input size from `(1, 128, 128, 128, 16)` to `(1, 256, 256, 256, 16)`.\r\nThis particular change makes average execution time without the first run change from **0.7s** to **92s**.\r\nIn comparison, the same operation on CPU goes from **2.3s** to **18.2s**.\r\nThis makes _conv3d_transpose_ perform better on CPU compared to CPU using this particular shape.\r\n\r\n**Describe the expected behavior**\r\nThe _conv3d_transpose_ operation should not see such a sudden drop in performance when increasing input size.\r\n\r\n**Code to reproduce the issue**\r\nAttached is a python notebook that reproduces the issue.\r\n[tf_conv3d.zip](https://github.com/tensorflow/tensorflow/files/2865718/tf_conv3d.zip)\r\n\r\n**Other info / logs**\r\n![download](https://user-images.githubusercontent.com/9284862/52798293-ee5fad80-3077-11e9-9b75-edc1ff504e8b.png)\r\nThis figure shows the average execution time of _conv3d_ and _conv3d_transpose_ operations on both CPU on GPU with a log scale for both x and y axis.\r\nThe x axis represents the resolution which determines the input shape in the following manner for _conv3d_ `(1, resolution, resolution, resolution, 16)` and determines the output shape for _conv3d_transpose_ `(1, resolution, resolution, resolution, 1)`.\r\nGPU runs are in blue while CPU runs are in red.\r\n_conv3d_ are represented with dash-dotted lines and _conv3d_tranpose_ with solid lines.\r\nThere is a sudden increase in the execution time of the _conv3d_transpose_ operation on GPU which goes from **0.7s** to **92s** when changing the resolution from **2^8** to **2^9**.\r\n\r\nAttached is a full run of the attached python notebook that includes the full data.\r\n[tf_conv3d.pdf](https://github.com/tensorflow/tensorflow/files/2865802/tf_conv3d.pdf)\r\n", "comments": ["@mauriceqch Could you try latest TF and check whether the bug persists or not? Thanks!", "@jvishnuvardhan Just tried on latest TF and the bug is still present.\r\nIn particular, I used the same setup as in the initial post with the following differences:\r\n\r\n* TensorFlow version (use command below):  ~~_'v1.12.0-0-ga6d8ffae09' 1.12.0_~~ -> _b'v1.13.1-0-g6612da8951' 1.13.1_\r\n* Bazel version (if compiling from source): ~~0.18.1~~ -> _0.21.0_", "Hi @mauriceqch \r\n\r\nThank you for reporting this.  Can you please minimize your reproducer and give us a simple Python file that prints out the various timings to the console?", "Hi @sanjoy \r\n\r\nThank you for your interest !\r\n\r\nBelow is a minimal python file that prints timings to the console.\r\nAdditionally, I can confirm that the problem persists with the following configuration:\r\n* TensorFlow version (use command below):  ~~_b'v1.13.1-0-g6612da8951' 1.13.1_~~ -> _'v1.15.0-0-g590d6eef7e'_\r\n* Bazel version (if compiling from source): ~~_0.21.0_~~ -> _0.26.1_\r\n* GCC/Compiler version (if compiling from source): ~~_4.8_~~ -> _7.4.0_\r\n* CUDA/cuDNN version: ~~_10.0, 7.3.1_~~ -> _10.0, 7.4.2_\r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\ntf.get_logger().setLevel('ERROR')\r\n\r\n\r\nkernel_size = (9, 9, 9)\r\nstrides = (2, 2, 2)\r\ndtype = np.float32\r\nbatch_dim = 1\r\ngpu_device = '/device:GPU:0'\r\ncpu_device = '/cpu:0'\r\nn_loops = 5\r\n# Range of resolutions to test\r\nmin_res_exp = 5\r\nmax_res_exp = 10\r\n\r\n\r\ndef benchmark(sess, x, iters):\r\n    times = np.zeros(iters)\r\n    \r\n    for i in range(iters):\r\n        start_time = time.time()\r\n        sess.run(x)\r\n        wall_time = time.time() - start_time\r\n        times[i] = wall_time\r\n    \r\n    ret = {\r\n        'average': np.average(times),\r\n        'std': np.std(times),\r\n        'first': times[0],\r\n        'average_without_first': np.average(times[1:]),\r\n        'std_without_first': np.std(times[1:]),\r\n        'iters': iters,\r\n    }\r\n    print(ret)\r\n    return ret\r\n\r\ndef spatial_dim_iters(spatial_dim):\r\n    return int((max_res_exp - np.log2(spatial_dim)) ** 2 * 3)\r\n\r\ndef run_conv3d(device, spatial_dim):\r\n    tf.reset_default_graph()\r\n    channels_dim = 4\r\n    n_filters = 16\r\n    sample_shape = (batch_dim, spatial_dim, spatial_dim, spatial_dim, channels_dim)\r\n    sample = np.random.rand(*sample_shape).astype(dtype)\r\n    print(f'Input shape: {sample.shape}')\r\n    x_init = tf.placeholder(tf.float32, shape=sample_shape)\r\n    x_val = tf.Variable(x_init)\r\n    with tf.device(device):\r\n        x = tf.layers.conv3d(x_val, n_filters, kernel_size, strides=strides, padding='same', data_format='channels_last')\r\n    print(f'Layer: {x}')\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer(), feed_dict={x_init: sample})\r\n        ret = benchmark(sess, x, spatial_dim_iters(spatial_dim))\r\n        return ret\r\n\r\n\r\ndef run_conv3d_transpose(device, spatial_dim):\r\n    tf.reset_default_graph()\r\n    channels_dim = 16\r\n    n_filters = 4\r\n    # We divide spatial_dim by two as spatial_dim is the spatial dimension of the output\r\n    sample_shape = (batch_dim, spatial_dim // 2, spatial_dim // 2, spatial_dim // 2, channels_dim)\r\n    sample = np.random.rand(*sample_shape).astype(dtype)\r\n    print(f'Input shape: {sample.shape}')\r\n    x_init = tf.placeholder(tf.float32, shape=sample_shape)\r\n    x_val = tf.Variable(x_init)\r\n    with tf.device(device):\r\n        x = tf.layers.conv3d_transpose(x_val, n_filters, kernel_size, strides=strides, padding='same', data_format='channels_last')\r\n    print(f'Layer: {x}')\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer(), feed_dict={x_init: sample})\r\n        ret = benchmark(sess, x, spatial_dim_iters(spatial_dim))\r\n        return ret\r\n\r\n\r\nCONV3D = 'CONV3D'\r\nCONV3D_TRANSPOSE = 'CONV3D_TRANSPOSE'\r\nmethods = {CONV3D: run_conv3d, CONV3D_TRANSPOSE: run_conv3d_transpose}\r\ntest_setups = [\r\n    [CONV3D, gpu_device],\r\n    [CONV3D_TRANSPOSE, gpu_device],\r\n    [CONV3D, cpu_device],\r\n    [CONV3D_TRANSPOSE, cpu_device]\r\n]\r\nresolutions = [int(2**x) for x in range(min_res_exp, max_res_exp)]\r\nprint(f'Testing resolutions: {resolutions}\\n')\r\n\r\n\r\nresults = []\r\nfor target, device in test_setups:\r\n    for res in resolutions:\r\n        print(f'Running test for: {target}, {device}, {res}')\r\n        results.append([target, device, res, methods[target](device, res)])\r\n        print('\\n')\r\n\r\nos.system('nvidia-smi')\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\nos.system('lscpu')\r\nos.system('cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2')\r\nos.system('lsb_release -a')\r\n\r\n```", "@timshen91 PTAL.  Maybe we can extract out a test case for cuDNN that demonstrates this issue (assuming TF is using cuDNN correctly)?  /CC @nluehr ", "I think this issue is mainly about slow conv3d_transpose with the shape of (1, 256, 256, 256, 16).\r\n\r\nOn my GV100 GPU, I can get 115ms for the (1, 128, 128, 128, 16) but 44762ms for the (1, 256, 256, 256, 16).\r\n\r\nAfter some digging, I found in the first case (1, 128, 128, 128, 16) the op chose the algo0 (115ms) over the algo1 (5643ms) during the autotuning phase. Whereas, in the second case (1, 256, 256, 256, 16), the op could only pick algo1 (44762ms) since the algo0 is not supported (insufficient workspace).\r\n\r\n@mauriceqch Can I ask what do you need this case for? For benchmarking or real models?", "@kaixih Many thanks for your in-depth investigation. \r\nI am using this for point cloud compression applications, specifically, the geometry information.\r\nCurrently, I am working around this issue by splitting point clouds into blocks.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25760\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25760\">No</a>\n"]}, {"number": 25759, "title": "gpudelegate + mobilenet_v1_1.0_224.tflite on a gles 3.1 device that reports MAX_WORK_COMPUTE_GROUP_INVOCATIONS = 256", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n* OS Platform and Distribution: Linux Ubuntu 16.04\r\n* Mobile device:  device running Android P and gpu with OpenGL ES 3.1  support\r\n* TensorFlow Lite version on Android: 0.0.0-gpu-experimental\r\n* Test app: TfLiteCameraDemo + GpuDelegate + mobilenet_v1_1.0_224.tflite\r\n\r\n**Describe the current behavior**\r\nHello,\r\nI have been trying to get TfLiteCameraDemo  working through the GpuDelegate on an Android P device with a gpu supporting opengles 3.1.\r\nI am trying to run mobilenet v1_1 float model.\r\n\r\nAccording to  OpenGL ES 3.1 spec, the total number of compute shader invocations in a single local work group (i.e. the product of all three values used to describe the local work group size) must not exceed the value reported for GL_MAX_COMPUTE_WORK_GROUP_INVOCATIONS. This limit is guaranteed to be no lower than 128.\r\n\r\nThe compute shaders being prepared by tensorfllow Gpu Delegate seem to not check this restriction and just try to dispatch  shaders with with a workgroup size of 1024 :\r\n \" layout(local_size_x = 16, local_size_y = 8, local_size_z = 8) in\", \r\nirrespective of what the platform declares for GL_MAX_COMPUTE_WORK_GROUP_INVOCATIONS.\r\n\r\nDue to this, the shaders fail to compile. This particular gpu supports a workgroup size of 256, which is above the minum 128 required by gles 3.1 spec.\r\n\r\nGpu Delegate  webpage doesn't mention any other requirements except a device with  OpenGl ES >=  3.1.\r\n\r\nIs there any chance the code can be tailored to check the limits  reported by the device and change the shaders accordingly?\r\n\r\nThank you", "comments": ["I ran into the same problem on a device with `GL_MAX_COMPUTE_WORKGROUP_INVOCATIONS == 512`. Yes, as @pontiusnn suggested, `GL_MAX_COMPUTE_WORKGROUP_INVOCATIONS` should be checked and respected.\r\n```\r\n...\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: GpuDelegate Prepare: Shader compilation failed: Compile failed.\r\n    ERROR: Work group size (16x8x8 = 1024) exceeds GL_MAX_COMPUTE_WORKGROUP_INVOCATIONS (512).\r\n    ERROR: main() function is missing.\r\n    2 compilation errors. No code generated\r\n...\r\n```", "@pontiusnn \r\n\r\nThanks for trying out the GPU delegate, and sorry that it has taken so long to respond; must have missed this post somehow :(\r\n\r\nThe sad news about the proper way to get `GL_MAX_COMPUTE_WORK_GROUP_INVOCATIONS` is to query the GL driver, but not all vendors implement to return the correct numbers.  Then, from  `glGetIntegerv(GL_MAX_COMPUTE_WORK_GROUP_INVOCATIONS, &max_work_group_invocations)`,\r\nwe may have a bad `max_work_group_invocations` resulting in a problem.  For now, until open-source is ready (coming soon ;) we're really working hard on it), there is unfortunately no way for you to change that.\r\n\r\nIf you let us know the exact information about your phone (make, model, GPU, GL driver etc.), that'd be great for some data point.", "@impjdi I ran into this on in-house prototype phone. I'll be in the TensorFlow Dev Summit. Maybe I can give you some of the phone next week.", "@freedomtan \r\n\r\nSounds good.  I'm not giving a talk at TF dev-summit, but can swing by.  Just let me know :)", "@impjdi How to reach you then? My email is freedom@computer.org", "Hi,\r\nThank you for your reply.\r\nFrom your post I understand that this could be a vendor's fault in not implementing correctly glGetIntegerv(GL_MAX_COMPUTE_WORK_GROUP_INVOCATIONS,...).\r\nThis is not the case on my GPU. The device reports 256 for this limit, but the gpudelegate still goes ahead and runs shaders with size of 1024.\r\n\r\nI am working with a Broadcom VC5 GPU, so probably not something you have access to test.\r\nIt is a GPU that is gles3.1 conformant and this limit is tested by one of the khronos cts tests.\r\n\r\nIf you are saying that the code that generates the shaders takes into consideration these limits but something else goes wrong I will wait till gpudelegate is open sourced. Looking forward to it.\r\n\r\nThank you.\r\n\r\n\r\n\r\n", "@freedomtan \r\n\r\nThanks for handing over the phone for investigation.  Yes, it looks like there was a bug re: workgroup sizes which is fixed currently in our codebase.  Currently, we are not 100% certain whether we should roll out a new binary or whether we should just let users wait and open source a month or two later.", "I will close this bug as it is fixed internally.  Should go out with the next GPU release, either open source or another binary release.", "@impjdi Thanks for your help.", "Hi @impjdi I'm having the exact same issue on Redmi 6:\r\n```\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #38 cannot be handled by this delegate.  Only the first 25 ops will run on the GPU, and the remaining 85 on the CPU.GpuDelegate Prepare: Shader compilation failed: Compile failed.\r\n    ERROR: Work group size (16x8x8 = 1024) exceeds GL_MAX_COMPUTE_WORKGROUP_INVOCATIONS (512).\r\n    ERROR: main() function is missing.\r\n    ERROR: 2 compilation errors. No code generated.\r\n```\r\n\r\nIs there a way to set `max_work_group_invocations` or another way to fix it now?\r\n\r\nHere's some hardware info:\r\nVendor Imagination Technologies\r\nOpenGL ES 3.2 Build 1.9@4971894\r\nPowerVR Rogue GE8320\r\n\r\nThanks", "@munum \r\n\r\nHi Michael, the last time I got the report was on the very first release.  Do you know the version of the AAR you are using?\r\n\r\nhttps://bintray.com/google/tensorflow/tensorflow-lite", "@impjdi I see. I've just tried `0.0.1-gpu-experimental` and the error is now gone. However, I'm actually using [`deeplab_mnv2_pascal_train_aug`](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz), but I get no meaningful result when gpu is turned on. Basically all pixels are classified to the same class. It works fine with CPU though. Any idea why this might be the case?"]}, {"number": 25758, "title": "Changed the type to const", "body": "Changed the type to const as it can be readonly", "comments": []}, {"number": 25757, "title": "tensorflow/stream_executor/cuda/cuda_dnn.cc:82] The primary convolution algorithm failed memory allocation, while a secondary algorithm is not provided", "body": "I am running Keras with TF backend on venv Python 3.7.2.  \r\n\r\n**System information**\r\n** pip list: \r\nabsl-py              0.6.1      \r\nastor                0.7.1      \r\nbackcall             0.1.0      \r\nbiwrap               0.1.6      \r\nbleach               3.0.2      \r\ncloudpickle          0.6.1      \r\ncycler               0.10.0     \r\ndask                 1.0.0      \r\ndecorator            4.3.0      \r\ndefusedxml           0.5.0      \r\nentrypoints          0.2.3      \r\ngast                 0.2.0      \r\ngrpcio               1.17.0     \r\nh5py                 2.8.0      \r\nimageio              2.4.1      \r\nimgaug               0.2.6      \r\nipykernel            5.1.0      \r\nipython              7.2.0      \r\nipython-genutils     0.2.0      \r\nipywidgets           7.4.2      \r\njedi                 0.13.1     \r\nJinja2               2.10       \r\njsonschema           2.6.0      \r\njupyter              1.0.0      \r\njupyter-client       5.2.4      \r\njupyter-console      6.0.0      \r\njupyter-core         4.4.0      \r\nKeras                2.1.3      \r\nKeras-Applications   1.0.6      \r\nKeras-Preprocessing  1.0.5      \r\nkiwisolver           1.0.1      \r\nMarkdown             3.0.1      \r\nMarkupSafe           1.1.0      \r\nmatplotlib           3.0.2      \r\nmistune              0.8.4      \r\nmock                 2.0.0      \r\nnbconvert            5.4.0      \r\nnbformat             4.4.0      \r\nnetworkx             2.2        \r\nnotebook             5.7.2      \r\nnumpy                1.15.4     \r\nopencv-python        3.4.4.19   \r\npandas               0.24.1     \r\npandocfilters        1.4.2      \r\nparso                0.3.1      \r\npbr                  5.1.1      \r\npexpect              4.6.0      \r\npickleshare          0.7.5      \r\nPillow               5.3.0      \r\npip                  18.1       \r\nprometheus-client    0.5.0      \r\nprompt-toolkit       2.0.7      \r\nprotobuf             3.6.1      \r\nptyprocess           0.6.0      \r\npydicom              1.2.1      \r\nPygments             2.3.0      \r\npyparsing            2.3.0      \r\npython-dateutil      2.7.5      \r\npytz                 2018.9     \r\nPyWavelets           1.0.1      \r\nPyYAML               3.13       \r\npyzmq                17.1.2     \r\nqtconsole            4.4.3      \r\nscikit-image         0.14.1     \r\nscipy                1.1.0      \r\nSend2Trash           1.5.0      \r\nsetuptools           39.0.1     \r\nShapely              1.6.4.post2\r\nsix                  1.12.0     \r\ntensorboard          1.12.0     \r\ntensorflow           1.12.0rc0  \r\ntensorflow-estimator 1.10.12    \r\ntensorflow-plot      0.3.0.dev0 \r\ntermcolor            1.1.0      \r\nterminado            0.8.1      \r\ntestpath             0.4.2      \r\ntoolz                0.9.0      \r\ntornado              5.1.1      \r\ntraitlets            4.3.2      \r\nwcwidth              0.1.7      \r\nwebencodings         0.5.1      \r\nWerkzeug             0.14.1     \r\nwheel                0.32.3     \r\nwidgetsnbextension   3.4.2 \r\n\r\n** python3 --version\r\nPython 3.7.2\r\n\r\n** lsb_release -a\r\nDescription:\tUbuntu 16.04.5 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n\r\n** lspci | grep VGA\r\n01:00.0 VGA compatible controller: NVIDIA Corporation Device 1b02 (rev a1)\r\n02:00.0 VGA compatible controller: NVIDIA Corporation Device 1b02 (rev a1)\r\n \r\n** VGAs detected by TF\r\n2019-02-14 13:56:21.125443: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1\r\n2019-02-14 13:56:21.125447: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): TITAN Xp, Compute Capability 6.1\r\n2019-02-14 13:56:21.125746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1431] Found device 0 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.91GiB freeMemory: 11.15GiB\r\n2019-02-14 13:56:21.125898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1431] Found device 1 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.91GiB freeMemory: 11.75GiB\r\n\r\n** python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'v1.12.0-rc0-4136-g840be30' 1.12.0-rc0\r\n\r\n** cat /usr/local/cuda-10.0/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 4\r\n#define CUDNN_PATCHLEVEL 1\r\n--\r\n#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n#include \"driver_types.h\"\r\n\r\n**  nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n\r\n\r\n\r\n\r\nAfter couple of iterations within the first epoch of training I get the error \"E tensorflow/stream_executor/cuda/cuda_dnn.cc:82] The primary convolution algorithm failed memory allocation, while a secondary algorithm is not provided\" then my loss function becomes \"nan\". ", "comments": []}, {"number": 25756, "title": "Updated export_test.cc", "body": "Fixed the typo error", "comments": []}, {"number": 25755, "title": "Updated export.cc", "body": "Fixed some Typo errors", "comments": []}, {"number": 25754, "title": "Model.fit() leaves training arg and learning_phase unset (TF2-preview)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION='2.0.0-dev20190213'\r\nGIT_VERSION=\"v1.12.0-7959-gabeb4d0acd\"\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen training a Keras model containing a custom layer whose behavior depends on the training phase, `training` is not set by the `fit()` method, and `K.learning_phase()` is `0`. I can work around this issue by calling `K.set_learning_phase(1)` before creating the model, but oddly this does not work if I call `K.set_learning_phase(1)` _after_ creating and compiling the model, and just before calling the `fit()` method. It looks like the `learning_phase` is \"hard-coded\" into the call() method's graph when the model is built?\r\n\r\n**Describe the expected behavior**\r\nI expected the `training` argument to be set to `True` (or `1`) automatically when I called the `fit()` method. Alternatively, I expected that at least `K.learning_phase()` would return `True` (or `1`), but it's always `0`.\r\n\r\n**Code to reproduce the issue**\r\nIf you're in a hurry, just look at the failing tests: C, H and I.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nK = keras.backend\r\n\r\nclass MyLayer(keras.layers.Layer):\r\n    @tf.function\r\n    def call(self, inputs, training=None):\r\n        if training is None:\r\n            training = K.learning_phase()\r\n        tf.print(\"training: \", training)\r\n        return keras.backend.in_test_phase(inputs + 1., inputs + 2., training)\r\n\r\nX = np.zeros((1, 2))\r\n\r\nprint(\"Test A: MyLayer()(X)\")\r\ny_pred = MyLayer()(X)\r\nprint(\"Pred:\", y_pred)\r\nprint(\"training should be 0\\n\")\r\n\r\nprint(\"Test B: MyLayer()(X, training=True)\")\r\ny_pred = MyLayer()(X, training=True)\r\nprint(\"Pred:\", y_pred)\r\nprint(\"training should be True\\n\")\r\n\r\nprint(\"Test C: model = ...; model.compile(...); model.fit(...)\")\r\nmodel = keras.models.Sequential([MyLayer(input_shape=[2])])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nhistory = model.fit(X, X, epochs=2)\r\nprint(\"training should be 1 (used to fail, now passes)\\n\")\r\n\r\nprint(\"Test D: K.set_learning_phase(1); MyLayer()(X)\")\r\nK.set_learning_phase(1)\r\ny_pred = MyLayer()(X)\r\nprint(\"Pred:\", y_pred)\r\nprint(\"training should be 1\\n\")\r\nK.clear_session()\r\n\r\nprint(\"Test E: layer = MyLayer(); K.set_learning_phase(1); layer(X)\")\r\nlayer = MyLayer()\r\nK.set_learning_phase(1)\r\ny_pred = layer(X)\r\nprint(\"Pred:\", y_pred)\r\nprint(\"training should be 1\\n\")\r\nK.clear_session()\r\n\r\nprint(\"Test F: K.set_learning_phase(1); layer = MyLayer(); K.set_learning_phase(0); layer(X)\")\r\nK.set_learning_phase(1)\r\nlayer = MyLayer()\r\nK.set_learning_phase(0)\r\ny_pred = layer(X)\r\nprint(\"Pred:\", y_pred)\r\nprint(\"training should be 0\\n\")\r\nK.clear_session()\r\n\r\nprint(\"Test G: K.set_learning_phase(1); model = ...; model.compile(...); model.fit(...)\")\r\nK.set_learning_phase(1)\r\nmodel = keras.models.Sequential([MyLayer(input_shape=[2])])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nhistory = model.fit(X, X, epochs=2)\r\nprint(\"training should be 1\\n\")\r\nK.clear_session()\r\n\r\nprint(\"Test H: model = ...; model.compile(...); K.set_learning_phase(1); model.fit(...)\")\r\nmodel = keras.models.Sequential([MyLayer(input_shape=[2])])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nK.set_learning_phase(1)\r\nhistory = model.fit(X, X, epochs=2)\r\nprint(\"training should be 1 (ERROR?)\\n\")\r\nK.clear_session()\r\n\r\nprint(\"Test I: K.set_learning_phase(1); model = ...; K.set_learning_phase(0); model.compile(...); model.fit(...)\")\r\nK.set_learning_phase(1)\r\nmodel = keras.models.Sequential([MyLayer(input_shape=[2])])\r\nK.set_learning_phase(0)\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nhistory = model.fit(X, X, epochs=2)\r\nprint(\"This test does not make much sense, why would you call fit with learning phase 0?\\n\")\r\nK.clear_session()\r\n```", "comments": ["Ah, I think I got it: I'm using `@tf.function`, so the `call()` method gets traced, and at that point `K.learning_phase()` gets called and ~it turns out that it returns `0` as an integer, instead of a symbolic tensor, so this value gets burnt into the graph~ (edit: actually it does return a symbolic tensor, but it always evaluates to `0`). This would explain all 3 failing tests above.\r\n\r\n~Shouldn't `K.learning_phase()` return a symbolic tensor when running in graph mode?~ (edit: it does, but apparently, it is always `0`)", "@ageron Is this resolved? If yes, please close it. Thanks!", "Hi @jvishnuvardhan ,\r\nNo, it's not resolved, \"I think I got it\" just meant I thought I understood the issue, not that it was fixed, I'll be clearer next time. I think what's happening is that when `K.learning_phase()` is run in the context of a `@tf.function`, it returns a symbolic tensor that is not affected by the `fit()` method.", "FYI, @fchollet wrote :\"I think in this case tf.function is extracting the layer's call in a form where it is no longer dependent on the learning phase placeholder. The `fit` method sets the value of the placeholder, but by that time the tf.function is completely independent from it (and relies on the default value of 0).\"\r\nAnd: \"I have a fix for this issue, that should be reflected in the nightly build within a few days.\"", "Hi @fchollet, test C (which is the most important), now seems to work, but unfortunately not test H and test I. I'm using version 2.0.0-dev20190226 (git version v1.12.0-8969-g0ae3728b74).\r\n\r\nSide note: it would be nice if `keras.set_learning_phase(None)` could reset the learning phase to the default mode, so we wouldn't have to clear the whole session for that. Wdyt?", "Oops, I think the fix created a new issue (I can create a new issue if you prefer, but I think it's all about the same thing):\r\n\r\n```python\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nmodel = keras.models.Sequential([keras.layers.Dense(1)])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nmodel.fit(np.random.rand(1000, 1), np.random.rand(1000, 1))\r\n\r\nwith keras.backend.learning_phase_scope(1):\r\n    model.fit(np.random.rand(1000, 1), np.random.rand(1000, 1))\r\n```\r\n\r\nLast line raise this exception:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-b6b901d6926c> in <module>\r\n      7\r\n      8 with keras.backend.learning_phase_scope(1):\r\n----> 9     model.fit(np.random.rand(1000, 1), np.random.rand(1000, 1))\r\n     10\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874\r\n    875   def evaluate(self,\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    349\r\n    350         # Get outputs.\r\n--> 351         batch_outs = f(ins_batch)\r\n    352         if not isinstance(batch_outs, list):\r\n    353           batch_outs = [batch_outs]\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3215         value = math_ops.cast(value, tensor.dtype)\r\n   3216       converted_inputs.append(value)\r\n-> 3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n   3219                                  [x.numpy() for x in outputs])\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    524       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n    525           list(kwargs.keys()), list(self._arg_keywords)))\r\n--> 526     return self._call_flat(args)\r\n    527\r\n    528   def _filtered_call(self, args, kwargs):\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    593     # Only need to override the gradient in graph mode and when we have outputs.\r\n    594     if context.executing_eagerly() or not self.outputs:\r\n--> 595       outputs = self._inference_function.call(ctx, args)\r\n    596     else:\r\n    597       self._register_gradient()\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    363       raise ValueError(\r\n    364           \"Arguments and signature arguments do not match: %s %s \" %\r\n--> 365           (len(args), len(list(self.signature.input_arg))))\r\n    366\r\n    367     function_call_options = ctx.get_function_call_options()\r\n\r\nValueError: Arguments and signature arguments do not match: 7 8\r\n```\r\n\r\nI suppose this is because the first call to `fit()` creates a `FuncGraph` with 7 arguments, but the `learning_phase_scope()` adds an extra argument, so it fails.", "Hi @fchollet, unfortunately, right now the `training` argument seems broken (always `None`), using:\r\n\r\n```\r\ntf.version.VERSION=2.0.0-dev20190405\r\ntf.version.GIT_VERSION=v1.12.0-11808-ga1e3d4490d\r\n```\r\n\r\nBecause of the `learning_phase` issues, I am trying to use the `training` argument instead, but it is not set by the `fit()` method, as shown in the following code:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass MyLayer(keras.layers.Layer):\r\n    def call(self, inputs, training=None):\r\n        print(\"(tracing) training =\", training)\r\n        tf.print(\"(running) training =\", training)\r\n        return inputs + 1.\r\n\r\nmodel = keras.models.Sequential([MyLayer()])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nX_train, Y_train, X_valid, Y_valid = np.random.rand(4, 100, 2)\r\nmodel.fit(X_train, Y_train, validation_data=(X_valid, Y_valid))\r\nY_pred = model.predict(X_valid)\r\n```\r\n\r\nThis code displays:\r\n\r\n```\r\n(tracing) training = None\r\n...\r\n(running) training = None\r\n...\r\n(running) training = None\r\n(running) training = None\r\n(running) training = None\r\n...\r\n```\r\n\r\nThis is true even if I decorate the `call()` method with `@tf.function`.", "Any updates on this?", "@ageron, @gijswobben , are you still seeing this issue? There was a commit https://github.com/tensorflow/tensorflow/commit/cf31e9001c5ab89fc2e92eb98443d48cd37a726b#diff-8bfbe586f2ba61d9aa9e39676c7f69ee which may have fixed this issue. Can you try on the latest nightly release?", "Hi @pavithrasv ,\r\nI just tested again, unfortunately there are still some issues:\r\n\r\n* The good news is that the `training` argument now works as expected.\r\n* But the `learning_phase` seems to be always equal to 0.  I think TF2 should get rid of global scopes anyway, and this includes `K.learning_phase`, so I would recommend just deprecating `learning_phase` and `learning_phase_scope` altogether. However, it's part of the Keras API.\r\n\r\nHere's a little test that highlights what works and what doesn't:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nK = keras.backend\r\n\r\nX = np.zeros((1, 2))\r\n\r\nclass MyLayer(keras.layers.Layer):\r\n    @tf.function\r\n    def call(self, inputs, training=None):\r\n        tf.print(\"training: \", training)\r\n        tf.print(\"K.learning_phase(): \", K.learning_phase())\r\n        return keras.backend.in_test_phase(inputs + 1., inputs + 2., training)\r\n\r\nmodel = keras.models.Sequential([MyLayer(input_shape=[2])])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nprint(\"_\" * 80)\r\nprint(\">>> model.fit(...)\")\r\nhistory = model.fit(X, X, epochs=2)\r\nprint(\"_\" * 80)\r\nprint(\">>> model(X)\")\r\nprint(\"=>\", model(X))\r\nprint(\"_\" * 80)\r\nprint(\">>> model(X, training=True)\")\r\nprint(\"=>\", model(X, training=True))\r\nprint(\"_\" * 80)\r\nprint(\">>> model(X, training=False)\")\r\nprint(\"=>\", model(X, training=False))\r\nprint(\"_\" * 80)\r\nprint(\">>> with K.learning_phase_scope(1): model(X)\")\r\nwith K.learning_phase_scope(1):\r\n    print(\"=>\", model(X))\r\n```\r\n\r\nIt outputs this:\r\n\r\n```\r\n________________________________________________________________________________\r\n>>> model.fit(...)\r\nEpoch 1/2\r\ntraining:  1\r\nK.learning_phase():  0\r\n1/1 [==============================] - 0s 25ms/sample - loss: 4.0000\r\nEpoch 2/2\r\ntraining:  1\r\nK.learning_phase():  0\r\n1/1 [==============================] - 0s 959us/sample - loss: 4.0000\r\n________________________________________________________________________________\r\n>>> model(X)\r\ntraining:  None\r\nK.learning_phase():  0\r\n=> tf.Tensor([[1. 1.]], shape=(1, 2), dtype=float64)\r\n________________________________________________________________________________\r\n>>> model(X, training=True)\r\ntraining:  True\r\nK.learning_phase():  0\r\n=> tf.Tensor([[2. 2.]], shape=(1, 2), dtype=float64)\r\n________________________________________________________________________________\r\n>>> model(X, training=False)\r\ntraining:  False\r\nK.learning_phase():  0\r\n=> tf.Tensor([[1. 1.]], shape=(1, 2), dtype=float64)\r\n________________________________________________________________________________\r\n>>> with K.learning_phase_scope(1): model(X)\r\ntraining:  None\r\nK.learning_phase():  0\r\n=> tf.Tensor([[1. 1.]], shape=(1, 2), dtype=float64)\r\n```", "When is this issue going to be fixed?", "Sorry about the delay. The issue here is because of the `@tf.function` decoration on call. It causes learning phase value to be cached. \r\n\r\nDecorating small code snippets using `tf.function` will lead to lot of performance overhead and is not recommended. Also, if you are training using `fit` this is not required. If you are training using custom training loops, we recommend decorating the training step function with `tf.function`.\r\n\r\nThank you!", "Closing this as the main questions have been addressed I think.Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25754\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25754\">No</a>\n"]}, {"number": 25753, "title": "Failed to load native tensorflow ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- **Windows 7**\r\n- **pip install tensorflow-gpu**\r\n- Python version: **3.6.6**\r\n- Installed using virtualenv? pip? conda?: **pip**\r\n- CUDA/cuDNN version: **CUDA 8.0 cuDNN 7.1**\r\n- GPU model and memory: **Model : Quadro 4000 , Memory : 18GB**\r\n\r\n\r\n\r\nInstalled using pip install tensorflow-gpu (No error, perfectly installed)\r\nWhile importing tensorflow , i got a error : **Failed to load native tensorflow .ImportError: DLL load failed: The specified module could not be found**\r\n\r\n\r\n\r\n", "comments": ["You need to update your cuda and cudnn versions.\r\nFor TF 1.12 you can install cuda 9.0 and\r\nfor TF 1.13.0-rc2 you can install cuda 10", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25752, "title": "Complex zero to the power zero is nan", "body": "This is a simple yet dramatic bug: \r\n0^0 does not return 1 for complex numbers  (left zero complex) !\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution : Linux Ubuntu 16.04 LTS\r\n- TensorFlow installed from binary : pip \r\n- TensorFlow version : b'v1.13.0-rc1-0-g54c5616308' 1.13.0-rc1\r\n- Python version: Python 3.5.2\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.pow(tf.complex(0., 0.), 0)` returns `nan + nan j`\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be `1 + 0j` as is returned by `np.complex(0., 0.)**0`\r\n\r\n**Code to reproduce the issue**\r\n\r\n`z = tf.complex(0., 0.);\r\nsess = tf.Session();\r\nsess.run( tf.pow(z, 0) )`\r\n\r\n\r\n", "comments": ["True for TF2.0 preview as well:\r\n\r\n```python\r\nIn [12]: tf.complex(0., 0.) ** 0\r\nOut[12]: <tf.Tensor: id=13, shape=(), dtype=complex64, numpy=(nan+nanj)>\r\n```", "My guess is that, `tf.pow(tf.complex(0., 0.), 0)` is casted to \r\n```\r\ntf.pow(tf.complex(0., 0.), tf.complex(0., 0.))\r\n```\r\n,  and C++'s `std::pow` is used, which results in  `nan + nan j`.\r\n\r\nThe fix may not be straightforward, as it likely may impact the existing APIs.", "@jvishnuvardhan I don't know very much about this. Can you find a more fitting assignee, please?", "@thevincentadam thanks for letting us know. I'll send out a fix, probably sometime next week.\r\n", "Hello, any progress on this issue?", "I'm running into the same problem with TensorFlow 2.0. What is the status of this issue?", "Issue is replicating with Tf 2.1 and tf-nightly\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/0a8985cbc57b9023cead7a0083c284c1/untitled441.ipynb) . Thanks", "Issue replicating with TF 2.2-rc3. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/02bc714d61ad2f4b3003c29ef19222de/untitled816.ipynb).Thanks!", "TensorFlow calls Eigen. Eigen calls std::pow. std::pow(0, 0) is implementation defined. \r\nhttps://en.cppreference.com/w/cpp/numeric/complex/pow\r\n\r\nTo elaborate, mathematically z^0 is not well defined for |z| = 0 because the angle of z is indeterminate. It may be convenient to choose to follow the pattern for |z| > 0, for which z^0 is 1 + 0i, but it is an implementation choice.\r\n\r\nPlease feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25752\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25752\">No</a>\n"]}, {"number": 25751, "title": "[question][tflite][java] How to allocate mulit dimensional output array to feed in interpreter.run()", "body": "I try to run a version `posenet `on an android app with tflite.\r\nThe app is based on the GPU delegate demo:\r\nhttps://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo\r\n\r\nPosenet takes an Image as Input and computes as output multiple arrays of the shape:\r\n`1x14x14x17,  1x14x14x34, 1x14x14x32,  1x14x14x32`\r\n\r\nI know how to allocate a bytbuffer for the image, so thats not the problem.\r\nBut how do i allocate a buffer for that output to be able to successfully feed the input and output buffer to the interpreter like:\r\n```\r\nimport org.tensorflow.lite.Interpreter;\r\nInterpreter tflite;\r\n\r\nByteBuffer input = null;\r\ninput = ByteBuffer.allocateDirect(...);\r\noutput = ?\r\n\r\ntflite.run(input,output);\r\n```\r\n\r\nI tried something like this for the float version:\r\n```\r\nfloat[][][][] output = null;\r\noutput = new float[1*14*14*17][1*14*14*34][1*14*14*32][1*14*14*32];\r\n```\r\nbut this leads to a memory oom. So how do I correctly allocate a buffe for the output with an array with the right dimensions. (I am not so used to java, more to python)\r\n\r\nEDIT:\r\nand I also tried something like this, but java does not allow it:\r\n```\r\nfloat[][][][] out1 = new float[1][14][14][17];\r\nfloat[][][][] out2 = new float[1][14][14][34];\r\nfloat[][][][] out3 = new float[1][14][14][32];\r\nfloat[][][][] out4 = new float[1][14][14][32];\r\nfloat[] output = new float[out1, out2, out3, out4];\r\n```", "comments": ["For models with multiple inputs, or multiple outputs, use: `interpreter.runForMultipleInputsOutputs(inputs, map_of_indices_to_outputs);` rather than `interpreter.run()`. See my dirty benchmark [code](https://github.com/freedomtan/glDelegateBench/blob/master/app/src/main/java/com/mediatek/gldelegatebench/MainActivity.java#L209-L212) for GPU delegate. ", "@freedomtan Your code works for your benchmark app, but not for mine with camera image and not dummy data.\r\nAlso I do not understand what you are doing here:\r\n`int oShape[] = {1 * 23 * 17 * 17 * 4, 1 * 23 * 17 * 34 * 4, 1 * 23 * 17 * 64 * 4, 1 * 23 * 17 * 1 * 4};`\r\nWhy are you multiplying the dimensions?\r\n\r\nAnother Question, how can you access the computed output of the model?\r\nI need it to feed it the Multi Pose decoding algorithm.", "Multiplication is used to calculate memory size needed for ByteBuffer, e.g., for (1, 23, 17, 17) floating point tensor, we need 1x23x23x17x4 bytes. Why ByteBuffer? It's supposed to be much faster than array of objects (considering object serialization/deserialization overhead).\r\n\r\nHow to get outputs? After calling `interpreter.runForMultipleInputsOutputs()`, use\r\n[`getOutputTensor()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L332-L340) to get output tensors.\r\n", "@freedomtan alright makes sense.\r\nBut If I define as output buffer a BytBuffer, do I get the same object back for getOutputTensor() if I define the output buffer as a generic multi dimensional array?\r\n\r\nI ask because I need to apply the multi person decoding algorithm on it, and for my feeling its easier on generic arrays as i am not to familiar with javas tensorflow library.\r\n\r\nbtw is it possible to cast an tensor into an array, with somehting like this:\r\n```\r\noutTensor = getOutputTensor();\r\nfloat[][] outArray = new float[?][?];\r\noutArray = outTensor.copyTo(outArray);\r\n```\r\n\r\nAlso I saw that you were dealing with a problem i also have here:\r\nhttps://github.com/tensorflow/tensorflow/issues/22535\r\nDid you get android to run a model you quantized your own witj toco / tflite_convert ?", "Surely you can use multi-dimension array instead of ByteBuffer. As far as I can remember, the `Tensor` class doesn't provide public methods to access data, you still have to deal with original data. If you use ByteBuffer, `interpreter.runForMultipleInputsOutputs()` will pass it thru JNI. If you use other kind of array of objects, they will be copied to a ByteBuffer and the result will copied back to the array of objects you provide.\r\n\r\nAnd as far as I can remember, you can cast ByteBuffer to FloatBuffer (`aByteBuffer.asFloatBuffer()`), and get a float array from FloatBuffer (`aFloatBuffer.arrary()`).\r\n\r\nFor qunatized model, type of tensor are supposed to be UINT8, you may want inspect types of your input and output tensors.\r\n", "Usingf an java multidimensional array does not work in this case as something like the following does not work (do you know another way to create such an array?):\r\n```\r\nfloat[][][][] out1 = new float[1][14][14][17];\r\nfloat[][][][] out2 = new float[1][14][14][34];\r\nfloat[][][][] out3 = new float[1][14][14][32];\r\nfloat[][][][] out4 = new float[1][14][14][32];\r\nfloat[] outputs = new float[out1, out2, out3, out4];\r\n```\r\nSo when using your `Map<Integer, Object> allocateOutputBuffers(int[] shapes)`  function approach, after running the tflite interpreter with `interpreter.runForMultipleInputsOutputs()` how can i access each (of the four) output tensors and cast them to an java multidimensional arrays?\r\n`.asFloatBuffer()` or `.arrary()` is not callable on this.\r\n\r\n`interpreter.getOutputTensor(i)` returns `org.tensorflow.lite.Tensors` which I dont know how to convert and \r\n\r\n`outputs.get(i)` returns  `java.nio.HeapByteBuffers`, but how to convert them to arrays to apply mathematical operations on them? \r\n\r\nIT is not possible to call `.asFloatBuffer()` on  `outputs.get(i)`, although in the documentation it says `.asFloatBuffer()` is a method of `HeapByteBuffer` and `outputs.get(i)` is a `HeapByteBuffer` but Android studio claims it is just a `java.lang.Object`.\r\nDo you understand my problem?\r\n\r\nThank you very much for your help", "I guess what you want is something like\r\n```\r\nMap<Integer, Object> outputs = new HashMap<>();\r\noutputs.put(0, out1)\r\noutputs.put(1, out2)\r\noutputs.put(2, out3)\r\noutputs.put(3, out4)\r\n```\r\nThen, no casting is needed. Just a bit slow.\r\n\r\nYou may want to check [source code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L252-L276)  of `interpreter.runForMultipleInputsOutputs()`", "yeah I also came up with this solution. \r\nI guess for my needs there is currently no better way than this, as dealing with tflite.getOutputTensors() and also the HeapByteBuffer does not really work for me.", "And if i want to input multiple inputs ? How to convert 2 TensorBuffers to Objects and pass to runForMultipleInputsOutputs ?"]}, {"number": 25750, "title": "Updated group_bidirectional_sequence_ops.cc", "body": "", "comments": []}, {"number": 25749, "title": "TF Lite stride_slice begin/end/strided supports int64", "body": "This PR supports stride_slice ops begin/end/strided params with int64 type.", "comments": ["@Dayananda-V Any update please ?", "@Dayananda-V could you please resolve the conflicts ?", "@Dayananda-V gentle ping to resolve the conflicts.", "@gbaned \r\n\r\nSorry for delay and closing this PR due to write perssion denied, same changes will be track with new PR #28591. TIA"]}, {"number": 25748, "title": "[tflite] export SetNumThreads to TFLite Python API", "body": "export SetNumThreads() and add an option to set the number of threads in the `label_image.py` for TFLite. On [PYNQ-Z1](https://www.xilinx.com/products/boards-and-kits/1-hydd4z.html), a board with 2xCA9,\r\n```\r\n$ python tensorflow/lite/examples/python/label_image.py --num_threads 1\r\n0.415686: 653:military uniform\r\n0.352941: 907:Windsor tie\r\n0.058824: 668:mortarboard\r\n0.035294: 458:bow tie, bow-tie, bowtie\r\n0.035294: 835:suit, suit of clothes\r\ntime:  0.691580057144\r\n\r\n$ python tensorflow/lite/examples/python/label_image.py --num_threads 2\r\n0.415686: 653:military uniform\r\n0.352941: 907:Windsor tie\r\n0.058824: 668:mortarboard\r\n0.035294: 458:bow tie, bow-tie, bowtie\r\n0.035294: 835:suit, suit of clothes\r\ntime:  0.412344932556\r\n```", "comments": ["added @miaout17 to the reviewer list since it involves interpreter change.", "Mine built well, but I get an object has no attribute\r\nTraceback (most recent call last):\r\n  File \"./label_image.py\", line 74, in <module>\r\n    interpreter.set_num_threads(int(args.num_threads))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter.py\", line 260, in set_num_threads\r\n    return self._interpreter.SetNumThreads(i)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 97, in <lambda>\r\n    __getattr__ = lambda self, name: _swig_getattr(self, InterpreterWrapper, name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 74, in _swig_getattr\r\n    return _swig_getattr_nondynamic(self, class_type, name, 0)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 69, in _swig_getattr_nondynamic\r\n    return object.__getattr__(self, name)\r\nAttributeError: type object 'object' has no attribute '__getattr__'\r\n", "@jdinkel88 maybe you forgot to install the newly built pip wheel (remember to `pip3 install -U YOUR_NEWLY_BUILT_PIP_WHEEL`) or ran a wrong python interpreter (make sure that the python interpreter uses the newly installed one)?", "@petewarden can you review this? RPI right now runs the apps using the python binding on single core :(", "\ud83d\udc4d ", "@miaout17 can you help review this? it seems some guys using RPI really want this.", "@miaout17 is there anyone else you can nominate as reviewer in case you don't have cycles?", "@jdduke any more changes required?", "I defer to @miaout17 for final review, but in general looks good.", "@jdduke anyone else other than @miaout17 can review this? Running tflite single threaded on CPUs with multiple cores is quite constraining.", "@jdduke any insight into what remains to get this merged? Not sure why CI is failing.", "It hit a snag in our submit pipeline due to a lint issue. Working on resolving now, sorry for the delay. Should get it in for the upcoming release.", "@jdduke when is this going to be merged any idea?", "Looks like it got hung up on one of our import scripts, will try to force it through today. Thanks for your patience.", "@jdduke build still broken ?", "We're having some internal discussion about whether this should be a mutable property of the Interpreter or if it should be a constructor arg. If you have a strong opinion either way let us know, we're trying to move away from mutable properties as they complicate the codebase and runtiime (i.e., we're considering deprecating the native SetNumThreads API).", "@jdduke Agree that immutable properties make life easier in most cases. I used `SetNumberThreads` because I didn't find other API.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25748) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25748) for more info**.\n\n<!-- ok -->", "Can one of the admins verify this patch?", "@freedomtan can you please resolve conflicts.", "@jdduke can you please review new changes ?", "Defer to @miaout17 for approval.", "We test this multi-threading whl on the label_image example and it works, but we test it on the lstm or rnn examples and it doesn't work. Can you tell me why it is? Thanks.", "@duxiaochao First, rnn and lstm are fundamentally hard to parallelize. Second, as far as I know, only few kernels in TFLite are multithreaded.", "Hello guys,\r\nWhen can this patch be merged? Now testing with Python is too slow.", "@rthadur and @gbaned I don't know why there is \"cla/google\" problem. What can I do?", "@freedomtan Can you please resolve conflicts? Thanks!", "@jdduke can you please review new changes ?", "Hi! Thank you for very useful feature. It helps me to increase ssd-mobilenet performance 4 times on Raspberry Pi 4.\r\n\r\nI'm tried to build tflite from this branch and to use this feature and it seems that `SetNumThreads` defenition for pybind11 is missed. Here is a patch with which I was able to fix the problem.\r\n[0001-Add-SetNumThreads-defenition-for-pybind11.txt](https://github.com/tensorflow/tensorflow/files/4473197/0001-Add-SetNumThreads-defenition-for-pybind11.txt)\r\n\r\n", "@Mehanik Thanks. Yes, TensorFlow switched from swig to pybind11. I'll rebase and apply your patch.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25748) for more info**.\n\n<!-- need_author_cla -->", "@Mehanik it seems you need to sign CLA.", "@Mehanik are you going to sign the CLA? Or, can I reformat the patch by removing you from the Author and putting your name in the git commit comment?", "@freedomtan sorry for the long wait. I'll try to sign the CLA today", "@googlebot I fixed it..", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25748) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25748) for more info**.\n\n<!-- ok -->", "@jdduke gentle ping if we need more changes here.", "I can add the tests internally, so that we don't have to reimport the changes. Will do it by tomorrow and get this in.", "How can I make a pip package to test it on RPI 4?", "@metanav Would you try this? https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package"]}, {"number": 25747, "title": "failed to \"bazel build -c opt //tensorflow:libtensorflow.so\"", "body": "**System information**\r\n- OS Platform and Distribution (centos 7):\r\n- TensorFlow installed from (source ):\r\n- TensorFlow version: master\r\n- Python-2.7.5-76.el7.x86_64\r\n- swig-2.0.10-5.el7.x86_64\r\n- Bazel version (0.22):\r\n- GPU model and memory:No\r\n\r\nI followed \"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go/README.md\"\r\n1) install bazel\r\n       bazel version: \r\n            INFO: Invocation ID: cbd6d19b-6c11-4652-a9c0-ffa1b57e10d0\r\n            Build label: 0.22.0- (@non-git)\r\n            Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel \r\n                       /BazelServer_deploy.jar\r\n            Build time: Tue Jan 29 18:22:45 2019 (1548786165)\r\n            Build timestamp: 1548786165\r\n            Build timestamp as int: 1548786165\r\n\r\n2) sudo yum install python swig python-numpy \r\n3) go get -d github.com/tensorflow/tensorflow/tensorflow/go\r\n4) cd ${GOPATH}/src/github.com/tensorflow/tensorflow\r\n   ./configure\r\n     I press enter all the time.\r\n5)  bazel build -c opt //tensorflow:libtensorflow.so \r\n    step 5 failed, the error info:\r\n\r\nINFO: Invocation ID: 95b3b7e1-bdf2-4141-b259-38569d76b255\r\nERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\r\n\tFile \"/home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 163\r\n\t\t_clone_or_update(ctx)\r\n\tFile \"/home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 73, in _clone_or_update\r\n\t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning build_bazel_rules_swift:\r\n+ cd /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external\r\n+ rm -rf /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift\r\n+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift\r\n+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift reset --hard tags/0.6.0\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\nERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\r\n\tFile \"/home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 163\r\n\t\t_clone_or_update(ctx)\r\n\tFile \"/home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 73, in _clone_or_update\r\n\t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning build_bazel_rules_swift:\r\n+ cd /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external\r\n+ rm -rf /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift\r\n+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift\r\n+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift reset --hard tags/0.6.0\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\nINFO: Elapsed time: 5.616s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    Fetching ; Cloning tags/0.6.0 of https://github.com/bazelbuild/rules_swift\\\r\n.git 5s\r\n", "comments": ["I met the same problem when building source from master branch using bazel 0.20.0.", "I faced the same issue when building source from master with bazel 0.22.0 on Windows via cygwin.", "I can build successfully using  bazel 0.19.2 \r\n\r\n> I met the same problem when building source from master branch using bazel 0.20.0.\r\n\r\n", "> I faced the same issue when building source from master with bazel 0.22.0 on Windows via cygwin.\r\n\r\nMe too.", "Looks like you may need to install or update git?", "Identical problem,\r\n$ uname -a\r\nLinux xxxx 3.10.0-862.6.3.el7.x86_64 #1 SMP Fri Jun 15 17:57:37 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n$ bazel version\r\nINFO: Invocation ID: 4e446e3b-c921-4d82-a9f3-579c2424e8aa\r\nBuild label: 0.22.0- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Feb 5 15:23:19 2019 (1549380199)\r\nBuild timestamp: 1549380199\r\nBuild timestamp as int: 1549380199\r\n\r\n$ git --version\r\ngit version 1.8.3.1\r\n\r\n$ which java\r\nxxxxx/java/1.8.0_131/bin/java\r\n\r\nThis is the result:\r\n$ bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Invocation ID: a578d7c7-61ae-4f26-aaed-9eb22c0c9fa5\r\nERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\r\n        File \"xxxxxx/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 163\r\n.... and lots of similar\r\n\r\n\r\n\r\n", "I can confirm that the issue can be resolved by installing or upgrading git and also making sure that msys2 usr/bin path is in the PATH variable.", "in the [workspace file](https://github.com/tensorflow/tensorflow/blob/b0022ada8fd3ba6ec1c359e5d4e374b4632d70aa/WORKSPACE) of tensorflow, they added apple_rule but not swift_rule\r\n\r\nI am trying to make a [PR](https://github.com/tensorflow/tensorflow/pull/26079) to solve this issue ", "CC @temrich", "Explicitly adding Swift rules to the WORKSPACE file should not be needed. Swift rules are pulled in via Apple rules: https://github.com/bazelbuild/rules_apple/blob/master/apple/repositories.bzl#L115. Also see: #25735.", "Does it mean that we need to use latest version of bazel?\r\n\r\nIf so please specify which one will be compatible on swift rules.", "Yes, I believe 0.21 and 0.22 (latest version) will work.", "Using bazel 0.22 does not fix the problem:\n\n$ bazel version\nINFO: Invocation ID: 73f4ed34-0d92-4e5b-932f-83bfd59148dc\nBuild label: 0.22.0- (@non-git)\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Feb 5 15:23:19 2019 (1549380199)\nBuild timestamp: 1549380199\nBuild timestamp as int: 1549380199\n\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nStarting local Bazel server and connecting to it...\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\nINFO: Invocation ID: 9a3ac723-3708-4261-9a45-667846ce1175\nERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 163\n         _clone_or_update(ctx)\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 73, in _clone_or_update\n         fail((\"error cloning %s:\\n%s\" % (ctx....)))\nerror cloning build_bazel_rules_swift:\n+ cd /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external\n+ rm -rf /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift reset --hard tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\nERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 163\n         _clone_or_update(ctx)\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 73, in _clone_or_update\n         fail((\"error cloning %s:\\n%s\" % (ctx....)))\nerror cloning build_bazel_rules_swift:\n+ cd /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external\n+ rm -rf /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift reset --hard tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\nINFO: Elapsed time: 7.586s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (0 packages loaded)\n    Fetching @build_bazel_rules_swift; Cloning tags/0.6.0 of https://github.com/bazelbuild/rules_swift.git 4s\n\n\nIf you tear a hole in a net you have fewer holes\n\nScott Teige PhD\nDeep Learning\nIndiana University Research Technologies\nsteige@iu.edu\nOffice: +1 812 856 7331\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> On Feb 25, 2019, at 7:19 PM, temrich <notifications@github.com> wrote:\n> \n> Yes, I believe 0.21 and 0.22 (latest version) will work.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/25747#issuecomment-467238497>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKfDqbVwIuaaRTUatG05a7n4yY6GdW2Xks5vRH2cgaJpZM4a7BtB>.\n> \n\n", "Using bazel 0.22 does not fix the problem:\n\n$ bazel version\nINFO: Invocation ID: 73f4ed34-0d92-4e5b-932f-83bfd59148dc\nBuild label: 0.22.0- (@non-git)\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Feb 5 15:23:19 2019 (1549380199)\nBuild timestamp: 1549380199\nBuild timestamp as int: 1549380199\n\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nStarting local Bazel server and connecting to it...\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\nINFO: Invocation ID: 9a3ac723-3708-4261-9a45-667846ce1175\nERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 163\n         _clone_or_update(ctx)\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 73, in _clone_or_update\n         fail((\"error cloning %s:\\n%s\" % (ctx....)))\nerror cloning build_bazel_rules_swift:\n+ cd /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external\n+ rm -rf /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git <https://github.com/bazelbuild/rules_swift.git> /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift reset --hard tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\nERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 163\n         _clone_or_update(ctx)\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 73, in _clone_or_update\n         fail((\"error cloning %s:\\n%s\" % (ctx....)))\nerror cloning build_bazel_rules_swift:\n+ cd /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external\n+ rm -rf /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git <https://github.com/bazelbuild/rules_swift.git> /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift reset --hard tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\nINFO: Elapsed time: 7.586s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (0 packages loaded)\n    Fetching @build_bazel_rules_swift; Cloning tags/0.6.0 of https://github.com/bazelbuild/rules_swift.git <https://github.com/bazelbuild/rules_swift.git> 4s\n\n\nIf you tear a hole in a net you have fewer holes\n\nScott Teige PhD\nDeep Learning\nIndiana University Research Technologies\nsteige@iu.edu <mailto:steige@iu.edu>\nOffice: +1 812 856 7331\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> On Feb 25, 2019, at 7:19 PM, temrich <notifications@github.com <mailto:notifications@github.com>> wrote:\n> \n> Yes, I believe 0.21 and 0.22 (latest version) will work.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/25747#issuecomment-467238497>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKfDqbVwIuaaRTUatG05a7n4yY6GdW2Xks5vRH2cgaJpZM4a7BtB>.\n> \n\n", "@steige2 which platform are you building on? I am not seeing the issue on MacOS or Linux using Bazel 0.22.", "$ uname -a\nLinux e1.carbonate.xxx.yyy 3.10.0-862.6.3.el7.x86_64 #1 SMP Fri Jun 15 17:57:37 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux\n\nIf you tear a hole in a net you have fewer holes\n\nScott Teige PhD\nDeep Learning\nIndiana University Research Technologies\nsteige@iu.edu\nOffice: +1 812 856 7331\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> On Feb 26, 2019, at 12:54 PM, temrich <notifications@github.com> wrote:\n> \n> @steige2 <https://github.com/steige2> which platform are you building on? I am not seeing the issue on MacOS or Linux using Bazel 0.22.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/25747#issuecomment-467541054>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKfDqe-AmXRlE9AaiJTRg1-in2h4xuV5ks5vRXTSgaJpZM4a7BtB>.\n> \n\n", "Did you recently update to 0.22? If so, did you by any chance run `bazel clean --expunge`?", "I did not run that, but here is the result:\n\n$ /N/u/steige/Carbonate/bazel/output/bazel clean --expunge\nStarting local Bazel server and connecting to it...\nINFO: Invocation ID: acc77cce-a369-4b17-8937-38522ad26183\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\n\n$ /N/u/steige/Carbonate/bazel/output/bazel version\nINFO: Invocation ID: dd0798e7-73bc-4bd8-9958-7d9cc6e9e770\nBuild label: 0.22.0- (@non-git)\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Feb 5 15:23:19 2019 (1549380199)\nBuild timestamp: 1549380199\nBuild timestamp as int: 1549380199\n\n$ /N/u/steige/Carbonate/bazel/output/bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nStarting local Bazel server and connecting to it...\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\nINFO: Invocation ID: 829c5556-c0d9-4741-a4b3-d326c023a6f1\nERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 163\n                _clone_or_update(ctx)\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 73, in _clone_or_update\n                fail((\"error cloning %s:\\n%s\" % (ctx....)))\nerror cloning build_bazel_rules_swift:\n+ cd /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external\n+ rm -rf /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift reset --hard tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\nERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 163\n                _clone_or_update(ctx)\n        File \"/gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 73, in _clone_or_update\n                fail((\"error cloning %s:\\n%s\" % (ctx....)))\nerror cloning build_bazel_rules_swift:\n+ cd /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external\n+ rm -rf /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift reset --hard tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n+ git -C /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0\nUnknown option: -C\nusage: git [--version] [--help] [-c name=value]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\nINFO: Elapsed time: 9.320s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (0 packages loaded)\n    Fetching @build_bazel_rules_swift; Cloning tags/0.6.0 of https://github.com/bazelbuild/rules_swift.git\n\n\n\nIf you tear a hole in a net you have fewer holes\n\nScott Teige PhD\nDeep Learning\nIndiana University Research Technologies\nsteige@iu.edu\nOffice: +1 812 856 7331\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> On Feb 26, 2019, at 1:43 PM, temrich <notifications@github.com> wrote:\n> \n> Did you recently update to 0.22? If so, did you by any chance run bazel clean --expunge?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/25747#issuecomment-467559602>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKfDqWy-GSMovCjaCQAjBuEKtn7ypTvFks5vRYBRgaJpZM4a7BtB>.\n> \n\n", "Thank you for continuing to look into this! DO you know what \"Unknown option: -C\" is referring to in the log you provided? Appears to be an invalid git command.", "Hello,\nNo I\u2019m sorry I don\u2019t know where that option is coming from.\nI\u2019m happy to continue looking into this but the day is getting late\nfor me (I\u2019m in US eastern timezone) I\u2019ll be back at it tomorrow AM,\nabout 16 hrs from now.\nBest,\nS\n\nIf you tear a hole in a net you have fewer holes\n\nScott Teige PhD\nDeep Learning\nIndiana University Research Technologies\nsteige@iu.edu\nOffice: +1 812 856 7331\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> On Feb 26, 2019, at 3:41 PM, temrich <notifications@github.com> wrote:\n> \n> Thank you for continuing to look into this! DO you know what \"Unknown option: -C\" is referring to in the log you provided? Appears to be an invalid git command.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/25747#issuecomment-467603668>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKfDqav6dRTu0wEaP1LbB_pWBVFfOjCLks5vRZv7gaJpZM4a7BtB>.\n> \n\n", "It's because on centos there is an old version of git available. You need to remove git and install the 2.2.* version launching: \r\nsudo yum install  https://centos7.iuscommunity.org/ius-release.rpm\r\nsudo yum install  git2u-all\r\n\r\nSee [here](https://computingforgeeks.com/how-to-install-latest-version-of-git-git-2-x-on-centos-7/).\r\nThis way i solved the problem.", "I don not have sudo permission\nS\n\nIf you tear a hole in a net you have fewer holes\n\nScott Teige PhD\nDeep Learning\nIndiana University Research Technologies\nsteige@iu.edu\nOffice: +1 812 856 7331\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> On Feb 27, 2019, at 7:31 AM, andreacds3 <notifications@github.com> wrote:\n> \n> It's because on centos there is an old version of git available. You need to remove git and install the 2.2.* version launching:\n> sudo yum install https://centos7.iuscommunity.org/ius-release.rpm <https://centos7.iuscommunity.org/ius-release.rpm>\n> sudo yum install git2u-all\n> \n> See here <https://computingforgeeks.com/how-to-install-latest-version-of-git-git-2-x-on-centos-7/>.\n> This way i solved the problem.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/25747#issuecomment-467843749>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKfDqYguXKuBHe4VQwm2t_yz45SFAv4Uks5vRnq5gaJpZM4a7BtB>.\n> \n\n", "Hello,\nIt seems you do not need to remove the old git and a local install will do, so with\n$ git version\ngit version 2.13.0\nI now get considerably farther along but this kills progress:\n\u2026.\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (383 packages loaded, 19566 targets configured).\nINFO: Found 1 target...\nERROR: /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/gif_archive/BUILD.bazel:8:1: undeclared inclusion(s) in rule '@gif_archive//:gif':\nthis rule is missing dependency declarations for the following files included by 'external/gif_archive/lib/openbsd-reallocarray.c':\n  '/N/soft/rhel7/gcc/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/stddef.h'\n  '/N/soft/rhel7/gcc/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/stdint.h'\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 811.872s, Critical Path: 46.61s\nINFO: 20 processes: 20 local.\nFAILED: Build did NOT complete successfully\n\n\nIf you tear a hole in a net you have fewer holes\n\nScott Teige PhD\nDeep Learning\nIndiana University Research Technologies\nsteige@iu.edu\nOffice: +1 812 856 7331\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n> On Feb 27, 2019, at 8:03 AM, Scott Teige <steige@iu.edu> wrote:\n> \n> I don not have sudo permission\n> S\n> \n> If you tear a hole in a net you have fewer holes\n> \n> Scott Teige PhD\n> Deep Learning\n> Indiana University Research Technologies\n> steige@iu.edu <mailto:steige@iu.edu>\n> Office: +1 812 856 7331\n> \n> \n> \n> \n> \n> \n> \n> \n> \n> \n> \n> \n> \n> \n> \n> \n> \n>> On Feb 27, 2019, at 7:31 AM, andreacds3 <notifications@github.com <mailto:notifications@github.com>> wrote:\n>> \n>> It's because on centos there is an old version of git available. You need to remove git and install the 2.2.* version launching:\n>> sudo yum install https://centos7.iuscommunity.org/ius-release.rpm <https://centos7.iuscommunity.org/ius-release.rpm>\n>> sudo yum install git2u-all\n>> \n>> See here <https://computingforgeeks.com/how-to-install-latest-version-of-git-git-2-x-on-centos-7/>.\n>> This way i solved the problem.\n>> \n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/25747#issuecomment-467843749>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKfDqYguXKuBHe4VQwm2t_yz45SFAv4Uks5vRnq5gaJpZM4a7BtB>.\n>> \n> \n\n", "I have got the same problem. And in my system I need to remove old git @steige2:\r\n```\r\nyum remove git \r\n```\r\nThen follow @andreacds3 's BKM to install git 2.X\r\nThis fix the problem."]}, {"number": 25746, "title": "Object detection ppn tflite works wrong on mobile", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):N\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: HUAWEI MATE20\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.10\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):1.15\r\n- GCC/Compiler version (if compiling from source):1.9\r\n- CUDA/cuDNN version:8.0\r\n- GPU model and memory:16GB\r\n\r\nI used  object detection API, the ssd_mobilenetV2 is working well including the train, eval ,inference on pc ,and .tflite on phone.\r\nRecently, i try ppn_mobilenetv1, the train and eval is well. The frozen_inference_graph.pb also work well on pc and the results are right. When i convert it to tflite_graph.pb, then convert it to detect.tflite. No errors occur. But when it runs on the mobile, it shows totally wrong results. I can not find anything wrong here. Could anyone please help me find it out?\r\n\r\n", "comments": ["@holyhao Could you provide any code to reproduce the bug? or could you provide more detailed steps/commands you followed? Thanks!   ", "@jvishnuvardhan hi\uff0cthanks for your reply, to reproduce the bug\uff1a\r\n1.Download the ppn_mobilnetV1 http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03.tar.gz\r\n2.Conver it to tflite, no errors occur\r\n```\r\nCONFIG_PATH=../pipeline.config \r\nCHECKPOINT_PATH=../model.ckpt \r\nOUTPUT_DIR=../results/\t\r\n```\r\n```\r\npython object_detection/export_tflite_ssd_graph.py \\ \r\n--pipeline_config_path $CONFIG_PATH \\ \r\n--trained_checkpoint_prefix $CHECKPOINT_PATH \\ \r\n--output_directory $OUTPUT_DIR \\ \r\n--add_postprocessing_op=true\r\n```\r\n```\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\ \r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/detect.tflite \\ \r\n--input_shapes=1,300,300,3 \\ \r\n--input_arrays=normalized_input_image_tensor \\ \r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\ \r\n--inference_type=FLOAT \\ --allow_custom_ops\r\n```\r\n3. Run it on mobile, i use the tensorflow-lite from tensoflow 1.10\r\n\r\nAs I mentioned above, ssd_mobilenetV2 works well from the framework. The frozen_inference_graph.pb of ppn_mobilenetv1 works well on pc. But when the tf.lite of ppn_mobilenetv1  runs on the mobile, it shows totally wrong results. I Look through the export_tflite_ssd_graph.py and export_inference_graph.py . I wonder how it works well for ssd_mobilenetV2 and wrong for ppn_mobilenetv1 on mobile, and how it works well  for ppn_mobilenetv1 on pc and wrong on the mobile. Looking forward to your help.\r\n\r\n", "Hi @holyhao, are you still experiencing a problem with the model? Thanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25746\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25746\">No</a>\n"]}, {"number": 25745, "title": "TF Keras Negative number test case added for Relu activation", "body": "Negative number test case added for Relu activation", "comments": []}, {"number": 25744, "title": "Error when converting MTCNN(P_Net) pb model to tflite model", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.10.0\r\n- Python version: 2.7.6\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 4.8.4\r\n- CUDA/cuDNN version: * N/A (build without support CUDA)\r\n- GPU model and memory: * N/A (build without support GPU)\r\n\r\n\r\n\r\n**Describe the problem**\r\nI failed to covert my pb file to tflite format because opeator max was not supported. I find that maximum is a builtin operator and I want ask if operator max is the same as maximum ? Or I need to add a custom operator?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n\t--input_file=/home/zhengrui/tensorflow/pnet_144-192.pb \\\r\n\t--output_file=/home/zhengrui/tensorflow/pnet_144-192.tflite \\\r\n\t--input_format=TENSORFLOW_GRAPHDEF \\\r\n\t--output_format=TFLITE \\\r\n\t--inference_type=FLOAT  \\\r\n\t--input_shape='1,144,192,3' \\\r\n\t--input_array=pnet/input \\\r\n\t--output_arrays=pnet/conv4-2/BiasAdd,pnet/prob1  \r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n2019-02-14 14:33:08.816780: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 52 operators, 68 arrays (0 quantized)\r\n2019-02-14 14:33:08.817309: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 52 operators, 68 arrays (0 quantized)\r\n2019-02-14 14:33:08.818129: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 29 operators, 45 arrays (0 quantized)\r\n2019-02-14 14:33:08.818535: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 29 operators, 45 arrays (0 quantized)\r\n2019-02-14 14:33:08.818828: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 3237696 bytes, theoretical optimal value: 3237696 bytes.\r\n2019-02-14 14:33:08.818936: I tensorflow/contrib/lite/toco/toco_tooling.cc:388] Estimated count of arithmetic ops: 0.0936027 billion (note that a multiply-add is counted as 2 ops).\r\n2019-02-14 14:33:08.819150: F tensorflow/contrib/lite/toco/tflite/export.cc:363] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Max.\r\nAborted (core dumped)\r\n", "comments": ["I used tensorflow 1.12.0 and fixed this issue.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25744)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25744)\r\n"]}, {"number": 25743, "title": "where I get branch or tag for tensorflowlite+gpudelegate?", "body": "dear tensorflow team.\r\nI did not find any branch for tensorflow-lite+gpudelegate.\r\n\r\nCan you share me?\r\n\r\n", "comments": ["@ilous12 the source code has not been open sourced", "Stay tuned for updates on the open source release of the GPU delegate."]}, {"number": 25742, "title": "Leonard951 patch 1", "body": "Fixed the code for aarch64", "comments": ["duplicated https://github.com/tensorflow/tensorflow/pull/16175 and  https://github.com/tensorflow/tensorflow/pull/22856 ", "@leonard951 gentle ping to resolve conflicts", "It says \"Some checks were not successful. 11 failing, 3 pending, and 1 successful checks\". What can be done to pass these checks?\r\n", "I noticed that most of my proposed changes have already shown at the tip of the master branch except for the new one at https://github.com/tensorflow/tensorflow/pull/28911. So, I am closing this pull request. Please review #28911 and upstream it if it gets an approval after code review."]}, {"number": 25741, "title": "Update gpu_fusible.h", "body": "conumser -> consumer", "comments": []}, {"number": 25740, "title": "Update call_graph.h", "body": "contex -> context", "comments": []}, {"number": 25739, "title": "Update hlo_instruction.h", "body": "conssits -> consists", "comments": []}, {"number": 25738, "title": "Update tensor_handle.h", "body": "compuatation -> computation", "comments": []}, {"number": 25737, "title": "Update hlo_runner.h", "body": "bounary -> boundary", "comments": []}, {"number": 25736, "title": "Add swift dependencies back", "body": "", "comments": ["Looks like we are good with the `r1.13` branch and this is no longer needed since `swift` has been fully removed."]}, {"number": 25735, "title": "WIP - Remove left over swift rules", "body": "Looks like `swift` rules were accidentally deleted and now I'm getting the error:\r\n```\r\n[0m\u001b[91mERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\r\n\tFile \"/root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 166\r\n\t\t_clone_or_update(ctx)\r\n\tFile \"/root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 72, in _clone_or_update\r\n\t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning build_bazel_rules_swift:\r\n+ cd /root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external\r\n+ rm -rf /root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/build_bazel_rules_swift /root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/build_bazel_rules_swift\r\n+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git /root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/build_bazel_rules_swift\r\nCloning into '/root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/build_bazel_rules_swift'...\r\nfatal: unable to access 'https://github.com/bazelbuild/rules_swift.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\n+ git clone https://github.com/bazelbuild/rules_swift.git /root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/build_bazel_rules_swift\r\nCloning into '/root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/build_bazel_rules_swift'...\r\nfatal: unable to access 'https://github.com/bazelbuild/rules_swift.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\r\n\u001b[0m\u001b[91mERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):\r\n\tFile \"/root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 166\r\n\t\t_clone_or_update(ctx)\r\n\tFile \"/root/.cache/bazel/_bazel_root/c04143c934ba98544255b77f5f229b2f/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 72, in _clone_or_update\r\n\t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning build_bazel_rules_swift:\r\n```", "comments": ["\r\n@temrich has been working on this, not sure about the status.", "They were removed intentionally since they are pulled in via the Apple bazel rules: https://github.com/bazelbuild/rules_apple/blob/master/apple/repositories.bzl#L115\r\n\r\nSame for skylib:\r\nhttps://github.com/bazelbuild/rules_apple/blob/master/apple/repositories.bzl#L99", "Can you share the BUILD file / targets you are trying to build that triggers the error you are seeing?", "Actually it turns out that we attempted to remove `swift` but we didn't completely cleanup \ud83d\ude42 \r\nSo hence removing the leftover junk.", "Once this one is merged I'll update this one as well:\r\nhttps://github.com/tensorflow/tensorflow/pull/25736", "What's the reasoning for removing swift_rules_dependencies() in the WORKSPACE file? Can you provide some more details on how you are getting a build error and which platform you are building on? ", "The details are the actual `rules` have been removed but we are still attempting to load them in lines `57` and `58` that we are removing in this PR, otherwise loading them would give the above error.", "Can you sync your branch to master? The TensorFlow Lite Swift library depends on the Swift bazel deps when building on Apple platforms: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/BUILD.apple#L10", "@temrich I synced my branch with master.", "Sounds good, thank you! Assuming you are still seeing an error because of `swift_rules_dependencies()` in the WORKSPACE? If so, can you provide info for the following:\r\n\r\n1) Which version of Bazel are you using?\r\n2) Are you building on Mac or Linux (or some other platform)?\r\n3) Which command(s) are you running that triggers this error?\r\n\r\nI would like to try to repro this on my end. Thank you!", "@temrich I don't have access to a machine to reproduce again but possibly tomorrow I can post my config here.\r\nThanks.", "@temrich I was able to spend some time on this and we may not need PR after all, since I was able to build the images manually. It's possible that this is specific to our `CI` system.\r\nI'm going to mark this PR and `WIP` for now and if that turns out to be the case, I'll close this one and I'll come back with a new PR specific to `MKL` area.", "Hey Abolfazl, thank you for continuing to look into this. Please let me know if you run into anymore issues.", "Ok, looks like this is no longer needed and the issue was in our `CI` scripts.\r\nClosing the PR."]}]