[{"number": 20874, "title": "Error when using batch_norm with MirroredStrategy", "body": "### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): using nvidia containter: https://docs.nvidia.com/deeplearning/dgx/tensorflow-release-notes/rel_18.06.html#rel_18.06\r\nTensorFlow version (use command below):1.8.0\r\nPython version: 3.5\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: 9.0.176\r\nGPU model and memory: nvidia tesla v100\r\nExact command to reproduce: N/A\r\n\r\n### Describe the problem\r\nI want to apply transfer learning using an existing pretrained network (inception v4). For this I use the tf-slim models. When running this on a single GPU, This works as expected, however when using `tf.contrib.distribute.MirroredStrategy`  I get an exception. Apparently the MirroredStrategy has some issues with batch_norm\r\n\r\n### Source code / logs\r\nI tried to extract the relevant part from my code:\r\n```\r\nimport research.slim.nets.nets_factory as nets_factory\r\n\r\n...\r\n\r\ndef construct_architecture(self, input_tensor, mode):\r\n        # network topology\r\n        network_fn = nets_factory.get_network_fn('inception_v4', num_classes=self.configuration.get(\"nr_classes\"),\r\n                                                      is_training=True)\r\n        logits,_ = network_fn(input_tensor)\r\n\r\n        if mode == tf.estimator.ModeKeys.PREDICT:\r\n            predicted_classes = tf.argmax(logits, 1)\r\n            predictions = {\r\n                'class_ids': predicted_classes[:, tf.newaxis],\r\n                'probabilities': tf.nn.softmax(logits),\r\n                'logits': logits,\r\n            }\r\n            self.output_tensor = predictions\r\n        else:\r\n            self.output_tensor = logits\r\n```\r\n\r\nThis results in the following error trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 27, in <module>\r\n    experimenter.run_training_experiment(config)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 39, in run_training_experiment\r\n    tf.estimator.train_and_evaluate(self.trainer, self.training_specs, self.eval_specs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 451, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 590, in run\r\n    return self.run_local()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 691, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 376, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1143, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1255, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 777, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 308, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/onnx/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 519, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1133, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 111, in get_model_fn\r\n    self.configure_network(input_tensor=features, output_tensor=labels, mode=mode)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 75, in configure_network\r\n    self.network.construct_network(input_tensor=input_tensor, output_tensor=output_tensor, mode=mode)\r\n  File \"/media/local/myfiles/mynetwork.py\", line 64, in construct_network\r\n    self.construct_architecture(input_tensor=input_tensor,mode=mode)\r\n  File \"/media/local/myfiles/mynetwork.py\", line 48, in construct_architecture\r\n    logits,_ = network_fn(input_tensor)\r\n  File \"/opt/tf-slim/models/research/slim/nets/nets_factory.py\", line 141, in network_fn\r\n    return func(images, num_classes, is_training=is_training, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/slim-0.1-py3.5.egg/nets/inception_v4.py\", line 286, in inception_v4\r\n  File \"/usr/local/lib/python3.5/dist-packages/slim-0.1-py3.5.egg/nets/inception_v4.py\", line 178, in inception_v4_base\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1154, in convolution2d\r\n    conv_dims=2)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1066, in convolution\r\n    outputs = normalizer_fn(outputs, **normalizer_params)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 650, in batch_norm\r\n    outputs = layer.apply(inputs, training=is_training)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 805, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 362, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 736, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/normalization.py\", line 158, in call\r\n    return super(BatchNormalization, self).call(inputs, training=training)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py\", line 514, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py\", line 420, in _fused_batch_norm\r\n    momentum)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py\", line 369, in _assign_moving_average\r\n    with ops.colocate_with(variable):\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3939, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3992, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1255, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 414, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n```\r\n\r\n\r\nThanks for the help\r\n\r\nJonas", "comments": ["@yuefengz has a fix in progress relating to `contrib.layers` that might address this issue as well. He should be able to merge that in the a few days. After that you could try again and see if the issue still persists. ", "I had the same issues \r\n\r\nWARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f1b5a16a320>) includes params argument, but params are not passed to Estimator.\r\n2018-09-11 16:17:09.727266: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-11 16:17:09.728842: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\n2018-09-11 16:17:09.728874: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: mqq\r\n2018-09-11 16:17:09.728885: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: mqq\r\n2018-09-11 16:17:09.728914: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 384.81.0\r\n2018-09-11 16:17:09.728945: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 384.130.0\r\n2018-09-11 16:17:09.728955: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:308] kernel version 384.130.0 does not match DSO version 384.81.0 -- cannot find working devices in this configuration\r\nWARNING:tensorflow:Not all devices in distribute strategy are visible by TensorFlow sessions.\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From /data/guanlin/od_0730/object_detection/core/preprocessor.py:1205: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the `axis` argument instead\r\nWARNING:tensorflow:From /data/guanlin/od_0730/object_detection/builders/dataset_builder.py:146: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.batch(..., drop_remainder=True)`.\r\nTraceback (most recent call last):\r\n  File \"object_detection/model_main.py\", line 102, in <module>\r\n    tf.app.run()\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"object_detection/model_main.py\", line 98, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 451, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 590, in run\r\n    return self.run_local()\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 691, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 376, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1143, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1255, in _train_model_distributed\r\n    self.config)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/training/distribute.py\", line 777, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 308, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 519, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1133, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/data/guanlin/od_0730/object_detection/model_lib.py\", line 246, in model_fn\r\n    preprocessed_images, features[fields.InputDataFields.true_image_shape])\r\n  File \"/data/guanlin/od_0730/object_detection/meta_architectures/ssd_meta_arch.py\", line 393, in predict\r\n    preprocessed_inputs)\r\n  File \"/data/guanlin/od_0730/object_detection/models/ssd_inception_v2_feature_extractor.py\", line 118, in extract_features\r\n    scope=scope)\r\n  File \"/data/guanlin/od_0730/slim/nets/inception_v2.py\", line 117, in inception_v2_base\r\n    scope=end_point)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 2785, in separable_convolution2d\r\n    outputs = normalizer_fn(outputs, **normalizer_params)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 650, in batch_norm\r\n    outputs = layer.apply(inputs, training=is_training)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 805, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 362, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 736, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py\", line 158, in call\r\n    return super(BatchNormalization, self).call(inputs, training=training)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 514, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 420, in _fused_batch_norm\r\n    momentum)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 369, in _assign_moving_average\r\n    with ops.colocate_with(variable):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3939, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3992, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1255, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/mqq/tensorflow1.9/env/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 414, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n", "The fix for this is in r1.11, could you please test with that and let us know?", "Nagging Assignee @yuefengz: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am closing this issue for now. Feel free to reopen it or create a new one if the problem is still there.", "I'm seeing the same issue in 1.12.0 (from the `tensorflow/tensorflow:1.12.0-gpu` Docker image):\r\n\r\nMinimal example to reproduce:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint('tensorflow version: %s' % tf.__version__)\r\n\r\ndef input_fn():\r\n    return (\r\n        tf.data.Dataset.from_tensor_slices([0])\r\n        .map(lambda _: tf.random_uniform([1], 0, np.pi * 2))\r\n        .map(lambda x: (x, tf.sin(x)))\r\n        .repeat()\r\n        .batch(10)\r\n    )\r\n\r\ndef model_fn(features, labels, mode):\r\n    net = tf.layers.dense(features, units=20)\r\n    net = tf.nn.tanh(net)\r\n    net = tf.contrib.layers.batch_norm(net)\r\n    net = tf.layers.dense(net, units=20)\r\n    net = tf.nn.tanh(net)\r\n    output = tf.layers.dense(net, units=1)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        loss = tf.reduce_mean(tf.pow(output - labels, 2))\r\n        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(tf.estimator.ModeKeys.TRAIN, loss=loss, train_op=train_op)\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n\r\nestimator.train(input_fn=input_fn, steps=1000)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\ntensorflow version: 1.12.0\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpF_Z_3F\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f29a0297d90>, '_model_dir': '/tmp/tmpF_Z_3F', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f2a82dfe110>, '_master': '', '_distribute_coordinator_mode': None}\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Error reported to Coordinator: \r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"<ipython-input-114-26a55e49533e>\", line 19, in model_fn\r\n    net = tf.contrib.layers.batch_norm(net)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 182, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 596, in batch_norm\r\n    scope=scope)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 416, in _fused_batch_norm\r\n    is_training, _delay_updates, moving_vars_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/utils.py\", line 214, in smart_cond\r\n    return static_cond(pred_value, fn1, fn2)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/utils.py\", line 192, in static_cond\r\n    return fn1()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 410, in _delay_updates\r\n    moving_mean, mean, decay, zero_debias=zero_debias_moving_mean)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/moving_averages.py\", line 84, in assign_moving_average\r\n    with ops.colocate_with(variable):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n```", "As a workaround, `tf.keras.layers.BatchNormalization` appears to be working fine.", "> As a workaround, `tf.keras.layers.BatchNormalization` appears to be working fine.\r\n\r\nHi there, i am also facing the same problem with mirrored strategy, even though i am using tf1.12...\r\nHow did u modify the code to put the keras layer in ? (Non-Keras user here)", "@cheneeheng you can replace\r\n\r\n```python\r\nnet = tf.contrib.layers.batch_norm(net)\r\n```\r\n\r\nwith\r\n\r\n```python\r\nnet = tf.keras.layers.BatchNormalization()(net)\r\n```\r\n\r\nin the example above.", "I've replaced `tf.contrib.layers.batch_norm(net)` with `tf.keras.layers.BatchNormalization()(net)` but I'm seeing the same problem", "any update on the solution here ?", "I have the same problem", "Same problem here as well. tf version 1.12.", "@SirjanK - have you tested with the latest versions of TF? say TF 2.0 or TF 1.15? If the problem still persists, please re-open along with code to repro the issue", "The problem persists on TensorFlow 2.2.0-rc2 with Ubuntu 20.04.1 LTS with `tf.keras.layers.BatchNormalization()(net)` with the following error `ValueError: Tried to convert 'mean' to a tensor and failed. Error: Device assignment required for nccl collective ops`"]}, {"number": 20873, "title": "Will tensorflow support 'PMI' like process management protocol for large scale cluster mode? ", "body": "### System information\r\n\r\nnot related.\r\n\r\n### Describe the problem\r\n\r\nI'm  trying to use distributed tensorflow in a very large scale mode, say, more that 5000 nodes.\r\n\r\nAs the distributed tf using guide described in: https://www.tensorflow.org/deploy/distributed, I have to launch the python script by giving it a very long parameter list,  which is used to specify the endpoint of each node in the cluster including itself. just like:\r\n\r\n`python train.py --worker_hosts=\"ip1:port1,ip2:port2,...\" --ps_hosts=\"ip1:port1,ip2:port2,...\"` \r\n\r\nThis list will be very long in large scale mode, and may cause stack overflow. Because the stack used to store parameters is limited in size.\r\n\r\nHave the tf developers considered to add support for PMI (refer to: http://www.mcs.anl.gov/papers/P1760.pdf)? With PMI, we can use a \"tracker\" like role to manage all the endpoints. Each node has only to know the tracker endpoint when launching, the it can connect to the tracker to acquire endpoints of the other nodes.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The issue template, I think may be a very good tool to report a bug or a failure.\r\nBut the issue I proposed is a feature request, or abort the tf design plan. It seems that the issue template doesn't work well for me.\r\nSo can you give me some response? @asimshankar ", "@lilbedwin -  The [`ClusterSpec`](https://www.tensorflow.org/api_docs/python/tf/train/ClusterSpec) data structure can easily list out ~5000 worker addresses. So perhaps you don't want to use a flag to list them out and instead create the `ClusterSpec` message directly?\r\n\r\nFurthermore, instead of filling out the addresses explicitly, one could use a [`ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/contrib/cluster_resolver/ClusterResolver). There is an implementation for [Google Compute Engine](https://www.tensorflow.org/api_docs/python/tf/contrib/cluster_resolver/GceClusterResolver).\r\n\r\nFor different cluster management systems, one may have to create a different `ClusterResolver` implementation and that would be great.\r\n\r\nClosing this bug out since I believe the feature request reduces to adding implementations of `ClusterResolver`, which we'd encourage as simple third-party library or as contributions back to TensorFlow. Feel free to reopen if I'm mistaken.\r\n\r\nThanks.", "Cluster Resolver is great! I have another question.\r\n\r\nHow can Cluster Resolver work with Estimator? As I know, in the distributed mode of Estimator,  the user has to set a json formatted environment variable to the python process.  \r\n\r\nCan Cluster Resolver make things more graceful?  Or maybe Estimator will accept a Cluster Spec object as a parameter in the future?", "The environment variable is a convenience. You can explicitly provide the `ClusterSpec` from the `ClusterResolver` to the [`RunConfig`](https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig) like in this [TPU example](https://github.com/tensorflow/tpu/blob/62be0d7d2c880127c817b929dbde2c7fbadf87b1/models/official/squeezenet/squeezenet_main.py#L69).\r\n\r\nThat said, yes, this should become easier as the [`DistributionStrategy` API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute) matures and becomes ready.\r\n\r\nFYI @yuefengz ", "I checked the tpu example you provided. It seems that 'tf.estimator.RunConfig' could not take a parameter named 'cluster' whose type is 'ClusterResolver' like ' tf.contrib.tpu.RunConfig' does.\r\n\r\nDo you mean I should write some code to make 'tf.estimator.RunConfig' take a parameter named 'cluster' ? which may be similar to 'tf.contrib.tpu.RunConfig' and 'tf.contrib.tpu.TPUEstimator'.\r\n\r\nThanks.", "Currently `tf.estimator.RunConfig` doesn't have cluster as an argument. It parses `cluster_spec` from `\"TF_CONFIG\"` environment variable. Its [documentation](https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig#__init__) provides examples for how to set it.\r\nYou can get `cluster_spec` from `ClusterResolver` and set the environment variable.", "@yuefengz , Will 'tf.estimator.RunConfig' take a 'ClusterResolver' argument in the future? The environment variable method, as I have mentioned in the previous talks, may cause stack overflow in the case that the cluster is very large. Because there will be a very large 'TF_CONFIG' environment variable which contains all the endpoints, and it will occupy the stack space, but stack size is limited.\r\n\r\nThanks."]}, {"number": 20872, "title": "Cannot allocate memory for the interpreter", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: example source code and custom model\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:github master branch\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n\r\n### Describe the problem\r\n\r\nI'm trying to build & run tensorflow lite example using custom model.\r\nBuild is ok, but error occurs running this program.\r\nwhy this error occur?\r\n\r\n`root@90f212114f89:/tensorflow# bazel-bin/tensorflow/contrib/lite/examples/minimal/minimal /root/DNNSE/model/tflite_model.lite`\r\n`tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.`\r\n`tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.`\r\n`tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.`\r\n`tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.`\r\n`Node 8 failed to prepare.`\r\n\r\n`Error at tensorflow/contrib/lite/examples/minimal/minimal.cc:59`\r\n\r\nUsed model is custom model training on tensorflow and convert tensorflow lite using toco.\r\nI think some error in the process of converting model to lite.\r\nBut i don't know how to debugging it.\r\n\r\nI also try to run in android example.\r\nThere are same error.\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n                  Process: android.example.com.tflitecamerademo, PID: 13202\r\n                  java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.Node 8 failed to prepare.\r\n\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2875)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2950)\r\n                      at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1616)\r\n                      at android.os.Handler.dispatchMessage(Handler.java:105)\r\n                      at android.os.Looper.loop(Looper.java:164)\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6759)\r\n                      at java.lang.reflect.Method.invoke(Native Method)\r\n                      at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:770)\r\n                   Caused by: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.Node 8 failed to prepare.\r\n\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:75)\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:54)\r\n                      at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)\r\n                      at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:92)\r\n                      at com.example.android.tflitecamerademo.ImageClassifierQuantizedMobileNet.<init>(ImageClassifierQuantizedMobileNet.java:38)\r\n                      at com.example.android.tflitecamerademo.Camera2BasicFragment.onActivityCreated(Camera2BasicFragment.java:332)\r\n                      at android.app.Fragment.performActivityCreated(Fragment.java:2620)\r\n                      at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1296)\r\n                      at android.app.FragmentManagerImpl.addAddedFragments(FragmentManager.java:2415)\r\n                      at android.app.FragmentManagerImpl.executeOpsTogether(FragmentManager.java:2194)\r\n                      at android.app.FragmentManagerImpl.removeRedundantOperationsAndExecute(FragmentManager.java:2148)\r\n                      at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:2049)\r\n                      at android.app.FragmentManagerImpl.dispatchMoveToState(FragmentManager.java:3044)\r\n                      at android.app.FragmentManagerImpl.dispatchActivityCreated(FragmentManager.java:2991)\r\n                      at android.app.FragmentController.dispatchActivityCreated(FragmentController.java:178)\r\n                      at android.app.Activity.performCreateCommon(Activity.java:6974)\r\n                      at android.app.Activity.performCreate(Activity.java:6982)\r\n                      at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2828)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2950)\u00a0\r\n                      at android.app.ActivityThread.-wrap11(Unknown Source:0)\u00a0\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1616)\u00a0\r\n                      at android.os.Handler.dispatchMessage(Handler.java:105)\u00a0\r\n                      at android.os.Looper.loop(Looper.java:164)\u00a0\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6759)\u00a0\r\n                      at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n                      at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\u00a0\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:770)\u00a0\r\nApplication terminated.\r\n", "comments": ["@sukkyoung : On which device are you running this?", "@shashishekhar I used LG G7. (android ver.8.0.0)", "Ah, this is a confusing error message: no where related to memory.\r\n\r\ntensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true which means one of the dimensions of your tf.reduce_op is causing problems, can you attach your model or list which tf.reduce_* ops you are using. ", "@shashishekhar  Can I share only tf.reduce_* ops ??\r\nI used only tf.reduce_mean. (not use reduction_indices attribute)\r\n[tflite_model.tar.gz](https://github.com/tensorflow/tensorflow/files/2209507/tflite_model.tar.gz)\r\n", "@sukkyoung :\r\nIt seems the model has a bug, I am trying to see if there is a bug in our conversion script.\r\nCan you attach:\r\n1. The original graphDef or saved model that you used to convert the model.\r\n2.  The command/script that you used to convert the model.", "@shashishekhar \r\nI attach original graph and frozen graph using freeze_graph script\r\n[graph.tar.gz](https://github.com/tensorflow/tensorflow/files/2217777/graph.tar.gz)\r\n\r\n1. freeze graph\r\nbazel build tensorflow/python/tools:freeze_graph && \\\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n--input_graph=/root/DNNSE/model/graph.pb \\\r\n--input_checkpoint=/root/DNNSE/model/epoch_71.ckpt-71 \\\r\n--output_graph=/root/DNNSE/model/frozen_graph.pb \\\r\n--output_node_names=Mean\r\n\r\n2. toco\r\nbazel build tensorflow/contrib/lite/toco:toco && \\\r\ntoco --graph_def_file=/root/DNNSE/model/frozen_graph.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=/root/DNNSE/model/tflite_model.lite \\\r\n--inference_type=FLOAT \\\r\n--input_type=FLOAT \\\r\n--input_arrays=x \\\r\n--output_arrays=Mean \\\r\n--input_shapes=64,257", "@sukkyoung : It seems that your model has x and y as inputs, if you specify both x,y in input_arrays then it should work. ", "@shashishekhar  Thank you! it works well.", "@shashishekhar How do you specify the X and y in input_arrays? Do you list their sizes? Which command do you use to create the tf lite file. I am running into the following issue: \r\n\r\n`Caused by: com.google.firebase.ml.common.FirebaseMLException: Internal error has occurred when executing Firebase ML tasks\r\nW/System.err:     at com.google.android.gms.internal.firebase_ml.zzhg.zza(Unknown Source)\r\nW/System.err: \t... 5 more\r\nW/System.err: Caused by: java.lang.NullPointerException: Can not allocate memory for the interpreter\r\nW/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\nW/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\nW/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:51)\r\nW/System.err:     at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:90)\r\nW/System.err:     at com.google.android.gms.internal.firebase_ml.zzif.zzfm(Unknown Source)\r\nW/System.err:     at com.google.android.gms.internal.firebase_ml.zzhr.zzfp(Unknown Source)\r\nW/System.err:     at com.google.android.gms.internal.firebase_ml.zzhr.call(Unknown Source)\r\nW/System.err:     at com.google.android.gms.internal.firebase_ml.zzhg.zza(Unknown Source)\r\nW/System.err: \t... 5 more`\r\n\r\n"]}, {"number": 20871, "title": "TFLite -- Converting the trained pb file into tflite failed predictions on android", "body": "**Question description\uff1a**\r\n####  training a pb file, and then convert to tflite file, i use python testing pb file is correct ,but after into tflite file on android testing is wrong, I think this problem should be related to BN(tf.nn.batch_normalization),because when i remove BN get results of android and python are all the same, but with BN is different, the BN is tflite support,and the input data are the same, I don't know why?\r\n------------------------------------\r\n\r\n**Test Demo:**\r\n```\r\ndef save_to_pb(sess):\r\n    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['out'])\r\n    with tf.gfile.FastGFile(MODEL_DIR + 'expert-graph.pb', mode='wb') as f:\r\n        f.write(constant_graph.SerializeToString())\r\n\r\ndef read_data(session):\r\n    image_name = '182133.jpg'\r\n    TEST_IMAGINE_PATH = '/home/leve/lcr/ClelebA/Celebra_crop_20w+_w128_dataset/images/test_100/'\r\n    image = cv.imread(TEST_IMAGINE_PATH + image_name)\r\n    image = tf.reshape(image, [128, 128, 3])\r\n    image = image.eval(session=session)\r\n    image = image[np.newaxis, :]\r\n    return image\r\n\r\ndef batch_norm_lite(x, train=True, bn_decay=0.5,epsilon = 0.001,name='bn'):\r\n    is_training = tf.convert_to_tensor(train,dtype='bool',name='is_training')\r\n    x_shape = x.get_shape()\r\n    params_shape = x_shape[-1:]\r\n    axis = list(range(len(x_shape) - 1))\r\n    beta = tf.get_variable(name+'_beta', params_shape, initializer=tf.zeros_initializer())\r\n    gamma = tf.get_variable(name+'_gamma', params_shape, initializer=tf.ones_initializer())\r\n    moving_mean = tf.get_variable(name+'_moving_mean', params_shape, initializer=tf.zeros_initializer(), trainable=False)\r\n    moving_variance = tf.get_variable(name+'_moving_variance', params_shape, initializer=tf.ones_initializer(), trainable=False)\r\n    mean, variance = tf.nn.moments(x, axis)\r\n    update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, bn_decay)\r\n    update_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, bn_decay)\r\n    tf.add_to_collection(name+'_update_moving_mean', update_moving_mean)\r\n    tf.add_to_collection(name+'_update_moving_variance', update_moving_variance)\r\n    mean, variance = control_flow_ops.cond(\r\n        is_training, lambda: (mean, variance),\r\n        lambda: (moving_mean, moving_variance))\r\n\r\n    return tf.nn.batch_normalization(x, mean, variance, beta, gamma, epsilon,name=name)\r\n\r\ndef train_model():\r\n    img = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 128, 128, 3))\r\n    b = tf.Variable(tf.truncated_normal((1, 128, 128, 3), seed=1),name='w1')\r\n    y_real = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])\r\n    val = tf.add(img, b)\r\n    val = batch_norm_lite(val)\r\n    out = tf.identity(val, name=\"out\")\r\n    MSE = tf.reduce_mean(tf.square(y_real - out), name='mse')\r\n    train_step = tf.train.GradientDescentOptimizer(0.9).minimize(MSE)\r\n    saver = tf.train.Saver(max_to_keep=10)\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.per_process_gpu_memory_fraction = 0.1\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session() as sess:\r\n        sess.run(tf.initialize_all_variables())\r\n        for epoch in range(100):\r\n            sess.run(train_step,feed_dict={img:read_data(sess)})\r\n            if epoch % 20 == 0:\r\n                save_path = saver.save(sess, MODEL_DIR + MODEL_NAME, global_step=epoch + 1)\r\n            print('step = ' + str(epoch))\r\n        save_to_pb(sess)\r\ntrain_model()\r\n```\r\n-------------------------\r\n\r\n#### run upper  code and get pb file, when you test pb file with python you maybe get a result like this:\r\n39.09046 45.927864 84.797905 45.952957 62.701195 68.00796 41.789146 80.99372 81.67459 81.2203....\r\n\r\n#### then run this command to create tflite file:\r\n```\r\nbazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/path/expert-graph.pb --output_file=/path/expert-graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,128,128,3 --input_array=img --output_array=out --inference_type=FLOAT --input_data_type=FLOAT --allow_custom_ops\r\n```\r\n\r\n#### you will get expert-graph.tflite,then use tflite file to android,will get result like this:\r\n49.392387\uff0c 42.290344\uff0c135.30707\uff0c 7.7764254 \uff0c 125.98051\uff0c79.317825 \uff0c157.29184 \uff0c107.58522 \uff0c 283.46997 \uff0c36.103508....\r\n\r\n#### Android and python have the same input data,but the result is different,if i remove BN and train, that's ok,but BN is very important,i can remove it ,i don't know how to solve it,this question has been around me for a long time,please help solve it, if don't use TF Lite, what should i do?Thanks\r\n------------------------\r\n\r\n### System information\r\nLinux Ubuntu 16.04:\r\nTensorFlow installed from source:\r\nTensorFlow version 1.8.0:\r\nPython version:3.6:\r\nBazel version 0.11.1:\r\nGCC/Compiler version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) :\r\nCUDA/cuDNN 9.0/7.0.5:\r\nGPU 1060-6G:", "comments": ["@longchr123  Can you include the test data and your implementation of custom op: SquaredDifference ?", "@shashishekhar  there is no test data for training, If i don't use --allow_custom_ops in bazel command,there will be errors,like this:\r\n```\r\nRunning command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/leve/lcr/model_result/tflite_test/expert-graph.pb' '--output_file=/home/leve/lcr/model_result/tflite_test/expert-graph.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--input_shape=1,128,128,3' '--input_array=img' '--output_array=out' '--inference_type=FLOAT' '--input_data_type=FLOAT'\r\n2018-07-18 10:50:45.979461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: SquaredDifference\r\n2018-07-18 10:50:45.979688: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 27 operators, 41 arrays (0 quantized)\r\n2018-07-18 10:50:45.979826: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 27 operators, 41 arrays (0 quantized)\r\n2018-07-18 10:50:45.980017: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 13 operators, 21 arrays (0 quantized)\r\n2018-07-18 10:50:45.980086: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 13 operators, 21 arrays (0 quantized)\r\n2018-07-18 10:50:45.980144: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 196736 bytes, theoretical optimal value: 196672 bytes.\r\n2018-07-18 10:50:45.980271: F tensorflow/contrib/lite/toco/tflite/export.cc:315] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: RSQRT, SquaredDifference.\r\n```\r\n\r\n**how to sovle it?**", "Ah, I am going to explain what the error means:\r\nFirst some background, TFLite doesn't have all the operators available in Tensorflow, we are working on improving and adding more operator support. When you convert your model the tool tells you if there are operators which don't have implementations. Here the tool is mentioning that RSQRT, SquaredDifference may need custom implementations. \r\n\r\nI am surprised the model didn't crash, I will take a look and see if these ops were used only for training, usually toco strips out the ops used for training.", "Nagging Assignee @shashishekhar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20870, "title": "[Question/Feature request] How to stack variable length tensors in a TensorArray?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs Sierra\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:no\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nHow to stack variable length tensors in a TensorArray cleanly and efficiently?\r\n\r\nI am implementing a form of RNN that produce variable length (time step dimension) tensors using tf.while_loop and tf.TensorArray to store the tensors. I need when `tf.while_loop` is done, all tensors are stacked into 1 tensor and the length dimension for each individual tensor is padded with 0 (or any constant) to the maximum length of the array.\r\n\r\nNote that the time dimension is unknown at compile time\r\n\r\n### Source code / logs\r\nIt would be something like this.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nBATCH = 2\r\nDIM = 4\r\n\r\nTIME_X = 2 # unknown at compile time\r\nTIME_Y = 4 # unknown at compile time\r\n\r\ntensor_array = tf.TensorArray(size=2, infer_shape=False, dtype=x.dtype)\r\nx = tf.random_uniform(shape=[BATCH, TIME_X, DIM], name='x')\r\n# at this point, x is created but y is not created yet\r\noutput_ta = tensor_array.write(0, x)\r\n\r\n# at this point, y is created\r\ny = tf.random_uniform(shape=[BATCH, TIME_Y, DIM], name='y')\r\noutput_ta = output_ta.write(1, y)\r\n\r\n# the maximum time dimension shape is unknown when the tensor is written to the TensorArray\r\n# meaning that we can't pad each individual tensor beforehand.\r\n\r\noutput = output_ta.stack()\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(output)) # this will raise exception as shape is not compatible\r\n\r\n# expected output shape: [2, BATCH, max(TIME_X, TIME_Y), DIM] = [2, 2, 4, 4]\r\n```\r\n\r\nThank you", "comments": ["@nxphi47 We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. please check **[link1](https://www.tensorflow.org/api_docs/python/tf/TensorArray)** , **[link2](https://stackoverflow.com/questions/56786937/how-to-get-values-in-tensorarray-which-contain-vary-shape-tensors)** If it is resolved then please feel free to move this issue to close status.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20870\">No</a>\n"]}, {"number": 20869, "title": "tf.contrib.lookup.HasTable", "body": "\r\n== cat /etc/issue ===============================================\r\nLinux george-OMEN-by-HP-Laptop 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"18.3 (Sylvia)\"\r\nVERSION_ID=\"18.3\"\r\nVERSION_CODENAME=sylvia\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux george-OMEN-by-HP-Laptop 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.0        \r\nnumpydoc                           0.7.0         \r\nprotobuf                           3.5.2.post1   \r\ntensorflow-gpu                     1.8.0         \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/home/george/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-9.1/lib64:/usr/local/cuda/lib64:/usr/local/cuda-9.1/lib64:/usr/local/cuda/extras/CUPTI/lib64/:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Jul 17 09:00:20 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1050    Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   56C    P0    N/A /  N/A |    709MiB /  4038MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1439      G   /usr/lib/xorg/Xorg                           286MiB |\r\n|    0      2503      G   cinnamon                                     160MiB |\r\n|    0      2630      G   ...-token=2E210DC1F44A55A6DDE70E07757D319C    63MiB |\r\n|    0      2865      G   /usr/lib/firefox/firefox                       3MiB |\r\n|    0      2995      G   ...-token=E5432E224C69AA6F39ED672CDE9108B3    79MiB |\r\n|    0      3500      G   /usr/lib/firefox/firefox                       1MiB |\r\n|    0      3574      G   /usr/lib/firefox/firefox                       1MiB |\r\n|    0      3632      G   /usr/lib/firefox/firefox                       1MiB |\r\n|    0      7199      C   /home/george/anaconda3/bin/python             97MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.1/lib64/libcudart.so.9.1.85\r\n/usr/local/cuda-9.1/lib64/libcudart_static.a\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n\r\nTensorflow 1.9\r\n\r\nPlease provide an oportunity to use tf.contrib.lookup.HashTables  \r\nwith key_dtype=tf.int32 and value_dtype=tf.string\r\nSynthetic code:\r\ntable = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(\r\n    tf.constant([1, 2, 3], dtype=tf.int32), tf.constant(['a', 'b', 'c'], dtype=tf.string)\r\n), '')\r\ntable.init.run() raises with error \r\nInvalidArgumentError: No OpKernel was registered to support Op 'HashTableV2' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_INT32]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_BOOL]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_STRING]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_FLOAT]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT64]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_STRING]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT64]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT32]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_FLOAT]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_DOUBLE]", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Added a PR #20884 for the fix."]}, {"number": 20868, "title": "Will cycle-gan have a Estimator implementation like tf.contrib.gan.estimator.GANEstimator?", "body": "or can GanEstimator be used on cycle-gan for training/predicting?\r\n\r\nThanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We don't have immediate plans to add a CycleGAN estimator, but we're going to add a StarGAN estimator within the week.\r\n\r\nIt should be straightforward to extend CycleGAN to CycleGANEstimator by following how GANEstimator is made; feel free to send me a pull request and I'll approve it.", "@joel-shor Thanks for information. Surely I will send a pull request while i finish it.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Expect a StarGANEstimator in the next week.\r\n\r\nAlso, any progress on the GANEstimator, @joneepenk ?", "sorry, the work on CycleGANEstimator is suspended for now, I am trapped in something else and will back to it asap.", "Please refer to the updated [link](https://www.tensorflow.org/tutorials/generative/cyclegan), moving this to closed status."]}, {"number": 20867, "title": "quantization deeplabv3(mobielentv2) error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.9.0\r\n- **Python version**: 2.7.3\r\n- **Bazel version (if compiling from source)**:0.12.0\r\n- **GCC/Compiler version (if compiling from source)**:c++11\r\n- **CUDA/cuDNN version**:7.5.18\r\n- **GPU model and memory**:TITAN,12GB\r\n- **Exact command to reproduce**:N/A\r\n\r\n\r\n### Describe the problem\r\nI want to train a quantization deeplabv3+(mobienetv2) model, use \"mobilenetv2_coco_voc_trainaug\" from https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md.\r\n\r\n\r\n### Source code / logs\r\n I add `tf.contrib.quantize.create_training_graph(quant_delay=0)` in line 315 https://github.com/tensorflow/models/blob/master/research/deeplab/train.py\r\nbut I got the error like below:\r\n```\r\nINFO:tensorflow:Training on train set\r\nTraceback (most recent call last):\r\n  File \"deeplab/train.py\", line 359, in <module>\r\n    tf.app.run()\r\n  File \"/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"deeplab/train.py\", line 281, in main\r\n    tf.contrib.quantize.create_training_graph(quant_delay=0)\r\n  File \"/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py\", line 112, in create_training_graph\r\n    freeze_bn_delay=freeze_bn_delay)\r\n  File \"/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py\", line 66, in _create_graph\r\n    is_training=is_training)\r\n  File \"/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py\", line 54, in FoldBatchNorms\r\n    graph, is_training, freeze_batch_norm_delay=freeze_batch_norm_delay)\r\n  File \"/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py\", line 100, in _FoldFusedBatchNorms\r\n    fused_batch_norm=True))\r\n  File \"/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py\", line 323, in _ComputeBatchNormCorrections\r\n    match.moving_variance_tensor + match.batch_epsilon)\r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'float'\r\n```\r\n\r\n\r\n", "comments": ["@drpngx Can you help me?", "Is `match.batch_epsilon` a `None`?", "@drpngx I found \"match.moving_variance_tensor\", \"match.moving_mean_tensor\" and \"match.bn_decay_mean_tensor\" are None", "@suharshs I find the code in tensorflow\\contrib\\quantize\\python\\fold_batch_norms.py, because the moving_mean_tensor, bn_decay_mean_tensor, moving_variance_tensor and bn_decay_var_tensor is not assign a value, can you take a look at it?\r\n```\r\n # TODO(suharshs): Find a way to get rid of this inner match.\r\n      for mul_match_result in moving_avg_mul_matcher.match_graph(graph):\r\n        sub_op = mul_match_result.get_op(moving_average_sub_pattern)\r\n        if sub_op.inputs[1].name == bn_op.outputs[1].name:\r\n          # During training: Batch Mean is bn_op.outputs[1]\r\n          moving_mean_tensor = sub_op.inputs[0]\r\n          bn_decay_mean_tensor = mul_match_result.get_tensor(bn_decay_pattern)\r\n        if sub_op.inputs[1].name == bn_op.outputs[2].name:\r\n          # During training: Batch Var is bn_op.outputs[2]\r\n          moving_variance_tensor = sub_op.inputs[0]\r\n          bn_decay_var_tensor = mul_match_result.get_tensor(bn_decay_pattern)\r\n```", "@suharshs Anyway to resolve this error? I'm attempting to add quantization nodes into a loaded pre-trained keras model (based on Mobilenet) and I get this very same error. Keras seems to handle batch norm differently than TensorFlow, and thus the `moving_variance_tensor` and `moving_mean_tensor` are `None` and this error occurs.", "I had encountered the same error too. I resolve it by replacing `tf.layers.BatchNormalization` with `tf.contrib.slim.batch_norm`. So the arrangement of parameters in the graph matches what `_FindFusedBatchNorms` is looking for.\r\n\r\nPS: `_FindFusedBatchNorms` is located in `tensorflow\\contrib\\quantize\\python\\fold_batch_norms.py`\r\n", "Indeed it seems that tf.layers.BatchNormalization isn't getting matched :( We will take a look! Thanks!", "@suharshs But I saw tf.contrib.slim.batch_norm in  slim.mobilenetv2.py, I still meet this issue.", "@raninbowlalala  same here, @HimariO  where did you make this change?", "@saeed68gm I'm getting this error with my own project, but the issue is the same\b. You can look into `tensorflow\\contrib\\quantize\\python\\fold_batch_norms.py` to compare the graph structure `graph_matcher` is looking for and what `silm.mobilenetv2.py` created. I had taken a quick glance at `silm.mobilenetv2.py` 's graph but doesn't see anything wrong with it.", "@HimariO  Thank you for your response. The interesting thing is I was able to quantize deeplab using their training code (which uses the specialize implementation of mobilenetv2) but when I try to use the line tf.contrib.quantize.create_training_graph in mobilenetv2 implementation in slim, I get this error. So I guess I can compare the two and find out the difference....", "@saeed68gm I will also check silm.mobilenetv2.py. If you have some progress, please comment here, we can discuss about this problem. Thank you very much!", "@saeed68gm I find a solution, you can use \"--fine_tune_batch_norm=true \" instead of \"--fine_tune_batch_norm=false\". This error will  disappear.", "@raninbowlalala This is awesome, I will give it a try this afternoon! Thank a lot of sharing your results :)", "@raninbowlalala I am using a variation of mobiletnet v2 (imported from slim) and not sure why but I still get the error. The only thing that fine_tune_batch_norm seems to be doing is setting the is_training flag to True?\r\n\r\n@HimariO I was not able to spot the difference between deeplab and mobilenetv2 from slim. Any luck for you?", "@raninbowlalala @HimariO I debugged a lot and found out the source of the issue, but have no solution yet. The issue happens when you want to create an evaluation graph and you don't want to have \"is_training=True\" hence you won't have variables like moving variance. For right now, I just turn off eval graph when I want to quantize, but that's just a workaournd. Any idea what to do to have an eval graph and be able to quantize as well?\r\n", "@saeed68gm how did you turn off eval? I am facing this issue to quantize faster rcnn model", "@KapoorHitesh it depends are your code. You gotta go dig into it and see where you create your eval graph and rip it all out.", "Hi, I also have the same issue here. I use the [MobileNet v2 Colab Example](https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb) and try to quantize the model with `create_training_graph`, then I get the same error. Is there any suggestion on this issue? Thanks!", "I trained my own  SSD model using `tensorflow/models/research/object_detection/` project and exported the inference graph by its `export_inference_graph.py` script, then I got the model.ckpt files and frozen_inference_graph.pb file.    \r\nNow I want to quantize the trained model using quantization-aware training method, but I got the same error! The code  `match.moving_variance_tensor + match.batch_epsilon` arises error because `match.moving_variance_tensor` is NoneType, but why it is NoneType? and how fix it???   \r\nThe BN layers is added into the graph by using `tf.layers.batch_normalization()` api.", "I have the same issue, but I fix it by using the method `tf.layers.batch_normalization( )` correctly.  \r\n   \r\nThis error arises because `match.moving_variance_tensor` variable is `NoneType`, so you can not add it to another float number. But why this variable in graph's batch normalization layer is `NoneType`? I think you are using `tf.layers.batch_normalization( )` or some other batch normalization api in TensorFlow mistakenly.   \r\n\r\nThere is an argument named `training` in the [method](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization). When you call this method, you must note that the `moving_mean` and `moving_variance` need to be updated when training, but not when inference. So the behavior of BN layer is different when in training mode or inference/test/eval mode, you just pass correct mode to the `tf.layers.batch_normalization(training=is_training )` method. If you do not pass correct argument to it, your model may not trained properly and `moving_mean` and `moving_variance` are not trained! So they are `NoneType` when quantizing the model.", "What and where are the changes to be done to enable quantization aware training for Deeplab V3 MobilenetV2??", "@SanthoshRajendiran did you find a solution to your earlier issue ?\r\n\r\n> What and where are the changes to be done to enable quantization aware training for Deeplab V3 MobilenetV2??\r\n\r\n", "Not yet @ajinkya933 Just waiting for some helping hand.. Do let me know if you have figured out a solution for the same.", "Did anyone solved this issue? I'm faced with exactly the same issue when adding a create_training_graph(). My model makes use of a pre-trained ResNet-v1 and is derived from it a bit. There is no explicit \"batch normalization\" in the derived model. Shall I find a quantized ResNet before further training?", "I was able to solve this issue by upgrading tensorflow version from 1.15 to 1.15.3.", "Same as @balazsbanto here. I was able to fix this by updating from 1.15.0 to 1.15.3", "Hi @raninbowlalala ! 1.x versions are not supported any more. It seems that it has been resolved in 1.15.3 from the above [comment ](https://github.com/tensorflow/tensorflow/issues/20867#issuecomment-716664588)though. Can we move this issue to closed status now?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20865, "title": "tensorflow 1.9 still asks for cuda 9.0 though i have 9.2", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10\r\n- **TensorFlow installed from (source or binary)**: pip3\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: 9.2\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nWhen importing tensorflow it fails and asks for cuda 9.0 dll\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"D:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"D:\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\n", "comments": ["The PyPI (pip) versions of TensorFlow are still [built against CUDA 9.0](https://github.com/tensorflow/tensorflow/pull/19474#issuecomment-394934043). You will have to build from source if you want to build against CUDA 9.2. See [here](https://github.com/tensorflow/tensorflow/issues/18906) for lots of discussion on how to do this.", "Thanks. Is there a tutorial of doing that step by step? ", "Tutorial for installing TensorFlow from source is available [here](https://www.tensorflow.org/install/install_sources). But as mentioned in the tutorial TensorFlow does not support building on windows so it would be easier for you to downgrade your CUDA and cuDNN installation.", "Thanks. I tried and failed on windows as well. Maybe I'll just wait till pip version gets updated. Hope it will happen soon.", "If you are building on Linux, feel free to use the following project that make TF compilation super easy: https://github.com/hadim/docker-tensorflow-builder", "Thanks for the advice. Unfortunately I'm running it on Windows now. Besides is running tensorflow on linux faster than that on windows?", "Is there an ETA for a CUDA 9.2 Windows build?", "If anyone is interested in an alternative tutorial for building from source, I wrote one for [cuda9.0](https://medium.com/@zhanwenchen/speed-up-learning-by-building-tensorflow-gpu-from-source-on-ubuntu-d03bb4e06b23) and another for [cuda9.2](https://medium.com/@zhanwenchen/install-cuda-9-2-and-cudnn-7-1-for-tensorflow-pytorch-gpu-on-ubuntu-16-04-1822ab4b2421).", "Nagging Assignee @poxvoculi: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It's as @FarhanTejani said. "]}, {"number": 20864, "title": "NCCL 2.x document updates and removal of license file", "body": "", "comments": ["Possibly rename the PR if you can. Unsure what \"y8 d0 b\" means", "@case540  Updated.  I was following the EngProd CherryPicking guide and missed the rename PR step.  It was pretty smooth I must say. ", "Just wondering if the removal of 3.0 from TF_CUDA_COMPUTE_CAPABILITIES in Dockerfile.devel-gpu was deliberate? IIUC this was explicitly added for #4002 and #4301.", "The removal was deliberate. @tfboyd may have some more comments.", "Those issues are almost 2 years old, but if we want the devel to have 3.0 we can add it back but if you are building from source, which is the purpose of that docker, you would set TF_CUDA_COMPUTE_CAPABILITIES to the GPU types you are using.  My motivation was to update it to match the binary release which starts at 3.5.  "]}, {"number": 20863, "title": "R1.10 cherry-pick request: Fix tensorrt conversion to ignore all preserved nodes", "body": "@samikama please help to make sure the change is correct.", "comments": []}, {"number": 20862, "title": "[tftrt]", "body": "  Added alignment in trt_allocator\r\n  Since TensorFlow gpu_bfc_allocator does not abide to the alignment requested.", "comments": ["@aaroey This PR should resolve the virtual destructor issue, as well as the error from cudnnFusedConvActLayer.cpp Cuda Error in createFilterTextureFused: 11 ", "Please rebase to resolve conflicts.", "Merged master to head of PR to resolve conflict", "When will be this  fix be merged to formal release than mainline?\r\nI encounter this issue with the following env:\r\nBuild env:\r\nHost: p3.16xlarge\r\nGcc: 4.8.5 (\r\nWhich gcc: /usr/bin/gcc\r\nll /usr/bin/gcc-> /usr/bin/gcc48\r\n)\r\nAMI: Deep Learning AMI (Amazon Linux) Version 12.0 (ami-45655f20)\r\nTensorRT: tar file(4.0.16 for ubuntu 14 ) installed at /home/ec2-user/tensorrt\r\nTensorflow 1.10\r\nBuild config:\r\nCuda 9, cudnn 7.1 nvcc 1.3, jemalloc enabled , others no\r\n \r\nRunning env:\r\nHost: p3.2xlarge\r\nAMI: Deep Learning AMI (Amazon Linux) Version 12.0 (ami-45655f20)\r\nTensorRT: tar file(4.0.16 for ubuntu 14 ) installed at /home/ec2-user/tensorrt\r\nTensorflow: wheel form build env\r\n\r\n", "@zhenyu r1.11 will be cut in a few days and will contain this, thanks."]}, {"number": 20861, "title": "[Intel MKL] Adding support for MKL builds with AVX2. ", "body": "MKL-DNN ignores the compiler switches and takes code paths based on the platform detected at runtime. This commit will add support for avx2 instructions to the rest of TensorFlow.", "comments": ["@rmlarsen Clarified comments and added more complete support for the haswell and sandybridge instruction sets. The mtune parameters won't add instructions, but will tune cache sizes etc for the specified architecture. ", "@claynerobison Thanks!", "@gunan @jhseu does this look good to you? ", "Sorry, @jhseu @rmlarsen @gunan further internal testing revealed that we didn't have the full haswell arch build flag being passed in to the first two containers. Fixed it with the last commit. "]}, {"number": 20860, "title": "Support dense tensors in _SequenceNumericColumn", "body": "Resolves https://github.com/tensorflow/tensorflow/issues/20478\r\n\r\nTo Do:\r\n\r\n- [ ] Add unit tests", "comments": ["Will do :+1: ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing. We switched away from feature columns."]}, {"number": 20859, "title": "[WIP] ProIO Dataset", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 20858, "title": "AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 20857, "title": "Issue with tf.gradients() taking too long", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 2.7.12\r\n- **CUDA/cuDNN version**: 7.5.17\r\n- **GPU model and memory**: GeForce GTX 1080 Ti\r\n- **Bazel version**: N/A\r\n- **Exact command to reproduce**: tf.gradients()\r\n\r\n### Describe the problem\r\n\r\nSo I am using tensorflow/models/object_detection with different networks (Currently using a SSD MobileNet network). I am trying to prune the network and I am using tf.gradients(total_loss, activation_tensor) on the activation tensors to see which filters need to be pruned. However, the issue I am currently facing is that each call to tf.gradients is costing me at least 3 seconds regardless if I am doing this on the GPU or CPU. Is this a known issue and is there a fix or a replacement for tf.gradients? \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow installed from\nBazel version\nExact command to reproduce", "Yes the issue still persists.", "@felsabbagh3 Sorry for the late response. I think this is a stale issue. \r\n\r\nThis issue is more related to models. You could post issues related to `object_detection` under models repo https://github.com/tensorflow/models/issues\r\n\r\nPlease share simple standalone code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20857\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20857\">No</a>\n"]}, {"number": 20856, "title": " GPUBFCAllocator breaks Allocator API", "body": "Tensorflow Allocator API has a guarantee that AllocateRaw() will honor the requested alignment. \r\n( https://github.com/tensorflow/tensorflow/blob/33af29b33f14cc74725ff081d50e6e59247ef546/tensorflow/core/framework/allocator.h#L82 )\r\n\r\nHowever, GPUBFCAllocator disregards this parameter and thus breaks the guarantee given by the base class.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/33af29b33f14cc74725ff081d50e6e59247ef546/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.h#L60", "comments": ["@zheng-xq is this a bug or intentional?", "I don't think it is intentional. But you should check if it breaks anything.", "@zheng-xq, it breaks tftrt integration. We rely on allocator to return aligned addresses", "We should definitely fix the code to respect the alignment. But we first need to make sure we are not exposed to other problems by fixing this, in real models. Lambda will look into this in parallel and it might take a while. Meanwhile, I'll leave it up to him to propose what we should do to enable tftrt.\r\n", "I discussed with @jjsjann123 and @samikama already, Jie issued #20862 as a workaround, just FYI.", "Just to clarify, PR #20862 fixes the alignment for TRT allocation. It does not fix the alignment for gpu_bfc_allocator for other use cases.", "Nagging Assignees @smit-hinsu, @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue as the workaround was found for the use-case.\r\n\r\nPlease reopen if this causes issue for other use-cases as well.", "@smit-hinsu it is correct that we worked around the problem but the fact that GPUBFCAllocator breaking Allocator API stands. Is there a reason for not fixing it?"]}, {"number": 20855, "title": "Update version strings for 1.10", "body": "", "comments": ["Unsure if anything in the support table needs to be updated. Copy/pasted entries from TF 1.9"]}, {"number": 20854, "title": "R1.9 cherry picks docs", "body": "Trying to eliminate a problematic cherry-pick commit from https://github.com/tensorflow/tensorflow/pull/20704", "comments": ["@av8ramit could you please delegate to whoever is in charge of the R1.9 branch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Ok, I think Mark and I will work on another way to update the site. No need to take up everyone's time on this. Sorry for the confusion!"]}, {"number": 20853, "title": "Improvement of shape function in HistogramFixedWidth", "body": "In the HistogramFixedWidth op, there are restrictions over the shape of range_value and nbins. The range_value should be a vector of 2 elements and nbins should be a scalar.\r\n\r\nThis fix adds the restriction to shape function of HistogramFixedWidth.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@rmlarsen Thanks for the review. The PR has been updated. Please take a look."]}, {"number": 20852, "title": "AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @angersson: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20851, "title": "Add support for possible extra : in input names", "body": "This PR adds support to handle node/tensor names with : in their names in the NodesToKeep list.", "comments": ["@aaroey I added the missing space. Can you review?", "Thanks @rmlarsen!"]}, {"number": 20850, "title": "Use com.android.support.test instead of com.androidx.test", "body": "This PR should fix https://github.com/tensorflow/tensorflow/issues/20748 and https://github.com/tensorflow/tensorflow/issues/20828", "comments": ["Was this fixed with https://github.com/tensorflow/tensorflow/commit/1d38580d6db3dc48b916453c2ce6cce691a00fe6? If so I'll just close this.", "It might have been fixed by that commit. I'd like to make sure it is working for you, though.", "Yea, I'm all good thanks. Closing this now."]}, {"number": 20849, "title": "Add initial draft of RELEASE notes for 1.10.", "body": "", "comments": ["Done, adding breaking changes section about dropping cmake support", "Here is a convienent link to the currently drafted 1.10 release notes...\r\nhttps://github.com/case540/tensorflow/blob/e265349b7c919c2684483dd531d0310a0adb72a8/RELEASE.md\r\n\r\nRajat, please tell me if you have an change suggestions or if they look good. Thanks!\r\n"]}, {"number": 20848, "title": "TF slim support for model_pruning not working", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:na\r\n- **GCC/Compiler version (if compiling from source)**:na\r\n- **CUDA/cuDNN version**:9/7.1\r\n- **GPU model and memory**:1080ti\r\n- **Exact command to reproduce**:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/model_pruning/python/learning.py\r\n```\r\nfrom tensorflow.contrib.model_pruning.python import learning\r\nfrom tensorflow.contrib.model_pruning.python import pruning\r\n  # Load data and create the model:\r\n  images, labels = LoadData(...)\r\n  predictions = MyModel(images)\r\n  # Define the loss:\r\n  slim.losses.log_loss(predictions, labels)\r\n  total_loss = slim.losses.get_total_loss()\r\n  # Define the optimizer:\r\n  optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\r\n  # Create the train_op\r\n  train_op = slim.learning.create_train_op(total_loss, optimizer)\r\n  # Set up sparsity\r\n  sparsity = pruning.setup_gradual_sparsity(self.global_step)\r\n  # Create mask update op\r\n  mask_update_op = pruning.add_mask_update_ip(sparsity)\r\n  # Run training.\r\n  learning.train(train_op,\r\n                 my_log_dir,\r\n                 mask_update_op)\r\n\r\n```\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nAttributeError: module 'tensorflow.contrib.model_pruning.python.pruning' has no attribute 'setup_gradual_sparsity'\r\n\r\nIf this is not possible yet, can you please describe a way to use apply_mask from pruning instead ?\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["The documentation in the code seems to be out of date. I'll try to find the maintainers of this contrib package and have it updated. (Note that symbols in the `tf.contrib` namespace aren't part of the stable TensorFlow API)", "The documentation has been updated in commit aa15692"]}, {"number": 20847, "title": "Describe that the semantics of gfile are different.", "body": "Fixes #19296.", "comments": []}, {"number": 20846, "title": "Tensorflow inference on Android, can't find 'Iterator' op. No OpKernel was registered to support Op", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8.0 (For training and for creating the .so and .jar files)\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: None, running using CPU\r\n- **Android Studio**: 3.1.3\r\n- **Android NDK**: 14b\r\n- **Andorid SDK**: v28\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI am trying to run a trained Tensorflow model on an Android device.  The model I am trying to run on the android mobile device uses an Iterator operation in the inference graph.\r\n\r\nI am compiling from source using:\r\n`bazel build -c opt --copt=-D__ANDROID_TYPES_FULL__ //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a`\r\n\r\n17.2 MB (.so file)\r\nI can load the graph and use the feed method from TensorFlowInferenceInterface, but whenever I try to make a prediction by using a run method from the  TensorFlowInferenceInterface.  I get the \"java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs.\" error.\r\n\r\nI asked on Stackoverflow here https://stackoverflow.com/questions/51331474/tensorflow-inferences-on-android-cant-find-iterator-op-no-opkernel-was-regi but I believe this might be a better place to ask because this appears to be a bug.\r\n\r\nI am compiling from source using the D__ANDROID_TYPES_FULL__ so I would expect it to be able to find the iterator operation, which is why I believe it's a bug.  \r\n\r\nI am able to run this using a Python/Tensorflow script on my laptop, but am unable to make it run on an Android device.  I give the python script I use to run the model below.\r\n\r\n I tried attaching the frozen model that I run in the code, but I couldn't attach on Github because the file size was too big.  The model was made from using this code: https://github.com/tensorflow/nmt and saving the model when making an inference.\r\n### Source code / logs\r\n\r\nThis the error I get in Android Studio\r\n```\r\nI/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\r\n    E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n    I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\n    I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\r\n    I/TensorFlowInferenceInterface: Model load took 764ms, TensorFlow version: 1.8.0\r\n    I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/laptop_frozen_graph_init_tables.pb'\r\n    E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[batch_size, src_data], outputs:[index_to_string_Lookup]\r\n    E/AndroidRuntime: FATAL EXCEPTION: Thread-5769\r\n                      Process: com.example.student.projecttest, PID: 6805\r\n                      java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                        <no registered kernels>\r\n    \r\n                      \t [[Node: Iterator = Iterator[container=\"infer\", output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], shared_name=\"\"]()]]\r\n                          at org.tensorflow.Session.run(Native Method)\r\n                          at org.tensorflow.Session.access$100(Session.java:48)\r\n                          at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n                          at org.tensorflow.Session$Runner.run(Session.java:248)\r\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)\r\n                          at com.example.student.projecttest.MainActivity.translateToFrench(MainActivity.java:79)\r\n                          at com.example.student.projecttest.MainActivity$1$1.run(MainActivity.java:42)\r\n```\r\n\r\n\r\nThis is the code I use to run the model on my laptop\r\n```\r\nimport tensorflow as tf\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    # We load the protobuf file from the disk and parse it to retrieve the \r\n    # unserialized graph_def\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    # Then, we import the graph_def into a new Graph and returns it \r\n    with tf.Graph().as_default() as graph:\r\n        # The name var will prefix every op/nodes in your graph\r\n        # Since we load everything in a new graph, this is not needed\r\n        tf.import_graph_def(graph_def, name=\"prefix\")\r\n    return graph\r\n\r\ntgt_eos = '</s>'\r\nfrozen_model_path =  'laptop_frozen_graph_init_tables.pb'\r\ngraph = load_graph(frozen_model_path)\r\n\r\nwith tf.Session(graph = graph) as sess:\r\n\r\n  #Get input tensors\r\n  src_file_placeholder = sess.graph.get_tensor_by_name(\"prefix/src_data:0\")\r\n  batch_size_placeholder = sess.graph.get_tensor_by_name(\"prefix/batch_size:0\")\r\n  \r\n  \r\n  #Initialize tables\r\n  init_tables_operation = sess.graph.get_operation_by_name(\"prefix/init_all_tables\")\r\n  sess.run(init_tables_operation)\r\n  \r\n  #Get init iterator operation\r\n  make_iterator_operation = sess.graph.get_operation_by_name(\"prefix/MakeIterator\")\r\n  \r\n  \r\n  src_data = ['Bonjour', 'tout', 'le', 'monde', 'c', '\\'', 'est', 'vrai', tgt_eos]\r\n  \r\n  #Make_iterator has to be fed with src_data\r\n  sess.run(make_iterator_operation, feed_dict={\r\n            src_file_placeholder: src_data,\r\n            batch_size_placeholder: 32\r\n        })\r\n\r\n  #Get output tensor\r\n  output0 = graph.get_tensor_by_name(\"prefix/index_to_string_Lookup:0\")\r\n  \r\n  #Iterate through input until we run out of input\r\n  try:\r\n    while True:\r\n      txt_output = sess.run(output0)\r\n      print(txt_output)\r\n  except tf.errors.OutOfRangeError:\r\n      print('Done inferencing')\r\n```\r\n\r\nThis is the code I use in Android Studio which produces a very error to the one mentioned above.\r\n```\r\n        String modelName = \"laptop_frozen_graph_init_tables.pb\";\r\n        String fullModelPath = \"file:///android_asset/\" + modelName;\r\n\r\n        AssetManager assetManager = getAssets();\r\n\r\n        TensorFlowInferenceInterface inferenceInterface = new TensorFlowInferenceInterface(assetManager, fullModelPath);\r\n        Graph graph = inferenceInterface.graph();\r\n\r\n        inferenceInterface.run(new String[]{\"init_all_tables\"});\r\n```\r\n\r\n", "comments": ["Hello @gerardocervantes8, I don't know about building with bazel for android but if you use the makefile building for android (tensorflow/contrib/makefile) you need to include the file tensorflow/core/kernels/data/iterator_ops.cc file in the tf_op_files.txt before compiling\r\nThen the OpKernel for the 'Iterator' op should be included in the library", "Nagging Assignee @robieta: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20845, "title": "[XLA:GPU] add XLA_AMDGPU device", "body": "Add XLA_AMDGPU device to XLA", "comments": ["@jlebar This is the PR to amend #20793 . On that PR it only address half of the issue in XLA. This PR addresses the other half and touches GPU common runtime as well.", "Reading this thread, there are a number of issues in common binary request, with  GPU Computing there is no OS System runtime like we have with CPU for Intel and AMD x86 equivalents.   CUDA, and HIP Runtime while similar are different how they operation since  GPU architectures between NVIDIA and AMD due to each vendors GPU are different architecturally, there is no common ISA like we have in the CPU world.  This request is more like asking for the equivalent of x86 and ARM common binary in many ways.    \r\n\r\nAlso, the case raised here are on Microarchitectural extension to x86. Which in this case you need more uniform binary to deal with these difference of Micro-Architecture  \"we should have tensorflow-sse, tensorflow-sse2, tensorflow-sse3, tensorflow-ssse3, tensorflow-avx, tensorflow-avx2, tensorflow-avx512, ..\"  not different Accelerators/CPU Architectures   Note when porting Application, library or OS generally we do not build fat binaries for X86, ARM, MIPS, Power for Tensorflow.    ", "@gstoner I don't think that's true. Ultimately CUDA code becomes PTX/SASS that is linked into the host-side binary and loaded and launched by host-side code. I assume something similar happens with the AMD toolchain. What is to stop us from linking both CUDA and AMD binary blobs into the same host-side binary? If we get the host-side code right, this should be possible.\r\n", "You have larger dependency beyond the code object for the kernel, but also call accelerator libraries which affect host code.    For you to understand how our compiler is different from NVIDIA. \r\n\r\nBLAS cuBLAS  for NVIDA and rocBLAS for AMD \r\nParallel Primitives CUB for NVIDIA, rocPRIM for AMD  \r\nDeep learning primatives cuDNN. for NVIDIA, MIOpen for AMD,  \r\n\r\nAlso, we do not generate IL Binary  like NVIDIA aka PTX.  We generate native Binary  obj. Here is a link to the AMDPGU compiler overview.   https://llvm.org/docs/AMDGPUUsage.html", "@hawkinsp Per discussion I've revised the PR so a new class of XLA_AMDGPU device is introduced. Notice no bazel targets depend on them yet as #20277 would have to be approved and merged before ROCm-specific bazel conditions can be introduced.", "@hawkinsp I've added `xla/service:amdgpu_plugin`. There are some targets not enabled yet (`amdgpu_compiler`, `stream_executor_rocm`) which should be introduced in later PRs.", "failing CI test in XLA target suggests it has nothing to do with the added logic in this pull request. how to trigger a re-test?", "I agree, that looks like an unrelated failure. I will retrigger the tests.", "@hawkinsp I've addressed failures found by CI. Would you mind help re-run the tests?", "@hawkinsp a gentle ping?", ">> @hawkinsp I've addressed failures found by CI. Would you mind help re-run the tests?\r\n> @hawkinsp a gentle ping?\r\n\r\nI believe you have the permission bit to rerun the tests now?  I'll try to kick them off, no idea if the fact that there's now a merge conflict prevents that.", "@whchung please rebase your PR.", "@whchung could you pull rebase and push again?", "will do. I've just managed to get AMDGPU XLA pipeline to a certain maturity. I'll rebase the PR and submit new ones for HLO opt passes/IR emitters for AMDGPU XLA pipeline.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 20844, "title": "MonitoredTrainingSession is not handling the dummy_QueueRunner cleanly?", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Fedora Red Hat Enterprise Linux Server release 7.5 (Maipo)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA9.0/ CUDNN7.0\r\n- **GPU model and memory**:  P100 16GB\r\n- **Exact command to reproduce**:  bash run.sh\r\n\r\n### Describe the problem\r\nI am using the MonitoredTrainingSession to test a simple script (base on [this](https://github.com/tmulc18/Distributed-TensorFlow-Guide/blob/master/Synchronous-SGD/ssgd.py) of synchronized distributed training/multi_gpu training. When the training is finished, the session always throws out Exception (CancelledError) in thread QueueRunnerThread.  I suspect this is related to QueueRunner is not handled cleanly when sess.close() is called. \r\n\r\n### Source code / logs\r\n`REPLICAS_TO_AGGREGATE = 1\r\ndef main():\r\n    # Configure\r\n    config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\r\n    config.gpu_options.allow_growth = True\r\n\r\n    # Server Setup\r\n    cluster = tf.train.ClusterSpec({\r\n        'ps':['localhost:2222'],\r\n        'worker':['localhost:2223']\r\n        }) #allows this node know about all other nodes\r\n    if FLAGS.job_name == 'ps': #checks if parameter server\r\n        with tf.device('/job:ps/task:0/cpu:0'):\r\n            server = tf.train.Server(cluster,\r\n                            job_name=\"ps\",\r\n                            task_index=FLAGS.task_index,\r\n                            config=config)\r\n            server.join()\r\n    else: #it must be a worker server\r\n        is_chief = (FLAGS.task_index == 0) #checks if this is the chief node\r\n        with tf.device('/gpu:%d' % (FLAGS.task_index)):\r\n            server = tf.train.Server(cluster,\r\n                    job_name=\"worker\",\r\n                    task_index=FLAGS.task_index,\r\n                    config=config)\r\n    \r\n        # Graph\r\n        # ps_device = \"/job:ps/task:0/cpu:0\"\r\n        worker_device = \"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, FLAGS.task_index)\r\n        with tf.device(tf.train.replica_device_setter(ps_tasks=1,\r\n                       #ps_device=ps_device,\r\n                       worker_device=worker_device)):\r\n\r\n            a = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)\r\n            b = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)\r\n            c= a+b\r\n\r\n            global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\r\n            target = tf.constant(100.,shape=[2],dtype=tf.float32)\r\n            loss = tf.reduce_mean(tf.square(c-target))\r\n\r\n            # create an optimizer then wrap it with SynceReplicasOptimizer\r\n            optimizer = tf.train.GradientDescentOptimizer(.0001)\r\n            optimizer1 = tf.train.SyncReplicasOptimizer(optimizer,\r\n                     replicas_to_aggregate=REPLICAS_TO_AGGREGATE, total_num_replicas=REPLICAS_TO_AGGREGATE)\r\n      \r\n            opt = optimizer1.minimize(loss,global_step=global_step) # averages gradients\r\n            #opt = optimizer1.minimize(REPLICAS_TO_AGGREGATE*loss,\r\n            #                           global_step=global_step) # hackily sums gradients\r\n\r\n        # Session\r\n        sync_replicas_hook = optimizer1.make_session_run_hook(is_chief)\r\n        stop_hook = tf.train.StopAtStepHook(last_step=10)\r\n        hooks = [sync_replicas_hook,stop_hook]\r\n\r\n        # Monitored Training Session\r\n        with tf.train.MonitoredTrainingSession(master = server.target, \r\n              is_chief=is_chief,\r\n              config=config,\r\n              hooks=hooks,\r\n              stop_grace_period_secs=10) as sess:\r\n\r\n            print('Starting training on worker %d'%FLAGS.task_index)\r\n            while not sess.should_stop():\r\n                _,r,gs=sess.run([opt,c,global_step])\r\n                print(r,'step: ',gs,'worker: ',FLAGS.task_index)\r\n                # if is_chief: time.sleep(1)\r\n                time.sleep(1)\r\n            print('Done',FLAGS.task_index)\r\n  \r\n            time.sleep(10) #grace period to wait before closing session\r\n            #sess.close() # if uncomment this, it will raise error 'Session is already closed' after the CanceledError \r\n        print('Session from worker %d closed cleanly'%FLAGS.task_index)\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    # Flags for defining the tf.train.ClusterSpec\r\n    parser.add_argument(\r\n        \"--job_name\",\r\n        type=str,\r\n        default=\"\",\r\n        help=\"One of 'ps', 'worker'\"\r\n      )\r\n    # Flags for defining the tf.train.Server\r\n    parser.add_argument(\r\n        \"--task_index\",\r\n        type=int,\r\n        default=0,\r\n        help=\"Index of task within the job\"\r\n      )\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    print(FLAGS.task_index)\r\n    main()`\r\n\r\n\r\n\r\n**************** I use the following script to run the code \r\n`#!/bin/bash -e\r\nexport CUDA_VISIBLE_DEVICES='' \r\npython text_multi_gpu.py --job_name=\"ps\" --task_index=0 &\r\nexport CUDA_VISIBLE_DEVICES='0' \r\npython text_multi_gpu.py --job_name=\"worker\" --task_index=0`\r\n\r\n\r\n\r\n****************** Below is part of the log\r\n`[0.15988803 0.15988803] step:  8 worker:  0\r\n[0.17985605 0.17985605] step:  9 worker:  0\r\nDone 0\r\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\r\n    coord.request_stop(e)\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1249, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1420, in _call_tf_sessionrun\r\n    status, run_metadata)\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to \"Session::Close()\".\r\n\r\nSession from worker 0 closed cleanly`", "comments": ["Sorry, I'm having a little trouble understanding the problem here-- you should not need to explicitly close the session in the `with ...` block, so the version with sess.close() commented out is the correct one. Is there a problem still with that version?", "@karmel \r\nYes, the problem shown above still happens when \"sess.close()\" is commented. \r\nIf \"sess.close()\" is uncommented, there is another error as you can expect in addition to the Error I shown above. ", "@karmel \r\nany clue? Thank for your help.", "Unfortunately, it is hard to debug from a distance. Is there any reason you are using the deprecated Queue API rather than the newer [Dataset API](https://www.tensorflow.org/guide/datasets)?", "I have the same issue, any ideas?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to staleness. Please use the latest version for TensorFlow and test again. Feel free to open a new issue if it still persists. Thanks!", "@tsoi2 how do you fix this problem? I met the same problem.."]}]