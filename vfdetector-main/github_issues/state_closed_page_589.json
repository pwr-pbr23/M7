[{"number": 36007, "title": "Fix bug with reduce_logsumexp v2 compatibility", "body": "", "comments": []}, {"number": 36006, "title": "keras model training - total loss discrepancy with multiple outputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 31\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.1.0-1-ga9af83a149 2.1.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 8.3.1\r\n- CUDA/cuDNN version: 10.2.89 / 7.6.5.33\r\n- GPU model and memory: Nvidia GeForce GTX 1070 TI 8GB\r\n\r\n**Describe the current behavior**\r\nWhile training a model with multiple outputs via keras, the `loss` and `val_loss` values do not agree with the documented definition. The documentation states:\r\n\r\n> The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients.\r\n\r\nHowever as can be seen by the following screencap of tensorboard, this does not match up:\r\n![image](https://user-images.githubusercontent.com/1826947/72649484-d2c9ea80-394b-11ea-8f44-818ff8b526f3.png)\r\n\r\n\r\nAnd the verbose training log output agrees with the tensorboard charts:\r\n```\r\nEpoch 1/30\r\n37/37 [==============================] - 3s 74ms/step - loss: 776.4654 - out_0_loss: 0.0281 - out_1_loss: 0.0189 - out_2_loss: 0.0310 - out_3_loss: 0.0319 - out_4_loss: 0.0525 - out_5_loss: 0.0643 - out_0_mean_squared_error: 0.0281 - out_1_mean_squared_error: 0.0189 - out_2_mean_squared_error: 0.0310 - out_3_mean_squared_error: 0.0319 - out_4_mean_squared_error: 0.0525 - out_5_mean_squared_error: 0.0643 - val_loss: 121.1278 - val_out_0_loss: 0.0116 - val_out_1_loss: 0.0119 - val_out_2_loss: 0.0110 - val_out_3_loss: 0.0136 - val_out_4_loss: 0.0186 - val_out_5_loss: 0.0273 - val_out_0_mean_squared_error: 0.0116 - val_out_1_mean_squared_error: 0.0119 - val_out_2_mean_squared_error: 0.0110 - val_out_3_mean_squared_error: 0.0136 - val_out_4_mean_squared_error: 0.0186 - val_out_5_mean_squared_error: 0.0273\r\n\r\nEpoch 2/30\r\n37/37 [==============================] - 2s 45ms/step - loss: 57.5657 - out_0_loss: 0.0251 - out_1_loss: 0.0237 - out_2_loss: 0.0244 - out_3_loss: 0.0311 - out_4_loss: 0.0361 - out_5_loss: 0.0479 - out_0_mean_squared_error: 0.0251 - out_1_mean_squared_error: 0.0237 - out_2_mean_squared_error: 0.0244 - out_3_mean_squared_error: 0.0311 - out_4_mean_squared_error: 0.0361 - out_5_mean_squared_error: 0.0479 - val_loss: 33.7718 - val_out_0_loss: 0.0241 - val_out_1_loss: 0.0222 - val_out_2_loss: 0.0226 - val_out_3_loss: 0.0290 - val_out_4_loss: 0.0349 - val_out_5_loss: 0.0447 - val_out_0_mean_squared_error: 0.0241 - val_out_1_mean_squared_error: 0.0222 - val_out_2_mean_squared_error: 0.0226 - val_out_3_mean_squared_error: 0.0290 - val_out_4_mean_squared_error: 0.0349 - val_out_5_mean_squared_error: 0.0447\r\n```\r\n\r\nAnd since I am using weighted values, the weights are:\r\n```\r\n[1.0, 1.0, 0.8, 0.6, 0.45, 0.3]\r\n```\r\n\r\nI'm not using weighted samples or anything. The weights are only defined within `model.compile()`.\r\nThe data is fed as a `tf.data.Dataset`, where the shape of the training & validation data is:\r\n```\r\n(TensorSpec(shape=(None, 3, 19546), dtype=tf.float32, name=None), (TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None)))\r\n```\r\n\r\n**Describe the expected behavior**\r\nFor epoch 1:\r\n```\r\nweights = np.array([1.0, 1.0, 0.8, 0.6, 0.45, 0.3])\r\nlosses = np.array([0.0281, 0.0189, 0.0310, 0.0319, 0.0525, 0.0643])\r\nval_losses = np.array([0.0116, 0.0119, 0.0110, 0.0136, 0.0186, 0.0273])\r\n\r\nloss = np.sum(losses * weights)\r\n0.133855\r\n\r\nval_loss = np.sum(val_losses * weights)\r\n0.05702\r\n```\r\n\r\n**Code to reproduce the issue**\r\nHave been trying to reproduce with a simple example I can share, but am having difficulty. The little bit of info I can provide is how the model was compiled, and fit:\r\n```\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\r\nmodel.compile(loss='mean_squared_error', loss_weights=[1.0,1.0,0.8,0.6,0.45,0.3], optimizer=optimizer, metrics=['mean_squared_error'])\r\nmodel.fit(\r\n        dataset_train,\r\n        epochs=30,\r\n        validation_data=dataset_validation,\r\n        callbacks=[\r\n            tf.keras.callbacks.LearningRateScheduler(lambda i: 0.0005 * 10 ** -(i/10)),\r\n            keras.callbacks.TensorBoard(log_dir=tensorboard_path, profile_batch=0),\r\n        ],\r\n    )\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["After finally figuring out how to get TF2 to stay in eager mode, allowing me to attach a debugger, I finally found where this unknown loss is coming from.\r\nI have a kernel regularizer (L1L2) on one of the layers in the model, and as per [the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer):\r\n\r\n> These penalties are summed into the loss function that the network optimizes.\r\n\r\nAnd if we look at the value:\r\n```python\r\n>>> model.layers[4].losses\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=2942.604>]\r\n```\r\n\r\nSo I think that answers the issue. It is documented, just not on the model documentation page.\r\n\r\nIt still seems a little odd to me that the regularization loss is so extremely high. If this is expected, then I guess this issue can be closed out.\r\n\r\nEdit: Closing out anyway, as I doubt this is a bug, and so not suitable for the issue tracker."]}, {"number": 36005, "title": "Compiling tf.keras.Model with hub.KerasLayer fails in distributed scope", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Debian 9.11 (Google Cloud tf2-latest-gpu image)\r\n- TensorFlow installed from: Preinstalled on Google Cloud image\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de\r\n- Python version: 3.5.3\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: None (Using Google Cloud TPU v3-8)\r\n\r\n**Describe the current behavior**\r\nWhen compiling a tf.keras.Model that includes a hub.KerasLayer (tensorflow hub), it fails to compile in a distribution strategy scope:\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-23-7b06371889c9> in <module>\r\n     16         optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\r\n     17         loss=tf.keras.losses.binary_crossentropy,\r\n---> 18         metrics=[\"accuracy\"]\r\n     19     )\r\n\r\n1 frames\r\n/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    469                 'with strategy.scope():\\n'\r\n    470                 '  model=_create_model()\\n'\r\n--> 471                 '  model.compile(...)'% (v, strategy))\r\n    472 \r\n    473   @trackable.no_automatic_dependency_tracking\r\n\r\nValueError: Variable (<tf.Variable 'bert/embeddings/word_embeddings:0' shape=(119547, 768) dtype=float32>) was not created in the distribution strategy scope of (<tensorflow.python.distribute.tpu_strategy.TPUStrategy object at 0x7f4df23cddd8>). It is most likely due to not all layers or the model or optimizer being created outside the distribution strategy scope. Try to make sure your code looks similar to the following.\r\nwith strategy.scope():\r\n  model=_create_model()\r\n  model.compile(...)\r\n```\r\n\r\n**Describe the expected behavior**\r\nModel should be able to compile.\r\n\r\n**Code to reproduce the issue**\r\nCode used to create scope:\r\n```python3\r\nTPU_ADDRESS = \"grpc://\" + \"10.0.0.2:8470\"\r\n\r\nwith tf.compat.v1.Session(TPU_ADDRESS) as session:\r\n    print('TPU devices:')\r\n    pprint.pprint(session.list_devices())\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\r\ntry:\r\n    tf.config.experimental_connect_to_cluster(resolver)\r\nexcept tf.errors.UnimplementedError as uie:\r\n    print(uie, \"This appears to be caused by the TPU already being connected. Ignoring.\", sep='\\n')\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n```\r\n\r\nCode used to compile model:\r\n```python3\r\nwith tpu_strategy.scope():\r\n    in_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\", dtype=np.int32)\r\n    in_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\", dtype=np.int32)\r\n    in_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\", dtype=np.int32)\r\n    bert_inputs = {\"input_ids\": in_id, \"input_mask\": in_mask, \"segment_ids\": in_segment}\r\n\r\n    bert_layer = hub.KerasLayer(BERT_MODEL_HUB, signature=\"tokens\", output_key=\"pooled_output\")(bert_inputs)\r\n    bert_layer.trainable = True\r\n\r\n    dense = tf.keras.layers.Dense(256, activation='relu')(bert_layer)\r\n    pred = tf.keras.layers.Dense(len(unique_labels), activation='sigmoid')(dense)\r\n\r\n    model = tf.keras.Model(inputs=bert_inputs, outputs=pred)\r\n\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\r\n        loss=tf.keras.losses.binary_crossentropy,\r\n        metrics=[\"accuracy\"]\r\n    )\r\n```\r\n\r\n**Other info / logs**\r\nPreviously opened issue with TF Hub here: https://github.com/tensorflow/hub/issues/469\r\n\r\nGoogle Cloud TPU v3-8 is running TPU Software 2.1\r\n", "comments": ["I get almost the same error message when trying to use an ALBERT model with a GPU through a one device strategy:\r\n```\r\nValueError: Variable (<tf.Variable 'bert/embeddings/word_embeddings:0' shape=(30000, 128) dtype=float32>) was not created in the distribution strategy scope of (<tensorflow.python.distribute.one_device_strategy.OneDeviceStrategyV1 object at 0x7f50cac42f60>). It is most likely due to not all layers or the model or optimizer being created outside the distribution strategy scope. Try to make sure your code looks similar to the following.\r\nwith strategy.scope():\r\n  model=_create_model()\r\n  model.compile(...)\r\n```", "Could you refer to this piece of code example? https://github.com/tensorflow/models/blob/master/official/nlp/bert_models.py#L370\r\nYou need to make hub.KerasLayer trainable", "> Could you refer to this piece of code example? https://github.com/tensorflow/models/blob/master/official/nlp/bert_models.py#L370\r\n> You need to make hub.KerasLayer trainable\r\n\r\nIn my code above I use these two lines to make the layer trainable:\r\n```python3\r\nbert_layer = hub.KerasLayer(BERT_MODEL_HUB, signature=\"tokens\", output_key=\"pooled_output\")(bert_inputs)\r\nbert_layer.trainable = True\r\n```\r\n\r\nHowever, I have also tried it this way:\r\n```python3\r\nbert_layer = hub.KerasLayer(BERT_MODEL_HUB, signature=\"tokens\", output_key=\"pooled_output\", trainable=True)(bert_inputs)\r\n```\r\n\r\nBoth methods result in the same error.", "Is there any update on this? Just wanted to check in being as it's been two weeks now.", "Can you try these tf2 hub modules here: https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/1?\r\n\r\nThe Albert modules from the TF2 model code are publishing soon.\r\n\r\nThe example here: https://github.com/tensorflow/models/blob/master/official/nlp/bert_models.py#L370\r\nshould work for TF 2.1", "@saberkun the new models do seem to work. I was just able to load and train a model using this TF hub model loaded as a `KerasLayer`: [https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1](https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1).\r\n\r\nThe example link you posted gives me a 404 error however.\r\n\r\nSo is the bug just an issue of lacking backwards compatibility?", "I don't think we support loading TF1 hub model from TF2, maybe someone from TF hub team can confirm.", "@rxsang it loooks like it should be possible based on this: [https://www.tensorflow.org/hub/migration_tf2](https://www.tensorflow.org/hub/migration_tf2)", "@uchua Looks like issue is resolved. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36005\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36005\">No</a>\n", "@saberkun was the conclusion here that the TFHub SavedModel had to be in TF2 format? There is an open issue at https://github.com/tensorflow/hub/issues/295 for a problem similar if not identical to this one. I am experiencing this same problem when I use the Efficientnet-lite TF1 SavedModel from TFHub on AI Platform runtime 2.3 with TPUs. In my case, this model is not available in TF2 format. Is it not possible to use TF1 SavedModels with TF2 distributed scope, and is there someone specific I should ask about making a TF2 version available?"]}, {"number": 36004, "title": "`python': double free or corruption", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n**Scientific Linux 7.7 (Nitrogen) \r\nLinux 3.10.0-1062.9.1.el7.x86_64** \r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below):\r\n**v1.15.0-rc3-22-g590d6ee 1.15.0**\r\nSame problem occurs with tf 1.14\r\n- Python version:\r\n**3.6**\r\n- Bazel version (if compiling from source):\r\n**NA**\r\n- GCC/Compiler version (if compiling from source):\r\n**4.8.5**\r\n- CUDA/cuDNN version:\r\n**NA, running on CPU**\r\n- GPU model and memory:\r\n**NA**\r\n\r\n**Describe the current behavior**\r\n\r\nI am using PBS to submit jobs to a homogeneous cluster. Each job is allocated up to 192GB memory, and the job itself uses only 50GB at peak. Each job is a loop that iteratively solves around 1000 Gaussian process regression problems using TF. In pseudo-code it is,\r\n``` python\r\ndef job():\r\n  N=1000\r\n  for i in range(N):\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n      solve_gp_problem(sess, sess.graph)\r\n      # around i ~ 300 we get `python': double free or corruption\r\n      # but only in ~50% of the jobs that are submitted (with the exact same data)\r\n```\r\nEach Gaussian process problem creates its own graph and session, and lasts approximately 10 seconds. Then the graph and session are not referenced again (and should be garbage collected). The job uses no multiprocessing or threading outside of TF's inherent use. \r\n\r\nApproximately 50% of my jobs are abruptly failing with ``python': double free or corruption` and a long stack trace, after approximately 1/3 of the work is done. This happens also when I give the jobs the exact same input data.\r\n\r\n**Describe the expected behavior**\r\n\r\nSome memory is trying to be freed twice in tensorflow, which should not happen. \r\n\r\n**Code to reproduce the issue**\r\nThis error cannot be deterministically reproduced since it doesn't occur all of the time. However, it looks like this,\r\n``` python\r\nimport tensorflow.compat.v1 as tf\r\ndef loss(x):\r\n  return tf.reduce_sum(x ** 2)\r\n\r\ndef loss_and_grad(x):\r\n  l = loss(x)\r\n  return l, tf.gradients(l, [x])[0]\r\n\r\ndef build_loss_np(sess):\r\n  x_pl = tf.placeholder(tf.float32)\r\n  l_g = loss_and_grad(x_pl)\r\n  def _loss(x):\r\n    return sess.run(l_g, {x_pl: x})\r\n  return _loss\r\n\r\nfrom scipy.optimize import minimize\r\nfor i in range(1000):\r\n  with tf.Session(graph=tf.Graph()) as sess:\r\n    loss_func = build_loss_np(sess)\r\n    # stand-in for GP hyper parameter optimisation and regression\r\n    res = minimize(loss_func, [0], method='BFGS', jac=True)\r\n```\r\n\r\n**Other info / logs**\r\nThe only consistent thing I notice is that the failure usually occurs around the same point after about 300 loops. Therefore there would be 300 graphs and sessions created.\r\nThe code that performs the gaussian process regression is a solid code base called GPFlow.\r\n[traceback.log](https://github.com/tensorflow/tensorflow/files/4079155/traceback.log)\r\n", "comments": ["@Joshuaalbert I tried running the code above in colab but ran into an error. Please take a look at it [here](https://colab.sandbox.google.com/gist/gowthamkpr/2cf51a1edacf960ff35581392bebd515/untitled277.ipynb) and let me know if you are facing the same issue. Thanks!", "I fixed the error in the colab. My bad, I placed the session in the wrong location.\r\nEdit: error that @gowthamkpr had in running the example was fixed, double free still a problem", "By \"the error\" do you mean the double free/corruption or just the error @gowthamkpr mentioned?\r\n\r\nIf the double free/corruption is fixed, can we close?", "Yes This error has been fixed @mihaimaruseac I am closing this issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36004\">No</a>\n", "@mihaimaruseac  @gowthamkpr please reopen this issue, as the problem is not resolved. The colab notebook that gow shared runs now that I fixed the code to do as I intended. But, please reread the issue report again. I said this is a non-deterministic problem. That is, it occurs in roughly 50% of jobs that I run, when they are run with identical input parameters.", "@Joshuaalbert I don't see any changes in my colab notebook. Can you please share  your colab gist. Thanks!", "@gowthamkpr I updated the first post to show the correct code.", "@Joshuaalbert I tried running it multiple times on colab but I didn't run into ``python': double free or corruption` error", "@gowthamkpr Indeed, this is non-deterministic. This problem requires thinking outside the normal MVCE box, and requires solving a puzzle by someone who really understands the TF system. The example that I provide (as stated in the issue) is the conceptual program. The clues that I can provide you are: \r\n - it occurs about 50% of the time \r\n - it does not occur right away, but after several hundred loops in the conceptual program\r\n - it will have created a session and graph each iteration of the loop, each of which should be garbage collected.\r\n - that a memory pointer is trying to be freed twice, which produces the error\r\n - it occurs on a cluster, where every node only has one job running on it\r\n - the job is containerized, yet the error does not occur on another cluster \r\n\r\nQuestions:\r\n - How does tensorflow deal with garbage collection?\r\n - Is there some reason that different CPUs might cause problems with memory allocation/freeing?\r\n - If you look at the traceback, can you find any peculiar things?", "@Joshuaalbert It may because your OS is too old for latest NumPy.\r\n\r\nTry `pip3 install --ignore-installed --no-binary=:all: numpy==1.16.2`.", "@Joshuaalbert if we cannot reproduce it we cannot provide a fix for it. The example you provide should be as complete as possible", "@byronyi I will try this now. The jobs take several hours to complete, so I should know in a few hours if this resolves the issue. I have to run a large batch to be certain.\r\n\r\n@mihaimaruseac Not all computer problems are deterministic, yet that does not rule out attempting to resolve them. A good blog post on it is [this](https://www.tablix.org/~avian/blog/archives/2014/10/on_hunting_non_deterministic_bugs/), and an even better paper about it is [this](https://www.hindawi.com/journals/sp/2018/8939027/).\r\nEdit: I do appreciate and understand your point though!", "I know that, but thank you for the links.\r\n\r\nAs some background, I'm working on creating better tools for debugging these issues. But don't have something usable at the moment.", "@mihaimaruseac That interesting. Would a real-time tool like [osquery](https://osquery.io/) be useful for logging things before they fail?", "@byronyi So it seems that did not work. Should numpy be 1.16.2 always for stability? Anyways, where next can I look?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36004\">No</a>\n"]}, {"number": 36003, "title": "ImportError: DLL Load failed on tensorflow 2.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (Windows10 64 bit):\r\n\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version:2.1\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: I have cuda runtime 8.0 and 10.1 (don't see cuDNN installed in the list, but at one point I could run tensorflow. Not sure what broke since that was months ago that I last needed it.\r\n- GPU model and memory:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nEncountered the problem using Anaconda to create my environment as well. Anaconda has been uninstalled, so these steps were to try and fix the issue by starting at ground 0.\r\n\r\nI installed Python 3.7 from the python.org site (after uninstalling Anaconda which also had this issue)\r\nUsed command prompt to create virtual env using python -m venv name\r\npip install packagename one line per package in this order pandas, numpy, wheel, six, scikit-learn, tensorflow\r\nalso pip is version 19.3\r\nSo now my virutal env should be ready to go and I activated it.\r\npip list provides:\r\n(envNoShow) C:\\Users\\USERNAME\\AllProjects\\noShow>pip list\r\nPackage              Version\r\n-------------------- ----------\r\nabsl-py              0.9.0\r\nastor                0.8.1\r\ncachetools           4.0.0\r\ncertifi              2019.11.28\r\nchardet              3.0.4\r\ngast                 0.2.2\r\ngoogle-auth          1.10.1\r\ngoogle-auth-oauthlib 0.4.1\r\ngoogle-pasta         0.1.8\r\ngrpcio               1.26.0\r\nh5py                 2.10.0\r\nidna                 2.8\r\njoblib               0.14.1\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.0\r\nMarkdown             3.1.1\r\nnumpy                1.18.1\r\noauthlib             3.1.0\r\nopt-einsum           3.1.0\r\npandas               0.25.3\r\npip                  19.3.1\r\nprotobuf             3.11.2\r\npyasn1               0.4.8\r\npyasn1-modules       0.2.8\r\npython-dateutil      2.8.1\r\npytz                 2019.3\r\nrequests             2.22.0\r\nrequests-oauthlib    1.3.0\r\nrsa                  4.0\r\nscikit-learn         0.22.1\r\nscipy                1.4.1\r\nsetuptools           41.2.0\r\nsix                  1.14.0\r\ntensorboard          2.1.0\r\ntensorflow           2.1.0\r\ntensorflow-estimator 2.1.0\r\ntermcolor            1.1.0\r\nurllib3              1.25.7\r\nvirtualenv           16.7.9\r\nWerkzeug             0.16.0\r\nwheel                0.33.6\r\nwrapt                1.11.2\r\n\r\n\r\n\r\nran the command python -c \"import tensorflow as tf\" and get error in the log file.\r\n\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n(envNoShow) C:\\Users\\USERNAME\\AllProjects\\noShow>python -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\USERNAME\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\USERNAME\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\USERNAME\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\USERNAME\\AllProjects\\noShow\\envNoShow\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\USERNAME\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\USERNAME\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Did you install Microsoft Visual C++ Redistributable for Visual Studio 2019?\r\nSee https://visualstudio.microsoft.com/vs/older-downloads/", "@brian1985 \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36003\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36003\">No</a>\n"]}, {"number": 36002, "title": "Allow Optimizers to perform global gradient clipping", "body": "Fixes #36001", "comments": ["@artemmavrin Can you please resolve conflicts? Thanks!", "@gbaned Conflicts removed!", "@fchollet @omalleyt12, can you review? I'm not sure how feasible it is to change this behavior at this point, but as #36001 mentions, this does make the behavior more consistent with standalone Keras.", "I **think** we should fix it", "@tanzhenyu Any update on this PR? Please. Thanks!", "@artemmavrin Thanks for the PR!\r\n\r\nChanging clipping to do clipping by global norm would be a backwards-incompatible change. However, we're in the process of making it much easier to provide custom gradient clipping and other gradient modifications, please check out the RFC here:\r\n\r\nhttps://github.com/tensorflow/community/pull/234"]}, {"number": 36001, "title": "Allow Optimizers to perform global gradient clipping", "body": "**System information**\r\n- TensorFlow version: 2.1\r\n- Are you willing to contribute it: Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, passing `clipnorm` to a `tf.keras.optimizers.Optimizer` makes it clip the gradient for each weight tensor locally, or *independently of other weight gradients*:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d8bcf5e4a24f9904e9a97b41da393e53149e999e/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L357-L358\r\n\r\nI am proposing clipping the weight gradient *globally*, i.e.,\r\n\r\n```python\r\n    if hasattr(self, 'clipnorm'):\r\n      grads, _ = clip_ops.clip_by_global_norm(grads, self.clipnorm)\r\n```\r\n\r\nThis was already proposed in #29108 and #29114.\r\n\r\nThis change has at least two benefits:\r\n\r\n1. Theoretical: the per-batch descent direction is preserved when gradients are clipped globally, but not when clipped locally.\r\n2. Practical: [standalone Keras implements global gradient clipping](https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/optimizers.py#L98-L100):\r\n    ```python\r\n        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\r\n            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\r\n            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\r\n    ```\r\n    so this change will unify `keras` and `tf.keras` behavior.\r\n\r\nIn fact, clipping by global norm is the behavior I *expected* when I first learned about the `clipnorm` parameter. The documentation does not clarify whether gradients are clipped locally or globally:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d8bcf5e4a24f9904e9a97b41da393e53149e999e/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L241-L245\r\n\r\n**Will this change the current api? How?**\r\n\r\nThe API will remain the same. Only the weight update behavior will change when the user passes `clipnorm` to an optimizer.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers who use `clipnorm` with their optimizers.\r\n\r\n**Any Other info.**\r\n\r\nOne of the first papers introducing gradient clipping by norm is [Pascanu, Mikolov, Bengio (2013)](http://proceedings.mlr.press/v28/pascanu13.html), where they propose\r\n![Screen Shot 2020-01-17 at 12 28 38 PM](https://user-images.githubusercontent.com/12015989/72644067-f299e800-3924-11ea-8a27-2c4d48880e52.png)\r\n\r\nIn Section 1.1, \u03b8 is defined to be the collection of *all* weights in the model, not just a particular weight matrix or bias vector. This addresses [a question](https://github.com/tensorflow/tensorflow/pull/29114/files#r294862824\r\n) In #29114 about whether the paper proposes global clipping or local clipping. Moreover, following Algorithm 1, Pascanu *et al* write,\r\n\r\n> we only diverged from the original proposal in an attempt to provide a better theoretical justification (see section 2.3; we also move in a descent direction for the current mini-batch)\r\n\r\nso they appear to be advocating altering the gradient in a way that preserves the descent direction (i.e., using global clipping).\r\n\r\nOther recent sources also appear to suggest *global* gradient clipping (e.g., see equation (5) of [this ICLR 2020 paper](https://openreview.net/forum?id=BJgnXpVYwS)).", "comments": ["With TF 2.2, optimizer.apply_gradients() can take an argument all_reduce_sum_gradient=False, which means that apply_gradients() will no longer implicitly aggregate gradients. Users can write custom training loop code by manually aggregating gradients (see [1]) and performing clip_by_global_norm or any other gradient/loss customization (either before or after gradient allreduce).\r\n\r\n[1] https://github.com/tensorflow/models/blob/master/official/staging/training/grad_utils.py \r\n\r\nThis is one step toward making the optimizer core focusing on the math of updating model and optimizer variables, so researchers and users could easily modify, subclass, and evolve.\r\n\r\nIn future release, Keras compile and fit will also support users-specified customize gradient operation. @omalleyt12 @fchollet "]}, {"number": 36000, "title": "Keras Hub Layer Does Not Work with Functional API. Throws Exception.", "body": "TensorFlow Version: 2.1\r\nPython Version: 3.76\r\nOS: Windows 10\r\nIssue:\r\nKeras Hub Layer Does Not Work with Functional API. Throws Exception. Unable to load IMDB dataset. See below code and exception:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, Concatenate, RepeatVector, Activation, Dot, Bidirectional, Flatten, Embedding, Input, SpatialDropout1D, LSTM, Dropout, Lambda, Conv2D, Conv1D, Attention, AdditiveAttention, GlobalAveragePooling1D, TimeDistributed, AveragePooling1D\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.backend import backend\r\nimport tensorflow_hub as hub\r\nfrom tensorflow.keras.datasets import imdb\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\nimport numpy as np\r\nimport tensorflow_datasets as tfds\r\n\r\ndef cnn_classifier():\r\n    # Encode each timestep\r\n    # embedding = Embedding(10000, 300, trainable=True)(input)\r\n\r\n    embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\r\n    input = Input(shape=(None,), name=\"Input\")\r\n    hub_layer = hub.KerasLayer(embedding, trainable=True)(input)\r\n    cnn = Conv1D(64,3, padding=\"same\", activation=\"relu\")(hub_layer)\r\n    cnn = Conv1D(32, 3, padding=\"same\", activation=\"relu\")(cnn)\r\n    cnn = Conv1D(16, 3, padding=\"same\", activation=\"relu\")(cnn)\r\n    cnn = Flatten()(cnn)\r\n    output = Dense(1, activation=\"sigmoid\")(cnn)\r\n\r\n    model = Model(input, output)\r\n    model.compile(loss=\"binary_crossentropy\",\r\n                  optimizer=\"adam\",\r\n                  metrics=[\"accuracy\"])\r\n    model.summary()\r\n    return model\r\n\r\nmodel = cnn_classifier()\r\n\r\ntrain_validation_split = tfds.Split.TRAIN.subsplit([8, 2])\r\n\r\n(train_data, validation_data), test_data = tfds.load(\r\n    name=\"imdb_reviews\",\r\n    split=(train_validation_split, tfds.Split.TEST),\r\n    as_supervised=True)\r\n\r\n#train_data = pad_sequences(train_data, maxlen=1100, dtype='int32', padding='post', truncating='post', value=0.0)\r\n#test_data = pad_sequences(test_data, maxlen=1100, dtype='int32', padding='post', truncating='post', value=0.0)\r\n\r\nmodel.fit(train_data.shuffle(25000).batch(512),\r\n                    epochs=20,\r\n                    validation_data=validation_data.batch(512),\r\n                    verbose=10)\r\n```\r\n**Exception:**\r\n```\r\n2020-01-17 15:46:42.273900: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nWARNING:tensorflow:AutoGraph could not transform <tensorflow.python.saved_model.function_deserialization.RestoredFunction object at 0x0000029880A70D88> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    Tensor(\"Input:0\", shape=(None, None), dtype=float32))\r\n  input_signature: (\r\n    TensorSpec(shape=(None,), dtype=tf.string, name=None))\r\nWARNING:tensorflow:AutoGraph could not transform <tensorflow.python.saved_model.function_deserialization.RestoredFunction object at 0x0000029880A70D88> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    Tensor(\"Input:0\", shape=(None, None), dtype=float32))\r\n  input_signature: (\r\n    TensorSpec(shape=(None,), dtype=tf.string, name=None))\r\nTraceback (most recent call last):\r\n  File \"C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/Classifiers.py\", line 47, in <module>\r\n    model = cnn_classifier()\r\n  File \"C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/Classifiers.py\", line 17, in cnn_classifier\r\n    hub_layer = hub.KerasLayer(embedding, trainable=True)(input)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 237, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_hub\\keras_layer.py:209 call  *\r\n        result = f()\r\n    C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\load.py:438 _call_attribute\r\n        return instance.__call__(*args, **kwargs)\r\n    C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py:568 __call__\r\n        result = self._call(*args, **kwds)\r\n    C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py:606 _call\r\n        results = self._stateful_fn(*args, **kwds)\r\n    C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py:2362 __call__\r\n        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n    C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py:2661 _maybe_define_function\r\n        *args, **kwargs)\r\n    C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py:2185 canonicalize_function_inputs\r\n        self._flat_input_signature)\r\n    C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py:2252 _convert_inputs_to_signature\r\n        format_error_message(inputs, input_signature))\r\n\r\n    ValueError: Python inputs incompatible with input_signature:\r\n      inputs: (\r\n        Tensor(\"Input:0\", shape=(None, None), dtype=float32))\r\n      input_signature: (\r\n        TensorSpec(shape=(None,), dtype=tf.string, name=None))\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@nectario ,\r\nAs per the description, issue belongs to [TF-hub](https://github.com/tensorflow/hub/issues/new). Can you raise issue in that repo?Thanks!", "Ok, will raise it there.", "For those interested the issue is raised here:\r\n\r\n[#483 TF Hub ](https://github.com/tensorflow/hub/issues/483)\r\n"]}, {"number": 35999, "title": "Fix typo `padding_shapes` --> `padded_shapes`", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35999) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35999) for more info**.\n\n<!-- ok -->", "Please open against `master` branch, not a release branch. When we do releases, we cherry-pick from `master` to `r2.x` branch but after `tensorflow==2.x` has been released we don't merge `r2.x` back into master (instead, we keep it for patch releases, but note that patch releases are only for critical issues)."]}, {"number": 35998, "title": "Change collection.Sequence to collection.abc.Sequence in keras recurrent.py (with python2 switch)", "body": "see https://github.com/tensorflow/tensorflow/pull/35895#issuecomment-575684267\r\n@gunan \r\n", "comments": ["@ulf1 Can you please check @qlzh727's comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 35997, "title": "Add 16 bit support to kernel operator TRANSPOSE_CONVOLUTION", "body": "This PR is one of steps to extend TFLite to support symmetric 16-bit activations.\r\n\r\nIn this PR we introduce implementation and tests for TRANSPOSE_CONV kernel reference function.\r\nThe specification of this operator:\r\n\r\nTRANSPOSE_CONVOLUTION\r\n\u202f Input 0:\r\n\u202f \u202f data_type \u202f: int16\r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767]\r\n\u202f \u202f granularity: per-tensor, zero_point=0\r\n\u202f Input 1 (Weight):\r\n\u202f \u202f data_type \u202f: int8\r\n\u202f \u202f range \u202f \u202f \u202f: [-127, 127]\r\n\u202f \u202f granularity: per-axis (dim = 0), zero_point=0\r\n\u202f Output 0:\r\n\u202f \u202f data_type \u202f: int16\r\n\u202f \u202f range \u202f \u202f \u202f: [-32768,32767]\r\n\u202f \u202f granularity: per-tensor, zero_point=0", "comments": ["This PR is relative to [TransposeConv with Bias](https://github.com/tensorflow/tensorflow/pull/34903)\r\nI can update this PR if the other merged, or update the other PR if this merged first.\r\nWe can discuss how to resolve it.", "Hi @suharshs @jdduke\r\nCould you please review this?\r\nAs discussed, this is one of the INT16 reference kernel reference function.\r\nThanks!", "~~I have a look on `Windows Bazel` build and ` Windows Bazel GPU` build:~~\r\n~~none of the logs seems failing relative to this PR~~\r\n~~https://source.cloud.google.com/results/invocations/ea48732b-6791-400f-9343-274a6bdce49f/log~~\r\n~~https://source.cloud.google.com/results/invocations/1f43a2e1-15b2-4cbf-b0c8-a098297e2eed/log~~\r\n\r\n~~@suharshs @rthadur can you help to have a look?~~\r\n~~Shall I rebase this PR?~~", "I have a look on Windows Bazel build and Windows Bazel GPU build:\r\nnone of the logs seems failing relative to this PR\r\nhttps://source.cloud.google.com/results/invocations/ea48732b-6791-400f-9343-274a6bdce49f/log\r\nhttps://source.cloud.google.com/results/invocations/1f43a2e1-15b2-4cbf-b0c8-a098297e2eed/log\r\n\r\n@suharshs @rthadur can you help to have a look?\r\nShall I rebase this PR?\r\n", "I am still  making some internal tests pass with this change. Should be in\nsoon.\n\nOn Wed, Feb 26, 2020 at 3:11 AM Peng Sun <notifications@github.com> wrote:\n\n> I have a look on Windows Bazel build and Windows Bazel GPU build:\n> none of the logs seems failing relative to this PR\n>\n> https://source.cloud.google.com/results/invocations/ea48732b-6791-400f-9343-274a6bdce49f/log\n>\n> https://source.cloud.google.com/results/invocations/1f43a2e1-15b2-4cbf-b0c8-a098297e2eed/log\n>\n> @suharshs <https://github.com/suharshs> @rthadur\n> <https://github.com/rthadur> can you help to have a look?\n> Shall I rebase this PR?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35997?email_source=notifications&email_token=AALCE5VPJW6WKJR3ARMSL3LREZE57A5CNFSM4KINAG5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM72BZI#issuecomment-591372517>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALCE5RVGPSYVKM6VBMGD6TREZE57ANCNFSM4KINAG5A>\n> .\n>\n", "@suharshs gentle ping for new status ?", "Can we rebase? I think there are some stale errors for this one.", "Here is the internal error , iam not sure if it is related , can you please verify \r\n\r\n`third_party/tensorflow/lite/kernels/test_util.cc:330\r\nExpected equality of these values:\r\n  CountPartitionsDelegatedTo(interpreter_.get(), TestNnApiDelegate())\r\n    Which is: 0\r\n  1\r\nExpecting operation to be accelerated but cannot find a partition associated to the NNAPI delegate`", "I think we just need to blacklist the new int16 tests @ /tensorflow/lite/delegates/nnapi/acceleration_test_list.cc. ", "I'm wondering, is add a version to int16 TRANSPOSE_CONV gonna be helpful?", "@psunn Can you please resolve conflicts? Thanks!", "@gbaned thanks for the reminder. done\r\n\r\n@jdduke @suharshs  Can you please have a review? thanks!", "@gbaned thanks for reminding me, done\r\n@jdduke @suharshs can you please review it? thanks!", "@jdduke sorry to bother you, can you have a review and merge if it's ok?"]}, {"number": 35996, "title": "Add 16 bit support to kernel operator SOFTMAX", "body": "This PR is one of steps to extend TFLite to support symmetric 16-bit activations.\r\n\r\nIn this PR we introduce implementation and tests for SOFTMAX kernel reference function.\r\nThe specification of this operator:\r\n\r\nSOFTMAX\r\n\u202f Input 0:\r\n\u202f \u202f data_type \u202f: int16\r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767]\r\n\u202f \u202f granularity: per-tensor, zero_point=0\r\n\u202f Output 0:\r\n\u202f \u202f data_type \u202f: int16\r\n\u202f \u202f range \u202f \u202f \u202f: [-32768,32767]\r\n\u202f \u202f granularity: per-tensor, zero_point=0", "comments": ["Hi @suharshs @jdduke \r\nCould you please review this?\r\nAs discussed, this is one of the INT16 reference kernel reference function.\r\nThanks!", "@psunn Can you please address the reviewer comments and resolve conflicts? Thanks!", "Hi @gbaned , thanks for remind me.\r\nI will resolve it soon.", "Sorry, but please don't merge this change for now, thanks.", "Just to confirm, this is ready for review?", "> Just to confirm, this is ready for review?\r\n\r\nHi Jared, no it's still WIP.\r\nalternative implementation is still under discussion internally. Hopefully will update soon.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@tensorflowbutler\r\nI'm currently working on this PR, should be updated very soon.\r\nThanks!", "This PR is ready to review, please have a look, thanks!\r\n@jdduke @suharshs @jianlijianli ", "I update with minor changes to addressing the comments.\r\nPlease have a review, and hopefully re-approve if no issues, thanks.\r\n@jianlijianli ", "Updated to addressing comments. Please re-approve if no issues.\r\n@rthadur @jianlijianli , thanks.", "Actually, there are a number of internal failures, I'll try to fix them manually.", "@psunn Looks like this PR breaks my application because std::round is not available on my system. Can we use TfLiteRound as defined in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/cppmath.h ?", "> Actually, there are a number of internal failures, I'll try to fix them manually.\r\n\r\nThanks for the fix them!", "> @psunn Looks like this PR breaks my application because std::round is not available on my system. Can we use TfLiteRound as defined in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/cppmath.h ?\r\n\r\nI raised a [PR](https://github.com/tensorflow/tensorflow/pull/38604), does it solve the problem?@Abhipray \r\n\r\n", "> > @psunn Looks like this PR breaks my application because std::round is not available on my system. Can we use TfLiteRound as defined in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/cppmath.h ?\r\n> \r\n> I raised a [PR](https://github.com/tensorflow/tensorflow/pull/38604), does it solve the problem?@Abhipray\r\n\r\nYes, thank you!"]}, {"number": 35995, "title": "The summary method could be changed for improved understandability and readability", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Maybe\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nConsider the following program\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\n\r\ndef get_model():\r\n    inp = tf.keras.layers.Input(shape=(1,))\r\n    x = tfp.layers.DenseFlipout(8)(inp)\r\n    x = tfp.layers.DenseFlipout(16)(x)\r\n    out = tfp.layers.DenseFlipout(1)(x)\r\n    model = tf.keras.Model(inputs=inp, outputs=out)\r\n    model.summary()\r\n    return model\r\n\r\n\r\nif __name__ == '__main__':\r\n    get_model()\r\n```\r\n\r\nwhich produces the following output\r\n\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 1)]               0         \r\n_________________________________________________________________\r\ndense_flipout (DenseFlipout) (None, 8)                 24        \r\n_________________________________________________________________\r\ndense_flipout_1 (DenseFlipou (None, 16)                272       \r\n_________________________________________________________________\r\ndense_flipout_2 (DenseFlipou (None, 1)                 33        \r\n=================================================================\r\nTotal params: 329\r\nTrainable params: 329\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nThe first layer has 24 parameters because there are 8 means, 8 standard deviations and 8 biases, for a total of `8+8+8=24`. Similarly, the second contains 272 parameters because there are `8*16` means \r\n `8*16` standard deviations and 16 biases. And the last layer contains 16 means, 16 standard deviations and 1 biases.  Now, in this case, I have domain knowledge and I was able to understand these numbers, but, in certain cases, for example, when Bayesian convolution layers are used, the number of parameters of certain layers may not be obvious. \r\n\r\nTherefore, if possible, it would be nice that `my_model.summary()` provided a more detailed description of the number of parameters for each layer (e.g. the actual calculation with a description).  This feature may be implemented by providing a `verbose` parameter for the `summary` method, which allows us to set the verbosity level of the output.\r\n\r\nFurthermore, there are cases where the output is also cut. For example, consider the following output of the `summary` method\r\n\r\n```\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ncamera (InputLayer)             [(None, 64, 80, 3)]  0                                            \r\n__________________________________________________________________________________________________\r\nconv2d_flipout (Conv2DFlipout)  (None, 64, 80, 8)    441         camera[0][0]                     \r\n__________________________________________________________________________________________________\r\nactivation (Activation)         (None, 64, 80, 8)    0           conv2d_flipout[0][0]             \r\n__________________________________________________________________________________________________\r\nmax_pooling2d (MaxPooling2D)    (None, 32, 40, 8)    0           activation[0][0]                 \r\n__________________________________________________________________________________________________\r\nflatten (Flatten)               (None, 10240)        0           max_pooling2d[0][0]              \r\n__________________________________________________________________________________________________\r\ndense_flipout (DenseFlipout)    (None, 512)          10486273    flatten[0][0]                    \r\n__________________________________________________________________________________________________\r\ntarget_locations (DenseFlipout) (None, 289)          296226      dense_flipout[0][0]              \r\n__________________________________________________________________________________________________\r\ntarget_rhos (DenseFlipout)      (None, 289)          296226      dense_flipout[0][0]              \r\n__________________________________________________________________________________________________\r\ntarget (DistributionLambda)     ((None, 289), (None, 0           target_locations[0][0]           \r\n                                                                 target_rhos[0][0]                \r\n==================================================================================================\r\n```\r\n\r\nIn this case, the output partially cuts the output shape of the last layer, so there should also be a way of adjusting the style of the output of `summary`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nI am not sure.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone.", "comments": ["@nbro ,\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/35515#issuecomment-571266052) and let us know if it helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Couldn't summary method just take an extra argument to provide more verbose info optionally? ", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35994, "title": "Expand registered kernels for variable ops on GPU", "body": "**System information**\r\n- TensorFlow version (you are using):1.14/1.15\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe current variable op on GPU is restricted to float, double, and int64.  Through the TF_CALL_GPU_NUMBER_TYPES and\r\nTF_CALL_int64 macros.\r\n\r\nThe notes say to restrict the dtype to those supported by the assignment op.  The assignment op has a broader range of dtypes registered through the\r\nTF_CALL_GPU_ALL_TYPES and \r\nTF_CALL_int64 macros.\r\n\r\nThis disallows users to store boolian data with small memory footprints and forces unnecessary casts.  For that matter int32's and half precision could probably also be added.\r\n\r\nIts a very easy change to make.  However, I am not sure if there is further rational for the restricted types.\r\n\r\n**Will this change the current api? How?**\r\nNothing other than allowing more dtypes to be passed into variable constuctors\r\n\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to lower variable memory footprint \r\n\r\n\r\n**Any Other info.**\r\n", "comments": ["anybody care to comment as to why the variables are limited? our testing so far is showing no compatibility issues.", "Replacing the registration to include TF_CALL_GPU_ALL_TYPES SGTM. @sanjoy for FYI.", "Can you be more specific on which ops you want to modify?  For instance, I'm not sure if we should be calling `TF_CALL_GPU_ALL_TYPES` on `AssignAddVariableOp` since we can't add `bool`s.", "@sanjoy I would prefer to see TF_CALL_GPU_ALL_TYPE on the variableop and assignop (as well as int32) to allow users to create and store it and then limit the assignAdd and/or assignSub to datatypes which are sensible.  That way users have freedom to create and store variables with limited memory use and can then use assignAdd/Sub etc as needed.  So assignAdd/Sub would be left alone (unless you wanted to add int32s to it).  Is it necessary for the creation/assigment to match assignAdd/Sub for some reason?  If not, it would make the code much more memory efficient in certain circumstances.", "@sanjoy Also, if you look at the existing code, the assignop already has TF_CALL_GPU_ALL_TYPE registration and the variableOp TF_CALL_GPU_NUMBER_TYPE.  IMHO, they should both be TF_CALL_GPU_ALL_TYPE and also include TF_CALL_INT32", "> @sanjoy I would prefer to see TF_CALL_GPU_ALL_TYPE on the variableop\r\n\r\nJust to be clear, you're talking about the `Variable` and `VariableV2` op, *not* `VarHandleOp`?", "@sanjoy yes", "> @sanjoy yes\r\n\r\nOk, that SGTM.  Are you going to send a PR?", "I can.  This is my first PR.  Will tests be run on servers or do I have to run tests locally?", "> I can. This is my first PR. Will tests be run on servers or do I have to run tests locally?\r\n\r\nThe PR won't be merged unless all tests pass (this is automatically enforced).  However it is usually better  to run tests locally so that your PR does not get blocked.", "Added a PR  #38848 to use  TF_CALL_GPU_ALL_TYPES."]}, {"number": 35993, "title": "variable_ops.h missing from pip install", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14.0\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?: Pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: p100\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen building custom ops, we want to link against pip installation for end user simplicity.  We were testing expanding the registered dtypes for variableson GPUs through subclassing.  We found that the header for the variable op is missing from the pip installation.  We had to copy the file locally from source and reference it in our cuatom op code which is undesirable from a maintenance perspective.  Can the file be added to the proper place in the includes?", "comments": ["@gunan, do you think headers for classes like VariableOp under core/kernels/ should be included?", "I definitely do not want any headers under `core/kernels` in the pip package.\r\nIf there are libraries there we would like to use for custom kernels, they should be turned into libraries and moves outside core/kernels.", "@gunan May I ask why the headers should not be included?  I'm not trying to argue, I am just curious and somewhat ignorant.  Is there a way that we can make the core kernels accessible from custom ops with a standard install?  For our purposes it is best to subclass and expand capability but it is difficult to make the workflow easy if the headers are not there.", "`core/kernels` by design are just our implementations of kernels. Most libraries there have not been designed to be reused. So in our code structure, they are meant to be hidden symbols not used by others.\r\n\r\nMoreover, on windows, we have to limit the number of symbols we export. This is because windows puts a limit on number of exported symbols from shared objects. So that folder is completely excluded.\r\n\r\nAgain, if you like to use any libraries under core/kernels, you may refactor the code out of that directory.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35993\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35993\">No</a>\n"]}, {"number": 35992, "title": "Keras TextVectorization AttributeError during save_weights", "body": "**System information**\r\n- Colab code: https://colab.research.google.com/drive/1Lf1s9O6Z5ss33GEtb4UXEgjq4wiHw7iy\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04/19.10, Google Colab\r\n- TensorFlow installed from (source or binary): pypi binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nAttributeError when calling save_weights on a trained model.\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in <listcomp>(.0)\r\n   3274   \"\"\"\r\n   3275   if context.executing_eagerly():\r\n-> 3276     return [x.numpy() for x in tensors]\r\n   3277   elif ops.inside_function():  # pylint: disable=protected-access\r\n   3278     raise RuntimeError('Cannot get value inside Tensorflow graph function.')\r\n\r\nAttributeError: 'TrackableWeightHandler' object has no attribute 'numpy'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nModel weights can be saved\r\n\r\n**Code to reproduce the issue**\r\n\r\nI simply modified Fran\u00e7ois Chollet's (@fchollet) collab and added save_weights at the end:\r\n\r\nhttps://colab.research.google.com/drive/1Lf1s9O6Z5ss33GEtb4UXEgjq4wiHw7iy#scrollTo=ZXQKEh53v948\r\n\r\nOriginal colab here:\r\n\r\nhttps://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3\r\n\r\n\r\n", "comments": ["@tripzero As the error mentioned, please use 'tf' format to save the model. \r\n\r\n> NotImplementedError: Save or restore weights that is not an instance of `tf.Variable` is not supported in h5, use `save_format='tf'` instead. Got a model or layer TextVectorization with weights [<tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler object at 0x7f79294bd780>]\r\n\r\nWhen I replaced model saving part as shown below, everything worked as expected.\r\n`model.save_weights(\"foobar_tf\",save_format='tf')`\r\n\r\nPlease take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/b1d3020b6e7adfd22082306b5388a94e/textvectorization-example.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "The TF format should be the default going forward. We will try to clarify the error message so that it's more obvious what the solution is.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35992\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35992\">No</a>\n", "How can I implement tihs inside a ModelCheckpoint? Im having difficulties with it", "@sacha-buengueroff As this is an old issue, can you please open a new issue with simple standalone code to reproduce the issue? Thanks!"]}, {"number": 35991, "title": "[ROCm][XLA] Enabling llvm compiler test", "body": "This CL enabled the `llvm_compiler_test` to test the dummy implementation of `GpuCompiler`.\r\n\r\n@whchung @cheshire ", "comments": ["@cheshire gentle ping to review new changes , thank you"]}, {"number": 35990, "title": "Text Classification RNN tutorial doesn't run under TF2.1", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/text/text_classification_rnn\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe sample for LSTM-based text classification doesn't run under Tensorflow 2.1 anymore.\r\n\r\nThe line:\r\n\r\n```\r\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)\r\n```\r\n\r\nDoes fail with the error:\r\n\r\n```\r\nAttributeError: 'ShuffleDataset' object has no attribute 'output_shapes'\r\n```\r\n", "comments": ["@hohl ,\r\nYou can try using \r\n\r\n`train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))`\r\nin place of\r\n`train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)`\r\n\r\n\r\n`test_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))`\r\nand in place of \r\n`test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)` \r\n\r\nWith above changes tutorials should work fine.\r\nFor more info you can refer this [SO link](https://stackoverflow.com/questions/55862178/attributeerror-shuffledataset-object-has-no-attribute-output-shapes-when).Thanks!", "It happens to the transformer tutorial as well. https://www.tensorflow.org/tutorials/text/transformer\r\n\r\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\r\n--> need to add tf.compat.v1.data.get_output_shapes(test_dataset)) for padded_shape.", "@hohl ,\r\nAny update on the issue ?Thanks!", "@oanush Well, yes, for me it works by using the v1 compat. \r\n\r\nBut, considering that this is part of the official Tensorflow documentation: wouldn't it be reasonable to update the documentation? So, that others don't have to search the issues on Github to run the official tutorials?", "@hohl  For TF2.1  without  v1 compat you may use:\r\n```python \r\n#correct version\r\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)\r\ntest_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)\r\n```\r\n![image](https://user-images.githubusercontent.com/42785357/74068706-2c549080-49b1-11ea-8494-3451937a5b8b.png)\r\n\r\nThe above code is given on TF website as well however colab shows different code which breaks. \r\n```python \r\n #incorrect version\r\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE)\r\ntest_dataset = test_dataset.padded_batch(BATCH_SIZE)\r\n```\r\ncc @lamberta ", "Thanks.\r\nThe tutorial runs using `tf-nightly` which is what's used in the source notebook: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_classification_rnn.ipynb\r\n\r\nWe currently strip come Colab magic when displaying on the webpage---but we should keep this line if we see \"tf-nightly\"", "Fixed: https://www.tensorflow.org/tutorials/text/text_classification_rnn#setup", "Thanks!"]}, {"number": 35989, "title": "tf.function stable doc typo issue", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/function?version=stable \r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\nOriginal:\r\n> It also restricts the dhape and datatype of Tensors that can be used:\r\n\r\nSo, I think it should be updated as \r\n\r\nIt also restricts the **shape** and datatype of Tensors that can be used:\r\n\r\n### Clear description\r\n\r\nFind a typo.\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["This has been fixed with https://github.com/tensorflow/tensorflow/commit/b8f3b2481ffe60f69ee385ee98659c70be8b02f8#diff-8a050116d943251fa084fd5be82feb2e\r\nThanks!"]}, {"number": 35988, "title": "\"ImportError: DLL load failed: The specified module could not be found.\" while trying to import tensorflow in tensorflow_cpu environment.", "body": "Traceback (most recent call last):   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>     from tensorflow.python.pywrap_tensorflow_internal import *   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>     _pywrap_tensorflow_internal = swig_import_helper()   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 243, in load_module     return load_dynamic(name, filename, file)   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 343, in load_dynamic     return _load(spec) ImportError: DLL load failed: The specified module could not be found.  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>     from tensorflow_core import *   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>     from tensorflow.python.tools import module_util as _module_util   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__     module = self._load()   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load     module = _importlib.import_module(self.__name__)   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\importlib\\__init__.py\", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>     from tensorflow.python import pywrap_tensorflow   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>     raise ImportError(msg) ImportError: Traceback (most recent call last):   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>     from tensorflow.python.pywrap_tensorflow_internal import *   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>     _pywrap_tensorflow_internal = swig_import_helper()   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 243, in load_module     return load_dynamic(name, filename, file)   File \"C:\\Users\\Shivani\\Anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 343, in load_dynamic     return _load(spec) ImportError: DLL load failed: The specified module could not be found.", "comments": ["Having the same issue with what appears to be the same trace. My setup is Pycharm 2019.2 although I have had other versions working previously using Pycharm. Interpreters I have tried are conda and pip through pycharm", "@Shivani29sheth @brian1985  What is your cpu make/model? TF 1.6 and above requires support for AVX instructions sets. ", "I am experiencing the same issue, running TF 1.12, tho previously (a few days ago) this was working fine. Any updates?", "It works now. I am not sure what caused the issue but I believe it was somehow related to Pycharm. I closed the program, restarted Pycharm AND then closed the Ipython terminal at the bottom. After that I assume it forced Pycharm to do something because it then worked. ", "I have switched temporarily to TF-CPU. I will re-switch to GPU in the following days and post here an update.", "@Shivani29sheth  and @brian1985, Please take a look at support [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements) of Tensorflow.\r\nUse [Dependency Walker](http://www.dependencywalker.com/), it will show you the DLL dependency tree, you will find which DLL cause the problem. You can then install the needed library or vcredist version, or add some path to system variable. ", "@Shivani29sheth, Is this still an issue", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35988\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35988\">No</a>\n", "> I have switched temporarily to TF-CPU. I will re-switch to GPU in the following days and post here an update.\r\n\r\nRe-installed CUDA and CuDNn and everything is fine now. Not sure what was the problem before, as I re-installed the same versions + same tf gpu version."]}, {"number": 35987, "title": "interpreter.invoke() of tflite model causes Aborted (core dumped) despite successful tflite conversion under tensorflow version 1.14.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Kubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nThe  [mobilenetvlad model](https://github.com/ethz-asl/hierarchical_loc/tree/master/global-loc/models/mobilenetvlad_depth-0.35) was successfuly converted to a tf lite model by [the sample code provided](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter) from the tensorflow website.  To archieve this I added the parameter `input_shapes` to the `from_saved_model` call:\r\n```converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_shapes={\"image\": [1, 640, 480, None]})```\r\n\r\nBut if inference is tested with the according [sample code](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python) from the tensorflow website, the program is aborted and the core dumped.\r\n\r\n**Describe the expected behavior**\r\nSucessfull inference and output of a [1, 4096] sized image descriptor.\r\n\r\n**Code to reproduce the issue**\r\n1. Download the  [mobilenetvlad model](https://github.com/ethz-asl/hierarchical_loc/tree/master/global-loc/models/mobilenetvlad_depth-0.35)\r\n2. With tensorflow 1.14.0 (tf2 will not work): Convert the saved_model to tflite, by setting an input shape (was set to None, None, None, 1]) in the [model description](https://github.com/ethz-asl/hierarchical_loc/blob/master/retrievalnet/retrievalnet/models/mobilenetvlad.py), but should be 640x480 according to the [paper.pdf](https://arxiv.org/pdf/1809.01019.pdf):\r\n```\r\nimport tensorflow as tf\r\nsaved_model_dir='hierarchical_loc/global-loc/models/mobilenetvlad_depth-0.35'\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_shapes={\"image\": [1, 640, 480, None]}) #only change of code beside filenames\r\ntflite_model = converter.convert()\r\nopen(\"converted_model_1_640_480.tflite\", \"wb\").write(tflite_model) \r\n```\r\n3. Run the inference with the [sample code](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python):\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model_1_640_480.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(input_details)\r\nprint(output_details)\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\nprint(input_data.shape)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n\r\n\r\n\r\n**Other info / logs**\r\n\r\nOutput of the tflite conversion. [Edited: copied the wrong output, sorry]. \r\nAnd don't worry about the virtual environmnet naming `VirtualEnvironments/tensorflow_1_15/`. I initially wanted to install tf 1.15.0, but only tf 1.14.0 worked:\r\n```/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-01-22 14:00:03.226815: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-22 14:00:03.250753: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194920000 Hz\r\n2020-01-22 14:00:03.251151: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a4df80 executing computations on platform Host. Devices:\r\n2020-01-22 14:00:03.251185: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING:tensorflow:From /home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nWARNING:tensorflow:From /home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\n2020-01-22 14:00:06.393038: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2020-01-22 14:00:10.226966: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-01-22 14:00:10.227122: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session                                                                                                                                 \r\n2020-01-22 14:00:10.347863: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize                                                                                          \r\n2020-01-22 14:00:10.347910: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.002ms.                                                                              \r\n2020-01-22 14:00:10.347930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.                                                                              \r\nWARNING:tensorflow:From /home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/lite/python/util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.                                                                                                                                                                                               \r\nInstructions for updating:                                                                                                                                                                                                                  \r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`                                                                                                                                                                                \r\nWARNING:tensorflow:From /home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.                                                                                                                                                                                            \r\nInstructions for updating:                                                                                                                                                                                                                  \r\nUse `tf.compat.v1.graph_util.extract_sub_graph`                                                                                                                                                                                             \r\n2020-01-22 14:00:11.150707: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)                                         \r\n2020-01-22 14:00:11.151014: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session                                                                                                                                 \r\n2020-01-22 14:00:12.806255: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize                                                                                          \r\n2020-01-22 14:00:12.806306: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 554 nodes (-265), 570 edges (-265), time = 1091.39502ms.                                                     \r\n2020-01-22 14:00:12.806326: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 554 nodes (0), 570 edges (0), time = 372.614ms.      \r\n```\r\nThe created model is then used within tfLiteInference.py:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model_1_640_480_test.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(input_details)\r\nprint(output_details)\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\nprint(input_data.shape)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n\r\nOutput of gdb debugging `gdb -ex r --args python3 tfLiteinference.py ` --> Aborted --> `py-list:`\r\n```\r\nThread 1 \"python3\" received signal SIGABRT, Aborted.\r\n__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n51      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) py-list\r\n 104    \r\n 105        def AllocateTensors(self):\r\n 106            return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n 107    \r\n 108        def Invoke(self):\r\n>109            return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\n 110    \r\n 111        def InputIndices(self):\r\n 112            return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_InputIndices(self)\r\n 113    \r\n 114        def OutputIndices(self):\r\n```\r\n\r\nI hope anyone can help me with this.\r\n\r\nCheers,\r\nAlex", "comments": ["Can you post the whole backtrace in gdb?\r\ni.e. write \"bt\" and print results here...\r\n", "Sure:\r\n```\r\nReading symbols from python3...Reading symbols from /usr/lib/debug/.build-id/28/7763e881de67a59b31b452dd0161047f7c0135.debug...done.\r\ndone.\r\nStarting program: /usr/bin/python3 tfLiteinference.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff412a700 (LWP 15400)]\r\n[New Thread 0x7ffff3929700 (LWP 15401)]\r\n[New Thread 0x7fffef128700 (LWP 15402)]\r\n[Thread 0x7fffef128700 (LWP 15402) exited]\r\n[Thread 0x7ffff3929700 (LWP 15401) exited]\r\n[Thread 0x7ffff412a700 (LWP 15400) exited]\r\n2.0.0\r\nINFO: Initialized TensorFlow Lite runtime.\r\n[{'name': 'image', 'index': 54, 'shape': array([  1, 640, 480,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n[{'name': 'descriptor', 'index': 52, 'shape': array([   1, 4096], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n(1, 640, 480, 1)\r\n[New Thread 0x7fffef128700 (LWP 15408)]\r\n[New Thread 0x7ffff3929700 (LWP 15409)]\r\n[New Thread 0x7ffff412a700 (LWP 15410)]\r\n[New Thread 0x7fffc0e53700 (LWP 15411)]\r\n\r\nThread 1 \"python3\" received signal SIGABRT, Aborted.\r\n__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n51      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) bt\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x00007ffff7a24801 in __GI_abort () at abort.c:79\r\n#2  0x00007fffc5e995c9 in void tflite::ops::builtin::sub::EvalSub<(tflite::ops::builtin::sub::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteSubParams*, tflite::ops::builtin::sub::OpData const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#3  0x00007fffc5e9be65 in TfLiteStatus tflite::ops::builtin::sub::Eval<(tflite::ops::builtin::sub::KernelType)2>(TfLiteContext*, TfLiteNode*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#4  0x00007fffc5ed8577 in tflite::Subgraph::Invoke() () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#5  0x00007fffc5eda22a in tflite::Interpreter::Invoke() () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#6  0x00007fffc5da7f58 in tflite::interpreter_wrapper::InterpreterWrapper::Invoke() () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#7  0x00007fffc5da6054 in _wrap_InterpreterWrapper_Invoke () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#8  0x000000000050a8af in _PyCFunction_FastCallDict (kwargs=<optimized out>, nargs=<optimized out>, args=<optimized out>, func_obj=<built-in method InterpreterWrapper_Invoke of module object at remote 0x7fffc61338b8>)\r\n    at ../Objects/methodobject.c:234\r\n#9  _PyCFunction_FastCallKeywords (kwnames=<optimized out>, nargs=<optimized out>, stack=<optimized out>, func=<optimized out>) at ../Objects/methodobject.c:294\r\n#10 call_function.lto_priv () at ../Python/ceval.c:4851\r\n#11 0x000000000050c5b9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#12 0x0000000000509d48 in PyEval_EvalFrameEx (throwflag=0, \r\n    f=Frame 0x7fffc613a048, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py, line 109, in Invoke (self=<InterpreterWrapper(this=<SwigPyObject at remote 0x7fffc6210db0>) at remote 0x7ffff67ed438>)) at ../Python/ceval.c:754\r\n#13 _PyFunction_FastCall (globals=<optimized out>, nargs=140736516563016, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933\r\n#14 fast_function.lto_priv () at ../Python/ceval.c:4968\r\n#15 0x000000000050aa7d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#16 0x000000000050c5b9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#17 0x0000000000509d48 in PyEval_EvalFrameEx (throwflag=0, \r\n    f=Frame 0x7fffc612d398, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter.py, line 453, in invoke (self=<Interpreter(_interpreter=<InterpreterWrapper(this=<SwigPyObject at remote 0x7fffc6210db0>) at remote 0x7ffff67ed438>, _delegates=[]) at remote 0x7ffff67ed400>)) at ../Python/ceval.c:754\r\n#18 _PyFunction_FastCall (globals=<optimized out>, nargs=140736516510616, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933\r\n#19 fast_function.lto_priv () at ../Python/ceval.c:4968\r\n#20 0x000000000050aa7d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#21 0x000000000050c5b9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#22 0x0000000000508245 in PyEval_EvalFrameEx (throwflag=0, f=Frame 0xae0b68, for file tfLiteinference.py, line 22, in <module> ()) at ../Python/ceval.c:754\r\n#23 _PyEval_EvalCodeWithName.lto_priv.1836 () at ../Python/ceval.c:4166\r\n#24 0x000000000050b403 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, locals=<optimized out>, globals=<optimized out>, _co=<optimized out>) at ../Python/ceval.c:4187\r\n#25 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:731\r\n#26 0x0000000000635222 in run_mod () at ../Python/pythonrun.c:1025\r\n#27 0x00000000006352d7 in PyRun_FileExFlags () at ../Python/pythonrun.c:978\r\n#28 0x0000000000638a8f in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:419\r\n#29 0x0000000000638c65 in PyRun_AnyFileExFlags () at ../Python/pythonrun.c:81\r\n#30 0x0000000000639631 in run_file (p_cf=0x7fffffffd9dc, filename=<optimized out>, fp=<optimized out>) at ../Modules/main.c:340\r\n#31 Py_Main () at ../Modules/main.c:810\r\n#32 0x00000000004b0f40 in main (argc=2, argv=0x7fffffffdbd8) at ../Programs/python.c:69\r\n(gdb) py-bt\r\nTraceback (most recent call first):\r\n  <built-in method InterpreterWrapper_Invoke of module object at remote 0x7fffc61338b8>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 109, in Invoke\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter.py\", line 453, in invoke\r\n    self._interpreter.Invoke()\r\n  File \"tfLiteinference.py\", line 22, in <module>\r\n    interpreter.invoke()\r\n```\r\n\r\nThe tensorflow version used during the inference is 2.0.0 (as shown in the output).\r\n\r\nThanks for helping!", "There is a specific line in your converter code:\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_shapes={\"image\": [1, 640, 480, None]}) #only change of code beside filenames\r\n\r\nCan you also specify the last dimension (channel size) for the input shape? Currently tf lite doesn't support dynamic shapes very well, so i'm guessing this might  be an issue. thanks.", "Thanks for the hint. I tested it, but the error persists:\r\n```\r\nStarting program: /usr/bin/python3 tfLiteinference.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff412a700 (LWP 14446)]\r\n[New Thread 0x7ffff1929700 (LWP 14447)]\r\n[New Thread 0x7fffef128700 (LWP 14448)]\r\n/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n[Thread 0x7fffef128700 (LWP 14448) exited]\r\n[Thread 0x7ffff1929700 (LWP 14447) exited]\r\n[Thread 0x7ffff412a700 (LWP 14446) exited]\r\n/home/alex/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/alex/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n1.14.0\r\nINFO: Initialized TensorFlow Lite runtime.\r\n[{'name': 'image', 'index': 54, 'shape': array([  1, 640, 480,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n[{'name': 'descriptor', 'index': 52, 'shape': array([   1, 4096], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n(1, 640, 480, 1)\r\n[New Thread 0x7fffef128700 (LWP 14452)]\r\n[New Thread 0x7ffff1929700 (LWP 14453)]\r\n[New Thread 0x7ffff412a700 (LWP 14454)]\r\n[New Thread 0x7fffc1984700 (LWP 14455)]\r\n\r\nThread 1 \"python3\" received signal SIGABRT, Aborted.\r\n__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n51      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) bt\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x00007ffff7a24801 in __GI_abort () at abort.c:79\r\n#2  0x00007fffc68e2203 in tflite::RuntimeShape::RuntimeShape(int, tflite::RuntimeShape const&, int) ()\r\n   from /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#3  0x00007fffc68e2b87 in void tflite::NdArrayDescsForElementwiseBroadcast<4>(tflite::RuntimeShape const&, tflite::RuntimeShape const&, tflite::NdArrayDesc<4>*, tflite::NdArrayDesc<4>*) ()\r\n   from /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#4  0x00007fffc69bf8ab in tflite::reference_ops::BroadcastSub4DSlow(tflite::ArithmeticParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*) ()\r\n   from /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#5  0x00007fffc69bfc94 in void tflite::ops::builtin::sub::EvalSub<(tflite::ops::builtin::sub::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteSubParams*, tflite::ops::builtin::sub::OpData const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*) () from /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#6  0x00007fffc69c0eb4 in TfLiteStatus tflite::ops::builtin::sub::Eval<(tflite::ops::builtin::sub::KernelType)2>(TfLiteContext*, TfLiteNode*) ()\r\n   from /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#7  0x00007fffc69fb31f in tflite::Subgraph::Invoke() () from /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#8  0x00007fffc69fdfa0 in tflite::Interpreter::Invoke() () from /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#9  0x00007fffc68dc768 in tflite::interpreter_wrapper::InterpreterWrapper::Invoke() () from /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#10 0x00007fffc68da507 in _wrap_InterpreterWrapper_Invoke () from /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n---Type <return> to continue, or q <return> to quit---\r\n#11 0x000000000050a8af in _PyCFunction_FastCallDict (kwargs=<optimized out>, nargs=<optimized out>, args=<optimized out>, func_obj=<built-in method InterpreterWrapper_Invoke of module object at remote 0x7fffc6a4ed68>)\r\n    at ../Objects/methodobject.c:234\r\n#12 _PyCFunction_FastCallKeywords (kwnames=<optimized out>, nargs=<optimized out>, stack=<optimized out>, func=<optimized out>) at ../Objects/methodobject.c:294\r\n#13 call_function.lto_priv () at ../Python/ceval.c:4851\r\n#14 0x000000000050c5b9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#15 0x0000000000509d48 in PyEval_EvalFrameEx (throwflag=0, \r\n    f=Frame 0x7fffc6a52c18, for file /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py, line 109, in Invoke (self=<InterpreterWrapper(this=<SwigPyObject at remote 0x7fffc6b3ed80>) at remote 0x7ffff67ed4a8>)) at ../Python/ceval.c:754\r\n#16 _PyFunction_FastCall (globals=<optimized out>, nargs=140736526101528, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933\r\n#17 fast_function.lto_priv () at ../Python/ceval.c:4968\r\n#18 0x000000000050aa7d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#19 0x000000000050c5b9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#20 0x0000000000509d48 in PyEval_EvalFrameEx (throwflag=0, \r\n    f=Frame 0x7fffce6721f0, for file /home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py, line 304, in invoke (self=<Interpreter(_interpreter=<InterpreterWrapper(this=<SwigPyObject at remote 0x7fffc6b3ed80>) at remote 0x7ffff67ed4a8>) at remote 0x7ffff67ed470>)) at ../Python/ceval.c:754\r\n#21 _PyFunction_FastCall (globals=<optimized out>, nargs=140736656253424, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933\r\n---Type <return> to continue, or q <return> to quit---\r\n#22 fast_function.lto_priv () at ../Python/ceval.c:4968\r\n#23 0x000000000050aa7d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#24 0x000000000050c5b9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#25 0x0000000000508245 in PyEval_EvalFrameEx (throwflag=0, f=Frame 0xae0b68, for file tfLiteinference.py, line 22, in <module> ()) at ../Python/ceval.c:754\r\n#26 _PyEval_EvalCodeWithName.lto_priv.1836 () at ../Python/ceval.c:4166\r\n#27 0x000000000050b403 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, locals=<optimized out>, globals=<optimized out>, _co=<optimized out>) at ../Python/ceval.c:4187\r\n#28 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:731\r\n#29 0x0000000000635222 in run_mod () at ../Python/pythonrun.c:1025\r\n#30 0x00000000006352d7 in PyRun_FileExFlags () at ../Python/pythonrun.c:978\r\n#31 0x0000000000638a8f in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:419\r\n#32 0x0000000000638c65 in PyRun_AnyFileExFlags () at ../Python/pythonrun.c:81\r\n#33 0x0000000000639631 in run_file (p_cf=0x7fffffffd94c, filename=<optimized out>, fp=<optimized out>) at ../Modules/main.c:340\r\n#34 Py_Main () at ../Modules/main.c:810\r\n#35 0x00000000004b0f40 in main (argc=2, argv=0x7fffffffdb48) at ../Programs/python.c:69\r\n(gdb) py-list\r\n 104    \r\n 105        def AllocateTensors(self):\r\n 106            return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n 107    \r\n 108        def Invoke(self):\r\n>109            return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\n 110    \r\n 111        def InputIndices(self):\r\n 112            return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_InputIndices(self)\r\n 113    \r\n 114        def OutputIndices(self):\r\n(gdb) py-bt\r\nTraceback (most recent call first):\r\n  <built-in method InterpreterWrapper_Invoke of module object at remote 0x7fffc6a4ed68>\r\n  File \"/home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 109, in Invoke\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\n  File \"/home/alex/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 304, in invoke\r\n    self._interpreter.Invoke()\r\n  File \"tfLiteinference.py\", line 22, in <module>\r\n    interpreter.invoke()\r\n```", "I'm having the same issue with different model - same as reported here: https://github.com/tensorflow/tensorflow/issues/32725 \r\nConversion is ok - I've tried different ways of making a tflite model- from Saved Model, from Frozen Graph, etc. I've also tried with quantisation- the conversion is always correct and invoke() fails. I've also tried it in C API - the same problem", "The error message gives some indication that it may happen inside the Sub operator:\r\n#2  0x00007fffc5e995c9 in void tflite::ops::builtin::sub::EvalSub<(tflite::ops::builtin::sub::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteSubParams*, tflite::ops::builtin::sub::OpData const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n\r\nI believe there might be some shape issues that triggers this bug. Have you tried converting your model with the new MLIR TF lite converter? To use the new converter, you can download tf-nightly and then set converter.experimental_new_converter = True, before you call converter.convert().", "@haozha111 I've tried that - it gives me Value Error (with experimental converter turned off but still in the latest tf-nightly it's the same). The input shape in my case is [1, 257, 250, 1]\r\n`File \"[...]/miniconda3/envs/tf-nightly-py3/lib/python3.7/site-packages/tensorflow_core/python/framework/convert_to_constants.py\", line 526, in _convert_variables_to_constants_v2_impl\r\n    raise ValueError(\"Cannot find the Placeholder op that is an input \"\r\nValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.`", "@haozha111 I've managed to fix this - my initial model is form keras, when I've modified its conversion to Saved Model (as suggested here: https://stackoverflow.com/questions/51858203/cant-import-frozen-graph-with-batchnorm-layer/52823701#52823701), the new SM converts properly with the latest tf-nightly and experimental converter - the problem with invoke is unfortunately still the same...", "any updates on this issue?\r\n", "same issue\r\n\r\n```\r\n(gdb) bt\r\n#0  0x00007fff80277520 in TfLiteStatus tflite::ops::builtin::sparse_to_dense::SparseToDenseImpl<long, long>(TfLiteContext*, TfLiteNode*) ()\r\n   from /home/sc/anaconda3/envs/env3.6/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#1  0x00007fff802bacdf in tflite::Subgraph::Invoke() ()\r\n   from /home/sc/anaconda3/envs/env3.6/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#2  0x00007fff802bd960 in tflite::Interpreter::Invoke() ()\r\n   from /home/sc/anaconda3/envs/env3.6/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#3  0x00007fff8019bfc8 in tflite::interpreter_wrapper::InterpreterWrapper::Invoke() ()\r\n   from /home/sc/anaconda3/envs/env3.6/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#4  0x00007fff80199d67 in _wrap_InterpreterWrapper_Invoke ()\r\n   from /home/sc/anaconda3/envs/env3.6/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#5  0x0000555555665b91 in _PyCFunction_FastCallDict () at /tmp/build/80754af9/python_1564510748219/work/Objects/methodobject.c:234\r\n#6  0x00005555556edabc in call_function () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4851\r\n#7  0x000055555571075a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:3335\r\n#8  0x00005555556e7c5b in _PyFunction_FastCall (globals=<optimized out>, nargs=1, args=<optimized out>, co=<optimized out>)\r\n    at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4933\r\n#9  fast_function () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4968\r\n#10 0x00005555556edb95 in call_function () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4872\r\n#11 0x000055555571075a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:3335\r\n#12 0x00005555556e7c5b in _PyFunction_FastCall (globals=<optimized out>, nargs=1, args=<optimized out>, co=<optimized out>)\r\n    at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4933\r\n#13 fast_function () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4968\r\n#14 0x00005555556edb95 in call_function () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4872\r\n#15 0x000055555571075a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:3335\r\n#16 0x00005555556e7c5b in _PyFunction_FastCall (globals=<optimized out>, nargs=1, args=<optimized out>, co=<optimized out>)\r\n    at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4933\r\n#17 fast_function () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4968\r\n#18 0x00005555556edb95 in call_function () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4872\r\n#19 0x000055555571075a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:3335\r\n#20 0x00005555556e89b9 in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, \r\n    kwstep=2, kwcount=<optimized out>, kwargs=0x0, kwnames=0x0, argcount=0, args=0x0, locals=0x7ffff7f5d240, \r\n    globals=0x7ffff7f5d240, _co=0x7ffff69f7660) at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4166\r\n#21 PyEval_EvalCodeEx () at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:4187\r\n#22 0x00005555556e975c in PyEval_EvalCode (co=co@entry=0x7ffff69f7660, globals=globals@entry=0x7ffff7f5d240, \r\n    locals=locals@entry=0x7ffff7f5d240) at /tmp/build/80754af9/python_1564510748219/work/Python/ceval.c:731\r\n#23 0x0000555555769744 in run_mod () at /tmp/build/80754af9/python_1564510748219/work/Python/pythonrun.c:1025\r\n#24 0x0000555555769b41 in PyRun_FileExFlags () at /tmp/build/80754af9/python_1564510748219/work/Python/pythonrun.c:978\r\n#25 0x0000555555769d43 in PyRun_SimpleFileExFlags () at /tmp/build/80754af9/python_1564510748219/work/Python/pythonrun.c:419\r\n#26 0x0000555555769e4d in PyRun_AnyFileExFlags () at /tmp/build/80754af9/python_1564510748219/work/Python/pythonrun.c:81\r\n#27 0x000055555576d833 in run_file (p_cf=0x7fffffffd89c, filename=0x5555558a82f0 L\"onnx2tf.py\", fp=0x55555593ebe0)\r\n---Type <return> to continue, or q <return> to quit---\r\n    at /tmp/build/80754af9/python_1564510748219/work/Modules/main.c:340\r\n#28 Py_Main () at /tmp/build/80754af9/python_1564510748219/work/Modules/main.c:811\r\n#29 0x000055555563788e in main () at /tmp/build/80754af9/python_1564510748219/work/Programs/python.c:69\r\n#30 0x00007ffff7810830 in __libc_start_main (main=0x5555556377a0 <main>, argc=2, argv=0x7fffffffdaa8, init=<optimized out>, \r\n    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffda98) at ../csu/libc-start.c:291\r\n#31 0x0000555555717160 in _start () at ../sysdeps/x86_64/elf/start.S:103\r\n\r\n```\r\ncore dump in tflite::ops::builtin::sparse_to_dense::SparseToDenseImp", "mee too", "any update on this issue, facing similar issues adding Instance Normalization. Removing Instance Normalization works. Is there a workaround for the Instance Norm/Group norm? ", "Dear TF team, @gargn @haozha111 , - are there any views for this issue to be resolved? People tend to continuously report this error. Please see if anything can be done! It would be greatly appreciated:)", "I get the same error. ", "I also got a similar error :  \r\n\r\n```\r\n[{'name': 'input_1', 'index': 0, 'shape': array([  1, 416, 416,   3]), 'shape_signature': array([ -1, 416, 416,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n[{'name': 'Identity', 'index': 229, 'shape': array([1, 1, 1]), 'shape_signature': array([-1, -1, -1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\nTime model loading:  0.020444154739379883 s\r\nTime pre-processing:  0.2051246166229248 s\r\nFatal Python error: Aborted\r\n\r\nThread 0x578b7460 (most recent call first):\r\n  File \"/usr/lib/python3.7/threading.py\", line 296 in wait\r\n  File \"/usr/lib/python3.7/threading.py\", line 552 in wait\r\n  File \"/home/vinorth/.virtualenvs/yolo-tflite/lib/python3.7/site-packages/IPython/core/history.py\", line 829 in run\r\n  File \"/home/vinorth/.virtualenvs/yolo-tflite/lib/python3.7/site-packages/IPython/core/history.py\", line 58 in needs_sqlite\r\n  File \"<decorator-gen-24>\", line 2 in run\r\n  File \"/usr/lib/python3.7/threading.py\", line 917 in _bootstrap_inner\r\n  File \"/usr/lib/python3.7/threading.py\", line 885 in _bootstrap\r\n\r\nCurrent thread 0x76f3cad0 (most recent call first):\r\n  File \"/home/vinorth/.virtualenvs/yolo-tflite/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 113 in Invoke\r\n  File \"/home/vinorth/.virtualenvs/yolo-tflite/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 511 in invoke\r\n  File \"detect.py\", line 79 in main\r\n  File \"/home/vinorth/.virtualenvs/yolo-tflite/lib/python3.7/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/vinorth/.virtualenvs/yolo-tflite/lib/python3.7/site-packages/absl/app.py\", line 299 in run\r\n  File \"detect.py\", line 186 in <module>\r\nAborted\r\n```", "I've tried yesterday with the latest nightly 2.4.0-dev20200728 and experimental new converter and it worked:) I've tested only with the dummy input so far, but the interpreter is working without errors and produced a result. Thank you tf team:)\r\n", " @gargn @haozha111 when can we expect tflite_runtime supporting this update? With the latest version 2.1.0.post1 I cannot run this new lite model with error: `ValueError: Didn't find op for builtin opcode 'SUB' version '3` Following the discussion above I guess this refers to the fix you've made in the SUB operator.", "I got this error earlier using tensorflow 2.2.0. I was able to get rid of the sigabrt error by upgrading to tensorflow 2.3.0 (the nightly build @moniGra suggested gave me errors along the lines of 'Didn't find op for builtin opcode 'CONV_2D' version '5'').\r\n\r\nI then got errors for 'Didn't find op for builtin opcode 'SUB' version '3'' and a similar version for DIV version 2. The sub error turned out to be that I specified a fixed batch size, and the subtracted tensor needed to match the same batch dimensions. The div error was fixed by replacing '/' operators with scalars by multiplying the inverse (it seems division is not quite supported).\r\n\r\nHope this helps.", "#44237 [opcode 'SUB' version '3' in tfllite_runtime](https://github.com/tensorflow/tensorflow/issues/44237)\r\n\r\n@anklebreaker Where did you replace _'/'_ operators or change batch size?", "@nfisdnf986 I replaced them in the original Tensorflow Keras model I created and converted it again to TFLite. Input layer has an argument for batch that you can specify (you can always add such a layer in Keras before the network if you don't have the original).  I replaced the '/' in post-processing part of my model.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "> Hi There,\r\n> \r\n> We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help.\r\n> \r\n> This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.\r\n\r\nFor me version 2.4.0 finally worked:) ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35987\">No</a>\n"]}, {"number": 35986, "title": "tflite_convert deletes (almost) all nodes and edges, .pb to .tflite, 585929kb to 1kb", "body": "Hello,\r\nI'm having problems with conversion from frozen graph (.pb) to tflite, using tflite_convert.\r\nIt does safe a .tflite file, but it's only got a size of 1kb.\r\nApparently it gets rid of almost every node and all edges.\r\nCan anybody give me advice on how to do it without losing all the information or what I'm doing wrong?\r\nBest regards\r\n\r\n**System information**\r\nWindows 10\r\nTensorflow Version 1.14.0 obtained via pip\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\ntflite_convert --graph_def_file=categorical_model.pb --output_file=optimized_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,224,224,3 --input_array=input_1 --output_array=training/SGD/Variable_215\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-01-17 12:22:40.734881: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\r\n2020-01-17 12:22:51.425546: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-01-17 12:22:51.432588: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2020-01-17 12:23:42.194315: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2020-01-17 12:23:42.199502: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 2 nodes (-1929), 0 edges (-2004), time = 45374.7031ms.\r\n```\r\n\r\n**Failure details**\r\nOriginal file (categorical_model.pb) has got 585 929kb, output file (optimized_graph.tflite) has only got 1kb.\r\n", "comments": ["@st170911 Can you please create a simple standalone code to reproduce the issue? Please share the model *.pb. Thanks!", "@st170911 Can you please create a simple standalone code to reproduce the issue? Please share the model *.pb. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35986\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35986\">No</a>\n"]}, {"number": 35985, "title": "[TFLite int16] 16-bit version of ADD/SUB reference kernel operators", "body": "This PR is one of steps to extend 8-bit quantization to support symmetric 16-bit activations.\r\n\r\nEach activation is of type int16 and symmetric around zero. The weight tensor precision remains at 8-bit signed values. The bias is set to int64 precision.\r\n\r\nIn this PR we introduce implementation and tests for ADD/SUB kernel reference function.\r\nThe specification of this operator:\r\n\r\nSUB \r\n\u202f Input 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f Input 1: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f Output 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\r\nADD \r\n\u202f Input 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f Input 1: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f Output 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0", "comments": ["Hi @suharshs ! \r\nThanks for the review. \r\nI corrected as you suggested. Could you please take a look ?\r\n\r\nRegarding the versioning : Should I bump the version for ADD/SUB ?\r\nI will set version 3, when input/output is int16. \r\n", "Thanks, yes you will also need to update the schema.fbs file to add a pot_scale attribute that the op_version.cc code can use to determine the new version. Since we need a way to distinguish between the pot and generic version in the op_version code.", "Hi @suharshs Could you please re-approve this PR - I had to resolve a merge conflict.\r\n\r\nBtw, import/copybara is red. Are there some internal errors ? Can I help with them ?\r\nThanks! ", "Hi @jdduke ! Could you please re-approve this PR ? I had to resolve conflicts.", "Hi @rthadur ! Sorry to bother, but could you please take a look at this PR ? Is it stuck internally ? Thanks!", "@jdduke can you please help merge this PR internally ?", "@wwwind Can you please resolve conflicts? Thanks!", "Hi @gbaned, I resolved the conflict. Could you please re-approve ? Thanks!", "@jdduke, @suharshs, what is holding this PR up? Do we need to change to the original implementation to move it forward?", "@wwwind Can you please fix build failures ? Thanks!", "Hi @suharshs Could you please re-approve this PR ? I had to push a small fix for build failure.\r\nThanks!", "Hi @rthadur \r\nSorry to bother you, but could you please check where this PR has stuck internally ?\r\nIt has \"ready-to-pull\" label for 11 days now and all checks are green.\r\nThanks for the help!", "@wwwind getting this error internally, can you please check \r\n`third_party/tensorflow/lite/kernels/test_util.cc:330\r\nExpected equality of these values:\r\n  CountPartitionsDelegatedTo(interpreter_.get(), TestNnApiDelegate())\r\n    Which is: 0\r\n  1\r\nExpecting operation to be accelerated but cannot find a partition associated to the NNAPI delegate` \r\n\r\n", "Hi @jdduke Could you please check a status of this PR? Has this internal error been fixed? Can it be merged? Thanks", "Hi @rthadur Sorry to bother you, could you please check the status of this PR ?\r\nIs it stuck with internal errors ?\r\nI think I fixed all errors.\r\nThanks a lot for the help!", "@wwwind it got stuck internally , tried to submit again , will keep you posted, thank you for your patience.", "@wwwind can you please resolve conflicts as well", "Hi @rthadur ! I resolved the conflict. Thanks!", "@wwwind Can you please check @jdduke's comments and keep us posted. Thanks!", "Hi @jdduke Thanks for the explanation. I pushed a change.", "@wwwind Can you please resolve conflicts? Thanks!", "Hi @gbaned Done. Thanks!", "@wwwind we see few more conflicts , can you please resolve those ", "Hi @rthadur I resolved the conflict. Could you please re-approve ? Thanks!", "@wwwind  Can you please resolve conflicts? Thanks!", "Hi @gbaned Done, Thanks!"]}, {"number": 35984, "title": "Add Usage Example to Keras Nadam Optimizer", "body": "", "comments": ["Hi thats nice, but the output doesn't show anything about the optimizer, can you try and give an output on something related to the optimizer?", "awesome!", "@boronhub @generationXcode Can someone take a look at the pull request? I made some changes as build on Linux CPU had failed.", "@jedlimlx Can you please resolve conflicts? Thanks!", "@jedlimlx Can you please check @tanzhenyu's comments and resolve conflicts?. Thanks!"]}, {"number": 35983, "title": "AOT tests fail on Windows (MSVC 2019)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10 Pro x64\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version:\r\nmaster (7be9f53)\r\n- Python version:\r\nPython 3.7.4 (Miniconda3)\r\n- Bazel version (if compiling from source):\r\nBuild label: 1.2.1\r\nBuild time: Tue Nov 26 15:24:17 2019 (1574781857)\r\nBuild timestamp: 1574781857\r\nBuild timestamp as int: 1574781857\r\n- GCC/Compiler version (if compiling from source):\r\nMSVC 2019\r\n- CUDA/cuDNN version:\r\n10.1/7\r\n- GPU model and memory:\r\nGTX 1080 Ti 11 GB\r\n\r\n**Describe the current behavior**\r\nCompiling a graph using AOT fails with linker error LNK1107:\r\n```\r\nINFO: Analyzed target //tensorflow/compiler/aot:test_graph_tfadd (172 packages loaded, 15111 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base E:/tmp/_bazel_patrik/asdmfctl/sandbox\r\n[10 / 1,550] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\n[727 / 4,195] checking cached actions\r\n[7,806 / 7,807] [Prepa] Linking tensorflow/compiler/aot/test_graph_tfadd.lib\r\nERROR: E:/doremir/onset-detector/graph/tensorflow/tensorflow/compiler/aot/BUILD:121:1: Linking of rule '//tensorflow/compiler/aot:test_graph_tfadd' failed (Exit 1107)\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\aot\\test_graph_tfadd_tfcompile_function.o : fatal error LNK1107: invalid or corrupt file: cannot read at 0x340\r\nTarget //tensorflow/compiler/aot:test_graph_tfadd failed to build\r\nINFO: Elapsed time: 13.822s, Critical Path: 1.89s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n**Describe the expected behavior**\r\nSuccessful build with a functioning executable at \r\n`bazel-bin/tensorflow/compiler/aot/test_graph_tfadd`.\r\n**Code to reproduce the issue**\r\nUsing MSYS2 (GNU bash, version 4.4.23(1)-release (x86_64-pc-msys))\r\n```bash\r\ngit clone --depth=1 https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\npython configure.py # xla - yes, rest default.\r\nbazel build //tensorflow/compiler/aot:test_graph_tfadd\r\n```\r\n**Other info / logs**\r\nEnvironment setup:\r\n```bash\r\n# Use a temporary directory with a short name.\r\nexport TMPDIR=\"E:/tmp\"\r\nexport TMP=\"E:/tmp\"\r\nexport TEMP=\"E:/tmp\"\r\nexport TEMPDIR=\"E:/tmp\"\r\nexport TEST_TMPDIR=\"E:/tmp\"\r\n\r\n# Add timestamps before each command.\r\nexport PS4='+ $(date) + '\r\n\r\n# Set bash path\r\nexport BAZEL_SH=\"E:/msys64/usr/bin/bash\"\r\nexport BAZEL_VC=\"E:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC\"\r\nexport BAZEL_VS=\"\"\r\n\r\nexport PYTHON_BASE_PATH=\"E:/Miniconda3/envs/tf2.0\"\r\n\r\n# Set the path to find bazel.\r\nexport PATH=\"/c/bazel-root/:$PATH\"\r\n\r\n# Set Python path for ./configure\r\nexport PYTHON_BIN_PATH=\"${PYTHON_BASE_PATH}/python.exe\"\r\nexport PYTHON_LIB_PATH=\"${PYTHON_BASE_PATH}/lib/site-packages\"\r\n\r\n# Add python into PATH, it's needed because gen_git_source.py uses\r\n# '/usr/bin/env python' as a shebang\r\nexport PATH=\"${PYTHON_BASE_PATH}:$PATH\"\r\n\r\n# Make sure we have pip in PATH\r\nexport PATH=\"${PYTHON_BASE_PATH}/Scripts:$PATH\"\r\n\r\n# Setting default values to CUDA related environment variables\r\nexport TF_CUDA_VERSION=${TF_CUDA_VERSION:-10.1}\r\nexport TF_CUDNN_VERSION=${TF_CUDNN_VERSION:-7}\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=${TF_CUDA_COMPUTE_CAPABILITIES:-6.0}\r\nexport CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v${TF_CUDA_VERSION}\"\r\nexport CUDNN_INSTALL_PATH=\"E:/cuda\"\r\n\r\n# Add Cuda and Cudnn dll directories into PATH\r\nexport PATH=\"${CUDA_TOOLKIT_PATH}/bin:$PATH\"\r\nexport PATH=\"${CUDA_TOOLKIT_PATH}/extras/CUPTI/libx64:$PATH\"\r\nexport PATH=\"${CUDNN_INSTALL_PATH}/bin:$PATH\"\r\n```\r\n", "comments": ["@patrikohlsson \r\n\r\nCan you please go through #15213 and see if it helps you. Thanks!", "@ravikyram yes, I've seen that issue and it doesn't help in this particular case.\r\n\r\nActually the issue seems to be that the `tf_library` macro doesn't default to the host `target_triple` value on windows which results in tfcompile producing ELF-style object-files. Adding the correct triple in the test BUILD files was successful, i.e.:\r\n```bazel\r\ntf_library(\r\n    name = \"test_graph_tfmatmul\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfmatmul.config.pbtxt\",\r\n    cpp_class = \"foo::bar::MatMulComp\",\r\n    graph = \"test_graph_tfmatmul.pb\",\r\n    tags = [\r\n        \"manual\",\r\n    ],\r\n    tfcompile_flags = \"--target_triple=x86_64-pc-windows\"\r\n)\r\n```\r\n\r\nBut I guess the proper fix would be to automatically infer the triple from the host.", "Just adding windows as a target at the end of `//tensorflow/compiler/aot/tfcompile.bzl` *seems* to do the trick.", "> Just adding windows as a target at the end of `//tensorflow/compiler/aot/tfcompile.bzl` _seems_ to do the trick.\r\n\r\n@patrikohlsson,\r\nIs this still an issue?\r\n\r\nPlease feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35983\">No</a>\n"]}, {"number": 35982, "title": "model.fit requests and leaks extra data from validation_data generator than the specified validation_steps", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nI tried this in two environments:\r\n\r\nThe first enviornment is: \r\n```\r\nmacOS Catalina 10.15.2\r\npython 3.6.5\r\ntensorflow 2.1 (v2.1.0-rc2-17-ge5bf8de410 2.1.0)\r\nkeras 2.3.1\r\n```\r\nThe second environment is:\r\n```\r\nLinux Ubuntu 16.04\r\npython 3.5.2\r\ntensorflow-gpu 2.1  (v2.1.0-rc2-17-ge5bf8de 2.1.0)\r\nkeras 2.3.1\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\nUpon using the validation generator while fitting the model, the validation generator is somehow requested to generate more data batches per epoch than the actually needed for validation (specified by `validation_steps`). I'm unable to identify where this extra data is used, however, this extra data grows indefinitely with more training (as shown when I increase the number of epochs from 2 to 5). This irregular behavior ruins the validation because it's not exactly the same data that is being passed every epoch.\r\n\r\nThis behavior only happens when I used `tensorflow.keras`, however, it works properly when using just `keras`.\r\n\r\n**Describe the expected behavior**\r\nFor every epoch, the validation generator generates a fixed number of data batches, and if there's extra data, it shouldn't grow indefinitely with more training epochs. This expected behavior happens below in the first two examples when I used `keras` instead of `tensorflow.keras`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n#!/usr/bin/env python3                                                                                                     \r\nimport os                                                                                                                  \r\nimport sys                                                                                                                 \r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'                                                                                   \r\n                                                                                                                           \r\nimport numpy as np                                                                                                         \r\n# Environment setup, using keras vs. tensorflow.keras                                                                      \r\n                                                                                                                           \r\nimport tensorflow as tf                                                                                                    \r\nif sys.argv[1] == 'tensorflow':                                                                                            \r\n    import tensorflow.keras as K                                                                                           \r\n    from tensorflow.keras.models import Model                                                                              \r\n    from tensorflow.keras.layers import Dense, Input, Dropout                                                              \r\nelif sys.argv[1] == 'keras':                                                                                               \r\n    import keras as K                                                                                                      \r\n    from keras.models import Model                                                                                         \r\n    from keras.layers import Dense, Input, Dropout                                                                         \r\nelse:                                                                                                                      \r\n    sys.exit()                                                                                                             \r\n                                                                                                                           \r\nepochs = int(sys.argv[2])                                                                                                  \r\n                                                                                                                           \r\n                                                                                                                           \r\n# Trackers of the generated data                                                                                           \r\ntrain_cntX = 0  # updated for each extracted training batch                                                                \r\ntrain_cntY = 0  # updated for each extracted training batch                                                                \r\nhacked = []  # updated for each extracted validation batch                                                                 \r\n                                                                                                                           \r\n# Reporting the size of hacked before/after each epoch                                                                     \r\nclass TrackingCB(K.callbacks.Callback):                                                                                    \r\n    def __init__(self, steps, *args, **kwargs):                                                                            \r\n        super().__init__(*args, **kwargs)                                                                                  \r\n        self.steps = steps                                                                                                 \r\n                                                                                                                           \r\n    def on_epoch_begin(self, epoch, logs={}):                                                                              \r\n        self.start_length = len(hacked)                                                                                    \r\n                                                                                                                           \r\n    def on_epoch_end(self, epoch, logs={}):                                                                                \r\n        for _ in range(self.steps):                                                                                        \r\n            hacked.pop(0)                                                                                                  \r\n        print('start: %d, end: %d' % (self.start_length, len(hacked)))                                                     \r\n        del self.start_length                                                                                              \r\n                                                                                                                           \r\n# Data generators                                                                                                          \r\ndef genX(stddev=0.01, update=True):                                                                                        \r\n    while True:                                                                                                            \r\n        for i in range(20):                                                                                                \r\n            if update:                                                                                                     \r\n                global train_cntX                                                                                          \r\n                train_cntX += 1                                                                                            \r\n            yield {'input': np.random.normal(i + 1, stddev, (32, 10))}                                                     \r\n    return                                                                                                                 \r\n                                                                                                                           \r\ndef geny(update=True):                                                                                                     \r\n    while True:                                                                                                            \r\n        for i in range(20):                                                                                                \r\n            if update:                                                                                                     \r\n                global train_cntY                                                                                          \r\n                train_cntY += 1                                                                                            \r\n            yield {'output': np.ones((32, 1)) * ((i + 10) % 20)}                                                           \r\n    return                                                                                                                 \r\n                                                                                                                           \r\ndef genTrain():                                                                                                            \r\n    for x, y in zip(genX(), geny()):                                                                                       \r\n        yield x, y                                                                                                         \r\n    return                                                                                                                 \r\n                                                                                                                           \r\ndef genValid():                                                                                                            \r\n    for x, y in zip(genX(1e-4, update=False), geny(update=False)):                                                         \r\n        data = x, y                                                                                                        \r\n        hacked.append(data)                                                                                                \r\n        yield data                                                                                                         \r\n    return                                                                                                                 \r\n                                                                                                                           \r\n                                                                                                                           \r\n# Model & training                                                                                                         \r\ninp = Input((10,), name='input')                                                                                           \r\nout = Dense(20, activation='relu')(inp)                                                                                    \r\nout = Dense(20, activation='relu')(out)                                                                                    \r\nout = Dropout(0.5)(out)                                                                                                    \r\nout = Dense(1, name='output')(out)                                                                                         \r\nmodel = Model(inp, out)                                                                                                    \r\nmodel.compile(loss=K.losses.mean_squared_error,                                                                            \r\n              optimizer=K.optimizers.Adadelta(0.1, decay=0.01))                                                            \r\n                                                                                                                           \r\nmodel.fit(genTrain(),                                                                                                      \r\n          epochs=epochs, steps_per_epoch=20,                                                                               \r\n          validation_data=genValid(), validation_steps=20,                                                                 \r\n          callbacks=[TrackingCB(20)], verbose=0)                                                                           \r\n                                                                                                                           \r\n                                                                                                                           \r\nprint('tensorflow:', tf.__version__)                                                                                       \r\nprint('keras:', K.__version__)                                                                                             \r\nprint('train generator counts:', train_cntX, train_cntY)                                                                   \r\nprint('validation remaining data length:', len(hacked))                                                                    \r\n\r\n```\r\n**Other info / logs**\r\n\r\nIn keras, the amount of extra data extracted is fixed after each epoch, and it doesn't grow with more epochs, which shows that the generator is used for exactly 20 validation steps as specified.\r\n```\r\n$ python3 keras_gen_debug.py keras 2\r\nUsing TensorFlow backend.\r\nstart: 0, end: 11\r\nstart: 11, end: 11\r\ntensorflow: 2.1.0\r\nkeras: 2.3.1\r\ntrain generator counts: 51 51\r\nvalidation remaining data length: 11\r\n```\r\n\r\n```\r\n$ python3 keras_gen_debug.py keras 5\r\nUsing TensorFlow backend.\r\nstart: 0, end: 11\r\nstart: 11, end: 11\r\nstart: 11, end: 11\r\nstart: 11, end: 11\r\nstart: 11, end: 11\r\ntensorflow: 2.1.0\r\nkeras: 2.3.1\r\ntrain generator counts: 111 111\r\nvalidation remaining data length: 11\r\n```\r\n\r\nIn tensorflow.keras, the amount of extra data extracted is growing after each epoch, which shows that after the 20 validation steps, there are extra 13 batches that were generated after each epoch. (this can be noticed in detail when `verbose=1`)\r\n\r\n```\r\n$ python3 keras_gen_debug.py tensorflow 2\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  {'output': '...'}\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  {'output': '...'}\r\n    to  \r\n  ['...']\r\nstart: 1, end: 13\r\nstart: 13, end: 26\r\ntensorflow: 2.1.0\r\nkeras: 2.2.4-tf\r\ntrain generator counts: 53 53\r\nvalidation remaining data length: 26\r\n```\r\n\r\n```\r\n$ python3 keras_gen_debug.py tensorflow 5\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  {'output': '...'}\r\n    to  \r\n  ['...']\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  {'output': '...'}\r\n    to  \r\n  ['...']\r\nstart: 1, end: 13\r\nstart: 13, end: 26\r\nstart: 26, end: 39\r\nstart: 39, end: 52\r\nstart: 52, end: 65\r\ntensorflow: 2.1.0\r\nkeras: 2.2.4-tf\r\ntrain generator counts: 113 113\r\nvalidation remaining data length: 65\r\n```", "comments": ["I have tried on colab with TF version 2.1 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1543a7d44b0816f57b5c2fdf7fd1fd8a/untitled573.ipynb). Thanks!", "Any update on this or workaround to avoid silently crashing the kernel once the leak gets too severe, as I'm experiencing?", "I am also experiencing silent crashes during validation using tensorflow.keras. Wondering if there is any update?", "@pavithrasv is there any updates regarding this issue? ", "@mostafa-mahmoud Sorry for the late response. I think this was resolved in recent `tf-nightly`. I am seeing same results with `keras` and `tf.keras`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/86776a51cdfa828d55d771fa8240bbe2/untitled573.ipynb) is the gist for your reference. \r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan \r\nThe issue is not resolved, due to the following reasons:\r\n\r\n1. `keras` 2.2.4 didn't have this issue, however, newer versions of `keras` have it because they implicitly import `tf.keras` which has the bug. This is shown [here](https://github.com/keras-team/keras/pull/14121)\r\n2. The issue is to resolve the memory leak done by the `fit_generator` and not to make the behaviour identical to `keras`, in the gist you posted, there are still more items being leaked after each epoch, this wasn't the case in the logs I posted above from `keras` v2.2.4. This can be monitored by seeing that the number in the last line increases with more epochs.\r\n\r\n", "@omalleyt12 Hello, is there any response regarding this bug?", "I experience the same issue on 2.4.0.", "@omalleyt12 is there any updates regarding this issue? Please note that comparing to keras (particularly v2.4+) is no longer valid , because \u201akeras\u2018 now internally is just importing \u201atensorflow.keras\u2018, so it\u2019s just an alias for the Same erroneous behavior.  Also the issue has improved from tf2.1 to 2.4, do the leakage is not as sever but it *still* happens.", "@mostafa-mahmoud Is this still an issue for you?  Can you please check with recent TF versions (`TF2.7` and `tf-nightly`) and let us know whether the issue persists.\r\n\r\nPlease note that Keras development moved to another repository to focus entirely on only keras. Could you please repost this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues). Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35982\">No</a>\n"]}, {"number": 35981, "title": "Error while generating TensorflowLite file", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nException ignored in: <bound method _CheckpointRestoreCoordinator.__del__ of <tensorflow.python.training.tracking.util._CheckpointRestoreCoordinator object at 0x7f0f9ab16f98>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\", line 244, in __del__\r\n    .format(pretty_printer.node_names[node_id]))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\", line 93, in node_names\r\n    path_to_root[node_id] + (child.local_name,))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/object_identity.py\", line 76, in __getitem__\r\n    return self._storage[self._wrap_key(key)]\r\nKeyError: (<tensorflow.python.training.tracking.object_identity._ObjectIdentityWrapper object at 0x7f0f91483ac8>,)\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-64-e4d06fcc1815> in <module>()\r\n     13 # Convert the model to standard TensorFlow Lite model\r\n     14 converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n---> 15 converted_tflite_model = converter.convert()\r\n     16 open(TFLITE_MODEL, \"wb\").write(converted_tflite_model)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    170       stderr = _try_convert_to_unicode(stderr)\r\n    171       raise ConverterError(\r\n--> 172           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    173   finally:\r\n    174     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-01-17 11:53:28.108739: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: IdentityN\r\n2020-01-17 11:53:28.144111: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 707 operators, 1294 arrays (0 quantized)\r\n2020-01-17 11:53:28.168667: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 707 operators, 1294 arrays (0 quantized)\r\n2020-01-17 11:53:28.306950: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 128 operators, 326 arrays (0 quantized)\r\n2020-01-17 11:53:28.316129: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 125 operators, 321 arrays (0 quantized)\r\n2020-01-17 11:53:28.319357: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 124 operators, 319 arrays (0 quantized)\r\n2020-01-17 11:53:28.322587: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 124 operators, 319 arrays (0 quantized)\r\n2020-01-17 11:53:28.325002: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 124 operators, 319 arrays (0 quantized)\r\n2020-01-17 11:53:28.333533: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 11139584 bytes, theoretical optimal value: 8297856 bytes.\r\n2020-01-17 11:53:28.334861: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MEAN, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MEAN, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\n\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@vipulraut, Please provide the minimal code snippet to replicate the reported issue. Thanks! ", "Hi \r\n@gadagashwini - Thank you for your response.\r\nI have generated TFLite files using steps given on https://medium.com/@yannicksergeobam/plant-disease-classification-with-tensorflow-2-0-268fe7f72c2a.\r\n\r\nWhen I am trying to use this TFLite file it is crashing at . [Github link](https://github.com/obeshor/Plant-Diseases-Detector/blob/master/GreenDoctor/app/src/main/java/isomora/com/greendoctor/Classifier.kt)\r\n\r\n init {\r\n        INTERPRETER = Interpreter(loadModelFile(assetManager, modelPath))\r\n        LABEL_LIST = loadLabelList(assetManager, labelPath)\r\n    }\r\n\r\nBelow is its crash log\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: isomora.com.greendoctor, PID: 513\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{isomora.com.greendoctor/isomora.com.greendoctor.MainActivity}: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'IdentityN' with version 1\r\n    Registration failed.\r\n    \r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2957)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3032)\r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1696)\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6944)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:327)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1374)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'IdentityN' with version 1\r\n    Registration failed.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:69)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:182)\r\n        at isomora.com.greendoctor.Classifier.<init>(Classifier.kt:35)\r\n        at isomora.com.greendoctor.MainActivity.onCreate(MainActivity.kt:37)\r\n        at android.app.Activity.performCreate(Activity.java:7183)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1220)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2910)\r\n", "@vipulraut If you are using the [Python converter](https://www.tensorflow.org/lite/convert/python_api), with the latest version of TF/TFLite (nightly maybe?) could you try adding the following line pre-conversion:\r\n\r\n```\r\nconverter.experimental_new_converter = True\r\n```\r\n\r\nIf that doesn't work, it looks like we don't support the op yet :-(. You could try using [Select TF ops](https://www.tensorflow.org/lite/guide/ops_select) if you are using our C++/Java APIs and binary-size isn't that big an issue.", "@vipulraut Does this model use any TF RNN? We recently announced e2e support for TF 2.0 Keras LSTM to TFLite fused LSTM op. Please see below:\r\nhttps://groups.google.com/a/tensorflow.org/g/tflite/c/Ub4apUvblN8\r\n\r\nDo you want to try this out?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35981\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35981\">No</a>\n"]}, {"number": 35980, "title": "Tensorflow predict call crashes when loading a model with gevent enabled", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6, also tried it on the Docker container nvidia/cuda:10.1-cudnn7-runtime\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7.4 on Mac, Python 3.6.9 :: Anaconda, Inc. in Docker\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nTensorflow crashes after calling `predict` on the model, this happens with gevent 1.4.0 and also 1.5a2\r\n\r\n**Describe the expected behavior**\r\nTensorflow doesn't crash\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom gevent import monkey\r\nmonkey.patch_all()\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclassifier = tf.keras.models.load_model('tensorflow_model_dir')\r\nclassifier.predict(np.array(\r\n    np.zeros((1, 12623))\r\n))\r\n```\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 430, in eager_learning_phase_scope\r\n    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value\r\n  File \"/opt/conda/envs/py36/lib/python3.6/weakref.py\", line 407, in __setitem__\r\n    self.data[ref(key, self._remove)] = value\r\nTypeError: cannot create weak reference to 'gevent._local.local' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"scripts/gevent_load_classifier.py\", line 9, in <module>\r\n    np.zeros((1, 12623))\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1013, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 498, in predict\r\n    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 475, in _model_iteration\r\n    total_epochs=1)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2164, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 212, in _predict_on_batch\r\n    result = predict_on_batch(model, x)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 555, in predict_on_batch\r\n    with backend.eager_learning_phase_scope(0):\r\n  File \"/opt/conda/envs/py36/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 437, in eager_learning_phase_scope\r\n    del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]\r\n  File \"/opt/conda/envs/py36/lib/python3.6/weakref.py\", line 391, in __delitem__\r\n    del self.data[ref(key)]\r\nTypeError: cannot create weak reference to 'gevent._local.local' object\r\n\r\n```\r\n", "comments": ["Tried replicating the issue from[ given code](https://colab.sandbox.google.com/gist/oanush/6e224557fe243a2e53b7421176afbfa0/35980.ipynb),it just keeps running without any output.Thanks!", "Thanks for checking @oanush, I tried the notebook, but I think the notebook environment loads tensorflow before doing the monkey patch:\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/usr/local/lib/python3.6/dist-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/usr/local/lib/python3.6/dist-packages/urllib3/util/ssl_.py)']. \r\n```\r\n\r\nThis is the same warning I get when I import tensorflow before importing gevent. Also, to reproduce the error you probably need to load an actual tensorflow model (replace `tensorflow_model_dir`)", "Looks like an error in the environment itself. Can you try this in a new virtual environment and let me know if you are facing the same issue?", "@gowthamkpr I've tried creating a seperate containter for this purpose:\r\n\r\n```\r\nFROM python:3.6-slim\r\n\r\nRUN apt-get update \\\r\n    && apt-get install -y --no-install-recommends build-essential \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\nRUN pip install tensorflow gevent\r\n\r\nCOPY tf_test.py /opt/test/tf_test.py\r\nCOPY model_dir /opt/test/model_dir\r\n\r\nCMD python /opt/test/tf_test.py\r\n```\r\n\r\nwhere `tf_test.py` contains the following code:\r\n\r\n```\r\nfrom gevent import monkey\r\nmonkey.patch_all()\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclassifier = tf.keras.models.load_model('/opt/test/model_dir')\r\nclassifier.predict(np.array(\r\n    np.zeros((1, 12623))\r\n))\r\n```\r\n\r\nThis is the complete output:\r\n```\r\n2020-01-24 09:56:31.304096: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-01-24 09:56:31.304280: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-01-24 09:56:31.304324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-01-24 09:56:31.867084: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-01-24 09:56:31.867143: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-01-24 09:56:31.867175: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b3b6a3003434): /proc/driver/nvidia/version does not exist\r\n2020-01-24 09:56:31.867464: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-24 09:56:31.874365: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\r\n2020-01-24 09:56:31.875321: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a8216a44f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-24 09:56:31.875375: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 430, in eager_learning_phase_scope\r\n    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value\r\n  File \"/usr/local/lib/python3.6/weakref.py\", line 407, in __setitem__\r\n    self.data[ref(key, self._remove)] = value\r\nTypeError: cannot create weak reference to 'gevent._local.local' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/test/tf_test.py\", line 9, in <module>\r\n    np.zeros((1, 12623))\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1013, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 498, in predict\r\n    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 475, in _model_iteration\r\n    total_epochs=1)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2164, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 212, in _predict_on_batch\r\n    result = predict_on_batch(model, x)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 555, in predict_on_batch\r\n    with backend.eager_learning_phase_scope(0):\r\n  File \"/usr/local/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 437, in eager_learning_phase_scope\r\n    del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]\r\n  File \"/usr/local/lib/python3.6/weakref.py\", line 391, in __delitem__\r\n    del self.data[ref(key)]\r\nTypeError: cannot create weak reference to 'gevent._local.local' object\r\n```", "You can also reproduce the error with the code from the [official beginner tutorial](https://www.tensorflow.org/tutorials/quickstart/beginner):\r\n\r\n```\r\nfrom gevent import monkey\r\nmonkey.patch_all()\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dropout(0.2),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\n\r\nmodel.evaluate(x_test, y_test, verbose=2)\r\n```", "I can confirm this issue. I spent some time browsing the code today, and discovered the following. \r\n\r\n`_GRAPH_LEARNING_PHASES ` is a `weakref.WeakKeyDictionary`, and at some point a learning phase is added with the key `_DUMMY_EAGER_GRAPH`, which is a `threading.local()` object. Because Gevent monkey patching replaces this local object with a `gevent._local.local`, which cannot be weakly referenced, lookups for `_DUMMY_EAGER_GRAPH` will throw the exception above.\r\n\r\nI am not sure the use of `WeakKeyDictionary` is really necessary here, or perhaps there are alternatives. @iganichev I see that you introduced them a while back, perhaps you can shed some light on this?", "WeakKeyDictionary is definitely needed. There are a couple of issues here and I can submit a fix. Can somebody quickly test if something like this would work with gevent?\r\n\r\n```\r\ndiff --git a/google3/third_party/tensorflow/python/keras/backend.py b/google3/third_party/tensorflow/python/keras/backend.py\r\n--- a/google3/third_party/tensorflow/python/keras/backend.py\r\n+++ b/google3/third_party/tensorflow/python/keras/backend.py\r\n@@ -110,7 +110,14 @@ py_any = any\r\n # _DUMMY_EAGER_GRAPH is used as a key in _GRAPH_LEARNING_PHASES.\r\n # We keep a separate reference to it to make sure it does not get removed from\r\n # _GRAPH_LEARNING_PHASES.\r\n-_DUMMY_EAGER_GRAPH = threading.local()\r\n+class DummyEagerGraph(threading.local):\r\n+  class Foo(object):\r\n+    pass\r\n+  def __init__(self):\r\n+    super(DummyEagerGraph, self).__init__()\r\n+    self.key = Foo()\r\n+\r\n+\r\n+_DUMMY_EAGER_GRAPH = DummyEagerGraph()\r\n \r\n # This boolean flag can be set to True to leave variable initialization\r\n # up to the user.\r\n@@ -295,17 +302,17 @@ def learning_phase():\r\n     # will always execute non-eagerly using a function-specific default\r\n     # subgraph.\r\n     if context.executing_eagerly():\r\n-      if _DUMMY_EAGER_GRAPH not in _GRAPH_LEARNING_PHASES:\r\n+      if _DUMMY_EAGER_GRAPH.key not in _GRAPH_LEARNING_PHASES:\r\n         # Fallback to inference mode as default.\r\n         return 0\r\n-      return _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]\r\n+      return _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key]\r\n     learning_phase = symbolic_learning_phase()\r\n     _mark_func_graph_as_unsaveable(graph, learning_phase)\r\n     return learning_phase\r\n \r\n \r\n def global_learning_phase_is_set():\r\n-  return _DUMMY_EAGER_GRAPH in _GRAPH_LEARNING_PHASES\r\n+  return _DUMMY_EAGER_GRAPH.key in _GRAPH_LEARNING_PHASES\r\n \r\n \r\n def _mark_func_graph_as_unsaveable(graph, learning_phase):\r\n@@ -356,7 +363,7 @@ def set_learning_phase(value):\r\n     if context.executing_eagerly():\r\n       # In an eager context, the learning phase values applies to both the eager\r\n       # context and the internal Keras graph.\r\n-      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value\r\n+      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = value\r\n     _GRAPH_LEARNING_PHASES[get_graph()] = value\r\n \r\n \r\n@@ -384,7 +391,7 @@ def learning_phase_scope(value):\r\n   with ops.init_scope():\r\n     if context.executing_eagerly():\r\n       previous_eager_value = _GRAPH_LEARNING_PHASES.get(\r\n-          _DUMMY_EAGER_GRAPH, None)\r\n+          _DUMMY_EAGER_GRAPH.key, None)\r\n     previous_graph_value = _GRAPH_LEARNING_PHASES.get(get_graph(), None)\r\n \r\n   try:\r\n@@ -395,9 +402,9 @@ def learning_phase_scope(value):\r\n     with ops.init_scope():\r\n       if context.executing_eagerly():\r\n         if previous_eager_value is not None:\r\n-          _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = previous_eager_value\r\n-        elif _DUMMY_EAGER_GRAPH in _GRAPH_LEARNING_PHASES:\r\n-          del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]\r\n+          _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = previous_eager_value\r\n+        elif _DUMMY_EAGER_GRAPH.key in _GRAPH_LEARNING_PHASES:\r\n+          del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key]\r\n \r\n       graph = get_graph()\r\n       if previous_graph_value is not None:\r\n@@ -427,14 +434,14 @@ def eager_learning_phase_scope(value):\r\n   if global_learning_phase_was_set:\r\n     previous_value = learning_phase()\r\n   try:\r\n-    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value\r\n+    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = value\r\n     yield\r\n   finally:\r\n     # Restore learning phase to initial value or unset.\r\n     if global_learning_phase_was_set:\r\n-      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = previous_value\r\n+      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = previous_value\r\n     else:\r\n-      del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]\r\n+      del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key]\r\n \r\n \r\n def _current_graph(op_input_list):\r\n```\r\n\r\n", "Yes, this seems to resolve the issue. I did have to modify the patch to reference 'Foo' correctly: `self.key = DummyEagerGraph.Foo()`.", "Great. I will make the patch and send it out internally (a little easier). I should make it to github tomorrow night or a bit later.", "That's amazing, thanks for the quick response!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35980\">No</a>\n"]}, {"number": 35979, "title": "What's the difference between Keras applications and TF Model Garden?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications\r\nhttps://github.com/tensorflow/models/tree/master/official\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThere are the pre-trained models for Keras found in `tf.keras.applications`. And there are those models found on GitHub in the Model Garden (under `/models`, as linked above). Now, from reading the docs (both for the applications as well as those in the Model Garden) I don't get the difference. Why do we have those two different model repos?", "comments": ["https://keras.io/#multi-backend-keras-and-tfkeras\r\nGo through this link,will help ", "@atharva1503 Sorry but I don't see how this helps. I understand what Keras applications are (= pre-trained models based on known architectures). But that page doesn't explain how they differ from those models found on GitHub Model Garden.\r\n\r\nOr in other words: Which of those repos (Keras apps vs. GitHub MG) should I use in which case?", "@haimat keras.applications are pre-trained models for CNNs. They include most frequently used CNN architectures such as ResNet, InceptionNet, VGG etc. \r\n\r\n- Whereas the other tf model garden includes models for other tasks such as NLP, object detection etc. These models are too maintained by TensorFlow (if they are in official).\r\n\r\ntf.keras.applications allow you to directly import a CNN  architecture (see docs) customize its loss and use it as you like.\r\n\r\n- tf model garden are not part of tf core API so you can't import them directly as you do for tf.applications. you would need to follow their docs.", "@oke-aditya Thanks for the explanation!", "y no es el \u00fanico en el  valle  de los modelos ya que tambi\u00e9n existe tensorflow hub  el cual tambi\u00e9n  incluye gran variedad de arquitecturas y todas ellas entrenadas en distintos conjuntos de datos(imagenet 1k y imagenet 21k). lo que sobresale es que siguen un nueva filosof\u00eda BIT o big transfer, la cual entrena los modelos en grandes conjuntos de datos como imagenet 21k con 21000 clases diferentes, pero grandes conjuntos de datos requieren grandes arquitecturas por eso modificaron arquitecturas como Resnet50 a Resnet50x4  arquitecturas mas anchas capases de resolver las tareas. mientras que keras.aplications solo estan entrenados en imagenet 1k(1000 clases). date una vuelta por mi github donde tengo muy bien detallada la diferencie entre estas opciones para elegir modelo: keras.aplications vs tensorflow hub vs tensor flow garden  vs model zoo"]}, {"number": 35978, "title": "[Intel MKL] Add weight cache for FP32 MatMul.", "body": "This PR will cache weight for FP32 MatMul if it's const. Also move some common code related to cache weight from `mkl_qmatmul_op.cc` to `mkl_matmul_ops_common.h`.\r\n\r\nModified:\r\n- tensorflow/core/graph/mkl_layout_pass.cc\r\n- tensorflow/core/kernels/mkl_fused_ops_test.cc\r\n- tensorflow/core/kernels/mkl_matmul_op_fused.cc\r\n- tensorflow/core/kernels/mkl_matmul_ops_common.h\r\n- tensorflow/core/kernels/mkl_qmatmul_op.cc\r\n- tensorflow/core/ops/mkl_nn_ops.cc\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["@Zantares Could you please check reviewer comments and keep us posted. Thanks!", "Hi @gbaned , thanks for your reminder. Because of the well-known virus in China, my work will be delayed in 1~2 weeks. I'll refine this PR as soon as I get back to work, thanks.", "@Zantares Please take your time and I wish you all the best!", "Hi @penpornk , please have a look at the modified code, thanks!"]}]