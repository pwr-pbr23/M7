[{"number": 18097, "title": "Spelling error breaks GANEstimator documentation example", "body": "Fixed a spelling error that broke the tfgan.estimator.GANEstimator documentation example", "comments": []}, {"number": 18095, "title": "Branch 190953197", "body": "", "comments": []}, {"number": 18094, "title": "`tf.keras.estimator._create_ordered_io` casts everything to floatx, which breaks non-floatx inputs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 3.16.36\r\n- **TensorFlow installed from (source or binary)**: Installed via pip\r\n- **TensorFlow version (use command below)**: `('v1.6.0-0-gd2e24b6039', '1.6.0')`\r\n- **Python version**: 2.7.9\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: Requires significant code, let me know if necessary\r\n\r\n### Describe the problem\r\n\r\nThis is kind of a simple issue with using `Keras` models as Tensorflow Estimators. I unfortunately need to do this awkward conversion in order to use SageMaker, which is even more awkwardly behind by two versions of Tensorflow. Which is fun.\r\n\r\nBasically, I have a `Keras` model that expects a `tf.string` input `dtype`, which is then passed through to a Lookup layer for some text embeddings. This works fine as a Keras model and works fine if I extract the input layers myself and connect them into an Estimator. However, if I go to create an estimator from the model using `model_to_estimator` I run into this code path: https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/keras/_impl/keras/estimator.py#L80\r\n\r\nThis conversion then causes the model to break further down the line. I'm not sure why this float cast occurs, but this commit https://github.com/tensorflow/tensorflow/commit/4c86ece040cb96ea689f5c0d084b6959274eab91#diff-69effda952f96b36c8015cc1a3462d65 seems to imply that Keras models are meant to only take floatx input, which doesn't really seem right.\r\n\r\nWould not doing this cast break anything? If so, is there a way to use a non-float32 input with Keras models that need to be converted to Estimators?\r\n\r\nThanks!\r\n\r\n### Source code / logs\r\n\r\nHere's the exact traceback for the issue:\r\n\r\n```\r\n/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument \r\nof issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.          \r\n  from ._conv import register_converters as _register_converters                                                                           \r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp6Wogzk                                                               \r\n2018-03-29 14:12:41.586292: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow       \r\nbinary was not compiled to use: AVX2 FMA                                                                                                   \r\nWARNING:tensorflow:Output \"final_representation\" missing from loss dictionary. We assume this was done on purpose, and we will not be      \r\nexpecting any data to be passed to \"final_representation\" during training.                                                                 \r\nWARNING:tensorflow:Output \"oov_code\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any    \r\ndata to be passed to \"oov_code\" during training.                                                                                           \r\nTesting common_estimator_fns.py locally                                                                                                    \r\nMaking estimator                                                                                                                           \r\nModel dir: /tmp/tmp6Wogzk                                                                                                                  \r\nTraining estimator                                                                                                                         \r\nfloat64                                                                                                                                    \r\nTensor(\"random_shuffle_queue_DequeueMany:1\", shape=(32, 1), dtype=string, device=/device:CPU:0)                                            \r\nTraceback (most recent call last):                                                                                                         \r\n  File \"common_estimator_fns.py\", line 423, in <module>                                                                                    \r\n    hooks=[tf_debug.LocalCLIDebugHook()])                                                                                                  \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 352, in train \r\n    loss = self._train_model(input_fn, hooks, saving_listeners)                                                                            \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 812, in       \r\n_train_model                                                                                                                               \r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)                                                                            \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 793, in       \r\n_call_model_fn                                                                                                                             \r\n    model_fn_results = self._model_fn(features=features, **kwargs)                                                                         \r\n  File \"common_estimator_fns.py\", line 381, in model_fn                                                                                    \r\n    return keras_model_fn(features, labels, mode)                                                                                          \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 160,  \r\nin model_fn                                                                                                                                \r\n    labels)                                                                                                                                \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 109,  \r\nin _clone_and_build_model                                                                                                                  \r\n    model = models.clone_model(keras_model, input_tensors=input_tensors)                                                                   \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/models.py\", line 1557, in \r\nclone_model                                                                                                                                \r\n    return _clone_functional_model(model, input_tensors=input_tensors)                                                                     \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/models.py\", line 1451, in \r\n_clone_functional_model                                                                                                                    \r\n    output_tensors = topology._to_list(layer(computed_tensor, **kwargs))                                                                   \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line \r\n258, in __call__                                                                                                                           \r\n    output = super(Layer, self).__call__(inputs, **kwargs)                                                                                 \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 673, in __call__      \r\n    self._assert_input_compatibility(inputs)                                                                                               \r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 1204, in              \r\n_assert_input_compatibility                                                                                                                \r\n    ', found dtype=' + str(x.dtype))                                                                                                       \r\nValueError: Input 0 of layer lookedup is incompatible with the layer: expected dtype=<dtype: 'string'>, found dtype=<dtype: 'float32'>     \r\n```\r\n\r\nI can provide code if absolutely necessary, but it'd take some work to get to a minimal reproduction.", "comments": ["Looking at the history of this file, using other types were never tested.\r\n\r\nWe should use https://www.tensorflow.org/api_docs/python/tf/is_numeric_tensor before casting", "@fchollet Can you take a look at this?", "I confirm this bug. /cc @lenlen", "@yifeif what do you think about this issue? Could it be a simple fix?", "I opened a PR https://github.com/tensorflow/tensorflow/pull/18104 as a starting point."]}, {"number": 18093, "title": "Initial commit for the demo notebook", "body": "", "comments": []}, {"number": 18092, "title": "Distributed TensorFlow got error message with the MPI collective Ops units test", "body": "Have I written custom code:  None\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nOpen MPI version: 3.0.0\r\nTensorFlow installed from: b'v1.7.0-rc1-816-g1712002ad0' 1.7.0-rc1\r\nBazel version: 0.11.1 \r\nPython version: 3.5.2\r\nGCC version: 6.3.0\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: `mpirun -np 1 python mpi_allreduce_test.py` under directory `tensorflow/tensorflow/contrib/mpi_collectives`\r\n\r\nI ran the unit test under directory `tensorflow/tensorflow/contrib/mpi_collectives` and the run command is `mpirun -np 1 python mpi_allreduce_test.py` . I got the following error message:\r\nCould someone please help me take a look at this? Thanks!\r\n\r\n```\r\nWARNING:tensorflow:From /ec/fm/disks/nrv_algo_home01/langjian/.tf_17/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\nTraceback (most recent call last):\r\n  File \"mpi_allreduce_test.py\", line 23, in <module>\r\n    import tensorflow.contrib.mpi_collectives as mpi\r\n  File \"$HOME/.tf_17/lib/python3.5/site-packages/tensorflow/contrib/mpi_collectives/__init__.py\", line 128, in <module>\r\n    from tensorflow.contrib.mpi_collectives.python.ops.mpi_ops import init\r\n  File \"$HOME/.tf_17/lib/python3.5/site-packages/tensorflow/contrib/mpi_collectives/python/ops/mpi_ops.py\", line 29, in <module>\r\n    resource_loader.get_path_to_datafile('_mpi_ops.so'))\r\n  File \"$HOME/.tf_17/lib/python3.5/site-packages/tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"$HOME/.tf_17/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"$HOME/.tf_17/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: $HOME/.tf_17/lib/python3.5/site-packages/tensorflow/contrib/mpi_collectives/python/ops/_mpi_ops.so: undefined symbol: _ZN10tensorflow7contrib15mpi_collectives7MPITypeIiEEP15ompi_datatype_tv\r\n-------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n-------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. The first process to do so was:\r\n\r\n  Process name: [[18657,1],0]\r\n  Exit code:    1\r\n--------------------------------------------------------------------------\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Thanks a lot for the quick response! See the updated issue template below:\r\nHave I written custom code:  None\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nOpen MPI version: 3.0.0\r\nTensorFlow installed from: b'v1.7.0-rc1-816-g1712002ad0' 1.7.0-rc1\r\nBazel version: 0.11.1 \r\nPython version: 3.5.2\r\nGCC version: 6.3.0\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: `mpirun -np 1 python mpi_allreduce_test.py` under directory `tensorflow/tensorflow/contrib/mpi_collectives`", "@jthestness @poxvoculi : Any suggestions here? \r\n\r\nIt seems to be complaining about not finding `ompi_datatype_t* tensorflow::contrib::mpi_collectives::MPIType<int>()`, which should be defined in `ring.cc`.\r\n\r\nJust to be clear: You did build from sources using the instructions in [`contrib/mpi/README.md`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mpi/README.md), right?\r\n", "@asimshankar I did build the code using the instructions in `contrib/mpi/README.md`. And then run the unit test `mpirun -np 1 python mpi_allreduce_test.py` under directory `tensorflow/tensorflow/contrib/mpi_collectives`. I did not set `server = tf.train.Server(cluster, job_name=\"local\", task_index=0, protocol='grpc+mpi')`.", "We've tested TF mpi_collectives using OpenMPI 2.0.x, but we haven't thoroughly tested with OpenMPI 3.0.x. OpenMPI 2.0.x defines `MPI_Datatype` as `ompi_datatype_t` in `include/mpi.h`, so the symbol exists and is detected correctly. I see the same declarations in 3.0.x, so it should work the same if OpenMPI is built the same way. I would recommend checking the symbol table in your MPI libraries to see if the symbol is defined. If not, you might need to reconfigure and rebuild OpenMPI.", "@jthestness We checked the mpi.h header file and did see that it defines `MPI_Datatype` as `ompi_datatype_t` in OpenMPI3.0.0.\r\nWhich gcc compiler version do you use for the test? Thanks!", "@jianyinglang: We're using GCC 4.8.4. I would be very surprised if the GCC version is causing the problem. Did you check whether the symbol is defined in your MPI libraries (e.g., `objdump -t <lib> | grep ompi_datatype_t`)?", "@jthestness Yes, I did see it in the lib folder.\r\n```\r\n00000000002e0b40 g     O .data  0000000000000040              ompi_datatype_t_class\r\n00000000002ffe70 g     O .bss   0000000000000008              ompi_datatype_t_type_force_inclusion\r\n00000000002e0b40 g     O .data  0000000000000040              ompi_datatype_t_class\r\n00000000002ffe70 g     O .bss   0000000000000008              ompi_datatype_t_type_force_inclusion\r\n00000000002e0b40 g     O .data  0000000000000040              ompi_datatype_t_class\r\n00000000002ffe70 g     O .bss   0000000000000008              ompi_datatype_t_type_force_inclusion\r\n```", "Ok, you're going to need to step through the chain of defining symbols across the libraries to see where there is a missing link. Check that the Tensorflow library you built points to the correct MPI library with `ldd <path_to_tf_build>/python/_pywrap_tensorflow_internal.so`. If that's correct, then check the mpi_collectives library: `ldd <path_to_tf_build>/contrib/mpi_collectives/python/ops/_mpi_ops.so` and then `objdump -t <path_to_tf_build>/contrib/mpi_collectives/python/ops/_mpi_ops.so | grep ompi_datatype_t`. The latter should show something like this (note \"F\" means the function is defined, you probably have a \"U\" or the functions are missing):\r\n\r\n```\r\n00000000000265f0 g     F .text\t0000000000000008              _ZN10tensorflow7contrib15mpi_collectives7MPITypeIxEEP15ompi_datatype_tv\r\n00000000000265e0 g     F .text\t0000000000000008              _ZN10tensorflow7contrib15mpi_collectives7MPITypeIiEEP15ompi_datatype_tv\r\n00000000000265d0 g     F .text\t0000000000000008              _ZN10tensorflow7contrib15mpi_collectives7MPITypeIfEEP15ompi_datatype_tv\r\n```\r\n\r\nIf the function is not defined (\"U\" or missing), then something went wrong with the TF build. Be sure that you either opt 'yes' when prompted to configure the build with MPI or set the environment variable `TF_NEED_MPI=1` before configuring.\r\n\r\nAnother thing you could try is to roll back to TF 1.5.0, which we have working internally. (We haven't tested 1.7.0-rc1, and unfortunately, the regressions architecture for TF doesn't permit us to run MPI tests, so changes to TF can break the mpi_collectives code).", "I run mpirun -np 1 python mpi_allreduce_test.py ;the test pass.\r\nbut when I run mpirun -np 3 python mpi_allreduce_test.py, the test failed\r\n```\r\n1: iter 0\r\nrank 2: My output is 330772870.0\r\nrank 2: Our output is 170543409.0\r\nrank 0: My output is 11908270.0\r\nrank 0: Our output is 170543409.0\r\n2: iter 0\r\n0: iter 0\r\n[e92e09614.em21:08680] Read -1, expected 2362084, errno = 38\r\n[e92e09614.em21:08681] Read -1, expected 2362080, errno = 38\r\n[e92e09614.em21:08679] Read -1, expected 2362080, errno = 38\r\n[e92e09614.em21:08680] Read -1, expected 2362080, errno = 38\r\n[e92e09614.em21:08681] Read -1, expected 2362084, errno = 38\r\n[e92e09614.em21:08679] Read -1, expected 2362080, errno = 38\r\n[e92e09614.em21:08680] Read -1, expected 2362080, errno = 38\r\n```", "Hi @chengdianxuezi: Your issue looks different from the issue in this thread. Can you please create a new TF Github issue and follow TF's issue creation process (i.e., include information about the version of TF and other software that you are using. See here: https://github.com/tensorflow/tensorflow/issues/new)? Please also include the distribution and version of MPI you are using", "Hi @jthestness, I have the same problem. Here is the output when I try mpi_allreduce_test.py:\r\n```\r\ntensorflow/contrib/mpi_collectives$ mpirun -np 1 python mpi_allreduce_test.py\r\nTraceback (most recent call last):\r\n  File \"mpi_allreduce_test.py\", line 23, in <module>\r\n    import tensorflow.contrib.mpi_collectives as mpi\r\n  File \"/home/research/virtualenvs/tfenv/lib/python3.5/site-packages/tensorflow/contrib/mpi_collectives/__init__.py\", line 128, in <module>\r\n    from tensorflow.contrib.mpi_collectives.python.ops.mpi_ops import init\r\n  File \"/home/research/virtualenvs/tfenv/lib/python3.5/site-packages/tensorflow/contrib/mpi_collectives/python/ops/mpi_ops.py\", line 29, in <module>\r\n    resource_loader.get_path_to_datafile('_mpi_ops.so'))\r\n  File \"$HOME/virtualenvs/tfenv/lib/python3.5/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"$HOME/virtualenvs/tfenv/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: $HOME/virtualenvs/tfenv/lib/python3.5/site-packages/tensorflow/contrib/mpi_collectives/python/ops/_mpi_ops.so: undefined symbol: _ZN10tensorflow7contrib15mpi_collectives7MPITypeIiEEP15ompi_datatype_tv\r\n-------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n--------------------------------------------------------------------------\r\nmpirun detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. The first process to do so was:\r\n  Process name: [[35887,1],0]\r\n  Exit code:    1\r\n--------------------------------------------------------------------------\r\n```\r\n\r\nwhen I check ompi_datatype_t in _mpi_ops.so, I got following output:\r\n```\r\n$ objdump -t $HOME/virtualenvs/tfenv/lib/python3.5/site-packages/tensorflow/contrib/mpi_collectives/python/ops/_mpi_ops.so | grep ompi_datatype_t\r\n0000000000000000         *UND*\t0000000000000000              _ZN10tensorflow7contrib15mpi_collectives7MPITypeIiEEP15ompi_datatype_tv\r\n0000000000000000         *UND*\t0000000000000000              _ZN10tensorflow7contrib15mpi_collectives7MPITypeIxEEP15ompi_datatype_tv\r\n0000000000000000         *UND*\t0000000000000000              _ZN10tensorflow7contrib15mpi_collectives7MPITypeIfEEP15ompi_datatype_tv\r\n```\r\n\r\nI have ubuntu 16.04, \r\ntensorflow branch r1.13, (only mpi is enabled, all other options are disabled on build)\r\npython 3.5,\r\ngcc (Ubuntu 4.8.5-4ubuntu2) 4.8.5, \r\ntested with both bazel 0.19 and 0.21.\r\ntested with OpenMPI 2.1.6, 3.1.2, 4.0.1\r\n\r\nnot sure how to proceed. Any help would be really appreciated. \r\n", "@jianyinglang ,\r\nWe see that you are using older version of tensorflow (1.x) which is not actively supported. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18092\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18092\">No</a>\n"]}, {"number": 18091, "title": "Implement faster resampling for tf.data", "body": "This method leverages an observation: if we allow ourselves to either sample from a filtered distribution OR the original distribution, we can reduce the amount of data rejected during rejection sampling.  I implemented this 'conditional filtering' in a sort-of hacky way by internally creating an iterator and sticking it in a tf.cond branch. Feedback from a tf.data expert would be great.\r\n\r\n I also added unit tests to verify that the new sampler works and is faster than the original, and I added benchmarks.\r\n\r\n Tested: \r\nbazel test :resample_test", "comments": ["@joel-shor could you look at the test failures, please?https://source.cloud.google.com/results/invocations/d41fb9c0-f785-4583-8ba6-80b63dc68905/log", "Woops sorry! Reviewers must have been automatically added.\r\n\r\nI think the CL is ready for review now.", "@mrry / @ebrevdo would you please take a look?", "Replaced by PR #18730."]}, {"number": 18090, "title": "Bounds-check node ID before getting it's name", "body": "When the edge is either a frame enter or exit edge then DescribeCycle() would segfault.\r\n\r\nHere's some code to reproduce this issue.\r\n\r\n```Python\r\nimport tensorflow as tf                                                                                                                                                                                                                       \r\n\r\nwith tf.device('/cpu:0'):\r\n    xin = tf.placeholder(tf.float32, [None, 1, 1], name='input')\r\n    rnn_cell = tf.contrib.rnn.LSTMCell(1)\r\n    out, _ = tf.nn.dynamic_rnn(rnn_cell, xin, dtype=tf.float32)\r\n    out = tf.layers.batch_normalization(out, training=True)\r\n    out = tf.identity(out, name='output')\r\n\r\n    optimiser = tf.train.AdamOptimizer(.0001)\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n        out = optimiser.minimize(out, global_step=tf.Variable(0, dtype=tf.float32), name='train_op')\r\n\r\nconfig = tf.ConfigProto(allow_soft_placement = False)\r\nsess = tf.Session(config=config)\r\nsess.run(tf.global_variables_initializer())\r\n\r\nsample_in = [[[0]]]\r\nsess.run(out, feed_dict={xin: sample_in})\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@rmlarsen Not sure if you've noticed that I made the changes you requested.", "@TBastiani Thanks!"]}, {"number": 18089, "title": "Fix math equation format in tf.contrib.bayesflow.monte_carlo", "body": "This PR is to fix mess-up math equation format in below tf.contrib.bayesflow.monte_carlo related api docstrings.\r\n- [monte_carlo](https://www.tensorflow.org/api_docs/python/tf/contrib/bayesflow/monte_carlo)\r\n- [expectation](https://www.tensorflow.org/api_docs/python/tf/contrib/bayesflow/monte_carlo/expectation)\r\n- [expectation_importance_sampler](https://www.tensorflow.org/api_docs/python/tf/contrib/bayesflow/monte_carlo/expectation_importance_sampler)\r\n- [expectation_importance_sampler_logspace](https://www.tensorflow.org/api_docs/python/tf/contrib/bayesflow/monte_carlo/expectation_importance_sampler_logspace)\r\n\r\nThis PR is to fix this math equation issue according to the [Math in markdown guideline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md#math-in-markdown).\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1680977/38099738-f88ad194-33ad-11e8-8eeb-e3d28742badc.png)\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1680977/38100259-7ccfbd56-33af-11e8-86de-c4fd305f98a4.png)\r\n", "comments": []}, {"number": 18088, "title": "Flushing prints to stdout in an autograph-generated py_func", "body": "This change is required for a TF dev summit demo that is being held on Friday March 30. It is a manual patch from a change that already exists internally but has not propagated to GitHub. This is time-sensitive.", "comments": ["@alexbw please rebase to resolve conflict", "Resolved conflict manually."]}, {"number": 18087, "title": "is the sytem wrong leading tensorboard No dashboards are active for the current data set.", "body": "\r\n\r\n### Describe the problem\r\n\r\n i fisrt input tensorboard --logdir='logs',then i enter ctrl+c, it shows:\r\n![image](https://user-images.githubusercontent.com/3324257/38097184-45b93700-33a7-11e8-884d-72aaab629ce9.png)\r\nis my system wrong?\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "yes, it is still an issue. Please fix this tensorflow\r\n\r\n```\r\n(kpython) mohith@kune:~/Downloads/catvsdog$ tensorboard --logdir=log\r\nTensorBoard 1.11.0 at http://kune:6006 (Press CTRL+C to quit)\r\n\r\n```\r\nAnd nothing happens at the give web page, except showing \"No dashboards are active for the current data set\"\r\n"]}, {"number": 18086, "title": "distributed Tensorflow using grpc is slow", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**:  python 3.4\r\n- **CUDA/cuDNN version**: 8.0/6.1\r\n\r\n\r\n### Describe the problem\r\nDistributed Tensorflow using grpc is very slow , here is my timeline of a step \r\n![image](https://user-images.githubusercontent.com/33949779/38095081-2628836e-33a2-11e8-9f4c-883562bbf42f.png)\r\nI want to know why there is a huge gap. GPU is idle for a long time, waiting for ps.\r\n\r\n", "comments": ["I recently ran some benchmarks with Tensorflow grpc interface  [here](https://github.com/diux-dev/cluster/tree/master/psbench) and did not observe slowness.\r\n\r\nGenerally speaking, there can be many reasons for things to be slow. Waiting for a slow machine perhaps? I recommend simplifying your code to the minimum case that isolates the problem", "OK, Thanks", "@yaroslavvb I have another two questions. How does ps know when to fetch gradients from Rendezvous\uff1fHow do recv nodes in ps work? I think the answers of these two questions may explain why the RecvTensor in ps starts so late."]}, {"number": 18085, "title": "Jsonify serialisation error for tensorflow output ", "body": "\r\nI created a flask client in python that returns output of tensorflow model like this: \r\n\r\nResult is:\r\n```\r\n\r\noutputs {\r\n key: \"output\"\r\n value {\r\n dtype: DT_FLOAT\r\n tensor_shape {\r\n }\r\n float_val: -3.33661770821\r\n }\r\n}\r\n```\r\nAnd when i tried to jsonify this output . \r\n\r\n> def inference():\r\n>     ........\r\n>     return jsonify({'result':result})\r\n\r\nIt sends error:\r\n\r\n```\r\nFile \"/usr/lib/python2.7/json/encoder.py\", line 184, in default\r\n       raise TypeError(repo(o) + \"is not JSON serializable\")\r\nTypeError:outputs {\r\n     key: \"output\"\r\n     value {\r\n     dtype: DT_FLOAT\r\n     tensor_shape {\r\n     }\r\n     float_val: -3.33661770821\r\n     }\r\n    }\r\n```\r\nHow do i jsonify the above output? Any idea? This is not in dictionary too. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 18084, "title": "ERROR: /tensorflow/third_party/mkl/BUILD:45:12: Configurable attribute \"deps\" doesn't match this configuration", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n     Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n      Installed from source (master)\r\n- **TensorFlow version (use command below)**:\r\n      (master)\r\n- **Python version**: \r\n     Python 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n       0.11.1\r\n- **CUDA/cuDNN version**:\r\n     NA\r\n- **GPU model and memory**:\r\n      NA\r\n- **Exact command to reproduce**:\r\n      bazel test -c opt //tensorflow/...\r\n\r\n### Describe the problem\r\nI have started working on TensorFlow master. The build passed successfully , however the test command is failing with below error  - \r\n```\r\nERROR: /tensorflow/third_party/mkl/BUILD:45:12: Configurable attribute \"deps\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n@org_tensorflow//tensorflow:darwin\r\n@org_tensorflow//tensorflow:linux_x86_64\r\n@org_tensorflow//tensorflow:windows\r\nERROR: Analysis of target '//tensorflow/core/kernels:mkl_softmax_op' failed; build aborted:\r\n\r\n/tensorflow/third_party/mkl/BUILD:45:12: Configurable attribute \"deps\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n@org_tensorflow//tensorflow:darwin\r\n@org_tensorflow//tensorflow:linux_x86_64\r\n@org_tensorflow//tensorflow:windows\r\nINFO: Elapsed time: 4.290s\r\nFAILED: Build did NOT complete successfully (8 packages loaded)\r\nERROR: Couldn't start the build. Unable to run tests\r\n```\r\n\r\nLooks like need some code changes to fix this on ppc64le, I have started my analysis. Please provide if any suggestions.Thanks!", "comments": ["Looks like we need to add a \"conditions:default\" case here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/mkl/BUILD#L57", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The MKL optimizations are made by Intel, so I'd be quite surprised if they have, or ever will, port their math kernels to powerpc64le. The solution might be to not enable MKL support in ./configure. A conditions:default case probably wouldn't be appropriate here, due to the nature of this library. See also: https://software.intel.com/en-us/forums/intel-math-kernel-library/topic/299632"]}, {"number": 18082, "title": "Android speech recognition sample averaging wrong values", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: none\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  not related to this question\r\n- **TensorFlow installed from (source or binary)**: not related to this question\r\n- **TensorFlow version (use command below)**: not related to this question\r\n- **Python version**: not related to this question\r\n- **Bazel version (if compiling from source)**: not related to this question\r\n- **GCC/Compiler version (if compiling from source)**: not related to this question\r\n- **CUDA/cuDNN version**: not related to this question\r\n- **GPU model and memory**: not related to this question\r\n- **Exact command to reproduce**: not related to this question\r\n\r\n### Describe the problem &  Source code / logs\r\nPlease refer to this question and comments on stackoverflow: https://stackoverflow.com/questions/49269555/logging-and-deque-operation-problems-in-tensorflow-android-speech-recognition-sa\r\n\r\nIn brief, previousResults.addLast should be revised. Otherwise, it will take wrong values to average. \r\n", "comments": ["I'm sorry, but we can't answer stackoverflow q's here. If you there's a bug, please file a new issue filling out the template."]}, {"number": 18081, "title": "running distributed tensorflow failed after update tf from 1.0 to 1.5", "body": "Hi TF Experts,\r\n\r\nafter I upgrade the tf from 1.0 to 1.5(cuda from 8.0 to 9.0), run the example distributed tensorflow codes failed. below are the codes, it is very simple. The log always output \"CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\". the code works fine for any of my machines whose tf version is under 1.5\r\n```\r\nimport tensorflow as tf\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": [\"localhost:33562\"], \"worker\": [\"localhost:12563\"]})\r\n\r\nps = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\r\nworker = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\r\n\r\nprint(\"PS: {0}\".format(ps.target))\r\nprint(\"Worker: {0}\".format(worker.target))\r\n\r\nwith tf.Session(worker.target) as sess:\r\n\r\n    W = tf.Variable(tf.zeros([784, 10]))\r\n    b = tf.Variable(tf.zeros([10]))\r\n\r\n    init = tf.global_variables_initializer()\r\n\r\n    print(\"RUNNING SESSION\")\r\n    sess.run(init)\r\n    print(\"SESSION FINISHED\")\r\n```\r\nbelow are the full log:\r\n\r\n2018-03-29 15:17:37.084856: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-03-29 15:17:38.367466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-03-29 15:17:38.865113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 1 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-03-29 15:17:39.369172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 2 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-03-29 15:17:39.884151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 3 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:09:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-03-29 15:17:40.416114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 4 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:85:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-03-29 15:17:40.903122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 5 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:86:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-03-29 15:17:41.388600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 6 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:89:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-03-29 15:17:41.869760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 7 with properties: \r\nname: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112\r\npciBusID: 0000:8a:00.0\r\ntotalMemory: 23.90GiB freeMemory: 23.78GiB\r\n2018-03-29 15:17:41.875020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1227] Device peer to peer matrix\r\n2018-03-29 15:17:41.875488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1233] DMA: 0 1 2 3 4 5 6 7 \r\n2018-03-29 15:17:41.875509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 0:   Y Y Y Y N N N N \r\n2018-03-29 15:17:41.875523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 1:   Y Y Y Y N N N N \r\n2018-03-29 15:17:41.875533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 2:   Y Y Y Y N N N N \r\n2018-03-29 15:17:41.875543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 3:   Y Y Y Y N N N N \r\n2018-03-29 15:17:41.875553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 4:   N N N N Y Y Y Y \r\n2018-03-29 15:17:41.875563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 5:   N N N N Y Y Y Y \r\n2018-03-29 15:17:41.875572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 6:   N N N N Y Y Y Y \r\n2018-03-29 15:17:41.875585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 7:   N N N N Y Y Y Y \r\n2018-03-29 15:17:41.875614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2018-03-29 15:17:46.028033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:ps/replica:0/task:0/device:GPU:0 with 23084 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:46.610150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:ps/replica:0/task:0/device:GPU:1 with 23082 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:47.276168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:ps/replica:0/task:0/device:GPU:2 with 23080 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:48.026291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:ps/replica:0/task:0/device:GPU:3 with 23080 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:49.020908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:ps/replica:0/task:0/device:GPU:4 with 23084 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:49.806995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:ps/replica:0/task:0/device:GPU:5 with 23082 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:86:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:50.654150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:ps/replica:0/task:0/device:GPU:6 with 23080 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:51.499643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:ps/replica:0/task:0/device:GPU:7 with 23080 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:8a:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:52.345019: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:33562}\r\n2018-03-29 15:17:52.345127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12563}\r\n2018-03-29 15:17:52.362801: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:33562\r\n2018-03-29 15:17:52.363232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2018-03-29 15:17:52.363789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 983 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:52.364275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 985 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:52.364715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 987 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:52.365138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 987 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:52.366627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:worker/replica:0/task:0/device:GPU:4 with 983 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:52.367325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:worker/replica:0/task:0/device:GPU:5 with 985 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:86:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:52.367962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:worker/replica:0/task:0/device:GPU:6 with 987 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:52.368731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:worker/replica:0/task:0/device:GPU:7 with 987 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:8a:00.0, compute capability: 5.2)\r\n2018-03-29 15:17:52.376139: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:33562}\r\n2018-03-29 15:17:52.376230: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12563}\r\n2018-03-29 15:17:52.382382: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:12563\r\nPS: grpc://localhost:33562\r\nWorker: grpc://localhost:12563\r\nRUNNING SESSION\r\n2018-03-29 15:18:02.506942: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-03-29 15:18:12.507132: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-03-29 15:18:22.507277: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-03-29 15:18:32.507427: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-03-29 15:18:42.507560: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n", "comments": ["I run the command \"netstat -a\" found the port is actually listening:\r\n\r\nroot@100.88.66.48:/data1/rigorosyang/person_relation/model/dist# netstat -a | grep 12563\r\ntcp        0      0 0.0.0.0:12563           0.0.0.0:*               LISTEN \r\n\r\nroot@100.88.66.48:/data1/rigorosyang/person_relation/model/dist# netstat -a | grep 33\r\ntcp        0      0 0.0.0.0:33562           0.0.0.0:*               LISTEN", "/CC @mrry, can you take a look?", "The code runs to completion without error when I use a recent `tf-nightly` build. Can you check whether it is still a problem with the latest version?", "I use the latest 1.7.0, but still has this problem.\r\n\r\n\r\n: 5.2)\r\n2018-04-04 10:22:16.395687: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:33562}\r\n2018-04-04 10:22:16.395707: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12563}\r\n2018-04-04 10:22:16.396040: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:12563\r\nPS: grpc://localhost:33562\r\nWorker: grpc://localhost:12563\r\nRUNNING SESSION\r\n2018-04-04 10:22:26.428694: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-04-04 10:22:36.428805: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-04-04 10:22:46.428906: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0", "Can you try replacing `\"localhost:...\"` with `\"127.0.0.1:...\"`?", "I tried, still not work.", "Can you try running the code with the following environment variable set: `GRPC_VERBOSITY=DEBUG` and share the logs?", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of response. Feel free to reopen the bug if you can capture the requested gRPC logs."]}, {"number": 18080, "title": "How to combine two frozen models (Tensorflow) for object detection?", "body": "I am trying to combine two frozen models (protobuffs) for object detection. The issue is one of the models is my own dataset and the other is the prebuilt model for coco dataset (just include more classes to the dataset itself). Is this possible? or is there a better approach to perform this?", "comments": ["For object detection why you even need to combine models, where you can use both models and get best of both.\r\n\r\nRun model-1 on input image and run model-2 on input image.\r\nDraw boxes for result of model-1 and result of model-2 on same image will give you detections for both COCO classes and your own classes.\r\n\r\nAnother thing you can do is to fine-tune COCO model with new dataset to add new classes. See tranfer Learning to know how to do that.", "How it will read two model file at the same time in tensorflow session.please send me some example code", "```\r\nwith tf.gfile.GFile(model_filename1, \"rb\") as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\nwith tf.Graph().as_default() as graph1:\r\n    tf.import_graph_def(graph_def)\r\n\r\nwith tf.gfile.GFile(model_filename2, \"rb\") as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\nwith tf.Graph().as_default() as graph2:\r\n    tf.import_graph_def(graph_def)\r\n\r\nsess1 = tf.Session(graph=graph1)\r\nsess2 = tf.Session(graph=graph2)\r\n```\r\n\r\nAbove code might not work as it is, as I have not got chance to test these. But idea was that we can create two session type object for each models and call their run methods, obviously if you are using CUDA Hardware you will be blocked by calling run() if both called same time due to locking mechanism set by CUDA HW (CUDA_VISIBLE_DEVICES). But you can run this by setting Queue or calling run() one after another.\r\n\r\nI have loaded two graphs on same GPU (one graph was for preprocessing and another one was object classification) and it worked well for me. (I called run() one after another as I required output of 1st graph to be fed in to 2nd).\r\n\r\nHope you get idea.", "Below is my code.please look into that and give some clarification about two model loading\r\n\r\n```\r\n`# What model to download.\r\nMODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'\r\nMODEL_FILE = MODEL_NAME + '.tar.gz'\r\nDOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\r\n\r\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\r\n#PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\r\nPATH_TO_CKPT = 'screwdriver_graph/frozen_inference_graph.pb'\r\n\r\n\r\n# List of the strings that is used to add correct label for each box.\r\n#PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\r\nPATH_TO_LABELS = os.path.join('data', 'screwdriver_label.pbtxt')\r\n\r\nNUM_CLASSES = 1\r\n#NUM_CLASSES = 90\r\n\r\n\r\n# ## Download Model\r\n\r\n#if not os.path.exists(MODEL_NAME + '/frozen_inference_graph.pb'):\r\nif not os.path.exists('screwdriver_graph/frozen_inference_graph.pb'):\r\n\tprint ('Downloading the model')\r\n\topener = urllib.request.URLopener()\r\n\topener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\r\n\ttar_file = tarfile.open(MODEL_FILE)\r\n\tfor file in tar_file.getmembers():\r\n\t  file_name = os.path.basename(file.name)\r\n\t  if 'frozen_inference_graph.pb' in file_name:\r\n\t    tar_file.extract(file, os.getcwd())\r\n\tprint ('Download complete')\r\nelse:\r\n\tprint ('Model already exists')\r\n\r\n# ## Load a (frozen) Tensorflow model into memory.\r\n\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n  od_graph_def = tf.GraphDef()\r\n  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n    serialized_graph = fid.read()\r\n    od_graph_def.ParseFromString(serialized_graph)\r\n    tf.import_graph_def(od_graph_def, name='')\r\n\r\n\r\n# ## Loading label map\r\n# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine\r\n\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n#intializing the web camera device\r\n\r\nimport cv2\r\ncap = cv2.VideoCapture(1)\r\n\r\n\r\n\r\n# Running the tensorflow session\r\nwith detection_graph.as_default():\r\n  with tf.Session(graph=detection_graph) as sess:\r\n   ret = True\r\n   while (ret):\r\n      ret,image_np = cap.read()\r\n      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\r\n      image_np_expanded = np.expand_dims(image_np, axis=0)\r\n      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n      # Each box represents a part of the image where a particular object was detected.\r\n      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n      # Each score represent how level of confidence for each of the objects.\r\n      # Score is shown on the result image, together with the class label.\r\n      scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\n      classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n      num_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n      # Actual detection.\r\n      (boxes, scores, classes, num_detections) = sess.run(\r\n          [boxes, scores, classes, num_detections],\r\n          feed_dict={image_tensor: image_np_expanded})\r\n      # Visualization of the results of a detection.\r\n      vis_util.visualize_boxes_and_labels_on_image_array(\r\n          image_np,\r\n          np.squeeze(boxes),\r\n          np.squeeze(classes).astype(np.int32),\r\n          np.squeeze(scores),\r\n          category_index,\r\n          use_normalized_coordinates=True,\r\n          line_thickness=8)\r\n#      plt.figure(figsize=IMAGE_SIZE)\r\n#      plt.imshow(image_np)\r\n      #cv2.putText(img,'OpenCV',(10,500), font, 4,(255,255,255),2,2)\r\n      cv2.imshow('Detected Tools',cv2.resize(image_np,(1000,850)))\r\n      #cv2.imshow('Tools Name',cv2.resize(cv2.putText(img,'OpenCV',(10,500), font, 4,(255,255,255),2,2),(100,150)))\r\n      if cv2.waitKey(25) & 0xFF == ord('q'):\r\n          cv2.destroyAllWindows()\r\n          cap.release()\r\n          break\r\n\r\n\r\n`\r\n```\r\n\r\nThanks for helping ", "```\r\nimport cv2\r\nimport tensorflow as tf\r\n\r\n# What model to download.\r\nMODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'\r\nMODEL_FILE = MODEL_NAME + '.tar.gz'\r\nDOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\r\n\r\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\r\n# PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\r\nPATH_TO_CKPT = 'screwdriver_graph/frozen_inference_graph.pb'\r\n\r\n\r\n# List of the strings that is used to add correct label for each box.\r\n# PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\r\nPATH_TO_LABELS = os.path.join('data', 'screwdriver_label.pbtxt')\r\n\r\nNUM_CLASSES = 1\r\n# NUM_CLASSES = 90\r\n\r\n\r\n# ## Download Model\r\n\r\n# if not os.path.exists(MODEL_NAME + '/frozen_inference_graph.pb'):\r\nif not os.path.exists('screwdriver_graph/frozen_inference_graph.pb'):\r\n    print('Downloading the model')\r\n    opener = urllib.request.URLopener()\r\n    opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\r\n    tar_file = tarfile.open(MODEL_FILE)\r\n    for file in tar_file.getmembers():\r\n        file_name = os.path.basename(file.name)\r\n        if 'frozen_inference_graph.pb' in file_name:\r\n            tar_file.extract(file, os.getcwd())\r\n    print('Download complete')\r\nelse:\r\n    print('Model already exists')\r\n\r\n# ## Load a (frozen) Tensorflow model into memory.\r\n\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\n\r\n# ---- Load SSD Model as graph_ssd ----\r\nwith tf.Graph().as_default() as graph_ssd:\r\n    graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(SSD_MODEL_FILE, 'rb') as fp:\r\n        graph_def.ParseFromString(fp.read())\r\n    tf.import_graph_def(graph_def)\r\n\r\nssd_sess = tf.Session(graph=graph_ssd)\r\n\r\nwith tf.Graph().as_default() as graph_custom:\r\n    graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(CUSTOM_MODEL_FILE, 'rb') as fp:\r\n        graph_def.ParseFromString(fp.read())\r\n    tf.import_graph_def(graph_def)\r\n\r\ncustom_sess = tf.Session(graph=graph_custom)\r\n\r\n\r\n\r\n# ## Loading label map\r\n# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine\r\n\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n# intializing the web camera device\r\n\r\n\r\ncap = cv2.VideoCapture(1)\r\n\r\n\r\n# Running the tensorflow session\r\n# with detection_graph.as_default():\r\n#     with tf.Session(graph=detection_graph) as sess:\r\nret = True\r\nwhile (ret):\r\n    ret, image_np = cap.read()\r\n    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\r\n    image_np_expanded = np.expand_dims(image_np, axis=0)\r\n    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n    # Each box represents a part of the image where a particular object was detected.\r\n    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n    # Each score represent how level of confidence for each of the objects.\r\n    # Score is shown on the result image, together with the class label.\r\n    scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\n    classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n    # Actual detection.\r\n    (boxes, scores, classes, num_detections) = ssd_sess.run(\r\n        [boxes, scores, classes, num_detections],\r\n        feed_dict={image_tensor: image_np_expanded})\r\n\r\n    # ----------\r\n    (predictions) = custom_sess.run([classifier_out], feed_dict={image_tensor_custom: image_np_expanded})\r\n    # ----------\r\n\r\n    # Visualization of the results of a detection.\r\n    vis_util.visualize_boxes_and_labels_on_image_array(\r\n        image_np,\r\n        np.squeeze(boxes),\r\n        np.squeeze(classes).astype(np.int32),\r\n        np.squeeze(scores),\r\n        category_index,\r\n        use_normalized_coordinates=True,\r\n        line_thickness=8)\r\n    # cv2.putText(img,'OpenCV',(10,500), font, 4,(255,255,255),2,2)\r\n    cv2.imshow('Detected Tools', cv2.resize(image_np, (1000, 850)))\r\n    # cv2.imshow('Tools Name',cv2.resize(cv2.putText(img,'OpenCV',(10,500), font, 4,(255,255,255),2,2),(100,150)))\r\n    if cv2.waitKey(25) & 0xFF == ord('q'):\r\n        cv2.destroyAllWindows()\r\n        cap.release()\r\n        break\r\n```", "How can we maintain If it will go to multiple model files because of python code will be increased\r\nIs there any other feasible solution?(for loop)", "@nfiedel, is there a better solution using saved model?", "Possibly, though this will be pretty advanced and likely not work easily out of the box.\r\n\r\nTLDR: [TensorFlow Hub](https://www.tensorflow.org/hub/) enables you to compose Hub Modules into a single model. If you can export/provide both models as Hub Modules, then you can later use the Hub APIs to import them into a single graph/model. You will still need to wire-up the inputs & outputs.", "Hi @nfiedel  Is there any sample code for combining two frozen model in Tensorflow Hub?", "I'm not aware of any sample code for this. Like I mentioned, this is pretty advanced and new area to research / explore.", "@balavenkatesh3322  Did you find any solution for combine two frozen model? if you get any idea please share to me as well.", "@jinay1991  I checked your code it's showing error for me. \r\nError is following below:\r\n(boxes, scores, classes, num_detections) = ssd_sess.run([boxes, scores, classes, num_detections],feed_dict={image_tensor: image_np_expanded})\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1078, in _run\r\n    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"image_tensor:0\", shape=(1, ?, ?, 3), dtype=uint8) is not an element of this graph.", "See Prasad's answer at https://stackoverflow.com/questions/50364281/how-to-use-two-models-in-tensorflow-object-detection-api# for the solution for this.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 18079, "title": "Can't get the dim of input when the Dataset is from generator.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nWhen use `from_generator` way to get dataset, it can't get the output shape of it. And then pass the features of it to the layers, the layer can't get the rank of it.\r\n\r\nBut it is wok well use `from_tensor_slices` way to get the dataset.\r\n\r\nI am not sure if it is a bug. The tutorial mainly use `from_tensor_slices` as emamples. But I think the action should be same when use these ways to get the dataset.\r\n\r\n### Source code / logs\r\n\r\n```\r\n    def csv_yield(csv_path):\r\n        with open(csv_path, newline='') as f:\r\n            next(f)  # skip the first line\r\n            reader = csv.reader(f)\r\n            features, labels = [], []\r\n            for line in reader:\r\n                feature, label = [float(i) for i in line[:-1]], int(line[-1])\r\n                yield feature, label\r\n\r\n\r\n    ds_train = tf.data.Dataset.from_generator(lambda: csv_yield(r\"../data/iris/iris_training.csv\"),\r\n                                              output_types=(tf.float32, tf.int32))\r\n    ds_train = ds_train.shuffle(1000).batch(16).repeat(5)\r\n    features, labels = ds_train.make_one_shot_iterator().get_next()\r\n\r\n    net = tf.layers.dense(features, units=10, activation=tf.nn.relu)\r\n```\r\n\r\n`ValueError: Input 0 of layer dense_1 is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.`\r\n\r\nBut is is ok when use `tf.data.Dataset.from_tensor_slices`, The only difference between of them is the way of dataset. They shouldn't have different actions.\r\n\r\n```\r\n    def read_csv(csv_path):\r\n        with open(csv_path, newline='') as f:\r\n            next(f)  # skip the first line\r\n            reader = csv.reader(f)\r\n            features, labels = [], []\r\n            for line in reader:\r\n                feature, label = [float(i) for i in line[:-1]], int(line[-1])\r\n                # yield feature, label\r\n                features.append(feature)\r\n                labels.append(label)\r\n            return features, labels\r\n\r\n\r\n    ds_train = tf.data.Dataset.from_tensor_slices(read_csv(r\"../data/iris/iris_training.csv\"))\r\n    ds_train = ds_train.shuffle(1000).batch(16).repeat(5)\r\n    features, labels = ds_train.make_one_shot_iterator().get_next()\r\n\r\n    net = tf.layers.dense(features, units=10, activation=tf.nn.relu)\r\n```\r\n\r\n", "comments": ["I'm no expert on `tf.data`, but according to the [from_generator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) documentation, you can specify the shapes with the `output_shapes` parameter. The generator is never iterated over until a session is run, so during Graph construction time, TensorFlow has no idea what the shapes will be unless `output_shapes` is specified.\r\n\r\nIn your `from_tensor_slices` example, the shape of the input to `from_tensor_slices`, so TensorFlow is able to infer the shapes of the output shapes.", "Thank you, I am not aware the para `output_shapes`. If so, I think it is better to remove the default value of this para. Actually, it took me a lot of time to find the reason, especially it work well when using `tf.placeholder` in my examples."]}, {"number": 18078, "title": "How to change the session options config while load the model in the Java?", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): Master\r\nPython version: 3.6\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\nExact command to reproduce: Run the snippet below.\r\n\r\nI use the python to train the model and predicate use java.\r\n\r\nSee the example code as below.\r\n`SavedModelBundle saver = SavedModelBundle.load(args[0], \"raintung\");\r\n\t\tSession session = saver.session();`\r\nSession had beed created while SavedModelBundle load the model, I want change some session configuration, ex. intra_op_parallelism_threads_ , use_per_session_threads control the session thread numbers and so on, How to do it?\r\nAnd I still not found the default session config that read which protocol files?\r\n\r\nPlease give me example to change the default `config.proto->message ConfigProto` value, or finger out how to do it. Appreciate your grateful help. \r\n\r\n\r\n", "comments": ["It look like the file \"/tensorflow/blob/master/tensorflow/core/protobuf/config.proto\" will generate the config.pb.cc file and compile it in the so module at last. My question: which file is default Message protocol or default configuration? Or only use default google protocol buffer value?", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Had submit the solution, see <contribute: Add the API to set the session configuration when load the model in the JAVA #18143>"]}, {"number": 18077, "title": "[INTEL MKL] Skip special nodes inserted by TF and MKL", "body": "This patch fixes SingleMachineTest.GraphOptimizations (//tensorflow/core/grappler/clusters:single_machine_test) unit test failure when MKL is enabled.", "comments": ["@benoitsteiner did the response satisfy you?"]}, {"number": 18076, "title": "Update README.md", "body": "Add YouTube channel", "comments": []}, {"number": 18075, "title": "Using shell=True for windows git.", "body": "", "comments": []}, {"number": 18074, "title": "Branch 190873003", "body": "", "comments": []}, {"number": 18073, "title": "Default disable including the coordinator in the TPU job", "body": "For the master branch so it doesn't make it into TF 1.8 without us verifying that it works.", "comments": []}, {"number": 18072, "title": "Default disable including the coordinator in the TPU job", "body": "", "comments": []}, {"number": 18071, "title": "Branch 190869028", "body": "", "comments": []}, {"number": 18070, "title": "Fix git binary not being found on Windows in gen_git_source.py.", "body": "", "comments": ["Apparently if you don't have shell=true in subprocess, Windows won't figure out that \"git\" command should run \"git.cmd\". At least according to a stack overflow I read. The other workaround would just to be to catch Exception in this block instead of CalledProcessError", "Please don't merge this until ive had a chance to look over something. I still don't understand why this issue this exists. Since we have Windows builds that interact with Bazel (and run the git_gen_source script)"]}, {"number": 18069, "title": "Fix math equation format in layers docstring", "body": "This PR is to fix mess-up math equation format in [tf.contrib.layers.legacy_fully_connected](https://www.tensorflow.org/versions/r1.7/api_docs/python/tf/contrib/layers/legacy_fully_connected).\r\n![image](https://user-images.githubusercontent.com/1680977/38064265-b46e23d6-332f-11e8-93db-c6a307ae0bf1.png)\r\n\r\nThis PR is to fix this math equation issue according to the [Math in markdown guideline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md#math-in-markdown). That is, using `\\\\(, \\\\)` to surround match equation instead of `\\\\\\(, \\\\\\)` as did in layers.py file.\r\n![image](https://user-images.githubusercontent.com/1680977/38064456-a35da9ee-3330-11e8-8828-2fd05efdc5a4.png)\r\n", "comments": []}, {"number": 18068, "title": "Branch 190861558", "body": "", "comments": []}, {"number": 18067, "title": "Branch 190858242", "body": "", "comments": []}, {"number": 18066, "title": "Lite label_image with Quantized MobileNet reporting varying, incorrect labels for test image", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.6.0-0-gd2e24b6039 1.6.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: [bazel release 0.8.1-homebrew]\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: See below, running Lite's `label_image` example\r\n\r\nI've set the Android SDK/NDK paths in my WORKSPACE file and I'm cross compiling the Lite `label_image` binary for my armv7a device as described in the README, without errors:\r\n```\r\nbazel build --config android_arm --config monolithic --cxxopt=-std=c++11   //tensorflow/contrib/lite/examples/label_image:label_image\r\n```\r\n\r\nWhen I run the resulting binary on my device with the stock `mobilenet_quant_v1_224.tflite` model and Grace Hopper image, I do not get the expected results, with \"military uniform\" at the top of the list. Instead I get:\r\n```\r\n # ./label_image\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model ./mobilenet_quant_v1_224.tflite\r\nresolved reporter\r\ninvoked \r\naverage time: 2153.24 ms \r\n0.862745: 188 Yorkshire terrier\r\n0.0862745: 194 Australian terrier\r\n0.0313726: 187 Norwich terrier\r\n0.0117647: 202 silky terrier\r\n0.00784314: 152 Chihuahua\r\n```\r\n\r\nI see the `nnapi` error about the lack of the `libneuralnetworks.so`, so is that the culprit here? I had hoped that if the library was missing, it would still run correctly, albeit more slowly, but perhaps that is not the case and the library is 100% required for Lite to work. (If it is truly _required_, it does seem odd/confusing that the error isn't \"more fatal\" instead of continuing to do forward inference and producing garbage output!)\r\n\r\nFurthermore, when I run with more iterations (via the `-c` flag), I continue to get the wrong labels, but a _different_ set of incorrect labels depending on the number of iterations. For example, with two iterations:\r\n```\r\n# ./label_image2 -c 2\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model ./mobilenet_quant_v1_224.tflite\r\nresolved reporter\r\ninvoked \r\naverage time: 2214.83 ms \r\n0.345098: 795 shower curtain\r\n0.109804: 906 window shade\r\n0.0470588: 563 fountain\r\n0.0470588: 534 dishrag\r\n0.0392157: 912 wool\r\n```\r\nAnd with three iterations:\r\n```\r\n# ./label_image2 -c 3\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model ./mobilenet_quant_v1_224.tflite\r\nresolved reporter\r\ninvoked \r\naverage time: 2407.64 ms \r\n0.0705882: 906 window shade\r\n0.0509804: 829 strainer\r\n0.0509804: 734 pole\r\n0.0431373: 795 shower curtain\r\n0.0431373: 620 lampshade\r\n```\r\nAnd so on.\r\n\r\nWhy does the number of iterations performed affect the output? (Perhaps this is just the same issue as above, due to missing nnapi lib?)\r\n", "comments": ["Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Seeing exactly the same phenomenon for the 2nd part of the issue reported. Though I can always get expected result if the interpreter is only invoked once.\r\n\r\nWhen it is invoked for multiple times via the '-c' command line parameter I am no longer getting correct result. Meanwhile different incorrect results are returned based on the number of iterations specified. I had observed the same issue on both linux-x64 and raspberry-pi.\r\n\r\nI made an attempt to enlarge the for-loop to include the step which sets up input tensor with resized image, then the issue is gone. Looks like the input had been altered in some way after a successful invoke call. ", "Thanks for the report. This happens because label_image runs Invoke() without resetting the input tensor, and TF Lite overwrites that memory location during inference (to keep memory usage low).", "@andrehentz  Thanks for explanation for the second part of my issue (wrong answer when running multiple times), but do you also have any thoughts as to why even running it once fails to return the expected label in my case?  (And can you confirm it's _not_ the lack of nnapi?)", "@andrew-anki Ah, that's strange indeed. It is certainly not the lack of nnapi (BTW, that 'error' message has been removed in recent builds). Could you try on desktop to see if this could be device specific?", "@andrehentz  I built for my Mac desktop and it worked no problem. So I started from scratch and rebuilt for Android again, re-deployed, and it worked! (?!?) No idea how I got into whatever state I was in, but it seems to be working now. Sorry for the false alarm, and thanks again for the explanation about running multiple times. I'll close this now and reopen if I can ever find a repro."]}]