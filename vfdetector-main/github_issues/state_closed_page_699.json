[{"number": 32617, "title": "[TF.1.14] Cannot load weights under bfloat16 using tf.keras", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):-\r\n- CUDA/cuDNN version:-\r\n- GPU model and memory: Using TPU\r\n\r\nI ve trained a custom CNN with TF 1.14 using keras, and trained under TPU strategy scope and by settting:  \r\n```\r\nK.set_floatx('float16')\r\nkeras.backend.set_epsilon(1e-4)\r\n```\r\nFor traininng with fp16. The training procedure was allright, since I would like to stop and load the trained weights. I compile the model under tpu_strategy.scope, and when trying to load weights using model.load_weigths, reaises the following error \r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0918 11:36:24.372959 139742800717568 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0918 11:37:32.353564 139742800717568 deprecation_wrapper.py:119] From 6dense_inv_frp16.py:326: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\r\n\r\nW0918 11:37:32.370291 139742800717568 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nTraceback (most recent call last):\r\n  File \"6dense_inv_frp16.py\", line 364, in <module>\r\n    model.load_weights(filepath)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 162, in load_weights\r\n    return super(Model, self).load_weights(filepath, by_name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\", line 1424, in load_weights\r\n    saving.load_weights_from_hdf5_group(f, self.layers)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 759, in load_weights_from_hdf5_group\r\n    K.batch_set_value(weight_value_tuples)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 3058, in batch_set_value\r\n    value = np.asarray(value, dtype=dtype(x))\r\n  File \"/usr/local/lib/python3.5/dist-packages/numpy/core/_asarray.py\", line 85, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nValueError: setting an array element with a sequence.\r\n```\r\n", "comments": ["@nsantavas ,\r\nThank you for reporting the issue, can you please provide a simple and standalone code to reproduce the issue? meanwhile you can also refer these links [1](https://stackoverflow.com/questions/56116628/how-to-use-tf-keras-with-bfloat16) and [2](https://stackoverflow.com/questions/4674473/valueerror-setting-an-array-element-with-a-sequence) let us know if it can help resolving your issue.Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32616, "title": "Dear team how could I use open images dataset v4 with tensorflow to detect a objects", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@DeveloperRachit You can follow this [tutorial ](https://blog.algorithmia.com/deep-dive-into-object-detection-with-open-images-using-tensorflow) which gives you an idea of how to perform Object Detection with Open Images, using Tensorflow. Also I am closing this issue as this is not a related place to ask this question.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32616\">No</a>\n"]}, {"number": 32615, "title": "Fix for tensorflow/tensorflow:devel", "body": "Workaround for the reported build issue https://github.com/tensorflow/tensorflow/issues/32152\r\n\r\nWhen this file /etc/passwd is writable inside of the docker image, then before we run any builds inside of the container we can \r\n\r\n- add passwd file entry for $(id -u)\r\n\r\n- add passwd file entry for $(id -g)\r\n\r\nAs reported, this is an issue only for Python 2.7+. \r\n\r\n", "comments": ["Thanks for the PR. Can you follow the instructions [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles) to update the partial files?", "@wwwind Could you please check reviewer comments and keep us posted. Thanks!", "@angerson moved. please take a look. thanks", "@wwwind Thanks, but there's still a little left to do. Can you take another look at the [instructions in the contributing section here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles#contributing) and run the file generator?", "@wwwind Could you please check reviewer comments and keep us posted. Thanks!", "@gbaned updated"]}, {"number": 32614, "title": "[TF2.0] can't restore path with brackets", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX\r\n- TensorFlow installed from (source or binary): `pip install tensorflow==2.0.0-rc1`\r\n- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: `None`\r\n- GPU model and memory: `None`\r\n\r\n**Describe the current behavior**\r\nI can't restore a model stored ina directory containing a bracket\r\n\r\n**Describe the expected behavior**\r\nI can restore a model stored ina directory containing a bracket\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os\r\nimport shutil\r\nimport tensorflow as tf\r\n\r\ncurrent_dir = os.path.realpath(os.path.dirname(__file__))\r\nsave_dir = os.path.join(current_dir, 'testhop')\r\nsave_dir_wbracket = os.path.join(current_dir, 'test[hop')\r\n\r\n\r\nif os.path.isdir(save_dir):\r\n    shutil.rmtree(save_dir)\r\nos.mkdir(save_dir)\r\nassert os.path.isdir(save_dir)\r\n\r\nif os.path.isdir(save_dir_wbracket):\r\n    shutil.rmtree(save_dir_wbracket)\r\nos.mkdir(save_dir_wbracket)\r\nassert os.path.isdir(save_dir_wbracket)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Dense(\r\n        16, batch_input_shape=[256, 1], activation='relu'),\r\n    tf.keras.layers.Dense(16, activation='tanh'),\r\n    tf.keras.layers.Dense(1),\r\n])\r\n\r\nlr = tf.Variable(1.)\r\nreg_param = tf.Variable(1.)\r\noptim = tf.keras.optimizers.SGD(lr)\r\n\r\nckpt = tf.train.Checkpoint(\r\n    model=model,\r\n    lr=lr,\r\n    reg_param=reg_param,\r\n)\r\n\r\nmanager = tf.train.CheckpointManager(ckpt, save_dir, max_to_keep=1)\r\nmanager_wbracket = tf.train.CheckpointManager(ckpt, save_dir_wbracket, max_to_keep=1)\r\n\r\nmanager.save()\r\nassert os.path.isfile(os.path.join(save_dir, 'checkpoint'))\r\nmanager_wbracket.save()\r\nassert os.path.isfile(os.path.join(save_dir_wbracket, 'checkpoint'))\r\n\r\n# Works\r\nprint(manager.latest_checkpoint)\r\nckpt.restore(manager.latest_checkpoint)\r\n\r\n# Does not work\r\nprint(manager_wbracket.latest_checkpoint)\r\nckpt.restore(manager_wbracket.latest_checkpoint)\r\n```\r\n\r\n**Other info / logs**\r\nNote: Saving is working\r\n\r\nOutputs:\r\n```\r\n/Users/morgangiraud/Sites/git/ray/python/ray/tune/examples/pbt/PBTTrain/testhop/ckpt-1\r\n/Users/morgangiraud/Sites/git/ray/python/ray/tune/examples/pbt/PBTTrain/test[hop/ckpt-2\r\nTraceback (most recent call last):\r\n  File \"debug.py\", line 51, in <module>\r\n    ckpt.restore(manager_wbracket.latest_checkpoint)\r\n  File \"/Users/morgangiraud/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1971, in restore\r\n    status = self._saver.restore(save_path=save_path)\r\n  File \"/Users/morgangiraud/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1228, in restore\r\n    reader = pywrap_tensorflow.NewCheckpointReader(save_path)\r\n  File \"/Users/morgangiraud/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 873, in NewCheckpointReader\r\n    return CheckpointReader(compat.as_bytes(filepattern))\r\n  File \"/Users/morgangiraud/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 885, in __init__\r\n    this = _pywrap_tensorflow_internal.new_CheckpointReader(filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files\r\n```", "comments": ["@morgangiraud Tried to reproduce your issue but was not able to reproduce it. Please find my github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/ee11f049cc9ecf3855959d0d9df5f970/untitled146.ipynb).", "First time that I have different behaviours on my mac and in collab but I'm even more puzzled now.\r\nI've tested again just in case but it keeps failing on my mac book.\r\n\r\nIt also works with `test]hop` and  `test{hop`. \r\n\r\nIs there anyone who could try to run the script on a macbook to see if it is reproducible?\r\n", "@morgangiraud You can take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/6082) and it should help you resolve your problem.", "Thanks, so it looks, like for some reason, I should just avoid the \"[\" character in my model's paths.\r\n\r\n@ppwwyyxx, why you had the insight of removing the \"[\" character?", "@morgangiraud Glad that it worked. Can i close this issue?", "Well, it depends on your policy I guess. \r\nThe problem is not only mine (based on the issue you linked) and is not solved (since I can reproduce it) but it is not happening on all platform so I don't know if this is the kind of bug the TF team is going after.", "@morgangiraud Will look into it in future. Closing this issue as of now as it has been resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32614\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32614\">No</a>\n"]}, {"number": 32613, "title": "Removed unnecessary cmsis includes preventing compilation from cloned\u2026", "body": "\u2026 repo", "comments": []}, {"number": 32612, "title": "Keras can not load custom Loss functions.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: Cuda 10.0\r\n- GPU model and memory: RTX 2080 Titan\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIf overloading the loss function in `tf.keras.losses.Loss` the created model can be compiled, trained and saved but not loaded.\r\n**Describe the expected behavior**\r\nCustom loss function should be loadable.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\nclass MyCustomLoss(tf.keras.losses.Loss):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def call(self, y_true, y_pred):\r\n        return 1.\r\n\r\n\r\n\r\na = tf.keras.layers.Input(shape=(32,))\r\nb = tf.keras.layers.Dense(32)(a)\r\nmodel = tf.keras.models.Model(inputs=a, outputs=b)\r\n\r\nmodel.compile('sgd', MyCustomLoss())\r\nmodel.save('/tmp/model.h5')\r\nmodel_new = tf.keras.models.load_model('/tmp/model.h5', custom_objects={'MyCustomLoss': MyCustomLoss}))\r\n```\r\nOutput:  \r\n```\r\n2.0.0-beta1\r\n2019-09-18 10:10:54.994663: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-09-18 10:10:55.075823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:65:00.0\r\n2019-09-18 10:10:55.077214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65\r\npciBusID: 0000:b3:00.0\r\n2019-09-18 10:10:55.077363: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-09-18 10:10:55.078368: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-09-18 10:10:55.079236: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-09-18 10:10:55.079453: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-09-18 10:10:55.080500: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-09-18 10:10:55.081301: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-09-18 10:10:55.083761: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-09-18 10:10:55.089032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1\r\n2019-09-18 10:10:55.089311: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-09-18 10:10:55.420058: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4d766e0 executing computations on platform CUDA. Devices:\r\n2019-09-18 10:10:55.420103: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-09-18 10:10:55.420116: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-09-18 10:10:55.442162: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3300000000 Hz\r\n2019-09-18 10:10:55.444231: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ef370 executing computations on platform Host. Devices:\r\n2019-09-18 10:10:55.444265: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-09-18 10:10:55.445930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:65:00.0\r\n2019-09-18 10:10:55.447265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65\r\npciBusID: 0000:b3:00.0\r\n2019-09-18 10:10:55.447316: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-09-18 10:10:55.447336: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-09-18 10:10:55.447352: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-09-18 10:10:55.447369: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-09-18 10:10:55.447385: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-09-18 10:10:55.447402: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-09-18 10:10:55.447419: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-09-18 10:10:55.452337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1\r\n2019-09-18 10:10:55.452385: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-09-18 10:10:55.455253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-18 10:10:55.455270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 \r\n2019-09-18 10:10:55.455279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N N \r\n2019-09-18 10:10:55.455286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N N \r\n2019-09-18 10:10:55.458601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9913 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5)\r\n2019-09-18 10:10:55.460320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10311 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:b3:00.0, compute capability: 7.5)\r\nTraceback (most recent call last):\r\n  File \"/home/ragnar/projects/mbda/mbda_infrared/sandbox.py\", line 19, in <module>\r\n    model_new = tf.keras.models.load_model('/tmp/model.h5')\r\n  File \"/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 137, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 178, in load_model_from_hdf5\r\n    training_config, custom_objects))\r\n  File \"/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py\", line 222, in compile_args_from_training_config\r\n    loss_config = losses.get(loss_config)\r\n  File \"/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py\", line 1124, in get\r\n    return deserialize(identifier)\r\n  File \"/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py\", line 1113, in deserialize\r\n    printable_module_name='loss function')\r\n  File \"/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 181, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 166, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown loss function: MyCustomLoss\r\n\r\nProcess finished with exit code 1\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 2.0 beta1,2.0.0-rc0, 2.0.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/cae08a626511eaab7fa6b4eda8f80405/untitled199.ipynb).Thanks!", "I also tested 2.0.0-rc1 and the issue persists. \r\n@ravikyram If there is a label for 2.0.0-rc1, can we also add it?", "The same problem also exists for classes implementing `tf.keras.metrics.Metric`.", "@somedadaism Can you save the model before compiling with `MyCustomLoss` function. Then load the saved model and compile with `MyCustomLoss` function as shown below. Thanks!\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\nclass MyCustomLoss(tf.keras.losses.Loss):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def call(self, y_true, y_pred):\r\n        return 1.\r\n\r\n\r\n\r\na = tf.keras.layers.Input(shape=(32,))\r\nb = tf.keras.layers.Dense(32)(a)\r\nmodel = tf.keras.models.Model(inputs=a, outputs=b)\r\nmodel.save('./model.h5') # save first and then compile\r\nmodel.compile('sgd', MyCustomLoss())\r\n\r\n# load the model\r\nmodel_new = tf.keras.models.load_model('./model.h5')\r\nmodel_new.compile('sgd', MyCustomLoss())\r\n```\r\n\r\n", "This does only work for this simple toy example. But if I want to fit the model and then save and the reload it will not work. I know how to workaround this thing myself but the intended way is not working for me. The same problem also exists for classes implementing `tf.keras.metrics.Metric.`\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\nclass MyCustomLoss(tf.keras.losses.Loss):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def call(self, y_true, y_pred):\r\n        return 1.\r\n\r\n\r\n\r\na = tf.keras.layers.Input(shape=(32,))\r\nb = tf.keras.layers.Dense(32)(a)\r\nmodel = tf.keras.models.Model(inputs=a, outputs=b)\r\nmodel.compile('sgd', MyCustomLoss()) # first compile, then fit, then save is the normal order.\r\nmodel.fit(some_data, epochs=1) \r\nmodel.save('./model.h5') # I can not save any earlier!!!\r\n# load the model\r\nmodel_new = tf.keras.models.load_model('./model.h5')\r\nmodel_new.compile('sgd', MyCustomLoss())\r\n```", "@somedadaism Custom functions are not compatible. \r\n\r\n`model.save` before `model.compile` works for small toy example and big model also.\r\n\r\nWhen you load the saved model, load the model using `tf.keras.models.load_model` and then `compile`.\r\n\r\nPlease try this even for the above example or any other big model and let us know if you have any issue there.\r\n\r\nIf you don't have any custom function, then follow `define model`, then `compile model`, then `save the model`, and then `reload the model`. Thanks!\r\n", "By simple toy example I meant my code was so minimal that your workaround would work for it but not for the standard usecase. It is not related to the model size. The difference is that usually you also `fit` the model. I had updated the code in my last post to show how `save` before `compile` usually does not make sense. Can you have a second look at my last code snippet?\r\nI think Custom Loss Functions and Metrics should be supported. Can you fix this so keras works as expected?", "@somedadaism I haven't tried `customloss` as `class object` but I tried it as `function`  like\r\n\r\n```\r\n# Custom Loss1 (for example) \r\n@tf.function() \r\ndef customLoss1(yTrue,yPred):\r\n  return tf.reduce_mean(yTrue-yPred) \r\n```\r\n\r\nIn this way you can save the model as usual (create model, compile, fit, save) and load the model using `load_model` with `custom_objects` like\r\n\r\n`new_model=tf.keras.models.load_model(\"./model.h5\",custom_objects={'customLoss1':customLoss1,'customLoss2':customLoss2})`\r\n\r\nPlease check entire gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/04028d08660378ee7c2cef2b9c3bb2e9/custom_metric.ipynb). Thanks!", "Hello, thank you for your effort. I know also how to workaround for the loss function. My favorite workaround for now is to use `load_model(path, compile=False)`. If it was only the loss functions this whole issue could be solved by just using normal functions as you propose. The fundamental problem is that the subclasses of `tf.keras.metrics.Metric` does not work either for presumably the same reason. Subclasses of this class can not be replaced by simple functions because metrics can be stateful. If I implement my own stateful metrics in the style of, for an example `tf.keras.metrics.mIoU` there is no alternative to this but if I save the model I can not load it anymore (or if I use my workaround I loose the optimizer state.) For this reason I think that a fix is necessary for the issue although there are workarounds that work in many cases.", "@k-w-w Can you give me a rough estimation for the timeframe to expect for the fix of the issue? (codebase I will deliver to a client depends on it.)", "@k-w-w @jvishnuvardhan Are there any updates on the issue?\r\n", "@k-w-w The solution proposed by @ thierryherrmann  [here](https://github.com/tensorflow/tensorflow/pull/33229) is good, but only if the inputs to the loss object are not tensors. Otherwise, a serialization exception is generated.\r\nWe could try to convert the tensor into a numpy array inside `get_config()`, as it is done by the `LossWrapperFunction` class. Unfortunately, that does not work either. It seems Keras is not well integrated with eager execution. (Side note: having access to the `LossWrapperFunction` class would lead to much cleaner solutions).\r\nI have created a gist [here](https://colab.research.google.com/gist/humcasma/d7d00d6d31c4cca623c1140f050e8f94/problem_loading_custom_loss.ipynb) with different attempts to solve the problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32612\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32612\">No</a>\n"]}, {"number": 32611, "title": "How to set the four parameters in toco", "body": "Linux Ubuntu 18.04:\r\n- TensorFlow installed from `pip install tensorflow-gpu`:\r\n- TensorFlow version 1.12.0:\r\n\r\nIn ckpt to pb I have used the \r\n`tf.contrib.quantize.create_eval_graph()`\r\nSo I use the type QUANTIZED_UINT8 in toco as \r\n`toco --graph_def_file=./my_freeze_graph_stft.pb --output_file=my_stft.tflite --output_format=TFLITE --input_shape=1024 --input_array=x_mixed --output_array=y_out2 --inference_type=QUANTIZED_UINT8 --inference_input_type=QUANTIZED_UINT8`\r\n\r\nbut I find the error\r\n`    input_array.mean_value, input_array.std_value = quantized_input_stats[idx]\r\nTypeError: 'NoneType' object is not subscriptable`\r\nSO I must set the four parameters\r\n`--std_dev_values`\r\n`--mean_values`\r\n`--default_ranges_min`\r\n`--default_ranges_max`\r\n\r\nSO how to set them ?any idea or advice/suggestuion ?\r\n\r\nthx\r\n\r\n", "comments": ["another problem\r\n\r\nif I use\r\n`--std_dev_values=1 --mean_values=0`\r\nthen comes \r\n`b'2019-09-18 18:06:25.827623: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\\n2019-09-18 18:06:25.827705: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: rnn/TensorArray\\n2019-09-18 18:06:25.827723: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\\n2019-09-18 18:06:25.827734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: rnn/TensorArray_1\\n2019-09-18 18:06:25.827766: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayScatterV3\\n2019-09-18 18:06:25.827796: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\\n2019-09-18 18:06:25.827815: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827846: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827867: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827910: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827923: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LoopCond\\n2019-09-18 18:06:25.827933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: rnn/while/LoopCond\\n2019-09-18 18:06:25.827965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827976: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\\n2019-09-18 18:06:25.827985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.827996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayReadV3\\n2019-09-18 18:06:25.828442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.828462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.828488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.828523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.828989: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.829024: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.829051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.829066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.829437: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.829475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.829516: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.829531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.829555: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\\n2019-09-18 18:06:25.829566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\\n2019-09-18 18:06:25.829576: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayWriteV3\\n2019-09-18 18:06:25.829590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: rnn/while/TensorArrayWrite/TensorArrayWriteV3\\n2019-09-18 18:06:25.829611: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit\\n2019-09-18 18:06:25.829630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArraySizeV3\\n2019-09-18 18:06:25.829647: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: rnn/TensorArrayStack/TensorArraySizeV3\\n2019-09-18 18:06:25.829667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayGatherV3\\n2019-09-18 18:06:25.830001: F tensorflow/contrib/lite/toco/tooling_util.cc:1382] Check failed: batch == 1 (2 vs. 1)\\nAborted\\n'`\r\n\r\n\r\nany one has this problem ?\r\n", "We're in the process of deprecating the old quantization rewriter. Our new keras based quantization-aware training API is on the way and welcome to try it out when it's released!\r\n\r\nMeanwhile, you can look into post-training integer quantization to quantize your model:\r\nhttps://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba\r\n\r\nFeel free to open separate issues when you run into problem with that. Thanks!"]}, {"number": 32610, "title": "fix typo", "body": "a typo", "comments": []}, {"number": 32609, "title": "Support for TensorForest", "body": "**System information**\r\n- OS Platform and Distribution Linux Ubuntu 18.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version 1.14\r\n\r\nText from TFLite converter:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, DIV, PACK, SUM. Here is a list of operators for which you will need custom implementations: DecisionTreeResourceHandleOp, TreePredictionsV4.\r\n\r\n", "comments": ["Can you please elaborate about the issue & the context.Will it be possible to provide related code.Thanks!", "I have build a tensor_forest model in tensorflow and have saved it as \"saved_model.ckpt\". I want to use this model in Android, for which below steps are followed:\r\n\r\n1. The freeze graph is created using:\r\nfrom tensorflow.python.tools import freeze_graph\r\ngraph = tf.get_default_graph()\r\ninput_graph_def = graph.as_graph_def()\r\n\r\nsaver.restore(sess, \"./saved_model.ckpt\")\r\nnode_names = [node.name for node in input_graph_def.node]\r\n\r\n2.  Frozen model .pb file is created using below code:\r\n\r\noutput_node_names = \"The node names\"\r\noutput_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, # The session\r\n            input_graph_def, # input_graph_def is useful for retrieving the nodes \r\n            output_node_names.split(\",\")  \r\noutput_graph=\"./saved_Frozen_Model.pb\"\r\nwith tf.gfile.GFile(output_graph, \"wb\") as f:\r\n    f.write(output_graph_def.SerializeToString())\r\nsess.close()\r\n\r\n3. The forzen model is converted into TFLite model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\".\")\r\nconverter_new.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n\r\nWhile doing this conversion, I get the error messages\r\nConverting unsupported operation: DecisionTreeResourceHandleOp\r\nUnable to determine output type for op: DecisionTreeResourceHandleOp\r\n\r\nAnd towards the end it asks to log an error in github and paste the below message:\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, DIV, PACK, SUM. Here is a list of operators for which you will need custom implementations: DecisionTreeResourceHandleOp, TreePredictionsV4.\r\n\r\n", "Request a reply on this. Will tensor-forest be supported in tensor-flow lite?", "Hi gd12345,\r\n\r\nI checked that the missing ops 'TreePredictionsV4' is under tensorflow/contrib directory, unfortunately this op can't be supported by TF Lite right now. Could you manage to find a replacement for the op outside of contrib/? (since contrib is going to be deprecated).", "How to manage it outside contrib?\n\nWhat is the alternative of tensor forest in TFLite?\n\nOn Fri 27 Sep, 2019, 22:39 Haoliang Zhang, <notifications@github.com> wrote:\n\n> Hi gd12345,\n>\n> I checked that the missing ops 'TreePredictionsV4' is under\n> tensorflow/contrib directory, unfortunately this op can't be supported by\n> TF Lite right now. Could you manage to find a replacement for the op\n> outside of contrib/? (since contrib is going to be deprecated).\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32609?email_source=notifications&email_token=AE2NJR7ZZBAUNUSGLVBYASLQLY46LA5CNFSM4IX2WGCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7ZQN5Q#issuecomment-536020726>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE2NJR62ZCDWVYKSARD5TCTQLY46LANCNFSM4IX2WGCA>\n> .\n>\n", "Sorry. I was meaning if you could find an API for TensorForest outside of tf.contrib, since that's being deprecated. I think tensor_forest is still under migration (which will eventually be moved to core):\r\nhttps://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n\r\nHowever, before that happens, I think there is no easy way to support TensorForest ops in TF Lite. Sorry about that.", "@gd12345  \r\nCan you please update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi Haoza111,\r\n\r\nAre there any near future plans to update tensorforest in the latest tensorflow versions? If so, we would like to et an update on it, as and when it happens.", "@jdduke ,do you know if there are timelines to support tensorforest?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Thanks for the reply.\n\nWe will try with tensorflow 2.0 and get back to you on this.\n\nRegards,\nGauri\n\nOn Mon, 1 Feb 2021, 19:40 Alfred Sorten Wolf, <notifications@github.com>\nwrote:\n\n> Hi There,\n>\n> We are checking to see if you still need help on this, as you are using an\n> older version of tensorflow which is officially considered end of life . We\n> recommend that you upgrade to the latest 2.x version and let us know if the\n> issue still persists in newer versions. Please open a new issue for any\n> help you need against 2.x, and we will get you the right help.\n>\n> This issue will be closed automatically 7 days from now. If you still need\n> help with this issue, please provide us with more information.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32609#issuecomment-770882962>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AE2NJR2YYT7QDW7K46BDX3LS42YVPANCNFSM4IX2WGCA>\n> .\n>\n", "Hi there,\r\n\r\nAlso wondering whether `tensor_forest` will be moved to `core` any time soon? It seems like this was the [original intent](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md)? Perhaps @jdduke would know?\r\n\r\nIt would be great to have this functionality back :)\r\n\r\nThanks\r\nAndrew", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32609\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32609\">No</a>\n"]}, {"number": 32608, "title": "Failed to convert an RNN built with tf.keras by TFLiteConverter", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): tensorflow-gpu 2.0.0rc1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: RESHAPE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\nCode which would reproduce the error log:\r\n```\r\nfrom tensorflow.keras import layers, models\r\nimport tensorflow as tf\r\n\r\nipt = layers.Input((10, 5))\r\nx = layers.SimpleRNN(1, return_sequences=True)(ipt)\r\nmodel = models.Model(ipt, x)\r\n\r\ncvt = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = cvt.convert()\r\n```\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi,\r\n\r\nWe are working on a new converter that can convert keras lstm/rnn into tflite. Please stay tuned!", "@bbb00437 Can you check the with the new converter with an experimental flag `cvt.experimental_new_converter = True`. I ran your code with the experimental flag and works without any issue. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/82af131c5bb275fbe683458510ca833a/untitled635.ipynb).\r\n\r\nI am closing the issue as it was resolved with the new converter. Please feel free to reopen the issue if it persists again. thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32608\">No</a>\n"]}, {"number": 32607, "title": "\"node optimization/gradients has inputs from different frames\" on GPU, but works fine on CPU", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n- **I'm writing a customer LSTMCell, I referred the source code in tensorflow/python/ops/rnn_cell_impl.py. I only add one line of operations on top of the source code.**:\r\n- **Linux Ubuntu**: 16.04\r\n- **TensorFlow version**: 1.13.1\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: v10.1.168\r\n- **GPU**: Tesla v100\r\n\r\n\r\n### Describe the problem\r\nI'm writing a customer LSTMCell, I followed the source code in tensorflow/python/ops/rnn_cell_impl.py, just add one more line \"d = tf.multiply(f, (tf.ones_like(f)-f))\". Everything works fine on CPU, but raise \"node optimization/gradients has inputs from different frames\" error while running on GPU. I didn't use any operation that's invalid on CUDA, only regular tensor operations like multiplication.\r\n\r\n### Source code / logs\r\n\"defined customer LSTM Cell\":\r\n\r\n\r\n    from __future__ import absolute_import\r\n    from __future__ import division\r\n    from __future__ import print_function\r\n\r\n\r\n    from tensorflow.python.framework import constant_op\r\n    from tensorflow.python.framework import dtypes\r\n\r\n    from tensorflow.python.layers import base as base_layer\r\n    from tensorflow.python.ops import array_ops\r\n\r\n    from tensorflow.python.ops import init_ops\r\n    from tensorflow.python.ops import math_ops\r\n    from tensorflow.python.ops import nn_ops\r\n    from tensorflow.python.platform import tf_logging as logging\r\n    from tensorflow.contrib.rnn import BasicLSTMCell as BasicLSTMCell\r\n    from tensorflow.contrib.rnn import LSTMStateTuple\r\n    import tensorflow as tf\r\n\r\n\r\n    _BIAS_VARIABLE_NAME = \"bias\"\r\n    _WEIGHTS_VARIABLE_NAME = \"kernel\"\r\n\r\n\r\n    class ProxLSTMCell(BasicLSTMCell):\r\n\r\n      def __init__(self, num_units, forget_bias=1.0,\r\n                   state_is_tuple=True, activation=None, reuse=None, name='basic_lstm_cell', lamb \r\n          = 1.0, delta = 1.0):\r\n          super(ProxLSTMCell, self).__init__(num_units = num_units, reuse=reuse, name=name)\r\n          if not state_is_tuple:\r\n          logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\r\n                       \"deprecated.  Use state_is_tuple=True.\", self)\r\n\r\n         # Inputs must be 2-dimensional.\r\n         self.input_spec = base_layer.InputSpec(ndim=2)\r\n\r\n         self._num_units = num_units\r\n         self._forget_bias = forget_bias\r\n         self._state_is_tuple = state_is_tuple\r\n         self.lamb = lamb\r\n         self.delta = delta\r\n\r\n     @property\r\n     def state_size(self):\r\n       return (LSTMStateTuple(self._num_units, self._num_units)\r\n              if self._state_is_tuple else 2 * self._num_units)\r\n\r\n      @property\r\n      def output_size(self):\r\n        return self._num_units\r\n\r\n      def build(self, inputs_shape):\r\n        if inputs_shape[1].value is None:\r\n         raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\r\n                       % inputs_shape)\r\n\r\n         input_depth = inputs_shape[1].value\r\n         self.input_size = input_depth\r\n         h_depth = self._num_units\r\n         self._kernel = self.add_variable(\r\n         _WEIGHTS_VARIABLE_NAME,\r\n         shape=[input_depth + h_depth, 4 * self._num_units])\r\n         self._bias = self.add_variable(\r\n         _BIAS_VARIABLE_NAME,\r\n         shape=[4 * self._num_units],\r\n         initializer=init_ops.zeros_initializer(dtype=self.dtype))\r\n\r\n         self.built = True\r\n\r\n      def call(self, inputs, state):\r\n                 \"\"\"Long short-term memory cell (LSTM).\r\n    sigmoid = math_ops.sigmoid\r\n    one = constant_op.constant(1, dtype=dtypes.int32)\r\n    # Parameters of gates are concatenated into one multiply for efficiency.\r\n    if self._state_is_tuple:\r\n      c, h = state\r\n    else:\r\n      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one)\r\n\r\n    gate_inputs = math_ops.matmul(\r\n        array_ops.concat([inputs, h], 1), self._kernel)\r\n    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)\r\n\r\n    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\r\n    i, j, f, o = array_ops.split(\r\n        value=gate_inputs, num_or_size_splits=4, axis=one)\r\n\r\n    forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)\r\n    # Note that using `add` and `multiply` instead of `+` and `*` gives a\r\n    # performance improvement. So using those at the cost of readability.\r\n    add = math_ops.add\r\n    multiply = math_ops.multiply\r\n    new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),\r\n                multiply(sigmoid(i), self._activation(j)))\r\n    new_h = multiply(self._activation(new_c), sigmoid(o))\r\n    d = multiply(f, (tf.ones_like(f)-f))\r\n\r\n    new_c = new_c + d\r\n    if self._state_is_tuple:\r\n      new_state = LSTMStateTuple(new_c, new_h)\r\n    else:\r\n      new_state = array_ops.concat([new_c, new_h], 1)\r\n    return new_h, new_state\r\n\r\n''Calling defined LSTM Cell'':\r\n```\r\ndef LSTM_graph():\r\n\r\n........\r\n\r\nprox_cell = ProxLSTM.ProxLSTMCell(\r\n        num_units=self.cell_size,\r\n        forget_bias=0.0,\r\n        reuse=tf.get_variable_scope().reuse,\r\n        lamb=FLAGS.lamb,\r\n        delta=FLAGS.delta)\r\ncell = tf.contrib.rnn.MultiRNNCell([\r\n              prox_cell\r\n              for _ in xrange(self.num_layers)\r\n          ])\r\n\r\nlstm_out, next_state = tf.nn.dynamic_rnn(\r\n          cell, x, initial_state=initial_state, sequence_length=seq_length)\r\n\r\nreturn lstm_out, next_state\r\n\r\n\r\n\r\n\"Error message\":\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: node optimization/gradients/LSTM/rnn/while/rnn/multi_\r\nrnn_cell/cell_0/basic_lstm_cell/MatMul_grad/MatMul (defined at /workspace/adversarial_text/layers.py:327)  has inpu\r\nts from different frames. The input node optimization/gradients/LSTM/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm\r\n_cell/split_grad/concat (defined at /workspace/adversarial_text/layers.py:327)  is in frame ''. The input node opti\r\nmization/gradients/LSTM/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul_grad/MatMul/Enter (defined at /w\r\norkspace/adversarial_text/layers.py:327)  is in frame 'optimization/gradients/LSTM/rnn/while/while_context'.\r\n```", "comments": ["@myy920213 ,\r\nLooks like the code provided is incomplete,can you please share a simple standalone code to reproduce the issue reported here ? Thanks!", "Sorry for the late response. I write up a simple test case to reproduce this error. I uploaded the python file as zip, run simple_test.py will reproduce the error. newLSTM class defines the customer LSTM layer, mainly copied from tensorflow/python/ops/rnn_cell_impl.py, only ''d = multiply(f, (tf.ones_like(f)-f))'' is newly added.\r\nApparently, change the gradient aggregation_method to tf.AggregationMethod.EXPERIMENTAL_TREE or tf.AggregationMethod.ADD_N  will fix this error but will take too much memory space. I'm just very confusing why this error occurs by just adding one line of code and I didn't use any operations other than multiply, +, - in this line.\r\nThanks!\r\n\r\n[simple_test.py.zip](https://github.com/tensorflow/tensorflow/files/3672233/simple_test.py.zip)\r\n\r\n\r\n\r\n\r\n> @myy920213 ,\r\n> Looks like the code provided is incomplete,can you please share a simple standalone code to reproduce the issue reported here ? Thanks!\r\n\r\n", "@myy920213 ,\r\nWhen tried executing the given code i got the as present in the [gist](https://colab.sandbox.google.com/gist/oanush/5a89ff690cd4c5e9ad5f75d4e53f6b61/32607.ipynb) of colab.Thanks!", "> @myy920213 ,\r\n> When tried executing the given code i got the as present in the [gist](https://colab.sandbox.google.com/gist/oanush/5a89ff690cd4c5e9ad5f75d4e53f6b61/32607.ipynb) of colab.Thanks!\r\n\r\nThanks for your response.\r\nThis code used to work on my own laptop, but will show the same error when running on Google cloud or AWS GPU instances. So I guessed the problem was running with GPU. \r\nAfter running the colab notebook you provided, I realized this problem may not be running with GPU, as I created a new colab notebook and run this code without using GPU, this error occur again. Here is the colab page: (https://colab.research.google.com/drive/1qWh8F2iFfM64seNESDUrnIuofWrev1Kv#scrollTo=zuetCbHVoKZA)", "> > @myy920213 ,\r\n> > When tried executing the given code i got the as present in the [gist](https://colab.sandbox.google.com/gist/oanush/5a89ff690cd4c5e9ad5f75d4e53f6b61/32607.ipynb) of colab.Thanks!\r\n> \r\n> Thanks for your response.\r\n> This code used to work on my own laptop, but will show the same error when running on Google cloud or AWS GPU instances. So I guessed the problem was running with GPU.\r\n> After running the colab notebook you provided, I realized this problem may not be running with GPU, as I created a new colab notebook and run this code without using GPU, this error occur again. Here is the colab page: (https://colab.research.google.com/drive/1qWh8F2iFfM64seNESDUrnIuofWrev1Kv#scrollTo=zuetCbHVoKZA)\r\n\r\n@myy920213 ,\r\nCan you please provide access to the colab gist? Thanks!", "> > > @myy920213 ,\r\n> > > When tried executing the given code i got the as present in the [gist](https://colab.sandbox.google.com/gist/oanush/5a89ff690cd4c5e9ad5f75d4e53f6b61/32607.ipynb) of colab.Thanks!\r\n> > \r\n> > \r\n> > Thanks for your response.\r\n> > This code used to work on my own laptop, but will show the same error when running on Google cloud or AWS GPU instances. So I guessed the problem was running with GPU.\r\n> > After running the colab notebook you provided, I realized this problem may not be running with GPU, as I created a new colab notebook and run this code without using GPU, this error occur again. Here is the colab page: (https://colab.research.google.com/drive/1qWh8F2iFfM64seNESDUrnIuofWrev1Kv#scrollTo=zuetCbHVoKZA)\r\n> \r\n> @myy920213 ,\r\n> Can you please provide access to the colab gist? Thanks!\r\n\r\nSorry about that, I updated the share access. Could you please try again? (https://colab.research.google.com/drive/1qWh8F2iFfM64seNESDUrnIuofWrev1Kv)", "@myy920213 ,\r\nWhen tried executing the code in TF-1.15rc1, i did not face error. Take a look at the [gist](https://colab.sandbox.google.com/gist/oanush/8b5a7733bef89bc0c729d1850945e14f/git_test.ipynb#scrollTo=Kaj240A0Yrty) of colab.Thanks!", "@myy920213,\r\nAny update on the issue ?Thanks!", "> @myy920213,\r\n> Any update on the issue ?Thanks!\r\n\r\nThanks! I will close this issue now."]}, {"number": 32606, "title": "[model_to_estimator] estimator not evaluate all outputs defined in keras model but only one", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\nWhen using keras model with **multiple outputs**, and convert it to estimator by `model_to_estimator`, it only evaluate one output by `estimator.evaluate` or `tf.estimator.train_and_evaluate`. \r\nAnd don't know which output it evaluate (it doesn't display output name but only metrics name).\r\n\r\n**Describe the expected behavior**\r\nIt should only evaluate all outputs defined in keras model, not only pick one.\r\n\r\n**Code to reproduce the issue**\r\nA example here:\r\n```python\r\n    ...\r\n\r\n    model = tf.keras.Model(inputs=input_layer, outputs=[output_layer1, output_layer2])\r\n\r\n    model.compile(optimizer=optimizer_builder.build(config.optimizer),\r\n                  loss={\r\n                      'output_layer1': 'sparse_categorical_crossentropy',\r\n                      'output_layer2': 'sparse_categorical_crossentropy'},\r\n                  metrics={\r\n                      'output_layer1': 'accuracy',\r\n                      'output_layer2': 'accuracy'})\r\n\r\n    estimator = tf.keras.estimator.model_to_estimator(\r\n        keras_model=model,\r\n        config=run_config,\r\n    )\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: train_input_fn(config))\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=lambda: eval_input_fn(config))\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n    # Only evaluate one output and don't know which output it evaluate\r\n```\r\n\r\n**Other info / logs**\r\ntf log here, except two outputs log, but only get one:\r\n```\r\n# Training is OK\r\nINFO:tensorflow:global_step/sec: 1.02106\r\nINFO:tensorflow:loss = 0.16959098, step = 33600 (97.938 sec)\r\nINFO:tensorflow:global_step/sec: 0.943188\r\nINFO:tensorflow:loss = 0.17629223, step = 33700 (106.023 sec)\r\nINFO:tensorflow:global_step/sec: 0.848641\r\nINFO:tensorflow:loss = 0.1101246, step = 33800 (117.835 sec)\r\nINFO:tensorflow:global_step/sec: 0.852682\r\nINFO:tensorflow:loss = 0.104427785, step = 33900 (117.277 sec)\r\nINFO:tensorflow:Saving checkpoints for 34000 into /summary/model.ckpt.\r\n...\r\n\r\n# Evaluating only evaluate one output\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [150/1500]\r\nINFO:tensorflow:Finished evaluation at 2019-09-17-14:14:45\r\nINFO:tensorflow:Saving dict for global step 34000: accuracy = 0.6914724, sparse_categorical_crossentropy = 1.0857606, global_step = 34000, loss = 1.1107497\r\n```\r\n\r\nTensorBoard except two outputs scalar, but only get one:\r\n![2019-09-18 11 27 54](https://user-images.githubusercontent.com/7609173/65105491-aaff6000-da07-11e9-916f-473ba7847ba6.png)\r\n", "comments": ["@Jasonnor, Thanks for reporting the issue.\r\nWill it be possible to provide the standalone code to reproduce the reported issue. Thanks!", "@gadagashwini Sure! Here is a testing code:\r\nhttps://colab.research.google.com/drive/1qU5uhOfCy1MFtvUA1-8nJdzdgcdbpQvM\r\n\r\nI compiled model by:\r\n```python\r\n    model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),\r\n                  loss={\r\n                      'output_1': 'sparse_categorical_crossentropy',\r\n                      'output_2': 'sparse_categorical_crossentropy'},\r\n                  metric={\r\n                      'output_1': 'accuracy',\r\n                      'output_2': 'accuracy'})\r\n```\r\nBut only get one loss result, not two result of two output. (and don't know why without accuracy metric result?)\r\n```\r\nINFO:tensorflow:Evaluation [100/100]\r\nINFO:tensorflow:Finished evaluation at 2019-09-19-06:39:42\r\nINFO:tensorflow:Saving dict for global step 0: global_step = 0, loss = 12.851205, sparse_categorical_crossentropy = 2.3049202\r\n```", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32606\">No</a>\n"]}, {"number": 32605, "title": "[xla] fix xla build on cuda devices without nccl", "body": "Some cuda devices, such as Jetson devices, do not support NCCL.\r\nBuilding `@local_config_nccl//:nccl` on such kind of devices\r\nwill cause problem.", "comments": ["Would you mind checking if --config=nonccl would work for Jetson?\r\n\r\nCommit 9b9bea6 should have made XLA respect this flag as well.", "@chsigg nope, it doesn't work. I used `--config=nonccl` to build tensorflow without success. Actually, I did `git pull`  and then `bazel build --config opt --config nonccl ...` about once per week the past month. Can't finish it without my one-line change. I'll do it one more time and log the error message later this week. Basically, without the one-line change, it tried to build **nccl** even with `--config=nonccl`.  ", "Without the `s/if_cuda/if_nccl/` I changed, I always got message like the following. As you can see, it's trying to use NCCL (note that I have `--config=nonccl`)\r\n \r\n<pre><font color=\"#3465A4\">SUBCOMMAND: </font># @nccl_archive//:device_dlink_hdrs [action &apos;nvlink external/nccl_archive/device_dlink_hdrs_register_sm_53.h&apos;]\r\n(cd /home/freedom/.cache/bazel/_bazel_freedom/e3dde6e75d2cbb04c9aca0ccc9afc1f6/execroot/org_tensorflow &amp;&amp; \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/local_config_cuda/cuda/cuda/bin/nvlink &apos;--arch=sm_53&apos; &apos;--register-link-binaries=bazel-out/aarch64-opt/bin/external/nccl_archive/device_dlink_hdrs_register_sm_53.h&apos; &apos;--output-file=bazel-out/aarch64-opt/bin/external/nccl_archive/device_dlink_hdrs_sm_53.cubin&apos; bazel-out/aarch64-opt/bin/external/nccl_archive/libdevice_lib.pic.a)\r\n<font color=\"#4E9A06\">[588 / 618]</font> nvlink external/nccl_archive/device_dlink_hdrs_register_sm_53.h; 0s local\r\n<font color=\"#EF2929\"><b>ERROR: </b></font>/home/freedom/.cache/bazel/_bazel_freedom/e3dde6e75d2cbb04c9aca0ccc9afc1f6/external/nccl_archive/BUILD.bazel:53:1: nvlink external/nccl_archive/device_dlink_hdrs_register_sm_53.h failed (Exit 255)\r\n<font color=\"#4E9A06\">[589 / 618]</font> checking cached actions\r\nnvlink error   : entry function &apos;_Z37ncclReduceScatterTreeLLKernel_sum_f648ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\nnvlink error   : entry function &apos;_Z37ncclReduceScatterRingLLKernel_sum_f648ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\nnvlink error   : entry function &apos;_Z30ncclReduceTreeLLKernel_sum_f648ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\nnvlink error   : entry function &apos;_Z30ncclReduceRingLLKernel_sum_f648ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\nnvlink error   : entry function &apos;_Z33ncclAllReduceTreeLLKernel_sum_f648ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\nnvlink error   : entry function &apos;_Z33ncclAllReduceRingLLKernel_sum_f648ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\nnvlink error   : entry function &apos;_Z37ncclReduceScatterTreeLLKernel_sum_f328ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\nnvlink error   : entry function &apos;_Z37ncclReduceScatterRingLLKernel_sum_f328ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\nnvlink error   : entry function &apos;_Z30ncclReduceTreeLLKernel_sum_f328ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\nnvlink error   : entry function &apos;_Z30ncclReduceRingLLKernel_sum_f328ncclColl&apos; with max regcount of 80 calls function &apos;_Z25ncclAllGatherRing_copy_i8P14CollectiveArgs&apos; with regcount of 96\r\n...\r\n</pre>"]}, {"number": 32604, "title": "EmptyTensorList, TensorListFromTensor, TensorListReserve, TensorListStack, While.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@LLLLLLoki, Please provide the information asked in the above template. Thanks!", "@LLLLLLoki, Please fill the template. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 32603, "title": "[model_to_estimator] summary in keras loss/metrics function get duplicate scalars with same value in TensorBoard", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\nUsing tf.summary.scalar in keras loss/metrics function, then use model_to_estimator to convert keras model to estimator, tensorboard get duplicate scalars with totally same value. (e.g., `metrics/accuracy/train` and `metrics/accuracy/train_1`)\r\n\r\n**Describe the expected behavior**\r\nIt should only summary one scalar.\r\n\r\n**Code to reproduce the issue**\r\nA example here:\r\n```python\r\n    ...\r\n\r\n    model = tf.keras.Model(inputs=input_layer, outputs=[output_layer1, output_layer2])\r\n\r\n    def accuracy(y_true, y_pred):\r\n        y_pred, y_true = reshape_keras_tensor(y_pred, y_true)\r\n        acc = tf.reduce_mean(\r\n            tf.cast(tf.equal(tf.cast(y_true, tf.int64), tf.argmax(input=y_pred, axis=1)), tf.float32))\r\n        # Where I summary, without this line, it will not summary anything during training\r\n        tf.summary.scalar('train', acc)\r\n        return acc\r\n\r\n    def total_loss(y_true, y_pred):\r\n        y_pred, y_true = reshape_keras_tensor(y_pred, y_true)\r\n        loss =  tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n            labels=y_true,\r\n            logits=y_pred)\r\n        loss = tf.reduce_mean(cross_entropy)\r\n        # Where I summary, without this line, it will not summary anything during training\r\n        tf.summary.scalar('train', loss)\r\n        return loss\r\n\r\n    ...\r\n\r\n    model.compile(optimizer=optimizer_builder.build(config.optimizer),\r\n                  loss={output_layer1: total_loss},\r\n                  metrics={output_layer1: accuracy})\r\n\r\n    estimator = tf.keras.estimator.model_to_estimator(\r\n        keras_model=model,\r\n        config=run_config,\r\n    )\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: train_input_fn(config))\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=lambda: eval_input_fn(config))\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\n**Other info / logs**\r\nTensorBoard with duplicate scalars:\r\n![2019-09-18 10 28 17](https://user-images.githubusercontent.com/7609173/65102919-27da0c00-d9ff-11e9-8a46-6d00ea39a4eb.png)\r\n![2019-09-18 10 27 56](https://user-images.githubusercontent.com/7609173/65102920-2872a280-d9ff-11e9-9d85-b626593688a7.png)\r\n\r\n", "comments": ["You can just edit the `tf.summary.scalar('train', ...)` calls to pass in different string tags - right now they're both using `train` so the second one gets mapped to `train_1`.", "Hi @nfelt, thanks for the reply, but I still get same problem after change scalar name `train` to `train_acc` and `train_f1_score` or something else without same name.\r\n![image](https://user-images.githubusercontent.com/7609173/65739989-d70e9580-e119-11e9-916f-1f87bc11d7ac.png)\r\n\r\nLooks like a bug of summary in custom loss/metrics function with model_to_estimator.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32603\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32603\">No</a>\n"]}, {"number": 32602, "title": "tensorrt6 compatibility", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.3 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0-rc2\r\n- Python version: 3.7.4 x64\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.1, 7.6\r\n- GPU model and memory:\r\nGTX1080Ti GDDR5X 11GB X 7\r\n\r\n\r\n**Describe the problem**\r\nbazel build failed\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\nadd cuda,tensorrt\r\nbazel build --config=opt --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/external/local_config_tensorrt/BUILD:43:1: Executing genrule @local_config_tensorrt//:tensorrt_include failed (Exit 1)\r\ncp: cannot stat '/usr/include/x86_64-linux-gnu/NvInferPlugin.h': No such file or directory\r\n```", "comments": ["@alanpurple,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.", "@gadagashwini \r\nadded", "Can you include a full log from ./configure, as well as a list of the contents of the TensorRT directory you configured?", "successfully build tensorflow 2.0 GA with tensorrt6\r\n\r\nlooks resolved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32602\">No</a>\n"]}, {"number": 32601, "title": "Is there any documents of TF Core", "body": "Such as:\r\n\r\n1. Global Flow Chart of Tensorflow\r\n2. Graph modify for Model Optimization(Insert an op in backend)\r\n3.  Basic Data Structure(Data management)\r\n4. Detail of Communication for Distribute Strategy", "comments": ["Please, go through [distribution strategy ](https://www.tensorflow.org/beta/guide/distribute_strategy#examples_and_tutorials)tutorials and see if it helps you. Thanks!\r\n\r\n", "@ravikyram the distribution strategy tutorials only tell use how to use the strategy rather than how to optimize our distributed task", "@MarkDaoust  any feedback?", "Aside from the \"Global overview of tensorflow\" I think we're doing okay here.\r\n\r\n> Graph modify for Model Optimization(Insert an op in backend)\r\n> Basic Data Structure(Data management)\r\n> Detail of Communication for Distribute Strategy\r\n\r\nMost of this is in the [\"customzation\" section of the guide](https://www.tensorflow.org/guide/eager). Which I think covers \"add an op\" and \"basic data-structures\".\r\n\r\nFor \"Graph modifications\" the team has opted not to document things like [Graph Transform tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms) because we really don't think this is the future of the platform, everything's moving towards MLIR. \r\n\r\n> Detail of Communication for Distribute Strategy\r\n> The distribution strategy tutorials only tell use how to use the strategy\r\n\r\nThe video linked from the guide  gives some more detail:  https://www.youtube.com/watch?v=jKV53r9-H14", "@MarkDaoust  Here I give another example: \r\nIn TF2.0, I have a keras model, if I use the **tf.keras.model.fit API**, my model will run in non-eager mode, if I use the **tf.keras.model.fit_generator** API and add data augmentation using **tf.keras.preprocessing.image.ImageDataGenerator** API , my model will run in eager mode. \r\nThere is no docs tell us this. Most users may not care about this but there still some users interested in. The develop of TF want to hide the complexity but some users want to know more details. ", "> tf.keras.model.fit API, my model will run in non-eager mode, if I use the tf.keras.model.fit_generator.\r\n\r\nInteresting. I was not aware of this. How did you come to this conclusion? \r\n\r\nIt may be worth adding a note to the relavant tutorials over in https://github.com/tensorflow/docs", "@KANGRuipeng Just a point to note. `model.fit` runs in a tf-function for performance related reasons, in case you want to run eagerly, you can use run_eagerly=True. On TF website, there is a place where it mentions about the eager mode as follows.\r\n\r\n> Additionally, to make sure the model trains and evaluates eagerly, you can make sure to pass run_eagerly=True as a parameter to compile.\r\n\r\nThe above statement is in this TF [webpage](https://www.tensorflow.org/guide/keras/overview#train_and_evaluate). I think may be it is better to add little more info in that page. Thanks!", "@jvishnuvardhan I know this and I have read related docs.", "@MarkDaoust I found this when we move our code TF2.0", "Closing this issue now as many changes have been made to the docs in the last year. \r\n\r\nA few notes\r\n`tf.keras.model.fit_generator` is now deprecated\r\nThere are new guides on basics of [graphs](https://www.tensorflow.org/guide/intro_to_graphs) and [performance with tf.function.](https://www.tensorflow.org/guide/function) There is also a [guide on optimizing GPU ](https://www.tensorflow.org/guide/gpu_performance_analysis)(including multi gpu) training."]}, {"number": 32600, "title": "Tensorflow seems lost some module No module named 'numpy.core._multiarray_umath'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n\r\nRequirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\r\nRequirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (41.0.1)\r\nRequirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\r\nInstalling collected packages: tensorflow\r\nSuccessfully installed tensorflow-1.14.0\r\n\r\n(base) C:\\Users\\Liu>python\r\nPython 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\r\nImportError: numpy.core.multiarray failed to import\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<frozen importlib._bootstrap>\", line 980, in _find_and_load\r\nSystemError: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set\r\nImportError: numpy.core._multiarray_umath failed to import\r\nImportError: numpy.core.umath failed to import\r\n2019-09-18 08:38:07.989471: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@david2588e, \r\nPlease provide the numpy version that you are using.\r\n```\r\nimport numpy as np\r\nnp.__version__\r\n```", "Thanks\r\n\r\n>>> import numpy as np\r\n>>> np.__version__\r\n'1.15.1'\r\n\r\n\u53d1\u9001\u81ea Windows 10 \u7248\u90ae\u4ef6<https://go.microsoft.com/fwlink/?LinkId=550986>\u5e94\u7528\r\n\r\n________________________________\r\n\u53d1\u4ef6\u4eba: gadagashwini <notifications@github.com>\r\n\u53d1\u9001\u65f6\u95f4: Wednesday, September 18, 2019 5:19:52 PM\r\n\u6536\u4ef6\u4eba: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\n\u6284\u9001: david2588e <liujinyi@hotmail.com>; Mention <mention@noreply.github.com>\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] Tensorflow seems lost some module No module named 'numpy.core._multiarray_umath' (#32600)\r\n\r\n\r\n@david2588e<https://github.com/david2588e>,\r\nPlease provide the numpy version that you are using.\r\n\r\nimport numpy as np\r\nnp.__version__\r\n\r\n\r\n\u2015\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/32600?email_source=notifications&email_token=AEXWBPG6NWVCGJFY4PICV2DQKHXDRA5CNFSM4IXXGLN2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD67MPJY#issuecomment-532596647>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AEXWBPB2M7A6IVWNQX76773QKHXDRANCNFSM4IXXGLNQ>.\r\n", "@david2588e, Please install numpy with 1.16.5 version. Let us know how it progresses. Thanks! ", "Hallo Sir:\r\n\r\nAfter install the numpy 1.16.5, now it is fine\r\n\r\nThanks\r\n\r\n\u53d1\u9001\u81ea Windows 10 \u7248\u90ae\u4ef6<https://go.microsoft.com/fwlink/?LinkId=550986>\u5e94\u7528\r\n\r\n________________________________\r\n\u53d1\u4ef6\u4eba: gadagashwini <notifications@github.com>\r\n\u53d1\u9001\u65f6\u95f4: Wednesday, September 18, 2019 7:55:21 PM\r\n\u6536\u4ef6\u4eba: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\n\u6284\u9001: david2588e <liujinyi@hotmail.com>; Mention <mention@noreply.github.com>\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] Tensorflow seems lost some module No module named 'numpy.core._multiarray_umath' (#32600)\r\n\r\n\r\n@david2588e<https://github.com/david2588e>, Please install numpy with 1.16.5 version. Let us know how it progresses. Thanks!\r\n\r\n\u2015\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/32600?email_source=notifications&email_token=AEXWBPDKL4F7CTKBM4ZZRWDQKIJKTA5CNFSM4IXXGLN2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD67ZFUA#issuecomment-532648656>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AEXWBPHUCKSNPYF2JLHSAZ3QKIJKTANCNFSM4IXXGLNQ>.\r\n", "@david2588e,\r\nGlad it worked. \r\nClosing the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32600\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32600\">No</a>\n"]}, {"number": 32599, "title": "Switch IteratorTest to use TF combinations", "body": "This PR switch `IteratorTest` to use TF combinations.", "comments": ["@jsimsa `testIteratorStructure()` is split into three test functions to avoid the use of `lambda` parameter. Could you please take a look at the change(https://github.com/tensorflow/tensorflow/pull/32599/commits/7268f8dc5cd53471b129cb52f76c4b3147ffd173)?", "@gbaned An error happened while migrating the change for the internal check. Could you please help re-trigger the internal check? Thanks!"]}, {"number": 32598, "title": "pix2pix tutorial is broken with tf2 rc", "body": "The tutorial given here: \r\n[https://www.tensorflow.org/beta/tutorials/generative/pix2pix](https://www.tensorflow.org/beta/tutorials/generative/pix2pix) no longer works on a clean install of the current tf2 rc. \r\n\r\nit breaks around the 16th block. \r\n\r\nI wasn't sure where to report this problem\r\n\r\n```\r\n\r\nWARNING:tensorflow:Entity <function load_image_train at 0x10e71cf80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <function load_image_train at 0x10e71cf80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n\r\n---------------------------------------------------------------------------\r\nOperatorNotAllowedInGraphError            Traceback (most recent call last)\r\n<ipython-input-16-e1fc1fbf2b89> in <module>\r\n      2 train_dataset = train_dataset.shuffle(BUFFER_SIZE)\r\n      3 train_dataset = train_dataset.map(load_image_train,\r\n----> 4                                   num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n      5 train_dataset = train_dataset.batch(1)\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls)\r\n   1902       return DatasetV1Adapter(\r\n   1903           ParallelMapDataset(\r\n-> 1904               self, map_func, num_parallel_calls, preserve_cardinality=False))\r\n   1905 \r\n   1906   @deprecation.deprecated(None, \"Use `tf.data.Dataset.map()\")\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\r\n   3452         self._transformation_name(),\r\n   3453         dataset=input_dataset,\r\n-> 3454         use_legacy_function=use_legacy_function)\r\n   3455     self._num_parallel_calls = ops.convert_to_tensor(\r\n   3456         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\r\n   2693       resource_tracker = tracking.ResourceTracker()\r\n   2694       with tracking.resource_tracker_scope(resource_tracker):\r\n-> 2695         self._function = wrapper_fn._get_concrete_function_internal()\r\n   2696         if add_to_graph:\r\n   2697           self._function.add_to_graph(ops.get_default_graph())\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal(self, *args, **kwargs)\r\n   1852     \"\"\"Bypasses error checking when getting a graph function.\"\"\"\r\n   1853     graph_function = self._get_concrete_function_internal_garbage_collected(\r\n-> 1854         *args, **kwargs)\r\n   1855     # We're returning this concrete function to someone, and they may keep a\r\n   1856     # reference to the FuncGraph without keeping a reference to the\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in wrapper_fn(*args)\r\n   2687           attributes=defun_kwargs)\r\n   2688       def wrapper_fn(*args):  # pylint: disable=missing-docstring\r\n-> 2689         ret = _wrapper_helper(*args)\r\n   2690         ret = structure.to_tensor_list(self._output_structure, ret)\r\n   2691         return [ops.convert_to_tensor(t) for t in ret]\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in _wrapper_helper(*args)\r\n   2632         nested_args = (nested_args,)\r\n   2633 \r\n-> 2634       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n   2635       # If `func` returns a list of tensors, `nest.flatten()` and\r\n   2636       # `ops.convert_to_tensor()` would conspire to attempt to stack\r\n\r\n~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    235       except Exception as e:  # pylint:disable=broad-except\r\n    236         if hasattr(e, 'ag_error_metadata'):\r\n--> 237           raise e.ag_error_metadata.to_exception(e)\r\n    238         else:\r\n    239           raise\r\n\r\nOperatorNotAllowedInGraphError: in converted code:\r\n\r\n    <ipython-input-14-e5f2b44984ba>:3 load_image_train\r\n        input_image, real_image = random_jitter(input_image, real_image)\r\n    <ipython-input-12-b7170a9df479>:8 random_jitter\r\n        if tf.random.uniform(()) > 0.5:\r\n    /Users/nathan/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:765 __bool__\r\n        self._disallow_bool_casting()\r\n    /Users/nathan/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:528 _disallow_bool_casting\r\n        \"using a `tf.Tensor` as a Python `bool`\")\r\n    /Users/nathan/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:513 _disallow_when_autograph_disabled\r\n        \" Try decorating it directly with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.\r\n\r\n```", "comments": ["@SirSharpest ,\r\nCan you please try using latest version of TF-2.0rc1? it worked fine me in the same.\r\nThanks!", "Yes, I can confirm that it is working! Sorry for confusion", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32598\">No</a>\n", "@oanush I was just checking some other of the examples and I noticed this one, even on colab doesn't work and throws crazy errors? Would you be able to help verify?\r\n\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/images/segmentation.ipynb ", "Seems to be related to https://github.com/tensorflow/tensorflow/issues/32319", "@SirSharpest ,\r\nI was able to run in colab without any issue's for TF 2.0rc1. What errors are you facing? \r\nMeanwhile try downgrading to` gast-0.2.2` as workaround `pip install gast==0.2.2` and tryout the tutorial.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32598\">No</a>\n"]}, {"number": 32597, "title": "Add explicit TensorShape conversion to Keras Base Layer", "body": "Fixes #32476\r\n\r\nIf you return a tuple in compute_output_shapes of a subclassed dynamic Keras Layer, it will be incorrectly interpreted (see #32476). In that case, map_structure will map to each item in the tuple individually, giving an incorrect output, as if each item in the tuple was intended to be a separate vector, which is incongruous to the [Keras docs](https://keras.io/layers/writing-your-own-keras-layers/). In this edge case, it will convert the tuple to a TensorShape directly.\r\n\r\nThis is my first PR for Tensorflow, so let me know if there's anything I should change.", "comments": ["it looks like you can't do TensorShape((TensorShape, int)); not good to merge", "committed fix"]}, {"number": 32596, "title": "Fix support for custom op namespacing in Python", "body": "Ops can have \">\" in their name to separate namespaces, and the python op gen code will still work.", "comments": []}, {"number": 32595, "title": "Add explicit TensorShape conversion to Keras Layer", "body": "Fixes #32476 \r\n\r\nIf you return a tuple in `compute_output_shapes` of a subclassed dynamic Keras Layer, it will be incorrectly interpreted (see #32476). The behavior doesn't occur when the output is, instead, a TensorShape. Since tf.TensorShape(tensor_shape) just returns tensor_shape, it is safe to add an explicit conversion to a TensorShape. This should address all potential return values of `compute_output_shapes` to make it inline with the Keras documentation (see: [Keras docs on custom layers](https://keras.io/layers/writing-your-own-keras-layers/)).\r\n\r\nThis is my first PR for Tensorflow, so let me know if there's anything I should change.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32595) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32595) for more info**.\n\n<!-- ok -->"]}, {"number": 32594, "title": "2 x WARNING: Entity <function ... initialize_variables at ...>could not be transformed and will be executed as-is", "body": "Hi! I'm getting two `Entity ... could not be transformed and will be executed as-is` errors while training a Keras model. Here's a Colab notebook illustrating the problem:\r\n\r\nhttps://colab.research.google.com/drive/1muFh7JjN6AJcWmF0Kp3zRwE043Uh-Tk6\r\n\r\nThis may be related to #32319 which I raised last week. Since that's been resolved, this might be, too. I don't know how to check that, though, so I thought I'd file to be on the safe side.", "comments": ["I am seeing the same problem working through the [official TensorFlow 2 text generation tutorial](https://www.tensorflow.org/beta/tutorials/text/text_generation).\r\n\r\n[The notebook](https://colab.research.google.com/drive/1nZfYKV2mecDx15aUiOEkHorzxly7PTZT) that demos the problem.\r\n\r\nThe log generated by `tf.autograph.set_verbosity(10)`:\r\n```\r\nINFO:tensorflow:Converted call: <function split_input_target at 0x7ff0e3afed90>\r\n    args: (<tf.Tensor 'args_0:0' shape=(101,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Not whitelisted: <method-wrapper '__call__' of function object at 0x7ff0e3afed90>: default rule\r\nINFO:tensorflow:Not whitelisted: <function split_input_target at 0x7ff0e3afed90>: default rule\r\nINFO:tensorflow:Entity <function split_input_target at 0x7ff0e3afed90> is not cached for key <code object split_input_target at 0x7ff0ed5f4540, file \"<ipython-input-4-8768199904c5>\", line 20> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7ff0e3b23550>, frozenset())\r\nINFO:tensorflow:Converting <function split_input_target at 0x7ff0e3afed90>\r\nINFO:tensorflow:Source code of <function split_input_target at 0x7ff0e3afed90>:\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import unicode_literals\r\ndef split_input_target(chunk):\r\n    input_text = chunk[:-1]\r\n    target_text = chunk[1:]\r\n    return input_text, target_text\r\n\r\n\r\nINFO:tensorflow:Compiled output of <function split_input_target at 0x7ff0e3afed90>:\r\n\r\n# coding=utf-8\r\ndef tf__split_input_target(chunk):\r\n  do_return = False\r\n  retval_ = ag__.UndefinedReturnValue()\r\n  with ag__.FunctionScope('split_input_target', 'split_input_target_scope', ag__.STD) as split_input_target_scope:\r\n    input_text = chunk[:-1]\r\n    target_text = chunk[1:]\r\n    do_return = True\r\n    retval_ = split_input_target_scope.mark_return_value((input_text, target_text))\r\n  do_return,\r\n  return ag__.retval(retval_)\r\n\r\n\r\nINFO:tensorflow:Compiled AST of <function split_input_target at 0x7ff0e3afed90>:\r\n\r\nFunctionDef:\r\n| name=u\"tf__split_input_target\"\r\n| args=arguments:\r\n| | args=[\r\n| | | Name:\r\n| | | | id=u\"chunk\"\r\n| | | | ctx=Param()\r\n| | | | annotation=None\r\n| | | | ___pyct_anno={'lineno': 5, ORIGIN: <ipython-input-4-8768199904c5>:20:23, QN: chunk, DEFINITIONS: (AnnotatedDef[140671222831704],), ORIG_DEFINITIONS: (AnnotatedDef[140671222829352],)}\r\n| | ]\r\n| | vararg=None\r\n| | kwonlyargs=[]\r\n| | kw_defaults=[]\r\n| | kwarg=None\r\n| | defaults=[]\r\n| | ___pyct_anno={SCOPE: Scope{r=(), w=(chunk,)}}\r\n| body=[\r\n| | Assign:\r\n| | | targets=[\r\n| | | | Name:\r\n| | | | | id=u\"do_return\"\r\n| | | | | ctx=Store()\r\n| | | | | annotation=None\r\n| | | | | ___pyct_anno={QN: do_return, DEFINITIONS: (AnnotatedDef[140672588987920],)}\r\n| | | ]\r\n| | | value=NameConstant:\r\n| | | | value=False\r\n| | | ___pyct_anno={SCOPE: Scope{r=(), w=(do_return,)}, LIVE_VARS_IN: frozenset({ag__.UndefinedReturnValue, split_input_target_scope.mark_return_value, chunk, ag__.retval, ag__.FunctionScope, ag__.STD, ag__})}\r\n| | Assign:\r\n| | | targets=[\r\n| | | | Name:\r\n| | | | | id=u\"retval_\"\r\n| | | | | ctx=Store()\r\n| | | | | annotation=None\r\n| | | | | ___pyct_anno={QN: retval_, DEFINITIONS: (AnnotatedDef[140671222945440],)}\r\n| | | ]\r\n| | | value=Call:\r\n| | | | func=Attribute:\r\n| | | | | value=Name:\r\n| | | | | | id=u\"ag__\"\r\n| | | | | | ctx=Load()\r\n| | | | | | annotation=None\r\n| | | | | | ___pyct_anno={QN: ag__, DEFINITIONS: ()}\r\n| | | | | attr=u\"UndefinedReturnValue\"\r\n| | | | | ctx=Load()\r\n| | | | | ___pyct_anno={QN: ag__.UndefinedReturnValue}\r\n| | | | args=[]\r\n| | | | keywords=[]\r\n| | | | ___pyct_anno={ARGS_SCOPE: Scope{r=(), w=()}}\r\n| | | ___pyct_anno={SCOPE: Scope{r=(ag__.UndefinedReturnValue, ag__), w=(retval_,)}, LIVE_VARS_IN: frozenset({ag__.UndefinedReturnValue, chunk, split_input_target_scope.mark_return_value, ag__.retval, ag__.FunctionScope, ag__.STD, ag__})}\r\n| | With:\r\n| | | items=[\r\n| | | | withitem:\r\n| | | | | context_expr=Call:\r\n| | | | | | func=Attribute:\r\n| | | | | | | value=Name:\r\n| | | | | | | | id=u\"ag__\"\r\n| | | | | | | | ctx=Load()\r\n| | | | | | | | annotation=None\r\n| | | | | | | | ___pyct_anno={QN: ag__, DEFINITIONS: ()}\r\n| | | | | | | attr=u\"FunctionScope\"\r\n| | | | | | | ctx=Load()\r\n| | | | | | | ___pyct_anno={QN: ag__.FunctionScope}\r\n| | | | | | args=[\r\n| | | | | | | Str:\r\n| | | | | | | | s=u\"split_input_target\"\r\n| | | | | | | Str:\r\n| | | | | | | | s=u\"split_input_target_scope\"\r\n| | | | | | | Attribute:\r\n| | | | | | | | value=Name:\r\n| | | | | | | | | id=u\"ag__\"\r\n| | | | | | | | | ctx=Load()\r\n| | | | | | | | | annotation=None\r\n| | | | | | | | | ___pyct_anno={QN: ag__, DEFINITIONS: ()}\r\n| | | | | | | | attr=u\"STD\"\r\n| | | | | | | | ctx=Load()\r\n| | | | | | | | ___pyct_anno={QN: ag__.STD}\r\n| | | | | | ]\r\n| | | | | | keywords=[]\r\n| | | | | | ___pyct_anno={ARGS_SCOPE: Scope{r=(ag__.STD, ag__), w=()}}\r\n| | | | | optional_vars=Name:\r\n| | | | | | id=u\"split_input_target_scope\"\r\n| | | | | | ctx=Store()\r\n| | | | | | annotation=None\r\n| | | | | | ___pyct_anno={QN: split_input_target_scope, DEFINITIONS: (AnnotatedDef[140671222947680],)}\r\n| | | | | ___pyct_anno={SCOPE: Scope{r=(ag__.FunctionScope, ag__.STD, ag__), w=(split_input_target_scope,)}}\r\n| | | ]\r\n| | | body=[\r\n| | | | Assign:\r\n| | | | | targets=[\r\n| | | | | | Name:\r\n| | | | | | | id=u\"input_text\"\r\n| | | | | | | ctx=Store()\r\n| | | | | | | annotation=None\r\n| | | | | | | ___pyct_anno={ORIG_DEFINITIONS: (AnnotatedDef[140672588527824],), ORIGIN: <ipython-input-4-8768199904c5>:21:4, QN: input_text, DEFINITIONS: (AnnotatedDef[140671222947792],)}\r\n| | | | | ]\r\n| | | | | value=Subscript:\r\n| | | | | | value=Name:\r\n| | | | | | | id=u\"chunk\"\r\n| | | | | | | ctx=Load()\r\n| | | | | | | annotation=None\r\n| | | | | | | ___pyct_anno={ORIG_DEFINITIONS: (AnnotatedDef[140671222829352],), ORIGIN: <ipython-input-4-8768199904c5>:21:17, QN: chunk, DEFINITIONS: (AnnotatedDef[140671222831704],)}\r\n| | | | | | slice=Slice:\r\n| | | | | | | lower=None\r\n| | | | | | | upper=UnaryOp:\r\n| | | | | | | | op=USub()\r\n| | | | | | | | operand=Num:\r\n| | | | | | | | | n=1\r\n| | | | | | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:21:25}\r\n| | | | | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:21:24}\r\n| | | | | | | step=None\r\n| | | | | | ctx=Load()\r\n| | | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:21:17}\r\n| | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:21:4, SCOPE: Scope{r=(chunk,), w=(input_text,)}, LIVE_VARS_IN: frozenset({split_input_target_scope, chunk, split_input_target_scope.mark_return_value, ag__.retval, ag__})}\r\n| | | | Assign:\r\n| | | | | targets=[\r\n| | | | | | Name:\r\n| | | | | | | id=u\"target_text\"\r\n| | | | | | | ctx=Store()\r\n| | | | | | | annotation=None\r\n| | | | | | | ___pyct_anno={ORIG_DEFINITIONS: (AnnotatedDef[140672588526816],), ORIGIN: <ipython-input-4-8768199904c5>:22:4, QN: target_text, DEFINITIONS: (AnnotatedDef[140671222947736],)}\r\n| | | | | ]\r\n| | | | | value=Subscript:\r\n| | | | | | value=Name:\r\n| | | | | | | id=u\"chunk\"\r\n| | | | | | | ctx=Load()\r\n| | | | | | | annotation=None\r\n| | | | | | | ___pyct_anno={ORIG_DEFINITIONS: (AnnotatedDef[140671222829352],), ORIGIN: <ipython-input-4-8768199904c5>:22:18, QN: chunk, DEFINITIONS: (AnnotatedDef[140671222831704],)}\r\n| | | | | | slice=Slice:\r\n| | | | | | | lower=Num:\r\n| | | | | | | | n=1\r\n| | | | | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:22:24}\r\n| | | | | | | upper=None\r\n| | | | | | | step=None\r\n| | | | | | ctx=Load()\r\n| | | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:22:18}\r\n| | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:22:4, SCOPE: Scope{r=(chunk,), w=(target_text,)}, LIVE_VARS_IN: frozenset({split_input_target_scope, chunk, split_input_target_scope.mark_return_value, ag__.retval, input_text, ag__})}\r\n| | | | Assign:\r\n| | | | | targets=[\r\n| | | | | | Name:\r\n| | | | | | | id=u\"do_return\"\r\n| | | | | | | ctx=Store()\r\n| | | | | | | annotation=None\r\n| | | | | | | ___pyct_anno={QN: do_return, DEFINITIONS: (AnnotatedDef[140672588527264],)}\r\n| | | | | ]\r\n| | | | | value=NameConstant:\r\n| | | | | | value=True\r\n| | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:23:4, SCOPE: Scope{r=(), w=(do_return,)}, LIVE_VARS_IN: frozenset({split_input_target_scope, target_text, split_input_target_scope.mark_return_value, ag__.retval, input_text, ag__})}\r\n| | | | Assign:\r\n| | | | | targets=[\r\n| | | | | | Name:\r\n| | | | | | | id=u\"retval_\"\r\n| | | | | | | ctx=Store()\r\n| | | | | | | annotation=None\r\n| | | | | | | ___pyct_anno={QN: retval_, DEFINITIONS: (AnnotatedDef[140672588529392],)}\r\n| | | | | ]\r\n| | | | | value=Call:\r\n| | | | | | func=Attribute:\r\n| | | | | | | value=Name:\r\n| | | | | | | | id=u\"split_input_target_scope\"\r\n| | | | | | | | ctx=Load()\r\n| | | | | | | | annotation=None\r\n| | | | | | | | ___pyct_anno={QN: split_input_target_scope, DEFINITIONS: (AnnotatedDef[140671222947680],)}\r\n| | | | | | | attr=u\"mark_return_value\"\r\n| | | | | | | ctx=Load()\r\n| | | | | | | ___pyct_anno={QN: split_input_target_scope.mark_return_value}\r\n| | | | | | args=[\r\n| | | | | | | Tuple:\r\n| | | | | | | | elts=[\r\n| | | | | | | | | Name:\r\n| | | | | | | | | | id=u\"input_text\"\r\n| | | | | | | | | | ctx=Load()\r\n| | | | | | | | | | annotation=None\r\n| | | | | | | | | | ___pyct_anno={ORIG_DEFINITIONS: (AnnotatedDef[140672588527824],), ORIGIN: <ipython-input-4-8768199904c5>:23:11, QN: input_text, DEFINITIONS: (AnnotatedDef[140671222947792],)}\r\n| | | | | | | | | Name:\r\n| | | | | | | | | | id=u\"target_text\"\r\n| | | | | | | | | | ctx=Load()\r\n| | | | | | | | | | annotation=None\r\n| | | | | | | | | | ___pyct_anno={ORIG_DEFINITIONS: (AnnotatedDef[140672588526816],), ORIGIN: <ipython-input-4-8768199904c5>:23:23, QN: target_text, DEFINITIONS: (AnnotatedDef[140671222947736],)}\r\n| | | | | | | | ]\r\n| | | | | | | | ctx=Load()\r\n| | | | | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:23:11}\r\n| | | | | | ]\r\n| | | | | | keywords=[]\r\n| | | | | | ___pyct_anno={ARGS_SCOPE: Scope{r=(target_text, input_text), w=()}}\r\n| | | | | ___pyct_anno={ORIGIN: <ipython-input-4-8768199904c5>:23:4, SCOPE: Scope{r=(split_input_target_scope, target_text, split_input_target_scope.mark_return_value, input_text), w=(retval_,)}, LIVE_VARS_IN: frozenset({split_input_target_scope, target_text, do_return, split_input_target_scope.mark_return_value, ag__.retval, input_text, ag__})}\r\n| | | ]\r\n| | | ___pyct_anno={BODY_SCOPE: Scope{r=(split_input_target_scope, ag__.FunctionScope, ag__, target_text, chunk, split_input_target_scope.mark_return_value, ag__.STD, input_text), w=(split_input_target_scope, do_return, target_text, retval_, input_text)}, LIVE_VARS_IN: frozenset({split_input_target_scope.mark_return_value, chunk, ag__.retval, ag__.FunctionScope, ag__.STD, ag__})}\r\n| | Expr:\r\n| | | value=Tuple:\r\n| | | | elts=[\r\n| | | | | Name:\r\n| | | | | | id=u\"do_return\"\r\n| | | | | | ctx=Load()\r\n| | | | | | annotation=None\r\n| | | | | | ___pyct_anno={QN: do_return, DEFINITIONS: (AnnotatedDef[140672588527264],)}\r\n| | | | ]\r\n| | | | ctx=Load()\r\n| | | ___pyct_anno={SCOPE: Scope{r=(do_return,), w=()}, LIVE_VARS_OUT: frozenset({ag__.retval, retval_, ag__}), LIVE_VARS_IN: frozenset({do_return, ag__.retval, retval_, ag__})}\r\n| | Return:\r\n| | | value=Call:\r\n| | | | func=Attribute:\r\n| | | | | value=Name:\r\n| | | | | | id=u\"ag__\"\r\n| | | | | | ctx=Load()\r\n| | | | | | annotation=None\r\n| | | | | | ___pyct_anno={QN: ag__, DEFINITIONS: ()}\r\n| | | | | attr=u\"retval\"\r\n| | | | | ctx=Load()\r\n| | | | | ___pyct_anno={QN: ag__.retval}\r\n| | | | args=[\r\n| | | | | Name:\r\n| | | | | | id=u\"retval_\"\r\n| | | | | | ctx=Load()\r\n| | | | | | annotation=None\r\n| | | | | | ___pyct_anno={QN: retval_, DEFINITIONS: (AnnotatedDef[140672588529392],)}\r\n| | | | ]\r\n| | | | keywords=[]\r\n| | | | ___pyct_anno={ARGS_SCOPE: Scope{r=(retval_,), w=()}}\r\n| | | ___pyct_anno={SCOPE: Scope{r=(ag__.retval, retval_, ag__), w=()}, LIVE_VARS_IN: frozenset({ag__.retval, retval_, ag__})}\r\n| ]\r\n| decorator_list=[]\r\n| returns=None\r\n| ___pyct_anno={'lineno': 5, ORIGIN: <ipython-input-4-8768199904c5>:20:0, SCOPE: Scope{r=(), w=(split_input_target,)}, BODY_SCOPE: Scope{r=(split_input_target_scope, do_return, input_text, ag__.retval, ag__.FunctionScope, ag__, target_text, retval_, chunk, split_input_target_scope.mark_return_value, ag__.UndefinedReturnValue, ag__.STD), w=(split_input_target_scope, do_return, target_text, retval_, input_text)}, 'function_context_name': 'split_input_target_scope'}\r\n\r\n\r\n\r\nINFO:tensorflow:Defaults of <function create_converted_entity_factory.<locals>.create_converted_entity.<locals>.tf__split_input_target at 0x7ff0e3ac9d90> : None\r\nINFO:tensorflow:KW defaults of <function create_converted_entity_factory.<locals>.create_converted_entity.<locals>.tf__split_input_target at 0x7ff0e3ac9d90> : None\r\nINFO:tensorflow:Calling <function create_converted_entity_factory.<locals>.create_converted_entity.<locals>.tf__split_input_target at 0x7ff0e3ac9d90> with\r\n    chunk: Tensor(\"args_0:0\", shape=(101,), dtype=int64)\r\n```", "@mgreenbe ,\r\nCan you please try using TF-2.0rc1, no warnings were received. Kindly take a look into this [gist](https://colab.sandbox.google.com/gist/oanush/b81dbbd4bf9db66dace575f1446add09/copy-of-bug.ipynb) of colab.Thanks!", "@oanush ,\r\n\r\nOk, pip-installing the most recent release candidate on Colab worked. (When specifying Tensorflow 2.x by running `%tensorflow_version 2.x`, I would have expected it to switch to the most recent release candidate.) I also ran it locally and the warnings disappeared. So this is resolved as far as I can tell. Thanks!\r\n\r\nI'm now seeing a `WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.` warning at the start of training. This also appeared in your gist, though, so I assume you're aware of it.", "@mgreenbe ,\r\nThanks for reporting the issue, yes its just a generic warning that is provided for the user regarding deprecation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32594\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32594\">No</a>\n"]}, {"number": 32593, "title": "Merge pull request #1 from tensorflow/master", "body": "pull to keep up to date", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32593) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 32592, "title": "In tf.Estimator training metric is not divided by the batch size", "body": "**System information**\r\n- TensorFlow version: TF 1.12\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\n<img width=\"345\" alt=\"metric\" src=\"https://user-images.githubusercontent.com/23496841/65050585-e9ddd900-d967-11e9-82b5-b8d0a04164db.png\">\r\n\r\nI am training a model with tf.Estimator and it seems that the training metric (root mean squared error) is not divided by the batch size (for both training and validation the batch size is 100). Indeed, for comparison this is the mean squared error loss for training and validation:\r\n\r\n<img width=\"364\" alt=\"loss\" src=\"https://user-images.githubusercontent.com/23496841/65050797-450fcb80-d968-11e9-9a67-565efced4684.png\">\r\n\r\n**Code to reproduce the issue**\r\nI am using the standard structure for `model_fn` in `tf.Estimator`. Here's the main part of the code:\r\n\r\n``` Python\r\ndef model_fn(features, labels, mode, params):\r\n\r\n    # model layers \r\n    # ...\r\n\r\n    rmse = tf.metrics.root_mean_squared_error(labels, predictions)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\r\n            'new_states': predictions\r\n        }\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    loss = tf.losses.mean_squared_error(labels, predictions)\r\n\r\n    metrics = {'rmse': rmse}\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\r\n   \r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n```", "comments": ["@robertah ,\r\nCan you share a standalone code to reproduce the issue?thanks!", "I can't share the original input data, but the error can be reproduced in the following way:\r\n``` Python\r\nimport tensorflow as tf\r\nimport sys\r\n\r\ndef model_fn(features, labels, mode, params):\r\n\r\n    x = tf.layers.dense(features, 100)\r\n    x = tf.layers.dense(x, 100)\r\n    predictions = tf.layers.dense(x, 100)\r\n\r\n    rmse = tf.metrics.root_mean_squared_error(labels, predictions)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\r\n            'predictions': predictions\r\n        }\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    loss = tf.losses.mean_squared_error(labels, predictions)\r\n\r\n    metrics = {'rmse': rmse}\r\n    print_op = tf.print((loss, rmse[1]), output_stream=sys.stdout)\r\n    with tf.control_dependencies([print_op]):\r\n        tf.summary.scalar('rmse', rmse[1])\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\r\n\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    import numpy as np\r\n\r\n    features = np.random.randn(5000, 100)\r\n    labels = features * 10 + np.random.randn(5000, 100)/1000\r\n\r\n    x_train = {\r\n        'features': features[:4500],\r\n        'labels': labels[:4500]\r\n    }\r\n\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(x_train['features'], x_train['labels'], batch_size=100, shuffle=True, num_epochs=300)\r\n\r\n    x_eval = {\r\n        'features': features[4500:],\r\n        'labels': labels[4500:]\r\n    }\r\n\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(x_eval['features'], x_eval['labels'], batch_size=100, shuffle=False)\r\n\r\n    config = tf.estimator.RunConfig(\r\n        keep_checkpoint_max=1,\r\n        model_dir='model',\r\n        save_summary_steps=100,\r\n        save_checkpoints_steps=100,\r\n        log_step_count_steps=100)\r\n\r\n    estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=500000)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\r\n\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```", "Please find the [gist](https://colab.research.google.com/gist/oanush/2fb22e808e53bc37dfd9ecab4cbfa2bf/32592.ipynb) of colab when tried executing the given code.Thanks!", "I've added a random seed in the notebook. This are the loss and the rmse value plots:\r\n\r\n\r\n<img width=\"376\" alt=\"Schermata 2019-09-18 alle 10 57 19\" src=\"https://user-images.githubusercontent.com/23496841/65133893-ba7caa00-da03-11e9-8cc5-a91b370cc7be.png\">\r\n", "So in the end I think the problem is related to `tf.losses.mean_squared_error` and `tf.metrics.mean_squared_error` that return different values during the training. Please see the notebook for more details.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32592\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32592\">No</a>\n", "Hi @robertah,\r\n\r\nI think I have the same problem at the moment. I get an RMSE of about 4500 instead of 900 (with a batch_size of 5). Could you help me? I dont even understand why the metrics should be divided by the batch_size...\r\n\r\nHow did you solve the problem? "]}, {"number": 32591, "title": "fix typo in pywrap_tfe_src.cc", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32591) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32591) for more info**.\n\n<!-- ok -->"]}, {"number": 32590, "title": "[XLA] Add DiagSlice HLO instruction", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n@jlebar \r\nThere is the purpose of the new HLO instruction:\r\nGenerate more efficient kernel for DiagPart and MatrixDiagPart. For example:\r\n> %iota.3 = s32[1024,1024]{1,0} iota(), iota_dimension=0\r\n %iota.2 = s32[1024,1024]{1,0} iota(), iota_dimension=1\r\n %compare.0 = pred[1024,1024]{1,0} compare(s32[1024,1024]{1,0} %iota.3, s32[1024,1024]{1,0} %iota.2), direction=EQ\r\n %param_0.3 = f32[1024,1024]{1,0} parameter(0)\r\n %constant_0 = f32[] constant(0)\r\n %broadcast.0 = f32[1024,1024]{1,0} broadcast(f32[] %constant_0), dimensions={}\r\n %select.0 = f32[1024,1024]{1,0} select(pred[1024,1024]{1,0} %compare.0, f32[1024,1024]{1,0} \r\n %param_0.3, f32[1024,1024]{1,0} %broadcast.0)\r\n ROOT %reduce.0 = f32[1024]{0} reduce(f32[1024,1024]{1,0} %select.0, f32[] %constant_0), dimensions={0}, to_apply=%add_F32.17\r\n\r\nHere is the hlo code for tf.diag_part(X), where X is tensor with shape 1024x1024. the generated kernel's launch dimension is 1024x1024, and it has '_compare_' and '_select_' operation to get the right index to output. and the more expensive operation '_reduce_' to get the right value to output. The kernel is less efficient for diag-part op. Actually it just needs 1024 threads to get the right value from the diag part of the source matrix which just determined by offset.", "comments": ["Do you have an example where the 1024x1024 kernel launch was not already the result of another operations where the select and reduce could not be fused into the previous operation?\r\n\r\nEither way this should be split into two parts.\r\n1. Add the Hlo and make an evaluator implementation and move the expander into an Hlo pass.\r\n2. Add the implementation for the CPU/GPU backends.", "Can we make this a generic optimization that works as a lowering strategy for fused reductions?  For instance can we recognize this kind of access pattern and use a smarter elemental lowering for the fused kernel?\r\n\r\n/CC @bixia1 ", "@blakehechtman The current implementation could be fused into the previous operations. However, it is hard to be fused into the following operations because of reduction. In this PR, the diag-part is lowing to a slice. The slice operation is more easy to be fused into previous operations and following operations.\r\n@sanjoy It's a good question, but I have no idea now. I think 'slice' is a more straight expression of diag-part than 'reduce'.", "> Can we make this a generic optimization that works as a lowering strategy for fused reductions? For instance can we recognize this kind of access pattern and use a smarter elemental lowering for the fused kernel?\r\n\r\n@sanjoy\r\nI think the diag-part is basically an index transformation as '_transpose_', '_slice_' and '_reshape_'. Should we design a '_shuffle_' instruction with a nested computation which describes the index transformation for general index transformation situation?\r\n", "> I think the diag-part is basically an index transformation as 'transpose', 'slice' and 'reshape'. Should we design a 'shuffle' instruction with a nested computation which describes the index transformation for general index transformation situation?\r\n\r\nSorry I responded without fully understanding the IR snippet.\r\n\r\nThe IR snippet looks like it should be easily representable as a gather_nd with an iota index?  At least on CPU/GPU the iota index should fuse into the gather and the gather should further fuse into its users.\r\n\r\nI agree using a reduction this was seems odd.", "squash the commits that resolve conflicts. "]}, {"number": 32589, "title": "Implement reference kernel for Softmax into TFLu - Int8", "body": "", "comments": ["Can someone please re-run these tests? They shouldn't be affected by this change", "Ready to merge", "@giorgio-arenarm done.", "Still not merged, no rebase issues expected", "> Still not merged, no rebase issues expected\r\n\r\nwill try again to merge this again , thanks for your patience.", "@giorgio-arenarm can you please check the build failures ?", "I've had a look at the failures:\r\nhttps://source.cloud.google.com/results/invocations/b7a114e2-a06d-4aea-8afe-98c75bc071e1/targets/%2F%2Ftensorflow%2Ftools%2Fci_build%2Fbuilds:gen_win_out/log\r\nand\r\nhttps://source.cloud.google.com/results/invocations/5ce7ad81-1317-4dc0-b366-bee85f0d0221/log\r\n\r\nNone of them seem to be related to my patch (The errors don't even seem to be related to TFLite). Plus I find weird that Windows Bazel is failing while Windows Bazel GPU isn't. Shall I try to rebase?", "Hi, I can se that after a rebase only four tests have been launched (against the 9 from before if I recall correctly). Two of them passed and the other two are still \"Waiting for status to be reported\". Though I don't think \"Ubuntu CC\" has ever finished running even with past PRs. If there are any changes please keep me updated, thanks!", "@petewarden this is failing internal tests, can you please help merge this ?", "Ready to merge", "gentle ping @petewarden for review ?", "Changes are submitted internally ,closing this PR. Thank you "]}, {"number": 32588, "title": "tf.compat.v2.summary should be used in /guide/eager#summaries_and_tensorboard", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/eager#summaries_and_tensorboard\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWe should not recommend the usage of `tf.contrib.summary` in eager mode.\r\n`tf.compat.v2.summary` (or `tf.summary` in TF2) should be used in this section.\r\n\r\nWith `tf.contrib.summary`, we need to use `always_record_summaries()` or `record_summaries_every_n_global_steps()` (#32587) to record events even in eager mode. This is very unnatural and confusing.\r\n\r\nWhen I read this section, I thought `record_summaries_every_n_global_steps()` is optional and spent a lot of time to notice why `tf.contrib.summary.scalar` operations in my code was no-op.\r\n\r\nIn eager mode, I think we should recommend code like:\r\n\r\n```\r\nfor i in range(num_steps):\r\n  loss = train_one_step()\r\n  step = i + 1\r\n  if step % 100 == 0:\r\n    tf_summary.scalar('loss', loss, step=step)\r\n```\r\n\r\nwith `tf.compat.v2.summary`.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? \r\n\r\n**Yes**\r\n\r\nSee the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@yunabe `tf.contrib` won't be there is `TF2.0`. The current doc is for `TF1.x`. TF community is working on `TF2.0` docs and will be updated in the near future. If you want to contribute, please raise a PR to update the docs for `TF2.0`. Please let us know what you think?. Thanks! ", "I'm happy to contribute to the doc. I'm sending a PR for the section.", "I noticed the section about Tensorboard and summary was deleted in `master` branch by [this commit](https://github.com/tensorflow/docs/commit/b1658ccbdd86e19251c5fd58ce962e78437e95ff#diff-32b6b3f97f424a88c5360b84619e5257).\r\n\r\nDoes this means we will not explain about tf.summary in eager.md after TF2 launch?", "Anyway, I sent a PR to `master` branch: https://github.com/tensorflow/docs/pull/1044\r\nCould you take a look into it?", "According to the discussion of https://github.com/tensorflow/docs/pull/1044, it seems like that the live site won't be updated until TF2 launch.\r\n\r\nI'm not very happy about the fact that TensoFlow guide aren't updated for a while though many people are interested to learn TF2.0. But I understand TF team is busy for TF2 releases.\r\n\r\nI'm closing this bug because I added a tf.summary section to the new TF2 tutorial and we can not fix this issue on the site until TF2 release.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32588\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32588\">No</a>\n"]}]