[{"number": 53887, "title": "Add negative bound check for row and column pooling_sequence in Fract\u2026", "body": "\u2026ionalAvgPoolGrad op to avoid out of bound heap access\r\n\r\nPiperOrigin-RevId: 413837346\r\nChange-Id: I2b86034101df31bee161abcb781755e236c7bccd", "comments": []}, {"number": 53886, "title": "Add negative bound check for row and column pooling_sequence in Fract\u2026", "body": "\u2026ionalAvgPoolGrad op to avoid out of bound heap access\r\n\r\nPiperOrigin-RevId: 413837346\r\nChange-Id: I2b86034101df31bee161abcb781755e236c7bccd", "comments": []}, {"number": 53885, "title": "Fix Integer overflow error in Dequantize op shape function, by adding\u2026", "body": "\u2026 a bound check on axis.\r\n\r\nPiperOrigin-RevId: 412121389\r\nChange-Id: I3088dbad9e90f9998d406b618c16694388a9dfb4", "comments": []}, {"number": 53884, "title": "Fix Integer overflow error in Dequantize op shape function, by adding\u2026", "body": "\u2026 a bound check on axis.\r\n\r\nPiperOrigin-RevId: 412121389\r\nChange-Id: I3088dbad9e90f9998d406b618c16694388a9dfb4", "comments": []}, {"number": 53883, "title": "Fix Integer overflow error in Dequantize op shape function, by adding\u2026", "body": "\u2026 a bound check on axis.\r\n\r\nPiperOrigin-RevId: 412121389\r\nChange-Id: I3088dbad9e90f9998d406b618c16694388a9dfb4", "comments": []}, {"number": 53882, "title": "Fix out of bound access in DequantizeOp by adding check for axis < in\u2026", "body": "\u2026put dimension\r\n\r\nPiperOrigin-RevId: 411214268\r\nChange-Id: I3249d2a69ddc82f182c589a3a5bbfb71543f4b29", "comments": []}, {"number": 53881, "title": "Fix out of bound access in DequantizeOp by adding check for axis < in\u2026", "body": "\u2026put dimension\r\n\r\nPiperOrigin-RevId: 411214268\r\nChange-Id: I3249d2a69ddc82f182c589a3a5bbfb71543f4b29", "comments": []}, {"number": 53880, "title": "Fix out of bound access in DequantizeOp by adding check for axis < in\u2026", "body": "\u2026put dimension\r\n\r\nPiperOrigin-RevId: 411214268\r\nChange-Id: I3249d2a69ddc82f182c589a3a5bbfb71543f4b29", "comments": []}, {"number": 53879, "title": "Cause: 'arguments' object has no attribute 'posonlyargs'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 professional 19042.1466\r\n- TensorFlow installed from (source or binary):pip install \r\n- TensorFlow version (use command below):v2.7.0-rc1-69-gc256c071bb2 , 2.7.0\r\n- Python version:3.7\r\n- CUDA/cuDNN version: CUDA:11.4.0_471.11,cuDNN:8.3.2.44\r\n- GPU model and memory:GTX 970,4G memory\r\n\r\n**Describe the current behavior**\r\n\r\nI use SARSA Lambda mode in CPU mode, 10 rounds per second, and in GPU DQN mode, 1 round 100 seconds, GPU utilization is less than 1%\uff0c and the prompt:\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001FD428A85E8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```\r\nI don't know if the slow performance is related to this hint.\r\n\r\n**Describe the expected behavior**\r\nI expect performance not lower than SARSA Lambda mode.\r\n\r\n**Standalone code to reproduce the issue**\r\n[https://github.com/jaried/Tensorflow/blob/main/MountainCar.py](https://github.com/jaried/Tensorflow/blob/main/MountainCar.py)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nprof file:\r\n[https://github.com/jaried/Tensorflow/blob/main/question.prof](https://github.com/jaried/Tensorflow/blob/main/question.prof)\r\n", "comments": ["Hi @jaried ! Could you please check this [thread](https://stackoverflow.com/a/65804633/11530462) for answer?", "@mohantym Thanks for your answer. After I reinstalled cuda, cuDNN, and tensorflow, the prompt no longer appeared, but the performance was still very slow. I used snakeviz to analyze the prof file and found that the longest time-consuming functions are the learn() and decide() functions. The longest time-consuming part of these two functions is to call \\Lib\\site-packages\\keras\\utils\\traceback_utils, the 58-line error_handler() function  . It seems to have nothing to do with the above prompt. What should I do next?", "Hi @Saduf2019 ! Could you please look at this issue? Attaching Gist in [2.6](https://colab.sandbox.google.com/gist/mohantym/d44b199cf3563c3dafef154155c4dec3/github_53789.ipynb#scrollTo=H67mMC0N15_m) ,[2.7 ](https://colab.sandbox.google.com/gist/mohantym/1cf24f8c5afc040ad21ec23fcd81b9b7/github_53789.ipynb)and [2.8 ](https://colab.sandbox.google.com/gist/mohantym/639140f83e16b60f286f88cedce7c4b3/github_53789.ipynb#scrollTo=H67mMC0N15_m)for reference.", "Hi @Saduf2019 \r\nI used debug mode and found that the `self.target_net.predict()` function takes about 70ms to run once.\r\n````\r\n%timeit qs = self.evaluate_net.predict(observation[np.newaxis])\r\n75.6 ms \u00b1 1.16 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n````\r\nHowever, when I debug the `build_network()` function and use `model.summary()`, all parameters of a neural network are only 4547.\r\nIn other codes, I built a CNN network with 1.6M parameters and a total of 9 layers, and the time to run once was only 8ms.\r\nWhat is the reason why the neural network runs slowly once?\r\n````\r\nmodel.summary()\r\nModel: \"sequential_1\"\r\n___________________________________________________\r\n Layer (type) Output Shape Param #\r\n===================================================== ===============\r\n dense_3 (Dense) (None, 64) 192\r\n                                                                 \r\n dense_4 (Dense) (None, 64) 4160\r\n                                                                 \r\n dense_5 (Dense) (None, 3) 195\r\n                                                                 \r\n===================================================== ===============\r\nTotal params: 4,547\r\nTrainable params: 4,547\r\nNon-trainable params: 0\r\n___________________________________________________\r\n````\r\nThe difference between the two codes is that the code that needs to be optimized has the following hints. Does it have anything to do with this? I checked stackoverflow and it says that Tensorflow needs to be recompiled from source, my OS is windows 10 and my CPU is Xeon E5, how can I recompile Tensorflow?\r\n````\r\nI tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n````\r\nI did not find the appropriate compiler flags in \"Build from Source\" on the official website:\r\n[https://www.tensorflow.org/install/source_windows](https://www.tensorflow.org/install/source_windows)\r\n\r\nI uninstalled TensorFlow, and  according to the[ Intel\u00ae Optimization for TensorFlow* Installation Guide](https://www.intel.com/content/www/us/en/developer/articles/guide/optimization-for-tensorflow-installation-guide.html), after installing intel_tensorflow 2.7, it runs slower, and using `gpus = tf.config.experimental.list_physical_devices(device_type='GPU')`, the GPU cannot be found.\r\n\r\nI also tried `TF_ENABLE_ONEDNN_OPTS = 1` and it didn't work.", " I found the reason. thanks.", "> I found the reason. thanks.\r\n\r\nWhat was it?  I may be having a similar issue.\r\n", "This speed is normal. I compared model.call with model.fit, of course model.call is much faster."]}, {"number": 53878, "title": "Fix out of bound error in ReverseSequence Op shape function", "body": "PiperOrigin-RevId: 411896080\r\nChange-Id: I7e59a38e2f960886edf2b6c54ed5a84e86a9b193", "comments": []}, {"number": 53877, "title": "Fix out of bound error in ReverseSequence Op shape function", "body": "PiperOrigin-RevId: 411896080\r\nChange-Id: I7e59a38e2f960886edf2b6c54ed5a84e86a9b193", "comments": []}, {"number": 53876, "title": "Fix out of bound error in ReverseSequence Op shape function", "body": "PiperOrigin-RevId: 411896080\r\nChange-Id: I7e59a38e2f960886edf2b6c54ed5a84e86a9b193", "comments": []}, {"number": 53875, "title": "Internal change", "body": "PiperOrigin-RevId: 411896058\r\nChange-Id: Ia031058247e3cf382957a6662d3f9e1cbb481ca2", "comments": []}, {"number": 53874, "title": "Internal change", "body": "PiperOrigin-RevId: 411896058\r\nChange-Id: Ia031058247e3cf382957a6662d3f9e1cbb481ca2", "comments": []}, {"number": 53873, "title": "Internal change", "body": "PiperOrigin-RevId: 411896058\r\nChange-Id: Ia031058247e3cf382957a6662d3f9e1cbb481ca2", "comments": []}, {"number": 53872, "title": "Remove `setuptools` upper bound.", "body": null, "comments": []}, {"number": 53871, "title": "Count number of leaves in the ensemble gradient BoostedTree", "body": "This is an issue related to the performance of [TensorFlow.BoostedTree](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier)\r\n\r\n\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIs there any method that returns the number of entire leaves in the built ensemble?\r\nIf the feature already exists, would you please introduce it a bit more here, if not could we develop it?\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nUsers of the tensorflow\r\n\r\n", "comments": ["@samanemami Could you please refer to the [thread](https://github.com/scikit-learn/scikit-learn/discussions/22270#discussioncomment-2024510),[ BoostedTreesClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier) and let us know if it helps? ", "Thank you @sushreebarsa \r\n\r\nThe [thread](https://github.com/scikit-learn/scikit-learn/discussions/22270#discussioncomment-2024510) is related to Sklearn, and all methods are different in TensorFlow.\r\n\r\nRegarding the [BoostedTreesClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier), I could not find any related method!", "I already know how to return leaves in `Sklearn`, `LightGBM`, or `XGBoost`.\r\nBut with the `tf`, I could not find any method!", "`BoostedTreesClassifier` is depreciated, you can refer to [TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests) and for contribution you can check in the github repo `tensorflow/decision-forests` [here](https://github.com/tensorflow/decision-forests). "]}, {"number": 53868, "title": "Set Env Variable to override Setuptools new behavior", "body": "PiperOrigin-RevId: 423468055\r\nChange-Id: I5b148103e1372a5eb73570bc77face27dbd5f914", "comments": []}, {"number": 53867, "title": "[oneDNN] Prevent errors due to conv2d + fbnv2/v3 fusion for bf16", "body": null, "comments": []}, {"number": 53866, "title": "Node is not unique when frozen graph using convert_variables_to_constants_v2()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Windows 10/11 and Ubutun 20\r\n- TensorFlow installed from pip\r\n- tf 2.5.0/2.7.0/2.9nightly\r\n- Python 3.6.9/3.8\r\n- NA\r\n- NA:\r\n- NA:\r\n\r\n**Describe the current behavior**\r\n\r\nFrom google/deeplab2/export_model.py, add the following line:\r\n```\r\n  signatures = module.__call__.get_concrete_function(module.get_input_spec())\r\n  #following is the added code.\r\n  frozen_func = convert_variables_to_constants_v2(signatures)\r\n```\r\nThis issue also appears in tf2onnx tool which uses this convert_variables_to_constants_v2() call.\r\n\r\nFull command to run the above script.\r\n```\r\npython export.py\r\n--experiment_option_path\r\n\"./configs/cityscapes/axial_deeplab/max_deeplab_l_backbone_os16.textproto\"\r\n--checkpoint_path\r\n\"C:/develop/max_deeplab_l_backbone_os16_axial_deeplab_cityscapes_trainfine_saved_model/\"\r\n```\r\n\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\nno. I have no profound knowledge in this conversion, and it may take too much time for me to look into it.\r\n\r\n**Standalone code to reproduce the issue**\r\n[export.py](https://github.com/google-research/deeplab2/blob/main/export_model.py).\r\nCan get the save model from the following page, I used MaX-DeepLab-L-Backbone\r\nhttps://github.com/google-research/deeplab2/blob/main/g3doc/projects/axial_deeplab.md\r\n\r\n**Other info / logs**\r\n\r\n> \r\n> 2022-01-21 17:28:22.472844: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r\n> 2022-01-21 17:28:22.473118: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n> Skipping registering GPU devices...\r\n> 2022-01-21 17:28:33.104143: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> I0121 17:28:33.102456 21356 deeplab.py:57] Synchronized Batchnorm is used.\r\n> I0121 17:28:33.110966 21356 axial_resnet_instances.py:144] Axial-ResNet final config: {'num_blocks': [3, 6, 3, 3], 'backbone_layer_multiplier': 1.0, 'width_multiplier': 1.0, 'stem_width_multiplier': 1.0, 'output_stride': 16, 'classification_mode': True, 'backbone_type': 'wider_resnet', 'use_axial_beyond_stride': 16, 'backbone_use_transformer_beyond_stride': 0, 'extra_decoder_use_transformer_beyond_stride': 16, 'backbone_decoder_num_stacks': 1, 'backbone_decoder_blocks_per_stage': 1, 'extra_decoder_num_stacks': 1, 'extra_decoder_blocks_per_stage': 3, 'max_num_mask_slots': 128, 'num_mask_slots': 128, 'memory_channels': 512, 'base_transformer_expansion': 2.0, 'global_feed_forward_network_channels': 512, 'high_resolution_output_stride': 4, 'activation': 'relu', 'block_group_config': {'attention_bottleneck_expansion': 4, 'drop_path_keep_prob': 1.0, 'drop_path_beyond_stride': 4, 'drop_path_schedule': 'linear', 'positional_encoding_type': None, 'use_global_beyond_stride': 0, 'use_sac_beyond_stride': -1, 'use_squeeze_and_excite': False, 'conv_use_recompute_grad': False, 'axial_use_recompute_grad': True, 'recompute_within_stride': 0, 'transformer_use_recompute_grad': False, 'axial_layer_config': {'query_shape': (129, 129), 'key_expansion': 2, 'value_expansion': 4, 'memory_flange': (32, 32), 'double_global_attention': False, 'num_heads': 8, 'use_query_rpe_similarity': True, 'use_key_rpe_similarity': True, 'use_content_similarity': True, 'retrieve_value_rpe': True, 'retrieve_value_content': True, 'initialization_std_for_query_key_rpe': 1.0, 'initialization_std_for_value_rpe': 1.0, 'self_attention_activation': 'softmax'}, 'dual_path_transformer_layer_config': {'num_heads': 8, 'bottleneck_expansion': 2, 'key_expansion': 1, 'value_expansion': 2, 'feed_forward_network_channels': 2048, 'use_memory_self_attention': True, 'use_pixel2memory_feedback_attention': True, 'transformer_activation': 'softmax'}}, 'bn_layer': functools.partial(<class 'keras.layers.normalization.batch_normalization.SyncBatchNormalization'>, momentum=0.9900000095367432, epsilon=0.0010000000474974513), 'conv_kernel_weight_decay': 0.0}\r\n> I0121 17:28:33.613099 21356 deeplab.py:96] Setting pooling size to (65, 129)\r\n> I0121 17:28:36.232671 21356 api.py:446] Eval with scales ListWrapper([1.0])\r\n> I0121 17:28:39.102459 21356 api.py:446] Eval scale 1.0; setting pooling size to [None, None]\r\n> I0121 17:28:58.836912 21356 api.py:446] Eval with scales ListWrapper([1.0])\r\n> I0121 17:28:58.883746 21356 api.py:446] Eval scale 1.0; setting pooling size to [None, None]\r\n> 2022-01-21 17:40:38.838944: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n> 2022-01-21 17:40:38.847874: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\r\n> 2022-01-21 17:40:38.883561: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n> Skipping registering GPU devices...\r\n> 2022-01-21 17:40:40.300834: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\r\n>   function_optimizer: Graph size after: 4654 nodes (1206), 6100 edges (1886), time = 239.541ms.\r\n>   function_optimizer: function_optimizer did nothing. time = 2.73ms.\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 497, in _import_graph_def_internal\r\n>     graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'DeepLab/max_deeplab_l_backbone/stage4/block2/attention/height_axis/query_rpe/Gather/axis' is not unique\r\n> During handling of the above exception, another exception occurred:\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\absl\\app.py\", line 303, in run\r\n>     _run_main(main, args)\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n>     sys.exit(main(argv))\r\n>   File \"C:/develop/deeplab2/export_model.py\", line 158, in main\r\n>     frozen_func = convert_variables_to_constants_v2(signatures)\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py\", line 1154, in convert_variables_to_constants_v2\r\n>     converted_input_indices)\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py\", line 1080, in _construct_concrete_function\r\n>     new_output_names)\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py\", line 650, in function_from_graph_def\r\n>     wrapped_import = wrap_function(_imports_graph_def, [])\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py\", line 628, in wrap_function\r\n>     collections={}),\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 1007, in func_graph_from_py_func\r\n>     func_outputs = python_func(*func_args, **func_kwargs)\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py\", line 87, in __call__\r\n>     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py\", line 93, in wrapped\r\n>     return fn(*args, **kwargs)\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py\", line 648, in _imports_graph_def\r\n>     importer.import_graph_def(graph_def, name=\"\")\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 549, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 405, in import_graph_def\r\n>     producer_op_list=producer_op_list)\r\n>   File \"C:\\Users\\tfan\\.conda\\envs\\XNNC\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 501, in _import_graph_def_internal\r\n>     raise ValueError(str(e))\r\n> ValueError: Node 'DeepLab/max_deeplab_l_backbone/stage4/block2/attention/height_axis/query_rpe/Gather/axis' is not unique\r\n", "comments": ["@posEdgeOfLife \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "> \r\n\r\nhi, excuse me  for asking, i have already provided the code from deeplab2 and the code I added, but I don't have any snippet can run  independently. ", "@posEdgeOfLife Thank you for the update! \r\nCould you please try on the latest TF v2.7.0 ,tf-nightly and let us know the outcome? Thanks!", "> @posEdgeOfLife Thank you for the update! Could you please try on the latest TF v2.7.0 ,tf-nightly and let us know the outcome? Thanks!\r\n\r\nactually I just tested a code snippet that would run independently as long as you have the model downloaded.\r\n ```\r\n model = tf.keras.models.load_model(r\"C:/develop/max_deeplab_l_backbone_os16_axial_deeplab_cityscapes_trainfine_saved_model/\")\r\n  # module = DeepLabModule(\r\n  #     config, _FLAGS_CKPT_PATH.value, _FLAGS_MERGE_WITH_TF_OP.value)\r\n\r\n\r\n  signatures = model.signatures['serving_default']\r\n  frozen_func = convert_variables_to_constants_v2(signatures)\r\n```\r\n", "> @posEdgeOfLife Thank you for the update! Could you please try on the latest TF v2.7.0 ,tf-nightly and let us know the outcome? Thanks!\r\n\r\nignore my previous msg.\r\ntried with 2.7 and same error.\r\nnew error log on 2.7:\r\n\r\n```\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 496, in _import_graph_def_internal\r\n    results = c_api.TF_GraphImportGraphDefWithResults(\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'StatefulPartitionedCall/DeepLab/max_deeplab_l_backbone/stage4/block2/attention/height_axis/query_rpe/Gather/axis' is not unique\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"export_model.py\", line 177, in <module>\r\n    app.run(main)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\r\n    _run_main(main, args)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"export_model.py\", line 158, in main\r\n    frozen_func = convert_variables_to_constants_v2(signatures)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1083, in convert_variables_to_constants_v2\r\n    return _construct_concrete_function(func, output_graph_def,\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1008, in _construct_concrete_function\r\n    new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 650, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 621, in wrap_function\r\n    func_graph.func_graph_from_py_func(\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 87, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 93, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 648, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 535, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 400, in import_graph_def\r\n    return _import_graph_def_internal(\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 501, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Node 'StatefulPartitionedCall/DeepLab/max_deeplab_l_backbone/stage4/block2/attention/height_axis/query_rpe/Gather/axis' is not unique\r\n```\r\n", "tried it on 2.9 nightly which returns same error. Logs:\r\n\r\n```\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"export_model.py\", line 177, in <module>\r\n    app.run(main)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/absl/app.py\", line 312, in run\r\n    _run_main(main, args)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/absl/app.py\", line 258, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"export_model.py\", line 158, in main\r\n    frozen_func = convert_variables_to_constants_v2(signatures)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1151, in convert_variables_to_constants_v2\r\n    return _construct_concrete_function(func, output_graph_def,\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1076, in _construct_concrete_function\r\n    new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 655, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 619, in wrap_function\r\n    func_graph.func_graph_from_py_func(\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1161, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 83, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 89, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 649, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 548, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 403, in import_graph_def\r\n    return _import_graph_def_internal(\r\n  File \"/home/tfan/.local/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 504, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Node 'StatefulPartitionedCall/DeepLab/max_deeplab_l_backbone/stage4/block2/attention/height_axis/query_rpe/Gather/axis' is not unique\r\ntfan@PC-TFAN-DT:/mnt/c/develop/deeplab2$ python3\r\nPython 3.8.10 (default, Sep 28 2021, 16:10:42)\r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.9.0-dev20220121\r\n```", "@posEdgeOfLife Could you please post this issue in [models repo](https://github.com/tensorflow/models/issues) to get the right help there ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53866\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53866\">No</a>\n"]}, {"number": 53865, "title": "Tensorflow Dataset: Image too large", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary):  **binary**\r\n- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**\r\n- Python version: **3.8.9**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **11.2 / 8.1**\r\n- GPU model and memory: **Tested on RTX 3080 10 GB & V100 16 GB**\r\n\r\n**Describe the current behavior**\r\n\r\nHello, I am creating a TF Dataset from Gigapixel WSIs (whole slide images). The dimensions of them are 51968 x 37632 x 1 (grayscale) yielding a size of 1955659776. \r\n\r\n```python\r\nWSI_SHAPE = (51968, 37632)\r\ntrain_ds = keras.preprocessing.image_dataset_from_directory(wsi_dataset_path, validation_split=0.2, color_mode='grayscale',\r\n                                                                labels=None, shuffle=True, subset='training', image_size=WSI_SHAPE,\r\n                                                                batch_size=32, seed=42)\r\n\r\nval_ds = keras.preprocessing.image_dataset_from_directory(wsi_dataset_path, validation_split=0.2, color_mode='grayscale',\r\n                                                              labels=None, shuffle=True, subset='validation', image_size=WSI_SHAPE,\r\n                                                              batch_size=32, seed=42)\r\n\r\nfor image in train_ds.take(1):\r\n    print('Image input shape:', image.input_shape)\r\n```\r\n\r\nThis gives an error:\r\n```\r\n2022-01-21 20:30:25.456609: E tensorflow/core/lib/jpeg/jpeg_mem.cc:183] Image too large: 1955659776\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected behavior is for tensorflow to load in the large images into the Dataset.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): **No**\r\n- Briefly describe your candidate solution(if contributing): **N/A**\r\n\r\n**Other info / logs**\r\n\r\nI looked into the location of this error: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/jpeg/jpeg_mem.cc#L182-L186\r\n```cpp\r\nif (total_size >= (1LL << 29)) {\r\n  LOG(ERROR) << \"Image too large: \" << total_size;\r\n  jpeg_destroy_decompress(&cinfo);\r\n  return nullptr;\r\n}\r\n```\r\n\r\nThe size of my images (1955659776) is 3.6 times larger than the maximum size (536870912). I'm curious as to why it is this specifically this number as the maximum size since I have access to 256 GB of memory. For large images like this would I have to create the dataset manually instead of `image_dataset_from_directory`?", "comments": ["@riteshahlawat \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "@sushreebarsa Filled!", "@riteshahlawat \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "> @riteshahlawat In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nI have already provided a code snippet that loads in a dataset of images with an extremely large resolution, uploading the dataset would be an issue but any high-resolution image over size 536870912 (width x height x channels) would work.", "@riteshahlawat \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53865\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53865\">No</a>\n"]}, {"number": 53864, "title": "Add basic TensorRT support on Windows ", "body": "For some reason TF-TRT is disabled on Windows. But this feature would be really good to speed up inference (in my case, the difference was 5x). Since TensorRT is available on both Linux and Windows, it is very frustrating that TF-TRT only works in Linux.\r\n\r\nThis PR enables TRT ops on Windows.\r\n\r\nChanges:\r\n- Add TRT to configure.py on Windows\r\n- Enable TRT on Windows in python libraries\r\n- Fix loading of TensorRT's libraries\r\n- Fix TF-TRT to be compilable with MSVC\r\n- Fix build errors", "comments": ["Squashed commits and fixed pylint warning", "Seems like Windows build is failing when TRT is disabled, I did not test this scenario.\r\n`//tensorflow/python/compiler/tensorrt:init_py` brings `//tensorflow/compiler/tf2tensorrt:_pywrap_py_utils -> ... -> //tensorflow/stream_executor:platform` which need `//tensorflow/stream_executor:stream_executor_pimpl`. I added `//tensorflow/stream_executor:stream_executor` to `//tensorflow/compiler/tf2tensorrt:tensorrt_stub` to include `stream_executor_pimpl`, but when TRT is disabled, `tensorrt_stub` is omitted. Adding `//tensorflow/stream_executor:stream_executor` directly to `//tensorflow/compiler/tf2tensorrt:_pywrap_py_utils` should fix this", "After we get the servers pass the checking, we will need to get the input from NVIDIA @DEKHTIARJonathan for this.", "I fixed build problems and bad formatting", "@itmo153277 are you committing this on your personal time or for the account of your employer ?\r\nMerging something like this has numerous consequences, one of them being long term support. And we don't have resources to support Windows TF-TRT.", "@DEKHTIARJonathan On my personal time. I understand that full support might take too much resources. I thought just giving it \"experimental\" status for Windows would be ok even if it means that it might become unstable in the future, There seems to be no major problem if I just forcefully enable it like this, but without this everything is actively preventing even building it for Windows. This PR is just to give an option to build it for Windows anyway if they want to for their own risk, Otherwise, it sounds like TF-TRT just doesn't work on Windows, but it is not true", "This PR causes two tests in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack to fail:\r\nconv_2d_test.cc and depthwise_conv_2d_test.cc. This is currently blocking the merge.", "> This PR causes two tests in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack to fail: conv_2d_test.cc and depthwise_conv_2d_test.cc. This is currently blocking the merge.\r\n\r\n@akuegel Is there anywhere I can see the test logs? I can't see how any of the changes in this PR can cause these tests to fail", "> > This PR causes two tests in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack to fail: conv_2d_test.cc and depthwise_conv_2d_test.cc. This is currently blocking the merge.\r\n> \r\n> @akuegel Is there anywhere I can see the test logs? I can't see how any of the changes in this PR can cause these tests to fail\r\n\r\nMaybe you can run the tests yourself, hopefully it should be possible to reproduce the failure.\r\n\r\nIn any case, here is one example failure:\r\n\r\nConv2D.SparseWeights test:\r\nThe difference between default_output_data[index] and delegate_output_data[index] is 0.1500244140625, which exceeds std::abs(default_output_data[index]) * 3.0e-6f, where\r\ndefault_output_data[index] evaluates to 1148.1500244140625,\r\ndelegate_output_data[index] evaluates to 1148, and\r\nstd::abs(default_output_data[index]) * 3.0e-6f evaluates to 0.0034444502089172602.\r\nbatch 0 / 2, y position 0 / 8, x position 0 / 7, channel 0 / 3\r\n\r\nAs far as I can tell, all tests fail due to precision issues. Maybe this is to be expected? In that case, there needs to be a discussion whether we are ok with increasing the error tolerances of these tests.", "This seems unrelated to this PR. I will try to reproduce it on my end and see what I can do.", "> This seems unrelated to this PR. I will try to reproduce it on my end and see what I can do.\r\n\r\nYes, it is possible this is unrelated. Just wanted to let you know what is currently blocking the merge. If it turns out the tests pass again at a later time, the PR will be merged.", "@akuegel (cc @bixia1 )\r\nI could not reproduce the test failure for `tensorflow/lite/delegates/xnnpack/conv2d_test`. I ran the test ~300 000 times inside `tensorflow/tensorflow:devel-gpu` docker but never got a failure.\r\n\r\nThe test cases that were failing are generated randomly (both op parameters and input data). Seeds for random generators are not logged. It is unlikely that I will be able to reproduce it. It is also possible that the failure is environment-dependent.\r\n\r\nThe tests can certainly be improved (e.g. make them deterministic or at least log the seed so that a failure can be reproduced). However, I think this goes beyond the scope of this PR. If you want, I can open an issue for this.\r\n\r\n@bixia1 What should I do now? Can this PR be merged?", "The failure is likely not relevant and I am working on \"manually merging\" this PR.", "@itmo153277 (cc @bixia1)\r\nSome Windows builds are failing with the following error:\r\n```\r\nLinking tensorflow/compiler/tf2tensorrt/_pywrap_py_utils.so failed: (Exit 1189): link.exe failed: error executing command \r\n...\r\nLINK : fatal error LNK1189: library limit of 65535 objects exceeded\r\n```\r\nIt seems the error is caused by the addition of [`//tensorflow/stream_executor`](https://github.com/tensorflow/tensorflow/blob/f3c5e2d4561e7a5a6887b80df24ccfe518a7689f/tensorflow/compiler/tf2tensorrt/BUILD#L1004) to the dependencies of [`_pywrap_py_utils`](https://github.com/tensorflow/tensorflow/blob/f3c5e2d4561e7a5a6887b80df24ccfe518a7689f/tensorflow/compiler/tf2tensorrt/BUILD#L957). Would there be a smaller dependency that can replace it?", "@learning-to-play Iirc the only actual object file needed for linkage comes from `//tensorflow/stream_executor:stream_executor_pimpl`."]}, {"number": 53863, "title": "Add missing Windows py3.10 files", "body": null, "comments": []}, {"number": 53862, "title": "`tf.math.accumulate_n` cannot handle a single tensor as input", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ninputs = tf.constant([0, 0, 0, 0, 0], dtype=tf.int32)  \r\nadd_n_res = tf.math.add_n(inputs)\r\nprint(\"add_n_res: \", add_n_res) # add_n_res:  tf.Tensor(0, shape=(), dtype=int32)\r\naccumulate_n_res = tf.math.accumulate_n(inputs,) # ERROR:The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\nprint(\"accumulate_n_res: \", accumulate_n_res) # Won't get here\r\n```\r\n\r\n**Expected output**\r\nAccording to the document of [`tf.math.accumulate_n`](https://www.tensorflow.org/api_docs/python/tf/math/accumulate_n?hl=en), `accumulate_n` performs the same operation as `tf.math.add_n`. However, `accumulate_n` throws an error when `inputs` is one single tensor, while `tf.math.add_n` supports such inputs.", "comments": ["@ArrowIntoTheSky  \r\nGiving 'inputs' as '[inputs]' as a parameter solved this. \r\ntf.accumulate_n([inputs]) worked for me.", "Hi @ArrowIntoTheSky ! Can you refer above [comment](https://github.com/tensorflow/tensorflow/issues/53862#issuecomment-1019510400)?As per this [document](https://www.tensorflow.org/api_docs/python/tf/math/accumulate_n#args), Inputs should be a list of tensors. Attaching resolved [gist](https://colab.sandbox.google.com/gist/mohantym/b45935092855ad5fbf164e20c2749975/github_53862.ipynb) for reference. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53862\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53862\">No</a>\n", "> Giving 'inputs' as '[inputs]' as a parameter solved this.\r\n> tf.accumulate_n([inputs]) worked for me.\r\n\r\nThanks @devnev39 and @mohantym for your comment! \r\nI am just curious:\r\nAs per this [tf.math.accumulate_n document](https://www.tensorflow.org/api_docs/python/tf/math/accumulate_n#args) and [tf.math.add_n document](https://www.tensorflow.org/api_docs/python/tf/math/add_n?#args), for **both** APIs, `inputs` should be **a list of tensors**. However, `add_n` **supports** a single tensor as inputs, but `accumulate_n` **does not support**.\r\n\r\nAlso, the document [tf.math.add_n](https://www.tensorflow.org/api_docs/python/tf/math/add_n) **explicitly claims**:\r\n>  [tf.math.add_n](https://www.tensorflow.org/api_docs/python/tf/math/add_n)  performs the same operation as [tf.math.accumulate_n](https://www.tensorflow.org/api_docs/python/tf/math/accumulate_n). \r\n\r\nIf they perform  **the same** operation, they should both accept single tensor or stricly only accept a list of tensors."]}, {"number": 53861, "title": "Remove the upper bound on setuptools.", "body": "It seems that our builds are not really determinstic since they now pick\r\nsetuptools > 60 in some places and fail. Hence, we remove this bound and\r\nhope that they would pass.", "comments": []}, {"number": 53860, "title": "[oneDNN] Fixing node_file_writer_test", "body": "This PR fixes a failure in //tensorflow/python/framework:node_file_writer_test exposed after eager_op_as_function [feature ](https://github.com/tensorflow/tensorflow/commit/586f4942c50833dad0714adef091a83445c5350d) was enabled by default.", "comments": ["@qqfish can you please have a look at this PR, it is a small one?"]}, {"number": 53859, "title": "TOSA legalization: Ensure fused activation clamp value within range", "body": "It is possible to specify a fused activation function for\r\na convolution where the output quantization range does not contain the\r\nedges of the activation. (6 for RELU6, -1 or 1 for RELU_N1_TO_1)\r\nWhen translating to TOSA, ensure that the maximum CLAMP value is valid\r\nfor the given quantization data type.\r\n", "comments": []}, {"number": 53858, "title": "normalization should be done using moving averages?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series#normalize_the_data\r\n\r\n## Description of issue (what needs changing):\r\n\r\nHello this is a small feature request. In the Time series tutorial at the normalization section in the 3rd paragraph there is a remark saying:\r\n\"and that this normalization should be done using moving averages.\"\r\n\r\nI am very curious on how a moving average would be used here. \r\nShould  a \"simple moving average\" for the whole dataset be computed and then on that new \"dataset-MA\" just use the normalization technique that is described in the tutorial?\r\n\r\nThe feature I am requesting is something like a remark on how this would be done or maybe a pointer to different tutorial if one exists or some further references/research users could look into.", "comments": ["@t0b4cc0 \r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thanks!", "I created the issue with the relevant parts of that template.", "did the tutorial just mean to use something like pandas.rolling function to create the data?", "The tutorial uses the simple average to normalize the data, the intention of the tutorial is to create a Time series forecasting using Tensorflow. \r\nIf you feel any of the description is irrelevant/inappropriate feel free to create a PR. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53857, "title": "Tensorflow Setup, pin protobuf <= 3.19.1", "body": "PiperOrigin-RevId: 423349190\r\nChange-Id: I40a902c188ad39023dce18c9c062f063d943ccad\r\n\r\nRunning Test Build to confirm that this works.", "comments": ["This actually doesn't fix the protobuf issue on Mac"]}, {"number": 53856, "title": "Expose half_pixel_center or anti-aliasing parameter in the keras resizing layer.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.6\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe feature that I am proposing is basically a fix for the current resizing keras layer which sets the `half_pixel_center` to true by default which causes problem on nnapi (android) based platforms and also on snapdragon based platforms. There are 2 possible solution to this problem - \r\n\r\n1. You expose the parameters `antialias` or `half_pixel_center` via the keras Resizing layer and let the user explicitly set either of the 2 properties. Better would be to expose `antialias` which can be defaulted to `None` or `True` since resizing is used mostly in later layers and not for downsampling it should be fine.\r\n2.  Add an automated check within the resize base function to check if the half_pixel_center is actually needed. From my understanding when you are resizing just check if the (inputsize -1)/(outputsize-1) when downsampling or vice versa when upsampling, is fractional or int. If fractional set half_pixel_center to true else set it to false. \r\n\r\n\r\n**Will this change the current api? How?**\r\nYes this will change the current api by either exposing certain optional parameters or by adding an automated check.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone running their NN on somekind of DSP.\r\n\r\n**Any Other info.**\r\n", "comments": ["@bayesian-mind \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]