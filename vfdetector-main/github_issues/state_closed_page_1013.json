[{"number": 22960, "title": "Tensorflow Compile error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Master\r\n- **Python version**: Python 3.6.7rc1\r\n- **Bazel version (if compiling from source)**: Build label: 0.17.2\r\nBuild target: bazel-out/x64_windows-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Sep 21 10:31:06 2018 (1537525866)\r\nBuild timestamp: 1537525866\r\nBuild timestamp as int: 1537525866\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 10.0 / 7.3.1\r\n- **GPU model and memory**: GeForce 1050\r\n- **Exact command to reproduce**:\r\nINFO: From Linking tensorflow/core/liblib_internal_impl.a:\r\nandroid_armv7a_cpu_utils_helper.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nrandom_distributions.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nERROR: D:/tensorflow/tensorflow/core/kernels/BUILD:4420:1: C++ compilation of rule '//tensorflow/core/kernels:scatter_nd_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/sirto/_bazel_sirto/26orbg4z/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=D:/Program Files/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=D:/Program Files/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\sirto\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\sirto\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op_gpu/scatter_nd_op_gpu.cu.o /c tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc\r\nc:\\users\\sirto\\_bazel_sirto\\26orbg4z\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/arch/CUDA/Half.h(212): error: more than one instance of overloaded function \"__hadd\" matches the argument list:\r\n            function \"__hadd(int, int)\"\r\n            function \"__hadd(__half, __half)\"\r\n            argument types are: (const Eigen::half, const Eigen::half)\r\n\r\n1 error detected in the compilation of \"C:/Users/sirto/AppData/Local/Temp/nvcc_inter_files_tmp_dir/scatter_nd_op_gpu.cu.cpp1.ii\".\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: D:/tensorflow/tensorflow/tools/pip_package/BUILD:124:1 C++ compilation of rule '//tensorflow/core/kernels:scatter_nd_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/sirto/_bazel_sirto/26orbg4z/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=D:/Program Files/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=D:/Program Files/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\sirto\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\sirto\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op_gpu/scatter_nd_op_gpu.cu.o /c tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc\r\nINFO: Elapsed time: 653.206s, Critical Path: 181.89s\r\nINFO: 2220 processes: 2220 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["I have been working on trying to compile this all day. I keep searching for fixes and use them then another error comes up. This one I was unable to find a fix. Maybe I just missed it, any help is welcomed. Thank you.", "Your actual compiler error is:\r\n```\r\nmore than one instance of overloaded function \"__hadd\" matches the argument list\r\n```\r\nThis is a duplicate of https://github.com/tensorflow/tensorflow/issues/19198"]}, {"number": 22959, "title": "improve Unified Memory performance", "body": "memory oversubscription", "comments": ["No new features on versions that have been released.\r\n\r\nPlease re-open a PR against the master branch. Thanks!", "Closing. As mention, please reopen or create a new PR against master branch"]}, {"number": 22958, "title": "Move stateless ops out of contrib", "body": "Phase two!", "comments": ["`//tensorflow/tools/api/tests:api_compatibility_test` still passes for me (on Mac with Python 3), and `--update_goldens` does nothing.  Someone else will have to run it for me.", "`//tensorflow/tools/api/tests:api_compatibility_test` should be run with Python 2 only. The test is set to just be skipped when it's invoked with Python 3.", "@martinwicke said it should work with Python 3 now, but indeed it's still skipped in that case.  Should that be fixed?", "I guess I was wrong. @annarev will know for certain. It's possible that the py2/py3 differences in the system libraries are too large to make it feasible to maintain this in the same files. We may have to hard-switch at some point.", "@girving you need to run the `api_compatibility_test` with py2 or update the goldens manually:\r\n ```\r\nIssue 1\t: 'path: \"tensorflow.random\"\\ntf_module {\\n  member_method {\\n    name: \"gamma\"\\n  [truncated]... != 'path: \"tensorflow.random\"\\ntf_module {\\n  member_method {\\n    name: \"gamma\"\\n  [truncated]...\r\n  path: \"tensorflow.random\"\r\n  tf_module {\r\n    member_method {\r\n      name: \"gamma\"\r\n      argspec: \"args=[\\'shape\\', \\'alpha\\', \\'beta\\', \\'dtype\\', \\'seed\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\\"<dtype: \\'float32\\'>\\\", \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"get_seed\"\r\n      argspec: \"args=[\\'op_seed\\'], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n    member_method {\r\n      name: \"log_uniform_candidate_sampler\"\r\n      argspec: \"args=[\\'true_classes\\', \\'num_true\\', \\'num_sampled\\', \\'unique\\', \\'range_max\\', \\'seed\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"multinomial\"\r\n      argspec: \"args=[\\'logits\\', \\'num_samples\\', \\'seed\\', \\'name\\', \\'output_dtype\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"normal\"\r\n      argspec: \"args=[\\'shape\\', \\'mean\\', \\'stddev\\', \\'dtype\\', \\'seed\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0.0\\', \\'1.0\\', \\\"<dtype: \\'float32\\'>\\\", \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"poisson\"\r\n      argspec: \"args=[\\'lam\\', \\'shape\\', \\'dtype\\', \\'seed\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\\"<dtype: \\'float32\\'>\\\", \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"set_random_seed\"\r\n      argspec: \"args=[\\'seed\\'], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n    member_method {\r\n      name: \"shuffle\"\r\n      argspec: \"args=[\\'value\\', \\'seed\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n+     name: \"stateless_multinomial\"\r\n+     argspec: \"args=[\\'logits\\', \\'num_samples\\', \\'seed\\', \\'output_dtype\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\\"<dtype: \\'int64\\'>\\\", \\'None\\'], \"\r\n+   }\r\n+   member_method {\r\n+     name: \"stateless_normal\"\r\n+     argspec: \"args=[\\'shape\\', \\'seed\\', \\'mean\\', \\'stddev\\', \\'dtype\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0.0\\', \\'1.0\\', \\\"<dtype: \\'float32\\'>\\\", \\'None\\'], \"\r\n+   }\r\n+   member_method {\r\n+     name: \"stateless_truncated_normal\"\r\n+     argspec: \"args=[\\'shape\\', \\'seed\\', \\'mean\\', \\'stddev\\', \\'dtype\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0.0\\', \\'1.0\\', \\\"<dtype: \\'float32\\'>\\\", \\'None\\'], \"\r\n+   }\r\n+   member_method {\r\n+     name: \"stateless_uniform\"\r\n+     argspec: \"args=[\\'shape\\', \\'seed\\', \\'minval\\', \\'maxval\\', \\'dtype\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0\\', \\'None\\', \\\"<dtype: \\'float32\\'>\\\", \\'None\\'], \"\r\n+   }\r\n+   member_method {\r\n      name: \"truncated_normal\"\r\n      argspec: \"args=[\\'shape\\', \\'mean\\', \\'stddev\\', \\'dtype\\', \\'seed\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0.0\\', \\'1.0\\', \\\"<dtype: \\'float32\\'>\\\", \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"uniform\"\r\n      argspec: \"args=[\\'shape\\', \\'minval\\', \\'maxval\\', \\'dtype\\', \\'seed\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'0\\', \\'None\\', \\\"<dtype: \\'float32\\'>\\\", \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"uniform_candidate_sampler\"\r\n      argspec: \"args=[\\'true_classes\\', \\'num_true\\', \\'num_sampled\\', \\'unique\\', \\'range_max\\', \\'seed\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'None\\'], \"\r\n    }\r\n  }\r\n```", "@drpngx I applied your patch to what are hopefully the correct files:\r\n\r\n```\r\n tensorflow/tools/api/golden/v1/tensorflow.random.pbtxt          |  16 +++\r\n tensorflow/tools/api/golden/v2/tensorflow.random.pbtxt          |  16 +++\r\n```", "Looks like I fixed the api compatibility tests, but now there's a copybara failing test with Details pointing to http://cl/217201990 (which doesn't work for me, as much as I miss that tool).", "@girving and @martinwicke api_compatibility_test still doesn't work with Python 3. I tried to run it with Python 3. It has a lot of differences and I didn't get a chance to go through all of them yet. sorry about the trouble!", "Thanks all!  That was substantially smoother than last time.\r\n\r\n@hawkinsp I don't think I'll have time to do the related TPU changes, so I'll leave that part to someone else.  Specifically that means (1) turn on and test `bfloat16` and (2) implement the integer case of `tf.random.stateless_uniform`."]}, {"number": 22957, "title": "Freezing network with batch norm does not work with TRT", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.10.1\r\n- **Python version**:\r\n3.5\r\n- **CUDA/cuDNN version**:\r\n9.0\r\n- **Bazel version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Mobile device**:\r\nN/A\r\n- **Exact command to reproduce**:\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\nimport numpy as np\r\nimport tensorflow.contrib.tensorrt as trt\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\nfrom tensorflow.keras import backend as K\r\n\r\npath         = \"/tmp\"\r\noutput_trt_pb    = os.path.join(path, \"output_trt.pb\")\r\n\r\nnp.random.seed(0)\r\n\r\nX, Y = np.random.rand(1000, 100, 100, 3), np.random.rand(1000, 100, 100, 16)\r\n\r\nwith K.get_session() as sess:\r\n    \r\n    inp = tf.keras.layers.Input(shape=(100,100,3), name=\"input\")\r\n    x = tf.keras.layers.Conv2D(16, (3,3), padding=\"same\", kernel_initializer=\"ones\", name=\"conv2d\", use_bias=False)(inp)\r\n    x = tf.keras.layers.BatchNormalization(name=\"bn\", fused=True)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    model = tf.keras.models.Model(inp, x)\r\n    model.compile(\"adam\", \"mse\")\r\n    model.fit(X, Y, epochs=3, verbose=True)\r\n    \r\n    # fix nodes (from https://github.com/tensorflow/tensorflow/issues/3628) here doesn't help\r\n    graph_def = sess.graph_def\r\n    \r\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n        sess,                                     # The session is used to retrieve the weights\r\n        graph_def,                                # The graph_def is used to retrieve the nodes\r\n        [i.name[:-2] for i in model.outputs]      # The output node names are used to select the useful nodes\r\n    )\r\n    \r\n    trt_graph = trt.create_inference_graph(output_graph_def, \r\n                                           [i.name[:-2] for i in model.outputs],\r\n                                           max_batch_size=1,\r\n                                           max_workspace_size_bytes= 1256 << 20,\r\n                                           precision_mode=\"FP32\") \r\n```\r\n\r\n### Describe the problem\r\n\r\nWhen I freeze a protobuf that contains batch normalization and then try to use it with TRT, it fails with the error \r\n\r\n`InvalidArgumentError: Input 0 of node bn/cond/ReadVariableOp/Switch_1 was passed float from bn/gamma_1:0 incompatible with expected resource.`\r\n\r\nIt seems like this is an issue in other threads, like https://github.com/tensorflow/tensorflow/issues/3628 and in my code I tried to include the suggested fixes, but this does not help.\r\nI tried using both fused=True and fused=False, and also tried trainable=False/True in BatchNormalization.\r\nIf you comment out\r\n`x = tf.keras.layers.BatchNormalization(name=\"bn\", fused=True)(x)`\r\nthen everything works fine.\r\n\r\nAny comment would be appreciated.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nGPU model and memory\nMobile device", "@samikama can you reproduce this?", "Can you also do remove training nodes?", "Yes,\r\n\r\nBut the error still looks like this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 39, in <module>\r\n    precision_mode=\"FP32\") \r\n  File \"/home/ferronfr/.local/lib/python3.5/site-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 153, in create_inference_graph\r\n    int(msg[0]))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node bn/cond/ReadVariableOp/Switch was passed float from bn/gamma:0 incompatible with expected resource.\r\n```\r\n\r\n I also tried to set K.set_learning_phase(0) but get an error like:\r\n\r\n```\r\n2018-10-30 00:47:18.401715: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-10-30 00:47:18.403717: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-10-30 00:47:18.406910: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n2018-10-30 00:47:18.535147: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\r\n2018-10-30 00:47:18.535494: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\r\n2018-10-30 00:47:18.535555: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:857] Engine creation for segment 0, composed of 7 nodes failed: Internal: Failed to build TensorRT engine. Skipping...\r\n```", "I get similar error when I try to import a `GraphDef` for my frozen model using `tf.import_graph_def`.\r\n\r\n```\r\nInput 0 of node instance_normalization/Gather was passed float from sequential/norm_conv2d/gamma:0 incompatible with expected resource\r\n```\r\n\r\nI've searched the error in the TF sources and I only found one place which can match this log message at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/graph_constructor.cc#L1196:\r\n```\r\nreturn errors::InvalidArgument(\r\n        \"Input \", input_index, \" of node \", dst->name(), \" was passed \",\r\n        DataTypeString(src_out), \" from \", src->name(), \":\", output_index,\r\n        \" incompatible with expected \", DataTypeString(dst_in), \".\");\r\n```\r\n\r\nSo at first it looks like a very generic message but it actually seems like `resource` is just another data type here?", "Ok, so I could fix my case by replacing `tf.gather` from my code with `tf.python.gather_v2`. I think that the former is using Resource Variable instead of Variables. I don't wrap my head around those concepts yet. Unfortunately my solution doesn't seem related to this issue.\r\n\r\nStrange enough I didn't get this error until I frozen my graph. So I can save and load the regular training graph just fine, but I can't load the frozen graph. I believe that treating a constant as resource variable is causing this error.", "@fferroni   I have meet the same problem in BN, do you have a method to solve it?", "Any update of this issue? I also encounter a similar issue.", "Facing similar issue while benchmarking my custom model with tensorflow benchmark tool:-\r\n\r\n`E tensorflow/tools/benchmark/benchmark_model.cc:275] Could not create TensorFlow Session: Invalid argument: Input 0 of node MobilenetV2/expanded_conv/project/bn/cond/ReadVariableOp/Switch was passed float from MobilenetV2/expanded_conv/project/bn/gamma:0 incompatible with expected resource.`\r\n", "I solved this similar issue, in my case, I added `tf.keras.backend.set_learning_phase(0)`. ", "as @ardianumam mentions, you need to set your keras to inference mode *before* loading or constructing your model. TFTRT works with frozen models but unless keras learning phase it set to inference mode graph partially stays in training mode. @fferroni as logs mention, your device is running out of memory. @anilsathyan7 @pkubik are these errors happen when you try to accelerate your models with TFTRT or when you load your keras models. If they are not happening on the graphs produced by TFTRT conversion please open a separate issue.", "@trevor-m @pooyadavoodi  Can you PTAL", "Closing this", "@fferroni Were you able to solve the issue? I don't see that setting tf.keras.backend.set_learning_phase(0) works because it actually don't generate the batch_norm variables at all, but if the code expects it while loading the model, it raises another issue \"No Operation named batch_normalization/x in the Graph. \r\n\r\nAny suggestions on how to resolve the issue?", "@samikama This is not a solution as `tf.keras.backend.set_learning_phase(0) ` puts the batch norm layers in inference mode and thus the will not update during training\r\n@bhavana3 @ardianumam @fferroni A workaround is to save the weights of the trained model, clear the session, recreate the model this time with `tf.keras.backend.set_learning_phase(0)` and then load the weights back in before freezing. https://github.com/tensorflow/tensorflow/issues/31331#issuecomment-518655879", "Don't forget to clear the session before setting the learning phase to 0 (which puts keras in inference mode, so the batch norm won't be updated)!\r\n\r\n```\r\nfrom keras import backend as K\r\n\r\nK.clear_session()\r\nK.set_learning_phase(0)\r\n\r\nwith K.get_session() as sess:\r\n    ...\r\n```"]}, {"number": 22956, "title": "r1.12-rc1 cherry-pick request: Explicitly set jdk8 in ci_parameterized_build.sh", "body": "PiperOrigin-RevId: 216946217", "comments": []}, {"number": 22955, "title": "How to convert Tensorrt Optimized graph to Tensorrt engine", "body": "I need to convert my tensorflow model to tensorrt engine. However, since there are some unsupported layers in the model, I decided to use create_inference_graph to skip those operations. Although it works well with Python, I need to write it to *.engine file so that I can read it in C++. I tried to convert it to uff model using uff_model_from_tensorflow_frozen_model, but got key error. Is there a way I can convert the optimized graph to an engine? Any suggestion would be appreciated.\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution : Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary) : source\r\n- **TensorFlow version 1.7 ~ 1.11\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: CUDA 8.0, 9.0; CUDNN 7.0, 7.3\r\n- Have I written custom code : No\r\n- Bazel version : NA\r\n- GPU model and memory : NA\r\n- Mobile device : NA\r\n- **Exact command to reproduce**:\r\n```\r\ndef load_graph(file):\r\n    with tf.gfile.GFile(file, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graph_def)\r\n    return graph, graph_def\r\n\r\ngraph = load_graph('tensorflow_model.pb')\r\ntensorrt_graph = trt.create_inference_graph(graph_def, outputs=['output_node'], max_batch_size=1, precision_mode='FP32', max_workspace_size_bytes=1<<33)\r\n\r\nwith tf.gfile.GFile('tensorrt_model.pb', 'wb') as f:\r\n    f.write(tensorrt_graph.SerializeToString())\r\n\r\nloaded_tensorrt_graph, loaded_tensorrt_graph_def = load_graph('tensorrt_model.pb')\r\nuff_model = uff.from_tensorflow_frozen_model(loaded_tensorrt_graph_def)\r\n```\r\n\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py\", line 149, in from_tensorflow_frozen_model\r\nreturn from_tensorflow(graphdef, output_nodes, preprocessor, **kwargs)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py\", line 120, in from_tensorflow\r\nname=\"main\")\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 76, in convert_tf2uff_graph\r\nuff_graph, input_replacements)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 63, in convert_tf2uff_node\r\nop, name, tf_node, inputs, uff_graph, tf_nodes=tf_nodes)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 38, in convert_layer\r\nfields = cls.parse_tf_attrs(tf_node.attr)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in parse_tf_attrs\r\nfor key, val in attrs.items()}\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in <dictcomp>\r\nfor key, val in attrs.items()}\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 204, in parse_tf_attr_value\r\nreturn cls.convert_tf2uff_field(code, val)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 189, in convert_tf2uff_field\r\n'type': 'dtype', 'list': 'list'}\r\nKeyError: 'shape'\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nGPU model and memory\nMobile device", "If there are unsupported operations, you will have to keep it inside a Tensorflow graph. What tensorflow.contrib.tensorrt does is replace the sections of the TF graph with TensorRT calls. You can still run this optimized protobuf in C++ but you will have to compile the tensorflow_cc.so and use the Tensorflow C++ API.\r\n\r\nIf you want to use a TensorRT engine, and you have unsupported operations in your graph, then the UFF parser will fail. One way is to export to ONNX and implement the unsupported operations as custom plugins.", "@tensorflowbutler fields have been filled out.", "Nagging Assignee @samikama: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@LinHungShi,\r\n\r\n@fferroni 's answer is correct. If there are unsupported ops, you either need to implement custom plugins for them or try to express them in terms of supported ops. You can also give TRT5 a try. The error you see above seems a to be due to a schema change in TF. Loading the frozen graph and saving it again with the TF version supported by UFF may resolve such issues. You can try asking question in Nvidia forums. \r\nI am closing this issue if the ops are not supported in TRT, there is nothing we can do and this issue seems more related to UFF than TFTRT."]}, {"number": 22954, "title": "Error Building from source on Windows / my CPU doesn't have AVX ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.11\r\n- **Python version**:3.6.6\r\n- **Bazel version (if compiling from source)**: 0.17.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nI can't build from source as it gives me the error\r\nERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command\r\nPerhaps because my CPU doesnt have AVX instructions set\r\n\r\non my CPU Supported Instructions sets\tMMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T\r\nLack of AVX don't allow me to pip install tf>1.5\r\nMy question is how to install from source without AVX instructions set?\r\n\r\n### Source code / logs\r\n\r\n> C:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n> Starting local Bazel server and connecting to it...\r\n> WARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\n> WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n> WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n> WARNING: C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/ivo/_bazel_ivo/xv6zejqw/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n> WARNING: C:/tensorflow/tensorflow/python/BUILD:2823:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/python/BUILD:73:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:137:1: in py_library rule //tensorflow/contrib/gan:losses_impl: target '//tensorflow/contrib/gan:losses_impl' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/gan/BUILD:33:1: in py_library rule //tensorflow/contrib/gan:train: target '//tensorflow/contrib/gan:train' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\n> WARNING: C:/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\n> INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).\r\n> INFO: Found 1 target...\r\n> INFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:\r\n> cl : Command line warning D9025 : overriding '/w' with '/W3'\r\n> ERROR: C:/users/ivo/_bazel_ivo/xv6zejqw/external/protobuf_archive/BUILD:266:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Illegal instruction): bash.exe failed: error executing command\r\n>   cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow\r\n>   SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin\r\n>     SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe\r\n>     SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n>     SET TF_DOWNLOAD_CLANG=0\r\n>     SET TF_NEED_CUDA=0\r\n>     SET TF_NEED_OPENCL_SYCL=0\r\n>     SET TF_NEED_ROCM=0\r\n>   C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc: bash.exe failed: error executing command\r\n>   cd C:/users/ivo/_bazel_ivo/xv6zejqw/execroot/org_tensorflow\r\n>   SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Users\\ivo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\ivo\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\bazel;C:\\msys64\\usr\\bin\r\n>     SET PYTHON_BIN_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/python.exe\r\n>     SET PYTHON_LIB_PATH=C:/Users/ivo/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n>     SET TF_DOWNLOAD_CLANG=0\r\n>     SET TF_NEED_CUDA=0\r\n>     SET TF_NEED_OPENCL_SYCL=0\r\n>     SET TF_NEED_ROCM=0\r\n>   C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\r\n> /usr/bin/bash: line 1:  7128 Illegal instruction     bazel-out/x64_windows-opt/bin/external/protobuf_archive/js_embed.exe external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 74.199s, Critical Path: 2.64s\r\n> INFO: 42 processes: 42 local.\r\n> FAILED: Build did NOT complete successfully\r\n", "comments": ["You should build without `--config=opt` flag. That by default enables AVX on windows.", "Thanks for the tip @gunan . It unlocked the build and went further, but broke after some time again\r\n\r\n`C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\include\\xmemory(217): error C2100: illegal indirection\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.15.26726\\include\\xmemory(217): error C2062: type 'unknown-type' unexpected\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 15455.294s, Critical Path: 443.06s\r\nINFO: 3314 processes: 3314 local.\r\nFAILED: Build did NOT complete successfully`\r\n\r\nNow I am trying to resolve this according to this #22387 decision \r\n", "After following up with the fix in file /tensorflow/core/framework/op_kernel.h\r\n I was able to continue and install TF 1.11, so I will close the issue. Thanks for the help!"]}, {"number": 22953, "title": "Merge branch r1.12 into master.", "body": "Merging branch r1.12 into master after 1.12.0rc0 release.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 22952, "title": "[tf.keras] Release 1.12.0 issue in model.predict_generator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: na\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.12.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: na\r\n- **GCC/Compiler version (if compiling from source)**: na\r\n- **CUDA/cuDNN version**: na\r\n- **GPU model and memory**: na\r\n- **Exact command to reproduce**: na\r\n\r\n### Describe the problem\r\nCalling ```model.predict_generator()``` on a loaded model or an uncompiled model produces an error. This does not happen for ``model.predict`` or previous releases of TensorFlow.\r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef create_model():\r\n    inputs = tf.keras.layers.Input(shape=(1,))\r\n    outputs = tf.keras.layers.Dense(1)(inputs)\r\n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\n    return model\r\n\r\n\r\nmodel = create_model()\r\n\r\nX = np.array([[1]])\r\nprint(model.predict(X))\r\n\r\nclass gen(object):\r\n    \"\"\" Dummy data generator. \"\"\"\r\n\r\n    def run(self):\r\n        while True:\r\n            yield np.array([[1]])\r\n\r\nprint(model.predict_generator(generator=gen().run(), steps=1))\r\n```\r\n\r\n```python\r\nline 23, in <module>\r\n    print(model.predict_generator(generator=gen().run(), steps=1))\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2298, in predict_generator\r\n    verbose=verbose)\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 354, in predict_generator\r\n    model._make_test_function()\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 716, in _make_test_function\r\n    raise RuntimeError('You must compile your model before using it.')\r\n```", "comments": ["This is annoying because if you do something along the lines of\r\n\r\n```model.load_weights(...)``` --> ```model.predict_generator``` without recompiling you get an error.", "@raymond-yuan or @pavithrasv could one of you two maybe take a look.\r\n\r\nI do not want to have to re-compile the optimizer, metrics, etc... at predict time to upgrade to TensorFlow ``1.12.0``", "Updating description and title, this bug is present in release ``1.12.0``.", "I am also experiencing this bug. Here is another scenario where this bug should be fixed. ``predict_generator()`` should not require the model to be recompiled.\r\n\r\n```python3\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nclass DataSequence(keras.utils.Sequence):\r\n\r\n    def __init__(self, x_set, batch_size):\r\n        self.x = x_set\r\n        self.batch_size = batch_size\r\n\r\n    def __len__(self):\r\n        return np.ceil(len(self.x) / self.batch_size)\r\n\r\n    def __getitem__(self, idx):\r\n        return self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n\r\ninput = keras.layers.Input(shape=[1.])\r\nhidden = keras.layers.Dense(2.)(input)\r\noutput = keras.layers.Dense(2.)(hidden)\r\n\r\nmodel = keras.models.Model(inputs=input, outputs=output)\r\nmodel_2 = keras.models.Model(inputs=input, outputs=hidden)\r\n\r\nmodel.compile(optimizer='adam', loss='mse')\r\nmodel.fit(np.random.normal(0, 1, (100)), np.random.normal(0, 1, (100, 2)))\r\nmodel_2.predict(np.random.normal(0, 1, (100)))  # it works without compiling model_2\r\nprint(\"using predict() successfully without compiling model\")\r\n\r\ngenerator = DataSequence(np.random.normal(0, 1, (100)), 100)\r\nmodel_2.predict_generator(generator=generator, steps=1)  # error raised, it only works if model_2 also compiled\r\n```", "@brge17 I believe this issue is fixed in the master branch. I manually modify ``model._make_test_function()`` to ``model._make_predict_function()`` under ``predict_generator()`` function in ``tensorflow/python/keras/engine/training_generator.py`` in the tensorflow 1.12.0 installation and it works fine. This also appeared in the master branch.", "Excellent, I will try tf-nightly :) soon.", "Yes this has been fixed in https://github.com/tensorflow/tensorflow/commit/3ea28fa0056fdd5c0821c6347d86a938a80e8e0c#diff-6561418ac6882a842d78dad52731895b\r\n\r\nThank you!", "Closing as this is resolved"]}, {"number": 22951, "title": "allows cond_v2 to return output tensors in nested structures", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLA has been signed. My github has 2 emails linked to it, but I signed the CLA using the primary email address for my github account. Not sure if this is causing an issue. ", "Can you try signing the CLA using the other email too?", "Formatting changes made. CLA signed with both emails tied to this github account. Let me know if you need anything else on this. Thanks! ", "@skye Please re-approve. Accidentally forgot to remove some redundant code in the last commit. ", "Also CLA issue may be stemming from the fact that my git email was configured improperly, and I was committing as Ubuntu instead of @pythonanonuser. Problem fixed in this latest commit. ", "The first commit (1c10a57) is authored by \"Ubuntu <ubuntu@ip-172-31-7-102.us-west-2.compute.internal>\".  That need to be updated to properly be attributed to you.  That should resolve the CLA checking issue.", "@pythonanonuser, I think you need to rebase in and force push in order to rewrite the previous commits that are still authored by \"Ubuntu\" (I would rebase -i + squash to make sure, ping me offline if you need help with this).", "@willnorris thanks for your help!", "@skye @willnorris just rebased and squashed this as 1 commit. Hopefully this fixes all issues? ", "@pythonanonuser Request you to sign the CLA. Thanks !", "@harshini-gadige CLA has been signed", "Hey @pythonanonuser, the commit is still authored by Ubuntu. Can you do:\r\ngit commit --amend --author=<the proper email address>\r\nAnd repush the branch?", "CLAs look good, thanks!\n\n<!-- ok -->", "@skye Just changed the author and force pushed an amended commit. Hopefully this works? ", "Yay CLA works!\r\n\r\nI think you may need to pull the latest changes and repush your branch, but otherwise LGTM.", "@skye I should have resolved the merge conflicts in the latest commit. "]}, {"number": 22950, "title": "[bazel] Use tmp dir for intermediate NVCC source files", "body": "* When using the local cuda cross-compilation toolchain,\r\n  header file changes to cc_library would not trigger rebuilds for\r\n  dependent cc_binary rules.\r\n\r\nMore info can be provided if needed to repro the bug in question.", "comments": ["This issue seems to be gone on e.g. commit `c379cd22b88c729150d70a33a36846d2fa26b4d6`. I'm curious what the fix was, as it seems different from the one in this PR?", "@gunan  PTAL", "Not familiar with this bug, I will let @meteorcloudy comment.\r\nI am very sorry about the delay, as last few months I was not able to look into github issues and PRs as much as I would like to.", "Nagging Reviewer @gunan, @meteorcloudy: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 22949, "title": "Adding our policy on creating new GitHub Issues", "body": "", "comments": ["Added all templates and Issues policy.", "Looks like copybara failed here. Looking into manually running/fixing it."]}, {"number": 22948, "title": "MacOSX & Bazel build errors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS High Sierra 10.13.6\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: 0.17.2\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Mobile device**: Samsung Note 9\r\n- **Exact command to reproduce**: bazel build -c opt --cxxopt=--std=c++11 //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so\r\n\r\n### Describe the problem\r\nI am trying to implement the TensorFlowLite Experimental Unity Plug-in. I am currently building tensorflow from the source. I am using android ndk version 13. When I run bazel build -c opt --cxxopt=--std=c++11 //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so, I get the errors below. \r\n\r\nSometimes, when I run the above command, I get an undeclared inclusion(s) rule on 'tensorflow/contrib/lite/profiling/time.cc' or another file without changing the statement.\r\n\r\nAll I need to do is use the tensorflowlite experimental unity plug-in so if there is a workaround to this or a solution, that'd be great.\r\n\r\n### Source code / logs\r\nINFO: Analysed target //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /Users/sarahanson/tensorflow/voyager/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:440:1: undeclared inclusion(s) in rule '//tensorflow/contrib/lite/kernels/internal:kernel_utils':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/lite/kernels/internal/kernel_utils.cc':\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/stdint.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_int8_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_int16_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_int32_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_int64_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uint8_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uint16_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uint32_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uint64_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/cdefs.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_symbol_aliasing.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_posix_availability.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/machine/_types.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/i386/_types.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_pthread/_pthread_types.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_intptr_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/machine/types.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/i386/types.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_u_int8_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_u_int16_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_u_int32_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_u_int64_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/sys/_types/_uintptr_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_intmax_t.h'\r\n  '/Library/Developer/CommandLineTools/SDKs/MacOSX10.13.sdk/usr/include/_types/_uintmax_t.h'\r\nIn file included from tensorflow/contrib/lite/kernels/internal/kernel_utils.cc:15:\r\nIn file included from ./tensorflow/contrib/lite/kernels/internal/kernel_utils.h:18:\r\n./tensorflow/contrib/lite/c/builtin_op_data.h:144:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\n./tensorflow/contrib/lite/c/builtin_op_data.h:147:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\n./tensorflow/contrib/lite/c/builtin_op_data.h:210:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\n./tensorflow/contrib/lite/c/builtin_op_data.h:213:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\n./tensorflow/contrib/lite/c/builtin_op_data.h:252:9: warning: empty struct has size 0 in C, size 1 in C++ [-Wextern-c-compat]\r\ntypedef struct {\r\n        ^\r\n5 warnings generated.\r\nTarget //tensorflow/contrib/lite/experimental/c:libtensorflowlite_c.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.374s, Critical Path: 0.22s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "CUDA/cuDNN version: N/A\r\nGPU model and memory : N/A\r\nMobile device: Samsung Note 9", "@jdduke, @aselle any idea?", "Any ideas @jdduke @aselle ?", "Thanks for the report, and apologies for the delay. Will try to investigate this asap.", "@sdhanson is there any chance you can try a more recent NDK version, e.g., 15c or 16b?", "@sdhanson  Any update ?", "Sorry for the wait @jdduke @harshini-gadige. I got someone else to make the build for me, and I have not tried this process again."]}, {"number": 22947, "title": "[tf.keras] Ominous Load Warning", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: na\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: na\r\n- **GCC/Compiler version (if compiling from source)**: na\r\n- **CUDA/cuDNN version**: na\r\n- **GPU model and memory**:na \r\n- **Exact command to reproduce**:na\r\n\r\n### Describe the problem\r\nCalling \r\n```tf.keras.models.model.save(...)```\r\n```model.load_weights(...)```\r\n\r\nProduces an ominous warning:\r\n\r\n```\r\nData loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\n```\r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef create_model():\r\n    inputs = tf.keras.layers.Input(shape=(1,))\r\n    outputs = tf.keras.layers.Dense(1)(inputs)\r\n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\n    return model\r\n\r\nX = np.array([[1]])\r\n\r\nmodel = create_model()\r\nmodel.compile(\"Adam\", loss=\"mse\")\r\nmodel.save(\"test.hdf5\")\r\nprint (model.predict(X))\r\ndel model\r\n\r\nmodel = create_model()\r\nmodel.load_weights(\"test.hdf5\")\r\nprint (model.predict(X))\r\n```\r\n\r\n```\r\n[[-1.1461306]]\r\n2018-10-12 13:05:33.052964: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ./test.hdf5: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\n[[-1.1461306]]\r\n```\r\n\r\nThe predictions match, the warning isn't actually an issue.", "comments": ["This was fixed in https://github.com/tensorflow/tensorflow/commit/34d58ca2de7a67be382fb1c7b7ca4868f6695ee3\r\n\r\nCould you try upgrading?", "I ran it with ``1.12.0rc0``, issue still exists. I'm using the latest release: ``1.11.0``.\r\n\r\n", "@allenlavoie I can try it on ``tf-nightly``, but I've tested it against the latest and greatest release + release candidate.\r\n\r\nThat PR is very old...", "Oh I see, you're using \".hdf5\". So we could just add that to _is_hdf5_filepath in https://github.com/tensorflow/tensorflow/commit/34d58ca2de7a67be382fb1c7b7ca4868f6695ee3 . Want to send a PR? It'll be a while before I get to it otherwise.", "Ya I will np."]}, {"number": 22946, "title": "Convolutional LSTM raises errors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNo\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.9\r\n- **Python version**:\r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 9\r\ncuDNN 7.1\r\n\r\n- **GPU model and memory**:\r\nGeForce GTX 1070\r\n8GB\r\n\r\n- **Exact command to reproduce**:\r\nListed in source code\r\n\r\n### Describe the problem\r\n\r\nUsing convolutional LSTM cell with dynamic_rnn raises errors:\r\n```\r\n2018-10-12 14:24:01.651741: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:581] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.\r\n2018-10-12 14:24:01.652168: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:581] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.\r\n```\r\nThe Code is listed below, it works on tf1.8. But tf1.9 and tf1.10 give errors above.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\ncell = tf.contrib.rnn.ConvLSTMCell(conv_ndims=2, input_shape=[28, 28, 3], output_channels=32, kernel_shape=[3, 3], use_bias=True)\r\nc_input = tf.placeholder(tf.float32, [None] + list(cell.state_size.c))\r\nh_input = tf.placeholder(tf.float32, [None] + list(cell.state_size.h))\r\nstate_in = tf.nn.rnn_cell.LSTMStateTuple(c_input, h_input)\r\nx = tf.placeholder(tf.float32, [None, None, 28, 28, 3])\r\nlstm_output, next_state = tf.nn.dynamic_rnn(cell = cell, inputs = x, initial_state = state_in, dtype = tf.float32, time_major = False)\r\nimport numpy as np\r\nv_x = np.ones([1, 1, 28, 28, 3])\r\nc_v = np.ones([1, 28, 28, 32])\r\nh_v = np.ones([1, 28, 28, 32])\r\nfeed_dict = {x: v_x, c_input: c_v, h_input: h_v}\r\ninit_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\r\nsess = tf.Session()\r\nsess.run(init_op)\r\ntest = sess.run(lstm_output, feed_dict = feed_dict)\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "I am facing the same issue. The coding is still able to run with this \"topological sort failed\" warning. I am wondering whether I will get correct training results with this warning.\r\n\r\nIn addition, static_rnn is ok with ConvLSTMCell.", "@Junchi-Liang I run through the code with nightly build and it works fine. Could you give nightly a try and see if this is still an issue?", "I tried '1.12.0-dev20181012', but it cannot work as it still gives the same error.\r\n\r\n@yongtang Could you tell me which version did you use?", "@Junchi-Liang I run with the following (not with GPU):\r\n```\r\nroot@ubuntu:/v# python -c 'import tensorflow as tf; print(tf.VERSION)'\r\n1.12.0-dev20181012\r\nroot@ubuntu:/v# python 22946.py       \r\n2018-10-16 16:26:34.662982: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nroot@ubuntu:/v# cat 22946.py \r\nimport tensorflow as tf\r\ncell = tf.contrib.rnn.ConvLSTMCell(conv_ndims=2, input_shape=[28, 28, 3], output_channels=32, kernel_shape=[3, 3], use_bias=True)\r\nc_input = tf.placeholder(tf.float32, [None] + list(cell.state_size.c))\r\nh_input = tf.placeholder(tf.float32, [None] + list(cell.state_size.h))\r\nstate_in = tf.nn.rnn_cell.LSTMStateTuple(c_input, h_input)\r\nx = tf.placeholder(tf.float32, [None, None, 28, 28, 3])\r\nlstm_output, next_state = tf.nn.dynamic_rnn(cell = cell, inputs = x, initial_state = state_in, dtype = tf.float32, time_major = False)\r\nimport numpy as np\r\nv_x = np.ones([1, 1, 28, 28, 3])\r\nc_v = np.ones([1, 28, 28, 32])\r\nh_v = np.ones([1, 28, 28, 32])\r\nfeed_dict = {x: v_x, c_input: c_v, h_input: h_v}\r\ninit_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\r\nsess = tf.Session()\r\nsess.run(init_op)\r\ntest = sess.run(lstm_output, feed_dict = feed_dict)\r\nroot@ubuntu:/v# \r\n```", "@Junchi-Liang Agree with @yongtang. Also, I ran the above code snippet and it works in TensorFlow 1.12.0.rc", "@yongtang @wt-huang \r\nI tried both 1.12.0-dev20181012 and 1.12.0.rc without GPU support, the code works. But it cannot work in their GPU versions. Does it imply a problem with CUDA or cuDNN? For instance, incompatible version.", "Thanks @Junchi-Liang I tried with gpu on latest master and the issue is reproduced. Still not exactly sure why there is a discrepancy between gpu and cpu from the problematic part of the code.", "Second @yongtang, the issue is also reproduced on TensorFlow 1.10.0, 1.12.0 as well as nightly. It appears the error comes from   \r\n```\r\nlstm_output, next_state = tf.nn.dynamic_rnn(cell = cell, inputs = x, initial_state = state_in, dtype = tf.float32, time_major = False)\r\n```", "I'm also seeing this issue on TensorFlow 1.10, with a custom estimator.", "I'm also seeing this issue on TensorFlow 1.10.1 on a gpu.", "Same issue here with TensorFlow version: 1.12.0, compiled from source with CUDA10\r\n", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue since it no longer persists with the latest TF build. Thanks!", "Same issue here with TensorFlow-GPU: 1.12.0, with CUDA-9, on Ubuntu 16.04\r\nCode: https://github.com/Honghe/Speaker_Verification", "@ymodak \r\n> Closing this issue since it no longer persists with the latest TF build\r\n\r\nWhich TensorFlow version? I met this error in v1.12.0-GPU. Thanks", "I also got this error with Ubuntu 16.04.6 LTS, Python 3.6.8, TensorFlow v1.12.2-0-gcf74798993, CUDA 9.0, and cuDNN 7.5.0. Although the error was raised, it looks like the code has been executed. "]}, {"number": 22945, "title": "Can't install form pip on MacOS Mojave and build from source failed", "body": "@tesorflowbutler asked nicely so I added this section :)\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution:\r\nMacOS Mojave\r\nTensorFlow installed from: \r\n- source, tag v1.11.0\r\n- source, branch master \r\n- pip, 1.11\r\nTensorFlow version: latest or 1.11.0\r\nBazel version: \r\n0.17.2 via brew\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: I\u2019ve tried a couple of things, listed below.\r\nMobile device: N/A\r\n\r\n\u2014-\r\n\r\nWhen I tried `pip3 install --user --upgrade tensorflow`\r\nit didn't worked see\r\n\r\n```\r\npip3 install --user --upgrade tensorflow\r\n\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n```\r\n\r\nwhen trying `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nit fails for\r\n\r\n```\r\n9 warnings generated.\r\nERROR: /private/var/tmp/_bazel_kobi.kadosh/2314413b591ef152edf49d9023c8bc65/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:172:13: error: assigning to 'char *' from incompatible type 'const char *'\r\n        if (PyString_AsStringAndSize(key, &name, &name_size) < 0) {\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:189:13: error: assigning to 'char *' from incompatible type 'const char *'\r\n        if (PyString_AsStringAndSize(key, &camelcase_name, &name_size) < 0) {\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n2 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 218.358s, Critical Path: 49.61s\r\nINFO: 1348 processes: 1348 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nthe only thing allowed me to get it installed was: \r\n\r\n`pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.11.0-py3-none-any.whl`\r\n\r\nwhich I saw here\r\nhttps://qiita.com/nahshi/items/fcf4898f7c45f11a5c63\r\n\r\nrunning this\r\n\r\n`python3 -c \"import tensorflow as tf; print(tf.__version__)\"`\r\n\r\noutput that it didn't match.. I guess that my only option is to compile.. but I can't get it to work..\r\n\r\n```\r\n/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\r\n  return f(*args, **kwds)\r\n1.11.0\r\n```\r\n\r\nAny help here would be greatly appreciated ^_^", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I'm having the same issue ", "Can you please update your pip3 version to 9.0.1 (typically requires 8.1 or later) and install tensorflow again?", "Same issue here.  My version info: \r\npip3 - 18.1\r\nPython 3.7 (TensorFlow website says it supports up to 3.6)\r\nvirtualenv 16.0\r\nXcode 10 w/ tools\r\n\r\nI also tried the following inside and outside virtualenv. \"pip3 install tensorflow\".  I cloned and tried build/install tensorflow using most up-to-date Java, Bazel, six, wheel, numpi, scipy, ect...  \r\n\r\nI am getting the same exact issues as wildcard.  \r\n\r\nRunning on 2017 MacBookPro 13\" Mojave (all software up-to-date). \r\n\r\nPretty sure the issue is Python 3.7.  I'm going to try to start a new virtualenv with Python3.6 and try again.  I'll keep you updated.", "@CraigOpie You are correct about Python 3.7 compatibility. TensorFlow 1.11 is tested against Python 3.6 and lower so should work. Please post here your observations.", "@wildcard @ymodak: Just confirmed, I had the same issues as you did with Python 3.7.  You need to install Python 3.6.6 or lower in your virtualenv to use tensorflow.  Just use google if you don't know how to install a different version of Python in your virtualenv.  If you still have questions after that, contact me.  ", "MacOS users do not upgrade to MacOS Mojave if you want to use tensorflow.\r\n I use it on MacOS 10.13.6. with Xcode 9.2  CUDA 9.2\r\n\r\nBuild with newest source code from github.\r\n\r\nI use anaconda so the \".bash_profile\" file is below:\r\n\r\n# added by Anaconda3 5.2.0 installer\r\nexport PATH=\"/Users/username/anaconda3/bin:$PATH\"\r\n\r\nexport CUDA_HOME=/usr/local/cuda\r\nexport DYLD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib:/Developer/NVIDIA/CUDA-9.2/lib\r\nexport PATH=$DYLD_LIBRARY_PATH:$PATH:/Developer/NVIDIA/CUDA-9.2/bin\r\n\r\n[ Download NCCL v2.3.5, for CUDA 9.2, Sept 25, 2018 ]\r\nhttps://developer.nvidia.com/nccl/nccl-download\r\nNCCL 2.3.5 O/S agnostic and CUDA 9.2\r\ntar xvf nccl_2.3.5-2+cuda9.2_x86_64.txz (cd ~/Downloads)\r\ncd nccl_2.3.5-2+cuda9.2_x86_64/lib\r\nsudo mv * /Developer/NVIDIA/CUDA-9.2/lib/\r\ncd nccl_2.3.5-2+cuda9.2_x86_64/include\r\nsudo mv nccl.h /Developer/NVIDIA/CUDA-9.2/include/\r\ncd tensorflow/third_party/nccl/\r\nln -s /Developer/NVIDIA/CUDA-9.2/include/nccl.h\r\n\r\nAnd Download CUDNN  and install to related path.\r\n\r\nBefore building, just modify some files:\r\n\r\nDelete all  __align__(sizeof(T)) or  __align__(8)  in following three files:\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc\r\ntensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\n\r\n\r\nDisable linkopts = [\u201c-lgomp\u201d] in file:\r\ntensorflow/third_party/gpus/cuda/BUILD.tpl\r\n\r\nDelete \"constexpr\" in \"constexpr Variant() noexcept = default; \" in file:\r\ntensorflow/core/framework/variant.h\r\n\r\nBecause NCCL will not work on Mac computer, we need delete each item contained nccl in \r\ntensorflow/contrib/BUILD\r\ntensorflow/contrib/distribute/python/BUILD\r\ntensorflow/contrib/all_reduce/BUILD\r\n\r\nespecially in tensorflow/contrib/BUILD , modified like following:\r\n\r\n\"\"//tensorflow/contrib/text:all_kernels\",\r\n    ] + if_mpi([\"//tensorflow/contrib/mpi_collectives:mpi_collectives_py\"]) + select({\r\n        \"//tensorflow:android\": [],\r\n\"\r\nThese operation will disable nccl related function in tensorflow running on Mac.\r\n\r\nThen use \r\n1. ./configure\r\n2. bazel build --config=cuda --config=opt --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package\r\n3. ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\n4. cd /tmp/tensorflow_pkg\r\n5. pip install tensorflow-1.12.0rc0-cp36-cp36m-macosx_10_7_x86_64.whl\r\n\r\nThen try to run it. It will be OK!\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Closing this issue since downgrading Python version to 3.6 solves the issue. Please feel free to reopen if it still persists with additional information. Thanks!"]}, {"number": 22944, "title": "Update README.md", "body": "Provided a link of docker document to start the docker instance.\r\nAdded command to access the Notebook on Windows.", "comments": []}, {"number": 22943, "title": "[INTEL MKL] Clean up MKL_ML code for BatchNorm op", "body": "Clean up BatchNorm op by removing legacy MKL-ML code.", "comments": ["@tatianashp  PTAL", "Just did. Looks good.", "Hi Tatiana, \r\nI just made a new commit, to address all \"clang format check\" related issues (not many).\r\nHopefully the commit is helpful for the CI test.\r\n\r\nThanks", "@tatianashp  Any update ?", "No more update. Since this PR only did code clean up (MKL ML related) and should not introduce \r\nany error. Only fix is clang style check, adjusting header file inclusion order. ", "> No more update. Since this PR only did code clean up (MKL ML related) and should not introduce\r\n> any error. Only fix is clang style check, adjusting header file inclusion order.\r\n\r\n@gzmkl  Are you working on clang style check ? (Correct me if I understood wrong)", "No, clang style check work has been done. Thanks!", "@tatianashp  Could you please approve this PR in order to help this get merged.", "This PR has been blocked by 3 internal checks while most others have passed.\r\n\r\nPlease help to check. Thanks!", "This PR removes some unused ifdefs. It should be independent from any current test failures.", "Hi Tatiana and your team, thank you very much for taking care of the internal build issue!   -GZ"]}, {"number": 22942, "title": "Keras tensorflow : Freeze calibrated mode, extract graph, input and output tensors", "body": "I create a tensorflow medel using keras embedded in tensorflow:\r\n\r\n    import tensorflow as tf\r\n    from tensorflow.python.keras.models import Sequential\r\n    from tensorflow.python.keras.layers import Dense \r\n    model = Sequential()\r\n    model.add(Dense(input_dim=input_dim, units=hidden_dim))\r\n    model.add(Dense(units=output_dim, activation='linear'))\r\n    model.compile(loss='mse', optimizer='adam')\r\n\r\nAfter fitting the model, I want to freeze it and to extract the graph, the input tensor, and the output tensor. \r\n    \r\n    session = tf.keras.backend.get_session()\r\n    output_node_names = [node.op.name for node in model.outputs]\r\n    graphdef = tf.graph_util.convert_variables_to_constants(session, session.graph_def, \r\n    output_node_names)\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graphdef)\r\n        input_tensor = graph.get_tensor_by_name(graph.get_operations()[0].name+':0')\r\n        output_tensor = graph.get_tensor_by_name(graph.get_operations()[-1].name+':0')\r\n\r\n\r\nSo that after the above code, the variable `graph` will contain the graph, and `input_tensor` and `output_tensor` will contain respectively the  input and output of the graph.\r\n\r\nThe graph does not offer a method to get its input and output tensors, hence my complicated method above. But it does not work.\r\n\r\nCan you help please or advise a better way to do it?\r\n\r\nUsing Tensorflow 1.5", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@volvador You can also use `session.run()` to retrieve output tensors, just specify the tensor name."]}, {"number": 22941, "title": "[INTEL MKL] Clean up MKL_ML code for Conv backward ", "body": "Clean up Conv backward ops by removing legacy MKL-ML code.", "comments": []}, {"number": 22940, "title": "Tensorflow 1.11.0 is still looking for cublas 9.0 (and other libraries) despite of having installed CUDA 10", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\npip3 install tensorflow-gpu==1.11.0\r\n\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n\r\nI use a local installation: /home/ivan/Python/Python-3.6.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 10.0 and CuDNN 7.3.1 compatible with CUDA 10.0\r\n\r\n- **GPU model and memory**:\r\n\r\nNVIDIA Corporation GM107GL [Quadro K2200] with 65 GB of RAM\r\n\r\n- **Exact command to reproduce**:\r\nimport tensorflow as tf\r\n\r\nIt produces this error message;\r\n\r\nTraceback (most recent call last):\r\n  File \"Main_55_Lithofacies_Classification.py\", line 19, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nCollecting system information...\r\nTraceback (most recent call last):\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/imarroquin/Python/Python-3.6.5/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nTensorflow 1.11.0 is trying to load cublas from CUDA 9.0. Unfortunately, I can't install CUDA 9.0 on Ubuntu 18.04 because is not supported by Nvidia (https://devtalk.nvidia.com/default/topic/1036640/cuda-setup-and-installation/cuda-9-0-install-on-ubuntu-18-04-quot-unmet-dependencies-quot-/ )\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["CUDA 10.0 support is coming soon in a released version of Tensorflow, see https://github.com/tensorflow/tensorflow/issues/22706 for more details.\r\n\r\nOn the other hand, it might not be officially supported by NVIDIA, but it is certainly possible to install CUDA 9.0 on Ubuntu 18.04 on certain systems. Here's proof from my system:\r\n```\r\n$ python -c 'import tensorflow as tf; print(tf.VERSION)'\r\n1.11.0\r\n\r\n$ python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib() + \"/python/_pywrap_tensorflow_internal.so\")' | xargs ldd | grep libcu\r\n\tlibcublas.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so.9.0 (0x00007fa334ec5000)\r\n\tlibcusolver.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusolver.so.9.0 (0x00007fa3302ca000)\r\n\tlibcudart.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0 (0x00007fa33005d000)\r\n\tlibcuda.so.1 => /usr/lib/x86_64-linux-gnu/libcuda.so.1 (0x00007fa32df33000)\r\n\tlibcudnn.so.7 => /usr/lib/x86_64-linux-gnu/libcudnn.so.7 (0x00007fa31bbfa000)\r\n\tlibcufft.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so.9.0 (0x00007fa313b59000)\r\n\tlibcurand.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0 (0x00007fa30fbf5000)\r\n\r\n$ lsb_release -d\r\nDescription:\tUbuntu 18.04.1 LTS\r\n```", "Thanks @jacquerie  for the quick reply. I hope that the new version will be made available soon", "Here is my new tutorial for Building Tensorflow 1.12 + CUDA 10.0 + CUDNN 7.3.1 + NCCL 2.3.5 + bazel-0.17.2 https://www.python36.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-ubuntu/, you will also get prebuilt wheel at last with AVX and compute capability 5.0.", "> Here is my new tutorial for Building Tensorflow 1.12 + CUDA 10.0 + CUDNN 7.3.1 + NCCL 2.3.5 + bazel-0.17.2 https://www.python36.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-ubuntu/, you will also get prebuilt wheel at last with AVX and compute capability 5.0.\r\n\r\nI'm trying to build with the exact same configs but it keeps failing in the bazel build step. Can you provide a link to your prebuilt wheel?", "https://drive.google.com/file/d/1QV7fBi5ZpTm02N1_QbyEhT6v80B6Zffg/view?usp=sharing", "@ivan-marroquin  - Is this still an issue ?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Yes it is still an issue. Tensorflow's docker images are broken: https://www.tensorflow.org/install/docker links against cuda 9, but deploys will cuda 10", "The latest release TF 1.13.0-rc0 comes with cuda 10 support. Therefore the docker tf gpu images are deployed with cuda 10.", "> The latest release TF 1.13.0-rc0 comes with cuda 10 support. Therefore the docker tf gpu images are deployed with cuda 10.\r\n\r\nMaybe google itself hasn't been updating their stuff?\r\n\r\n```\r\ndocker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu    python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "This is inside of the official nightly devel-gpu image:\r\n\r\n```\r\nldd libtensorflow_framework.so \r\n\tlinux-vdso.so.1 =>  (0x00007ffc8a15a000)\r\n\tlibcublas.so.9.0 => not found\r\n\tlibcuda.so.1 => /usr/lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f0258749000)\r\n\tlibcudnn.so.7 => /usr/lib/x86_64-linux-gnu/libcudnn.so.7 (0x00007f0243922000)\r\n\tlibcufft.so.9.0 => not found\r\n\tlibcurand.so.9.0 => not found\r\n\tlibcudart.so.9.0 => not found\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f024371e000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f0243415000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f02431f8000)\r\n\tlibstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f0242e76000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f0242c60000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f0242896000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f025a7aa000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f024268e000)\r\n\tlibnvidia-fatbinaryloader.so.415.27 => /usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.415.27 (0x00007f0242441000)\r\n```\r\n\r\nSo, clearly linked to cuda 9. BUT:\r\n\r\n```\r\nls -la /usr/local/cuda/lib64/\r\ntotal 441176\r\ndrwxr-xr-x 1 root root      4096 Feb  1 15:12 .\r\ndrwxr-xr-x 1 root root      4096 Feb  1 15:11 ..\r\nlrwxrwxrwx 1 root root        14 Sep 12 21:37 libOpenCL.so -> libOpenCL.so.1\r\nlrwxrwxrwx 1 root root        16 Sep 12 21:37 libOpenCL.so.1 -> libOpenCL.so.1.1\r\n-rw-r--r-- 1 root root     27096 Sep 12 21:35 libOpenCL.so.1.1\r\nlrwxrwxrwx 1 root root        19 Sep 12 21:37 libaccinj64.so -> libaccinj64.so.10.0\r\nlrwxrwxrwx 1 root root        23 Sep 12 21:37 libaccinj64.so.10.0 -> libaccinj64.so.10.0.130\r\n-rw-r--r-- 1 root root   7407024 Sep 12 21:35 libaccinj64.so.10.0.130\r\nlrwxrwxrwx 1 root root        21 Sep 12 21:37 libcublas.so.10.0 -> libcublas.so.10.0.130\r\n-rw-r--r-- 1 root root  70796360 Sep 12 21:35 libcublas.so.10.0.130\r\n-rw-r--r-- 1 root root    695156 Sep 12 21:35 libcudadevrt.a\r\nlrwxrwxrwx 1 root root        17 Sep 12 21:37 libcudart.so -> libcudart.so.10.0\r\nlrwxrwxrwx 1 root root        21 Sep 12 21:37 libcudart.so.10.0 -> libcudart.so.10.0.130\r\n-rw-r--r-- 1 root root    495736 Sep 12 21:35 libcudart.so.10.0.130\r\n-rw-r--r-- 1 root root    955082 Sep 12 21:35 libcudart_static.a\r\nlrwxrwxrwx 1 root root        20 Sep 12 21:37 libcufft.so.10.0 -> libcufft.so.10.0.145\r\n-rw-r--r-- 1 root root 103177128 Sep 12 21:35 libcufft.so.10.0.145\r\nlrwxrwxrwx 1 root root        21 Sep 12 21:37 libcufftw.so.10.0 -> libcufftw.so.10.0.145\r\n-rw-r--r-- 1 root root    561192 Sep 12 21:35 libcufftw.so.10.0.145\r\nlrwxrwxrwx 1 root root        18 Sep 12 21:37 libcuinj64.so -> libcuinj64.so.10.0\r\nlrwxrwxrwx 1 root root        22 Sep 12 21:37 libcuinj64.so.10.0 -> libcuinj64.so.10.0.130\r\n-rw-r--r-- 1 root root   7792472 Sep 12 21:35 libcuinj64.so.10.0.130\r\n-rw-r--r-- 1 root root     31954 Sep 12 21:35 libculibos.a\r\nlrwxrwxrwx 1 root root        21 Sep 12 21:37 libcurand.so.10.0 -> libcurand.so.10.0.130\r\n-rw-r--r-- 1 root root  60806128 Sep 12 21:35 libcurand.so.10.0.130\r\nlrwxrwxrwx 1 root root        23 Sep 12 21:37 libcusolver.so.10.0 -> libcusolver.so.10.0.130\r\n-rw-r--r-- 1 root root 139257368 Sep 12 21:35 libcusolver.so.10.0.130\r\nlrwxrwxrwx 1 root root        23 Sep 12 21:37 libcusparse.so.10.0 -> libcusparse.so.10.0.130\r\n-rw-r--r-- 1 root root  59078736 Sep 12 21:35 libcusparse.so.10.0.130\r\nlrwxrwxrwx 1 root root        18 Sep 12 21:37 libnvToolsExt.so -> libnvToolsExt.so.1\r\nlrwxrwxrwx 1 root root        22 Sep 12 21:37 libnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\r\n-rw-r--r-- 1 root root     37240 Sep 12 21:35 libnvToolsExt.so.1.0.0\r\nlrwxrwxrwx 1 root root        21 Sep 12 21:37 libnvblas.so.10.0 -> libnvblas.so.10.0.130\r\n-rw-r--r-- 1 root root    596080 Sep 12 21:35 libnvblas.so.10.0.130\r\ndrwxr-xr-x 2 root root      4096 Feb  1 15:11 stubs\r\n```"]}, {"number": 22939, "title": "LSTMBlockFusedCell not supported by optimize_for_inference?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, https://github.com/mozilla/DeepSpeech/blob/963edc12deeaa9d2307c3c352d7471a97057e6f2/DeepSpeech.py#L385-L480 and https://github.com/mozilla/DeepSpeech/blob/963edc12deeaa9d2307c3c352d7471a97057e6f2/DeepSpeech.py#L1786-L1824\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian/Sid\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary and source\r\n- **TensorFlow version (use command below)**: r1.11 and current master (9e0fa9578638f9147c0b180e6ea89d67d5c0bae3)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: 8.2 (Debian Sid)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\n - Train a model using DeepSpeech\r\n - Optimize with: `python tensorflow/python/tools/optimize_for_inference.py --frozen_graph true --input ~/tmp/deepspeech-fr/model/output_graph.pb --input_names input_node,input_lengths,previous_state_c,\r\nprevious_state_h --output_names logits --output ~/tmp/deepspeech-fr/model/output_graph_opt.pb`\r\n - `optimize_for_inference.py` runs without any issue\r\n - Load and run the model for inference results in `Invalid argument: Input 0 of node lstm_fused_cell/Max was passed float from input_lengths:0 incompatible with expected int32.`", "comments": ["I'm working on providing simpler STR.", "[Frozen graph, OK](https://github.com/tensorflow/tensorflow/files/2473686/output_graph.pbtxt.txt)\r\n[Optimized, NOK](https://github.com/tensorflow/tensorflow/files/2473687/output_graph_opt.pbtxt.txt)\r\n", "Somehow, `tensorflow/tools/quantization/graph_to_dot.py` errors about UTF-8 stuff: `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 130: invalid start byte`, and relying on protobuf text file format also fails ...", "Ok, I might have been too quick in my judgement. It looks like `--placeholder_type_enum 3` does fix the first issue, but yields a new error: `Invalid argument: Input 0 of node transpose was passed int32 from input_node:0 incompatible with expected float.`", "@harshini-gadige Hello, since you are assigned, do you know if `optimize_for_inference` is some deprecated old Python-based tooling and that we should instead use `transform_graph` ?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@harshini-gadige I'm sorry, but I already filled the issue template, and documented that using `optimize_for_inference` from multiple TensorFlow releases transforms the graph in a way that is broken. So I do not think that it should be on StackOverflow ...", "Hi @lissyx !We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "> Hi @lissyx !We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!\r\n\r\nI'm not working on TensorFlow-related tooling anymore. For what it is worth, the original question whether this tool was deprecated on that branch still holds with no proper answer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22939\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22939\">No</a>\n", "@reuben might care / need for a new issue, though ?"]}, {"number": 22938, "title": "RuntimeError: TOCO failed see console for info.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["i download the ssd_mobilenet_v1_quantized_coco  model from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\r\ni want to quantize it ,then used on mobile,but i got  the error:\r\n`WARNING:tensorflow:From quant.py:9: TocoConverter.from_frozen_graph (from tensorflow.contrib.lite.python.lite) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `lite.TFLiteConverter.from_frozen_graph` instead.\r\n2018-10-12 13:26:50.610018: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nbut ^H^H^HTraceback (most recent call last):\r\n  File \"quant.py\", line 17, in <module>\r\n    mobilenet_tflite_file.write_bytes(converter.convert())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 317, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-10-12 13:26:54.789335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1109] Converting unsupported operation: TFLite_Detection_PostProcess\\n2018-10-12 13:26:54.795610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1182] Unable to determine output type for op: TFLite_Detection_PostProcess\\n2018-10-12 13:26:54.838750: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 900 operators, 1352 arrays (0 quantized)\\n2018-10-12 13:26:54.886856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 900 operators, 1352 arrays (0 quantized)\\n2018-10-12 13:26:56.012297: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 111 operators, 220 arrays (0 quantized)\\n2018-10-12 13:26:56.016248: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 111 operators, 220 arrays (0 quantized)\\n2018-10-12 13:26:56.022161: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 11520000 bytes, theoretical optimal value: 11520000 bytes.\\n2018-10-12 13:26:56.022669: I tensorflow/contrib/lite/toco/toco_tooling.cc:397] Estimated count of arithmetic ops: 2.49483 billion (note that a multiply-add is counted as 2 ops).\\n2018-10-12 13:26:56.023102: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023121: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023129: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023137: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023144: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023152: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023160: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023170: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023179: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023187: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023198: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023207: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023214: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023223: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023234: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023243: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023253: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023261: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023269: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023277: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023285: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023292: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023301: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023309: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023318: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023327: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023336: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023345: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023354: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023363: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023372: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023381: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023389: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023398: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023407: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023416: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023425: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023434: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023443: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023452: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023461: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023469: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023477: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023484: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023491: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023499: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023508: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:26:56.023519: F tensorflow/contrib/lite/toco/tflite/export.cc:460] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.TFLiteConverter(). Here is a list of operators for which  you will need custom implementations: TFLite_Detection_PostProcess.\\nAborted (core dumped)\\n'\r\nNone\r\n`\r\n\r\n", "Please provide following system information:\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version (use command below):\r\nPython version:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:", "INFO:\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): Anaconda \r\nTensorFlow version (use command below): 1.11\r\nPython version: 3.5\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\n\r\nHi, \r\nI get the same error as the above. And I've been using the example given in [converter python api guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/convert/python_api.md)\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(2, input_shape=(3,)))\r\nmodel.add(tf.keras.layers.RepeatVector(3))\r\nmodel.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))\r\nmodel.compile(loss=tf.keras.losses.MSE,\r\n              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\r\n              metrics=[tf.keras.metrics.categorical_accuracy],\r\n              sample_weight_mode='temporal')\r\nx = np.random.random((1, 3))\r\ny = np.random.random((1, 3, 3))\r\nmodel.train_on_batch(x, y)\r\nmodel.predict(x)\r\nkeras_file = \"keras_model.h5\"\r\ntf.keras.models.save_model(model, keras_file)\r\nconverter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()`\r\n\r\n\r\nI ran this code and this is the error I get:\r\n\r\n> RuntimeError: TOCO failed see console for info.\r\nb'Traceback (most recent call last):\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"c:\\\\users\\\\sgavvala\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\imp.py\", line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"c:\\\\users\\\\sgavvala\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__main__\", mod_spec)\\r\\n  File \"c:\\\\users\\\\sgavvala\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals)\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone\r\n\r\nMy plan is to convert a keras model with my custom layers into tflite quantized version. Figured I would start with a given example but that doesn't seem to run.", "@ymodak  the system information is as fllowed:\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n`import tensorflow as tf\r\nimport pathlib\r\narchive_dir = pathlib.Path(\"/home/caojinrong/ssd_mobilenet_v1_quantized_coco/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18\")\r\nprint(str(archive_dir))\r\ngraph_def_file = pathlib.Path(archive_dir)/\"tflite_graph.pb\"\r\ninput_arrays = [\"normalized_input_image_tensor\"]\r\noutput_arrays = [\"TFLite_Detection_PostProcess\"]\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(\r\n str(graph_def_file), input_arrays, output_arrays, input_shapes={\"normalized_input_image_tensor\":[1,300,300,3]})\r\nconverter.post_training_quantize = True\r\nmobilenet_tflite_file = graph_def_file.parent/\"ssd_mobilenet_v1_coco_quantized.tflite\"\r\nmobilenet_tflite_file.write_bytes(converter.convert())\r\n~                                                              `\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\nTensorFlow installed from (source or binary): \r\nTensorFlow version (use command below):1.12.0-dev20181012\r\nPython version: 3.5\r\nBazel version (if compiling from source):bazel-0.17.2-installer-linux-x86_64\r\nGCC/Compiler version (if compiling from source):no\r\nCUDA/cuDNN version : \r\nGPU model and memory : 1080t \r\nExact command to reproduce:", "@soumy94  Can you please open a new issue explaining your problem? You have a different system config and therefore its better to focus on it separately. Thanks!", "@ymodak I have raised a new issue [#22993](https://github.com/tensorflow/tensorflow/issues/22993)", "This is an issue with the Windows build. It's being tracked by https://github.com/tensorflow/tensorflow/issues/22897.", "Let's keep this issue focused on the TFLite_Detection_PostProcess op. @achowdhery can advise on its use.", "Closing this issue since its resolved. Feel free to reopen if still have problems. Thanks!"]}, {"number": 22937, "title": "[Features] Support more types for Partitionedvariable assginment method", "body": "This pull request enhances `PartitionedVariable` assignment function.  `List` and `PartitionedVariable` types is supported for right value. Any comments are welcome. ", "comments": ["Hi  @alextp,\r\n  This is the enhancement of https://github.com/tensorflow/tensorflow/pull/22473 . Please check it.\r\nThanks. "]}, {"number": 22936, "title": "Invalid loop strucure", "body": "I had converted a h5 file to pb file, then while loading the pb file I get the following error.\r\n\r\nTraceback (most recent call last):\r\n  File \"import_model.py\", line 244, in <module>\r\n    detections = sess.run(detectionsT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid loop structure: Loop \"mrcnn_detection/map/while/while_context\" has more than one LoopCond node: \"mrcnn_detection/map/while/LoopCond_1\" and \"mrcnn_detection/map/while/LoopCond\". This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n\r\n\r\nHave I written custom code - No, forked from - https://github.com/GustavZ/Mobile_Mask_RCNN\r\nOS Platform and Distribution- linux ubuntu 16.04\r\nTensorFlow installed from - using pip\r\nBazel version- N/A\r\nCUDA/cuDNN version- N/A\r\nGPU model and memory- N/A\r\nExact command to reproduce - convert the h5 file to pb and then load the pb file\r\nMobile device - N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@ankitesh97 Hi, request you to provide a code snippet to reproduce the error and share the following information. \r\n\r\nHave I written custom code\r\nOS Platform and Distribution\r\nTensorFlow installed from\r\nBazel version\r\nCUDA/cuDNN version\r\nGPU model and memory\r\nExact command to reproduce\r\nMobile device", "Hey, I have updated the details please check", "@skye   PTAL", "@ankitesh97 can you provide the exact commands you run in the Mobile_Mask_RCNN repo to get this error?", "> @ankitesh97 can you provide the exact commands you run in the Mobile_Mask_RCNN repo to get this error?\r\n\r\n@ankitesh97  Could you please provide the information asked above. It would help us to look into the issue. Thanks !", "@skye Just run [this](https://github.com/GustavZ/Mobile_Mask_RCNN/blob/master/notebooks/export_model.ipynb) python notebook.", "Hi, sorry for the delay. This looks like a control flow bug, which we're currently reimplementing (see https://github.com/tensorflow/community/blob/master/rfcs/20180507-cond-v2.md and https://github.com/tensorflow/community/blob/master/rfcs/20180821-differentiable-functional-while.md for details).\r\n\r\nWould you mind running your code with the following environment variables set:\r\nTF_ENABLE_WHILE_V2=1 TF_ENABLE_COND_V2=1\r\n\r\nPlease report any errors you get with these env variables set. Thanks!", "@ankitesh97  Have you tried with the above setting of environment variables ? Please keep us posted and we will reopen."]}, {"number": 22935, "title": "(Feature request) Add synthetic gradient training per \"Decoupled Neural Interfaces using Synthetic Gradients\"", "body": "## Feature request\r\n### Related papers\r\nhttps://arxiv.org/abs/1703.00522\r\n### Related blog : https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/\r\n\r\n\r\n\r\n**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\n\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n**Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**\r\nNo\r\n\r\n**TensorFlow installed from (source or binary):**\r\nsource\r\n\r\n**TensorFlow version (use command below):**\r\ntf.VERSION = 1.9.0\r\n\r\n**Python version:** python 2.7\r\n\r\n**Bazel version (if compiling from source):**\r\n0.11.1\r\n\r\n**GCC/Compiler version (if compiling from source):**\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n\r\n**CUDA/cuDNN version:**\r\n9.1\r\n\r\n**GPU model and memory:**\r\nTesla K80\r\n\r\n**Exact command to reproduce:**\r\n( No command)\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I have updated the template.", "Thanks for bringing this to our attention. Synthetic gradient is a neat approach, we will note this down. You can also implement it yourself using TensorFlow and common python libraries.   \r\n\r\n"]}, {"number": 22934, "title": "[Intel MKL] Update Eigen to get new optimization contributed by Intel", "body": " Update Eigen to get these feature:\r\n1. broadcasting optimization for channel first shape\r\n2. fix a crush when build Tensorflow with Eigen\r\n\r\nmodified:\r\n- tensorflow/workspace.bzl\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["Update Eigen to get broadcasting optimization for 'channel first' shape, detail is here: http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1567", "@yifeif  PTAL", "@rmlarsen would know the best about eigen version :)", "Nagging Reviewer @rmlarsen: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Eigen has been updated to the newest version, close this PR."]}, {"number": 22933, "title": "Tensor .eval() on loaded tensor", "body": "Hi, I can convert tensor to numpy array, when creating a new const\r\nnp_array = np.random.rand(3, 2)\r\nwith tf.Session() as sess:\r\n    tensor = tf.constant(np_array)\r\n    numpy_array_2 = tensor.eval()\r\n    print(numpy_array_2)\r\n\r\nworks fine, everything is ok, but when I load a saved tensor - this no longer works\r\n\r\nloaded_graph = tf.Graph()\r\n\r\nwith tf.Session(graph=loaded_graph) as sess:\r\n    loader = tf.train.import_meta_graph('checkpoints/vgg_16/test_animals/model-18000.meta')\r\n    loader.restore(sess, 'checkpoints/vgg_16/test_animals/model-18000')\r\n        \r\n    tensor = loaded_graph.get_tensor_by_name('conv1_1/Conv2D:0')\r\n    #tf.train.start_queue_runners(sess)    \r\n    arr = tensor.eval()\r\n\r\nat this point it freezes, I have tried running .start_queue_runners(sess) before, but it says that session is closed, if I run it before .import_meta_graph(path), .eval() is still frozen.\r\n\r\nIs there any other way to get numbers from that tensor except sending it to numpy array?\r\nWhat should I do to get .eval() working?", "comments": ["Tensor shape=(32, 56, 56, 64), with smaller tensors it works quick, so what should I do to enhance execution of .eval()?"]}, {"number": 22932, "title": "shard segment reduction op to multi threads", "body": "shards the segment reduction op to multi threads when computing with CPU, so as to speed up the op.\r\n\r\nIn my test, the time may reduce to about half of original version.\r\n\r\nSome other codes are modified automatically when I do the code format with: \r\n```\r\nclang-format-3.9 -i segment_reduction_ops.cc --style=google\r\n```\r\nSo I'm wondering if there's  something wrong with my clang-format.\r\n\r\n## Env\r\n\r\n* CPU: 8 cores\r\n* Mem: 14G\r\n\r\nthe benchmark program: https://gist.github.com/shengofsun/cf9b203cb722a290667360770923c55f", "comments": ["@caisq could you please take a look at this patch?", "@caisq PTAL", "@gbaned I work on XLA and sometimes StreamExecutor, but not TensorFlow.  I get asked to do a lot of TensorFlow reviews and I have to say \"no\" every time.  Is there something I can do to fix this?", "@shengofsun could you please resolve the conflicts? Thanks!", "@shengofsun gentle ping to resolve the conflicts. Thanks!", "@shengofsun Did you get a chance to look on conflicts? Please let us know on the update. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 22931, "title": "Reindex broken in tf docker image (v1.11.0) -> Broken numpy dependency!", "body": "# TLDR\r\n\r\nThe pandas, which is distributed with the tf docker image is broken!\r\n\r\n# Description\r\n\r\nI'm new to tf and I'm walking thru the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro) at the moment. [First Steps with TF](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/programming-exercises) point me to the [Quick Introduction to pandas](https://colab.research.google.com/notebooks/mlcc/intro_to_pandas.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=pandas-colab&hl=de) notebook. \r\n\r\n## Run example on colab\r\n\r\n Based on this. I created the following MWE on colab (actually, it is only pandas code):\r\n\r\nhttps://colab.research.google.com/drive/19uDE_H4AtpLaEL6INrRrDMXkdANsNr69\r\n\r\nIt basically just does the following: \r\n\r\n1) Import the california housing data set\r\n2) reorder the data set\r\n3) prints the merged dataset\r\n\r\nIf you run my notebook, you can see that step 3 prints up to 20 different values for the data set (makred with `left only` and `right only`):\r\n\r\n```\r\n`   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\r\n0      -114.3      34.2                15.0       5612.0          1283.0   \r\n1      -114.5      34.4                19.0       7650.0          1901.0   \r\n2      -114.6      33.7                17.0        720.0           174.0   \r\n3      -114.6      33.6                14.0       1501.0           337.0   \r\n4      -114.6      33.6                20.0       1454.0           326.0   \r\n..        ...       ...                 ...          ...             ...   \r\n15     -117.3      33.2                13.0       3619.0           791.0   \r\n16     -118.3      33.8                25.0       4177.0           832.0   \r\n17     -117.7      34.0                25.0       1859.0           463.0   \r\n18     -118.1      34.0                50.0       1146.0           238.0   \r\n19     -118.5      34.0                41.0       1240.0           320.0   \r\n\r\n    population  households  median_income  median_house_value      _merge  \r\n0       1015.0       472.0            1.5             66900.0   left_only  \r\n1       1129.0       463.0            1.8             80100.0   left_only  \r\n2        333.0       117.0            1.7             85700.0   left_only  \r\n3        515.0       226.0            3.2             73400.0   left_only  \r\n4        624.0       262.0            1.9             65500.0   left_only  \r\n..         ...         ...            ...                 ...         ...  \r\n15      1759.0       806.0            2.8             98500.0  right_only  \r\n16      2123.0       789.0            5.1            446800.0  right_only  \r\n17      1070.0       374.0            2.5            187500.0  right_only  \r\n18       579.0       213.0            3.0            172600.0  right_only  \r\n19       711.0       304.0            3.3            318100.0  right_only  \r\n```\r\n\r\n## Run example on local python\r\n\r\nIn have Python 3.6.6 and I install tf with `pip install  tensorflow`. \r\n\r\nNow I run the same code in a python script as follows (on Windows 10):\r\n\r\n```\r\npython Reorder.py\r\n```\r\n\r\nThe reordered data is exactly the same as the imported data (all are marked with `both`):\r\n\r\n```\r\nlongitude  latitude  housing_median_age  total_rooms  total_bedrooms  population  households  median_income  median_house_value _merge\r\n0     -114.3      34.2                15.0       5612.0          1283.0      1015.0       472.0            1.5             66900.0   both\r\n1     -114.5      34.4                19.0       7650.0          1901.0      1129.0       463.0            1.8             80100.0   both\r\n2     -114.6      33.7                17.0        720.0           174.0       333.0       117.0            1.7             85700.0   both\r\n3     -114.6      33.6                14.0       1501.0           337.0       515.0       226.0            3.2             73400.0   both\r\n4     -114.6      33.6                20.0       1454.0           326.0       624.0       262.0            1.9             65500.0   both\r\n5     -114.6      33.6                29.0       1387.0           236.0       671.0       239.0            3.3             74000.0   both\r\n6     -114.6      33.6                25.0       2907.0           680.0      1841.0       633.0            2.7             82400.0   both\r\n7     -114.6      34.8                41.0        812.0           168.0       375.0       158.0            1.7             48500.0   both\r\n8     -114.6      33.6                34.0       4789.0          1175.0      3134.0      1056.0            2.2             58400.0   both\r\n9     -114.6      34.8                46.0       1497.0           309.0       787.0       271.0            2.2             48100.0   both\r\n```\r\n\r\n## Run example in tf docker container\r\n\r\nNow I run the same python script in the [tf docker container](https://hub.docker.com/r/tensorflow/tensorflow/) (v1.11.0) as follows (on Windows 10):\r\n\r\n```bash\r\ndocker run --rm -it -v C:\\Users\\boldt\\docker\\tf\\scripts\\:/scripts tensorflow/tensorflow:1.11.0-py3 python /scripts/Reorder.py\r\n```\r\n\r\nThe result is the same. The reordered data is exactly the same as the imported data (all are marked with `both`):\r\n\r\n# Conclusion\r\n\r\nBecause of this, I am unable to randomize the data set and split it in a training and a test set using the tf docker container. Currently, my more complex example always results the same training and test sets.", "comments": ["Since my MWE is reduced to only pandas code without tf dependencies, I assumed an issue in pandas:\r\n\r\n* colab uses pandas `0.22.0`\r\n* tf docker container (v1.11.0) uses pandas ``0.23.4``\r\n* my local python installation uses pandas ``0.23.4``\r\n\r\nI downgraded pandas on my local environment now:\r\n\r\n* 0.22.0 (`pip install --force-reinstall pandas==0.22.0`) - Broken\r\n* 0.23.0 (`pip install --force-reinstall pandas==0.23.0`) - Broken\r\n\r\n# Conclusion\r\n\r\n<!-- \r\nThe pandas version >=0.23.0, which is distributed with the current tf docker image, is broken.\r\n-->\r\n\r\n@see https://github.com/pandas-dev/pandas/issues/23104", "The [pandas guys figured out](https://github.com/pandas-dev/pandas/issues/23104), that it's a [bug in numpy](https://github.com/numpy/numpy/issues/11975)", "@boldt Hi, as there is a new ticket raised in Pandas/Numpy, feel free to close the issue here. Or else if you would like to add more points on this, please specify. Thank you !", "@HaebinShin Since the index does not work in the tf container atm, I'll not close the issue. \r\nI'll close it as soon the new numpy version is released and available into the container\r\n", "[Numby v1.15.3 was released on Oct, 22nd](https://github.com/numpy/numpy/releases/tag/v1.15.3). I just checked the tag `nightly-py3`. It contains the newest numpy version. \r\n\r\nI'll close this ticket as soon the new docker image 1.12.0 is released (rc1 is out and does not contain the newest numpy yet)\r\n", "Just saw, that 1.12.0 is pushes to docker hub. Thus I'm closing it.", "> Just saw, that 1.12.0 is pushes to docker hub. Thus I'm closing it.\r\n\r\nThis is still open :)", "Nagging Assignee @harshini-gadige: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "> This is still open :)\r\n\r\nThanks "]}]