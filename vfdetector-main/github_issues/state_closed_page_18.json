[{"number": 54928, "title": "Move LiftAsFunctionCall to a util library", "body": "Move LiftAsFunctionCall to a util library\n", "comments": []}, {"number": 54927, "title": "Add XNNPACK delegate per operator profiling to TFLite", "body": "Add XNNPACK delegate per operator profiling to TFLite\n", "comments": []}, {"number": 54926, "title": "[ROCm] ROCm bef thunk blasgemm refactor", "body": "/cc @chsigg @hanbinyoon ", "comments": []}, {"number": 54925, "title": "TF_CPP_MIN_VLOG_LEVEL and TF_CPP_MIN_LOG_LEVEL do not output log info in TF 2.8.0/2.9", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20\r\n- TensorFlow installed from (source or binary): Yes\r\n- TensorFlow version (use command below): v2.8.0-2-ge994fb9c3ad and 2.9 (master branch)\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nSetting TF_CPP_MIN_LOG_LEVEL and TF_CPP_MIN_VLOG_LEVEL does not work with the latest release 2.8.0 and 2.9 of the master branch (both built from source) -- Nor exporting an envvar before launching the TF script, nor using the os module in the code (tried setting it before and after the tensorflow module import).\r\n\r\nFor source build, I followed the steps mentioned in the [official guide](https://www.tensorflow.org/install/source\r\n), using the build command: \r\n`bazel build //tensorflow/tools/pip_package:build_pip_package` \r\n\r\nBoth builds are CPU only without CUDA nor ROCm support. \r\n\r\n**Describe the expected behavior**\r\nAs discussed in #31870, setting TF_CPP_MIN_LOG_LEVEL=0 and/or TF_CPP_MIN_VLOG_LEVEL=2 should show logging/debugging information regarding the C++ implementation inner operations, memory allocations, etc. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport os \r\n\r\n#os.environ['TF_CPP_MIN_VLOG_LEVEL'] = \"2\"\r\n\r\nimport tensorflow as tf\r\n\r\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = \"2\"\r\n\r\na = tf.Variable(tf.zeros(shape=(2)), name=\"a\")\r\n\r\nprint(a)\r\n\r\n```\r\n\r\n", "comments": ["Hi @chunduriv ! Could you please look at this issue? It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/769d205b6f4760f538659d358714c4c7/github_54925_2-8.ipynb#scrollTo=zQ0fZ5mhPixF), [2.8](https://colab.sandbox.google.com/gist/mohantym/42528c8349ad1091584bc16f3495cce2/github_54925_2-8.ipynb#scrollTo=8sTRdFNUPQ4M) and[ nightly.](https://colab.sandbox.google.com/gist/mohantym/95db88ce1b0737980d98fe80111069bc/github_54925_2-8.ipynb#scrollTo=8sTRdFNUPQ4M)", "Update: I just read in the RELEASE.md file that the TF_CPP_MIN_VLOG_LEVEL environment variable has been renamed to TF_CPP_MAX_VLOG_LEVEL. I tried it and it worked only when `os.environ['TF_CPP_MAX_VLOG_LEVEL'] = \"3\"` is set before importing the tensorflow module. Closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54925\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54925\">No</a>\n"]}, {"number": 54924, "title": "Add Arm Ltd to AUTHORS", "body": "Adding Arm Ltd to AUTHORS file.", "comments": []}, {"number": 54922, "title": "Fix a few minor formatting issues in the new bug template for TF Lite in Play Services.", "body": "Fix a few minor formatting issues in the new bug template for TF Lite in Play Services.\n", "comments": []}, {"number": 54921, "title": "Avoid including the same .cc file in srcs of multiple rules.", "body": "Avoid including the same .cc file in srcs of multiple rules.\n\nEnsure that the .cc files for variable ops kernels are only included in\nthe \"srcs\" of one rule (\"variable_ops_kernels\"), and change the other\nrule that had them listed in srcs (\"builtin_ops_kernels\") to instead just\nhave a dependency on variable_ops_kernels. This is cleaner and avoids link\nerrors if you try to link both builtin_op_kernels and variable_op_kernels\ninto the same target.\n", "comments": []}, {"number": 54920, "title": "Fixed Input check in #54849", "body": "Added a ValueError for checking whether splits[0] is 0 or not.", "comments": ["Also worth noting here that there are other requirements that don't get checked -- e.g., that the values are monotonic increasing.\r\n", "Please don't open PRs against release branches unless they are cherrypicking from master."]}, {"number": 54919, "title": "Disable failing test on TFRT", "body": "Disable failing test on TFRT\n", "comments": []}, {"number": 54918, "title": "Added function to convert DataType to Glsl shader type.", "body": "Added function to convert DataType to Glsl shader type.\n", "comments": []}, {"number": 54917, "title": "Visibility changes for the internal builds", "body": "Visibility changes for the internal builds\n", "comments": []}, {"number": 54916, "title": "problem with the eager execution", "body": "I retrain the resnet152v2 from keras to calassify my own set of image. I did so using a data generator that looks like this:\r\n```\r\nclass My_Custom_Generator(Sq):\r\n\r\n    def __init__(self, image_filenames, labels, batch_size):\r\n        self.image_filenames = image_filenames\r\n        self.labels = labels\r\n        self.batch_size = batch_size\r\n\r\n    def __len__(self):\r\n        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(int)\r\n\r\n    def __getitem__(self, idx):\r\n        batch_x = self.image_filenames[idx * self.batch_size: (idx + 1) * self.batch_size]\r\n        batch_y = self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\r\n        return np.array([\r\n            resize(imread('E:\\\\data\\\\allFrames\\\\' + str(file_name)), (559, 331))\r\n            for file_name in batch_x]) / 255.0, np.array(batch_y)\r\n```\r\nfor training the model the code looks like this:\r\n```\r\n baseModel = EfficientNetB2(weights=None, include_top=False,\r\n                               input_tensor=Input(shape=(559, 331, 3)))\r\n    # construct the head of the model that will be placed on top of the\r\n    # the base model\r\n    headModel = baseModel.output\r\n    headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\r\n    headModel = Flatten(name=\"flatten\")(headModel)\r\n    headModel = Dense(512, activation=\"relu\")(headModel)\r\n    headModel = Dropout(0.5)(headModel)\r\n    headModel = Dense(5, activation=\"softmax\")(headModel)\r\n    # place the head FC model on top of the base model (this will become\r\n    # the actual model we will train)\r\n    model = Model(inputs=baseModel.input, outputs=headModel)\r\n    # loop over all layers in the base model and freeze them so they will\r\n    # *not* be updated during the training process\r\n    for layer in baseModel.layers:\r\n        layer.trainable = False\r\nopt = SGD(learning_rate=0.005, momentum=0.9, decay=1e-4 / 50)\r\n    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\r\n                  metrics=[\"accuracy\"])\r\n    model.summary()\r\n    NAME = 'resnet152-559x331-{}'.format(int(time.time()))\r\n    tboard_log_dir = os.path.join(\"C:\\\\Users\\\\dsyr\\\\PycharmProjects\\\\VideoProcessingProject\\\\tmp\\\\checkpoint\", NAME)\r\n    model_checkpoint_callback = ModelCheckpoint(\r\n        filepath=tboard_log_dir,\r\n        save_weights_only=False,\r\n        monitor='val_accuracy',\r\n        mode='max',\r\n        save_freq=1000,\r\n        save_best_only=False)\r\n    print(\"[INFO] training model...\")\r\n    model.fit(my_training_batch_generator,\r\n                  steps_per_epoch=int(621108 // batch_size),\r\n                  epochs=50,\r\n                  verbose=1,\r\n                  validation_data=my_validation_batch_generator,\r\n                  validation_steps=int(155277 // batch_size),\r\n                  callbacks=[model_checkpoint_callback])\r\n```\r\ntill there there was no problem, I've a couple of thousand images to validate and produce the output. The issue is that I've not been able to get the same prediction with and without the batch generator. \r\nwhen I do model.predict(img, verbose=0, use_multiprocessing=True, workers=4)\r\nwhere img is a numpy array that is the same as generated by the batch I get this:\r\n\r\n> \r\n> in user code:\r\n>     File \"C:\\Users\\dsyr\\anaconda3\\envs\\intel\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\r\n>         return step_function(self, iterator)\r\n>     File \"C:\\Users\\dsyr\\anaconda3\\envs\\intel\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\r\n>         outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>     File \"C:\\Users\\dsyr\\anaconda3\\envs\\intel\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\r\n>         outputs = model.predict_step(data)\r\n>     File \"C:\\Users\\dsyr\\anaconda3\\envs\\intel\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\r\n>         return self(x, training=False)\r\n>     File \"C:\\Users\\dsyr\\anaconda3\\envs\\intel\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n>         raise e.with_traceback(filtered_tb) from None\r\n>     File \"C:\\Users\\dsyr\\anaconda3\\envs\\intel\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\r\n>         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\r\n> \r\n>     ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 559, 331, 3), found shape=(None, 331, 559, 3)\r\n\r\nthe exact same error happens if I run the prediction (or training) with the comand \"disable_eager_execution()\"\r\nit is to say with eager execution eneable batch processing can be done, with the picture in the format I wanted (559, 331, 3) if eager execution is disabled I cant. I've tried even transposing the picture but the result is widly different than the real forecast done in the batch. \r\n \r\nI'm using intel-tensorflow 2.8.0 and intel python 3.9.7 on a windows computer without GPU", "comments": ["@hapicatto ,\r\nCan you please take a look at this SO link [1](https://stackoverflow.com/questions/70067588/valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer-expect) and [2](https://stackoverflow.com/questions/67047026/valueerror-input-0-is-incompatible-with-layer-model-expected-shape-none-1499) with the similar error.It helps.Thanks!", "Hello, thanks for your response. I've looked at those before and I'm not declaring the batch size in the network architecture. \r\nIn fact I can feed the network without the data generator but the image has to be transposed. if you notice, the dimensions are flipped in the response.  \r\nI can provide access to a colab notebook so you can check code.", "Link to the colab code: https://colab.research.google.com/drive/1GdzxZn9LjE4JdzW1n0qGmubsTFpDuTTo?usp=sharing\r\nLink to the data and model file: https://drive.google.com/drive/folders/1ZInGRY0FB25LaFcE9vCG1vgqqaGBS2nW?usp=sharing", "@tilakrayal could you check what I mentioned? the entrances to SO that you referenced were not useful. I don't think the bug label was removed correctly\r\nthanks", "@hapicatto ,\r\nSorry for the delay response.\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54916\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54916\">No</a>\n"]}, {"number": 54915, "title": "Reduce the dependencieson dtypes.py", "body": "Reduce the dependencieson dtypes.py\n", "comments": []}, {"number": 54914, "title": "Added GpuInfo argument to virtual GPUObjectDescriptor::PerformConstExpr.", "body": "Added GpuInfo argument to virtual GPUObjectDescriptor::PerformConstExpr.\n", "comments": []}, {"number": 54913, "title": "Added resolver of const expressions for GpuObjects.", "body": "Added resolver of const expressions for GpuObjects.\n", "comments": []}, {"number": 54912, "title": "FIXES github issue #47546", "body": "FIXES github issue #47546\n", "comments": []}, {"number": 54910, "title": "Stage data on gpu pinned memory during transfer", "body": "Stage data on gpu pinned memory during transfer\n", "comments": []}, {"number": 54909, "title": "Stage data on gpu pinned memory during transfer", "body": "Stage data on gpu pinned memory during transfer\n", "comments": []}, {"number": 54908, "title": "Minor internal changes", "body": "Minor internal changes\n", "comments": []}, {"number": 54907, "title": "Accept default attributes in quantized composite functions", "body": "Accept default attributes in quantized composite functions\n", "comments": []}, {"number": 54906, "title": "This is just for testing", "body": "This is just for testing\n", "comments": []}, {"number": 54905, "title": "[TF:TRT] Enable out-of-tree op converter registration", "body": "This change moves the static registration system symbols for TF-TRT into `libtensorflow_framework.so`. This enables users to register custom op converters with TF-TRT without requiring TF to be integrated into their build system. Users will only need a C++ compiler and a `pip` installed Tensorflow package to register a custom converter. It also adds utilities to print out the list of op converters registered with TF-TRT.", "comments": ["@bixia1 for review when available", "@christopherbate  Can you please check @bixia1's comments and keep us posted ? Thanks!", "rebased and comments addressed", "@bixia1 Any progress on this?"]}, {"number": 54904, "title": "Added additional compiler `-mfp16-format=ieee`", "body": "For successful cross-compilation for ARM using CMake, we should add `-mfp16-format=ieee` an additional compiler option to fix `unknown type name 'float16x8_t'`. For more  details please refer [this issue](https://github.com/tensorflow/tensorflow/issues/54337#issuecomment-1057618003).Thanks!", "comments": []}, {"number": 54903, "title": "Update the CMake ARM compilation note when enabling XNNPACK, i.e. suggesting adding \"-mfp16-format=ieee\" as an additional compiler flag.", "body": "Update the CMake ARM compilation note when enabling XNNPACK, i.e. suggesting adding \"-mfp16-format=ieee\" as an additional compiler flag.\n", "comments": []}, {"number": 54902, "title": "Use correct dimensions when legalizing tf.IRFFT to mhlo", "body": "Use correct dimensions when legalizing tf.IRFFT to mhlo\n\nThis fixes a bug in tf legalization: While we need to calculate the sizes of tensor slices using after running the IRFFT, the fft_length before running the non-inverse FFT is the argument to mhlo.fft, according to the HLO specification.\n\nThis fix unblocks merging the mhlo.fft verifier.\n", "comments": []}, {"number": 54901, "title": "[mhlo] Verifier for mhlo.cholesky", "body": "[mhlo] Verifier for mhlo.cholesky\n", "comments": []}, {"number": 54900, "title": "MirroredStrategy with custom model throws AttributeError: 'NoneTensorSpec' object has no attribute 'rank'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n- Python version: 3.10.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.6/8.3\r\n- GPU model and memory: GTX1080Ti 11GB\r\n\r\n**Describe the current behavior**\r\nWhen training a custom model with `keras.fit` and `MirroredStrategy` an `AttributeError` is thrown. Without the `MirroredStrategy` training proceeds without error. I also notice that the console output does not mention something like `INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)` even though the strategy reports having 3 replicas in sync\r\n\r\nCode snippet\r\n```\r\n    ### ----- create datasets\r\n    \r\n    strategy = tf.distribute.MirroredStrategy()\r\n    print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\r\n\r\n    # with strategy.scope():\r\n        model = MyModel()\r\n\r\n        # Setup the optimiser with the learning rate schedule\r\n        lr_schedule = CosineDecayWithWarmup(\r\n            initial_learning_rate=args.lr,\r\n            decay_steps=args.num_epochs,\r\n            warmup_steps=args.warmup_steps,\r\n            warmup_learning_rate=args.warmup_lr,\r\n            hold_base_rate_steps=args.hold_base_rate_steps,\r\n        )\r\n        optimiser = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\r\n\r\n        # Compile the model\r\n        model.compile(optimizer=optimiser, loss=MyLoss())\r\n\r\n    # Build the model and print a summary\r\n    model.build(input_shape=(args.batch_size, image_shape[0], image_shape[1], 3))\r\n    model.summary()\r\n\r\n    history = model.fit(\r\n        training_dataset,\r\n        epochs=args.num_epochs,\r\n        validation_data=validation_dataset,\r\n        callbacks=callbacks,\r\n    )\r\n```\r\n\r\nConsole output\r\n```\r\n2022-03-03 14:30:59.481815: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-03-03 14:31:00.853869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8972 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\r\n2022-03-03 14:31:00.854710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10405 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\r\n2022-03-03 14:31:00.855231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10405 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:43:00.0, compute capability: 6.1\r\nNumber of devices: 3\r\n\r\n#### ----- MODEL SUMMARY ----\r\n\r\nTraceback (most recent call last):\r\n  File \"$HOME/network/train.py\", line 279, in <module>\r\n    main()\r\n  File \"$HOME/network/train.py\", line 242, in main\r\n    history = model.fit(\r\n  File \"$HOME/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"$HOME/.local/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/distribute.py\", line 513, in get_static_batch_dim\r\n    if output_shape.rank is None:\r\nAttributeError: 'NoneTensorSpec' object has no attribute 'rank'\r\n```\r\n\r\n**Describe the expected behavior**\r\nNetwork trains with the `MirroredStrategy` with no error.\r\n", "comments": ["@Bidski ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.", "Unfortunately a reproducible example is going to be difficult to provide in the short term. I suspect that [this commit](https://github.com/tensorflow/tensorflow/commit/5a8ef901f4f0b50f798feb5429ed4b962a8153a2) might resolve the problem but I don't currently have the time to build tensorflow from scratch to test this theory. Are there any pre-built python wheels (or similar) that contain this commit yet?\r\n\r\nIs there any way for me to construct my `tf.data.Dataset` such that this function will work? I am already calling `.batch()` on my dataset.\r\n", "@Bidski ,\r\nWithout the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide the code.Thanks!", "@tilakrayal the code is closed source so I cant provide that and constructing a MWE that show-cases the problem is difficult since I dont know where the problem lies. Are you able to provide any insight at all into what might be going on so that I may try and create a MWE for you?\r\n\r\nI tried building tensorflow from source (from current master) to see if the commit I referenced solved the problem and it didnt seem to help.", "Ok, I have figured out the problem. I was returning a tuple `(x, y, None)` (being `x`, `y`, `sample_weights`) from my `tf.data.Dataset`. This resulted in the `sample_weights` having a `NoneTensorSpec` which `compute_batch_size()` was not happy about. \r\n\r\nChanging my `tf.data.Dataset` to return just `(x, y)` fixes the problem", "@tilakrayal please re-read my last comment. I have now solved the problem and I have described what I needed to do in order to fix the problem that I was experiencing.", "@Bidski ,\r\nGlad the issue got resolved.Could you please feel free to move this issue to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54900\">No</a>\n"]}, {"number": 54899, "title": "Unable to find valid certification path", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.7.0\r\n- Installed using virtualenv? pip? conda?: source build\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n~/tensorflow$ ./configure\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /home/user/miniconda3/envs/tf-2.5/bin/python3]: /home/user/miniconda3/envs/tf-2.5/bin/python3]^H\r\n\r\n\r\nInvalid python path: /home/user/miniconda3/envs/tf-2.5/bin/python3 cannot be found.\r\nPlease specify the location of python. [Default is /home/user/miniconda3/envs/tf-2.5/bin/python3]: /home/user/miniconda3/envs/tf-2.5/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /home/user/miniconda3/envs/tf-2.5/lib/python3.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/user/miniconda3/envs/tf-2.5/lib/python3.7/site-packages]\r\n/home/user/miniconda3/envs/tf-2.5/lib/python3.7/site-packages\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n\r\n$ bazel build  --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=252\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/user/miniconda3/envs/tf-2.5/bin/python3 --action_env PYTHON_LIB_PATH=/home/user/miniconda3/envs/tf-2.5/lib/python3.7/site-packages --python_path=/home/user/miniconda3/envs/tf-2.5/bin/python3\r\nINFO: Found applicable config definition build:short_logs in file /home/user/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/user/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file /home/user/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:linux in file /home/user/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/user/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Repository io_bazel_rules_closure instantiated at:\r\n  /home/user/tensorflow/WORKSPACE:11:14: in <toplevel>\r\n  /home/user/tensorflow/tensorflow/workspace3.bzl:6:17: in workspace\r\nRepository rule http_archive defined at:\r\n  /home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nWARNING: Download from https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nERROR: An error occurred during the fetch of repository 'io_bazel_rules_closure':\r\n   Traceback (most recent call last):\r\n        File \"/home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/bazel_tools/tools/build_defs/repo/http.bzl\", line 111, column 45, in _http_archive_impl\r\n                download_info = ctx.download_and_extract(\r\nError in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz, https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz] to /home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/io_bazel_rules_closure/temp13563286205250891784/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nERROR: no such package '@io_bazel_rules_closure//closure': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz, https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz] to /home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/io_bazel_rules_closure/temp13563286205250891784/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nINFO: Elapsed time: 28.763s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\n", "comments": ["Hi @taegeonum ! Can you try following steps ?\r\n\r\nStep-1: bazel clean --expunge ( it will clean previous builds and cache)\r\nStep-2: Uninstall and Reinstall JDK latest version.\r\nStep-3: Try again  Bazel build with 3.7.2  or 4.2.2 for 2.5 or Bazel 4.2.2 (with   with branch 2.8).\r\n\r\nAttaching relevant threads for reference. [1](https://stackoverflow.com/questions/21076179/pkix-path-building-failed-and-unable-to-find-valid-certification-path-to-requ),[ 2](https://github.com/tensorflow/tensorflow/issues/7497#issuecomment-355743662). Thanks!", "@mohantym Thanks for the reply! I will share the result after trying the steps. ", "@mohantym Hmm, I reinstalled my jdk (jdk 11) but got the following error \r\n```\r\n  /home/user/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nERROR: no such package '@upb//bazel': java.io.IOException: Error downloading [https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz] to /home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/upb/temp5565003095748082168/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nINFO: Elapsed time: 19.725s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\nThe error message is different from the previous one, but also has the same exception (unable to find valid certification path). \r\nPerhaps is this because of firewall? but I'm using proxy and `wget https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz` is successful. \r\n\r\nThe attatched reference explains registering certifcate in java toolchain, but I don't have `sudo` permission so cannot execute the keytool command that requires `sudo` permission. ", "Hi @gadagashwini ! Could you please look at this issue?", "@taegeonum, Add these lines in Tensorflow WORKSPACE file\r\n\r\n```\r\nhttp_archive(\r\n    name = \"io_bazel_rules_closure\",\r\n    sha256 = \"5b00383d08dd71f28503736db0500b6fb4dda47489ff5fc6bed42557c07c6ba9\",\r\n    strip_prefix = \"rules_closure-308b05b2419edb5c8ee0471b67a40403df940149\",\r\n    urls = [\r\n        \"http://mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",\r\n        \"https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",  # 2020-02-14\r\n    ],\r\n)\r\n```", "@gadagashwini I got the following error\r\n```\r\nERROR: /home/user/tensorflow/WORKSPACE:25:1: name 'http_archive' is not defined\r\nERROR: error loading package 'external': Package 'external' contains errors\r\n```", "@taegeonum, \r\nTry \r\n\r\n```\r\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\r\n\r\nhttp_archive(\r\n    name = \"io_bazel_rules_closure\",\r\n    sha256 = \"5b00383d08dd71f28503736db0500b6fb4dda47489ff5fc6bed42557c07c6ba9\",\r\n    strip_prefix = \"rules_closure-308b05b2419edb5c8ee0471b67a40403df940149\",\r\n    urls = [\r\n        \"http://mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",\r\n        \"https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",  # 2020-02-14\r\n    ],\r\n)\r\n```", "@gadagashwini I added the code in WORKSPACE, but still got this error:\r\n```\r\n  /home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz failed: class javax.net.ssl.SSLHandshakeException\r\nPKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nWARNING: Download from https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provi\r\nder.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nERROR: An error occurred during the fetch of repository 'io_bazel_rules_closure':\r\n   Traceback (most recent call last):\r\n        File \"/home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/bazel_tools/tools/build_defs/repo/http.bzl\", line 111, column 45, in _http_archive_impl\r\n                download_info = ctx.download_and_extract(\r\nError in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz, h\r\nttps://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz] to /home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/io_bazel_rules_closure/temp1303\r\n3208819056549827/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nERROR: no such package '@io_bazel_rules_closure//closure': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee047\r\n1b67a40403df940149.tar.gz, https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz] to /home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/io_\r\nbazel_rules_closure/temp13033208819056549827/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification pa\r\nth to requested target\r\n```", "@taegeonum, You need to do `HTTPS SSL certificate` setup on your browser. Follow the workaround mention [#9619030](https://stackoverflow.com/a/12524960/14290244). Thanks!", "@gadagashwini I added the certificate to java keystore as suggested in the link, but still got PKIX error (but the download file is different).\r\n```\r\nRepository rule http_archive defined at:\r\n  /home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb930889866229/external/bazel_tools/tools/build_defs/repo/htt\r\np.bzl:336:31: in <toplevel>\r\nERROR: no such package '@io_bazel_rules_go//go': java.io.IOException: Error downloading [https://github.com/bazelbuild/rule\r\ns_go/releases/download/0.18.5/rules_go-0.18.5.tar.gz] to /home/user/.cache/bazel/_bazel_taegeon.um/f9f5c94ad789d21bcfdb9308\r\n89866229/external/io_bazel_rules_go/temp12392403403281936150/rules_go-0.18.5.tar.gz: PKIX path building failed: sun.securit\r\ny.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\n````", "@gadagashwini I also added github.com cert to java keystore, but still got the same error. Is there any way  to change `https` to `http`?", "Or is there a workaround for downloading all dependencies in my local machine, and using the downloaded files during bazel build? ", "I resolved this issue by changing my proxy server. Thanks for your help", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54899\">No</a>\n"]}, {"number": 54898, "title": "Internal document update", "body": "Internal document update\n\nOn the TFLite 16x8 quantization operator coverage.\n", "comments": []}, {"number": 54897, "title": "PR #54504: [ROCm] Syncing contents of the rocm configuration files with the ROCm fork", "body": "PR #54504: [ROCm] Syncing contents of the rocm configuration files with the ROCm fork\n\nImported from GitHub PR https://github.com/tensorflow/tensorflow/pull/54504\n\nThis commit syncs the contents of the rocm configuration files (in the `third_party/gpus/` directory) from the ROCm TF fork.\n\nThe changes mostly involves updates asssociated with the newer ROCm reelases (4.5, 5.0), like\n* addition of new libs (hipBlas, hipSolver)\n* new location for rocm version information, + new routine to provide access to it\n* explicitly adding `-lstdc++` linker flag as the last option to the linker command\n  * see this PR for more details - https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/1311\n\n-------------------------\n\n/cc @cheshire @chsigg \nCopybara import of the project:\n\n--\n74e9961ac19004143f23ff7fa5a0cb0d447066de by Deven Desai <deven.desai.amd@gmail.com>:\n\n[ROCm] Syncing contents of the rocm configuration files with the ROCm fork\n\nThis commit syncs the contents of the rocm configuration files (in the `third_party/gpus/` directory) from the ROCm TF fork.\n\nThe changes mostly involves updates asssociated with the newer ROCm reelases (4.5, 5.0), like\n* addition of new libs (hipBlas, hipSolver)\n* new location for rocm version information, + new routine to provide access to it\n* explicitly adding `-lstdc++` linker flag as the last option to the linker command\n  * see this PR for more details - https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/1311\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/54504 from ROCmSoftwarePlatform:deven/upstream_rocm_config_changes 74e9961ac19004143f23ff7fa5a0cb0d447066de\n", "comments": []}]