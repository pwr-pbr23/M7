[{"number": 29206, "title": "Deep Learning VM  train failed", "body": "on GCP\r\n('v1.13.1-1-g0bcc025', '1.13.1')\r\n\r\n```\r\n# TensorFlow already pre-baked on the image\r\ncd src/tensorflow\r\nbazel run -c opt --copt=-mavx2 --copt=-mfma \\\r\ntensorflow/examples/speech_commands:train -- \\\r\n--model_architecture=tiny_conv --window_stride=20 --preprocess=average \\\r\n--wanted_words=\"yes,no\" --silence_percentage=25 --unknown_percentage=25 --quantize=1\r\n```\r\n\r\nIn file included from ./tensorflow/core/profiler/internal/tfprof_show.h:32:0,\r\n                 from ./tensorflow/core/profiler/internal/tfprof_show_multi.h:32,\r\n                 from ./tensorflow/core/profiler/internal/tfprof_code.h:31,\r\n                 from ./tensorflow/core/profiler/internal/tfprof_stats.h:37,\r\n                 from tensorflow/core/profiler/internal/tfprof_stats.cc:16:\r\n./tensorflow/core/profiler/internal/tfprof_tensor.h: In member function 'bool tensorflow::tfprof::TFProfTensor::AddValue(const T&, tensorflow::tfprof::TFProfTensorProto*)':\r\n./tensorflow/core/profiler/internal/tfprof_tensor.h:79:3: warning: no return statement in function returning non-void [-Wreturn-type]\r\n   }\r\n   ^\r\nIn file included from ./tensorflow/core/profiler/internal/tfprof_stats.h:40:0,\r\n                 from tensorflow/core/profiler/internal/tfprof_stats.cc:16:\r\n./tensorflow/core/profiler/internal/tfprof_op.h: In member function 'virtual bool tensorflow::tfprof::TFOp::ShouldShowIfExtra(const tensorflow::tfprof::ShowMultiNode*, const tensorflow::tfprof::Options&, int) const':\r\n./tensorflow/core/profiler/internal/tfprof_op.h:60:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (opts.min_occurrence > node->node->graph_nodes().size()) {\r\n         ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/kernels/tensor_forest/resources.cc [for host]:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/lib/core/errors.h:21,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:23,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:22,\r\n                 from ./tensorflow/core/framework/shape_inference.h:20,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:25,\r\n                 from ./tensorflow/core/kernels/tensor_forest/resources.h:19,\r\n                 from tensorflow/core/kernels/tensor_forest/resources.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:452:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                              \r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:461:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                              \r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nERROR: /opt/deeplearning/src/tensorflow/tensorflow/core/kernels/BUILD:3206:1: C++ compilation of rule '//tensorf\r\nlow/core/kernels:reduction_ops' failed (Exit 1)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:124:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/kernels/reduction_ops_common.h:27,\r\n                 from tensorflow/core/kernels/reduction_ops_sum.cc:16:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In static member function 'static v\r\noid std::_Function_handler<void(_ArgTypes ...), _Functor>::_M_invoke(const std::_Any_data&, _ArgTypes&& ...) [wi\r\nth _Functor = Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable, Tileable>::run(\r\nconst Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorM\r\nap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<E\r\nigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0l> >, const Eigen::T\r\nensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> \r\n>; bool Vectorizable = true; bool Tileable = false]::<lambda(Eigen::internal::TensorExecutor<const Eigen::Tensor\r\nAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eig\r\nen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2inde\r\nx<0l> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer\r\n>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex, Eigen::internal::TensorExecutor<c\r\nonst Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakeP\r\nointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexLis\r\nt<Eigen::type2index<0l> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, \r\nEigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex)>; _ArgTypes = {\r\nlong int, long int}]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801:9: internal compiler error: in e\r\nmit_move_insn, at expr.c:3547\r\n         values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduc\r\ne,\r\n         ^~~~~~\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-6/README.Bugs> for instructions.\r\nTarget //tensorflow/examples/speech_commands:train failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1790.965s, Critical Path: 244.43s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, \r\nprocess: 0.00%]\r\nINFO: 3599 processes: 3599 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n", "comments": ["@goog Were you able to import tensorflow successfully? Please provide details about what platform you are using (operating system, architecture). Thanks!\r\n", "@gadagashwini  google's Deep Learning VM , debian 9, it built-in deep learning framework", "This looks like an internal compiler error with the particular toolchain on that Linux image. I'm not able to debug this, since it's not happening with our nightly CI builds. I'm unassigning myself, but if the VMs are supported by a Cloud team maybe they can take a look?", "Hi There,\r\n\r\nWe are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29206\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29206\">No</a>\n"]}, {"number": 29205, "title": "[XLA] Fix type mismatch of operations on slices in slice-sinker pass", "body": "Fix bug: the different primitive types of the operations(like convert) on slices are not peer instructions, and the their slice operands should not be sunk down.\r\nAnd add UT for this bug.\r\n\r\nExample:\r\n    test {\r\n      p0 = f32[8,9] parameter(0)\r\n      s00 = f32[2,9] slice(f32[8,9] p0), slice={[0:2], [0:9]}\r\n      s01 = f32[6,9] slice(f32[8,9] p0), slice={[2:8], [0:9]}\r\n      convert0 = s32[2,9] convert(f32[2,9] s00)\r\n      convert1 = s64[6,9] convert(f32[6,9] s01)\r\n      ROOT tuple = (s32[2,9], s64[6,9]) tuple(convert0, convert1)\r\n    }  \r\n", "comments": ["@bixia1 please help to review."]}, {"number": 29204, "title": "Support any iterable object in keras.fit_generator", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29204) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29204) for more info**.\n\n<!-- ok -->", "Thanks for the PR! This code path is actually being deprecated in 2.0 in favor of keras/engine/training_v2.py \r\n\r\nIf you want to add support for this, it should be there", "@noamwies Could you please address the reviewer comments. Thanks!", "Can one of the admins verify this patch?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 29203, "title": "how to use interactive_graphviz for xla?", "body": "in [xla doc](https://www.tensorflow.org/xla/jit#tutorial) it suggest that \r\n\r\n> /tmp/foo will contain the HLO before and after optimizations for each HLO module that's run. You can read this as-is, or you can visualize it using tensorflow/compiler/xla/tools:interactive_graphviz.\r\n\r\nbut I cannot locate this binary.", "comments": ["@jackalcooper Can you provide more details about the issue?", "@gadagashwini in the TensorFlow 1.14 release note, it mentioned that:\r\n\r\n> XLA\r\nXLA HLO graphs can be inspected with interactive_graphviz tool now.\r\n\r\nbut I couldn't find how to use it.", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 29202, "title": "Could you provide the weight pruning code for MobileNetV1 and InceptionV3?", "body": "", "comments": ["Duplicate of #29163 . Closing the issue. Thanks!"]}, {"number": 29201, "title": "Snap package.", "body": "Includes a wrapper script and snapcraft.yaml to build [snap](https://snapcraft.io/) for Tensorflow(GPU). I'm still running tests and will make some updates along the way.", "comments": ["@mikeroyal let me know when this PR is ready for review. Thank you", "Hi @rthadur, I'm ready for review. Let me know if I need to change anything. ", "Can one of the admins verify this patch?"]}, {"number": 29200, "title": "[TF 2] Expose tf.layers as alias for issue #26176,", "body": "In #26176, @MarkDaoust said about this\r\nIt's been some time since then, so I thought I'd go ahead and send the PR. Let me know if anything needs changing!\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29200) for more info**.\n\n<!-- need_sender_cla -->", "> `I signed it!`)\r\n\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29200) for more info**.\n\n<!-- ok -->", "There was a discussion around making this change and the summary was that this change will break checkpoints and also there are default arg value differences between tf layers and keras layers. We do not want to make this change at this point is my understanding.", "Plagiarized from #29197"]}, {"number": 29199, "title": "model pruning about conv3d ?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact code snippet  to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Looks like a spam. Closing this issue. If you are stuck, please open a separate issue with all information filled as per this [Template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!"]}, {"number": 29197, "title": "[TF 2] Expose tf.layers as alias for tf.keras.layers", "body": "cc #26176 ", "comments": ["In In https://github.com/tensorflow/tensorflow/issues/26176, @MarkDaoust said that:\r\n\r\n> I think this is the plan. It's just not quite implemented yet.\r\n\r\nIt's been some time since then, so I thought I'd go ahead and send the PR. Let me know if anything needs changing!", "There was a discussion around making this change and the summary was that this change will break checkpoints and also there are default arg value differences between tf layers and keras layers. We do not want to make this change at this point is my understanding."]}, {"number": 29196, "title": "To unify the regularizer in keras layer and variable_scope.py?", "body": "**System information**\r\n- TensorFlow version (you are using): master\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\nDon't change current api.\r\n\r\n**Who will benefit with this feature?**\r\nI think the regularizer should be declared with variables creation. It is reasonable to put them in variable_scope.py after one atomic variable's creation. But Keras Layer also has the regularization construction after weights' creation and it almost the same with in the variable_scope. \r\nTo unify the regularizer and reserve the logic in variable_scope.py will benefit `DistributionStrategy`. Because when we create one `MirroredVariable`, the corresponding all regularizations should also be created. We can get them from `GraphKeys.REGULARIZATION_LOSS` and wrap them with `Mirrored` type and replace in the `tf.collections`.\r\n\r\n**Any Other info.**\r\nI don't know whether it is suitable to make this change because `Keras` is highly recommended in TF2.0. But the same function in two different places is unnecessary. If you agree with me, I will try to contribute for this.\r\n@lukaszkaiser @fchollet ", "comments": ["In 2.0, we have removed collections (eg REGULARIZATION_LOSSES) and variable_scope. Regularizers have also been removed from the variable construction APIs, such that they are managed by Keras only. Does that work for you? ", "> In 2.0, we have removed collections (eg REGULARIZATION_LOSSES) and variable_scope. Regularizers have also been removed from the variable construction APIs, such that they are managed by Keras only. Does that work for you?\r\n\r\nThanks for your reply. I think this is a big change. But how to get regularization loss in 2.0 as your description? Before your reply, I think some special resource variables like `MirroredVariable` should manage their regularization themselves.", "You can use [Keras regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers) in 2.0. Examples involving regularization:\r\n\r\n* https://www.tensorflow.org/beta/tutorials/keras/overfit_and_underfit \r\n* https://www.tensorflow.org/beta/guide/keras/training_and_evaluation#handling_losses_and_metrics_that_dont_fit_the_standard_signature\r\n* https://www.tensorflow.org/beta/guide/migration_guide#before_converting", "> You can use [Keras regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers) in 2.0. Examples involving regularization:\r\n> \r\n> * https://www.tensorflow.org/beta/tutorials/keras/overfit_and_underfit\r\n> * https://www.tensorflow.org/beta/guide/keras/training_and_evaluation#handling_losses_and_metrics_that_dont_fit_the_standard_signature\r\n> * https://www.tensorflow.org/beta/guide/migration_guide#before_converting\r\n\r\nOK. I want to report a bug here. In Keras, the regularizer's declaration depends on whether the variables exists in global collections. See here :https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/base.py#L421.\r\nBut in `MirroredStrategy`, the `MirroredVariable` across all devices is created in the first replica and the other replica will reuse the `MirroredVariable`. That is to say, the variables have been available to other replicas. This will lead to the regularizers are declared only on the first replica, the other replicas will not have any regularizers because of the `existing_variables`. \r\nAnother question is that if you remove global collections, how to  make judgement about whether to declare regularizers\uff1f", "The code linked to is for v1 tf.layers, not Keras layers, hence the use of global variables. Those layers do not support distribution strategies.", "> The code linked to is for v1 tf.layers, not Keras layers, hence the use of global variables. Those layers do not support distribution strategies.\r\n\r\nI see. Thanks very much. ", "Closing this issue since its addressed. Feel free to reopen if have any further questions. Thanks!"]}, {"number": 29195, "title": "can't deserialize tf.keras functional api created model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190530\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI created [a model](https://github.com/breadbread1984/2DFAN4-tf2.0/blob/master/Model.py) with only tf.keras functional api. I can load parameters from [checkpoint](https://pan.baidu.com/s/13rEZz3CxaIOWHEeQaCfobg) (download password: bpy9) and inference with the model correctly. I convert the checkpoint to hdf5 model (graph with weights) with [this script](https://github.com/breadbread1984/2DFAN4-tf2.0/blob/master/save_model.py). When I load from the [hdf5 file](https://pan.baidu.com/s/1MN_HSvYiFDxFJqdbsUe5jQ) (download password: w2k6) with the code\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport tensorflow as tf;\r\nmodel = tf.keras.models.load_model('model.h5');\r\n```\r\n\r\nI get error messsage that\r\n\r\n> 2019-05-31 10:03:03.862129: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-05-31 10:03:03.866805: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2019-05-31 10:03:03.866861: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: xieyi-desktop\r\n2019-05-31 10:03:03.866877: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: xieyi-desktop\r\n2019-05-31 10:03:03.866957: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.104.0\r\n2019-05-31 10:03:03.867003: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 410.104.0\r\n2019-05-31 10:03:03.867019: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 410.104.0\r\n2019-05-31 10:03:03.895878: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1995530000 Hz\r\n2019-05-31 10:03:03.898224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6617f00 executing computations on platform Host. Devices:\r\n2019-05-31 10:03:03.898260: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"Landmarker.py\", line 140, in <module>\r\n    landmarker = Landmarker();\r\n  File \"Landmarker.py\", line 18, in __init__\r\n    self.model = tf.keras.models.load_model('model.h5');\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 215, in load_model_from_hdf5\r\n    load_weights_from_hdf5_group(f['model_weights'], model.layers)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 749, in load_weights_from_hdf5_group\r\n    layer, weight_values, original_keras_version, original_backend)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 374, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 350, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 374, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 350, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 374, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 350, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 374, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 350, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 374, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 362, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 374, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 350, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 455, in preprocess_weights_for_loading\r\n    if K.int_shape(layer.weights[0]) != weights[0].shape:\r\nIndexError: list index out of range\r\n\r\nit is worth noting that the serialized model is 92 MB while the checkpoint model is over 500 MB.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect correct model loading behavior from both checkpoint and hdf5.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport tensorflow as tf;\r\nmodel = tf.keras.models.load_model('model.h5');\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 29194, "title": "When building quantize_graph in r1.14 with mkl, it shows \"nothing to build\"", "body": "**System information**\r\n- OS Platform and Distribution (CentOS 7):\r\n- TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow.git\r\n- TensorFlow version: r1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Build from source\r\nbazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -s -c opt //tensorflow/tools/pip_package:build_pip_package can work\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 6.3\r\n\r\n\r\n**Describe the problem**\r\nTrying\r\nbazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt tensorflow/contrib/quantize:quantize_graph\r\n\r\nIt shows\r\nTarget //tensorflow/contrib/quantize:quantize_graph up-to-date (nothing to build)\r\nI can't find quantize_graph binary after built.\r\n", "comments": ["@lihaofd  You can get it from the Intel AI tools here https://github.com/IntelAI/tools/tree/master/tensorflow_quantization .  Pinging @TensorFlow-MKL for support.", "@lihaofd Please let us know if you were able to successfully build quantization tool based on the early comment", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29194\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29194\">No</a>\n", "it works for me, thanks!", "it works for me, thanks!"]}, {"number": 29193, "title": "Fix minor typos", "body": "exection -> execution\r\nhitogram -> histogram\r\nopertor  -> operator", "comments": ["Check out this pull request on ReviewNB: https://app.reviewnb.com/tensorflow/tensorflow/pull/29193 \n\n Visit www.reviewnb.com to know how we simplify your Jupyter Notebook workflows."]}, {"number": 29192, "title": "[TF-TRT] Support BatchMatMulV2", "body": "* Support BatchMatMulV2 by allowing broadcasting\r\n* Update error message for BatchMatMul(V1/V2) when a batched constant is attempted to be used\r\n* Update python BatchMatMul tests. The previous test had a BatchMatMul which was incompatible with TRT and wasn't being converted (wasn't caught because ExpectedEnginesToBuild didn't check that if it was being included the engine).\r\n* Fixes a bug when using MatMul in INT8 where the old logic would always try to use FC layer even if it was not compatible, potentially causing a segfault.", "comments": []}, {"number": 29191, "title": "tf.function spuriously fails for branched super() calls", "body": "`tf.function` fails spuriously under Python 3.7.3 for the following example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Base(tf.Module):\r\n    def __call__(self, x):\r\n        return x + 1.\r\n\r\nclass Sub(Base):\r\n    def __call__(self, x):\r\n        return super().__call__(x) if True else 1.\r\n\r\n@tf.function\r\ndef test():\r\n    return Sub()(tf.constant(42.))\r\n\r\nprint(test())\r\n\r\n```\r\n\r\n(Colab: https://colab.research.google.com/drive/1_pS1K0biwse1oTIuuEv0lZYavbE61HAP)\r\n\r\nThis produces the following error:\r\n```python\r\nRuntimeError: in converted code:\r\n\r\n    bug.py:16 test  *\r\n        return Sub()(tf.constant(42.))\r\n    bug.py:12 __call__  *\r\n        return super().__call__(x) if True else 1.\r\n\r\n    RuntimeError: super(): no arguments\r\n```\r\n\r\n**Observations**\r\n\r\n- Everything works correctly without the `tf.function` decoration\r\n- The branch in `__call__` seems necessary to trigger the bug. Skipping the branch doesn't trigger it. Replacing `True` with `False` doesn't trigger it.\r\n- The bug can be triggered even if the condition evaluates to False (for example, replacing `True` with `x < 0` for `x=42`)\r\n- Replacing `tf.constant(42.)` with `42.` doesn't trigger the bug\r\n\r\nTested on TensorFlow 2.0 nightly `2.0.0-dev20190529` on Ubuntu `16.04` with Python `3.7.3`\r\n", "comments": ["change\r\n```\r\nreturn super().__call__(x) if True else 1.\r\n```\r\nto\r\n```\r\nreturn super(Sub, self).__call__(x) if True else 1.\r\n```\r\nthere will be no errors", "@zakizhou Sure, but the point is that `super()` without any arguments is [perfectly valid in Python 3](https://www.python.org/dev/peps/pep-3135/) and as such, this should not result in a failure. Furthermore, the sensitivity to the conditional hints at further issues in `tf.function`", "I am able to reproduce the issue with Tensorflow  nightly 2.0.0-dev20190529. Thanks!", "I believe this is related to #26029 - the Python3-style super() is indeed not handled correctly yet; we're working to fix that.\r\n\r\nIt's indeed strange that the issue doesn't replicate reliably - I'll have a closer look.", "@mdanatg: this continues to be an issue even with the recent attempt at supporting argument-less super.\r\n\r\nIn the new [super_in_original_context](https://github.com/tensorflow/tensorflow/blob/612ceb6c488e228fa5246d2452799cf2691ef5f1/tensorflow/python/autograph/operators/py_builtins.py#L92-L130) implementation, the technique used for getting `self` seems a bit fragile. In particular, existence of conditionals (and, presumably, other scoping constructs) around the `super` call causes it to break in various ways.\r\n\r\nI've included a couple of samples in [this colab notebook](https://colab.research.google.com/drive/1aoqSo9StvVHDPHhH7Ao4WYm_t6UycnWd)", "@ethereon Thank you for follow-up and the samples! Indeed, our current resolution for handling `super()` is not yet complete, and will only work if the call is in a method (or outside control flow, as you pointed out).\r\n@kkimdev has been working on addressing this. We intentionally oversimplified the solution to assume the original caller of `super()` is exactly one frame up the call stack in the converted code, and we're working on a follow-up improvement that removes that incorrect assumption and instead walks the call stack to find the caller, which can be any number of levels above. We hope to have this ready shortly, and will ping this thread when ready.\r\n\r\nIn the mean time, the following should be a reliable wokraround:\r\n\r\n```\r\nbase_self = super()\r\nreturn base_self.__call__(x) if True else 1\r\n```\r\n```\r\nbase_self = super()\r\nif True:\r\n  return base_self.__call__(x)\r\nreturn 1\r\n```\r\n```\r\nbase_self = super()\r\ndef local_nested_function():\r\n  return base_self.__call__(x)\r\nreturn local_nested_function()\r\n```\r\n\r\nAgain, all these workarounds should hopefully not be needed soon.", "This should not be more robustly handled with the commit that will land soon. If you have a chance to give it a try, I'd love to know if we missed any corner case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29191\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29191\">No</a>\n"]}, {"number": 29190, "title": "Make TensorRT convert_nodes.cc C++11 conforming, fixing build under GCC 8", "body": "Make TensorRT convert_nodes.cc C++11 conforming, fixing build under GCC 8.\r\n\r\nGCC 8 (correctly) complains that lambdas are not literals in C++11 and therefore cannot be \"constexpr\". Fix is trivial.\r\n\r\nElided GCC 8 diagnostics follow.\r\n\r\ntensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:3960:3: error: the type '...::<lambda(nvinfer1::ITensor*, bool)>' of 'constexpr' variable 'get_matrix_op' is not literal\r\n       };\r\n       ^\r\n...because:\r\n           [](nvinfer1::ITensor* in, bool transpose) -> nvinfer1::MatrixOperation {\r\n            ^\r\ncc1plus: note:   ...::<lambda(nvinfer1::ITensor*, bool)>' is a closure type, which is only literal in C++17 and later\r\n", "comments": ["Quick note: This one-line change cannot possibly affect the \"Ubuntu Python2\" build.\r\n"]}, {"number": 29189, "title": "Keras LSTM does not work with tf.distribute [2.0]", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Minor tweak to tutorial code.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.\r\n- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0/7.4\r\n\r\nI copy-pasted [this tutorial code](https://www.tensorflow.org/alpha/tutorials/distribute/multi_worker) (MNIST distributed training with TF2.0) but used tf.distribute.MirroredStrategy() (instead of MultiWorker). It worked. Then I changed the model architecture to a simple Embedding -> LSTM -> Dense architecture. It broke with the following errror: \r\n\r\n```\r\nCannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:worker/replica:0/task:0/device:GPU:0 vs /job:worker/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal\r\n\t [[node sequential/lstm/StatefulPartitionedCall (defined at tf2_multiworker_tutorial/main.py:109) ]]\r\n```\r\n\r\nThis was executed on a remote cluster single-machine with 2 GPUs. Note that I've been seeing this error ever since the initial 2.0 alpha release. The code is as follows:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\nLEARNING_RATE = 1e-4\r\n\r\n\r\ndef input_fn(mode, input_context=None):\r\n    max_seq_len = 3\r\n    rnn_dataset = tf.data.Dataset\\\r\n        .range(10)\\\r\n        .repeat(10 * BUFFER_SIZE) \\\r\n        .map(lambda x: (\r\n        tf.ones(shape=(max_seq_len,), dtype=tf.int64),\r\n        tf.ones(shape=(max_seq_len,), dtype=tf.int64)))\r\n    if input_context:\r\n        rnn_dataset = rnn_dataset.shard(\r\n            input_context.num_input_pipelines,\r\n            input_context.input_pipeline_id)\r\n    return rnn_dataset.batch(BATCH_SIZE)\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    vocab_size = 100\r\n    embed_size = 16\r\n    state_size = 7\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size),\r\n        tf.keras.layers.LSTM(units=state_size, return_sequences=True),\r\n        tf.keras.layers.Dense(10, activation='softmax')])\r\n    logits = model(features, training=False)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(\r\n            tf.estimator.ModeKeys.PREDICT,\r\n            predictions={'logits': logits})\r\n\r\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(\r\n        learning_rate=LEARNING_RATE)\r\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(\r\n        from_logits=True,\r\n        reduction=tf.keras.losses.Reduction.NONE)\r\n    loss = tf.reduce_sum(loss(labels, logits)) * (1. / BATCH_SIZE)\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode, loss=loss,\r\n        train_op=optimizer.minimize(\r\n            loss, tf.compat.v1.train.get_or_create_global_step()))\r\n\r\n\r\ndef main():\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    config = tf.estimator.RunConfig(\r\n        train_distribute=strategy,\r\n        log_step_count_steps=1)\r\n\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=model_fn, model_dir='/tmp/multiworker', config=config)\r\n    tf.estimator.train_and_evaluate(\r\n        classifier,\r\n        train_spec=tf.estimator.TrainSpec(input_fn=input_fn, max_steps=10),\r\n        eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nAgain, the common theme I've observed is that if tf.keras.LSTM is part of my model and I'm using tf.distribute, it breaks with this error. Otherwise, it works just fine.", "comments": ["@mckinziebrandon Ran the code in colab using GPU got the below error.\r\nAttributeError: module 'tensorflow._api.v1.keras.losses' has no attribute 'SparseCategoricalCrossentropy'.\r\n\r\n", "@muddham using TF 2.0? That's an error I get if I use TF 1.13. For example, [here is the link](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) for its documentation.\r\n\r\nAlso, @muddham are you able to run this on a multi-GPU setup? The above code works as expected for tensorflow 2.0 CPU, but breaks when run with tf-nightly-gpu-2.0-preview (or tensorflow-gpu=2.0.0-alpha0) on 2 GPUS.", "@muddham Any update on this?", "@mckinziebrandon Is this still an issue?", "@gadagashwini Just confirmed that yes, this is still an issue on the current TF gpu-nightly (2.0). ", "Can you use Distribution Strategies directly with the Keras model, instead of passing through Estimator? There are likely bad effects here resulting from the mixing of many APIs (including v1 optimizers), and if you can swap to the fully v2 version (Dist Strat + keras optimizers + keras model + keras LSTM), this should work.", "@karmel I can try that, sure. However, if that's the case (that people should not mix them), then perhaps TensorFlow should not suggest doing so in their own tutorials (recall that this post is a fairly trivial modification to an official tutorial). It does look like a lot has been updated regarding the tutorials on distributed training, so I'll check those out too. Will update here after I try your suggestion.", "Ah, that tutorial has been replaced with this one: https://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_keras , and @rchao is removing references to the old. Can you start from the new one?", "Sure, starting from the new one, I made the following incremental changes. Reporting my results for each change. In summary, I end up getting basically the same error as before. Everything run below was with tensorflow-gpu==2.0.0-beta1.\r\n\r\n## Trying to Run the Tutorial with CNN\r\n\r\nFirst, I went through the updated tutorial linked to by @karmel and ran into more issues than before with the tutorial code itself when making certain minor changes. Again, below are the sequence of tweaks I tried and what I observed. \r\n\r\n1. Copy-paste tutorial code and run without modification. **Works as expected.** \r\n    * Question here: I noticed the tutorial does not mention single-worker *multi-gpu* explicitly, and also doesn't mention tf.distribute until the multi-worker case. Is it not required to use tf.distribute when using multi-gpu on a single worker? If so, would be very helpful to clarify that in the docs. \r\n2. Specify `strategy = tf.distribute.MirroredStrategy()` and wrap the build_and_compile_cnn_model with strategy.scope(). **Works as expected**. \r\n3. Specify TF_CONFIG as described by the tutorial, but only run on single worker. **FAILS** with error:\r\n    ```\r\n    RuntimeError: Error copying tensor to device: /job:worker/replica:0/task:0/device:GPU:0.         /job:worker/replica:0/task:0/device:GPU:0 unknown device.\r\n    ```\r\n    Note that TensorFlow itself acknowledged (as usual) the two GPUs on the machine when the program started:\r\n    ```\r\n    2019-06-26 19:40:02.642377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11312 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:84:00.0, compute capability: 6.0)\r\n    2019-06-26 19:40:02.643711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11312 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-12GB, pci bus id: 0000:89:00.0, compute capability: 6.0)\r\n    ```\r\n    It seems to somehow have forgotten them. Not sure.\r\n4. Increase number of workers from 1 to 2: **FAILS** with error:\r\n    ```\r\n    RuntimeError: Collective ops must be configured at program startup\r\n    ```\r\n    This happens when creating MultiWorkerMirroredStrategy. This was particularly surprising because the tutorial you linked seems to suggest that this should be a *warning*, not an error. Searching further around the docs, it is quite unclear how to fix this error.\r\n\r\n## Replacing CNN with RNN\r\n\r\nLet's start over, back to the updated tutorial again. Insert my tweaks to use an RNN instead of CNN as described in my original post. Concretely: Instead of build_and_compile_cnn_model(), use: \r\n\r\n```python\r\ndef build_and_compile_rnn_model():\r\n    vocab_size = 100\r\n    embed_size = 16\r\n    state_size = 7\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size),\r\n        tf.keras.layers.LSTM(units=state_size, return_sequences=True),\r\n        tf.keras.layers.Dense(10, activation='softmax')])\r\n    model.compile(\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001))\r\n    return model\r\n````\r\n\r\nInstead of loading dataset from tfds, use:\r\n\r\n```python \r\ndef get_rnn_dataset():\r\n    max_seq_len = 3\r\n    rnn_dataset = tf.data.Dataset\\\r\n        .range(10)\\\r\n        .repeat(10 * BUFFER_SIZE) \\\r\n        .map(lambda x: (\r\n            tf.ones(shape=(max_seq_len,), dtype=tf.int64),\r\n            tf.ones(shape=(max_seq_len,), dtype=tf.int64)))\r\n    return rnn_dataset.batch(BATCH_SIZE)\r\n```\r\n\r\nIf I then do:\r\n\r\n```python\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    model = build_and_compile_rnn_model()\r\nmodel.fit(x=get_rnn_dataset(), epochs=3)\r\n```\r\n\r\nI get essentially the same error as in my original post:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal\r\n\t [[node replica_1/sequential/lstm/StatefulPartitionedCall (defined at /threading.py:916) ]]\r\n\t [[loss/mul/_88]]\r\n  (1) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:1 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_22/exit/_100 , and the dst node is while_0_RetVal\r\n\t [[node replica_1/sequential/lstm/StatefulPartitionedCall (defined at /threading.py:916) ]]\r\n0 successful operations.\r\n1 derived errors ignored. [Op:__inference_distributed_function_6308]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```", "UPDATE: something that might be useful to know is that if I replace `tf.keras.LSTM` with `tf.keras.RNN(cell=tf.compat.v1.nn.rnn_cell.LSTMCell(...))`, everything works as expected for single-worker multi-gpu. The issue is definitely within tf.keras.LSTM itself.\r\n\r\nFor multi-worker multi-gpu, I still get the collective ops error (that the tutorial suggests should be a warning). This error goes away if I wraps things with tf.estimator. \r\n\r\nOverall, @qlzh727 @karmel are these being worked on? I understand there are a ton of changes for TF2.0 but having Keras LSTM be incompatible with tf.distribute seems like a fairly significant issue.", "Hi @mckinziebrandon, we also got bug report from internal and we are trying to address it. \r\n\r\nFor now, i think you can work around the issue by using tf.keras.RNN(cell=tf.keras.LSTMCell(...)), which should give the same math result. ", "Hi,\r\n\r\nI am trying to run mnist example using distributed training tf 2.0.I am getting below similar error. I request to help me with workaround for this error.\r\n\r\n\r\n    tf.distribute.experimental.CollectiveCommunication.NCCL)\r\n  File \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 80, in __init__\r\n    communication=communication))\r\n  File \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 110, in __init__\r\n    self._initialize_strategy(cluster_resolver)\r\n  File \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 116, in _initialize_strategy\r\n    self._initialize_multi_worker(cluster_resolver)\r\n  File \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 208, in _initialize_multi_worker\r\n    device_filters=(\"/job:%s/task:%d\" % (task_type, task_id),))\r\n  File \"/home/administrator/venv/lib/python3.5/site-packages/tensorflow/python/eager/context.py\", line 541, in configure_collective_ops\r\n    raise RuntimeError(\"Collective ops must be configured at program startup\")\r\nRuntimeError: Collective ops must be configured at program startup\r\n\r\n \r\n\r\n", "@mvp2301 you error seems to be totally different issue, please file a separate bug with more details.", "@qlzh727 Hi, running into the same issue. I was running on tf.keras.layers.RNN(cell=tf.keras.layers.LSTMCell(...)) until recently, but decided to switch to tf.keras.layers.LSTM for the CuDNN implmentation. Is the LSTM layers the only access point for the CuDNN LSTM ? In fact, I also opened a feature request on it but that's a different matter (https://github.com/tensorflow/tensorflow/issues/30535)", "I would also like to point out that I am getting this error without even being on a distributed strategy. Using LSTM, Dataset, TF 2.0...\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-11-c81bfda0c541> in <module>\r\n      7         optimizer=optimizer,\r\n      8         batch=(seq_x, len_x, len_y),\r\n----> 9         global_batch_size=240\r\n     10     )\r\n     11     t1 = time.time()\r\n\r\n~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    426         # Lifting succeeded, so variables are initialized and we can run the\r\n    427         # stateless function.\r\n--> 428         return self._stateless_fn(*args, **kwds)\r\n    429     else:\r\n    430       canon_args, canon_kwds = \\\r\n\r\n~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1333     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1334     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1335     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1336 \r\n   1337   @property\r\n\r\n~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n    587     \"\"\"\r\n    588     return self._call_flat(\r\n--> 589         (t for t in nest.flatten((args, kwargs), expand_composites=True)\r\n    590          if isinstance(t, (ops.Tensor,\r\n    591                            resource_variable_ops.ResourceVariable))))\r\n\r\n~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    669     # Only need to override the gradient in graph mode and when we have outputs.\r\n    670     if context.executing_eagerly() or not self.outputs:\r\n--> 671       outputs = self._inference_function.call(ctx, args)\r\n    672     else:\r\n    673       self._register_gradient()\r\n\r\n~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    443             attrs=(\"executor_type\", executor_type,\r\n    444                    \"config_proto\", config),\r\n--> 445             ctx=ctx)\r\n    446       # Replace empty list with None\r\n    447       outputs = outputs or None\r\n\r\n~/anaconda3/envs/seq3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~/anaconda3/envs/seq3/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal\r\n\t [[{{node se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall}}]]\r\n\t [[se_q3/seq_decoder/encoder/bidirectional/cond_1/then/_958/while/exit/_6952/_2016]]\r\n  (1) Invalid argument:  Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal\r\n\t [[{{node se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_step_54702]\r\n\r\nFunction call stack:\r\ntrain_step -> train_step", "@jkamalu, can u provide the full code snippet to reproduce your issue?", "> \r\n> \r\n> @mvp2301 you error seems to be totally different issue, please file a separate bug with more details.\r\n\r\nThanks for reply.created a new bug issue #30551", "@qlzh727 I will when I find the time and am able to produce it on a smaller example. In the meantime, check out this issue which may be related (the logs were the same for me when I got the cannot place graph error): https://github.com/tensorflow/tensorflow/issues/29525#\r\n", "Hi @qlzh727, I finally got around to finding a minimal example that shows that the LSTM implementation breaks in some cases without the distributed strategy. Here you go: https://github.com/jkamalu/tensorflow_bugs/blob/master/LSTMGraphPlacement.py\r\n\r\nIt has to do the the inclusion of the loop. Without the loop, it works fine. My real use case of the while loop is to perform dynamic decoding, but I get the same error as you will run into here.", "@mckinziebrandon I am currently able to run a model containing the tf.keras.layers.LSTM layer with the MirroredStrategy on 3 GPUs in parallel as of the July 23 2.0 gpu nightly. See this comment by @qlzh727 on my now closed issue pointing to two recent commits that may have resolved a myriad of problems: https://github.com/tensorflow/tensorflow/issues/30639#issuecomment-513599641", "Thanks for reporting the issue, the issue should be solved by ca7acec and f0fd2be. Could u try the code again with the tf-2.0-nightly builds?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29189\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29189\">No</a>\n", "does this already fixed?", "Yes.", "Hi, I still encountered a similar error with the official 2.0 version and tf-nightly.\r\n\r\nHere is the code snippet: https://gist.github.com/matthew-z/e5848d545b60792dd84bfb9470ea541f\r\n\r\n`model.fit` works well, but `model.evaluate` raises this error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-20-8e7870f41c98> in <module>\r\n----> 1 model.evaluate([left, right], labels)\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n    831         max_queue_size=max_queue_size,\r\n    832         workers=workers,\r\n--> 833         use_multiprocessing=use_multiprocessing)\r\n    834 \r\n    835   def predict(self,\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in evaluate(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\r\n    454     return self._model_iteration(\r\n    455         model, ModeKeys.TEST, x=x, y=y, batch_size=batch_size, verbose=verbose,\r\n--> 456         sample_weight=sample_weight, steps=steps, callbacks=callbacks, **kwargs)\r\n    457 \r\n    458   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\r\n    442               mode=mode,\r\n    443               training_context=training_context,\r\n--> 444               total_epochs=1)\r\n    445           cbks.make_logs(model, epoch_logs, result, mode)\r\n    446 \r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    524               *args, **kwds)\r\n    525       # If we did not create any variables the trace we have is good enough.\r\n--> 526       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n    527 \r\n    528     def fn_with_cond(*inner_args, **inner_kwds):\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: 2 root error(s) found.\r\n  (0) Not found:  Resource AnonymousIterator/AnonymousIterator8/N10tensorflow4data16IteratorResourceE does not exist.\r\n\t [[node IteratorGetNext (defined at /home/zhaohao/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n\t [[IteratorGetNext/_10]]\r\n  (1) Not found:  Resource AnonymousIterator/AnonymousIterator8/N10tensorflow4data16IteratorResourceE does not exist.\r\n\t [[node IteratorGetNext (defined at /home/zhaohao/.anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_45417]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\n", "@matthew-z, thanks for reporting the issue, I can't reproduce it. Seems that to be a different issue since the your code does not use distribution strategy. Do u mind create new a ticket for that, with the details about your local environment? \r\n\r\nThanks.", "@qlzh727 Thank you for the reply. I have filed a new issue:  #33258 with detailed environment info (tested on GCE machine and image).", "@qlzh727  I have this issue when using TimeDistributed LSTM with mask_zero=True\r\nthis is my model :\r\n`        model=tf.keras.Sequential()\r\n        embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=True)\r\n        model.add(TimeDistributed(embeding_layer))\r\n        model.add(TimeDistributed(tf.keras.layers.LSTM(50)))\r\n        model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(50)))\r\n        # model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(100)))\r\n        model.add(layers.Dense(3,activation='softmax'))\r\n        opt=tf.keras.optimizers.Adam(learning_rate=0.001)\r\n        model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy',self.f1_m,self.precision_m, self.recall_m])\r\n        self.model=model\r\n`\r\n\r\nand this is the error :\r\n`C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\Scripts\\python.exe C:/Users/jalil/PycharmProjects/untitled1/main_file.py\r\n2019-11-14 14:11:36.983144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-11-14 14:11:45.638679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-11-14 14:11:46.216495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038\r\npciBusID: 0000:01:00.0\r\n2019-11-14 14:11:46.216676: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-14 14:11:46.217282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-14 14:11:50.885396: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-11-14 14:11:51.214275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038\r\npciBusID: 0000:01:00.0\r\n2019-11-14 14:11:51.214484: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-14 14:11:51.218182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-14 14:11:51.905201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-14 14:11:51.905307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-14 14:11:51.905366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-14 14:11:51.906228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4757 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\nWARNING:tensorflow:From C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3983: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nTrain on 35000 samples, validate on 6447 samples\r\nEpoch 1/1000\r\n2019-11-14 14:12:33.178251: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_671418_672877' and '__inference___backward_cudnn_lstm_with_fallback_671418_672877_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_675292' both implement 'lstm_81cdaa4a-fa6f-4675-abbb-02fb4cd0189b' but their signatures do not match.\r\n2019-11-14 14:12:33.544669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-11-14 14:12:34.397677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n\r\n   32/35000 [..............................] - ETA: 2:03:422019-11-14 14:12:35.151804: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n2019-11-14 14:12:35.152137: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond_64/then/_0/CudnnRNNV3}}]]\r\n2019-11-14 14:12:35.152541: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond_64/then/_0/CudnnRNNV3}}]]\r\n\t [[sequential/time_distributed_1/lstm/StatefulPartitionedCall]]\r\n\r\n   32/35000 [..............................] - ETA: 2:25:41Traceback (most recent call last):\r\n  File \"C:/Users/jalil/PycharmProjects/untitled1/main_file.py\", line 102, in <module>\r\n    main_model_instance.train_model(train_batch_data,train_batch_labels,test_batch_data,test_batch_labels)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\main_model.py\", line 103, in train_model\r\n    history = self.model.fit(x=np.array(train_batch_data),y=np.array(train_batch_labels),validation_data=(np.array(test_batch_data),np.array(test_batch_labels)),epochs=1000,callbacks=[tensorboard_callback])\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 734, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 439, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1822, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError:  [_Derived_]  CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond_64/then/_0/CudnnRNNV3}}]]\r\n\t [[sequential/time_distributed_1/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_675292]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n`\r\n\r\ndo you have any suggestions, please?", "by the way, on CPU it works fine, the problem appears when using the GPU version. I also tried every GPU version of TensorFlow and the problem is still there.", "@jalilasadi, do u mind opening another ticket with more details about your issue? We don't want to track multiple issue in the same ticket if they are different.", "@qlzh727  thanks for your answer, actually there is an issue on this problem  #33148   but no one helped us there.  I will be happy to see you there. ", "Ok, we will take a look for that issue.", "@qlzh727 we are waiting for you please. ", "I'm still getting this in TF 2.2.0 when using `MirroredStrategy`\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.UnknownError:  [_Derived_]  CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1459): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\r\n\t [[{{node CudnnRNN}}]]\r\n\t [[StatefulPartitionedCall/StatefulPartitionedCall/replica_2/transducer/encoder_network/lstm/StatefulPartitionedCall]]\r\n\t [[StatefulPartitionedCall/StatefulPartitionedCall/Identity_1/_230]] [Op:__inference__test_function_85050]\r\n```"]}, {"number": 29188, "title": "[ROCm] Adding ROCm support for tensor list kernels/methods", "body": "This PR adds ROCm support for tensor list kernels/methods\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-----------------------------\r\n\r\n@tatianashp @whchung ", "comments": []}, {"number": 29187, "title": "TF 2.0: Cannot use recurrent_dropout with LSTMs/GRUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No (one line modification to stock example)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0-alpha0 (also fails with every other tf 2.0 build I have explored)\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: Tried multiple\r\n- GPU model and memory: Tried multiple\r\n\r\n**Describe the current behavior**\r\nThe program crashes with a TypeError as below:\r\n\r\n`TypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: encoder/unified_gru/ones_like:0`\r\n\r\nThis occurs when trying to backprop the gradients through the LSTM/GRU with `recurrent_dropout` enabled.\r\n\r\n**Describe the expected behavior**\r\nNo error\r\n\r\n**Code to reproduce the issue**\r\nSince this problem shows up at the time of training, one needs to have the entire training pipeline (dataset, model etc.) setup to demonstrate this bug. As a result, I used the [Neural Machine Translation tutorial](https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention) from TensorFlow and modified their model to include `recurrent_dropout`. The entire code can be found in [this Colab notebook](https://colab.research.google.com/drive/1dLE58i2tttY6J_Yr8dX8f57Ai0_54kTE); run the code blocks all the way till the block where we're training the model to see the bug.\r\n\r\n**Other info.logs**\r\n<pre>\r\nx---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-26-fa5128338d20> in <module>()\r\n      8 \r\n      9   for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\r\n---> 10     batch_loss = train_step(inp, targ, enc_hidden)\r\n     11     total_loss += batch_loss\r\n     12 \r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    436         # Lifting succeeded, so variables are initialized and we can run the\r\n    437         # stateless function.\r\n--> 438         return self._stateless_fn(*args, **kwds)\r\n    439     else:\r\n    440       canon_args, canon_kwds = self._canonicalize_function_inputs(args, kwds)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1286     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1287     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1288     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1289 \r\n   1290   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n    572     \"\"\"\r\n    573     return self._call_flat(\r\n--> 574         (t for t in nest.flatten((args, kwargs))\r\n    575          if isinstance(t, (ops.Tensor,\r\n    576                            resource_variable_ops.ResourceVariable))))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    625     # Only need to override the gradient in graph mode and when we have outputs.\r\n    626     if context.executing_eagerly() or not self.outputs:\r\n--> 627       outputs = self._inference_function.call(ctx, args)\r\n    628     else:\r\n    629       self._register_gradient()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    413             attrs=(\"executor_type\", executor_type,\r\n    414                    \"config_proto\", config),\r\n--> 415             ctx=ctx)\r\n    416       # Replace empty list with None\r\n    417       outputs = outputs or None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n     69       raise core._SymbolicException\r\n---> 70     raise e\r\n     71   # pylint: enable=protected-access\r\n     72   return tensors\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n     59                                                op_name, inputs, attrs,\r\n---> 60                                                num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: encoder/unified_gru/ones_like:0\r\n</pre>", "comments": ["Have tried with TensorFlow version 2.0.0-alpha and was able to reproduce the issue.", "Thanks for reporting the issue, let me take a look.", "Thanks for reporting the issue, it should now be fixed by https://github.com/tensorflow/tensorflow/commit/180f28a26660ca2e1ba27477f4f9592db5f9c4e8", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29187\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29187\">No</a>\n", "Btw, the current colab might not apply the dropout correctly if you only enable the dropout/recurrent_dropout on the GRU layer. Under the hood, the keras layer will check whether the current context is in training or inference, and only apply the dropout during training. If the GRU layer was using by a keras model together with model.fit/eval/predict, then the training context will be applied correctly. However, if the user is writing their own custom training loop, then the training context need to be set manually, eg by\r\n\r\n```python\r\ntf.keras.backend.set_learning_phase(1)  # training\r\nrun_train_step()\r\n\r\ntf.keras.backend.set_learning_phase(0)\r\nrun_eval_step()\r\n```\r\n\r\nThe other alternative is that make sure the encoder/decoder's call() method is training state aware. eg, the method could take a new kwarg training=None, and set to different value during training and inference. The training value need to be popagated to GRU's call() method as well. ", "@qlzh727: Thanks a ton for your help on this!\r\n\r\nQuick follow-up: has this been fixed in the GPU version as well? I tried the (nightly) version from yesterday and it didn't seem to work.", "The issue still persists in the beta release", "I still have this issue in beta 2.0.0b1", "For any of you that still facing the issue, could u provide a snippet to reproduce the issue? ", "> could u provide a snippet to reproduce the issue?\r\n\r\nSimilar error even without GRU.\r\n\r\n```\r\ncp_callback = ModelCheckpoint(filepath=\"checkpoints/\", save_weights_only=False, verbose=0, save_best_only=True)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], run_eagerly=True)\r\nmodel.fit(train_ds, validation_data=val_ds, epochs=5, callbacks=[cp_callback])\r\n```", "@knobel-dk, I am bit confused about your message, this issue was about the recurrent_dropout for the LSTM/GRU layer, but your code doesn't have any LSTM/GRU layer within it.\r\n\r\nCould you be more specific about the error you are facing?", "> @knobel-dk, I am bit confused about your message, this issue was about the recurrent_dropout for the LSTM/GRU layer, but your code doesn't have any LSTM/GRU layer within it.\r\n> \r\n> Could you be more specific about the error you are facing?\r\n\r\nThanks. Yes I have confused myself too. Those stateful Jupyter notebooks.. I fixed my problem by updating the TF2 version. Thanks."]}, {"number": 29186, "title": "[ROCm] Adding ROCm support for the dilation ops", "body": "This PR adds ROCm support for the dilation ops\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-------------------------------\r\n\r\n@tatianashp @whchung ", "comments": []}, {"number": 29185, "title": "[ROCm] Add ROCm support for 2D and 3D convolution ops.", "body": "MIOpen has its own algorithm find logic and would return the size of scratch\r\nmemory required to client applications. Modify AlgorithmConfig to track such\r\ninformation.\r\n\r\nThis is a follow-up PR for #29172 .", "comments": ["looking at the log I don't believe those failures are relevant to changes proposed in this PR.", "looking at the log I don't believe those failures are relevant to changes proposed in this PR.", "@chsigg a gentle ping.", "@chsigg a gentle ping for this particular PR.", "Again looking at the log I don't believe those failures are relevant to changes proposed in this PR."]}, {"number": 29184, "title": "[ROCm] Adding ROCm support to optional_ops", "body": "This PR adds ROCm support for optional_op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-----\r\nTest performed: //tensorflow/core/kernels/data:optional_ops and //tensorflow/python/data/ops:optional_ops compiles\r\n@tatianashp @whchung", "comments": []}, {"number": 29183, "title": "Test cases for GPU delegate GL ops", "body": "**System information**\r\n- TensorFlow version (you are using): 0.0.1-gpu\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI was going through the GL ops and noticed that test cases are available for just 3 ops (`add`, `elementwise` and `relu`). Will test cases be available for ops like `reshape`?\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAnyone trying to debug GPU delegate ops.\r\n\r\n", "comments": ["@mun3 \r\n\r\nGreat time to ask.  Yes, we are in the process of moving our unit tests from Google-internal location to TF repository.  You will see more in coming weeks.  Well, there's CVPR conference in the middle, but at least by end of June, you should see unit test for all ops."]}, {"number": 29182, "title": "[ROCm] Adding ROCm support to host_constant_op", "body": "This PR adds ROCm support for host_constant_op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-----\r\nTest performed: tensorflow/core/kernels:host_constant_op and tensorflow/tools/pip_package:build_pip_package compiles\r\n@tatianashp @whchung", "comments": []}, {"number": 29181, "title": "[ROCm] Renaming GetCudaStream to GetGpuStream", "body": "This PR renames the GetCudaStream to GetGpuStream. It is a follow-up from PR#28568.\r\n\r\n-----\r\nTest performed: //tensorflow/tools/pip_package:build_pip_package compiles at both develop-upstream and master branch.\r\n@tatianashp @whchung @chsigg ", "comments": ["@jerryyin can you please resolve conflicts ?", "> @jerryyin can you please resolve conflicts ?\r\n\r\n@rthadur Will do thanks for reminding", "@jerryyin, looks like more merge conflicts have shown up...please re-resolve. thanks", "@deven-amd Thanks for reminding! Done", "This PR seems to get stuck in the queue for a while, could you help take a second look @chsigg @whchung \r\n\r\nIn my last rebase, I accepted the code change from tensorflow/core/kernels/searchsorted_op.cc in #28479 as the latest version.", "Rebasing again to cope with conflict. Looks like `GetGpuStream()` is already merged [in this commit](https://github.com/tensorflow/tensorflow/commit/3277ca09a88bf39d9fe1bec69d960059d9b61913). This PR does not block further `GetGpuStream()` dependent PRs.", "@jerryyin can you please rebase again , i guess new changes came in , sorry for the trouble.", "@rthadur No worries, just done. I think this one that does the vertical stride collide with other PRs that deals with one operator at a time.\r\n\r\nFYI in the rebase I accepted all upstream changes from `bincount_op` `histogram_op` and `reduction_gpu_kernels`.", "@jerryyin  please fix merge conflicts.", "@dagamayank Thanks for reminding. Done.\r\n\r\nThis is my 7th rebase followed by a conflict from somewhere else. It is a special case though because it collide with multiple in-working PRs of ours as well. A successful merge of another operator will block this one to be merged. I'd like to thank @whchung @chsigg for their repeatedly prompt review and approval. Taking that my intended update already went in from a dependent PR, it doesn't block anything at this point.", "Closing this PR because it is already merged in [this commit](https://github.com/tensorflow/tensorflow/commit/41c36223dcd265603ca6916fec0a88016eb9abaa)"]}, {"number": 29180, "title": "[ROCm] Adding ROCm support for gather_op", "body": "This PR adds ROCm support for gather_op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-----\r\nTest performed: tensorflow/core/kernels:gather_op_test\r\n@tatianashp @whchung", "comments": []}, {"number": 29179, "title": "docs: fix typo in keras' convolutional.py", "body": "", "comments": []}, {"number": 29178, "title": "[TF2] Variable List Shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): `pip install tf-nightly-2.0-preview`\r\n- TensorFlow version (use command below): `v1.12.1-2995-ge09b015 2.0.0-dev20190530`\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nRight now, the `tf.Variable` constructor accepts a list as a shape -- but if you do this, then calls to `.assign` raise an error. \r\n\r\n**Describe the expected behavior**\r\nEither `tf.Variable` should automatically convert the list to a `tf.TensorShape` (or it could raise a type error for a list shape, but that seems significantly less ergonomic).\r\n\r\n**Code to reproduce the issue**\r\nThe following code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable(tf.zeros([50, 50]), shape=[None, 50])\r\nx.assign(tf.zeros([10, 50]))\r\n```\r\n\r\nfails with:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"a.py\", line 4, in <module>\r\n    x.assign(tf.zeros([10, 50]))\r\n  File \"/opt/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1145, in assign\r\n    self._shape.assert_is_compatible_with(value_tensor.shape)\r\nAttributeError: 'list' object has no attribute 'assert_is_compatible_with'\r\n```\r\n\r\nIf I use `shape=tf.TensorShape([None, 50])` then this works just fine.", "comments": ["Have tried with TensorFlow version 2.0.0-dev20190531 on Colab and was able to reproduce the issue.", "@alanhdu Hi, I've tried this on tensorflow 2.1 and tf-nightly and haven't got this issue. Can you please check on your end?", "@alanhdu, This is fixed in Tf-nightly==2.2.0.dev20200312.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/39f97b21499f862848df44f6f24f680d/untitled445.ipynb) and let us know can we close this issue. Thanks", "Oh, sorrry! Yes, it looks like support has been added.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29178\">No</a>\n"]}, {"number": 29177, "title": "docs: typo in convolutional.py", "body": "", "comments": ["@haraldurt please push the changes to master , we are not accepting changes to r1.13 if it is doc fix."]}, {"number": 29176, "title": "1.14-rc1 cherry-pick request: Support explicit padding in conv2d second order gradients.", "body": "PiperOrigin-RevId: 249105971\r\n\r\nThis fixes a crash when second order gradients are taken with Conv2d when explicit padding is used.", "comments": []}]