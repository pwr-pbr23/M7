[{"number": 26934, "title": "Added Support for different data types.", "body": "Added different data types along with test cases.", "comments": ["@miaout17 , can you please review the PR.\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "@rthadur, could you follow up on the review.", "@gunan sure , thanks for the reminder.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 26933, "title": "Can we have demo CNN application using ARM micro controller with Camera for object detection please?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Experimental Micro-controller version\r\n- Are you willing to contribute it (Yes/No): Not now.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_vision might the closest example code for now. It's still image classification not yet detection though.", "I'm closing this as we do have a vision example available now."]}, {"number": 26932, "title": "tf lite java api model interpretation error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5 emulator, API version 23\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.12.0-10426-g4b1ee1a7b1', '1.14.1-dev20190319')\r\n- Python version: Python 2.7.15rc1\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen loading a custom model using the TF lite api for android, the output tensor dimensions become malformed. This causes an exception:\r\n\r\n```\r\njava.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [0, 2] and a Java object with shape [1, 2].\r\n        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:242)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:116)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:157)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:250)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:228)\r\n        ...\r\n```\r\nIt is a custom model, see attachment. Various validation scripts have shown no anomalies in the model.\r\nI have spent some effort on debugging, validating the behavior of the API. However, I get stuck at the native methods.\r\n\r\nThe code below shows that before execution, the tensors do have the right shape.\r\n```\r\n        network = new Interpreter(modelFile, options);\r\n\r\n        logger.info(String.format(\"Input tensor count: %d\",network.getInputTensorCount()));\r\n        for(int input_i = 0; input_i< network.getInputTensorCount(); input_i ++ ) {\r\n            Tensor input = network.getInputTensor(input_i);\r\n            logger.info(String.format(\" Tensor %d:\", input_i));\r\n            logger.info(String.format(\"  Type %s\", toStringName(input.dataType())));\r\n            logger.info(String.format(\"  Dimensions %d\", input.numDimensions()));\r\n            logger.info(String.format(\"  Shape %s\", Arrays.toString(input.shape())));\r\n        }\r\n\r\n        logger.info(String.format(\"Output tensor count: %d\",network.getOutputTensorCount()));\r\n        for(int output_i = 0; output_i< network.getInputTensorCount(); output_i ++ ) {\r\n            Tensor output = network.getOutputTensor(output_i);\r\n            logger.info(String.format(\" Tensor %d:\", output_i));\r\n            logger.info(String.format(\"  Type %s\", toStringName(output.dataType())));\r\n            logger.info(String.format(\"  Dimensions %d\", output.numDimensions()));\r\n            logger.info(String.format(\"  Shape %s\", Arrays.toString(output.shape())));\r\n        }\r\n\r\n        float[][][][] input = adapter.convert(_input);\r\n        float[][] output = {{0f, 0f}};\r\n\r\n        network.run(input, output);\r\n\r\n```\r\n\r\nIt seems like in NativeInterpreterWrapper, the size of the output tensor is corrected after model execution, near the line:\r\n```\r\n    run(interpreterHandle, errorHandle);\r\n    long inferenceDurationNanoseconds = System.nanoTime() - inferenceStartNanos;\r\n\r\n    // Allocation can trigger dynamic resizing of output tensors, so refresh all output shapes.\r\n    if (needsAllocation) {\r\n      for (int i = 0; i < outputTensors.length; ++i) {\r\n        if (outputTensors[i] != null) {\r\n          outputTensors[i].refreshShape(); // <-- \r\n        }\r\n      }\r\n    }\r\n```\r\nIt seems like this method checks the output dimensions with the c++ backend code, and corrects this in the Java part. At this point, the mismatch arrises.\r\n\r\nHowever, if I perform the check before the model runs, the mismatch is already present.\r\n\r\n**Describe the expected behavior**\r\nNo exception, no need to resize the tensor to a 0-dimensional array.\r\n\r\n**Code to reproduce the issue**\r\n[bugreport.zip](https://github.com/tensorflow/tensorflow/files/2987811/bugreport.zip)\r\nModel: lite_model_v2.zip\r\nJava: LiteEvaluator.java\r\n\r\nDependency:\r\nEqual behavior exists for both\r\nimplementation 'org.tensorflow:tensorflow-lite:1.13.1'\r\nand\r\nimplementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n\r\n\r\n\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["Thanks for the report, I'll take a look.", "What is the shape of your input tensor?", "I've run your code, and if I give it the natural (1, 3, 200, 1) shape it behaves as expected. I'll need to know what shape you're providing.\r\n\r\nNote that not all graphs support dynamic resizing based on the input shape. If you have any operators in the graph with fixed shape assumptions, the resize may fail or produce an invalid shape.", "The bug was in my own code; the shape provided was (1, 200, 3, 1) i.s.o. (1, 3, 200, 1). Appologies, and thanks for looking into it.\r\n\r\nI'm not much of an expert in using neural networks, but the fact that no error is ever produced about mismatching input shapes seems odd to me.\r\nIs it desired to insert different-shaped inputs, or would it be a nice new feature to validate the inputs?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26932\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26932\">No</a>\n"]}, {"number": 26931, "title": "TF Lite allocations of std::vectors performance improve", "body": "Todo activity.", "comments": ["This is duplicate of #25887."]}, {"number": 26930, "title": "[XLA] Add option to omit the layout string when printing HloInstructions", "body": "Add an option to omit the layout part of any shape info in the string serialization of an instruction.\r\n\r\nThis helps keep the size of the printed graph down when dumping to a log.  For some backends, the layout may not be an important property of the shape.\r\n\r\n\r\n", "comments": ["Not sure I am the right person for this review. I am not familiar with this code", "@sanjoy do you mind taking a look or reassign? Thank you!"]}, {"number": 26929, "title": "Please add the window size parameter (and other options) to SSIM", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, the SSIM in tf.image.ssim has many parameters fixed inside the script with no way to alter them. Most importantly, the window size parameter is fixed at 11x11. \r\n\r\nHowever, many people have found that changing the window size when using SSIM as a loss function changes the learning outputs. (See https://arxiv.org/pdf/1511.08861.pdf from the authors who first proposed the use of SSIM as a loss function)\r\n\r\nIt would be very useful if SSIM and its relatives (e.g. MS-SSIM) could have their parameters (such as window size and sigma) set as defaults but be made alterable in the user interface. This is already the case in the implementation on skimage (skimage.measure.compare_ssim). \r\n\r\n\r\n**Will this change the current api? How?**\r\nNot much. If the window size and other parameters are set as defaults, instead of being fixed values, most people will never notice the difference. There would be no backward incompatible changes.\r\n\r\n\r\n**Who will benefit with this feature?**\r\nPeople who wish to use SSIM as a loss function.\r\n\r\n\r\n**Any Other info.**\r\nIn the current implementation many variables are fixed in the script instead of being defined in the functions. \r\n\r\nMoreover, functions such as SSIM in the current Tensorflow image_ops implementation are heavily intertwined with many other functions in the page.\r\n\r\nThis makes it very difficult for outsiders to simply copy-paste the necessary parts out to change only the parts that they need. \r\n\r\nIt would be great if the API changed to allow parameter tuning of key hyperparameters such as window size.\r\n\r\nMany thanks in advance to those who look into this issue.", "comments": ["@veritas9872 there are only 2 hyper-parameter for SSIM filter and sigma [ssim](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py#L2912-L2913) so we are supposed to add these two as default parameter? ", "@jvishnuvardhan are you working on this issue ? \ud83d\ude04 would love to help", "Hello. Thank you for helping out. \r\n@Gurpreetsingh9465 It is my understanding that there are also K1 and K2 parameters. However, I don't understand their function perfectly.\r\nAlso, it would be great if MS-SSIM (Multi-scale SSIM) could also have default parameters. \r\nMany thanks again for helping me out.", "@veritas9872 sir, added k1 and k2 as default arguments \ud83d\udc4d  [changes](https://github.com/tensorflow/tensorflow/pull/27076).  and sir can you open a new issue regarding mssim so that the code didn't clash. ", "@Gurpreetsingh9465 Thank you very much. \r\nHowever, I find that many of the checks were not successful. \r\nI will open a separate issue for MS-SSIM.\r\nThank you for helping out.", "@Gurpreetsingh9465 I opened a separate issue for MS-SSIM at  https://github.com/tensorflow/tensorflow/issues/27138 \r\nI also suggested that some values (such as K1 and K2) might be better left as keyword arguments (as is the case in skimage) since almost no one ever changes them.", "I closed the issue on MS-SSIM because I saw that everything had already been done.\r\n@Gurpreetsingh9465 Thank you for your help on this issue!", "@veritas9872 Is this resolved or still an issue? If it was resolved, could you close it. Thanks!", "Thank you. I think that this issue has been placed in a branch, though it has not been merged to master yet. \r\nI will close this issue. Thank you everyone for helping out."]}, {"number": 26928, "title": "Memory always exceeds 10% system memory", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): conda gpu version\r\n- TensorFlow version (use command below): 1.10\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: Quadro GP100 16Gb\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nIn the linked [colab] I try to train a WALSModel, with a sparse matrix with shape `(210000, 44000)`\r\nWhen I try to run it locally on my Quadro I get: \r\n\r\n```\r\n Allocator (GPU_0_bfc) ran out of memory trying to allocate 17.32GiB.  Current allocation summary follows.\r\n\r\n```\r\n\r\nWhich, ok, I have a 16GB memory card, that makes sense, I guess.\r\n\r\nSince this is run on GPU it kills the process.\r\n\r\nSo I set my session config as follows to use the CPU which has 100Gb of Ram. Surely enough for the 17.32GiB that TF tried to allocate:\r\n\r\n```python\r\n# inside train_fn\r\nconfig = tf.ConfigProto(device_count = {'GPU': 0})\r\nwith tf.Session(config=config) as sess:\r\n    #...\r\n```\r\n\r\nand lo and behold:\r\n\r\n```\r\nAllocation of 18599850000 exceeds 10% of system memory.\r\n```\r\n\r\nI get that the sparse matrix is not exactly small, but it is not excessively large either. Storing the coordinate form on disk is <2 gb. so What is with the rampant memory usage?\r\n\r\nPlease help.\r\n\r\nFull logs in last cell of colab\r\n\r\n[colab]: https://colab.research.google.com/drive/1oJDfvIWnf7sY5uFcJ7XhTSEdA1n_3Jxv#scrollTo=8pqa0A7BUrG-\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["tl;dr: the issue is that the linked script tries to process the entire sparse matrix in one iteration. Batching the sparse input matrix will solve the problem.\r\n\r\nDetailed explanation: Suppose the embedding dimension is d, and the number of rows in a batch is n. When processing a batch, the update op needs to solve n linear systems, each involving a different d x d matrix. The combined linear system has size O(nd^2), see the output tensor of the [WALSComputePartialLhsAndRhsOp kernel](https://github.com/tensorflow/tensorflow/blob/e86f5c2b735cd4f3045b41ff6eab14305190176a/tensorflow/contrib/factorization/kernels/wals_solver_ops.cc#L140)\r\nthe shape of the output shape is `[block_size, factor_dim, factor_dim]` where `block_size` is n and `factor_dim` is d.\r\n\r\nIn the example colab, d = 150 and n = 210000 (for the row solve), so the kernel allocates n d^2 floats which corresponds roughly to the 18GB seen in the error message.\r\n\r\nThis problem can be solved by batching the sparse tensor, you can follow the example `input_fn` in [wals_test.py](https://github.com/tensorflow/tensorflow/blob/f07558116ac7c90858cf0572a1bca1e50e208a37/tensorflow/contrib/factorization/python/ops/wals_test.py#L141).\r\n\r\nNote that it is not enough to use `tf.batch` on the sparse tensor, as this will change the row ids (the resulting row id is a 'local' index within the batch instead of the 'global' index within the original matrix). Instead, one should batch then remap the index, this is what the function `remap_sparse_tensor_rows ` in the example is doing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26928\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26928\">No</a>\n", "@walidk  @agarwal-ashish the linked test code is for the `tf.estimator` wrapper of `WALSModel`, `WALSMatrixFactorization` and does not provide sufficient comments / guidance as to how to make and run a sharded `WALSModel`. In addition, the documentation for `WALSModel` is far from sufficient. The pseudo-code leads the reader down a rabbit hole of documentation back to the `.proto` files, and that is just for setting up the training environment. It is no where addressed in that documentation how to handle sharding the sparse tensor.\r\n\r\nin this other [colab](https://colab.research.google.com/drive/1xGe3lRiKnwPuhc1FfgdZiayEE8rQQwvM)\r\nI start to fill in as much of the pseudo-code as possible including using the relevant code linked by @walidk \r\n\r\nAs it currently stands this issue has only been semi-addressed in that why, when running on a GPU, this error occurs, however this error also occurs when running on a CPU with 100GiB ram. It runs on the CPU, but it still claims to over allocate.\r\n\r\nThe linked code is appreciated, but it is in sufficient to address how to resolve the issue completely. \r\nIn short documentation for WALSModel with sharding is needed."]}, {"number": 26927, "title": "Compile error for overlapping nets with Tf.Keras metrics", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: macOS 10.14\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below):  broken with 1.13.1, works with 1.12.0\r\n- Python version: 3.6.3\r\n\r\n**Describe the current behavior**\r\nCompiling concatenated networks with Keras metrics causes an InvalidArgumentError in the input of the second network (You must feed a value for placeholder tensor 'dense_5_target' with dtype float and shape [?,?]). This used to work in 1.12.0 but broke with 1.13.1. Without metrics it's not an issue.\r\n\r\n**Describe the expected behavior**\r\nThe networks should be trainable individually and in the concatenated version, no matter if we specify metrics or not.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n# coding: utf-8\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model, Sequential\r\nfrom tensorflow.keras.layers import Dense, Input\r\n\r\n# works with 1.12, fails with 1.13.1\r\nprint(tf.__version__)\r\n\r\n# Layer sizes of net 1 and 2\r\nn_input_1 = 5\r\nn_output_1 = 4\r\n\r\nn_input_2 = 4\r\nn_output_2 = 2\r\n\r\n# Generating dummy data\r\nN = 13\r\nx1 = np.random.rand(N, n_input_1)\r\ny1 = np.random.rand(N, n_output_1)\r\n\r\nx2 = np.random.rand(N, n_output_1)\r\ny2 = np.random.rand(N, n_output_2)\r\n\r\n\r\n# Build net1, net2 and net_full\r\n# net_full concats net1 and net2\r\ndef build_models():\r\n    input_layer_1 = Input(shape=(n_input_1,))\r\n    output_layer_1 = Dense(n_output_1)(input_layer_1)\r\n    net1 = Model(inputs=input_layer_1, outputs=output_layer_1, name=\"net1\")\r\n\r\n    input_layer_2 = Input(shape=(n_input_2,), name = \"topmodel_input\")\r\n    output_layer_2 = Dense(n_output_2)(input_layer_2)\r\n    net2 = Model(inputs=input_layer_2, outputs=output_layer_2, name=\"net2\")\r\n\r\n    net_full = Model(inputs=input_layer_1, outputs=net2(net1.output))\r\n    \r\n    return net2, net_full\r\n\r\nnet2, net_full = build_models()\r\n# compile with passing metrics only to net2 --> runs\r\nnet2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\r\nnet_full.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\nnet_full.fit(x1, y2)\r\n\r\nnet2, net_full = build_models()\r\n# compile with passing metrics only to net_full --> runs\r\nnet2.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\nnet_full.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\r\nnet_full.fit(x1, y2)\r\n\r\nnet2, net_full = build_models()\r\n# compile with passing metrics to both net2 AND net_full --> crashes\r\nnet2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\r\nnet_full.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\r\nnet_full.fit(x1, y2)\r\n```\r\n\r\nI think this is a bug. If you consider this improper use instead, please let me know.\r\n\r\nBest, Boris", "comments": ["For TF 2.0, it has been reported as https://github.com/tensorflow/tensorflow/issues/26738. From what you say, it seems that it also affects 1.13.", "659c981 should fix this issue. Please give it a try in the next nightly and let me know if it works as expected.\r\n\r\nThank you!", "The fix was verified by @foxik in #26738 . Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26927\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26927\">No</a>\n"]}, {"number": 26926, "title": "Fix typo in doc", "body": "Nit", "comments": []}, {"number": 26925, "title": "\"tensorflow/core/framework/attr_value.pb.h\"", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n- Doc Link:\r\n\r\n\r\n**Describe the documentation issue**\r\nin \"tensorflow/core/framework/function.h\" file inclued \"tensorflow/core/framework/attr_value.pb.h\" file, but i can not find the file. please tell me where is the dir(\"tensorflow/core/framework/attr_value.pb.h\"). thanks\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["It is autogenerated from the protocol buffers", "@laiqb I had it in one of my virtual environment /usr/local/google/home/vishnuvardhanj/venv/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/framework.\r\n\r\nIf there is any issue, please fill issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md) so that we can resolve it as soon as possible. If this is not related to bug or build installation issue, then close here and post it in stackoverflow. Thanks!"]}, {"number": 26924, "title": "iOS example compile error", "body": "iOS example app compile error because \"third_party/tensorflow/core/framework/types.h\" file not found. (Xcode 10.1)\r\n\r\nI can fix this bug by modify \r\n`#include \"third_party/tensorflow/core/framework/types.h\" `\r\nto\r\n`#include \"tensorflow/core/framework/types.h\" `\r\n(tensorflow/tensorflow/examples/ios/simple/ios_image_load.h # line20)\r\n\r\nMany thanks!", "comments": ["Seems to be fixed by https://github.com/tensorflow/tensorflow/pull/31506. Closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26924\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26924\">No</a>\n"]}, {"number": 26923, "title": "Add Registration for non-stateful scatter_nd_min and scatter_nd_max. Fixes issue #20402", "body": "Modified tensorflow/tensorflow/core/kernels/scatter_nd_op.cc to add registration for scatter_min and scatter_max ops.", "comments": ["Great. I will start working on it as soon as I can.", "Great. Let me know if you need any more specific pointers. Searching for the names of the existing ops (TensorScatterAdd and ResourceScatterNdAdd) in the TF codebase will mostly point you to the right places to change (except you'll also catch a few generated files, like ops.pbtxt, if you do that)", "I've created the .pbtxt files for ResourceScatterNdMax/Min and TensorScatterMax/Min. I'm having a bit trouble as to what to enter in the summary section in the files.\r\n\r\nEg: ResourceScatterNdMin.pbtxt file.\r\n\r\n```\r\nop {\r\n  graph_op_name: \"ResourceScatterNdMin\"\r\n  in_arg {\r\n    name: \"ref\"\r\n    description: <<END\r\nA resource handle. Must be from a VarHandleOp.\r\nEND\r\n  }\r\n  in_arg {\r\n    name: \"indices\"\r\n    description: <<END\r\nA Tensor. Must be one of the following types: int32, int64.\r\nA tensor of indices into ref.\r\nEND\r\n  }\r\n  in_arg {\r\n    name: \"updates\"\r\n    description: <<END\r\nA Tensor. Must have the same type as ref. A tensor of\r\nvalues whose element wise min is taken with ref.\r\nEND\r\n  }\r\n  attr {\r\n    name: \"use_locking\"\r\n    description: <<END\r\nAn optional bool. Defaults to True. If True, the assignment will\r\nbe protected by a lock; otherwise the behavior is undefined,\r\nbut may exhibit less contention.\r\nEND\r\n  }\r\n```\r\n\r\nEg. TensorScatterMin.pbtxt\r\n```\r\nop {\r\n  graph_op_name: \"TensorScatterMin\"\r\n  in_arg {\r\n    name: \"tensor\"\r\n    description: <<END\r\nTensor to update.\r\nEND\r\n  }\r\n  in_arg {\r\n    name: \"indices\"\r\n    description: <<END\r\nIndex tensor.\r\nEND\r\n  }\r\n  in_arg {\r\n    name: \"updates\"\r\n    description: <<END\r\nUpdates to scatter into output.\r\nEND\r\n  }\r\n  out_arg {\r\n    name: \"output\"\r\n    description: <<END\r\nA new tensor copied from tensor whose values are element-wise minimum between \r\ntensor and updates according to the indices.\r\nEND\r\n}\r\n```\r\n\r\nAlso should both ScatterNdMax/Min and ResourceScatterMax/Min ops be hidden in the python API ?\r\nAnd in which files should I write unit tests for these new ops?", "Re what to put in the summary section, for those hidden ops it doesn't matter too much.\r\n\r\nYes, scatter_nd_min and resource_scatter_nd_min should be hidden. Expose them by adding scatter_nd_max.min methods to ResourceVariable which call the ops (see how the other scatter methods there are implemented).", "Also the unit tests for the resource scatter can go in tensorflow/python/kernel_tests/scatter_nd_ops_test.py", "API golden files cannot be generated as tensorflow is giving the error \r\nFile \"api_compatibility_test.py\", line 46, in <module>\r\n    from tensorflow.tools.api.lib import api_objects_pb2\r\nModuleNotFoundError: No module named 'tensorflow.tools.api'\r\n\r\nI've looked into similar issues and installed the latest release of tensorflow using pip but still this error is not resolving. Do I have to build tensorflow from source?", "You need to run the test with bazel run\n\nOn Wed, Apr 3, 2019 at 9:12 AM Pratik Joshi <notifications@github.com>\nwrote:\n\n> API golden files cannot be generated as tensorflow is giving the error\n> File \"api_compatibility_test.py\", line 46, in\n> from tensorflow.tools.api.lib import api_objects_pb2\n> ModuleNotFoundError: No module named 'tensorflow.tools.api'\n>\n> I've looked into similar issues and installed the latest release of\n> tensorflow using pip but still this error is not resolving. Do I have to\n> build tensorflow from source?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26923#issuecomment-479555508>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxfNs-OObxsmZFKY87pMhncBSrYoHks5vdNLkgaJpZM4b-o80>\n> .\n>\n\n\n-- \n - Alex\n", "Bazel build failed as Scatter_Nd_UpdateOp does not contain MAX and MIN definitions. I am defining them in tensorflow/tensorflow/core/kernels/dense_update_functor.cc and it's related files according to the definitions already present in them and the ones in scatter_functor.cc.", "Ah makes sense. Thanks!\n\nOn Thu, Apr 4, 2019 at 10:43 PM Pratik Joshi <notifications@github.com>\nwrote:\n\n> Bazel build failed as Scatter_Nd_UpdateOp does not contain MAX and MIN\n> definitions. I am defining them in\n> tensorflow/tensorflow/core/kernels/dense_update_functor.cc and it's related\n> files according to the definitions already and the ones in\n> scatter_functor.cc.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26923#issuecomment-480154553>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxUIhOaPSfdL2ANTIgvRM6GydABgmks5vduJvgaJpZM4b-o80>\n> .\n>\n\n\n-- \n - Alex\n", "I've implemented the changes but the build process is failing when I try to generate the api_golden files. Can you tell me how to resolve this error?\r\n```\r\nERROR: /home/pj/Code/OpenSource/tensorflow/tensorflow/BUILD:713:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash failed: error executing command \r\n  (cd /home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/pj/anaconda3/envs/tflow/bin:/home/pj/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1 --root_init_template=tensorflow/api_template_v1.__init__.py --apidir=bazel-out/k8-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --package=tensorflow.python,tensorflow.lite.python.lite --output_package=tensorflow._api.v1 bazel-out/k8-opt/genfiles/tensorflow/_api/v1/v1.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/audio/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/autograph/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/autograph/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/config/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/config/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/config/gpu/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/config/optimizer/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/config/threading/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/distribute/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/distribute/cluster_resolver/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/distribute/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/io/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/queue/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/layers/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/lite/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/lite/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/lite/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/lite/experimental/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/lookup/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/lookup/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nest/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/ragged/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/random/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/raw_ops/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/signal/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/tpu/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/tpu/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/version/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/audio/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/autograph/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/autograph/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/bitwise/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/compat/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/config/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/config/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/config/gpu/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/config/optimizer/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/config/threading/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/data/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/data/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/debugging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/distribute/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/distribute/cluster_resolver/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/distribute/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/dtypes/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/errors/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/feature_column/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/io/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/graph_util/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/queue/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/linalg/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/lite/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/lite/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/lite/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/lite/experimental/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/lookup/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/lookup/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/math/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/nest/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/quantization/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/ragged/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/random/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/random/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/raw_ops/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/saved_model/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/sets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/signal/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/sparse/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/strings/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/summary/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/summary/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/sysconfig/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/test/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/tpu/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/tpu/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/train/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/train/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v2/version/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/app/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/audio/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/autograph/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/autograph/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/bitwise/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/compat/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/config/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/config/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/config/gpu/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/config/optimizer/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/config/threading/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/data/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/data/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/debugging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/distribute/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/distribute/cluster_resolver/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/distribute/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/distributions/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/dtypes/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/errors/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/feature_column/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/io/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/graph_util/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/queue/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/layers/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/linalg/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/lite/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/lite/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/lite/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/lite/experimental/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/logging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/lookup/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/lookup/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/manip/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/math/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/nest/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/nn/rnn_cell/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/profiler/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/python_io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/quantization/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/ragged/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/random/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/random/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/raw_ops/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/resource_loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/strings/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/builder/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/main_op/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/signature_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/signature_def_utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/tag_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/saved_model/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/sets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/signal/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/sparse/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/spectral/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/summary/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/sysconfig/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/test/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/tpu/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/tpu/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/train/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/train/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/train/queue_runner/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/user_ops/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/v1/version/__init__.py')\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor16ScatterNdFunctorIN5Eigen16ThreadPoolDeviceESt7complexIfExLNS_13scatter_nd_op8UpdateOpE3ELi7EEclERKS3_xNS2_5arrayIlLm7EEENS2_9TensorMapINS2_6TensorIS5_Li2ELi1ElEELi16ENS2_11MakePointerEEENSD_INSE_IKxLi2ELi1ElEELi16ESG_EENSD_INSE_IKS5_Li2ELi1ElEELi16ESG_EESH_\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor16ScatterNdFunctorIN5Eigen16ThreadPoolDeviceESt7complexIfExLNS_13scatter_nd_op8UpdateOpE3ELi7EEclERKS3_xNS2_5arrayIlLm7EEENS2_9TensorMapINS2_6TensorIS5_Li2ELi1ElEELi16ENS2_11MakePointerEEENSD_INSE_IKxLi2ELi1ElEELi16ESG_EENSD_INSE_IKS5_Li2ELi1ElEELi16ESG_EESH_\r\n```\r\n\r\nI've implemented the functors for the new functions as follows:\r\nIn **scatter_functor_gpu.cu.h**\r\n```\r\ntemplate <typename T>\r\nstruct ScatterOpKernelBody<T, scatter_op::UpdateOp::MIN> {\r\n  __device__ void operator()(T* dest, T src) const { CudaAtomicMin(dest, src); }\r\n};\r\n\r\ntemplate <typename T>\r\nstruct ScatterOpKernelBody<T, scatter_op::UpdateOp::MAX> {\r\n  __device__ void operator()(T* dest, T src) const { CudaAtomicMax(dest, src); }\r\n};\r\n```\r\nIn **scatter_nd_op_cpu_impl.h**\r\n```\r\ntemplate <typename Input, typename Update, typename Output>\r\nclass UpdateExecutor<Input, Update, Output, scatter_nd_op::UpdateOp::MAX> {\r\n public:\r\n  EIGEN_STRONG_INLINE static void Execute(Input  value, Update update,\r\n                                          Output output) {\r\n    output = value.cwiseMax(update);\r\n  }\r\n};\r\n\r\ntemplate <typename Input, typename Update, typename Output>\r\nclass UpdateExecutor<Input, Update, Output, scatter_nd_op::UpdateOp::MIN> {\r\n public:\r\n  EIGEN_STRONG_INLINE static void Execute(Input value, Update update,\r\n                                          Output output) {\r\n    output = value.cwiseMin(update);\r\n  }\r\n};\r\n```\r\nIn **scatter_nd_op_gpu.cc**\r\n```\r\ntemplate <typename T>\r\nstruct LeftUpdate<T, scatter_nd_op::UpdateOp::MAX> {\r\n  EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC void operator()(T* out, const T& val) {\r\n    CudaAtomicMax(out, val);\r\n  }\r\n};\r\n\r\ntemplate <typename T>\r\nstruct LeftUpdate<T, scatter_nd_op::UpdateOp::MIN> {\r\n  EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC void operator()(T* out, const T& val) {\r\n    CudaAtomicMin(out, val);\r\n  }\r\n};\r\n```\r\n\r\nEarlier the build process was failing due to errors in these files, so I modified scatter_nd_op.cc and removed MAX/MIN definitions for complex types.", "@pratjosh9 you say you've made all these changes but they don't show up on the \"files changed\" tab of the PR.\r\n\r\nCan you make sure you made the changes to the right branch?", "The changes are on my local system. Since I couldn't generate the api golden files due to the bazel error (mentioned above). I've not added the changed files to the PR. The changes I've made are mentioned above. \r\n\r\nShould I add changes (without api golden files) to the PR?", "Yes\n\nOn Mon, Apr 22, 2019 at 11:49 PM Pratik Joshi <notifications@github.com>\nwrote:\n\n> The changes are on my local system. Since I couldn't generate the api\n> golden files due to the bazel error (mentioned above). I've not added the\n> changed files to the PR. The changes I've made are mentioned above.\n>\n> Should I add changes (without api golden files) to the PR?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26923#issuecomment-485664126>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRI6ZR3267I55V7BRJTPR2WN3ANCNFSM4G72R42A>\n> .\n>\n\n\n-- \n - Alex\n", "Done", "Do you get the bazel error when you do `bazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens=True`?\r\n\r\nIf so, can you post the full stack trace here?", "> Do you get the bazel error when you do `bazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens=True`?\r\n> \r\n> If so, can you post the full stack trace here?\r\n\r\nI've removed the unnecessary changes and tried to generate the api_golden files. This is the error that I get. Can it be because of tensorflow being installed using Anaconda?\r\n\r\n```\r\nERROR: /home/pj/Code/OpenSource/tensorflow/tensorflow/python/keras/api/BUILD:45:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor16ScatterNdFunctorIN5Eigen16ThreadPoolDeviceESt7complexIfExLNS_13scatter_nd_op8UpdateOpE3ELi7EEclERKS3_xNS2_5arrayIlLm7EEENS2_9TensorMapINS2_6TensorIS5_Li2ELi1ElEELi16ENS2_11MakePointerEEENSD_INSE_IKxLi2ELi1ElEELi16ESG_EENSD_INSE_IKS5_Li2ELi1ElEELi16ESG_EESH_\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/pj/.cache/bazel/_bazel_pj/f508fdcc670cd71c05e44d5ea5f363d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor16ScatterNdFunctorIN5Eigen16ThreadPoolDeviceESt7complexIfExLNS_13scatter_nd_op8UpdateOpE3ELi7EEclERKS3_xNS2_5arrayIlLm7EEENS2_9TensorMapINS2_6TensorIS5_Li2ELi1ElEELi16ENS2_11MakePointerEEENSD_INSE_IKxLi2ELi1ElEELi16ESG_EENSD_INSE_IKS5_Li2ELi1ElEELi16ESG_EESH_\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/api/tests:api_compatibility_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2402.569s, Critical Path: 2076.87s\r\nINFO: 406 processes: 406 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "@pratjosh9 your bazel tensorflow setup is borked. Did you run ./configure?", "> @pratjosh9 your bazel tensorflow setup is borked. Did you run ./configure?\r\n\r\nYes. I configured it to compile with clang this time. \r\n\r\nIn the file [scatter_nd_op_cpu_impl.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op_cpu_impl.h)\r\n\r\nif I define the MIN op as \r\n```C++\r\ntemplate <typename Input, typename Update, typename Output>\r\nclass UpdateExecutor<Input, Update, Output, scatter_nd_op::UpdateOp::MIN> {\r\n public:\r\n  EIGEN_STRONG_INLINE static void Execute(Input value/* input */, Update update,\r\n                                          Output output) {\r\n    output = value.cwiseMin(update);\r\n  }\r\n};\r\n```\r\nI get the error mentioned [here](https://github.com/tensorflow/tensorflow/pull/26923#issuecomment-485221991).\r\n\r\n\r\n\r\nAnd, If I define the MIN op as \r\n```C++\r\ntemplate <typename Input, typename Update, typename Output>\r\nclass UpdateExecutor<Input, Update, Output, scatter_nd_op::UpdateOp::MIN> {\r\n public:\r\n  EIGEN_STRONG_INLINE static void Execute(Input /* input */, Update update,\r\n                                          Output output) {\r\n    output = output.cwiseMin(update);\r\n  }\r\n};\r\n```\r\nI get the following error\r\n```\r\n./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h:99:21: error: no member named 'cwiseMin' in 'Eigen::TensorDevice<Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>'\r\n    output = output.cwiseMin(update);\r\n             ~~~~~~ ^\r\n./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h:150:18: note: in instantiation of member function 'tensorflow::update_executor::UpdateExecutor<Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, MakePointer> >, Eigen::TensorChippingOp<0, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long>, 16, MakePointer> >, Eigen::TensorDevice<Eigen::TensorChippingOp<0, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>, tensorflow::scatter_nd_op::UpdateOp::MIN>::Execute' requested here\r\n            OP>::Execute(input_chip, update_chip, output_chip);\r\n```\r\n\r\nIn the definition of [Eigen::TensorDevice](https://eigen.tuxfamily.org/dox/unsupported/TensorDevice_8h_source.html) it turns out that only +=, -= and the = operators are supported, using which the ops  `scatter_nd::UpdateOp ADD`, `scatter_nd::UpdateOp SUB` and `scatter_nd::UpdateOp ASSIGN` are implemented.\r\n\r\nSo how do I define the MIN and MAX functions without using cwiseMin() and cwiseMax() functions.", "@rmlarsen I think this goes beyond my knowledge of eigen; do you know what the right answer is here?", "Any updates?", "Let me take a look...", "Is there anything with which I can help with in this issue?", "@pratjosh9 This code has a little too much C++ magic for its own good. The functors should be changed to take both the output TensorMap and the device as separate arguments. So instead of \r\n```\r\n      } else {\r\n        auto input_chip = Toutput.template chip<0>(i);\r\n        auto output_chip = input_chip.device(d);\r\n        auto update_chip = Tupdates.template chip<0>(loc);\r\n        update_executor::UpdateExecutor<\r\n            decltype(input_chip), decltype(update_chip), decltype(output_chip),\r\n            OP>::Execute(input_chip, update_chip, output_chip);\r\n        ...\r\n```\r\n\r\nYou would call   this as\r\n\r\n```\r\n        auto input_chip = Toutput.template chip<0>(i);\r\n        auto output_chip = input_chip;\r\n        auto update_chip = Tupdates.template chip<0>(loc);\r\n        update_executor::UpdateExecutor<\r\n            DeviceType, decltype(input_chip), decltype(update_chip), decltype(output_chip),\r\n            OP>::Execute(d, input_chip, update_chip, output_chip);\r\n```\r\n\r\nwhere the functor should be defined as:\r\n\r\n```\r\ntemplate <typename DeviceType, typename Input, typename Update, typename Output>\r\nclass UpdateExecutor<DeviceType, Input, Update, Output, scatter_nd_op::UpdateOp::ADD> {\r\n public:\r\n  EIGEN_STRONG_INLINE static void Execute(DeviceType device, Input /* input */, Update update,\r\n                                          Output output) {\r\n    output.device(device) += update;\r\n  }\r\n};\r\n```\r\n   \r\n\r\n", "I've implemented the above mentioned changes. I'm getting the following error\r\n```\r\n./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h:151:26: error: no viable conversion from 'const tensorflow::CPUDevice' (aka 'const Eigen::ThreadPoolDevice') to 'const tensorflow::DeviceType'\r\n```\r\nShould I implement separate set of templates for CPUDevice and SYCLDevice as I couldn't find any base type for these two devices? Even using Eigen::DeviceType results in a similar error given as follows\r\n```\r\n./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h:151:26: error: no viable conversion from 'const tensorflow::CPUDevice' (aka 'const Eigen::ThreadPoolDevice') to 'const int'\r\n```\r\n", "Any updates?", "@pratjosh9 It looks like you are mixing device type enums and device types. For new ops you don't have to implement them for SYCL unless you want to, but most ops have such implementations and related instantiations.", "Can one of the admins verify this patch?", "@rmlarsen I've made the requested changes. And I've got the following error when trying to build tensorflow.\r\n\r\n```\r\nERROR: /home/pj/Projects/OpenSource/tensorflow/tensorflow/BUILD:713:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor16ScatterNdFunctorIN5Eigen16ThreadPoolDeviceESt7complexIfEiLNS_13scatter_nd_op8UpdateOpE3ELi1EEclERKS3_iNS2_5arrayIlLm1EEENS2_9TensorMapINS2_6TensorIS5_Li2ELi1ElEELi16ENS2_11MakePointerEEENSD_INSE_IKiLi2ELi1ElEELi16ESG_EENSD_INSE_IKS5_Li2ELi1ElEELi16ESG_EESH_\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/pj/anaconda3/envs/tflow/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/pj/.cache/bazel/_bazel_pj/ee5d99083eabb25e24858d5137604fab/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor16ScatterNdFunctorIN5Eigen16ThreadPoolDeviceESt7complexIfEiLNS_13scatter_nd_op8UpdateOpE3ELi1EEclERKS3_iNS2_5arrayIlLm1EEENS2_9TensorMapINS2_6TensorIS5_Li2ELi1ElEELi16ENS2_11MakePointerEEENSD_INSE_IKiLi2ELi1ElEELi16ESG_EENSD_INSE_IKS5_Li2ELi1ElEELi16ESG_EESH_\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/api/tests:api_compatibility_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 10519.098s, Critical Path: 503.64s\r\nINFO: 8387 processes: 8387 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "Is this an issue currently resolved?", "@pratjosh9 can you please resolve conflicts ?", "> @pratjosh9 can you please resolve conflicts ?\r\n\r\nResolved", "@pratjosh9  Could you please check failed build errors? Thanks!", "> @pratjosh9 Could you please check failed build errors? Thanks!\r\n\r\nI'm removing declarations for min / max operations made for complex data types. \r\n\r\nIs there anyway to build and check these changes online? The build process on my local system is very slow and often lead to a non-responsive system. Thanks!", "@rthadur @alextp Can anyone generate API goldens? The process is getting stuck on my laptop. The errors in the CPU build process were related to the goldens not being updated, and the errors in GPU build process were due to a typo in the name of a macro call and has been fixed.", "@pratjosh9 sure will try to generate goldens internally, thank you", "Also, can you please fix tests. It cannot be imported properly at the moment", "@pratjosh9 Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "> Also, can you please fix tests. It cannot be imported properly at the moment\r\n\r\nThe tests are failing because the API goldens are outdated. Unfortunately, I can't generate goldens as my current machine does not have the required compute power to build TensorFlow.", "@mihaimaruseac Any update on this PR? Please. Thanks!", "I imported the PR internally but there are ~5000 failing tests. Looking for a fix", "So, the failure to generate the goldens is not because something being broken in the VM. Instead, this PR causes linker errors which manifest as `_ZN10tensorflow7functor16ScatterNdFunctorIN5Eigen16ThreadPoolDeviceESt7complexIfExLNS_13scatter_nd_op8UpdateOpE3ELi7EEclERKS3_xNS2_5arrayIlLm7EEENS2_9TensorMapINS2_6TensorIS5_Li2ELi1ElEELi16ENS2_11MakePointerEEENSD_INSE_IKxLi2ELi1ElEELi16ESG_EENSD_INSE_IKS5_Li2ELi1ElEELi16ESG_EESH_` (and similar not being found).\r\n\r\nProbable reason (one of the many, you'll actually have to look closely at the build log and see more linker failures and fix them one by one):\r\n\r\n```cc\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/libscatter_nd_op.lo(scatter_nd_op.o): In function `tensorflow::Status tensorflow::functor::DoScatterNd<Eigen::GpuDevice, std::complex<double>, long long, (tensorflow::scatter_nd_op::UpdateOp)4>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::TensorShape const&, tensorflow::Tensor*, bool)':\r\nscatter_nd_op.cc:(.text._ZN10tensorflow7functor11DoScatterNdIN5Eigen9GpuDeviceESt7complexIdExLNS_13scatter_nd_op8UpdateOpE4EEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorESD_RKNS_11TensorShapeEPSB_b[_ZN10tensorflow7functor11DoScatterNdIN5Eigen9GpuDeviceESt7complexIdExLNS_13scatter_nd_op8UpdateOpE4EEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorESD_RKNS_11TensorShapeEPSB_b]+0x319): undefined reference to `tensorflow::functor::ScatterNdFunctor<Eigen::GpuDevice, std::complex<double>, long long, (tensorflow::scatter_nd_op::UpdateOp)4, 7>::operator()(Eigen::GpuDevice const&, long long, Eigen::array<long, 7ul>, Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<long long const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<std::complex<double> const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<std::complex<double>, 2, 1, long>, 16, Eigen::MakePointer>)'\r\n... (many many many more lines)\r\nscatter_nd_op.cc:(.text._ZN10tensorflow7functor11DoScatterNdIN5Eigen9GpuDeviceESt7complexIfEiLNS_13scatter_nd_op8UpdateOpE3EEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorESD_RKNS_11TensorShapeEPSB_b[_ZN10tensorflow7functor11DoScatterNdIN5Eigen9GpuDeviceESt7complexIfEiLNS_13scatter_nd_op8UpdateOpE3EEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorESD_RKNS_11TensorShapeEPSB_b]+0x842): undefined reference to `tensorflow::functor::ScatterNdFunctor<Eigen::GpuDevice, std::complex<float>, int, (tensorflow::scatter_nd_op::UpdateOp)3, 6>::operator()(Eigen::GpuDevice const&, int, Eigen::array<long, 6ul>, Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<std::complex<float> const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 2, 1, long>, 16, Eigen::MakePointer>)'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\nInstead of building everything you can also try building smaller targets that depend on `scatter_nd_op`.\r\n\r\nInternally we have nearly 5000 tests failing with linker errors such as\r\n\r\n```cc\r\nld.lld: error: undefined symbol: tensorflow::functor::ScatterNdFunctor<Eigen::GpuDevice, std::__u::complex<float>, int, (tensorflow\r\n::scatter_nd_op::UpdateOp)4, 6>::operator()(Eigen::GpuDevice const&, int, Eigen::array<long, 6ul>, Eigen::TensorMap<Eigen::Tensor<s\r\ntd::__u::complex<float>, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 2, 1, long>, 16, Eigen::Ma\r\nkePointer>, Eigen::TensorMap<Eigen::Tensor<std::__u::complex<float> const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<E\r\nigen::Tensor<std::__u::complex<float>, 2, 1, long>, 16, Eigen::MakePointer>)                                                       \r\n>>> referenced by scatter_nd_op.cc                                                                                                 \r\n>>>               blaze-out/k8-cuda101-opt/bin/third_party/tensorflow/core/kernels/_objs/scatter_nd_op/scatter_nd_op.o:(tensorflow:\r\n:Status tensorflow::functor::DoScatterNd<Eigen::GpuDevice, std::__u::complex<float>, int, (tensorflow::scatter_nd_op::UpdateOp)4>(t\r\nensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::TensorShape const&, tensorflow::Tens\r\nor*, bool))  \r\n```", "Solved the linker error. Will be merged internally. Nothing to do here", "@pratjosh9 Can you please check @mihaimaruseac's comments and resolve conflicts?. Thanks!", "This is handled internally. No need to change here (especially since changing will mean resolving merge conflicts first)", "Thank you @pratjosh9 for the PR. It has been merged now."]}, {"number": 26922, "title": "AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes'", "body": "\r\nusing keras API in tensorflow 2.0", "comments": ["How do you import keras, are you importing from tensorflow? Have you tried import tf.keras?", "yes i have used tf.keras", "UnknownError                              Traceback (most recent call last)\r\n<ipython-input-14-c90c3b2c3d99> in <module>\r\n      3 earlystopper = tf.keras.callbacks.EarlyStopping(patience=5, verbose=1)\r\n      4 checkpointer = tf.keras.callbacks.ModelCheckpoint('test_1.h5', verbose=1, save_best_only=True)\r\n----> 5 results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=16, epochs=50,shuffle=True,steps_per_epoch=None)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874 \r\n    875   def evaluate(self,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    350 \r\n    351         # Get outputs.\r\n--> 352         batch_outs = f(ins_batch)\r\n    353         if not isinstance(batch_outs, list):\r\n    354           batch_outs = [batch_outs]\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3237         value = math_ops.cast(value, tensor.dtype)\r\n   3238       converted_inputs.append(value)\r\n-> 3239     outputs = self._graph_fn(*converted_inputs)\r\n   3240     return nest.pack_sequence_as(self._outputs_structure,\r\n   3241                                  [x.numpy() for x in outputs])\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    538       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n    539           list(kwargs.keys()), list(self._arg_keywords)))\r\n--> 540     return self._call_flat(args)\r\n    541 \r\n    542   def _filtered_call(self, args, kwargs):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    607     # Only need to override the gradient in graph mode and when we have outputs.\r\n    608     if context.executing_eagerly() or not self.outputs:\r\n--> 609       outputs = self._inference_function.call(ctx, args)\r\n    610     else:\r\n    611       self._register_gradient()\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    414             attrs=(\"executor_type\", executor_type,\r\n    415                    \"config_proto\", config),\r\n--> 416             ctx=ctx)\r\n    417       # Replace empty list with None\r\n    418       outputs = outputs or None\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nUnknownError: AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes'\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/adventum/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 205, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/home/adventum/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 107, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"<ipython-input-12-1bbcb510438c>\", line 58, in iou_metric_batch\r\n    value = iou_metric(y_true_in[batch], y_pred_in[batch])\r\n\r\n  File \"<ipython-input-12-1bbcb510438c>\", line 2, in iou_metric\r\n    labels = label(y_true_in > 0.5)\r\n\r\n  File \"/home/adventum/anaconda3/lib/python3.7/site-packages/skimage/measure/_label.py\", line 93, in label\r\n    return clabel(input, neighbors, background, return_num, connectivity)\r\n\r\n  File \"skimage/measure/_ccomp.pyx\", line 351, in skimage.measure._ccomp.label_cython\r\n\r\n  File \"skimage/measure/_ccomp.pyx\", line 325, in skimage.measure._ccomp.reshape_array\r\n\r\n  File \"skimage/measure/_ccomp.pyx\", line 302, in skimage.measure._ccomp._apply_swaps\r\n\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes'\r\n\r\n\r\n\t [[{{node metrics_1/my_iou_metric/EagerPyFunc}}]] [Op:__inference_keras_scratch_graph_47213]\r\n", "@shashanka300 Could you fill the issues template [here](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). \r\nWere you able to install TF2.0 without any issues? What were the commands used to install TF?\r\nOther than keras, were you able to run any TF code? Please provide more details to find root-cause of the issue. Thanks!", "yes i was able to get a glean installation for GPU with cuda 10.1  and cundd 7.5 after trying a few time , standard commands provided on website,haven't explored much yet, the issue occurs whenever i try to use meaniou accuracy metric which has been changed in the current version,even after changing it to adapt to the current version using the source code.", "@shashanka300 \r\n1. Could you provide a code to reproduce the bug? \r\n2. Was there any issue when you ran any tf.keras tutorials on the TF website?\r\n\r\nPlease provide as many details (along with the code) as possible to find root-cause of the issue. Thanks!", "1.I can reveal the exact on but https://www.kaggle.com/aglotero/another-iou-metric this code and https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277 , if ported to tensorflow 2.0 gives the same error when using mea_iou as accuracy metric.\r\n2. no", "I get same error in using tensorflow with eager execution\r\nFor example\r\n```python\r\ny_hat = np.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\r\ny = np.array([0,2], dtype='int32')\r\n\r\ndef cross_entropy(y_hat, y):\r\n    y = y.tolist()\r\n    y = np.array([[i ,v] for i,v in enumerate(y)])\r\n    return -np.log(tf.gather_nd(y_hat, y))\r\n```\r\n`cross_entropy(y_hat, y)`\r\nOUT:\r\n`array([2.30258509, 0.69314718])`\r\nIf I run cross_entropy(y_hat, y), I will get correct result, but if I use\r\n```python\r\ngrad = tfe.gradients_function(cross_entropy)\r\ngrad_result = grad(y_hat, y_train)\r\n```\r\nAn error occurred\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-169-0e92aeb945ae> in <module>()\r\n----> 1 grad_result = grad(y_hat, y_train)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)\r\n    359     \"\"\"Computes the gradient of the decorated function.\"\"\"\r\n    360 \r\n--> 361     _, grad = val_and_grad_function(f, params=params)(*args, **kwds)\r\n    362     return grad\r\n    363 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)\r\n    459       raise ValueError(\"Functions to be differentiated cannot \"\r\n    460                        \"receive keyword arguments.\")\r\n--> 461     val, vjp = make_vjp(f, params)(*args, **kwds)\r\n    462     return val, vjp(dy=dy)\r\n    463 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)\r\n    515         sources.append(args[i])\r\n    516         tape.watch(this_tape, args[i])\r\n--> 517       result = f(*args)\r\n    518       if result is None:\r\n    519         raise ValueError(\"Cannot differentiate a function that returns None; \"\r\n\r\n<ipython-input-122-b40eccea66e9> in cross_entropy(y_hat, y)\r\n      1 def cross_entropy(y_hat, y):\r\n----> 2     y = y.tolist()\r\n      3     y = np.array([[i ,v] for i,v in enumerate(y)])\r\n      4     return -np.log(tf.gather_nd(y_hat, y))\r\n\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'tolist'\r\n```\r\n\r\nI think it may be that tensorflow with eager execution has made a mistake.", "Is there any update on this? I am also having the same issue. ", "just check the functions if they are available in 2.0 ", "Hi,\r\njust ran into same issue. Looks like operator overloading for eager tensors is the issue. Here is a simple code to reproduce error.\r\nthis works -\r\n```\r\ndef l(x):\r\n    x=tf.cast(x,tf.float32)\r\n    w=tf.Variable(tf.random.normal((64,8),name=\"embed_w\"))\r\n    b=tf.Variable(tf.random.normal((8,),name=\"embed_w\"))    \r\n    return tf.matmul(x,w)+b,(w,b)\r\n\r\nx=np.random.randn(100,64)\r\ny=np.random.randint(high=8,low=0,size=100)\r\n\r\nopt=tf.keras.optimizers.Adam()\r\n\r\nfor i in range(5):\r\n    with tf.GradientTape() as tape:\r\n        logits,v = l(x)\r\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=np.array(y),logits=logits)\r\n    \r\n    if i%2==0:\r\n        probs=tf.nn.softmax(logits)\r\n        accuracy=tf.reduce_sum(tf.cast(tf.equal(tf.cast(tf.math.argmax(probs,axis=-1),tf.int32),tf.constant(y)),tf.int8))/len(probs)\r\n        print(\"accuracy at step: \",i,\" is: \",accuracy)\r\n    gradients = tape.gradient(loss, v)\r\n    opt.apply_gradients(zip(gradients, v))\r\n\r\n```\r\nBelow code does not work, gives same error.\r\n```\r\ndef l(x):\r\n    x=tf.cast(x,tf.float32)\r\n    w=tf.Variable(tf.random.normal((64,8),name=\"embed_w\"))\r\n    w=w+w  ------------------------------------------------------------------->>>>>>>> error\r\n    b=tf.Variable(tf.random.normal((8,),name=\"embed_w\"))    \r\n    return tf.matmul(x,w)+b,(w,b)\r\n\r\nx=np.random.randn(100,64)\r\ny=np.random.randint(high=8,low=0,size=100)\r\n\r\nopt=tf.keras.optimizers.Adam()\r\n\r\nfor i in range(5):\r\n    with tf.GradientTape() as tape:\r\n        logits,v = l(x)\r\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=np.array(y),logits=logits)\r\n    \r\n    if i%2==0:\r\n        probs=tf.nn.softmax(logits)\r\n        accuracy=tf.reduce_sum(tf.cast(tf.equal(tf.cast(tf.math.argmax(probs,axis=-1),tf.int32),tf.constant(y)),tf.int8))/len(probs)\r\n        print(\"accuracy at step: \",i,\" is: \",accuracy)\r\n    gradients = tape.gradient(loss, v)\r\n    opt.apply_gradients(zip(gradients, v))\r\n\r\n```\r\ntensorflow 2.0\r\nCUDA 10\r\nCuDNN 7.6\r\nERROR:\r\n\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-52-1b521dcfdf23> in <module>\r\n     23         print(\"accuracy at step: \",i,\" is: \",accuracy)\r\n     24     gradients = tape.gradient(loss, v)\r\n---> 25     opt.apply_gradients(zip(gradients, v))\r\n\r\nc:\\anaconda3\\envs\\librosa\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n    399     self._create_hypers()\r\n    400     with ops.init_scope():\r\n--> 401       self._create_slots(var_list)\r\n    402 \r\n    403     self._prepare(var_list)\r\n\r\nc:\\anaconda3\\envs\\librosa\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py in _create_slots(self, var_list)\r\n    147     # Separate for-loops to respect the ordering of slot variables from v1.\r\n    148     for var in var_list:\r\n--> 149       self.add_slot(var, 'm')\r\n    150     for var in var_list:\r\n    151       self.add_slot(var, 'v')\r\n\r\nc:\\anaconda3\\envs\\librosa\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py in add_slot(self, var, slot_name, initializer)\r\n    515     if slot_name not in self._slot_names:\r\n    516       self._slot_names.append(slot_name)\r\n--> 517     var_key = _var_key(var)\r\n    518     slot_dict = self._slots.setdefault(var_key, {})\r\n    519     weight = slot_dict.get(slot_name, None)\r\n\r\nc:\\anaconda3\\envs\\librosa\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py in _var_key(var)\r\n    948   if getattr(var, \"_distributed_container\", None) is not None:\r\n    949     var = var._distributed_container()\r\n--> 950   if var._in_graph_mode:\r\n    951     return var._shared_name\r\n    952   return var._unique_id\r\n\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_in_graph_mode'\r\n\r\n```", "I'm closing this issue as it looks like something like 3 or 4 completely unrelated issues. I'll address them separately.\r\n\r\n@harrylyx your cross_entropy function uses numpy methods like ndarray.tolist; those are not supported on tf tensors which is why you're seeing this error. Use tf methods and then eager mode and gradients will work.\r\n\r\n@hegman12 your code is creating a new tf.Variable every time you call l(x) which I'm pretty sure is not what you intended.\r\n\r\nPlease reopen specific issues derived from this one with clear instructions to reproduce.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26922\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26922\">No</a>\n", "I'm also having an issue with using pyfunction in dataset...\r\n\r\ntensorflow 2.0b\r\nCUDA 10.0\r\n\r\nmy code : \r\n`\r\n    def read_npy_file(item):\r\n        data = np.load(item.decode())\r\n        return data.astype('uint8')\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices(false_list[:3])\r\n\r\n    dataset = dataset.map(\r\n        lambda item: tuple(tf.py_function(read_npy_file, [item], [tf.uint8, ])))\r\n\r\n    for file in dataset.take(3): ------------------------------> error\r\n        print(file)\r\n        pass\r\n`", "Please file a separate issue and follow the issue template.\n\nOn Tue, Jun 18, 2019 at 11:09 AM Alon Samuel <notifications@github.com>\nwrote:\n\n> I'm also having an issue with using pyfunction in dataset...\n>\n> tensorflow 2.0b\n> CUDA 10.0\n>\n> my code :\n> `\n> def read_npy_file(item):\n> data = np.load(item.decode())\n> return data.astype('uint8')\n>\n> dataset = tf.data.Dataset.from_tensor_slices(false_list[:3])\n>\n> dataset = dataset.map(\n>     lambda item: tuple(tf.py_function(read_npy_file, [item], [tf.uint8, ])))\n>\n> for file in dataset.take(3): ------------------------------> error\n>     print(file)\n>     pass\n>\n> `\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26922?email_source=notifications&email_token=AAABHRJGATVLC6N24LKX5ZDP3EQHBA5CNFSM4G72RZI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODX7O6XY#issuecomment-503246687>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRITOXN5O2UZAXDPN3DP3EQHBANCNFSM4G72RZIQ>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 26921, "title": "Tests for PrefetchDatasetOp", "body": "This PR adds the tests for `PrefetchDatasetOp` and updates the error message for the invalid buffer_size. \r\n\r\ncc: @jsimsa  ", "comments": []}, {"number": 26920, "title": "Removed the dynamic allocations for optmization.", "body": "This is one of the TODO in the file.", "comments": ["Thanks for the review, i am closing this PR as similar is already closed.\r\n\r\nRegards\r\nAmit"]}, {"number": 26919, "title": "Update ir_array.cc", "body": "Compilation warning fixed", "comments": ["I'm just curious here, how will this fix the compilation warning? @PariksheetPinjari909 ", "@sanjoy `int` is creating unnecessary warnings at the time of compilation. So thought of removing it."]}, {"number": 26918, "title": "r2.0-alpha compile on Ubuntu 18.04 has not installed tensorboard nor tf_upgrade_v2 properly", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): AWS Ubuntu 18.04 server\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source from Github\r\ngit clone https://github.com/tensorflow/tensorflow.git tfgit\r\nCloning into 'tfgit'...\r\nremote: Enumerating objects: 2136, done.\r\nremote: Counting objects: 100% (2136/2136), done.\r\nremote: Compressing objects: 100% (985/985), done.\r\nremote: Total 539589 (delta 1286), reused 1621 (delta 1145), pack-reused 537453\r\nReceiving objects: 100% (539589/539589), 319.72 MiB | 39.79 MiB/s, done.\r\nResolving deltas: 100% (433795/433795), done.\r\nChecking out files: 100% (16348/16348), done.\r\n\r\n- TensorFlow version: r2.0-alpha\r\ngit checkout r2.0\r\nBranch 'r2.0' set up to track remote branch 'r2.0' from 'origin'.\r\nSwitched to a new branch 'r2.0'\r\n- Python version: 3.6.7\r\n$ python3\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\n[GCC 8.2.0] on linux\r\n\r\n- Installed using virtualenv? pip? condo?:\r\nInstalled at /usr/lib/python3/dist-packages/tensorflow\r\n\r\n- Bazel version (if compiling from source):\r\n# wget https://github.com/bazelbuild/bazel/releases/download/0.23.0/bazel-0.23.0-installer-linux-x86_64.sh\r\n# chmod 700 bazel-0.23.0-installer-linux-x86_64.sh\r\n# ./bazel-0.23.0-installer-linux-x86_64.sh\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n$ gcc --version\r\ngcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n\r\n\r\n- CUDA/cuDNN version: Not supported\r\n- GPU model and memory: Not supported\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\ntensor board is not properly installed.\r\n$ tensorboard --logdir=./statsGraph\r\ntensorboard: command not found\r\n\r\nAlso tf_upgrade_v2 script is not installed properly:\r\n\r\n$ tf_upgrade_v2.py\r\ntf_upgrade_v2.py: command not found\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI can get tensorboard to run like this:\r\n\r\n$ python3 -m tensorboard.main --logdir=./statsGraphs/\r\nTensorBoard 1.14.0a20190301 at http://***.***.***.***:**** (Press CTRL+C to quit)\r\n\r\n\r\n**Any other info / logs**\r\n[uploadTF2BUILDlogs.txt](https://github.com/tensorflow/tensorflow/files/2986598/uploadTF2BUILDlogs.txt)\r\n\r\nI cannot get tf_upgrade_v2.py script to work:\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/tools/compatibility# python3 -m tf_upgrade_v2.py --infile /srv/projects/t1/LSTMBlockCell4HLDropoutAdam.py --outfile /srv/projects/t1/LSTMBlockCell4HLDropoutAdam_v2.py\r\n/usr/bin/python3: Error while finding module specification for 'tf_upgrade_v2.py' (AttributeError: module 'tf_upgrade_v2' has no attribute '__path__')\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["I have been able to get tf_upgrade_v2.py to run. And it does process the file as expected.\r\n\r\n$ python3 -m tensorflow.tools.compatibility.tf_upgrade_v2_main --infile ../../t1LSTMBlockCell4HLDropoutAdam.py --outfile ../../t1LSTMBlockCell4HLDropoutAdam_v2.py\r\n\r\nMy understanding is I should not need to run it this way. Presume it is not properly installed.", "Sorry for delay replying.\r\nWhich command do you run to build TensorFlow? Do you run build_pip_package?\r\n\r\nI just tried building TF package from source, installing it in a clean env and both tf_upgrade_v2 and tensorboard work as expected.\r\n\r\nFor reference, these are the steps I followed:\r\n```bash\r\npython3 -m venv venv\r\nsource venv/bin/activate\r\npip install keras_applications==1.0.4\r\npip install keras_preprocessing==1.0.2\r\npip install h5py==2.8.0\r\npip install grpcio portpicker scipy tf-estimator-nightly wheel\r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow/\r\ngit checkout r2.0\r\n\r\nexport TF_NEED_CUDA=0\r\nyes \"\" | ./configure\r\n\r\nbazel build --config=opt --config=v2 //tensorflow/tools/pip_package:build_pip_package --incompatible_disallow_dict_plus=false\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```\r\nThat last command would output a wheel file to `/tmp/tensorflow_pkg`. Note that I pass in `--incompatible_disallow_dict_plus=false` due to bazel version I use (otherwise I see an error that asks to pass that flag). You might or might not need that flag.\r\nNow, we can install the package we just built in a different virtual environment.\r\n```bash\r\ndeactivate\r\ncd ..\r\npython3 -m venv tf_package_venv\r\nsource tf_package_venv/bin/activate\r\npip install /tmp/tensorflow_pkg/tensorflow-2.0.0a0-cp36-cp36m-linux_x86_64.whl\r\n```\r\n", "nm I see that you do run `build_pip_package` in the log.\r\nThere are a couple of things I noticed:\r\n1. To get 2.0 version of the APIs, pass `--config=v2` when building `build_pip_package`\r\n2. Upgrade script command is `tf_upgrade_v2` instead of `tf_upgrade_v2.py`", "Rebuilt r2.0 alpha from source. Used --config=v2. This gives me the v2 API. Thanks. Tensor board is continuing to fail to start up.\r\n\r\nTensor board version 1.14.0a20190301 continues to fail to startup:\r\n\r\n$ tensorboard --logdir='statsGraphs'\r\ntensorboard: command not found\r\n\r\n$ python3 -m tensorboard.main --logdir='statsGraphs'\r\nTensorBoard 1.14.0a20190301 at http://ip-25-18-48-51:6006 (Press CTRL+C to quit)\r\n\r\n$ tf_upgrade_v2\r\ntf_upgrade_v2: command not found\r\n\r\nAttaching the build log for your information.\r\n[tfalpha_r2.0.buildlog.txt](https://github.com/tensorflow/tensorflow/files/3123044/tfalpha_r2.0.buildlog.txt)\r\n", "This is strange. Do you see `tensorboard` and `tf_upgrade_v2` scripts if you just install `tensorflow==2.0.0a0` instead of building from source?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26918\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26918\">No</a>\n"]}, {"number": 26917, "title": "Reorganised the code and shifted common code.", "body": "This is one of the TODO item.", "comments": ["@karimnosseir , thanks for the review, i have updated the code as per the suggestion, kindly check and approve.\r\n\r\nRegards\r\nAmit", "Thanks for the review,Closing this PR as a similar is already closed. \r\n\r\nRegards\r\nAmit"]}, {"number": 26916, "title": "Bugfix: Memory leak due to TF_LITE_ENSURE fixed", "body": "There is a memory leak due to the return path in TF_LITE_ENSURE at line 223\r\n```\r\n    TF_LITE_ENSURE(context, beam_search.TopPaths(top_paths, &best_paths_b,\r\n                                                 &log_probs, merge_repeated));\r\n```", "comments": ["Hi Samuel, the original way of dealing with temporary tensor isn't great, can you allocate a \"scratch\" tensor like here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/transpose_conv.cc#L191-L197", "@siju-samuel did you get a chance to look at @renjie-liu comments ?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26915, "title": "Fixed warnings in the files", "body": "Removed trival warnings from the files", "comments": []}, {"number": 26914, "title": "NNAPI don't support dilateconv,but tflite support.How can I add this funtion in it?is there any way to add coustom op for nnapi", "body": "", "comments": ["Dilated conv support in NNAPI should be coming in the next version of Android."]}, {"number": 26913, "title": "TensorFlow Lite Android Kotlin sample", "body": "I complete a TensorFlow Lite Android Kotlin sample, please review and want to be merged.", "comments": ["Hi @xiaobailong24, thanks for the PR!\r\n\r\nWe haven't yet decided whether we want to host Kotlin samples. While we're quite supportive of the use of Kotlin, the Java APIs for TFLite can be freely used from Kotlin. In the future, we might add a new sample which uses Kotlin, but I don't think we want a fork of an existing sample.\r\n\r\nAlso note, we're in the process of migrating all of our examples to a new repo: https://github.com/tensorflow/examples"]}, {"number": 26912, "title": "Can't load integrated model(two models cascade) with multiple inputs (Invalid input_shape argument error)", "body": "Hi~\r\n\r\nI have made two models which have input with dimension (?, 1771) and output with dimension(?,161) each.\r\n\r\nAnd two models are connected in cascade and new integrated model have 11 inputs with the same size.\r\n\r\nWhen the training is finish and can't load the saved model with below message.\r\n\r\n**ValueError: Invalid input_shape argument (None, 1771): model has 0 tensor inputs.**\r\n----------------------------------------------------------------------------------------------\r\nMy code is below(for simplicity, only a few important parts)\r\n----------------------------------------------------------------------------------------------\r\n```\r\nimport keras\r\nfrom keras.layers import Activation, Input, Dense, BatchNormalization\r\nfrom keras.callbacks import ModelCheckpoint\r\nimport scipy.io as sio\r\nimport numpy as np\r\nfrom keras.models import load_model,Model\r\n\r\nfirst_model = load_model('first_model .hdf5')\r\nfirst_model.name='first_model'\r\n\r\nfor ii in range(11):\r\n    exec(\"input_1_\"+str(ii)+\"=Input(shape=(1771,))\")\r\n    exec('output_1_'+str(ii)+'=first_model(input_1_'+str(ii)+')')\r\n\r\nconcatenated = keras.layers.concatenate([output_1_0, output_1_1, output_1_2, output_1_3, output_1_4, output_1_5,\r\n                                         output_1_6, output_1_7, output_1_8, output_1_9, output_1_10],name='concat')\r\n\r\nsecond_model = load_model('speech(noisy_to_s).hdf5')\r\nsecond_model.name='second_model'\r\n\r\nx=second_model(concatenated) \r\n\r\nmodel = Model(inputs=[input_1_0, input_1_1, input_1_2, input_1_3, input_1_4, input_1_5, \r\n                       input_1_6, input_1_7, input_1_8, input_1_9, input_1_10],outputs=[x])\r\n\r\nbatch_size = 1024\r\nAdam = keras.optimizers.Adam(lr=0.001)\r\nmodel.compile(loss='mean_squared_error', optimizer=Adam, metrics=['accuracy'])\r\ncheckpointer = ModelCheckpoint(filepath='save.hdf5',\r\n                               monitor='val_loss', verbose=1, save_best_only=True)\r\nhistory=model.fit([x_train[0:-10,:],x_train[1:-9,:],x_train[2:-8,:],x_train[3:-7,:],x_train[4:-6,:],x_train[5:-5,:],\r\n                   x_train[6:-4,:],x_train[7:-3,:],x_train[8:-2,:],x_train[9:-1,:],x_train[10:,:]],y_train[10:,:],\r\n                  batch_size=batch_size, epochs=200,verbose=0,\r\n                  validation_data=([x_valid[0:-10,:],x_valid[1:-9,:],x_valid[2:-8,:],x_valid[3:-7,:],x_valid[4:-6,:],\r\n                                    x_valid[5:-5,:],x_valid[6:-4,:],x_valid[7:-3,:],x_valid[8:-2,:],x_valid[9:-1,:],\r\n                                    x_valid[10:,:]],y_valid[10:,:]),\r\n                  callbacks=[checkpointer])\r\n\r\nmodel = load_model(checkpointer.filepath)\r\n```", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "That's clearly an issue, why close?", "\r\nIf anyone else (like me) is facing this issue. Stop renaming your layer names, see the link below:\r\nhttps://www.gitmemory.com/MoyanZitto"]}, {"number": 26911, "title": "[INTEL MKL] Add MKL-DNN quantized Matmul op with some fusions - Part3.", "body": "This PR is to replace an older PR #26271 by splitting into three. Current PR is part3. It adds unit tests for MKL-DNN quantized Matmul ops. Tests depends on part1 (#26909) and part2 (#26910).", "comments": ["@penpornk Thank you very much for the comments. Please check the latest commit.", "@mdfaijul Some of the failed tests are from the missing `mkl_qmatmul_op` target. (My bad, I thought the build is also guarded by `is_mkl`. I'll look into that after all three parts are merged.) We still have some time. I'll wait until part 2 is merged and rerun the tests then. "]}, {"number": 26910, "title": "[INTEL MKL] Add MKL-DNN quantized Matmul op with some fusions - Part2.", "body": "This PR is to replace an older PR #26271 by splitting into three. Current PR is part2. It enables MKL-DNN quantized Matmul ops through graph optimization.", "comments": ["@penpornk Yes, it depends on #26909 (Part 1).", "There are four failed tests. `MacOS Python2 and CC`, `Ubuntu Makefile`, `Ubuntu Python 2` seem unrelated to this PR. I can't view the log for `Windows Bazel` but it's likely to be unrelated too. \r\n\r\n@rthadur Could you please help pull this PR in? Thank you very much! :)"]}, {"number": 26909, "title": "[INTEL MKL] Add MKL-DNN quantized Matmul op with some fusions - Part1.", "body": "This PR is to replace an older PR #26271 by splitting into three. Current PR is part1.\r\nIncluded new ops are:\r\n- QuantizedMatmul + BiasAdd\r\n- QuantizedMatmul + BiasAdd + Relu\r\n- QuantizedMatmul + BiasAdd + Relu + Requantize", "comments": ["@penpornk Any update on this PR?", "@mdfaijul Sorry for the delay! I have to review some other PRs before I get to this one. Is this urgent?", "@penpornk Thanks for your kind reply. We wanted to get it before TF 1.14 release.", "@mdfaijul Thanks for your patience! I'll make sure we get all three parts of this PR in in time. :)", "@mdfaijul please resolve conflicts", "@penpornk @rthadur Thanks for your comments. I have addressed the comments. Could you please take a look.", "@penpornk Sorry for a delayed response addressing the review comments. Could you please check the current updates.", "@penpornk @rthadur Any update on submitted review-response.", "@penpornk @rthadur Any update on submitted review-response.", "@mdfaijul Apologies for the delay! I'll review this by tomorrow.", "@penpornk Sorry, I missed most of the comments before. Now I have addressed all. Could you please check it. I appreciate your comments.", "@penpornk Any update on the latest commit that addressed the review comments?", "@penpornk I have addressed more comments. Please have a look.", "@penpornk Thank you very much for the comments. Please check the latest commit.", "@penpornk Thank you very much for the comments. Please check the latest commit.", "Please re-generate the API golden files ([Ubuntu Python 2 failure log](https://source.cloud.google.com/results/invocations/8cd7af1a-1155-41d7-92d6-6636e1dc1ac0/targets/%2F%2Ftensorflow%2Ftools%2Fapi%2Ftests:api_compatibility_test/log)). \r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\nPlease also check if your PR is up-to-date ([Linux GPU failure log](https://source.cloud.google.com/results/invocations/b865ca69-f989-45e1-8a82-d22df840476d/log)).", "@penpornk Regenerated the API golden files. Also rebased to latest master. Thanks!", "@penpornk Thanks! Reverted the changes to unrelated golden API files. Please check.", "@penpornk Should I just checkout `tensorflow/tools/api/golden/v1/tensorflow.train.-looper-thread.pbtxt` from master?", "@mdfaijul Yes, please. :)", "@penpornk Checked out tensorflow.train.-looper-thread.pbtxt from master. Thanks!", "The failed `Ubuntu Python 2` tests don't seem related to this PR. Please fix `Ubuntu Sanity` [buildifier failure](https://source.cloud.google.com/results/invocations/5bbd94ca-4ca0-4a66-85ab-349fbc3ec14d/log).\r\n```\r\ntensorflow/core/kernels/BUILD # reformat\r\nbuildifier took 1 s\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n7247d7246\r\n<\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```\r\nLooks like there is an extra space or empty line. Please try running buildifier on `tensorflow/core/kernels/BUILD`.", "@penpornk Fixed format issue in `tensorflow/core/kernels/BUILD`."]}, {"number": 26908, "title": "[INTEL MKL] Point MKL wheel links to PyPi project", "body": "Looks like we need another changes for the MKL links so that we point users to PyPi.", "comments": ["Hi @drpngx my team wants this update as well.\r\nThanks.", "Thanks @drpngx "]}, {"number": 26907, "title": "Specs", "body": "Tensorflow: version 1.7.0 (CPU based installation)\r\nPython: 3.6\r\nOS: Windows 8.1\r\nOS Name\tMicrosoft Windows 8.1\r\nVersion\t6.3.9600 Build 9600\r\nOther OS Description \tNot Available\r\nSystem Model\tXPS 8700\r\nProcessor\tIntel(R) Core(TM) i7-4790 CPU @ 3.60GHz, 3601 Mhz, 4 Core(s), 8 Logical \r\nInstalled Physical Memory (RAM)\t24.0 GB\r\nGPU: Nvidia GTX 1050Ti\r\nIDE: Spyder 3.3.1\r\n\r\n\r\nAslo this operation\r\n\r\nj=1\r\n\r\naa=self.sess.run(tf.map_fn(lambda x: (tf.tensordot(x[0],x[1],0)),elems=tf.convert_to_tensor(self.vt[:,j,:]), tf.convert_to_tensor(self.ht[:,j,:])),dtype=tf.float64))\r\n\r\nis twice as slow compared  to:\r\n\r\n[np.outer(self.vt[i,j,:],self.ht[i,j,:])*expF[i] for i in self.lRepls]\r\n\r\n\r\nself.lRepls=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\r\nnp.shape(self.vt)=(16,10,784)\r\nnp.shape(self.ht)=(16,10,340)\r\n", "comments": ["@phquanta Could you try latest version of TF and check whether the bug persists? There were lots of improvements between TF1.7.0 to TF1.13.1 or TF2.0. Some of the ops run much faster than previous version. Please let me know what you think? Thanks!", "I think it was resolved. I am closing the issue. Please open a new ticket if you see similar issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26907\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26907\">No</a>\n"]}, {"number": 26906, "title": "Multiple CheckpointSavers when using MonitoredTrainingSession", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 2.7.14\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI'm currently using MonitoredTrainingSession and passing it the CheckpointSaverHook. It works fine for new models. When I want to restore a model, I give a path to the `checkpoint_dir` arg. However, this also inits the default CheckpointSaver, and it will double save checkpoints in the directory. This is evident if I change `checkpoint_basename` as I get duplicate checkpoints at each global step.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would not expect the default CheckpointSaver to init, and instead use the params given by the CheckpointSaverHook. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n...\r\n    initializer_hook = _DatasetInitializerHook(init_train, self.train_tfrecords_glob,\r\n                                               queue_name=self.tensorq_name)\r\n    summary_hook = tf.train.SummarySaverHook(save_steps=50, save_secs=None, summary_op=summary_op)\r\n    logging_hook =  tf.train.LoggingTensorHook(tensors={'step': global_step, 'loss': loss,\r\n                                                        'accuracy': acc_op}, \r\n                                                         every_n_iter=logging_step)\r\n    saver = tf.train.Saver(max_to_keep=2)\r\n    saver_hook = tf.train.CheckpointSaverHook(checkpoint_dir=self.ckpt_dir, \r\n                                              saver=saver, save_steps=save_step,\r\n                                              checkpoint_basename='latest_model')\r\n    step_hook = tf.train.StepCounterHook(every_n_steps=logging_step)\r\n\r\n    hooks = [summary_hook, logging_hook]\r\n\r\n    # Checkpoint Saver - Check for new model\r\n    if self.new_model:\r\n      logging.info('Starting New Model...existing ckpts will be deleted')\r\n      hooks.extend([saver_hook, step_hook])\r\n      fl = [f for f in os.listdir(self.ckpt_dir)]\r\n      for f in fl:\r\n        os.remove(os.path.join(self.ckpt_dir, f))\r\n      checkpoint_dir =  None\r\n\r\n    else:\r\n      checkpoint_dir = self.ckpt_dir\r\n      hooks.append(saver_hook)\r\n    \r\n    # Start Training\r\n    with tf.train.MonitoredTrainingSession(\r\n        checkpoint_dir=checkpoint_dir,\r\n        hooks=[initializer_hook],\r\n        chief_only_hooks=hooks,\r\n        log_step_count_steps=logging_step,\r\n        config=tf.ConfigProto(allow_soft_placement=True)) as mon_sess:\r\n        while not mon_sess.should_stop():\r\n          mon_sess.run(training_op)\r\n\r\n    logging.info(\"Training Completed\")\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Passed to karmel for triage. \r\n\r\nAs far as I remember, Estimator has dedup code but likely not in MonitoredTrainingSession", "Hi @bdod6 ! \r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26906\">No</a>\n"]}, {"number": 26905, "title": "Add tests for SkipDatasetOp", "body": "This PR adds the tests for `SkipDatasetOp` and fixes `Cardinality()` when `count < 0` (#26903).\r\n\r\ncc: @jsimsa ", "comments": []}]