[{"number": 7813, "title": "many functions not found in libtensorflow-core.a", "body": "I am trying to compile Image Recognition with makefile (which is compiled with bazel originally). However, it raise error that:\r\n\r\nimage_ops.h: No such file..\r\n\r\nFollowing the instructions in [#3017](https://github.com/tensorflow/tensorflow/issues/3017), I add **tensorflow/bazel-genfiles** to my header search path.\r\n\r\nIt does work, but another error comes out as fllows:\r\n\r\n11 warnings generated.\r\nUndefined symbols for architecture x86_64:\r\n  \"tensorflow::ops::DecodeJpeg::DecodeJpeg(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodeJpeg::Attrs const&)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::ops::ExpandDims::ExpandDims(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::ops::ResizeBilinear::ResizeBilinear(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::ops::Div::Div(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::ops::Sub::Sub(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::ops::Cast::Cast(tensorflow::Scope const&, tensorflow::Input, tensorflow::DataType)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::ops::TopKV2::TopKV2(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input)\", referenced from:\r\n      GetTopLabels(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*) in main-b19045.o\r\n  \"tensorflow::ops::ReadFile::ReadFile(tensorflow::Scope const&, tensorflow::Input)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::ops::DecodeGif::DecodeGif(tensorflow::Scope const&, tensorflow::Input)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::ops::DecodePng::DecodePng(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodePng::Attrs const&)\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n  \"tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)\", referenced from:\r\n      tensorflow::Input::Input(std::initializer_list<tensorflow::Input::Initializer> const&) in main-b19045.o\r\n  \"tensorflow::Scope::NewRootScope()\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n      GetTopLabels(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*) in main-b19045.o\r\n  \"tensorflow::Operation::Operation(tensorflow::Node*)\", referenced from:\r\n      tensorflow::Output::Output() in main-b19045.o\r\n      tensorflow::Input::Input(std::initializer_list<tensorflow::Input::Initializer> const&) in main-b19045.o\r\n      tensorflow::Input::Input(tensorflow::Tensor const&) in main-b19045.o\r\n      tensorflow::Input::Input(tensorflow::Input::Initializer const&) in main-b19045.o\r\n  \"tensorflow::Scope::ToGraphDef(tensorflow::GraphDef*) const\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n      GetTopLabels(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*) in main-b19045.o\r\n  \"tensorflow::Scope::WithOpName(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const\", referenced from:\r\n      ReadTensorFromImageFile(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, int, float, float, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) in main-b19045.o\r\n      GetTopLabels(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*) in main-b19045.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n\r\nI think these ops are not included in libtensor-core.a. \r\nMy question is: how to link these ops using makefile? ", "comments": ["The makefile is aimed at mobile development, and doesn't include a lot of functionality that the label_image example depends on (e.g. jpeg decoding, generated C++ operators). Closing this as working as intended.", "So what should we do to implent this on iOS ?"]}, {"number": 7812, "title": "Build issues", "body": "Building from source shows a lot of warnings,mostly due to either:\r\n\r\n- Comparison between signed and unsigned integers\r\n- Initialized variables\r\n- Code reaching \"non-return\" path for functions that return a non-void.\r\n\r\nI have submitted a patch that fixes a bunch of these (https://github.com/tensorflow/tensorflow/pull/7752)\r\n\r\n`Operating System`:\r\nLinux (ubuntu 14.04) \r\n\r\n`Commit hash` : 4ac9c09d5ca57a03b8daa5fb9e295947b1619854\r\n\r\n`gcc version`: 4.8.4\r\n\r\n`bazel version`:\r\n\r\nbazel version\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n", "comments": ["Thanks for the patch! I'll look at that, and close this issue."]}, {"number": 7811, "title": "tensorflow1.0   ArgumentError Running CIFAR-10 models", "body": "I am trying to run the CIFAR-10 example of Tensorflow-r1.0. However when executing python cifar10_multi_gpu_train.py I am getting the error attached below.\r\n~/models/tutorials/image/cifar10# python cifar10_multi_gpu_train.py \r\nTraceback (most recent call last):\r\n  File \"cifar10_multi_gpu_train.py\", line 271, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"cifar10_multi_gpu_train.py\", line 263, in main\r\n    cifar10.maybe_download_and_extract()\r\n  File \"/home/molys/workspace/tensorflow-r1.0/models/tutorials/image/cifar10/cifar10.py\", line 398, in maybe_download_and_extract\r\n    tarfile.open(filepath, 'r:gz').extractall(dest_directory)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 2081, in extractall\r\n    self.extract(tarinfo, path)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 2118, in extract\r\n    self._extract_member(tarinfo, os.path.join(path, tarinfo.name))\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 2194, in _extract_member\r\n    self.makefile(tarinfo, targetpath)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 2235, in makefile\r\n    copyfileobj(source, target)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 266, in copyfileobj\r\n    shutil.copyfileobj(src, dst)\r\n  File \"/usr/lib/python2.7/shutil.py\", line 49, in copyfileobj\r\n    buf = fsrc.read(length)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 831, in read\r\n    buf += self.fileobj.read(size - len(buf))\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 743, in read\r\n    return self.readnormal(size)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 758, in readnormal\r\n    return self.__read(size)\r\n  File \"/usr/lib/python2.7/tarfile.py\", line 748, in __read\r\n    buf = self.fileobj.read(size)\r\n  File \"/usr/lib/python2.7/gzip.py\", line 268, in read\r\n    self._read(readsize)\r\n  File \"/usr/lib/python2.7/gzip.py\", line 315, in _read\r\n    self._read_eof()\r\n  File \"/usr/lib/python2.7/gzip.py\", line 354, in _read_eof\r\n    hex(self.crc)))\r\nIOError: CRC check failed 0x3c81b31b != 0x6d214c80L\r\n\r\nTank you !\r\n", "comments": ["@molyswu \r\n\r\nHere's a similar issue:\r\nhttps://github.com/tensorflow/models/issues/756\r\n\r\nIn general, the ```IOError: CRC check failed``` indicates the downloaded data file is corrupt.  Try deleting the file and trying again.", "bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nERROR: /home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n\tFile \"/home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/BUILD\", line 4\r\n\t\terror_gpu_disabled()\r\n\tFile \"/home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n\t\tfail(\"ERROR: Building with --config=c...\")\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/molys/.cache/bazel/_bazel_root/f7299e5dc785b6a7f5b5291a103be54f/external/local_config_cuda/crosstool/BUILD.\r\nINFO: Elapsed time: 0.672s\r\n", "@molyswu I'm confused.\r\n\r\nYour original issue was running `cifar10_multi_gpu_train.py`, which looked like a corrupt downloaded data file.\r\n\r\nYour latest update is about building a new pip package (which suggests re-running ./configure to build with GPU support).\r\n\r\nAre these two separate issues?", "root@ubuntu:~/workspace/tensorflow-r1.0/models/tutorials/image/cifar10# python cifar10_multi_gpu_train.py \r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n", "python cifar10_multi_gpu_train.py  ,cifar10 no run GPU?", "./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7\r\nPlease specify optimization flags to use during compilation [Default is -march=native]: -march=native\r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] y\r\njemalloc enabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python2.7/site-packages]\r\n/usr/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda]\r\nInvalid path to CUDA 8.0 toolkit. /usr/local/cuda]/lib64/libcudart.so.8.0 cannot be found\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0]: /usr/local/cuda-8.0\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 7.0\r\n............................\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n..........................\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@jemalloc' (requested by nodes 'REPOSITORY:@jemalloc')\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:429)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: Invalid EvalException:\r\njava.lang.InterruptedException\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\r\n\tat java.util.concurrent.Semaphore.acquire(Semaphore.java:312)\r\n\tat com.google.devtools.build.lib.bazel.repository.downloader.HttpDownloader.download(HttpDownloader.java:196)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.downloadAndExtract(SkylarkRepositoryContext.java:594)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:316)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)\r\n\tat com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)\r\n\tat com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n\tat com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:344)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)\r\n\tat com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)\r\n\tat com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)\r\n\t... 4 more\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@jemalloc' (requested by nodes 'REPOSITORY:@jemalloc')\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:429)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: Invalid EvalException:\r\njava.lang.InterruptedException\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\r\n\tat java.util.concurrent.Semaphore.acquire(Semaphore.java:312)\r\n\tat com.google.devtools.build.lib.bazel.repository.downloader.HttpDownloader.download(HttpDownloader.java:196)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.downloadAndExtract(SkylarkRepositoryContext.java:594)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:316)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)\r\n\tat com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)\r\n\tat com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n\tat com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:344)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)\r\n\tat com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)\r\n\tat com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)\r\n\t... 4 more\r\n", "I'm sorry @molyswu , I don't understand if there's a question in here; you've just dumped various logs of commands.\r\n\r\nI'm closing this out.  If you have a bug or feature request, please file a new issue, and try to describe what the issue is."]}, {"number": 7810, "title": "How to disable peer to peer gpu memory accessing?", "body": "Our system with 2GPUs always reboot if run training with 2GPUs, but 1 GPU is OK. And the cuda testing case of 'simpleP2P' also make the system reboot.\r\n\r\nHow could I disable the peer to peer feature?", "comments": ["So you want to disable DMA because it's buggy even though NVidia driver reports that DMA is available? Not sure there's this level of fine-grained configuration at TF level, cc @poxvoculi just in case", "Do you want to disable all direct memory transfers between the two GPUs, or just make it so that only one GPU is visible to TF?  The second can be done with the CUDA_VISIBLE_DEVICES env variable.\r\n\r\nI believe that TF should never dereference a foreign GPU pointer and thus trigger automatic DMA.  If you're using 2 GPUs they should appear as separate devices and all memory transfers will be done via DeviceToDeviceCopy which ultimately turns into a cudaMemcpy call which uses DMA to do a transfer over the PCI bus.   If TF can only see and use one GPU, you'll never trigger a DMA operation involving the other GPU.  If you want to use both GPUs, but stage all transfers via CPU RAM and never direct DMA between the GPUs, that's harder: I don't think there's a setting to accomplish that.  One technique might be always to explicitly copy to a CPU resident tensor before forwarding a value to the other GPU.", "@poxvoculi We have another system&1 2 GPUs are connected by SOC legend, so the HW is not support peer to peer memory accessing (I mean GPUDirect), this system has no issue, and I believe it should do DeviceToDeviceCopy also. But our current system&2, the 2 GPUs are connected by PXI which is supported GPPUDirect, and it will cause the system reboot. CUDA has a testing program 'simpleP2P' can test the peer to peer memory accessing. For system &1, 'simpleP2P' will not run since the GPU connection doesn't support it, but for system&2, 'simpleP2P' will make the system reboot. It must be HW or driver issue. To workaround it, I want to disable the GPUDirect feature. For Tensorflow, I find the following code: (./tensorflow/stream_executor/cuda/cuda_gpu_executor.cc)\r\nbool CUDAExecutor::CanEnablePeerAccessTo(StreamExecutorInterface *other) {\r\n  CUDAExecutor *cuda_other = static_cast<CUDAExecutor *>(other);\r\n  return CUDADriver::CanEnablePeerAccess(context_, cuda_other->context_);\r\n}\r\nI'd like to hack the code to make 'CUDAExecutor::CanEnablePeerAccessTo' always return 'false'.", "@AlvinChen13 did you ever manage to resolve this? I'm having the same issue.", "Hi AlvinChen13, \r\n\r\nI have the same requirement to disable TF's P2P feature and you provide me with a feasible method. Could you please tell me the result about changing the code you claimed? "]}, {"number": 7809, "title": "Getting the following the error while importing", "body": "python bin/run_analysis.py\r\nTraceback (most recent call last):\r\n  File \"bin/run_analysis.py\", line 7, in <module>\r\n    from src.run_analysis import analyze, render_results_as_images\r\n  File \"/DeepOSM/src/run_analysis.py\", line 6, in <module>\r\n    import label_chunks_cnn_cifar\r\n  File \"/DeepOSM/src/label_chunks_cnn_cifar.py\", line 11, in <module>\r\n    import tflearn\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/__init__.py\", line 21, in <module>\r\n    from .layers import normalization\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/layers/__init__.py\", line 10, in <module>\r\n    from .recurrent import lstm, gru, simple_rnn, bidirectional_rnn, \\\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/layers/recurrent.py\", line 8, in <module>\r\n    from tensorflow.contrib.rnn.python.ops.core_rnn import static_rnn as _rnn, \\\r\nImportError: No module named core_rnn\r\n\r\nwhile running the github project [https://github.com/zilongzhong/DeepOSM](url)\r\nIn this one I am able to create training data successfully but not able to run_analysis.py and while runnin I'm getting this error and ", "comments": ["This appears to be an issue with DeepOSM. Please file a bug on their issue tracker.", "Already raised an issue but I think this one is with tensorflow. can you help me in this ?", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks! My feeling is that DeepOSM has not updated for TensorFlow 1.0 yet and you are using TensorFlow 1.0, but that is speculation, since you didn't mention which versions.", "Apple Macbook Pro\r\nProcessor: 2.2 GHz Intel Core i7\r\nMemory: 16 GB 1600 MHz DDR3\r\nUsing with Docker\r\nDocker Version: 1.12.6 (14937)\r\nTensorflow Version: 0.8.0\r\n\r\nalso tried with tensorflow version 1.0.0 got the same error and tried the middle versions also\r\nlike tensorflow 0.10, 0.11, 0.12 .", "I had the same issue, but with a clean install of python 3.5, Tensorflow 0.12 and TFlearn 0.3 in a conda environment. After checking to confirm that this module was missing in my site-packages folder, I checked the Github repository for Tensorflow and discovered that 0.12 did not have the core_rnn module (possibly due to a rollback), but 1.0 did. The fix for me was to update to Tensorflow 1.0. \r\n\r\nI've done some limited tests to verify that TFLearn is working with Tensorflow 1.0, and so far so good. ", "Is there a way to use core_rnn without upgrading to 1.0?", "@ebrevdo, I believe the answer is no, can you confirm?", "That's right; you must update to TF 1.0 since that is what TFLearn seems to\nbe targeting.\n\nOn Thu, Mar 9, 2017 at 9:45 AM, Andrew Selle <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo>, I believe the answer is no, can\n> you confirm?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7809#issuecomment-285425321>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_Kw529jwQHS2sid-bI5dHTGbcNYks5rkDqlgaJpZM4MJjVL>\n> .\n>\n", "Please try the upgrade suggestions and reopen if there is still an issue."]}, {"number": 7808, "title": "TensorFlow builds LLVM even if ./configured without XLA", "body": "At HEAD (4ac9c09d5ca57a03b8daa5fb9e295947b1619854), if I configure TensorFlow without XLA support and run\r\n\r\n    blaze build //tensorflow/...\r\n\r\nit still builds both XLA and LLVM (or at least parts, since I haven't waited for it to finish).  Needless to say, it also downloads LLVM, which ideally would not be necessary without XLA support.\r\n\r\nIs it possible to skip building a compiler if I don't want XLA?", "comments": ["@jart Do you know whether this is WAI?", "@hawkinsp setup `tensorflow/compiler` folder and the related `BUILD` files .\r\nPeter, is this expected?", "This is expected but I would agree that it isn't desirable.\r\n\r\nA workaround for now is to build //tensorflow -//tensorflow/compiler/...\r\n\r\nTo fix this, the best idea I have is to add xla variants of all the BUILD rules in //tensorflow/compiler/...\r\ne.g., replace cc_library() rules with xla_cc_library() rules, where xla_cc_library() expands to cc_library() if XLA is enabled, or to nothing, otherwise. This is tedious but should work.\r\n\r\nA lighter-weight variant of the proposal above might be to only wrap the XLA CPU and GPU backends, which are the pieces that depend on LLVM. You would still end up building the common parts of XLA, but those parts of XLA are small and quick to build compared to the big LLVM dependency. I'll try this out and see how feasible it is.", "I will try to make this happen with some build file trickery.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@girving What we have been doing in our CI for this is instead of just building all of TF, we build\r\n```\r\nbazel build //tensorflow/... -//tensorflow/compiler -//tensorflow/contrib/...\r\n```\r\nAs parts of contrib may also have different requirements, like android dependencies for lite, GPU dependency for nccl, etc.", "We also have plans to start enabling XLA by default in the future. So any solution here would be temporary.", "The way I typically solve situations like this is by running `bazel query 'somepath(//tensorflow/..., @llvm//...)'` refactoring and repeating until empty.", "One problem with that is, bazel query does not let us query for different paths on the select statements.\r\nWe do have a lot of correct LLVM dependencies that are disabled using switch statements, so bazel query will keep returning these.\r\n\r\nI guess we can temporarily comment them out and continue with the task, and at the very end reenable these dependencies.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm going to mark this contributions welcome, since it would be nice if we could make the build go faster, but `bazel query` doesn't make this as easy to troubleshoot as we would like. If anyone can figure out the secret, please send us a change.", "Hi @girving , \r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/7808\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/7808\">No</a>\n"]}, {"number": 7807, "title": "How to extract the native library", "body": "I am new to the tensorflow for java. I want to ask how to extract the native library? Is there some guidance?", "comments": ["This question is probably better asked on [stackoverflow](https://www.stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests.\r\n\r\nI didn't quite follow what you meant by \"extract the native library\". The [Java README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md) provides instructions for downloading the native binary archive and extracting it with `tar` as well as instructions for building the native library from source."]}, {"number": 7806, "title": "Change pip.sh to take Bazel flags directly so AVX packages are built properly", "body": "Cherry-pick for r1.0 branch", "comments": ["Abandoning since it's not needed anymore. Built off of my own github branch since I needed another change there."]}, {"number": 7805, "title": "Is it a error?", "body": "### Environment info\r\nOperating System: CentOS 7 \r\ntensorflow version: 0.12.1\r\n\r\n### Installed version of CUDA and cuDNN: \r\n\r\n/usr/local/cuda-8.0/lib64/libcudadevrt.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.3\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\n### Question:\r\nWhen I run a program. It print the below info. (Then the machine shutdown.)\r\n![_20170223100205](https://cloud.githubusercontent.com/assets/4702353/23242344/321555be-f9b3-11e6-8115-63fc129f3922.png)\r\n\r\nI remember when it run successfully on two GPU , it print:\r\n```\r\nY N\r\nN Y\r\n```\r\nnot\r\n```\r\nY Y\r\nY Y\r\n```", "comments": ["This information comes from cuda driver which tells which pairs of GPUs can use DMA, there are probably some NVidia cuda tools that let you see this matrix", "Closing since this doesn't seem to be a tensorflow error"]}, {"number": 7804, "title": "how copy a tensor to another tensor??", "body": "how copy a tensor to another tensor??", "comments": ["This should be a StackOverflow question."]}, {"number": 7803, "title": "Add pre-compilation of Pascal binaries (CC 6.0/6.1) to CUDA-enabled D\u2026", "body": "\u2026ocker build (#6941)\r\n\r\n(cherry picked from commit 83a5427ce8cf8bbfa665022fdd6aa125d8a8883b)", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Failure doesn't look relevant to the change. Triggering again @tensorflow-jenkins test this please."]}, {"number": 7802, "title": "fixes erroneous collection in contrib batch_norm", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 7801, "title": "Switch to an official release of buildifier.", "body": "The current git master of buildifier has a broken build.", "comments": []}, {"number": 7800, "title": "Feature Request: access gradients for a specific layer", "body": "Currently tensorflow will provide the gradients for the entire network, i.e. getting grads_and_vars will return all of the gradients but there isn't any nice way to specifically get the gradients of a specific layer.\r\n\r\nAn example use case:  For the paper Neural Networks are easily fooled, they took the gradients from the last layer and added them to the original image.\r\n\r\nThere is no current way to do this in tensorflow as far as I know.\r\n", "comments": ["You can get gradients for any layer, `tf.gradients(loss, layer2)` will give you gradients at layer2 . (ps this q is better suited for stackoverflow)", "Sorry I didn't know it was already implemented.  I'll read the documentation more thoroughly next time."]}, {"number": 7799, "title": "Assert Bazel version from tf_workspace()", "body": "This ensures that projects depending on TensorFlow, e.g. TFServing, will\r\nbenefit from the version check.\r\n\r\nThe mandatory version has also been bumped up to Bazel 0.4.4 which has been out\r\nfor 21 days. It includes numerous very important fixes. TensorFlow also appears\r\nto not work without it.\r\n\r\nCC: @martinwicke ", "comments": ["Leaving the test failures aside, the details of the change look reasonable.  However, @martinwicke's a better person to answer whether we want to force people to upgrade.  It may be that Bazel 0.4.4 is only required by folks using TensorFlow as a submodule or via `http_archive`.\r\n\r\nOne cludgy option if forcing everyone to upgrade is bad: we could require 0.4.4 for anyone calling `tf_workspace` *except* for `tensorflow/WORKSPACE` itself, via an extra boolean flag to `tf_workspace`.", "We've traditionally been tracking bazel close to head, so I have no problem requiring fresh bazel versions. Historically, the threshold has been \"Is it in homebrew yet?\", which has usually happened a few days after release. \r\n\r\nSo I like this change, but it does mean that we need to upgrade our test machines to have the new bazel (i.e. change the bazel version in ci_build/install/install_bazel.sh, and then wait while we update all the other machines that don't use docker before we merge this.", "0.4.4 is in Homebrew.", "0.4.4 breaks TF build, so we cannot use it. (There is a post mortem for the release).\r\nWe have to wait for 0.4.5 to bump our bazel dependency.", "What happened?\n\nOn Thu, Feb 23, 2017 at 1:07 AM, gunan <notifications@github.com> wrote:\n\n> 0.4.4 breaks TF build, so we cannot use it. (There is a post mortem for\n> the release).\n> We have to wait for 0.4.5 to bump our bazel dependency.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7799#issuecomment-281935397>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AADAbhgY-kjhFPp9zJI0qPml1afx1zhWks5rfUxggaJpZM4MJSz->\n> .\n>\n", "If Bazel 0.4.4 is broken, this is problematic for deepmath including tensorflow as a submodule.  Using it as a submodule does not work with 0.4.3.  Is there a workaround that will make 0.4.4 work?  Otherwise I'll end up blocked for a long time.", "0.4.5 RC came out yesterday, so it may not be a long time.\n", "Jenkins, test this please.", "Jenkins, test this please.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Updated and testing."]}, {"number": 7798, "title": "Fix nccl when referenced from a different repo", "body": "This fixes the following error:\r\n\r\nERROR: external/nccl_archive/BUILD.bazel:33:1: no such package 'tensorflow': Package crosses into repository `@org_tensorflow` and referenced by `'@nccl_archive//:nccl'`.\r\n\r\nAlso mirror URL for great justice.\r\n\r\nCC @cwhipkey", "comments": ["Thanks for fixing this!"]}, {"number": 7797, "title": "Change pip.sh to take Bazel flags directly so AVX packages are built properly.", "body": "Also fix EXTRA_ARGS typo.\r\n\r\nHaven't tested yet, will try on Jenkins.", "comments": ["Also tested locally"]}, {"number": 7796, "title": "Problems in Defining New Op ", "body": "I'm trying to define a new Op (a simple matmul) by myself. I defined the both CPU and GPU Opkernel and ran REGISTER_KERNEL_BUILDER with DEVICE_CPU and DEVICE_GPU respectively to register them. When I was testing it on my server installed with GPU, and used\r\n```\r\nwith tf.Graph().as_default():\r\n    with tf.device('/gpu:0'):\r\n    # some code for graph\r\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\r\n    sess.run(tf.initialize_all_variables())\r\n    print sess.run(out)\r\n```\r\nto define variable and Ops, I found only the code of CPU Opkernel is used.\r\n\r\nWeird thing is that if I delete the CPU Opkernel code and CPU Opkernel registration, keep only GPU Opkernel and register only the GPU Opkernel, GPU Opkernel code will be used now and everything works and with the following output:\r\n\r\n```\r\nE tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Not found: No registered 'MyMatMul' OpKernel for CPU devices compatible with node MyMatMul = MyMatMul[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](Const, Const_1)\r\n        .  Registered:  device='GPU'\r\n\r\n         [[Node: MyMatMul = MyMatMul[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](Const, Const_1)]]\r\n```\r\n\r\nAnyone has idea for it?", "comments": ["How are you compiling your new op and kernel? Are you using the custom op loading mechanism or are you compiling it as part TensorFlow source code?", "Closing because of inactivity."]}, {"number": 7795, "title": "Typo in \"Validate Your Installation\" document.", "body": "https://www.tensorflow.org/install/install_linux#ValidateYourInstallation\r\n\r\nPartway down:\r\n\r\n> If you are new to TensorFlow, see [Getting Started with TensorFlow]Getting Started With TensorFlow.\r\n\r\nNote the extraneous square brackets; looks like a markdown typo. Interestingly, the \"Getting Started With\" link still works (which did not survive my copy-paste into the issue tracker).", "comments": ["I've fixed this internally but it will take a bit to propagate out to the site.  Thanks for reporting.", "This doesn't look to have propagated yet; did you mean to close this before the fix landed?", "Hey, it has propagated out now, although now we have a separate bug in the page (an unclosed bold). "]}, {"number": 7794, "title": "Cannot specify XLA config for learn.estimators ?", "body": "Seems that the MonitoredSession is being allocated directly:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f821ce046df71f5784ed4ce7fb6b87f77d96b031/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L962\r\n\r\nand there is no way to pass or specify different `graph_options` to the session to enable the XLA jit configuration. Is that the case or I am missing something? ", "comments": ["Looks like a missing functionality. @ispirmustafa -- is there any plan to allow configuring Session options for session used in estimator?\r\n\r\nAs a work-around, you could monkey-patch `Session.__init__` method to add XLA config, something like this\r\n\r\n```\r\noldinit = tf.Session.__init__\r\ndef myinit(session_object, target='', graph=None, config=None):\r\n    print(\"Intercepted!\")\r\n    optimizer_options = tf.OptimizerOptions()\r\n    optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n    config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=optimizer_options))\r\n    return oldinit(session_object, target, graph, config)\r\ntf.Session.__init__ = myinit\r\n```", "@ispirmustafa This might be a good requirement/feature to add while working on Estimators.  As if the list is not long enough.  :-)  Assigning to you as the best person to answer to follow.", "I came across what appears to be the same need wanting to set `log_device_placement` as another use case.", "Having the same issues. Also it is not able to pass options to the session.run inside the estimator, Given the new estimators in tf.estimators which are much cleaner, I was thinking of adding the functionality to provide both tf.ConfigProto to the session initialization and run_metadata and run_options via the RunConfig class expected by the estimator. Any ideas or suggestions before I write the code? ", "Some is assigned to work on this internally.  While we cannot promise an ETA, weeks is the estimate.  I know this may seem hacky but I think you could wrap the model code and operations in a big \"with\" and get the results you want in the mean time.  My python is not super strong and while I played with the JIT I never tried to wrap all of the operations this way.  I do not see why it would not work.  I might be over simplifying but trying this should not take too much effort to try.\r\n\r\n```python\r\n jit_scope = tf.contrib.compiler.jit.experimental_jit_scope\r\n\r\n    x = tf.placeholder(np.float32)\r\n    with jit_scope():\r\n      y = tf.add(x, x)  # The \"add\" will be compiled with XLA.\r\n```\r\n", "I think this was already fixed. session_config could be set in RunConfig now  [1] and used by Estimator.\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/076799bdfae0057723d96f47cf78cb623c8bcd57/tensorflow/contrib/learn/python/learn/estimators/run_config.py#L233\r\n[2] \r\nhttps://github.com/tensorflow/tensorflow/blob/076799bdfae0057723d96f47cf78cb623c8bcd57/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L380"]}, {"number": 7793, "title": "pip3 No matching distribution found on OSX 10.9.5 py3.5", "body": "I am trying to install tensorflow in a virtualenv environment on OSX. \r\n\r\nI think I have a good Python version (3.5). This is what it says:\r\n```\r\n$ python\r\nPython 3.5.2 (default, Oct  1 2016, 01:07:00) \r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\r\n```\r\n\r\nthat happens when I give the pip install command:\r\n\r\n```\r\n$ pip3 install -vvv --upgrade tensorflow\r\nCollecting tensorflow\r\n  1 location(s) to search for versions of tensorflow:\r\n  * https://pypi.python.org/simple/tensorflow/\r\n  Getting page https://pypi.python.org/simple/tensorflow/\r\n  Looking up \"https://pypi.python.org/simple/tensorflow/\" in the cache\r\n  Current age based on date: 471\r\n  Freshness lifetime from max-age: 600\r\n  Freshness lifetime from request max-age: 600\r\n  The response is \"fresh\", returning cached response\r\n  600 > 471\r\n  Analyzing links from page https://pypi.python.org/simple/tensorflow/\r\n    Skipping link https://pypi.python.org/packages/00/16/c8ba385fc6511ca362f32326cd1d6a99bbbabbc8341607ff70c290e0be7b/tensorflow-0.12.1-cp34-cp34m-manylinux1_x86_64.whl#md5=981c0a406eb9865423b11c03b489040d (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/03/51/b68d9d6481d16fd709290030ae8a8a13212587bc87fae718e521bdafa723/tensorflow-0.12.0rc1-cp27-cp27mu-manylinux1_x86_64.whl#md5=9329984aa4f6388f5ec2a170e4ae968e (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/0c/7c/29ac00dd86a419a6ecba16d2c319598adb92b261e7984666305f411bbd7d/tensorflow-1.0.0-cp35-cp35m-macosx_10_11_x86_64.whl#md5=51b1aba26e837825a4f1eafd2eb81fe7 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/0d/d7/b49a6ceb055f392f91bce25eb6e1665f9b2f0a4628f7acdbccf1cd1d0ee6/tensorflow-1.0.0-cp27-cp27m-macosx_10_11_x86_64.whl#md5=51235fbe34e705bcdb1b5bbd6a482c57 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/11/c9/2ec86336a8a401a57fabed5b10ee7f0493fc334b33e9672766b1149dd923/tensorflow-0.12.0-cp35-cp35m-win_amd64.whl#md5=d515d2ae08ae25fc8bf81e3374448146 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/16/6e/fa1f09a32d3ba5bf6a2b79c6ec16a43c91e3a79e0035ab3eb52ecad22e0d/tensorflow-0.12.1-cp35-cp35m-macosx_10_11_x86_64.whl#md5=faf5d055cdaecd75e5f0b37a04676102 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/25/23/24681ce011cd33ea5b22b8efd0f28a3f1294085c6ca6f70cc3abd62bf75d/tensorflow-0.12.0rc1-cp35-cp35m-macosx_10_11_x86_64.whl#md5=6c964e9575ed88892b94195a05aa8e65 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/25/c4/162ea5fa9e012f5a5e125105f6b290e56f9fd617a7aadd57d9a26bb386ca/tensorflow-0.12.0rc1-cp34-cp34m-manylinux1_x86_64.whl#md5=4e77b31ae9fc0e36489875ce0f72b267 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/33/4b/20e517870effa573405d30dafc22f330f24c0a8928659b4ad5a44b9a9af2/tensorflow-0.12.0rc0-cp35-cp35m-macosx_10_11_x86_64.whl#md5=8d1376a68f768efa57b1344fc2df0472 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/33/ab/3a62133d6c9f6281959f3ca96ad2a796fb4fed8d642c4f33d9fd97d8bf6f/tensorflow-0.12.0rc1-cp27-cp27m-macosx_10_11_x86_64.whl#md5=491802c10e992905d7e80108b9b16920 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/34/53/0e10581ad88bca25e5005874e46130b12efbd5eb1bda493dbf9b1648cbe2/tensorflow-0.12.1-cp27-cp27m-macosx_10_11_x86_64.whl#md5=d3f44a0596a623458c42c2a694abb0af (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/3f/8a/e80e2fef57bb1ffcff65c597adcddd5a0de17d7a7da5a6187503a28e9d66/tensorflow-1.0.0-cp33-cp33m-manylinux1_x86_64.whl#md5=ed46aa42e9716649572e0f861210b8f5 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/48/58/b71480f9ec9d08d581d672a81b15ab5fec36a5fcda2093558a23614d8468/tensorflow-1.0.0-cp36-cp36m-manylinux1_x86_64.whl#md5=2742cfc22068004af8a1f31071d96546 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/4a/c3/1ad85e5c4fde90b2e9a5101283d97dd41ba6c24f44a1c3a8495ef7098bb5/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl#md5=dad56a38acf7ca501b1bfeeef53f3710 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/64/9c/72aff7713c507f7e6c15df011e0ed18ac85e5bfa16c3763d8cba44585d79/tensorflow-0.12.0rc0-cp34-cp34m-manylinux1_x86_64.whl#md5=93431c5cd8a019d08a72e6c0a75eaaf4 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/64/a3/0054a3329579de44d557f491adbcaf8127809a7992bc46af80f0a589e29b/tensorflow-0.12.1-cp35-cp35m-win_amd64.whl#md5=d657836c76a5cd3c6d5560034bd7ade6 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/65/a3/40bbfb783d955c1a959c8fa939f63b00d4fac9d0c3777e0607d39e31ba0a/tensorflow-1.0.0-cp36-cp36m-macosx_10_11_x86_64.whl#md5=5926a0360b2979cb969e8c38a18f9c0a (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/66/47/d6bb91a11684733ad565b891d561097ef10f8cb7bae87e5ad692207024e3/tensorflow-0.12.0rc1-cp35-cp35m-manylinux1_x86_64.whl#md5=2351fe799a0869487699537ab0e33019 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/67/06/15153c48b2281bc59f8a70f2ae681723ece29ebc0015883117fb28abaf68/tensorflow-0.12.0rc0-cp35-cp35m-manylinux1_x86_64.whl#md5=ebc1ee880633a60e5256e745a27d4058 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/7b/c5/a97ed48fcc878e36bb05a3ea700c077360853c0994473a8f6b0ab4c2ddd2/tensorflow-1.0.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=a7483a4da4d70cc628e9e207238f77c0 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/7e/c6/837f4e249aae5c86a632eaaa9779e601eca1487772f8ad75c347bf9e813f/tensorflow-0.12.1-cp27-cp27mu-manylinux1_x86_64.whl#md5=c98fd26b79a97cc490c942bbafed5462 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/7e/ea/e42e47ddb39d2043a06dc93b5aec042bae8571be52ea664aa03246a83c32/tensorflow-0.12.0-cp35-cp35m-macosx_10_11_x86_64.whl#md5=f8b165ae638eb169220ccbf02658475b (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/90/cf/1d1e12f9f39b6a0ed1c49792ef5ce7615dddc2ce7287fc83ede0dddb9b3c/tensorflow-0.12.0rc0-cp27-cp27m-macosx_10_11_x86_64.whl#md5=7be7b7488a6be83546758a40d5442901 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/91/7c/98b25b74241194d4312a7e230c85a77b254224191dbf17b484811f8a9f61/tensorflow-0.12.1-cp36-cp36m-macosx_10_11_x86_64.whl#md5=3b2f5f08fe61471ea973a4bdbdfbae43 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/96/d6/096f6624a8a48d41801eb23863d35a37532baf32f6d3aa2e03c96f66e6ab/tensorflow-0.12.0-cp34-cp34m-manylinux1_x86_64.whl#md5=a6a0af9cf2a5fc33053f638b05cb3940 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/9e/7c/f50ca985e43cff46a6d92abb088e8328b9be13335dc44af291ac1ed11862/tensorflow-0.12.0-cp35-cp35m-manylinux1_x86_64.whl#md5=2c07109df1a9865a7a44b6fe08756e8d (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/a7/86/cb47c1213779939583091fc97e0950c323d655186824cdbc0f657f42930c/tensorflow-0.12.0-cp27-cp27m-macosx_10_11_x86_64.whl#md5=fc429a5b5b74517e08216ee8b1d04e59 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/b0/a6/a8b857c8b383c4a2a3dece8a3c0f86e9273b8fd7ce31528f8ac809fc5910/tensorflow-1.0.0-cp35-cp35m-manylinux1_x86_64.whl#md5=88f8b298a2efb3f2d236019c5dcc1220 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/b2/da/9132af1e2ef3771b63619072a6d82800bd60acbf2c8bea8e4f26514c768a/tensorflow-0.12.0rc1-cp35-cp35m-win_amd64.whl#md5=69b6c2e9440fd2d7a2d86249933fcde0 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/c0/9c/f094a4a79a5d45612394d97c27d03837a8f012f5f7ad085f2705b4ca0510/tensorflow-1.0.0-cp34-cp34m-manylinux1_x86_64.whl#md5=1e137fc0cb113d544758ba4a3334b3d6 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/ce/2c/6a1cf90746879c2d05df04efc86a8b1edd79d7b06323a5c8fa63f5520824/tensorflow-1.0.0-cp35-cp35m-win_amd64.whl#md5=7e73a31ef4eb5914991cb2ad401fd21d (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/e5/5c/56e6522fdcd6f8739dcbc4de62e8b0040e141785bb42e5b53a83b0ba3e58/tensorflow-0.12.1-cp35-cp35m-manylinux1_x86_64.whl#md5=c6e3ba8579754f37d37be26e863f9d95 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/e7/73/85ff235957f2652f78a2cd7f0a045d3f983302991cb7af2fddedf27d56fe/tensorflow-0.12.1-cp33-cp33m-manylinux1_x86_64.whl#md5=d77e48ef3b45e543ca165db93f27e0ff (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/e9/a5/45b172f20e2fabd19c7f18a44570fc82acc4c628ec9bc4b313de39c4fe37/tensorflow-0.12.1-cp36-cp36m-manylinux1_x86_64.whl#md5=def6ab7dc3930f34f3000caef34f4333 (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n    Skipping link https://pypi.python.org/packages/f6/2a/e5bb6320a3fc2886f2677ffa0d4396eefb5914cfc19db94e672c650f0700/tensorflow-0.12.0rc0-cp27-cp27mu-manylinux1_x86_64.whl#md5=00a6b89122d86fd527558a2740fd6f8f (from https://pypi.python.org/simple/tensorflow/); it is not compatible with this Python\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nCleaning up...\r\nNo matching distribution found for tensorflow\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/commands/install.py\", line 335, in run\r\n    wb.build(autobuilding=True)\r\n  File \"/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/wheel.py\", line 749, in build\r\n    self.requirement_set.prepare_files(self.finder)\r\n  File \"/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/req/req_set.py\", line 380, in prepare_files\r\n    ignore_dependencies=self.ignore_dependencies))\r\n  File \"/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/req/req_set.py\", line 554, in _prepare_file\r\n    require_hashes\r\n  File \"/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/req/req_install.py\", line 278, in populate_link\r\n    self.link = finder.find_requirement(self, upgrade)\r\n  File \"/Users/myname/Envs/tensorflow/lib/python3.5/site-packages/pip/index.py\", line 514, in find_requirement\r\n    'No matching distribution found for %s' % req\r\npip.exceptions.DistributionNotFound: No matching distribution found for tensorflow\r\n```\r\n\r\nThese are my current packages in this environment:\r\n```\r\n$ pip freeze\r\nappdirs==1.4.0\r\nnose==1.3.7\r\npackaging==16.8\r\npbr==1.10.0\r\npyparsing==2.1.10\r\nQScintilla==2.9.2\r\nsip==4.18\r\nsix==1.10.0\r\nstevedore==1.20.0\r\nvirtualenv==15.1.0\r\nvirtualenv-clone==0.2.6\r\nvirtualenvwrapper==4.7.2\r\n\r\n```\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nStackOverflow has questions that deal with the period before the tensorflow wheel was on PyPi, or when the Python is not 64-bit or earlier than 3.5.\r\n\r\n### Environment info\r\nOperating System:\r\nOSX 10.9.5\r\nInstalled version of CUDA and cuDNN: I don't have CUDA I believe.\r\n\r\n", "comments": ["Ok, so it seems the problem might be that the wheels are provided for OSX 10.11 only and I have OSX 10.9.5. I don't want to reinstall my OS and break things on my work machine. So I downloaded the OSX 10.11 .whl file from Pypi and renamed it to OSX_10_9. This worked fine, and I was able to install tensorflow. \r\n\r\n```\r\n$ cp ~/Downloads/tensorflow-1.0.0-cp35-cp35m-macosx_10_11_x86_64.whl ~/Downloads/tensorflow-1.0.0-cp35-cp35m-macosx_10_9_x86_64.whl\r\n$ pip install ~/Downloads/tensorflow-1.0.0-cp35-cp35m-macosx_10_9_x86_64.whl \r\n```\r\nI was able to run the hello world validation, and it just gave me the SSE warning (copied below). I hope the library is safe to run. I would request Pypi wheels to include support for OSX 10.9 and 10.11 as well. Or do you recommend people install from source on these versions?\r\n\r\n```\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: sess = tf.Session()\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n\r\nIn [3]: hello = tf.constant('Hello, TensorFlow!')\r\n\r\nIn [4]: print(sess.run(hello))\r\nb'Hello, TensorFlow!'\r\n\r\n```\r\n", "OSX versions below 10.11 are not supported by the TF team, but you may find community support elsewhere.  The warnings you see are merely to let you know that performance will be better if you compile specifically for your machine.  "]}, {"number": 7792, "title": "update BUILD file to include client session in libtensorflow_cc.so", "body": "libtensorflow_cc target didn't have client session. \r\nTested on my end and working \r\nhttps://github.com/memo/ofxMSATensorFlow/blob/master/example-graph-build/src/example-graph-build.cpp", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 7791, "title": "glorot_uniform_initializer and others not exported", "body": "Many initializers defined in `init_ops` are not exported in the global TensorFlow package. These include:\r\n\r\n- `glorot_uniform_initializer` (this is even explicitly mentioned in the docs as the default initializer for variables)\r\n- `glorot_normal_initializer`\r\n- `variance_scaling_initializer`\r\n\r\nIt seems like this might be an oversight \u2013 `glorot_uniform_initializer` is mentioned in existing documentation, and `glorot_normal_initializer` has no other hits in the code base, which makes it seem like it's for user consumption.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nSearched \r\n\r\n### Environment info\r\nv1.0.0 docker image\r\n\r\n### What other attempted solutions have you tried?\r\nSimilar initializers are available in `tf.contrib.layers`, but it seems preferable to use the ones that already exist in core.", "comments": ["@lukaszkaiser  Can you comment on this?  I asked @martinwicke and he was not sure if this was an oversight or on purpose. ", "I believe it's an oversight, but could you confirm with fchollet@ to be sure? Thanks!", "That's an oversight, I will send out a fix. Thanks for the report.", "This issue was resolved here: https://github.com/tensorflow/tensorflow/commit/4cf4f524cb93ad0c1d6c35e9ccec62cf9015b217 So this can be closed.", "Thanks!", "Sweet!"]}, {"number": 7790, "title": "fix debian jessie ci_build", "body": "Workaround deb packaging issue in debian jessie. The packages openjdk-8-jre-headless does not automatically install ca-certificates-java because there are wrong metadata in backport repository. The workaround is to force \"-t jessie-backports\".", "comments": ["Jenkins, test this please", "760 seconds to run one test? Could be faster.\r\n\r\n@tensorflow-jenkins test this please"]}, {"number": 7789, "title": "Generalize TensorFlowTestCase to handle arbitrary devices", "body": "Currently, TensorFlowTestCase can only handle CPU and GPU devices.\r\n\r\n```python\r\nclass TensorFlowTestCase(googletest.TestCase):\r\n  def test_session(self,\r\n                   graph=None,\r\n                   config=None,\r\n                   use_gpu=False,\r\n                   force_gpu=False)\r\n```\r\n\r\nWe are currently extending TensorFlow to run on another device and would like to update the signature to handle arbitrary devices. This could be done by changing the signature as follows, for example:\r\n\r\n```python\r\nclass TensorFlowTestCase(googletest.TestCase):\r\n  def test_session(self,\r\n                   graph=None,\r\n                   config=None,\r\n                   use_device=None,\r\n                   force_device=False)\r\n```\r\n\r\nThis would certainly have some significant ripples in the test cases, so I would like like to get some feedback before I start anything.\r\n\r\nThoughts?\r\n", "comments": ["So how would you tell a test to force GPU? Set `force_device=True`? That seems a bit counter-intuitive.\r\n\r\nI think the easiest thing to do may be first fork `TensorFlowTestCase` to a new version that supports other devices, and once things stabilize, look at two versions side by side and see if it makes sense to factor out common functionality (and that the refactoring effort justifies the costs imposed by Conway's Law)", "Perhaps it would be a little more clear to have an interface such as:\r\n```python\r\nclass TensorFlowTestCase(googletest.TestCase):\r\n  def test_session(self,\r\n                   graph=None,\r\n                   config=None,\r\n                   use_device=\"/device:CPU:0\",\r\n                   force_device_placement=False)\r\n```\r\nForcing GPU use could then be done with `sess = self.test_session(use_device=\"/device:GPU:0\", force_device_placement=True)`. This would also deconvolve the selecting of a device and the forcing of its use, unlike the current implementation (i.e. `sess = self.test_session(use_gpu=False, force_gpu=True)` <- what happens?).\r\n\r\nHowever, the goal of the change is to generalize all of the tests, which may also impose significant costs. There is also a bit more gpu-specific code in the method that would have to be dealt with. I agree that forking and seeing how it plays out is a good idea. I'll report back in a couple of days with how it is going.", "The `test_session` code was not really planned in advance and grew organically. There are probably bugs. @drpngx saw some weird/unexpected behavior in `force_gpu` --https://github.com/tensorflow/tensorflow/issues/7026#issuecomment-281741007", "Yeah, `force_gpu` was ineffectual when I tried it. Looking at the code, it basically just says `allow_soft_placement=False` and `with sess.graph.device(\"/gpu:0\"): yield sess`, so it should work.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7788, "title": "Branch 148228903", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 7787, "title": "Branch 148225260", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 7786, "title": "Feature Request: Embedding Lookup Gradient", "body": "Hi,\r\n\r\nSince `embedding_lookup` is just a fast way of doing a matrix multiplication between the embedding weights and a tensor of one-hot vectors, it would be nice to have an option to get the gradient of the `embedding_lookup` as if it were such a matrix multiplication.  There are situations where this is actually useful - for example, the one-hots could be sampled from a high-dimensional multinomial distribution computed by a first neural network and passed as input to a second neural network, where using `embedding_lookup` would be much faster because of the high dimensionality.  There are some recent papers with various methods for back-propagating through the sampling procedure that I would like to implement, but I don't see any way to do it efficiently for high-dimensional distributions without a feature like this. \r\n\r\nThanks,\r\nShawn\r\n", "comments": ["@alextp: thoughts?", "embedding_lookup has a gradient now, implemented in terms of the gradients of gather and sparse_segment_sum. It returns an IndexedSlices which might get converted into a full Tensor if necessary.\r\n\r\nIs this sufficient for you?", "Closing due to lack of activity.  Please reopen if necessary."]}, {"number": 7785, "title": "Distributed Tensorflow fails : no device", "body": "I am following the example in https://www.tensorflow.org/versions/r0.10/how_tos/distributed/  to create a distributed tensorflow model with a parameter server and n workers. I do not have any GPU, all work is distributed on CPU\r\n\r\nIn the chief worker, I want to save my variables every some steps, but invoking the saver results in the following exception :\r\n\r\n    Cannot assign a device to node 'save_1/RestoreV2_21': \r\n    Could not satisfy explicit device specification \r\n    '/job:ps/task:0/device:CPU:0' because no devices matching that \r\n    specification are registered in this process; available devices: \r\n    /job:localhost/replica:0/task:0/cpu:0\r\n\r\n    [[Node: save_1/RestoreV2_21 = RestoreV2[dtypes=[DT_INT32],\r\n    _device=\"/job:ps/task:0/device:CPU:0\"](save_1/Const, \r\n    save_1/RestoreV2_21/tensor_names, save_1/RestoreV2_21/shape_and_slices)]]\r\n\r\nI tried :\r\n\r\n    server = tf.train.Server(cluster,\r\n                             job_name=self.calib.params['job_name'],\r\n                             task_index=self.calib.params['task_index'],\r\n                             config=tf.ConfigProto(allow_soft_placement=True)\r\n\r\nI am using a supervisor as in the example in the documentation:\r\n\r\n    sv = tf.train.Supervisor(\r\n                             is_chief=is_chief,\r\n                           ...)\r\n\r\nand creating my session as follows :\r\n\r\n    sess = sv.prepare_or_wait_for_session(server.target) as sess:\r\n\r\nbut I am still having the exact same error.\r\n\r\nWhen I print server.target I obtain \r\n\r\n    grpc://localhost:xxxx\r\n\r\nwhere xxxx is 2200 for my first worker, 2201 for my second worker and so on", "comments": ["Although this kind of questions should be asked on StackOverflow, I recommend you look up the latest documentation. The preferred way to do so seems to be `MoniteredTrainingSession`.", "Can you share a complete reproducible example? As I mentioned [on Stack Overflow](http://stackoverflow.com/a/42397517/3574081), the direct cause of the error message is that a `Session(\"\")` is being used to restore the checkpoint, whereas it should be a `Session(server.target)`. It appears that you're doing the right thing by passing `server.target` to `sv.prepare_or_wait_for_session()`, so this could be a bug in the `Supervisor` or `SessionManager` code.\r\n\r\nCan you also let us know which version of the code you are using, and the complete stack trace for the error? Thanks!", "Hello and thanks a lot for the help with this issue,\r\nI am using tensorflow1.0 and keras 1.2.2 to generate the model. Below is a complete code to reproduce the error. The same error occurs if instead of `model.save_weights` I use a saver. \r\nTo test, please start the following three processes \r\n\r\n    python test.py --job_name ps\r\n    python test.py --job_name worker --task_index 0\r\n    python test.py --job_name worker --task_index 1\r\n\r\nHere is the code of the file test.py in which I catch the exception generated after the save_weights instruction. Notice that the file weights.h5 is indeed produced\r\n\r\n    import os\r\n    import numpy as np\r\n    import pandas as pd\r\n    import argparse\r\n    \r\n    from keras.models import Sequential\r\n    from keras.layers.core import Dense\r\n    from keras.regularizers import l2\r\n    import tensorflow as tf\r\n    import keras\r\n    \r\n    nb_samples = 50\r\n    nb_features = 5\r\n    X_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\r\n    Y_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\r\n    \r\n    def build_keras_model(input_dim):\r\n      hidden_dim = 10\r\n    \r\n      model = Sequential()\r\n      model.add(Dense(input_dim = input_dim,\r\n                      output_dim=hidden_dim,\r\n                      activation='tanh'\r\n                      ))\r\n    \r\n      model.add(Dense(output_dim=1, activation='linear'))\r\n    \r\n      model.compile(loss='mse', optimizer='adam')\r\n      \r\n      return model\r\n    \r\n    \r\n    \r\n    \r\n    ################################################\r\n    # DISTRIBUTE\r\n    ################################################\r\n    \r\n    parser = argparse.ArgumentParser(description='tensorflow')\r\n    parser.add_argument('--job_name', dest='job_name')\r\n    parser.add_argument('--task_index', dest='task_index', default=0)\r\n    args = parser.parse_args()\r\n    \r\n    \r\n    ps_hosts = ['localhost:2222']\r\n    worker_hosts = ['localhost:2223', 'localhost:2224']\r\n    job_name = args.job_name\r\n    task_index = int(args.task_index)\r\n    \r\n    # Create a cluster from the parameter server and worker hosts.\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n      \r\n    server = tf.train.Server(cluster,\r\n                             job_name=job_name,\r\n                             task_index=task_index,\r\n                             config=tf.ConfigProto(log_device_placement=True,\r\n                                                   inter_op_parallelism_threads=1,\r\n                                                   intra_op_parallelism_threads=1))\r\n    \r\n    \r\n    if job_name =='ps':\r\n      server.join()\r\n    \r\n    else:\r\n      with tf.device(tf.train.replica_device_setter(\r\n                                  worker_device=\"/job:worker/task:%d\" % task_index,\r\n                                  cluster=cluster)):\r\n    \r\n        keras.backend.set_learning_phase(1)\r\n        keras.backend.manual_variable_initialization(True)\r\n    \r\n        model = build_keras_model(nb_features)\r\n        preds = model.output\r\n        targets = tf.placeholder(tf.float32, [None, 1])\r\n        total_loss = tf.reduce_mean(\r\n                            keras.objectives.mean_squared_error(targets, preds))\r\n    \r\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n    \r\n        is_chief=(task_index == 0)\r\n    \r\n        opt = tf.train.AdamOptimizer()\r\n    \r\n        train_op = opt.minimize(total_loss, global_step=global_step)\r\n    \r\n        init_op = tf.global_variables_initializer()\r\n    \r\n    \r\n        sv = tf.train.Supervisor(\r\n              is_chief=is_chief,\r\n              logdir=\"/tmp/train_logs\",\r\n              init_op=init_op,\r\n              global_step=global_step)\r\n        \r\n        print '######################################### ALL CREATED'\r\n        sess = sv.prepare_or_wait_for_session(server.target)\r\n        print '#######  SESSION OK ********'\r\n        local_step = 0\r\n        while True:\r\n          train_feed = {model.input: X_train, targets: Y_train}\r\n    \r\n          _, step = sess.run([train_op, global_step], feed_dict=train_feed)\r\n          loss = sess.run(total_loss, feed_dict = train_feed)\r\n          if is_chief:\r\n            try:\r\n                print '## Trying to save'\r\n                model.save_weights('weights.h5')\r\n            except Exception as e:\r\n                print e\r\n                break\r\n          local_step += 1\r\n          print 'local_step : ', local_step, 'global_step :', step, 'total_loss:', loss\r\n    \r\n    \r\n          if step >= 20:\r\n            break\r\n    \r\n        # Ask for all the services to stop.\r\n        sv.stop()\r\n", "Can you please share the complete stack trace for the error you're seeing as well?", "Here is the complete stack trace\r\n\r\n    Traceback (most recent call last):\r\n          File \"tensorflow_data_parallel.py\", line 113, in <module>\r\n            model.save_weights('weights.h5')\r\n          File \"..../keras-1.2.2-1/lib/python2.7/site-packages/keras/engine/topology.py\", line 2652, in save_weights\r\n            self.save_weights_to_hdf5_group(f)\r\n          File \"..../keras-1.2.2-1/lib/python2.7/site-packages/keras/engine/topology.py\", line 2668, in save_weights_to_hdf5_group\r\n            weight_values = K.batch_get_value(symbolic_weights)\r\n          File \"..../keras-1.2.2-1/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1834, in batch_get_value\r\n            return get_session().run(xs)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 767, in run\r\n            run_metadata_ptr)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 965, in _run\r\n            feed_dict_string, options, run_metadata)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n            target_list, options, run_metadata)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n            raise type(e)(node_def, op, message)\r\n        tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'save/RestoreV2_19': Could not satisfy explicit device specification '/job:ps/task:0/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\r\n                 [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_INT32], _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\r\n\r\n        Caused by op u'save/RestoreV2_19', defined at:\r\n          File \"tensorflow_data_parallel.py\", line 101, in <module>\r\n            global_step=global_step)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/training/supervisor.py\", line 313, in __init__\r\n            self._init_saver(saver=saver)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/training/supervisor.py\", line 459, in _init_saver\r\n            saver = saver_mod.Saver()\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/training/saver.py\", line 1051, in __init__\r\n            self.build()\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/training/saver.py\", line 1081, in build\r\n            restore_sequentially=self._restore_sequentially)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/training/saver.py\", line 675, in build\r\n            restore_sequentially, reshape)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\r\n            tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/training/saver.py\", line 242, in restore_op\r\n            [spec.tensor.dtype])[0])\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\r\n            dtypes=dtypes, name=name)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n            op_def=op_def)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n            original_op=self._default_original_op, op_def=op_def)\r\n          File \"..../tensorflow-1.0.0/lib/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n            self._traceback = _extract_stack()\r\n\r\n        InvalidArgumentError (see above for traceback): Cannot assign a device to node 'save/RestoreV2_19': Could not satisfy explicit device specification '/job:ps/task:0/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\r\n                 [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_INT32], _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\r\n", "OK, I think this problem might be due to Keras creating its own session, based on this line in the stack trace:\r\n\r\n```\r\n      File \"..../keras-1.2.2-1/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1834, in batch_get_value\r\n        return get_session().run(xs)\r\n```\r\n\r\nI'm not particularly familiar with Keras, but I think there's [a `K.set_session()` method](https://github.com/fchollet/keras/blob/4fa7e5d454dd4f3f33f1d756a2a8659f2e789141/keras/backend/tensorflow_backend.py#L129) to which you can pass your own `sess`, and if you do that the error should be resolved.", "This is very kind of you.. It solved it indeed", "could you provide an example to use it ?", "I am running into a similar problem. I am using VGG16 model in Keras, but when I attempt to call model.load_weights() inside the \"with tf.device(tf.train.replica_device_setter(...))\" block I get the same error because setting the weights runs Keras session. Any idea how you solved this?", "> could you provide an example to use it ?\r\n\r\n```\r\nimport keras.backend as K\r\nsess = tf.Session()\r\nK.set_session(sess)\r\n```"]}, {"number": 7784, "title": "ValueError while implementing GMM using skflow", "body": "Hi I am trying to implement GMM using tensorflow. But I am getting the following error :- ValueError: Features are incompatible with given information. Given features: Tensor(\"input:0\", shape=(?, 198), dtype=float32), required signatures: TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(None), Dimension(198)]), is_sparse=False)\r\n\r\nOperating System: Ubuntu 16.04\r\ntensorflow version 1.0.0\r\n\r\n\r\n`from tensorflow.contrib.factorization.python.ops import gmm as gmm_lib\r\ngmm = gmm_lib.GMM(num_clusters, batch_size=1)\r\ngmm.fit(x.astype('float32'),steps=300)\r\nyy = gmm.predict(x,y=None)`\r\n\r\nx is a numpy array of shape (2384, 198)\r\n\r\nPlease find the stack trace in stackoverflow [question](http://stackoverflow.com/questions/42299378/valueerror-features-are-incompatible-with-given-information-given-features-te)\r\n\r\nIf I pass a `float64` data then I get the following error:-\r\n`ValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(\"sub_1:0\", shape=(), dtype=float32)'`\r\n\r\nPlease find the stack trace in stackoverflow [question](http://stackoverflow.com/questions/42299378/valueerror-features-are-incompatible-with-given-information-given-features-te)\r\n\r\nI even tried to implement the gmm_test from [here](https://github.com/tensorflow/tensorflow/blob/32bd3d024f33e920a67a1081bc0ae0048350fdee/tensorflow/contrib/factorization/python/ops/gmm_test.py)\r\n\r\nBut I got a new error :- \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/xxxxx/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-61-c8c19df9240a>\", line 1, in <module>\r\n    gmm.fit(input_fn=input_fn(), steps=0)\r\nTypeError: fit() got an unexpected keyword argument 'input_fn'\r\n```\r\n\r\nIf it is a bug please fix it, otherwise provide an alternative\r\n\r\n\r\n\r\n", "comments": ["Let's pursue this on stackoverflow for now.", "Hi Paul, I am facing similar issue with skflow KMeansClustering. I have raised a stackoverflow question [here](http://stackoverflow.com/questions/42250340/runtimeerror-no-c-shape-function-registered-for-standard-op-nearestneighbors) and a github issue [here](https://github.com/tensorflow/tensorflow/issues/7524). There pretty similar issues. If possible please look into it.", "If I have an easily reproducible example, I can open a bug, but so far I can't reproduce.  You need to post a small but complete program that executes on TF 1.0.   Your stackoverflow posts are too vague.  I can't guess what values any underspecified program elements might have.", "Hi Paul,\r\nPlease try this code\r\n```\r\nfrom tensorflow.contrib.factorization.python.ops import gmm as gmm_lib \r\nimport random \r\nimport numpy as np \r\nx = np.array([[random.random() for i in range(198)] for j in range(2384)]) \r\ngmm = gmm_lib.GMM(200,random_seed=0) \r\ngmm.fit(x)\r\n```\r\nThe error varies changing the type of x in line `gmm.fit(x)`.\r\nfor example `x.astype('float32')` or  `x.astype('float64')`.", "OK, thanks for posting @mahatosourav91, this program reproduces the first error, which is:\r\n\r\n`ValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(\"sub_1:0\", shape=(), dtype=float32)'\r\n`\r\n\r\nMy understanding is that numpy by default will create an array of float64, and many parts of TF will require float32.  The logic path hit in this example can't force a conversion, so it's an error.  Changing the type of the argument as follows will get past that error:\r\n\r\n`x = np.array([[random.random() for i in range(198)] for j in range(2384)], dtype=np.float32) `\r\n\r\nThe next error hit is \r\n\r\n`InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x <= y did not hold element-wise: x = ] [assert_less_equal/x:0] [200] [y = ] [strided_slice_1:0] [128]`\r\n\r\nI'm still looking into that one.", "The next error appears to be caused by the num_clusters argument to GMM exceeding the (implicit) batch_size of 128.  If I change to the following:\r\n\r\n```\r\nfrom tensorflow.contrib.factorization.python.ops import gmm as gmm_lib \r\nimport random \r\nimport numpy as np \r\nx = np.array([[random.random() for i in range(198)] for j in range(2384)], dtype=np.float32) \r\ngmm = gmm_lib.GMM(128,random_seed=0) \r\ngmm.fit(x)\r\n\r\n```\r\n\r\nNow I hit a new error:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Input is not invertible.\r\n\t [[Node: MatrixInverse_2 = MatrixInverse[T=DT_FLOAT, adjoint=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add_138)]]\r\n```\r\n\r\nI wonder whether this may be caused by using random input.\r\nIn any case, this is a contrib library, and I can't see that anyone is maintaining it.\r\n@rmlarsen: What do you know about this?\r\n", "Hi @poxvoculi , @rmlarsen \r\nI tested the code in scikit-learn and it works there. So random inputs are valid inputs\r\n```\r\n    import random\r\n    import numpy as np\r\n    from sklearn.mixture import GaussianMixture as GMM\r\n\r\n    x = np.array([[random.random() for i in range(198)] for j in range(2384)])\r\n    gmm = GMM(n_components=200)\r\n    gmm.fit(x)\r\n    \r\n    Y = gmm.predict(x)\r\n```\r\n\r\nAlso I have obeserved that the gmm.py file in [master ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/python/ops/gmm.py) and [r1.0](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/factorization/python/ops/gmm.py) branch are 2 completely different files. \r\nEven though tensorflow 1.0 release date was 15 Feb 2017, gmm.py in r1.0 branch does contain code prior to 11 Jan 2017 only. Some refactoring was done on 21 Jan 2017 and 2 Feb 2017.  Maybe this is cause of the issue. \r\n\r\n\r\n", "Hi any update on this issue. \r\n@poxvoculi Is there going to any new release for tensorflow. Also please look into my last [comment](https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-282273778).", "@gunan: was there a release problem with these files?", "I do not recall any issues on these files.\r\nHowever, they are in contrib, so the checks on those files are looser than the checks we have for any file in core TF.", "Hi @poxvoculi , As per your comment [here](https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-282081517). I have looked the code of [gmm_ops.py](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/factorization/python/ops/gmm_ops.py). I found that the input provided to the `linalg_ops.matrix_inverse` is not invertible in line `inverse = linalg_ops.matrix_inverse(cov + self._min_var)`. I propose to calculate a pseudo matrix inverse in place of matrix inverse. Following code should work in its place:-\r\n```\r\ns,u,v=np.linalg.svd(cov + self._min_var)\r\ninverse = math_ops.matmul(array_ops.transpose(v), math_ops.matmul(array_ops.diag(math_ops.pow(s,-1)),array_ops.transpose(u)))\r\n```\r\nI believe this is equivalent to `numpy.linalg.pinv`\r\n\r\nIt would be much better to provide a `pseudo_matrix_inverse` in `linalg_ops`.\r\n\r\nPlease look into it.", "@xavigonzalvo: Would this be a welcome fix?", "Yes, I think it would.\n\nThanks for letting me know.\n\n2017-02-28 15:13 GMT-05:00 Paul Tucker <notifications@github.com>:\n\n> @xavigonzalvo <https://github.com/xavigonzalvo>: Would this be a welcome\n> fix?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-283148291>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEVsoZ8Yl1gRpLDhHXrLMvpeenY8aZIhks5rhH_1gaJpZM4MI6E5>\n> .\n>\n\n\n\n-- \nXavi.\n", "Hi @poxvoculi , @xavigonzalvo , As per previous comments, I have modified [gmm_ops.py](https://github.com/mahatosourav91/tensorflow/blob/pseudo_matrix_inverse/tensorflow/contrib/factorization/python/ops/gmm_ops.py) and [linalg_ops.py](https://github.com/mahatosourav91/tensorflow/blob/pseudo_matrix_inverse/tensorflow/python/ops/linalg_ops.py) [here](https://github.com/mahatosourav91/tensorflow/blob/pseudo_matrix_inverse/tensorflow/contrib/factorization/python/ops/gmm_ops.py).\r\n\r\nBut when I tried to execute the following code I am getting a `NanLossDuringTrainingError`\r\n\r\n```\r\nfrom tensorflow.contrib.factorization.python.ops import gmm as gmm_lib\r\nfrom tensorflow.python.framework import constant_op\r\nimport random\r\nimport numpy as np\r\n\r\n\r\ndef input_fn(x):\r\n    \"\"\"Returns an input_fn that randomly selects batches from given points.\"\"\"\r\n    def _fn():\r\n      return constant_op.constant(x.astype('float32')), None\r\n    return _fn\r\n\r\nx = np.array([[random.random() for i in range(10)] for j in range(2384)], dtype=np.float32)\r\ngmm = gmm_lib.GMM(200, random_seed=0)\r\ngmm.fit(input_fn=input_fn(x), max_steps=300)\r\n\r\ny = gmm.predict_assignments(input_fn=input_fn(x))\r\n```\r\n\r\nError :- \r\n\r\n```\r\nERROR:tensorflow:Model diverged with loss = NaN.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-7be5ab638a15>\", line 15, in <module>\r\n    gmm.fit(input_fn=input_fn(x), max_steps=300)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 281, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 418, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 968, in _train_model\r\n    return loss\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3623, in get_controller\r\n    yield default\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 968, in _train_model\r\n    return loss\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3049, in device\r\n    yield\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 966, in _train_model\r\n    _, loss = mon_sess.run([model_fn_ops.train_op, model_fn_ops.loss])\r\n  File \"C:\\Users\\gidnri6\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 483, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 818, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 775, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 936, in run\r\n    run_metadata=run_metadata))\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\", line 481, in after_run\r\n    raise NanLossDuringTrainingError\r\ntensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.\r\n\r\n```\r\nPlease note that this error occurs in ubuntu 16.04 also", "Hi @poxvoculi , @xavigonzalvo , Please note that if the data is increased a new error is raised\r\n\r\n```\r\nfrom tensorflow.contrib.factorization.python.ops import gmm as gmm_lib\r\nfrom tensorflow.python.framework import constant_op\r\nimport random\r\nimport numpy as np\r\n\r\n\r\ndef input_fn(x):\r\n    \"\"\"Returns an input_fn that randomly selects batches from given points.\"\"\"\r\n    def _fn():\r\n      return constant_op.constant(x.astype('float32')), None\r\n    return _fn\r\n\r\nx = np.array([[random.random() for i in range(198)] for j in range(2384)], dtype=np.float32)\r\ngmm = gmm_lib.GMM(200, random_seed=0)\r\ngmm.fit(input_fn=input_fn(x), max_steps=300)\r\n\r\ny = gmm.predict_assignments(input_fn=input_fn(x))\r\n```\r\n\r\nI have modified the line \r\n`x = np.array([[random.random() for i in range(198)] for j in range(2384)], dtype=np.float32)`\r\nfrom \r\n`x = np.array([[random.random() for i in range(10)] for j in range(2384)], dtype=np.float32)`\r\n\r\nThe new error is :- \r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n\t [[Node: Diag_186/_317 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_1909_Diag_186\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\t [[Node: Pow_146/_939 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3690_Pow_146\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\n", "This error looks related to GPU. We have not tested GMM support on GPU. Can you try to force this on cpu ?  ", "If I'm not mistaken that seems to be a problem related to overflows.\n\nYou can change line containing the computation\nof self._prior_probs[shard_id] by:\n\nself._prior_probs[shard_id] = math_ops.reduce_logsumexp(\n    self._probs[shard_id], axis=1, keep_dims=True)\n\nwhich is numerically stable.\n\nI am making some other changes to the code and that will be updated when\nreleased.\n\nThanks\n\n\n\n2017-03-09 1:51 GMT-05:00 agarwal-ashish <notifications@github.com>:\n\n> This error looks related to GPU. We have not tested GMM support on GPU.\n> Can you try to force this on cpu ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-285270008>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEVsoZuKkpr7zG5m7YPNjhNqtfHonSWFks5rj6FdgaJpZM4MI6E5>\n> .\n>\n\n\n\n-- \nXavi.\n", "commit e6126230200e2ce9c96da5c9e4dc7f104c645d11 should have fixed these issues. Please reopen if you continue to see these errors.", "@poxvoculi , I am newbie to TensorFlow so please bear with me. I have to estimate GMM parameters like weights, means. In this regards, I am using code in [link](https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-282081517). It would be very helpful if you can explain the below.\r\n1) I don't see any sess.run() to evaluate the graph. How does it get executed? If you have complete script please provide it\r\n2) I also tried to execute the code in [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/python/ops/gmm_test.py). This script is executing but again I am not able to see sess.run(). If you have idea about this kindly let me know", "The GMM part is encapsulated as part of an Estimator (see\nhttps://www.tensorflow.org/extend/estimators). Think of that as a wrapper\nwhere you define your model and then use the class method \".fit()\" to do\nthe training. That will run sess.run() for you.\n\n2017-05-02 14:50 GMT-04:00 vinay-hebb <notifications@github.com>:\n\n> @poxvoculi <https://github.com/poxvoculi> , I am newbie to TensorFlow so\n> please bear with me. I have to estimate GMM parameters like weights, means.\n> In this regards, I am using code in link\n> <https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-282081517>.\n> It would be very helpful if you can explain the below.\n>\n>    1. I don't see any sess.run() to evaluate the graph. How does it get\n>    executed? If you have complete script please provide it\n>    2. I also tried to execute the code in link\n>    <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/python/ops/gmm_test.py>.\n>    This script is executing but again I am not able to see sess.run(). If you\n>    have idea about this kindly let me know\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-298726114>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEVsoa4OV2_shkp7v7I0SBRVKSUR2P9vks5r13sCgaJpZM4MI6E5>\n> .\n>\n\n\n\n-- \nXavi.\n", "@xavigonzalvo, Thanks for your reply. Yeah, I understand about sess.run() now. I have another question, could you clarify me if you know about this.\r\n\r\nI am using [gmm.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/python/ops/gmm.py) from TF 1.0 and it can estimate means and covariances of each component in GMM. But I am not able to get weights (or prior probabilities). I don't see any API defined (similar to clusters() or covariances()) in TF 1.0. They are added in TF 1.1. But I should be able to tap into internal variables and get those variables. could you please let me know how to do this. I am using the wrapper code as in [link](https://github.com/tensorflow/tensorflow/issues/9682)", "Estimator takes an input_fn. I suggest you use this:\n\ndef get_input_fn(x):\n  def input_fn():\n    return tf.constant(x.astype(np.float32)), None\n  return input_fn\n\ngmm.fit(input_fn=get_input_fn(x_1d), steps=100)\nprint gmm.weights()\n\n\n2017-05-08 2:37 GMT-04:00 vinay-hebb <notifications@github.com>:\n\n> @xavigonzalvo <https://github.com/xavigonzalvo>, Thanks for your reply.\n> Yeah, I understand about sess.run() now. I have another question, could you\n> clarify me if you know about this.\n>\n> I am using gmm.py\n> <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/python/ops/gmm.py>\n> from TF 1.0 and it can estimate means and covariances of each component in\n> GMM. But I am not able to get weights (or prior probabilities). I don't see\n> any API defined (similar to clusters() or covariances()) in TF 1.0. They\n> are added in TF 1.1. But I should be able to tap into internal variables\n> and get those variables. could you please let me know how to do this. I am\n> using the wrapper code as in link\n> <https://github.com/tensorflow/tensorflow/issues/9682>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-299784802>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEVsoaY4nE3AgBqKp3DAG3CxMzlpuDFwks5r3rgNgaJpZM4MI6E5>\n> .\n>\n\n\n\n-- \nXavi.\n"]}]