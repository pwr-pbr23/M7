[{"number": 40148, "title": "adding_an_op compile fail Windows unistd.h", "body": "Tensorflow r2.2 compile on Windows 10.  The example adding_an_op will fail with error message file not found unistd.h.  Error occurs on line 74 of external/eigen_archive/unsupported/Eigen/CXX11/Tensor.  \r\n\r\nThere is an #ifdef EIGEN_USE_GPU conditional #include <unistd.h> which will fail on Windows, as this is a unix file.  It is actually conditionally excluded from Windows previously in the file at line 51, so it is unclear why the include is not part of that conditional.  Commenting out the line 74 will enable successful compilation on Windows without any immediately apparent side effects.  This was not tested on *nix.\r\n\r\nThank you for looking at this.", "comments": ["@sr99622 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n ", "This problem occurs on Windows\r\nThe command used to compile the example adding_an_op is\r\n\r\n`bazel build tensorflow/examples/adding_an_op/... `\r\n\r\nFile not found unistd.h\r\nError occurs on line 74 of external/eigen_archive/unsupported/Eigen/CXX11/Tensor.\r\n\r\nThe code of interest in that file is starting at line 41\r\n\r\n#ifdef _WIN32\r\ntypedef __int16 int16_t;\r\ntypedef unsigned __int16 uint16_t;\r\ntypedef __int32 int32_t;\r\ntypedef unsigned __int32 uint32_t;\r\ntypedef __int64 int64_t;\r\ntypedef unsigned __int64 uint64_t;\r\n#include <windows.h>\r\n#else\r\n#include <stdint.h>\r\n#include <unistd.h>  //  First instance of unistd.h\r\n#endif\r\n\r\n#ifdef _WIN32\r\n#include <windows.h>\r\n#elif defined(__APPLE__)\r\n#include <mach/mach_time.h>\r\n#else\r\n#include <time.h>\r\n#endif\r\n\r\n#if defined(EIGEN_USE_THREADS) || defined(EIGEN_USE_SYCL)\r\n#include \"ThreadPool\"\r\n#endif\r\n\r\n#ifdef EIGEN_USE_GPU\r\n  #include <iostream>\r\n  #if defined(EIGEN_USE_HIP)\r\n    #include <hip/hip_runtime.h>\r\n  #else\r\n    #include <cuda_runtime.h>\r\n  #endif\r\n  #include <atomic>\r\n  #include <unistd.h>  //  Second instance of unistd.h\r\n#endif\r\n\r\nThank you so much for your help with this", "@sr99622 \r\nPlease confirm if you have referred to this [link](https://www.tensorflow.org/install/source_windows) for the installation.\r\nPlease refer to this issue and let us know if it helps.\r\n#39829", "This is the same issue as #39829 and the same path has been followed.  The bug is in the code as described above.  The second instance of #include <unistd.h> is not excluded from the windows build as the first one is, and this will manifest when using the GPU section.  The problem can be temporarily fixed by commenting out the second instance of the include.\r\n\r\nI would surmise from reading the previous post that the issue has not been addressed in the official source distribution.\r\n\r\nYour assistance is greatly appreciated.", "@sr99622 It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40148\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40148\">No</a>\n"]}, {"number": 40147, "title": "tf.sparse_tensor_to_dense get the wrong result", "body": "Python 3.6.1\r\ntensorflow = 1.15.0\r\n```\r\nids_index = tf.sparse_tensor_to_dense(ids_sparse_tensor, default_value=0, validate_indices=True,name=None) \r\nwith tf.Session() as session:\r\n    session.run(tf.global_variables_initializer())\r\n    session.run(tf.tables_initializer())\r\n    print(\"ids_sparse_tensor\"+\"*\"*30)\r\n    print(ids_sparse_tensor.indices.eval().shape)\r\n    print(ids_sparse_tensor.values.eval().shape)\r\n    print(ids_sparse_tensor.eval())\r\n    print(\"ids_index\"+\"*\" * 30)\r\n    ids_index_value=ids_index.eval()\r\n    print(ids_index_value.shape) #(1024, 91)\r\n    print(ids_index_value)               \r\n...\r\n```\r\nI got result below:\r\nids_sparse_tensor******************************\r\n(93184, 2)\r\n(93184,)\r\nSparseTensorValue(indices=array([[   0,    0],\r\n       [   0,    1],\r\n       [   0,    2],\r\n       ...,\r\n       [1023,   88],\r\n       [1023,   89],\r\n       [1023,   90]], dtype=int64), values=array([7250, 1622, 2987, ...,    0,    0,    0], dtype=int64), dense_shape=array([1024,   91], dtype=int64))\r\nids_index******************************\r\n(1024, 91)\r\n[[1233 1144 7106 ...    0    0    0]\r\n [4758 6123 6134 ...    0    0    0]\r\n [2487 1965 4448 ...    0    0    0]\r\n ...\r\n [   0    0    0 ...    0    0    0]\r\n [  57 5203    0 ...    0    0    0]\r\n [5848 4164 2790 ...    0    0    0]]\r\n\r\nMy question is : Why ids_index[0,0]=1233 ? I think it should be 7250.", "comments": ["@xueyuan1990 \r\n\r\nLooks like code is incomplete.Request you provide colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40147\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40147\">No</a>\n"]}, {"number": 40146, "title": "Java API doc for TFLite", "body": "As pointed out on the tflite@tensorflow.org list recently, there's no documentation for the use of Java against the TFLite API as yet. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/index.md\r\n\r\nThere is however API doc for Java for full TF.\r\n\r\nIs there anyone assigned to work on this ? If not, I'm happy to have a go.\r\n\r\n", "comments": ["Yes, thanks for flagging! We're actively working on this and hope to have it available around the time of the upcoming TF 2.3 release. We'll use this issue for external tracking.", "See the Java API doc here: https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/package-summary"]}, {"number": 40145, "title": "[TF/MLIR] tf-mlir-translate crash with specific build config", "body": "With the current TF HEAD (1421933a1d442bcc624f030d3be468620ce94164), tf-mlir-translate crashes on minimal input with this build config: \r\n```C=clang CXX=clang++  bazel --per_file_copt=mlir,llvm-project@-UNDEBUG --linkopt=\"-fuse-ld=lld\"     //tensorflow/compiler/mlir:tf-mlir-translate``` and ```bazel 3.0.0```.\r\n\r\n(Removing --per_file_copt=mlir,llvm-project@-UNDEBUG from the build config makes it run fine. )\r\n\r\n```\r\n$ tf-mlir-translate -graphdef-to-mlir   model_graph_crash.pbtxt\r\n\r\nPLEASE submit a bug report to  and include the crash backtrace.\r\nStack dump:\r\n0.\tProgram arguments: bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate -graphdef-to-mlir model_graph_crash.pbtxt \r\n #0 0x00000000075575cd llvm::sys::PrintStackTrace(llvm::raw_ostream&) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x75575cd)\r\n #1 0x0000000007555665 llvm::sys::RunSignalHandlers() (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x7555665)\r\n #2 0x00000000075578da SignalHandler(int) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x75578da)\r\n #3 0x00007faf5f479b20 __restore_rt (/lib64/libpthread.so.0+0x14b20)\r\n #4 0x0000000001a4b362 tensorflow::(anonymous namespace)::ImporterBase::InferOutputType(tensorflow::Node const&, int, mlir::Builder) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a4b362)\r\n #5 0x0000000001a52e4a tensorflow::(anonymous namespace)::ImporterBase::ConvertNode(tensorflow::Node const&) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a52e4a)\r\n #6 0x0000000001a40032 tensorflow::(anonymous namespace)::ImporterBase::Convert(llvm::StringRef, mlir::FunctionType, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::Node*, 4ul, std::allocator<tensorflow::Node*> > const&, llvm::ArrayRef<std::pair<mlir::Identifier, mlir::Attribute> >, bool) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a40032)\r\n #7 0x0000000001a382cc tensorflow::(anonymous namespace)::GraphDefImporter::Convert(mlir::MLIRContext*, tensorflow::Graph const&, tensorflow::GraphDebugInfo const&, tensorflow::FunctionLibraryDefinition const&, tensorflow::GraphImportConfig const&, llvm::StringRef) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a382cc)\r\n #8 0x0000000001a3635a tensorflow::ConvertGraphdefToMlir(tensorflow::GraphDef const&, tensorflow::GraphDebugInfo const&, tensorflow::GraphImportConfig const&, mlir::MLIRContext*, bool) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a3635a)\r\n #9 0x0000000001a22d1a tensorflow::GraphdefToMlirImport(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a22d1a)\r\n#10 0x0000000001a22352 tensorflow::GraphdefToMlirTranslateFunction(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a22352)\r\n#11 0x0000000001a203e5 mlir::GraphdefToMlirTranslateFunction(llvm::StringRef, mlir::MLIRContext*) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a203e5)\r\n#12 0x0000000001a207a8 std::_Function_handler<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*), mlir::OwningModuleRef (*)(llvm::StringRef, mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, llvm::StringRef&&, mlir::MLIRContext*&&) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a207a8)\r\n#13 0x00000000073bb991 std::_Function_handler<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*), mlir::TranslateToMLIRRegistration::TranslateToMLIRRegistration(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*)> const&)::$_0>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, mlir::MLIRContext*&&) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x73bb991)\r\n#14 0x00000000073bb6bd std::_Function_handler<mlir::LogicalResult (llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*), registerTranslateToMLIRFunction(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*)> const&)::$_3>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*&&) (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x73bb6bd)\r\n#15 0x0000000001901d88 main::$_0::operator()(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, llvm::raw_ostream&) const (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1901d88)\r\n#16 0x0000000001901c5e main (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1901c5e)\r\n#17 0x00007faf5f2a91a3 __libc_start_main (/lib64/libc.so.6+0x271a3)\r\n#18 0x000000000190102e _start (bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate+0x190102e)\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n\r\n\r\nInput file: model_graph_crash.pbtxt\r\n\r\n```\r\nnode {\r\n  name: \"Placeholder\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"_output_shapes\"\r\n    value {\r\n      list {\r\n        shape {\r\n          dim {\r\n            size: 1\r\n          }\r\n          dim {\r\n            size: 3\r\n          }\r\n          dim {\r\n            size: 224\r\n          }\r\n          dim {\r\n            size: 224\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shape\"\r\n    value {\r\n      shape {\r\n        dim {\r\n          size: 1\r\n        }\r\n        dim {\r\n          size: 3\r\n        }\r\n        dim {\r\n          size: 224\r\n        }\r\n        dim {\r\n          size: 224\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```", "comments": ["Mmm, this works for me. Let me try a different variant. What compiler version do you have on your system?", "> Mmm, this works for me. Let me try a different variant. What compiler version do you have on your system?\r\n\r\nIt was built with clang 9.0.1.\r\n\r\n```\r\nclang version 9.0.1 (Fedora 9.0.1-2.fc31)\r\nTarget: x86_64-unknown-linux-gnu\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\n```\r\n", "I've been able to reproduce this on multiple different systems (Fedora, RHEL) that have Clang 9.0.1.", "valgrind shows the following:\r\n\r\n```\r\n==1721870== Memcheck, a memory error detector\r\n==1721870== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\r\n==1721870== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info\r\n==1721870== Command: tf-mlir-translate -graphdef-to-mlir /home/uday/vgg_19_v2.pbtxt\r\n==1721870== \r\n==1721870== Use of uninitialised value of size 8\r\n==1721870==    at 0x1A516E2: tensorflow::(anonymous namespace)::ImporterBase::InferOutputType(tensorflow::Node const&, int, mlir::Builder) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A593EC: tensorflow::(anonymous namespace)::ImporterBase::ConvertNode(tensorflow::Node const&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A46441: tensorflow::(anonymous namespace)::ImporterBase::Convert(llvm::StringRef, mlir::FunctionType, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::Node*, 4ul, std::allocator<tensorflow::Node*> > const&, llvm::ArrayRef<std::pair<mlir::Identifier, mlir::Attribute> >, bool) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A3E6FB: tensorflow::(anonymous namespace)::GraphDefImporter::Convert(mlir::MLIRContext*, tensorflow::Graph const&, tensorflow::GraphDebugInfo const&, tensorflow::FunctionLibraryDefinition const&, tensorflow::GraphImportConfig const&, llvm::StringRef) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A3C7A9: tensorflow::ConvertGraphdefToMlir(tensorflow::GraphDef const&, tensorflow::GraphDebugInfo const&, tensorflow::GraphImportConfig const&, mlir::MLIRContext*, bool) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A29089: tensorflow::GraphdefToMlirImport(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A28761: tensorflow::GraphdefToMlirTranslateFunction(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A267F4: mlir::GraphdefToMlirTranslateFunction(llvm::StringRef, mlir::MLIRContext*) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A26BB7: std::_Function_handler<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*), mlir::OwningModuleRef (*)(llvm::StringRef, mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, llvm::StringRef&&, mlir::MLIRContext*&&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x73E7560: std::_Function_handler<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*), mlir::TranslateToMLIRRegistration::TranslateToMLIRRegistration(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*)> const&)::$_0>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, mlir::MLIRContext*&&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x73E728C: std::_Function_handler<mlir::LogicalResult (llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*), registerTranslateToMLIRFunction(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*)> const&)::$_3>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*&&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1907D87: main::$_0::operator()(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, llvm::raw_ostream&) const (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870== \r\n==1721870== Invalid read of size 8\r\n==1721870==    at 0x1A516E2: tensorflow::(anonymous namespace)::ImporterBase::InferOutputType(tensorflow::Node const&, int, mlir::Builder) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A593EC: tensorflow::(anonymous namespace)::ImporterBase::ConvertNode(tensorflow::Node const&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A46441: tensorflow::(anonymous namespace)::ImporterBase::Convert(llvm::StringRef, mlir::FunctionType, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::Node*, 4ul, std::allocator<tensorflow::Node*> > const&, llvm::ArrayRef<std::pair<mlir::Identifier, mlir::Attribute> >, bool) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A3E6FB: tensorflow::(anonymous namespace)::GraphDefImporter::Convert(mlir::MLIRContext*, tensorflow::Graph const&, tensorflow::GraphDebugInfo const&, tensorflow::FunctionLibraryDefinition const&, tensorflow::GraphImportConfig const&, llvm::StringRef) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A3C7A9: tensorflow::ConvertGraphdefToMlir(tensorflow::GraphDef const&, tensorflow::GraphDebugInfo const&, tensorflow::GraphImportConfig const&, mlir::MLIRContext*, bool) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A29089: tensorflow::GraphdefToMlirImport(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A28761: tensorflow::GraphdefToMlirTranslateFunction(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A267F4: mlir::GraphdefToMlirTranslateFunction(llvm::StringRef, mlir::MLIRContext*) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A26BB7: std::_Function_handler<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*), mlir::OwningModuleRef (*)(llvm::StringRef, mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, llvm::StringRef&&, mlir::MLIRContext*&&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x73E7560: std::_Function_handler<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*), mlir::TranslateToMLIRRegistration::TranslateToMLIRRegistration(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*)> const&)::$_0>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, mlir::MLIRContext*&&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x73E728C: std::_Function_handler<mlir::LogicalResult (llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*), registerTranslateToMLIRFunction(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*)> const&)::$_3>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*&&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1907D87: main::$_0::operator()(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, llvm::raw_ostream&) const (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==  Address 0xe8 is not stack'd, malloc'd or (recently) free'd\r\n==1721870== \r\nPLEASE submit a bug report to  and include the crash backtrace.\r\nStack dump:\r\n0.\tProgram arguments: tf-mlir-translate -graphdef-to-mlir /home/uday/vgg_19_v2.pbtxt \r\n #0 0x000000000758627d llvm::sys::PrintStackTrace(llvm::raw_ostream&) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x758627d)\r\n #1 0x0000000007584315 llvm::sys::RunSignalHandlers() (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x7584315)\r\n #2 0x000000000758658a SignalHandler(int) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x758658a)\r\n #3 0x000000000aa06dd0 __restore_rt (/lib64/libpthread.so.0+0x12dd0)\r\n #4 0x0000000001a516e2 tensorflow::(anonymous namespace)::ImporterBase::InferOutputType(tensorflow::Node const&, int, mlir::Builder) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a516e2)\r\n #5 0x0000000001a593ed tensorflow::(anonymous namespace)::ImporterBase::ConvertNode(tensorflow::Node const&) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a593ed)\r\n #6 0x0000000001a46442 tensorflow::(anonymous namespace)::ImporterBase::Convert(llvm::StringRef, mlir::FunctionType, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::Node*, 4ul, std::allocator<tensorflow::Node*> > const&, llvm::ArrayRef<std::pair<mlir::Identifier, mlir::Attribute> >, bool) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a46442)\r\n #7 0x0000000001a3e6fc tensorflow::(anonymous namespace)::GraphDefImporter::Convert(mlir::MLIRContext*, tensorflow::Graph const&, tensorflow::GraphDebugInfo const&, tensorflow::FunctionLibraryDefinition const&, tensorflow::GraphImportConfig const&, llvm::StringRef) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a3e6fc)\r\n #8 0x0000000001a3c7aa tensorflow::ConvertGraphdefToMlir(tensorflow::GraphDef const&, tensorflow::GraphDebugInfo const&, tensorflow::GraphImportConfig const&, mlir::MLIRContext*, bool) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a3c7aa)\r\n #9 0x0000000001a2908a tensorflow::GraphdefToMlirImport(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a2908a)\r\n#10 0x0000000001a28762 tensorflow::GraphdefToMlirTranslateFunction(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a28762)\r\n#11 0x0000000001a267f5 mlir::GraphdefToMlirTranslateFunction(llvm::StringRef, mlir::MLIRContext*) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a267f5)\r\n#12 0x0000000001a26bb8 std::_Function_handler<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*), mlir::OwningModuleRef (*)(llvm::StringRef, mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, llvm::StringRef&&, mlir::MLIRContext*&&) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1a26bb8)\r\n#13 0x00000000073e7561 std::_Function_handler<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*), mlir::TranslateToMLIRRegistration::TranslateToMLIRRegistration(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*)> const&)::$_0>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, mlir::MLIRContext*&&) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x73e7561)\r\n#14 0x00000000073e728d std::_Function_handler<mlir::LogicalResult (llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*), registerTranslateToMLIRFunction(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*)> const&)::$_3>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*&&) (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x73e728d)\r\n#15 0x0000000001907d88 main::$_0::operator()(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, llvm::raw_ostream&) const (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1907d88)\r\n#16 0x0000000001907c5e main (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x1907c5e)\r\n#17 0x000000000ae4f6a3 __libc_start_main (/lib64/libc.so.6+0x236a3)\r\n#18 0x000000000190702e _start (/ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate+0x190702e)\r\n==1721870== \r\n==1721870== Process terminating with default action of signal 11 (SIGSEGV): dumping core\r\n==1721870==  Access not within mapped region at address 0xE8\r\n==1721870==    at 0x1A516E2: tensorflow::(anonymous namespace)::ImporterBase::InferOutputType(tensorflow::Node const&, int, mlir::Builder) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A593EC: tensorflow::(anonymous namespace)::ImporterBase::ConvertNode(tensorflow::Node const&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A46441: tensorflow::(anonymous namespace)::ImporterBase::Convert(llvm::StringRef, mlir::FunctionType, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::OutputTensor, 4ul, std::allocator<tensorflow::OutputTensor> > const&, absl::lts_2020_02_25::InlinedVector<tensorflow::Node*, 4ul, std::allocator<tensorflow::Node*> > const&, llvm::ArrayRef<std::pair<mlir::Identifier, mlir::Attribute> >, bool) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A3E6FB: tensorflow::(anonymous namespace)::GraphDefImporter::Convert(mlir::MLIRContext*, tensorflow::Graph const&, tensorflow::GraphDebugInfo const&, tensorflow::FunctionLibraryDefinition const&, tensorflow::GraphImportConfig const&, llvm::StringRef) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A3C7A9: tensorflow::ConvertGraphdefToMlir(tensorflow::GraphDef const&, tensorflow::GraphDebugInfo const&, tensorflow::GraphImportConfig const&, mlir::MLIRContext*, bool) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A29089: tensorflow::GraphdefToMlirImport(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A28761: tensorflow::GraphdefToMlirTranslateFunction(llvm::StringRef, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view, bool, bool, bool, bool, bool, mlir::MLIRContext*) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A267F4: mlir::GraphdefToMlirTranslateFunction(llvm::StringRef, mlir::MLIRContext*) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1A26BB7: std::_Function_handler<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*), mlir::OwningModuleRef (*)(llvm::StringRef, mlir::MLIRContext*)>::_M_invoke(std::_Any_data const&, llvm::StringRef&&, mlir::MLIRContext*&&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x73E7560: std::_Function_handler<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*), mlir::TranslateToMLIRRegistration::TranslateToMLIRRegistration(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::StringRef, mlir::MLIRContext*)> const&)::$_0>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, mlir::MLIRContext*&&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x73E728C: std::_Function_handler<mlir::LogicalResult (llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*), registerTranslateToMLIRFunction(llvm::StringRef, std::function<mlir::OwningModuleRef (llvm::SourceMgr&, mlir::MLIRContext*)> const&)::$_3>::_M_invoke(std::_Any_data const&, llvm::SourceMgr&, llvm::raw_ostream&, mlir::MLIRContext*&&) (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==    by 0x1907D87: main::$_0::operator()(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, llvm::raw_ostream&) const (in /ws/uday/e709de84875137bc446430ff58539a24/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-mlir-translate)\r\n==1721870==  If you believe this happened as a result of a stack\r\n==1721870==  overflow in your program's main thread (unlikely but\r\n==1721870==  possible), you can try to increase the size of the\r\n==1721870==  main thread stack using the --main-stacksize= flag.\r\n==1721870==  The main thread stack size used in this run was 8388608.\r\n==1721870== \r\n==1721870== HEAP SUMMARY:\r\n==1721870==     in use at exit: 14,660,131 bytes in 227,099 blocks\r\n==1721870==   total heap usage: 525,927 allocs, 298,828 frees, 32,313,760 bytes allocated\r\n==1721870== \r\n==1721870== LEAK SUMMARY:\r\n==1721870==    definitely lost: 0 bytes in 0 blocks\r\n==1721870==    indirectly lost: 0 bytes in 0 blocks\r\n==1721870==      possibly lost: 335,572 bytes in 12,044 blocks\r\n==1721870==    still reachable: 14,324,559 bytes in 215,055 blocks\r\n==1721870==                       of which reachable via heuristic:\r\n==1721870==                         newarray           : 21,776 bytes in 17 blocks\r\n==1721870==         suppressed: 0 bytes in 0 blocks\r\n==1721870== Rerun with --leak-check=full to see details of leaked memory\r\n==1721870== \r\n==1721870== Use --track-origins=yes to see where uninitialised values come from\r\n==1721870== For lists of detected and suppressed errors, rerun with: -s\r\n==1721870== ERROR SUMMARY: 4 errors from 2 contexts (suppressed: 0 from 0)\r\nSegmentation fault (core dumped)\r\n```", "It seems to be crashing due to --per_file_copt=mlir,llvm-project@-UNDEBUG, setting -UNDEBUG uniformly or not, works. \"--per_file_copt=llvm-project@-UNDEBUG\" also works, but seems like setting per file macros might be too fragile (e.g., it may work but not supported).\r\n\r\nI'd recommend uniformly setting it either way.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40145\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40145\">No</a>\n", "> It seems to be crashing due to --per_file_copt=mlir,llvm-project@-UNDEBUG, setting -UNDEBUG uniformly or not, works. \"--per_file_copt=llvm-project@-UNDEBUG\" also works, but seems like setting per file macros might be too fragile (e.g., it may work but not supported).\r\n> \r\n> I'd recommend uniformly setting it either way.\r\n\r\nThanks very much!\r\n"]}, {"number": 40144, "title": "GPU Delegate doesn't work for tflite model on Android device", "body": "I was trying to increase the run speed of Posenet, these are the performances:\r\n\r\n2020-06-04 08:41:21.143 I/InterpreterPosenet: Scaling to [-1,1] took 207.85 ms\r\n2020-06-04 08:41:21.894 I/posenet: Interpreter took 743.94 ms\r\n2020-06-04 08:41:22.559 I/InterpreterPosenet: Scaling to [-1,1] took 226.94 ms\r\n2020-06-04 08:41:23.058 I/posenet: Interpreter took 498.60 ms\r\n2020-06-04 08:41:23.497 I/InterpreterPosenet: Scaling to [-1,1] took 191.16 ms\r\n2020-06-04 08:41:23.858 I/posenet: Interpreter took 360.01 ms\r\n2020-06-04 08:41:24.324 I/InterpreterPosenet: Scaling to [-1,1] took 223.37 ms\r\n2020-06-04 08:41:24.866 I/posenet: Interpreter took 541.24 ms\r\n\r\nAs you can see this aren't very good performances and I was trying to find a way to increase the speed, and then I found out the GPU Delegate from tensorflow in this link:\r\nhttps://www.tensorflow.org/lite/performance/gpu\r\n\r\nAnd also in this link you can see that the speed of PoseNet increased somewhere by 2x,\r\n\r\nI followed the instructions and this is my init method of the class Posenet.kt:\r\n  init {\r\n    val gpuDelegate = GpuDelegate()\r\n    val options = Interpreter.Options().addDelegate(gpuDelegate)\r\n    interpreter = Interpreter(loadModelFile(\"posenet_model.tflite\", context), options)\r\n  }\r\n\r\nAlso the gradle dependencies: \r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.0.0'\r\n    implementation 'org.tensorflow:tensorflow-lite:2.1.0'\r\n \r\nI also made some measurements for 40 frames and Posenet took: 550.2947368 ms on average and after GPU Delegate: 528.7305263 ms on average \r\n\r\nAnd also I don't get any errors from the method run() or from Interpreter, so what I am doing wrong ?\r\n\r\nThe device has the RK3399 processor: http://opensource.rock-chips.com/wiki_RK3399\r\n\r\n**EDIT:** \r\nI Found out that this processor (RK3399) doesn't support GPU delegate, but I also tried on a device with Qualcomm 835 processor ( https://www.qualcomm.com/products/snapdragon-835-mobile-platform ) which supports GPU Delegate but still doesn't improve the performance.", "comments": ["Hi Terry, can you help take a look?\r\n\r\nthanks!", "@ArtanBerisha1 could you sync the repo and try again?\r\nI've updated the sample code to use TFLite 2.2.0\r\nMine shows around 40ms on average.", "@terryheo Thanks for the response, \r\nI updated TFLite implementation to 2.2.0 in gradle file, and the 'org.tensorflow:tensorflow-lite-gpu:2.0.0' is the same, I have sync the repo and tried again but the speed is 300ms on average with  Qualcomm 835 processor and its the same when I dont have the GpuDelegate as Interpreter Option. \r\nThe only thing that PoseNet performed better is when I set setNumThreads(4) then the speed is 180ms on average, but I need to use gpu delegate.\r\nWhat processor does your device has , because 40ms on average looks to fast. \r\n", "I used Pixel 4 and Galaxy S10e.\r\nDid you modify source to use GPU?\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/posenet/android/posenet/src/main/java/org/tensorflow/lite/examples/posenet/lib/Posenet.kt#L76\r\nIt should be changed to\r\n```\r\nval device: Device = Device.GPU\r\n```", "As I have written before in the question I have the code a little different, but I think it does the same thing:\r\nprivate var interpreter: Interpreter? = null\r\ninit {\r\n    val gpuDelegate = GpuDelegate()\r\n    val options = Interpreter.Options()\r\n    options.setNumThreads(4)\r\n    options.addDelegate(gpuDelegate)\r\n    interpreter = Interpreter(loadModelFile ( \"posenet.tflite\" , contex), options )\r\n}\r\nI create the GpuDelegate() and pass it to the Interpreter Options, but not improvement on the speed.\r\n\r\n\r\n", "Can you see the following log from logcat?\r\n```\r\n06-11 10:39:04.868 14446 14524 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n06-11 10:39:04.873 14446 14524 I tflite  : Initialized TensorFlow Lite runtime.\r\n06-11 10:39:04.895 14446 14524 I tflite  : Initialized OpenGL-based API.\r\n06-11 10:39:05.305 14446 14524 I posenet : Interpreter took 192.60 ms\r\n```\r\n(The third line could be \"OpenCL\")\r\n\r\nThis is a log from my Pixel 1 which uses 821 processor. I think result from 835 should be better.\r\n", "@terryheo thanks for the response\r\nThese 2 lines are showing:\r\n2020-06-12 08:55:33.250 I/tflite: Created TensorFlow Lite delegate for GPU.\r\n2020-06-12 08:55:33.259 I/tflite: Initialized TensorFlow Lite runtime.\r\n\r\nCan you try to remove options.setNumThreads(4) and then see if the time is the same with and without GpuDelegate ?\r\nBecause when I try without setNumThreads(4), I have the same result with GpuDelegate and without it. \r\n\r\n**EDIT**\r\nThose 2 lines are showing when I search for tflite in Logcat, and when I search about Posenet I get the line nr.4 ", "With GPU delegate, the thread number doesn't affect the performance.\r\nSo since you can't see the improvement, I guess you're using GPU delegate correctly.\r\n(In my devices, 4 threads CPU performs similar to GPU)\r\nBTW, I'm curious why you don't see \"Initialized OpenGL-based API.\" or \"Initialized OpenCL-based API.\"\r\nCould you check if you're using 'org.tensorflow:tensorflow-lite-gpu:2.2.0' ?", "@terryheo I had the gpu:2.0.0 version and upgrade to gpu:2.2.0, and I worked the speed of the model is around 110-100ms, but I have a question about the thread number:\r\nYou said that the thread number doesn't affect the performance but when I try just GpuDelegate without the number of threads then the speed increase around 150ms and its not to stable like it is when I set the number of threads, so is this happening just by chance ?\r\nMy code:\r\n    val options = Interpreter.Options()\r\n    val gpuDelegate = GpuDelegate()\r\n    options.setNumThreads(4)\r\n    options.addDelegate(gpuDelegate)", "If a model is not fully delegated to GPU, some graph will still run on CPU. In such case, setting the thread number makes sense. But this case, the graph is fully delegated to GPU, so the setting thread number is meaningless.\r\nBTW, I've just submitted several changes including the library version change and improving preprocessing. You'll see some performance improvement with the master branch.\r\n\r\nI think you have success with GPU delegate now. So let's close this.", "Thanks for your help @terryheo "]}, {"number": 40143, "title": "OperatorNotAllowedInGraphError: using a tf.Tensor as a Python bool is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.", "body": "```\r\ndef unet(pretrained_weights = None,input_size = (256,256,1)):\r\n    inputs = keras.Input(shape = input_size)\r\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\r\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\r\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\r\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\r\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\r\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\r\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\r\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\r\n    drop4 = Dropout(0.5)(conv4)\r\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\r\n\r\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\r\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\r\n    drop5 = Dropout(0.5)(conv5)\r\n\r\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\r\n    merge6 = concatenate([drop4,up6], axis = 3)\r\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\r\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\r\n\r\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\r\n    merge7 = concatenate([conv3,up7], axis = 3)\r\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\r\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\r\n\r\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\r\n    merge8 = concatenate([conv2,up8], axis = 3)\r\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\r\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\r\n\r\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\r\n    merge9 = concatenate([conv1,up9], axis = 3)\r\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\r\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\r\n\r\n    model = Model(inputs = inputs, outputs = conv10)\r\n\r\n    def iou(y_pred, y_true):\r\n        y_pred = tf.cast((y_pred > 0), dtype=tf.float32)\r\n        i = tf.reduce_sum(y_true * y_pred)\r\n        u = tf.reduce_sum(y_true + y_pred)\r\n        return (i / u).item()if u != 0 else u.item()\r\n    \r\n    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy',iou])\r\n    \r\n    model.summary()\r\n\r\n\r\n    if(pretrained_weights):\r\n    \tmodel.load_weights(pretrained_weights)\r\n\r\n    return `model`\r\n\r\nmodel = unet()\r\n```\r\nWhen I run the code above, I encounter the following errors:\r\n\r\n```\r\nOperatorNotAllowedInGraphError Traceback (most recent call last)\r\nin \r\n----> 1 model = unet()\r\n\r\nin unet(pretrained_weights, input_size)\r\n51 return (i / u).item()if u != 0 else u.item()\r\n52\r\n---> 53 model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy',iou])\r\n54\r\n55 model.summary()\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n455 self._self_setattr_tracking = False # pylint: disable=protected-access\r\n456 try:\r\n--> 457 result = method(self, *args, **kwargs)\r\n458 finally:\r\n459 self._self_setattr_tracking = previous_value # pylint: disable=protected-access\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n437 targets=self._targets,\r\n438 skip_target_masks=self._prepare_skip_target_masks(),\r\n--> 439 masks=self._prepare_output_masks())\r\n440\r\n441 # Prepare sample weight modes. List with the same length as model outputs.\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _handle_metrics(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)\r\n2002 metric_results.extend(\r\n2003 self._handle_per_output_metrics(self._per_output_metrics[i],\r\n-> 2004 target, output, output_mask))\r\n2005 if return_weighted_and_unweighted_metrics or return_weighted_metrics:\r\n2006 metric_results.extend(\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)\r\n1953 with K.name_scope(metric_name):\r\n1954 metric_result = training_utils.call_metric_function(\r\n-> 1955 metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n1956 metric_results.append(metric_result)\r\n1957 return metric_results\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py in call_metric_function(metric_fn, y_true, y_pred, weights, mask)\r\n1153\r\n1154 if y_pred is not None:\r\n-> 1155 return metric_fn(y_true, y_pred, sample_weight=weights)\r\n1156 # Mean metric only takes a single value.\r\n1157 return metric_fn(y_true, sample_weight=weights)\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in call(self, *args, **kwargs)\r\n194 from tensorflow.python.keras.distribute import distributed_training_utils # pylint:disable=g-import-not-at-top\r\n195 return distributed_training_utils.call_replica_local_fn(\r\n--> 196 replica_local_fn, *args, **kwargs)\r\n197\r\n198 @Property\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)\r\n1133 with strategy.scope():\r\n1134 return strategy.extended.call_for_each_replica(fn, args, kwargs)\r\n-> 1135 return fn(*args, **kwargs)\r\n1136\r\n1137\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in replica_local_fn(*args, **kwargs)\r\n177 def replica_local_fn(*args, **kwargs):\r\n178 \"\"\"Updates the state of the metric in a replica-local context.\"\"\"\r\n--> 179 update_op = self.update_state(*args, **kwargs) # pylint: disable=not-callable\r\n180 with ops.control_dependencies([update_op]):\r\n181 result_t = self.result() # pylint: disable=not-callable\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n74\r\n75 with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\r\n---> 76 update_op = update_state_fn(*args, **kwargs)\r\n77 if update_op is not None: # update_op will be None in eager execution.\r\n78 metric_obj.add_update(update_op)\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in update_state(self, y_true, y_pred, sample_weight)\r\n585 y_pred, y_true)\r\n586\r\n--> 587 matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n588 return super(MeanMetricWrapper, self).update_state(\r\n589 matches, sample_weight=sample_weight)\r\n\r\nin iou(y_pred, y_true)\r\n49 i = tf.reduce_sum(y_true * y_pred)\r\n50 u = tf.reduce_sum(y_true + y_pred)\r\n---> 51 return (i / u).item()if u != 0 else u.item()\r\n52\r\n53 model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy',iou])\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in bool(self)\r\n755 TypeError.\r\n756 \"\"\"\r\n--> 757 self._disallow_bool_casting()\r\n758\r\n759 def nonzero(self):\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in _disallow_bool_casting(self)\r\n524 else:\r\n525 # Default: V1-style Graph execution.\r\n--> 526 self._disallow_in_graph_mode(\"using a tf.Tensor as a Python bool\")\r\n527\r\n528 def _disallow_iteration(self):\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in _disallow_in_graph_mode(self, task)\r\n513 raise errors.OperatorNotAllowedInGraphError(\r\n514 \"{} is not allowed in Graph execution. Use Eager execution or decorate\"\r\n--> 515 \" this function with @tf.function.\".format(task))\r\n516\r\n517 def _disallow_bool_casting(self):\r\n\r\nOperatorNotAllowedInGraphError: using a tf.Tensor as a Python bool is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```\r\n\r\nHow can I correct my code so that I can run it successfully?", "comments": ["I forgot to put the imports just now. The imports are as follow.\r\n```\r\nimport numpy as np \r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow.keras.models as models\r\nimport tensorflow.keras.layers as layers\r\nimport tensorflow.keras.optimizers as optimizers\r\nfrom tensorflow.keras.models import *\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.optimizers import *\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\nfrom tensorflow.keras import backend\r\n```", "@mkw18,\r\nCould you please provide the TensorFlow version you are using to run the code. \r\n\r\nI am able to run the code without any issues with TF v2.2. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/772eef4aeda4d7976a1fcd6c7790230a/40143.ipynb). Thanks!", "@amahendrakar,\r\nMy tensorflow version is 2.1.0, and my tf.keras version is 2.2.4-tf. I have some problem to open the link, maybe because I do not have the access for it. Could you please paste the content here? Thank you!", "@mkw18,\r\nPlease check this link and let us know if it works\r\n\r\nhttps://colab.research.google.com/gist/amahendrakar/772eef4aeda4d7976a1fcd6c7790230a/40143.ipynb\r\n\r\nThanks!", "![image](https://user-images.githubusercontent.com/56500720/83855069-26e23980-a74a-11ea-816a-a49b1841bba5.png)\r\nSorry, I still cannot access it.", "@amahendrakar,\r\nNow I can access that link, thank you!", "@rmothukuru \r\nBut I still cannot run it on tensorflow 2.1.0, and I cannot update the tensorflow because I am using the remote server to run my code, Do you know how can I change my code so that I can run it on the earlier tensorflow version?", "In this code I want to use mean-iou as the unet's metrics, but I do not know whether my code is correct.", "@mkw18,\r\nI was able to reproduce the issue with TF v2.1, please check [this gist](https://colab.sandbox.google.com/gist/amahendrakar/c29799e8f4ad45fb49a08311387e9d42/40143-2-1.ipynb) for reference.\r\n\r\nSeems like the issue was caused due a bug in TF v2.1 which was fixed in TF v2.2. ", "@amahendrakar\uff0c\r\nThank you! I will try TF v2.2 to run it.", "> Thank you! I will try TF v2.2 to run it.\r\n\r\n@mkw18,\r\nDid you get a chance to look at this? Please feel free to close the issue if resolved. Thanks!", "@amahendrakar,\r\nThank you! The problem have been resolved.", "Hi @mkw18 Can you please elaborate on how you solved this?", "> @mkw18,\r\n> I was able to reproduce the issue with TF v2.1, please check [this gist](https://colab.sandbox.google.com/gist/amahendrakar/c29799e8f4ad45fb49a08311387e9d42/40143-2-1.ipynb) for reference.\r\n> \r\n> Seems like the issue was caused due a bug in TF v2.1 which was fixed in TF v2.2.\r\n\r\n@amahendrakar \r\nThat's not a solution. If it was due to a bug then this bug is again present in tf2.3.0.\r\nCan you please elaborate more on the nature of the problem and potential areas to explore to fix it without modifying the tf version.\r\nThanks!", "> If it was due to a bug then this bug is again present in tf2.3.0.\r\n\r\n@haseeb33,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template along with a minimal reproducible code, so that we can track the issue there. Thanks!", "import numpy as np \r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow.keras.models as models\r\nimport tensorflow.keras.layers as layers\r\nimport tensorflow.keras.optimizers as optimizers\r\nfrom tensorflow.keras.models import *\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.optimizers import *\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\nfrom tensorflow.keras import backend\r\n\r\n\r\ndef unet(pretrained_weights = None,input_size = (256,256,1)):\r\n    inputs = keras.Input(shape = input_size)\r\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\r\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\r\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\r\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\r\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\r\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\r\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\r\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\r\n    drop4 = Dropout(0.5)(conv4)\r\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\r\n\r\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\r\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\r\n    drop5 = Dropout(0.5)(conv5)\r\n\r\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\r\n    merge6 = concatenate([drop4,up6], axis = 3)\r\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\r\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\r\n\r\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\r\n    merge7 = concatenate([conv3,up7], axis = 3)\r\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\r\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\r\n\r\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\r\n    merge8 = concatenate([conv2,up8], axis = 3)\r\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\r\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\r\n\r\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\r\n    merge9 = concatenate([conv1,up9], axis = 3)\r\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\r\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\r\n\r\n    model = Model(inputs = inputs, outputs = conv10)\r\n\r\n    def iou(y_pred, y_true):\r\n        y_pred = tf.cast((y_pred > 0), dtype=tf.float32)\r\n        i = tf.reduce_sum(y_true * y_pred)\r\n        u = tf.reduce_sum(y_true + y_pred)\r\n        return (i / u).item()if u != 0 else u.item()\r\n\r\n    ssim1 = tf.image.ssim(inputs, conv10, max_val=255, filter_size=11,filter_sigma=1.5, k1=0.01, k2=0.03)\r\n    \r\n    model.compile(optimizer = Adam(lr = 1e-4), loss = 'ssim1', metrics = ['accuracy',iou])\r\n    \r\n    model.summary()\r\n\r\n\r\n    if(pretrained_weights):\r\n    \tmodel.load_weights(pretrained_weights)\r\n\r\n    return model\r\n\r\nmodel = unet()", "I got same error when i use this code in google colab but i am using SSIM loss please correct it.\r\n\r\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n", "@amahendrakar i met the same problem in TF2.3.0", "@sunzhe09,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Same problem with 2.3.1", "@mfoglio,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n", "@gupta35 Did you resolve the problem? I am facing the same problem with ssim loss"]}, {"number": 40142, "title": "speech command related issue on custom dataset", "body": "steps i followed for making dataset\r\n1) collect 4 word (number,tollfree ,call,virtual) \r\nconvert them into TTS and changed its pitch to make more data and achieved \r\n2500 data per sample\r\n2) while training im getting an accuracy of 100% and validation is also 100% but when i test this on call data the result are all wrong.\r\n99% is all wrong.\r\n`I0604 07:34:51.632841 140415957108544 train.py:242] Step #1500: rate 0.000020, accuracy 100.00%, cross entropy 0.004794`\r\n\r\n\r\ni dont know what im doing wrong", "comments": ["@dimanshu \r\nThis doesnt seem like a tensorflow issue,  In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here", "@dimanshu\r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40141, "title": "ERROR: /tensorflow_src/tensorflow/core/kernels/BUILD:4135:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 4)", "body": "\r\n**System information**\r\n- OS Platform: host: mac 14.10 \r\n- docker image:  tensorflow/tensorflow:latest-devel\r\n- TensorFlow version: 2.2\r\n- Python version: 3.6.9\r\n- building tensorflow from source using docker on mac with ubuntu in docker image\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NONE ( cpu only build)\r\n- GPU model and memory: NONE \r\n\r\n**the problem**\r\n```\r\nERROR: /tensorflow_src/tensorflow/core/kernels/BUILD:4135:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 4)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1941.471s, Critical Path: 387.25s\r\nINFO: 668 processes: 668 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n- I am trying to build tensorflow from source to enable the AVX and FMA instruction. i followed the exact set of instructions inorder to start building. As given [here](https://www.tensorflow.org/install/source#docker_linux_builds) i am building the cpu only version.\r\n- Since i am building from the latest one, I should be building from master branch. so I initiated using the below command: \r\n\r\n` bazel build --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --local_cpu_resources=1 --local_ram_resources=2048  //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n- after this the analysis phase was done. The error occurred during compilation.\r\n\r\nunable to diagnose the error. Please help\r\n\r\n", "comments": ["@Amarnathde ,\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40141\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40141\">No</a>\n"]}, {"number": 40140, "title": "TF model convert to TFLite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04.3\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):1.9.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = \"/home/pychen/MobileNetV2-SSD/MobileNetV2-SSD_600x300/frozen_inference_graph.pb\"\r\ninput_arrays = [\"image_tensor\"]\r\noutput_arrays = [\"detection_classes\", \"detection_boxes\", \"detection_scores\"]\r\n\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(\r\n  graph_def_file=graph_def_file, input_arrays=input_arrays, output_arrays=output_arrays)\r\ntflite_model = converter.convert()\r\nopen(\"MobileNetV2_SSD.tflite\", \"wb\").write(tflite_model)\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last):\r\n  File \"/home/pychen/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 418, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_UINT8, Truncate=false](image_tensor). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf_to_tflite_graphdef.py\", line 8, in <module>\r\n    graph_def_file=graph_def_file, input_arrays=input_arrays, output_arrays=output_arrays)\r\n  File \"/home/pychen/.local/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py\", line 204, in from_frozen_graph\r\n    import_graph_def(graph_def, name=\"\")\r\n  File \"/home/pychen/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/pychen/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 422, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_UINT8, Truncate=false](image_tensor). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@ivs-pychen,\r\nIn order to expedite the trouble-shooting process, could you please share the `frozen_inference_graph.pb` file you are using in the code.\r\n\r\nAlso, is there any specific reason you are using TensorFlow v1.9? Please upgrade to TensorFlow 2.2 or 1.15 and let us know if you are facing the same issue. Thanks!", "\r\n[frozen_inference_graph.zip](https://github.com/tensorflow/tensorflow/files/4734220/frozen_inference_graph.zip)\r\n", "@ivs-pychen,\r\nTensorFlow 1.9 is outdated. Could you please update TensorFlow to v2.2 or v1.15 and let us know if you are facing the same issue. Thanks!", "Do I need to retrain the model using TensorFlow v1.15 and use export_tflite_ssd_graph.py to convert Mobilenet v2 SSD to tensorflow lite?", "@ivs-pychen Looks like this model have control flows. Can you please enable control flow v2 in TF, so your model will use control flow v2 instead of v1 (which we don't support)\r\nYou can add tf.enable_control_flow_v2()\r\n\r\nThanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40139, "title": "Timeseries forecasting tutorial has bias issues", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the time-series forecasting tutorial, the normalization is done prior to obtaining time-series windows. \r\nConsider this:\r\n`uni_data = (uni_data-uni_train_mean)/uni_train_std`\r\nThis is done before:\r\n```\r\nx_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\r\n                                           univariate_past_history,\r\n                                           univariate_future_target)\r\n```\r\nThis is causing the past_history samples using values of future targets as well during the normalization. This is a bias. In reality, we cannot use future values to normalize current values.\r\nThis, I think, is a bias and a bug.\r\n\r\n### Correct links\r\n\r\n\r\n\r\n### Parameters defined\r\n\r\n\r\n### Returns defined\r\n\r\n\r\n### Raises listed and defined\r\n\r\n\r\n### Usage example\r\n\r\nNormalization should be done after extraction of sequences and only using the LHS of the sequence. I still dont know if normalizing the RHS of the sequence is desired. but does not hurt as long as we denormalize\r\n\r\n### Request visuals, if applicable\r\n\r\n\r\n### Submit a pull request?\r\n\r\n", "comments": ["@SarnathK \r\nThis does not seem like a tensor flow issue.\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "This is an issue in the Documentation. When I raised this issue, I remember selecting \"documentation issue\". ", "One of you, Can you at least confirm that this is actually an issue? Thanks!", "Hi, \r\n\r\nThanks for reporting this. You do have a valid point: all the training data is normalized based on all the training data, making the training performance unrealistic.\r\n\r\nIn some ways this could be dismissed as a non-issue, since this is the point of validation and test splits, to get a better view of real-world performance.\r\n\r\nBut it could be a good way to introduce moving averages, both for smoothing, or [ARIMA](https://otexts.com/fpp2/MA.html) style models.\r\n\r\nI'm working on this tutorial right now, I'll see what I can do.", "[--Below was EDITED for more clarity]\r\n\r\nHi Mark,\r\n\r\nThanks for sharing your thoughts. Agree that validation and test _may_ bring out the gap. But then, imho, it does not help the cause of creating a good model.... \r\n\r\nWe ran into some unrealistic performance in our own project esp. for a longer multi-step model. We had used 6 horizons.\r\n\r\nApart from this, if we decide to do normalization after creation of the actual training set (univariate_data() and multivariate_data() functions) then there is a need to be aware of another type of Bias. The way the sliding windows are created for training set, also introduces a bias because future values of 1 row will enter the history of another row creating a bias while calculating mean and standard deviation. So using mean and deviation after creation of the  training-set is also not free from Bias.\r\n\r\nFor our project, we finally settled on slicing history_size of the mother data (before the sliding windows method of creating training data) alone starting from t=0 (i.e. dataset[: , 0:history_size] or dataset[0:history_size, :] depending on whichever axis represents time) and using its mean and standard-deviation for the training-set and test-set. This ensures that the mean and standard-deviation are completely bias-free.\r\n\r\nFinally, My request would be to include a note on this in the documentation so that readers understand the pitfalls of the other approaches.\r\n\r\nThanks,\r\nBest,\r\nSarnath", "The above `d7ff105` commit addresses the reported issue. Thanks! \r\nFeel free to reopen if necessary."]}, {"number": 40138, "title": "tf.broadcast_to abort() and core dump when given shape value overflows int32", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0 & v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.broadcast_to` aborts and core dump if given `shape` could overflow `int32`. \r\nThe `abort()` only seems to happen when `input` is in `uint32` or `uint64`.\r\nIf `input` is other dtype like `int32` or `float32`, tensorflow would not abort, but instead throw an exception to report the issue.\r\n**Describe the expected behavior**\r\nTensorflow should not abort() and core dump.\r\nTensorflow should give a more meaningful message when overflow occurs and make the error handling behavior consistent.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nWith `input` being `uint32` or `uint64` and a `shape` big enough to overflow `int32`:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nin_tensor = np.array([1, 2]).astype('uint32')  # would later cause abort() and core dump\r\nshape = np.array([1e18, 2]).astype('int64') # make shape big enough to overflow int32\r\ntf.broadcast_to(in_tensor, shape)\r\n```\r\nwould cause the following:\r\n> ...\r\n2020-06-04 06:16:17.434615: F tensorflow/core/framework/tensor_shape.cc:345] Check failed: size >= 0 (-1486618624 vs. 0)\r\nAborted (core dumped)\r\n\r\nWhen `input` is not `uint32` nor `uint64` and a `shape` big enough to overflow `int32`:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nin_tensor = np.array([1, 2]).astype('float32')  # would later cause exception\r\nshape = np.array([1e18, 2]).astype('int64') # make shape big enough to overflow int32\r\ntf.broadcast_to(in_tensor, shape)\r\n```\r\nwould cause exception thrown instead of abort() and core dump:\r\n>...\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension -1486618624 must be >= 0 [Op:BroadcastTo]\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.1.0, 2.2.0 and the session is going to crash. Is this the expected behavior?.Thanks!", "@ravikyram Yes, the first case where tensorflow abort() and core dump will make colab session to crash.", "Added a PR #40294 for the fix.", "@leeyeetonn When I ran your codes with `tf-nightly`, it throws a different error than you are expecting but it is not crashing as you noticed earlier. [Here](https://colab.research.google.com/gist/jvishnuvardhan/bf06e470a5c4ba8be3c13d142700b78e/untitled86.ipynb) is a gist for our reference. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40138\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40138\">No</a>\n"]}, {"number": 40137, "title": "Can't training model with multi GPU", "body": "Hi, I try to train my model on Keras 2.3 and TF 2.2. I use tf.distribute.MirroredStrategy() and have issue #38500 and tf.keras.utils.multi_gpu_model but it was removed after 2020-04-01. I also face with OOM errors because i can't specify device to train. Please tell me how can i train model with multi-gpu and avoid OOM errors. Thank you so much!", "comments": ["In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the OOM issue. Thanks!\r\n\r\nAlso, please take a look at [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if limiting GPU memory growth resolves the issue. Thanks!", "My code:\r\n\r\nbatch_size = 16\r\n\r\ndata = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2,\r\n                          shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\", validation_split=0.2)\r\ntraindata = data.flow_from_directory(directory=\"../image-docs-similarity/visual-image-search/data/\",target_size=(380,380), batch_size=batch_size, subset='training')\r\ntestdata = data.flow_from_directory(directory=\"../image-docs-similarity/visual-image-search/data/\", target_size=(380,380), batch_size=batch_size, subset='validation')\r\n\r\nwith tf.device('/cpu:0'):\r\n    model = efn.EfficientNetB4(weights='imagenet')\r\n    predictions = Dense(115, activation='softmax')(model.layers[-2].output)\r\n    model = Model(inputs=model.input, outputs=predictions)\r\n\r\nopt = SGD(lr=1e-2) #num_epoch=100\r\nparallel_model = multi_gpu_model(model, gpus=2)\r\nparallel_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\r\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\r\n\r\nhist = parallel_model.fit_generator(steps_per_epoch=9501//batch_size,generator=traindata, validation_data= testdata, validation_steps=2324//batch_size,epochs=20,callbacks=[early])\r\n\r\nI face OOM when increasing batch_size. It's work with <=8. I used both MirroredStrategy() and multi_gpu_model but they aren't work. I hope keras have another way to train with multi gpu. Does it have? thank you\r\n", "@ChungNPH,\r\nI am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: '../image-docs-similarity/visual-image-search/data/'`.\r\n\r\nCould you please share the dataset you are using or provide a link to it, so that we can reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40136, "title": "Transfer segment_sum op from 2.2 to 1.15 version\uff0cOp type wrong\u3002", "body": "As the title, I try to transfer segment_sum op from 2.2 to 1.15 version. For some reason, we must use tensorflow 1.15. I diff the code between the two version. Finally, the  converted tflite model can compute correct, but the Op Type is wrong. \r\n<img width=\"182\" alt=\"TFLite Model\" src=\"https://user-images.githubusercontent.com/41407473/83709781-cf60a280-a651-11ea-81dc-00b8798a3d05.png\">\r\n<img width=\"143\" alt=\"TF Model\" src=\"https://user-images.githubusercontent.com/41407473/83710045-5dd52400-a652-11ea-9a7e-cf9def6dcb9a.png\">\r\n![image](https://user-images.githubusercontent.com/41407473/83710109-8c52ff00-a652-11ea-9b7a-3602347b046c.png)\r\nI find 120 corresponds to NonMaxSuppressionV4 in tf 2.2.\r\nThanks for your help.", "comments": ["@lizhen2017 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here or is possible please share a colab gist with the error faced. Thanks!\r\n", "> @lizhen2017\r\n> In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here or is possible please share a colab gist with the error faced. Thanks!\r\n- My tf 1.15 key modification follows below. And the test script is\r\n```bash\r\nbazel test -c opt //tensorflow/lite/kernels:segment_sum_test\r\nbazel build --copt=-march=native --copt=\"-Wno-error\" --config=noaws --config=nogcp --config=nohdfs --config=nokafka -c opt //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package ./tensorflow_pkg\r\npip install -U tensorflow_pkg/tensorflow-1.15.0-cp27-cp27mu-linux_x86_64.whl\r\npython segment_sum_test.py\r\n```\r\n\r\n1. tensorflow/lite/schema/schema.fbs\r\n```cpp\r\nenum BuiltinOperator : byte {\r\n\t...\r\n  WHILE = 119,\r\n  SEGMENT_SUM = 120\r\n}\r\n```\r\n```cpp\r\nunion BuiltinOptions {\r\n\t...\r\n  DepthToSpaceOptions,\r\n  SegmentSumOptions\r\n}\r\ntable SegmentSumOptions {\r\n} \r\n```\r\n2. generated tensorflow/lite/schema/schema_generated.h\u548ctensorflow/lite/builtin_ops.h\r\n```bash\r\nmv tensorflow/lite/schema/schema_generated.h\u4e3atensorflow/lite/schema/schema_generated.h.bak\r\nbazel run \\\r\n  //tensorflow/lite/schema/builtin_ops_header:generate > \\\r\n  tensorflow/lite/builtin_ops.h\r\nmv bazel-genfiles/tensorflow/lite/schema/schema_generated.h tensorflow/lite/schema/schema_generated.h\r\n```\r\n3. tensorflow/lite/core/api/flatbuffer_conversions.cc\r\n```cpp\r\nTfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\r\n                         ErrorReporter* error_reporter,\r\n                         BuiltinDataAllocator* allocator, void** builtin_data) {\r\n\t...\r\n    case BuiltinOperator_SEGMENT_SUM:       \r\n      break;\r\n}\r\n```\r\n4. [segment_sum.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/segment_sum.cc)\r\n[segment_sum_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/segment_sum_test.cc)\r\n5. tensorflow/lite/kernels/register.cc\r\n```cpp\r\nnamespace builtin {\r\n...\r\nTfLiteRegistration* Register_SEGMENT_SUM();\r\n}\r\n...\r\n```\r\n```cpp\r\nnamespace builtin {\r\n...\r\nBuiltinOpResolver::BuiltinOpResolver() {\r\n\t...\r\n\tAddBuiltin(BuiltinOperator_SEGMENT_SUM, Register_SEGMENT_SUM());\r\n}\r\n```\r\n6. tensorflow/lite/kernels/register_ref.cc\r\n```cpp\r\nnamespace builtin {\r\nTfLiteRegistration* Register_SEGMENT_SUM();\r\n}\r\n```\r\n```cpp\r\nnamespace builtin {\r\n...\r\nBuiltinRefOpResolver::BuiltinRefOpResolver() {\r\n\t...\r\n\tAddBuiltin(BuiltinOperator_SEGMENT_SUM, Register_SEGMENT_SUM());\r\n}\r\n}\r\n```\r\n7. tensorflow\\lite\\toco\\import_tensorflow.cc\r\n```cpp\r\nConverterMapType GetTensorFlowNodeConverterMap() {\r\n  return std::unordered_map<std::string, ConverterType>({\r\n\t\t...\r\n      {\"Rsqrt\", ConvertSimpleOperator<TensorFlowRsqrtOperator, 1, 1>},\r\n      {\"SegmentSum\", ConvertSimpleOperator<SegmentSumOperator, 2, 1>},\r\n      {\"Select\", ConvertSimpleOperator<SelectOperator, 3, 1>},\r\n\t\t...\r\n```\r\n- [tensorflow\\lite\\toco\\model.h](https://git.jd.com/cornerstone/tensorflow-1.15/blob/master/tensorflow/lite/toco/model.h#L149)\r\n```cpp\r\nenum class OperatorType : uint8 {\r\n\t...\r\n  kReorderAxes,\r\n  kSegmentSum,\r\n  kSelect,\r\n\t...\r\n}\r\n...\r\n```\r\n8. tensorflow\\lite\\toco\\model.h\r\n```cpp\r\nstruct SegmentSumOperator : Operator {\r\n  SegmentSumOperator() : Operator(OperatorType::kSegmentSum) {}\r\n};\r\n...\r\n```\r\n9. tensorflow\\lite\\toco\\tflite\\operator.cc\r\n```cpp\r\nstd::vector<std::unique_ptr<BaseOperator>> BuildOperatorList(\r\n    bool enable_select_tf_ops = false) {\r\n\t...\r\n\t  ops.push_back(MakeUnique<SimpleOperator<TensorFlowRankOperator>>(\r\n      \"RANK\", OperatorType::kRank));\r\n  \tops.emplace_back(new SimpleOperator<SegmentSumOperator>(\r\n      \"SEGMENT_SUM\", OperatorType::kSegmentSum));\r\n  \treturn ops;\r\n}\r\n```\r\n10. tensorflow\\lite\\toco\\tflite\\operator_test.cc\r\n```cpp\r\nTEST_F(OperatorTest, BuiltinSegmentSum) {\r\n  SegmentSumOperator op;\r\n  auto output_toco_op = SerializeAndDeserialize(\r\n      GetOperator(\"SEGMENT_SUM\", OperatorType::kSegmentSum), op);\r\n  ASSERT_NE(nullptr, output_toco_op.get());\r\n}\r\n```\r\n11. tensorflow\\lite\\toco\\tflite\\op_version.cc\r\n```cpp\r\nstring GetMinimumRuntimeVersionForModel(const Model& model) {\r\n  // Use this as the placeholder string if a particular op is not yet included\r\n  // in any Tensorflow's RC/Final release source package. Once that op is\r\n  // included in the release, please update this with the real version string.\r\n  static constexpr char kPendingReleaseOpVersion[] = \"UNKNOWN\";\r\n  // A map from the version key of an op to its minimum runtime version.\r\n  // For example, {{kAveragePool, 1}, \"1.5.0\"},  means the 1st version of\r\n  // AveragePool requires a minimum TF Lite runtime version '1.5.0`.\r\n  static const std::map<std::pair<OperatorType, int>, string>* op_version_map =\r\n      new std::map<std::pair<OperatorType, int>, string>({\r\n\t\t\t...\r\n          {{OperatorType::kLessEqual, 1}, \"1.14.0\"},\r\n          {{OperatorType::kLessEqual, 2}, \"1.14.0\"},\r\n          {{OperatorType::kSegmentSum, 1}, \"1.15.0\"},\r\n          {{OperatorType::kSelect, 1}, \"1.14.0\"},\r\n\t\t\t...\r\n```\r\n12. tensorflow\\lite\\toco\\graph_transformations\\propagate_fixed_sizes.cc\r\n```cpp\r\n  switch (op->type) {\r\n\t...\r\n    case OperatorType::kMatrixSetDiagV3:\r\n      // MatrixSetDiagV3 operators are converted to MatrixSetDiag, after which\r\n      // their shapes are propagated.\r\n      break;\r\n    case OperatorType::kSegmentSum:\r\n      break;\r\n\t...\r\n```\r\n13. tensorflow\\lite\\toco\\tooling_util.cc\r\n```cpp\r\nconst char* OperatorTypeName(OperatorType type) {\r\n  switch (type) {\r\n#define HANDLE_OPERATORTYPENAME_CASE(c) \\\r\n  case OperatorType::k##c:              \\\r\n    return #c;\r\n\t...\r\n\r\n    HANDLE_OPERATORTYPENAME_CASE(Rsqrt)\r\n    HANDLE_OPERATORTYPENAME_CASE(SegmentSum)\r\n    HANDLE_OPERATORTYPENAME_CASE(Shape)\r\n\t...\r\n```\r\n14. segment_sum_test.py\r\n```python\r\n\"\"\"TF 2.2.0rc2 or 1.15.0 could pass this example.\r\n\"\"\"\r\n# import tensorflow.compat.v1 as tf\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import lookup_ops\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.lite.python import interpreter as interpreter_wrapper\r\n\r\noutput_dir = \"./segment_sum\"\r\n\r\nwith tf.Session() as sess:\r\n    input_tensor = tf.placeholder(tf.float32, shape=[3,], name='input')\r\n    segment_ids = tf.placeholder(tf.int32, shape=[3,], name='segment_id')\r\n    output_tensor = tf.segment_sum(input_tensor, segment_ids, name=None)\r\n    res = sess.run(output_tensor, feed_dict={input_tensor:[1, 2, 3], segment_ids:[0, 0, 1]})\r\n    print(res)\r\n    graph_def = tf.get_default_graph().as_graph_def()\r\n    # tf.io.write_graph(graph_def, output_dir, 'graph_def.pbtxt')\r\n\r\n    converter = tf.lite.TFLiteConverter(graph_def, \r\n            [input_tensor, segment_ids],\r\n            [output_tensor])\r\n    # export model with tf ops\r\n    supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.target_spec.supported_ops = supported_ops\r\n    tflite_model = converter.convert()\r\n    open(output_dir + \"/model_with_tf.tflite\", \"wb\").write(tflite_model)\r\n    # export model without tf ops\r\n    supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n    converter.target_spec.supported_ops = supported_ops\r\n    # converter.allow_custom_ops = True\r\n    tflite_model = converter.convert()\r\n    open(output_dir + \"/model_without.tflite\", \"wb\").write(tflite_model)\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    input_shape = input_details[0]['shape']\r\n    input_data = np.array([1, 2, 3], dtype=np.float32)\r\n    segment_ids = np.array([0, 0, 1], dtype=np.int32)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n    interpreter.set_tensor(input_details[1]['index'], segment_ids)\r\n\r\n    interpreter.invoke()\r\n    interpreter.invoke()\r\n\r\n    tflite_results = interpreter.get_tensor(output_details[0]['index'])\r\n    print(tflite_results)\r\n```\r\n", "This isn't really something that can easily be supported. You'd have to manually set the SEGMENT_SUM op id to 125, rather than 120. Your best best is probably to train your model in 1.15, export to a SavedModel, then convert it with 2.2."]}, {"number": 40135, "title": "Can contrib_audio.mfcc has gardient ?", "body": "tensorflow version:1.14\r\npython:3.6\r\nI need to used operation(mfcc) in `tensorflow.python.ops import gen_audio_ops as contrib_audio` to extract mfcc feature from an audio, then compute the gradient for this audio. But I got error `LookupError: No gradient defined for operation 'mfcc' (op type: Mfcc)`. This operation can not via gradient? If not, how can I fix this error?\r\n", "comments": ["@winterwindwang Can you please provide a standalone code to reproduce issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40134, "title": "Fix missing tick in BatchNorm doc", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40134) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40134) for more info**.\n\n<!-- ok -->", "okay @kyscg I fixed a couple more missing ticks / typos in another file.", "Any updates? I find it interesting that it takes less than 3 hours for someone to tell me that the patch is correct but \"too small\", but more than 2 weeks (and counting) for this 6-line PR to get reviewed.", "Nearly 50 days now.", "If the tf team are willing to spend 10s on this PR, as they did when this PR was first issued to tell me that this is correct but can't be merged, contributing to tf won't need to be this painful, and tf documentation won't need to be so terrible.", "@SsnL sorry for the delay , will merge it as soon as possible."]}, {"number": 40133, "title": "in resolution of [Wsign-compare] warning ids : [16, 18, 19, 20, 21 ]", "body": "@mihaimaruseac ", "comments": ["@mihaimaruseac "]}, {"number": 40131, "title": "DLL load failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install \r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- GPU model and memory: \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nCannot succesfully import keras\r\n\r\n**Any other info / logs**\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\u726\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\u726\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\u726\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\u726\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\u726\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["@alecoto,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from a similar issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40131\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40131\">No</a>\n"]}, {"number": 40130, "title": "No Debug Symbols when using per_file_copt option", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 8 (Linux 4.18.0-147)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:virtualenv\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 8.3.1\r\n- CUDA/cuDNN version: Not used\r\n- GPU model and memory: Not used\r\n\r\n\r\nI've been building 2.2.0 from source (targets:build_pip_package and libtensorflow_cc.so) while  selectively including debug flags, using the per_file_copt option (per suggestion in issue #27495). \r\n\r\n My build succeeds, but the debug symbols aren't there for either libtensorflow_cc.so.2.2.0 or the libtensorflow_framework.so.2.2.0.\r\n\r\n Below are some of the options I've tried. If I examine the .so files, with 'readelf -S libtensorflow_cc.so.2.2.0' they indicate that the debug symbols exist, but not so for libtensorflow_framework.so.2.2.0. TotalView debugger and gdb don't appear to find any of the debug symbols\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI always build the build_pip_package target first. After it's complete I provide the exact same command (below), changing the build target to libtensorflow_cc.so\r\n\r\n\r\nVariation 1 (This doesn't uses the per_file_copt option, but demonstrates that the --strip=never compile option gives an error when put after the '-c' flags):\r\n bazel build -c dbg --strip=never //tensorflow/tools/pip_package:build_pip_package\r\n\r\nVariation 2: (moving the --strip=never flag in front of the -c options)\r\n bazel build --strip=never -c opt --config=opt --per_file_copt=//tensorflow/c/.*\\.cc,//tensorflow/cc/.*/.*\\.cc,//tensorflow/core/.*/.*\\.cc@-g //tensorflow/tools/pip_package:build_pip_package\r\n\r\nVariation 3 (try to prevent stripping by passing -O0 to the per_file_copt)\r\nbazel build -c opt --config=opt --per_file_copt=//tensorflow/c/.*\\.cc,//tensorflow/cc/.*/.*\\.cc,//tensorflow/core/.*/.*\\.cc@-g,O0  //tensorflow/tools/pip_package:build_pip_package\r\n\r\nPlease advise as to what I'm doing wrong and how to make sure that the debug symbols (which I believe are built), don't get stripped during linking.\r\n\r\nThanks\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40130\">No</a>\n", "I myself never sucessfully build TF with debug symbols.\r\n@hlopko could you comment on the options used above to build with bazel with debug symbols?", "This is a very difficult problem for me. I'm trying to advocate that my company migrate their software to TF 2.X, from 1.X. We use the C/C++ API for inference and Python API for model training. \r\n\r\nWe've observed threading contention and other issues when calling TF C++ API from within our product. So, I'd really like to be able to do some debugging. If I compile the entire program with debug symbols, the .so file is > 4G and I can't even link to it. So unfortunately that's not an option. \r\n\r\nI need debug symbols for a handful of files. Thanks", "You could take a look at https://github.com/tensorflow/tensorflow/issues/27495#issuecomment-481101932", "Thanks. I did look at that, and followed it very closely. It's cross-referenced in my original post.\r\n\r\n I had the same issues as that user. I also got desperate and decided to compile with the  -g flag set for all files.  After compiling, the linker was trying to link for hours so I gave up. In addition to the fact that the .so file was > 15G, compiling a full debug build seems unhelpful for me. I can't link it.\r\n\r\nSince my original post I've tried a few additional experiments. For example, I found out that the option _--strip=never_ needs to passed to the bazel build command before an options that have a '-c' or '-copt' flag. But, unfortunately this didn't help.\r\n\r\nIn issue #27495, the individual who submitted the ticket made the following statement:\r\n> with this new command, gdb can read symbols from libtensorflow.so, but still unable to read from libtensorflow_framework.so\r\n\r\nI'm not sure how this was determined, but if I run \r\n`readelf -S bazel-bin/tensorflow/libtensorflow_cc.so.2.2.0`, \r\nI see lines matching the pattern _.debug\\_\\***_\r\n\r\nIf I run \r\n`readelf -S bazel-bin/libtensorflow_framework.so.2.2.0`\r\nI don't see any lines that match the pattern _.debug\\_\\***_\r\n\r\nThis seems to confirm the findings in #27495.\r\n\r\nSo maybe the fact that I build the _libtensorflow_cc.so_ target after the _build_pip_package_ target is causing my debug symbols to be stripped when _libtensorflow_cc.so_ is linked?\r\n\r\nI just ran\r\n<bazel query 'kind(\"generated file\", deps(//tensorflow:libtensorflow_cc.so))' >\r\nto get a list of the complete set of source files required to build libtensorflow_cc.so\r\n\r\nCursorily,I don't see the files with the functions I'd like to debug on this list. So I'm going to assume that the symbols I want are in _libtensor_framework.so_\r\n\r\nI think I can confirm if the linker is stripping the debug symbols from libtensorflow_framework.so, as a result of building the libtensorflow_cc.so after build_pip_package. Let me try to change the ordering of the build and see if libtensor_framework.so has the debug symbols.\r\n\r\nIf there are any other suggestions. Please let me know ", "Hi all, I apologize for not investigating deeper (I'm on vacation and in the process of moving to a different city), so I'll only throw some breadcrumbs that might help.\r\n\r\nTo tell Bazel you want to build in the debug mode use [--compilation_mode dbg](https://docs.bazel.build/versions/master/command-line-reference.html#flag--compilation_mode) or `-c dbg`. \r\n\r\nIf you only want to include debug symbols for some files, use [--per_file_copt](https://docs.bazel.build/versions/master/command-line-reference.html#flag--per_file_copt).\r\n\r\nAnd for stripping, I'm not sure which C++ toolchain are you using, but the one built into Bazel will strip debug info [during linking](https://cs.opensource.google/bazel/bazel/+/master:tools/cpp/unix_cc_toolchain_config.bzl;l=711) depending on the value of the [--strip](https://docs.bazel.build/versions/master/command-line-reference.html#flag--strip). Put `never` there to be sure.", "hlopko,\r\nThanks for replying, especially since you are on vacation. \r\n\r\nI think from you post, I can see what I may have done incorrectly.\r\n\r\nOriginally I put the '--strip=never' after -c dbg, but got an error because of the ordering. \r\n\r\nI thought that by doing \r\n`bazel build --strip=never -c opt --config=opt --per_file_copt=//tensorflow/c/..cc,//tensorflow/cc/./..cc,//tensorflow/core/./.*.cc@-g //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nThat the '-g' flag would be applied to all files matching my regex\r\n\r\nBut seems I should have done\r\n`bazel build --strip=never -c dbg --per_file_copt=//tensorflow/c/..cc,//tensorflow/cc/./..cc,//tensorflow/core/./.*.cc //tensorflow/tools/pip_package:build_pip_package` \r\n\r\nI'll give this a try. I'm using bazel 2.0.0, that I built from source. My gcc is 8.3.1. \r\n\r\n", "OK\r\n\r\nThis doesn't work. \r\n`bazel build -c dbg --strip=never --per_file_copt=//tensorflow/c/.*\\.cc,//tensorflow/cc/saved_model/.*\\.cc,//tensorflow/core/framework/.*\\.cc,//tensorflow/cc/framework/.*\\.cc //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nUsing the command above for either the target `build_pip_package` or `libtensorflow_cc.so` , results in the following error:\r\n\r\n` external/aws-checksums/source/intel/crc32c_sse42_asm.c: In function 's_crc32c_sse42_clmul_256':\r\nexternal/aws-checksums/source/intel/crc32c_sse42_asm.c:61:5: error: 'asm' operand has impossible constraints\r\n     asm volatile(\"enter_256_%=:\"\r\n     ^~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build `\r\n\r\nThis is apparently issue #37498 \r\n\r\nI've combed over the internet for months trying to find a way to build with debug symbols for select files. I also posted a question on stack overflow months ago.  I haven't come across a solid answer yet. \r\n\r\nIt seems that @gunan's response is fairly common. Many people, Google employees included,  haven't built TF with debug symbols. \r\n\r\nI've only been developing software in industry for 10 years, but I write code every day that has mistakes. I can't imagine trying to debug my code without a debugger. In fact, the fancier the debugger, the more I like it. For a while my debugger of choice has been TotalView.  \r\n\r\nI know you guys are all top developers, but how do you debug code if you don't build it with debug symbols? This isn't meant to be condescending, I'm really just curious :) . \r\n\r\nIs the only way forward to use the work around suggested in that ticket?", "I would suggest you to use -c opt with the per file copt option.", "Can you please elaborate on how this is different from what I tried and listed as 'Variation 2' and 'Variation 3' in the original ticket submission? ", "```//tensorflow/c/..cc,//tensorflow/cc/./..cc,//tensorflow/core/./.*.cc```\r\n is not a valid path filter.\r\n\r\nSee https://docs.bazel.build/versions/master/user-manual.html#flag--per_file_copt for an example.\r\n\r\nPS: you could try ```//tensorflow/c/.*\\.cc,//tensorflow/cc/.*\\.cc,//tensorflow/core/.*\\.cc@-g,-O0```", "I'm sorry @byronyi,\r\n\r\nSomehow the path filter I posted in my original post got corrupted when I pasted it.\r\n\r\nI did have the `.` properly escaped in my bazel build command. This filter path I used is below, I got this from the  bazel user manual. I should have the proper level of recursion. \r\n`--per_file_copt=//tensorflow/c/.*\\.cc,//tensorflow/cc/.*/.*\\.cc,//tensorflow/core/.*/.*\\.cc`\r\n\r\nI'll try your path filter, but I'd think that at least `//tensorflow/c/.*\\.cc` should have produced debug symbols for the .cc files in the C API. Thanks", "@byronyi ,\r\n\r\nI used your filter path. The debugger definitely loaded some debug symbols, but there was just nothing but addresses when I stepped through my source.\r\n\r\nI do have a habit of changing my home directory temporarily when I do bazel build, to keep from filling up my home directory with its .cache (which is a far smaller size then the drive where I put the installs). Could this have anything to do with the observed behavior?\r\n\r\nI know there is an option to set the cache directory elsewhere, but I've never had luck with this. ", "It should work if you just use copt_per_file, without --strip=-never, using readElf -S to look for debug_info", "@makn87,\r\n\r\nCan you take a look at the above [comment](https://github.com/tensorflow/tensorflow/issues/40130#issuecomment-913197290) by @tanzhenyu and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40130\">No</a>\n"]}, {"number": 40129, "title": "TFLite Converter not quantizing supported op weights", "body": "**System information**\r\n-Mac OSX\r\n-TF 2.3.0-dev20200602\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n    # From Saved Model\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(curr_dir + \"saved_model\")\r\n    # converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                           tf.lite.OpsSet.SELECT_TF_OPS]\r\n    tflite_model = converter.convert()\r\n\r\n    # Save the TF Lite model.\r\n    with tf.io.gfile.GFile(curr_dir + '/model.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n```\r\n\r\n\r\nModel obtained from Model Zoo\r\nhttp://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\r\n\r\nFollow-up issue to : https://github.com/tensorflow/tensorflow/issues/40059\r\n\r\nRunning the above code to generate a tflite file from the pretrained ssdlite_mobilenet model. However I'm suspecting not a whole lot of optimization is happening. The file size for the frozen graph is 19911343 bytes and the optimized tflite model is 18905520 bytes. \r\n\r\nDissecting the model (using Netron), I can also see that the [supported ops](https://www.tensorflow.org/lite/guide/ops_compatibility) are still using float32 type weights. \r\n\r\nI was hoping to see 2-4x size reduction as a good chunk of the model is conv2d with float32 weights. Any ideas why I'm not able to really optimize this particular model? \r\n\r\nWant to also add:\r\n\r\nI modified my conversion code above to avoid use of the experimental flags:\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\nreplacing it with a stable-version documented configuration:\r\n```\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n```\r\n\r\nIn this case the conversion fails:\r\n```\r\nerror: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n        tf.NonMaxSuppressionV2 {T = f32, T_threshold = f32, device = \"\"}\r\n        tf.Size {device = \"\"}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<100>}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<100x4>}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<300x300x3>}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<3>}\r\n        tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<>}\r\n        tf.TensorArrayReadV3 {device = \"\"}\r\n        tf.TensorArrayScatterV3 {device = \"\"}\r\n        tf.TensorArraySizeV3 {device = \"\"}\r\n        tf.TensorArrayV3 {clear_after_read = true, device = \"\", dtype = f32, dynamic_size = false, element_shape = #tf.shape<*>, identical_element_shapes = false, tensor_array_name = \"\"}\r\n        tf.TensorArrayV3 {clear_after_read = true, device = \"\", dtype = i32, dynamic_size = false, element_shape = #tf.shape<*>, identical_element_shapes = false, tensor_array_name = \"\"}\r\n        tf.TensorArrayWriteV3 {device = \"\"}\r\n```", "comments": ["I can also see another ticket related to this that was closed\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/38104", "For anyone else following this thread:\r\nwas able to address this by having both `converter.optimizations` set **and** the experimental `converter.target_spec.supported_ops` defined to accept both ops. \r\n\r\nBoth alone won't do the trick. Without the converter.optimization flag it will simply convert the frozen graph to a float tflite file. But without enabling both tflite ops and tf native ops, more complex models cannot be converted. (This last feature to select op type is still experimental though)", "@aselva-eb Is this still an issue for you? \r\n\r\nI ran your code and I can see the reduction in file size (4 times with OPTIMIZE_DEFAULT for size) and 2 x smaller with FLOAT16 TF_Lite model. [Here](https://colab.research.google.com/gist/jvishnuvardhan/d89b000adca26c3eb684e0376c84010e/untitled1100.ipynb) is a gist for a reference. Thanks!\r\n\r\nThere are other models (such as full integer model) described in TF Lite tutorial https://www.tensorflow.org/lite/performance/post_training_quantization", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40129\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40129\">No</a>\n"]}, {"number": 40128, "title": "[INTEL MKL] Batch Matmul enhancements.", "body": "This PR removes CBLAS support for double and complex type. We still leave the support for float to enable broadcasting. For threadpool case we fall back to eigen when broadcasting is detected.", "comments": ["@penpornk I have made the changes requested. Please let me know.", "@Srini511 Can you please resolve conflicts? Thanks!", "@gbaned fixed it.", "@penpornk Quick question regarding build configuration. I recently observed that matmul decompositions for native tensorflow + mkldnn contraction kernels vary greatly depending on whether what --march option is specified. Is this meant to happen this way? Do you have  recommendation on what to use for performance comparison purposes?"]}, {"number": 40127, "title": "Hierarchical display for TensorBoard", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Not a frontend guy so I don't know if I can help.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm using TensorBoard to display an optimisation process with two stages, `lambda_sweep` and `counterfactual_search`, so I used the `name` argument to the summary ops to display the plots for the two stages separtely:\r\n\r\n![image](https://user-images.githubusercontent.com/30216068/83668596-1e61f580-a5c8-11ea-9fb8-5199c97b3012.png)\r\n\r\nI would like this behaviour to be hierarchical, that is, other drop-downs would appear when clicking one of the stages. Instead, I get the follwing:\r\n\r\n![image](https://user-images.githubusercontent.com/30216068/83669076-df806f80-a5c8-11ea-9828-866bcb4d2071.png)\r\n\r\nI was wondering if there is any way to extend the first level logic so that, for example, under `counterfactual_search` I get another page I can click on that is called `lambda`  and so on.\r\n\r\n\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\nI don't think so, it sounds more like a frontnend feature to me?\r\n**Who will benefit with this feature?**\r\nAll tensorboard users.\r\n\r\n**Any Other info.**\r\n", "comments": ["@alexcoca,\r\nSince TensorBoard issues are tracked in the TensorBoard repo. Could you please submit a feature request from [this link](https://github.com/tensorflow/tensorboard/issues/new/choose), so that we can track it there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> \r\n> \r\n> @alexcoca,\r\n> Since TensorBoard issues are tracked in the TensorBoard repo. Could you please submit a feature request from [this link](https://github.com/tensorflow/tensorboard/issues/new/choose), so that we can track it there. Thanks!\r\n\r\nHi @amahendrakar,\r\n\r\nThank you for getting back. I will request at the link indicated and close this issue!\r\n\r\nAlex"]}, {"number": 40126, "title": "Dynamic libtensorflow-lite for ARM64", "body": "I followed [cross-compile for ARM64](https://www.tensorflow.org/lite/guide/build_arm64#cross-compile_for_arm64) and produced a static library `libtensorflow-lite.a`\r\nIs it possible to cross-compile for ARM64 this library as dynamic (shared) *so* version for ARM64 as well? ", "comments": ["Could you try this?\r\n\r\n```\r\nbazel build -c opt --config=elinux_aarch64 tensorflow/lite:tensorflowlite\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40126\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40126\">No</a>\n"]}, {"number": 40124, "title": "Cross-compile the label_image with ARM64 libtensorflow-lite.a", "body": "I cross-compiled the TensorFlow Lite static library for ARM64-based computer and I'd like to build the label_image example for that target as well. To build label_image, it should run this command `bazel build tensorflow/examples/label_image/...` How should I set it to use that already built static library and cross-compile for ARM64-based target?\r\n\r\nThank you for your advice.", "comments": ["You can build label_image with Makefile.\r\nPlease check 56cea6e30cdec9a12832def2ad97634101d8c784\r\n```\r\n./tensorflow/lite/tools/make/build_aarch64_lib.sh label_image\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40124\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40124\">No</a>\n"]}, {"number": 40123, "title": "sess.run() in tensorflow2.1", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (pip source form Tsinghua mirror):\r\n- TensorFlow version (2.1):\r\n- Python version:3.6.7\r\n- GPU model and memory:without GPU\r\n\r\n\r\n**Describe the current behavior**\r\nwhen I used such code:\r\nwith tf.Graph().as_default() as graph:\r\n    features = input_fn(input_fn_params)\r\n    model_fn(features=features,\r\n             labels=None,\r\n             mode=tf.estimator.ModeKeys.PREDICT,\r\n             params=model_fn_params)\r\n\r\n    tf.train.Saver()\r\n    graph_def = graph.as_graph_def(add_shapes=True)\r\n    tf.train.write_graph(graph_def, directory, 'inference.pbtxt')\r\n    meta_graph_name = os.path.join(directory, 'inference.meta')\r\n    tf.train.export_meta_graph(filename=meta_graph_name)\r\n\r\nHow can I transform tensor to ndarray in  tf.Graph().as_default()\r\n\r\n\r\n", "comments": []}, {"number": 40122, "title": "Incorrect result of _MKLMaxPoolGrad", "body": "**System information**\r\n- OS Platform and Distribution: `Arch Linux 5.5.2-arch1-1  x86_64`\r\n- TensorFlow installed from: `source`\r\n- TensorFlow version: `v1.12.1-33097-g83eb4048ba 2.2.0` and `v2.2.0-0-g2b96f3662b 2.2.0`\r\n- Python version: `Python 3.6.10`\r\n- Bazel version: `3.0.0` for master, `2.0.0` for r2.2\r\n- GCC/Compiler version: `GCC 9.3.0`\r\n\r\nThe package was built with the commands:\r\n```bash\r\nbazel build --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n# For master (commit #83eb40)\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag ./master-83eb40\r\n# For r2.2\r\nbazel build --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Describe the current behavior**\r\nThe gradient of the max pooling 2D is wrong.\r\n\r\nCode:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_v2_behavior()\r\n\r\nx = np.array([\r\n    [3, 0, 0, 2, 3],\r\n    [0, 0, 0, 0, 1],\r\n    [0, 0, 0, 1, 3],\r\n    [0, 0, 0, 0, 0],\r\n    [1, 1, 3, 8, 6]\r\n]).astype(np.float32).reshape([1, 5, 5, 1])\r\n\r\nx_t = tf.compat.v1.placeholder(tf.float32, shape=[1, 5, 5, 1])\r\nw = np.array([1]).reshape([1, 1, 1, 1]).astype(np.float32)\r\nconv_t = tf.nn.conv2d(x_t, w, [1, 1, 1, 1], 'SAME')\r\npool_t = tf.nn.max_pool(conv_t, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')\r\ngrad_t = tf.gradients(ys=pool_t, xs=conv_t)\r\n\r\ntensors = [conv_t, pool_t, grad_t]\r\ntensors = [tf.squeeze(t, [-1]) for t in tensors]\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n    conv, pool, grad = sess.run(tensors, feed_dict={x_t: x})\r\n    print('conv\\n', conv, '\\npool\\n', pool, '\\ngrad\\n', grad)\r\n```\r\nOutput:\r\n```\r\nconv\r\n [[[3. 0. 0. 2. 3.]\r\n  [0. 0. 0. 0. 1.]\r\n  [0. 0. 0. 1. 3.]\r\n  [0. 0. 0. 0. 0.]\r\n  [1. 1. 3. 8. 6.]]]\r\npool\r\n [[[3. 2.]\r\n  [0. 1.]]]\r\ngrad\r\n [[[[1. 0. 1. 0. 0.]\r\n   [0. 0. 0. 0. 0.]\r\n   [1. 0. 1. 0. 0.]\r\n   [0. 0. 0. 0. 0.]\r\n   [0. 0. 0. 0. 0.]]]]\r\n```\r\n\r\n**Describe the expected behavior**\r\nIf we run the code with `TF_DISABLE_MKL=1`, the gradient will be\r\n```\r\n [[[[1. 0. 0. 1. 0.]\r\n   [0. 0. 0. 0. 0.]\r\n   [1. 0. 0. 1. 0.]\r\n   [0. 0. 0. 0. 0.]\r\n   [0. 0. 0. 0. 0.]]]]\r\n```\r\nNote that the positions of the second `1`'s in the first and the third rows are different.\r\n\r\n**Other info / logs**\r\nIf I directly feed the input to max pooling, the result is correct.\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_v2_behavior()\r\n\r\n\r\nx = np.array([\r\n    [3, 0, 0, 2, 3],\r\n    [0, 0, 0, 0, 1],\r\n    [0, 0, 0, 1, 3],\r\n    [0, 0, 0, 0, 0],\r\n    [1, 1, 3, 8, 6]\r\n]).astype(np.float32).reshape([1, 5, 5, 1])\r\n\r\nx_t = tf.compat.v1.placeholder(tf.float32, shape=[1, 5, 5, 1])\r\npool_t = tf.nn.max_pool(x_t, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')\r\ngrad_t = tf.gradients(ys=pool_t, xs=x_t)\r\n\r\ntensors = [x_t, pool_t, grad_t]\r\ntensors = [tf.squeeze(t, [-1]) for t in tensors]\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n    x, pool, grad = sess.run(tensors, feed_dict={x_t: x})\r\n    print('x\\n', x, '\\npool\\n', pool, '\\ngrad\\n', grad)\r\n```\r\nOutput:\r\n```\r\nx\r\n [[[3. 0. 0. 2. 3.]\r\n  [0. 0. 0. 0. 1.]\r\n  [0. 0. 0. 1. 3.]\r\n  [0. 0. 0. 0. 0.]\r\n  [1. 1. 3. 8. 6.]]]\r\npool\r\n [[[3. 2.]\r\n  [0. 1.]]]\r\ngrad\r\n [[[[1. 0. 0. 1. 0.]\r\n   [0. 0. 0. 0. 0.]\r\n   [1. 0. 0. 1. 0.]\r\n   [0. 0. 0. 0. 0.]\r\n   [0. 0. 0. 0. 0.]]]]\r\n```\r\nIn addition, if the I replace the conv2d with relu, which will be also rewritten, the result is also correct.\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_v2_behavior()\r\n\r\n\r\nx = np.array([\r\n    [3, 0, 0, 2, 3],\r\n    [0, 0, 0, 0, 1],\r\n    [0, 0, 0, 1, 3],\r\n    [0, 0, 0, 0, 0],\r\n    [1, 1, 3, 8, 6]\r\n]).astype(np.float32).reshape([1, 5, 5, 1])\r\n\r\nx_t = tf.compat.v1.placeholder(tf.float32, shape=[1, 5, 5, 1])\r\nrelu_t = tf.nn.relu(x_t)\r\npool_t = tf.nn.max_pool(relu_t, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')\r\ngrad_t = tf.gradients(ys=pool_t, xs=relu_t)\r\n\r\ntensors = [relu_t, pool_t, grad_t]\r\ntensors = [tf.squeeze(t, [-1]) for t in tensors]\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n    relu, pool, grad = sess.run(tensors, feed_dict={x_t: x})\r\n    print('relu\\n', relu, '\\npool\\n', pool, '\\ngrad\\n', grad)\r\n```\r\nOutput:\r\n```\r\nrelu\r\n [[[3. 0. 0. 2. 3.]\r\n  [0. 0. 0. 0. 1.]\r\n  [0. 0. 0. 1. 3.]\r\n  [0. 0. 0. 0. 0.]\r\n  [1. 1. 3. 8. 6.]]]\r\npool\r\n [[[3. 2.]\r\n  [0. 1.]]]\r\ngrad\r\n [[[[1. 0. 0. 1. 0.]\r\n   [0. 0. 0. 0. 0.]\r\n   [1. 0. 0. 1. 0.]\r\n   [0. 0. 0. 0. 0.]\r\n   [0. 0. 0. 0. 0.]]]]\r\n```\r\nI checked the log with `TF_CPP_MIN_VLOG_LEVEL=1` and confirmed that the OP was rewritten.\r\n\r\nIt seems that the result is affected by the convolution.", "comments": ["@djshen \r\n\r\nI have tried in colab with TF version 2.2 and i am not seeing any issue here.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b89f9c38e637d8507935e6aeb6fb8a88/untitled47.ipynb).Thanks!", "@ravikyram \r\n\r\nI think the official release is not compiled with MKL. This issue does not exist when I install TF from PyPI.\r\n\r\nI try to install TF from https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/\r\nand can reproduce the result.\r\nPlease find the [gist](https://colab.research.google.com/gist/djshen/2dde585e30b2da59342cb8e0a3de6c3b/untitled47.ipynb).", "we are looking at this issue and will respond once we reproduce", "@djshen We are checking this issue now. ", "@djshen @ravikyram @ymodak @pre\r\nI reproduce the issue in TF 2.2 (Installed by Conda: tensorflow  2.2.0   mkl_py36h5a57954_0).\r\nThere is no such problem in TF2.1.\r\n\r\nTF2.2 use DNNL 1.2.2 release.\r\nAfter check the DNNL log, it's the fault of DNNL (MKLDNN). \r\nI have reported the issue to Intel DNNL team as high level.\r\n\r\nHope get the confirm and fix as soon!", "DNNL team confirm it's not DNNL issue. I have reported to Intel Tensorflow team as high level.", "@penpornk Internally we are also working to solve this bug. Just a heads up. No milestone set for TF 2.3 yet. But making a mention for your internal tracking.\r\n", "@nammbash Got it. Thank you!", "@djshen \r\n\r\nThis issue is fixed in Tf 2.4.\r\n\r\nCould you check it?", "@NeoZhangJianyu \r\n\r\nI compiled TF v2.4.0-rc0 from source with `--config=mkl` and confirmed that the result is correct.", "@djshen \r\nIt's great news!\r\n\r\nCould you close this issue? :)", "@NeoZhangJianyu \r\nSure!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40122\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40122\">No</a>\n", "@djshen \r\n\r\nThank you very much!"]}, {"number": 40121, "title": "TFlite Converter Error: from tensorflow.lite.toco.python.toco_from_protos import main ModuleNotFoundError: No module named 'tensorflow'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (or github SHA if from source): tensorflow 2.2.0 \r\n- Python 3.7.6\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\nopen(\"model/train/converted_model.tflite\", \"wb\").write(tflite_quant_model)\r\nprint('tflite convert finish')\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTotal params: 3,084,997\r\nTrainable params: 3,084,997\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\nEpoch 1/10\r\n25/25 [==============================] - 2s 79ms/step - loss: 0.6899 - accuracy: 0.6162\r\nEpoch 2/10\r\n25/25 [==============================] - 2s 77ms/step - loss: 0.5860 - accuracy: 0.6862\r\nEpoch 3/10\r\n25/25 [==============================] - 2s 75ms/step - loss: 0.4454 - accuracy: 0.8050\r\nEpoch 4/10\r\n25/25 [==============================] - 2s 76ms/step - loss: 0.2366 - accuracy: 0.9038\r\nEpoch 5/10\r\n25/25 [==============================] - 2s 76ms/step - loss: 0.1573 - accuracy: 0.9400\r\nEpoch 6/10\r\n25/25 [==============================] - 2s 76ms/step - loss: 0.0459 - accuracy: 0.9875\r\nEpoch 7/10\r\n25/25 [==============================] - 2s 76ms/step - loss: 0.0103 - accuracy: 0.9975\r\nEpoch 8/10\r\n25/25 [==============================] - 2s 75ms/step - loss: 0.0634 - accuracy: 0.9725\r\nEpoch 9/10\r\n25/25 [==============================] - 2s 76ms/step - loss: 0.0796 - accuracy: 0.9712\r\nEpoch 10/10\r\n25/25 [==============================] - 2s 75ms/step - loss: 0.0063 - accuracy: 0.9975\r\nSaved trained model at ./model/train/sts_rmsprop_binary_crossentropy_epochs_10.h5 \r\n2020-06-03 20:42:32.237929: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-06-03 20:42:32.238000: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-03 20:42:32.249905: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-03 20:42:32.249928: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.005ms.\r\n2020-06-03 20:42:32.249932: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-03 20:42:32.472526: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-06-03 20:42:32.472630: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-03 20:42:32.594738: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-03 20:42:32.594761: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 66 nodes (-14), 66 edges (-16), time = 79.678ms.\r\n2020-06-03 20:42:32.594766: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 66 nodes (0), 66 edges (0), time = 12.165ms.\r\nTraceback (most recent call last):\r\n  File \"sts_v2.py\", line 117, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/Users/dingxirong/venv/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 518, in convert\r\n    **converter_kwargs)\r\n  File \"/Users/dingxirong/venv/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 496, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/Users/dingxirong/venv/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\nTraceback (most recent call last):\r\n  File \"/Users/dingxirong/venv/bin/toco_from_protos\", line 5, in <module>\r\n    from tensorflow.lite.toco.python.toco_from_protos import main\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n```", "comments": ["@rambowding,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and the model you are using, to reproduce the issue reported here. Thanks!", "> @rambowding,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code and the model you are using, to reproduce the issue reported here. Thanks!\r\n\r\n```\r\n# python 3.7.6, Tensorflow 2.2.0\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport data_preprocess\r\n\r\n# \u8d85\u53c2\u6570\r\nMAX_NB_WORDS = 200000\r\nMAX_SEQUENCE_LENGTH = 200\r\nVALIDATION_SPLIT = 0.2\r\nEMBEDDING_DIM = 300\r\n\r\nEPOCHS = 10\r\nOPT = 'rmsprop'\r\nLOSS = 'binary_crossentropy'\r\nMODEL_DIR = './model/train/'\r\nMODEL_FORMAT = '.h5'\r\nfilename_str = \"{}sts_{}_{}_epochs_{}{}\"\r\n# \u6a21\u578b\u6587\u4ef6\r\nMODEL_FILE = filename_str.format(MODEL_DIR, OPT, LOSS,  str(EPOCHS), MODEL_FORMAT)\r\n\r\nprint('Indexing word vectors.')\r\nembeddings_index = data_preprocess.build_word_embedding()\r\n\r\ntrain_data_file = 'data/train.txt'\r\ns1, s2, score = data_preprocess.read_train_data(train_data_file)\r\n# print('s1=', s1[0])\r\nleft, right, texts = data_preprocess.change_for_tokenizer(s1, s2)\r\n# print('left=', left[0])\r\n# print('texts=', texts[:2])\r\nprint('Found %s left.' % len(left))\r\nprint('Found %s right.' % len(right))\r\nprint('Found %s labels.' % len(score))\r\n\r\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS)\r\ntokenizer.fit_on_texts(texts)\r\n\r\nseq_left = tokenizer.texts_to_sequences(left)\r\n# print(seq_left)\r\nseq_right = tokenizer.texts_to_sequences(right)\r\nword_index = tokenizer.word_index\r\nprint('Found %s unique tokens.' % len(word_index))\r\n\r\ndata_left = tf.keras.preprocessing.sequence.pad_sequences(seq_left, maxlen=MAX_SEQUENCE_LENGTH, truncating='post')\r\ndata_right = tf.keras.preprocessing.sequence.pad_sequences(seq_right, maxlen=MAX_SEQUENCE_LENGTH, truncating='post')\r\nscore = np.array(score)\r\n# print(data_left)\r\n\r\nindices = np.arange(data_left.shape[0])\r\nnp.random.shuffle(indices)\r\ndata_left = data_left[indices]\r\ndata_right = data_right[indices]\r\nscore = score[indices]\r\n\r\nval_index = int(VALIDATION_SPLIT * data_left.shape[0])\r\ninput_train_left = data_left[:-val_index]\r\ninput_train_right = data_right[:-val_index]\r\nval_left = data_left[-val_index:]\r\nval_right = data_right[-val_index:]\r\ntrain_score = score[:-val_index]\r\nval_score = score[-val_index:]\r\n\r\nprint('Preparing embedding matrix.')\r\nnb_words = min(MAX_NB_WORDS, len(word_index))\r\nembedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\r\nfor word, i in word_index.items():\r\n    if i > MAX_NB_WORDS:\r\n        continue\r\n    embedding_vector = embeddings_index.get(word)\r\n    if embedding_vector is not None:\r\n        embedding_matrix[i] = embedding_vector\r\n\r\nprint('Training model.')\r\ntweet_a = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,))\r\ntweet_b = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,))\r\ntweet_input = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,))\r\n\r\nembedding_layer = tf.keras.layers.Embedding(nb_words + 1,\r\n                                            EMBEDDING_DIM,\r\n                                            input_length=MAX_SEQUENCE_LENGTH,\r\n                                            weights=[embedding_matrix],\r\n                                            trainable=True)(tweet_input)\r\nconv1 = tf.keras.layers.Conv1D(128, 3, activation='tanh')(embedding_layer)\r\ndrop1 = tf.keras.layers.Dropout(0.2)(conv1)\r\nmax1 = tf.keras.layers.MaxPooling1D(3)(drop1)\r\n# conv2 = tf.keras.layers.Conv1D(128, 3, activation='tanh')(max1)\r\n# drop2 = tf.keras.layers.Dropout(0.2)(conv2)\r\n# max2 = tf.keras.layers.MaxPooling1D(3)(drop2)\r\nout = tf.keras.layers.Flatten()(max1)\r\n\r\nmodel_encode = tf.keras.models.Model(tweet_input, out)\r\nencoded_a = model_encode(tweet_a)\r\nencoded_b = model_encode(tweet_b)\r\nmerged = tf.keras.layers.concatenate([encoded_a, encoded_b])\r\ndense1 = tf.keras.layers.Dense(128, activation='relu')(merged)\r\ndense2 = tf.keras.layers.Dense(128, activation='relu')(dense1)\r\ndense3 = tf.keras.layers.Dense(128, activation='relu')(dense2)\r\nprediction = tf.keras.layers.Dense(1, activation='sigmoid')(dense3)\r\nmodel = tf.keras.Model(inputs=[tweet_a, tweet_b], outputs=prediction)\r\nmodel.summary()\r\nmodel.compile(optimizer=OPT,\r\n              loss=LOSS,\r\n              metrics=['accuracy'])\r\n# \u8bad\u7ec3\u6a21\u578b\r\nmodel.fit([input_train_left, input_train_right], train_score, epochs=EPOCHS)\r\n\r\n# \u4fdd\u5b58\u6a21\u578b\r\nif not tf.io.gfile.exists(MODEL_DIR):\r\n    tf.io.gfile.makedirs(MODEL_DIR)\r\n\r\nmodel.save(MODEL_FILE)\r\nprint('Saved trained model at %s ' % MODEL_FILE)\r\n\r\n# \u6a21\u578b\u8f6c\u6362\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n# converter.experimental_new_converter = True\r\ntflite_quant_model = converter.convert()\r\nopen(\"model/train/converted_model.tflite\", \"wb\").write(tflite_quant_model)\r\nprint('tflite convert finish')\r\n\r\n# \u9884\u6d4b\u6d4b\u8bd5\u96c6\r\nloss_and_accuracy = model.evaluate([val_left, val_right], val_score)\r\nprint(\"Test Loss: {}\".format(loss_and_accuracy[0]))\r\nprint(\"Test Accuracy: {}%\".format(loss_and_accuracy[1]*100))\r\n\r\n```", "@rambowding,\r\nLooks like you are using custom Python modules in your script. On running the code I'm facing an error stating `ModuleNotFoundError: No module named 'data_preprocess'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/aa977cef13474f90a899aac99f319d86/40121.ipynb). \r\n\r\nCould you please share the complete code and all the supporting files you are using in the code. Thanks!", "@amahendrakar \r\nthe 'data_preprocess.py' file is shown in below, other modules are from 'pip install'\r\n```\r\n# \u6570\u636e\u9884\u5904\u7406\r\nimport word2vec\r\nimport pandas as pd\r\nimport numpy as np\r\nimport jieba\r\n\r\n# \u8bcd\u5411\u91cf\u6587\u4ef6\u8def\u5f84\r\nword_vector_file_path = 'data/sgns.weibo.word.txt'\r\n\r\n\r\ndef build_word_embedding():\r\n    embeddings_index = {}\r\n    f = open(word_vector_file_path)\r\n    for line in f:\r\n        values = line.split()\r\n        word = values[0]\r\n        vector = np.asarray(values[1:], dtype='float32')\r\n        embeddings_index[word] = vector\r\n    f.close()\r\n    print('Found %s word vectors.' % len(embeddings_index))  # embeddings_index type=dict\r\n    return embeddings_index\r\n\r\n\r\ndef build_word_vectors():\r\n    wv = word2vec.load(word_vector_file_path)\r\n    vocab = wv.vocab\r\n    vectors = wv.vectors\r\n    print('build_word_dic fun vocab len=', len(vocab))\r\n    # print(vocab)\r\n    # print(word_embedding)\r\n    return vocab, vectors\r\n\r\n\r\ndef read_train_data(train_file):\r\n    df_train = pd.read_csv(train_file, sep='\\t', usecols=[0, 1, 2], names=['s1', 's2', 'score'],\r\n                           dtype={'s1': object, 's2': object, 'score': object})\r\n    s1 = df_train.s1.values\r\n    s2 = df_train.s2.values\r\n    score = np.asarray(df_train.score.values, dtype=np.float32)\r\n    print('train data len=', len(score))\r\n    return s1, s2, score\r\n\r\n\r\ndef change_for_tokenizer(s1, s2):\r\n    size = len(s1)\r\n    texts = []\r\n    left = []\r\n    right = []\r\n    for i in range(size):\r\n        seg_left = jieba.lcut(s1[i])\r\n        seg_right = jieba.lcut(s2[i])\r\n        text_left = (' '.join(seg_left))  # .encode('utf-8', 'ignore').strip()\r\n        text_right = (' '.join(seg_right))  # .encode('utf-8', 'ignore').strip()\r\n\r\n        texts.append(text_left)\r\n        texts.append(text_right)\r\n\r\n        left.append(text_left)\r\n        right.append(text_right)\r\n    return left, right, texts\r\n\r\n```", "@rambowding,\r\nCould you please share `train.txt`, `sgns.weibo.word.txt` and any other supporting files you are using in your code. Thanks!", "> @rambowding,\r\n> Could you please share `train.txt`, `sgns.weibo.word.txt` and any other supporting files you are using in your code. Thanks!\r\n\r\nI hope that my data files are not public on github or web\uff0ccould I send the file's download url to you via email? ", "> I hope that my data files are not public on github or web\uff0ccould I send the file's download url to you via email?\r\n\r\n@rambowding,\r\nPlease send the supporting files to amahendrakar@google.com. Thanks!", "> @rambowding,\r\n> Please send the supporting files to [amahendrakar@google.com](mailto:amahendrakar@google.com). Thanks!\r\n\r\n@amahendrakar \r\ndone!", "@rambowding,\r\nDue to network restrictions I am unable to access the link you have sent. Could you please try to send the files in another way.  Thanks!", "> @rambowding,\r\n> Due to network restrictions I am unable to access the link you have sent. Could you please try to send the files in another way. Thanks!\r\n\r\nshared with you via Google Drive", "@rambowding,\r\nI was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/81358916b6a1c56eda894a25aa9888a4/40121.ipynb).\r\n\r\nCould you please run the code in a virtual environment and check if you are facing the same issue. Thanks!", "@amahendrakar \r\nIt also works in my jupyter environment,  it is way that I convert tf model to tflite model temporarily. The issue happens in venv environment, I am sorry for not pointing it out definitely.", ">The issue happens in venv environment\r\n\r\n@rambowding,\r\nCould you please check the TensorFlow version in the virtual environment? Make sure that you are using TensorFlow v2.2 in the virtual environment too. Thanks!", "@amahendrakar \r\npython 3.7.6, Tensorflow 2.2.0\r\n![image](https://user-images.githubusercontent.com/7675778/85551732-dd07b780-b654-11ea-8c5d-ccb91fdab717.png)\r\n\r\n\r\n", "Closing this issue since it is not reproducible. Please try using latest tensorflow version and try again. Thanks!"]}, {"number": 40120, "title": "[MLIR] Fix hlo to lhlo conversion amid std.constant ops", "body": "Add a conversion pattern to convert std.constant to xla_lhlo.const - to\r\nrun during HLO to LHLO conversion. Without such a pattern, xla_hlo to\r\nxla_lhlo conversion would fail when std.constant ops provide constant tensor\r\noperands to other xla_hlo ops. The tf-mlir-translate -hlo-to-mlir-hlo\r\nfor example generates such std.constant ops from HLO constant nodes.\r\n\r\nstd.constant tensor generating ops are to be replaced by constant memref\r\ngenerating ops.\r\n\r\nFixes #39895.\r\n\r\nSigned-off-by: Uday Bondhugula <uday@polymagelabs.com>", "comments": ["@MaheshRavishankar @sherhut @pifon2a for visibility.", "(discussion is going on in the tracking bug  #39895)", "@bondhugula, @joker-eph This PR is in draft, any update on this? Please. Thanks!", "@bondhugula, @joker-eph This PR is in draft, any update on this? Please. Thanks!", "> @bondhugula, @joker-eph This PR is in draft, any update on this? Please. Thanks!\r\n\r\nI just posted on the other issue thread #39895 to resurrect discussion.", "> > @bondhugula, @joker-eph This PR is in draft, any update on this? Please. Thanks!\r\n> \r\n> I just posted on the other issue thread #39895 to resurrect discussion.\r\n\r\n@bondhugula  Any update on this PR? Please. Thanks!", "There isn't a clear solution to this issue and this has become moot / low priority - given that there are ways to avoid it trivially. I'm closing this. The bug it was trying to fix should remain open however - the behavior / error isn't proper."]}, {"number": 40119, "title": "RuntimeError: Encountered unresolved custom op: RandomStandardNormal.Node number 1 (RandomStandardNormal) failed to prepare", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.1.0\r\n\r\n**Output from the converter and interpreter invocations**\r\n```\r\n2020-06-03 10:23:11.668039: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-06-03 10:23:11.668089: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2020-06-03 10:23:11.668114: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-06-03 10:23:11.850912: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-06-03 10:23:11.851102: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-03 10:23:11.856571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:09:00.0 name: Tesla K80 computeCapability: 3.7\r\ncoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\r\n2020-06-03 10:23:11.856639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-03 10:23:11.856672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-03 10:23:11.856706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-03 10:23:11.856737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-03 10:23:11.856768: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-03 10:23:11.856808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-03 10:23:11.856835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-03 10:23:11.865046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-06-03 10:23:11.865092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-03 10:23:11.865108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-06-03 10:23:11.865121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-06-03 10:23:11.916336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10770 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:09:00.0, compute capability: 3.7)\r\n2020-06-03 10:23:11.936932: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-06-03 10:23:11.936982: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 70 nodes (-33), 73 edges (-37), time = 7.528ms.\r\n2020-06-03 10:23:11.936998: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 70 nodes (0), 73 edges (0), time = 2.65ms.\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 30, in <module>\r\n    tflite_interpreter.allocate_tensors()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter.py\", line 247, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 110, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: Encountered unresolved custom op: RandomStandardNormal.Node number 1 (RandomStandardNormal) failed to prepare.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nfrom tensorflow_probability import distributions as tfd\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\nx_train = x_train.astype('float32')/255.\r\nx_test = x_test.astype('float32')/255.\r\n\r\nkl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /\r\n                          tf.cast(x_train.shape[0], dtype=tf.float32))\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Flatten(),\r\n    tfp.layers.DenseFlipout(\r\n        10, kernel_divergence_fn=kl_divergence_function,\r\n        activation=tf.nn.softmax\r\n    ),\r\n])\r\n\r\noptimizer = tf.keras.optimizers.Adam(lr=0.001)\r\nmodel.compile(optimizer, loss='sparse_categorical_crossentropy')\r\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3)\r\n\r\ntflite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_converter.allow_custom_ops = True\r\ntflite_model = tflite_converter.convert()\r\n\r\ntflite_interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ntflite_interpreter.allocate_tensors()\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/4722692/model.zip)\r\n\r\n**Issue details**\r\n\r\nI have been trying to convert a keras/saved_model into a tflite model. While the converter is not producing errors, suggesting that the conversion is succesful, when the interpreter is invoked I get the following:\r\n`RuntimeError: Encountered unresolved custom op: RandomStandardNormal.Node number 1 (RandomStandardNormal) failed to prepare.`\r\n\r\nI have attached a simple script to reproduce the error, as well as the saved_model files.\r\n\r\n", "comments": ["I have tried in colab with TF version 2.2, nightly version (`2.3.0-dev20200603`) and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/6d426fd9b63e75b39227f7410577155f/untitled950.ipynb)Thanks!", "TFLite doesn't support 'RandomStandardNormal' op. The reason the conversion didn't fail is because you set 'allow_custom_ops' to True, which will export unknown operations as custom ops. \r\nSee 'allow_custom_ops' attribute in the docs\r\nhttps://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter\r\n\r\nClosing as this is expected and explained it above.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40119\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40119\">No</a>\n", "One more note, you can still be able to convert and run the model using SELECT but it has an impact on binary size.\r\nSee\r\nhttps://www.tensorflow.org/lite/guide/ops_select"]}, {"number": 40118, "title": "Add shape and type check for IteratorGetNextOp and ToSingleElementOp", "body": "This pr adds the same shape and type check to IteratorGetNextOp and ToSingleElementOp as IteratorGetNextAsOptionalOp.\r\n\r\nThank you for your time on reviewing this pr.", "comments": ["@jsimsa \r\nI've changed the two functions according to your nice suggestion. Could you have a second look?"]}, {"number": 40117, "title": "'os' has no attribute 'errno' when using keras.utils.plot_model", "body": "**System information**\r\n- Tensorflow install via pip\r\n- `pydot` version 1.2.3\r\n- `tensorflow` version 2.2.0\r\n- `python` 3.7.7\r\n\r\n**Error Details**\r\nWhen running `tf.keras.utils.plot_model` an error is raised:\r\n\r\n```\r\n~\\Anaconda3\\lib\\site-packages\\pydot.py in create(self, prog, format)\r\n   1878                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\r\n   1879         except OSError as e:\r\n-> 1880             if e.errno == os.errno.ENOENT:\r\n   1881                 raise Exception(\r\n   1882                     '\"{prog}\" not found in path.'.format(\r\n\r\nAttributeError: module 'os' has no attribute 'errno'\r\n```\r\n\r\nI understand that this error is a problem with `pydot`, but perhaps a `pydot` version bump would fix this problem (most recent `pydot` version is 1.4.*).\r\n\r\n**Code to Reproduce**\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.Input(shape=(512, 512, 1)),\r\n    tf.keras.layers.Conv2D(3, 3),\r\n    tf.keras.layers.MaxPool2D(),\r\n])\r\n\r\ntf.keras.utils.plot_model(model)\r\n```", "comments": ["@celsomilne,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "@amahendrakar\r\nSorry - updated the issue now. ", "@celsomilne,\r\nI was able to run the code with TF v2.2 without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7d15484807d471f73abece2a9f0a220e/40117.ipynb). \r\n\r\nCould you please update TensorFlow to v2.2 and let us know if it works. Thanks!", "@amahendrakar \r\nThanks! I updated to TF v2.2, and downgraded `pydot` to 1.3.0 and `graphviz` to 0.10.1 (as it appears in your Collab) but I'm now getting the (rather unhelpful) message:\r\n\r\n`Failed to import pydot. You must install pydot and graphviz for 'pydotprint' to work.`\r\n\r\nI've also tried running tensorflow from both windows and a linux VM, with no difference in outcome. I've updated the question to reflect TF 2.2\r\n\r\nUnfortunately, I can't use Collab for the analysis I'm doing. ", "> `Failed to import pydot. You must install pydot and graphviz for 'pydotprint' to work.`\r\n\r\n@celsomilne,\r\nRegarding this error, please take a look at [this](https://github.com/XifengGuo/CapsNet-Keras/issues/25) similar issue and let us know if it helps. Thanks!", "Thanks @amahendrakar. After reading through the issue, I fixed my problem with \r\n```\r\nsudo apt-get update\r\nsudo apt-get install graphviz\r\n```\r\nI feel that the documentation for `plot_model` should mention the fact the `graphviz` needs to be installed separately (especially for inexperienced users who'd simply assume the `pip install graphviz` is enough). \r\n\r\nShould TF installation also explicitly require `pydot==1.3.0`?\r\n\r\nWhat do you think?", "> I feel that the documentation for `plot_model` should mention the fact the `graphviz` needs to be installed separately (especially for inexperienced users who'd simply assume the `pip install graphviz` is enough).\r\n\r\n@celsomilne,\r\nIf we take a look at the [source code](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/utils/vis_utils.py#L28) for `tf.keras.utils.plot_model`, it uses `pydot_ng` or `pydotplus` and if both the packages are not found it then falls back to `pydot`.\r\n\r\nIf none of these packages are found (including `graphviz`), it throws an [ImportError](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/utils/vis_utils.py#L110) stating \r\n`Failed to import pydot. You must install pydot and graphviz for pydotprint to work.`\r\n\r\nHope this clears things up, please feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40117\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40117\">No</a>\n"]}]