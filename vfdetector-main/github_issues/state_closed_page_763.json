[{"number": 30667, "title": "Add kZlib to compression namespace", "body": "This PR adds the missing `kZlib` to the compression namespace so that the related files can reuse this named constant like [kNone](https://github.com/tensorflow/tensorflow/pull/30667/files#diff-88365edc1cef483dfe0eb2946fd3c5ceL50) and [kGzip](https://github.com/tensorflow/tensorflow/pull/30667/files#diff-88365edc1cef483dfe0eb2946fd3c5ceR42).", "comments": ["Can one of the admins verify this patch?", "@feihugis can you please resolve conflicts so that we can review ?", "Just come back from vacation. Will do it soon.", "@mihaimaruseac Thanks for reviewing! I rebased this PR and resolved the conflicts. Could you please take another look?"]}, {"number": 30666, "title": "tf.keras.layers.Conv2D does not initialize kernel and bias when called inside name_scope", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: Python 2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: No GPU\r\n\r\n**Describe the current behavior**\r\n\r\nIt throws this\r\nFailedPreconditionError: Error while reading resource variable name9/conv_linear/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/gcnn2d_d1/conv_linear/bias)\r\n\r\nwhen trying to run the graph with a feed dict to get variable value\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nExpect to see a normal run since the variables are initialized inside the Conv2D function\r\n\r\n**Code to reproduce the issue**\r\n\r\ngraph = tf.Graph()\r\nwith graph.name_scope('name9'):\r\n  with graph.as_default():\r\n    sign_in = tf.placeholder(tf.float32,(data_shape[0],data_shape[1],data_shape[2],data_shape[3]), name='signal_in')\r\n\r\n    conv = tf.keras.layers.Conv2D(  10, (10,2), padding='valid', name='conv_linear', use_bias=True,  kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137)  )(sign_in)\r\n\r\ndata_tensor = np.random.rand(10,40,2,1)\r\nfeed_dict = {\r\n  graph.get_tensor_by_name('signal_in:0'):data_tensor\r\n}\r\n\r\nop_value = session.run('/name9/conv_linear:0', feed_dict=feed_dict)  \r\n \r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Update, just corrected it by using\r\n``\r\nwith graph.as_default():\r\n  with tf.variable_scope('name9'):\r\n``\r\nIn relation to this one has to say \" never use name_scope to define ops, unless you wanna recover the name_scope of the graph\".\r\n\r\nand then before running the Op in question\r\n\r\n`` \r\ntf.global_variables_initializer()\r\nsess.run(tf.get_operations()[-1])\r\n``\r\n\r\nbut have to do and silly if condition to see if any init Op was not added. \r\n\r\nMy question relies, why add many Initializers on Variables such Convolutions Variables Scopes and then only initialize them in the run ? Why not immediatly initialize it ? \r\n\r\nWhat do you think ? \r\n", "@Uiuran ,\r\nWe tried executing the code given by you and we are encountering the error mentioned below\r\n\r\n`KeyError: \"The name 'signal_in:0' refers to a Tensor which does not exist. The operation, 'signal_in', does not exist in the graph.\"`", "```python\r\ngraph = tf.Graph()\r\ndata_shape=(10,40,2,1)\r\n\r\ntf.reset_default_graph()\r\nwith graph.as_default():\r\n  with tf.variable_scope('name9'):  \r\n    sign_in = tf.placeholder(tf.float32,(data_shape[0],data_shape[1],data_shape[2],data_shape[3]), name='signal_in')\r\n    conv = tf.keras.layers.Conv2D(  10, (10,2), padding='valid', name='conv_linear', use_bias=True,  kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137)  )(sign_in)\r\n\r\ndata_tensor = np.random.rand(10,40,2,1)\r\n    \r\nfeed_dict = {\r\n  graph.get_tensor_by_name('name9/signal_in:0'):data_tensor\r\n}\r\n\r\nsession = tf.Session(graph=graph)\r\nop_value = session.run('name9/conv_linear/BiasAdd:0', feed_dict=feed_dict)\r\n```\r\n\r\nalternatively you could do graph.name_scope('name9')\r\n\r\nHowever are still required to tf.global_variables_initializer() on the run function (the custom one i made)\r\nWhat does not make sense, what makes sense is the initializator to be added programmatically as you add the variables op in the layers, then just get them and run in a shot.", "maybe this is just a more aesthetical question, either to add the op programmatically or add all in once, like the changes in the 2.0 version. However it seems to me that it does change the inner workings of tensorflow in a case of serving models to learn in clusters.", "I was able to replicate the issue with TF version 1.14.Thanks!", "It looks like you are mixing low-level TF APIs with Keras, which can yield strange behavior. When using Keras, tf.keras.Input is preferable to tf.placeholder, and model.predict is preferable to trying to run individual outputs. You can see some examples [here](https://www.tensorflow.org/guide/keras).", "however the conv implementations force us to use keras, what should be only\na short to a name-space with all ops to do convolution. Also, why input is\nsupposed to have a different behavior than tf.placeholder ?\n\nKeras, supposing to make the things easier, changes the behavior that one\nis waiting from the low level API and make things lot harder to debug.\n\nOn Thu, Jul 18, 2019 at 3:01 PM Karmel Allison <notifications@github.com>\nwrote:\n\n> It looks like you are mixing low-level TF APIs with Keras, which can yield\n> strange behavior. When using Keras, tf.keras.Input is preferable to\n> tf.placeholder, and model.predict is preferable to trying to run individual\n> outputs. You can see some examples here\n> <https://www.tensorflow.org/guide/keras>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30666?email_source=notifications&email_token=AAOOCXZHNTWSSHP7SNMHE4DQACVXVA5CNFSM4ICSCNQ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2JI7GQ#issuecomment-512921498>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOOCX7QNAH4SZV2BLHNCVDQACVXVANCNFSM4ICSCNQQ>\n> .\n>\n", "I think that gives me a better sense for what you are trying to do, and I would highly recommend trying out the [TF 2.0 beta](https://www.tensorflow.org/beta), in which you can just pass the np array directly through the conv layer:\r\n\r\n```\r\ndata = np.random.rand(10,40,2,1)\r\nconv = tf.keras.layers.Conv2D(  10, (10,2), padding='valid', name='conv_linear', use_bias=True,  kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137))\r\nconv(data)\r\n```", "Agree with you, TF 2.0 beta seems to be beautifull and intuitive to use. But i would like to know how much they changed the API and the Backend. \r\n\r\nHowever to me seems that \" , either to add the op programmatically or add all in once \" affects the behaviour in the case of distributed computing, what make of it a bug or an interface feature request, am i missing anything ?", "@Uiuran Please check [`effective_tf2`](https://www.tensorflow.org/beta/guide/effective_tf2), and [`release_notes`](https://github.com/tensorflow/tensorflow/releases) to know how much was changed in 2.0. There are lots of other resources too including [`tutorials`](https://www.tensorflow.org/tutorials).\r\n\r\nPlease let us know whether we can close the issue. Thanks!", "That's fine for now, i will have look on 2.0beta, the fact is i have a lot of code and in moment cant stop sprinting to migrate or to learn the changes, but will do it asap. \r\n\r\nThank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30666\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30666\">No</a>\n"]}, {"number": 30665, "title": "Non-OK-status: CudaLaunchKernel Internal: invalid configuration argument", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.2, V9.2.148\r\n- GPU model and memory: two gpus, GTX 1080 Ti, each 11178MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am trying to run the follow sample code(keras tensorflow backend) my my GPU\r\n\r\n```\r\nleft = Input(shape=(128, 3072), dtype='float32', name='Input-Left')\r\nright = Input(shape=(128, 3072), dtype='float32', name='Input-Right')\r\nlstm = Bidirectional(LSTM(units=768,\r\n                          activation='tanh'),\r\n                      name='Bidirectional-LSTM')\r\nl_lstm = lstm(left)\r\nr_lstm = lstm(right)\r\nsubtracted = Subtract(name='Subtract')([l_lstm, r_lstm])\r\nabs_subtracted = Lambda(function=backend.abs)(subtracted)\r\nmul = Multiply(name='multiplication')([l_lstm, r_lstm])\r\nconcat = concatenate([abs_subtracted, mul])\r\noutput = Dense(units=1)(concat)\r\nmodel = Model(inputs=[left, right],\r\n              outputs=output)\r\nmodel = multi_gpu_model(model, gpus=2)\r\nmodel.compile(loss='mean_squared_error',\r\n              optimizer='Adam',\r\n              metrics=['acc'])\r\nimport numpy as np\r\nx1 = np.random.rand(100, 128, 3072)\r\nx2 = np.random.rand(100, 128, 3072)\r\ny = np.random.rand(100)\r\nmodel.fit(x = [x1, x2],\r\n         y=y,\r\n         epochs=10)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nexpected it run normally without error.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nWhen I ran the code on my GPUs, I got the following error, However, if I uninstall tensorflow-gpu, there was no error, but I can only use cpu.\r\n\r\n```\r\nUsing TensorFlow backend.\r\n2019-07-12 15:38:28,543 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\n2019-07-12 15:38:28,552 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\n2019-07-12 15:38:28,552 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\r\n\r\n2019-07-12 15:38:29,759 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\r\n\r\n2019-07-12 15:38:29,837 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\r\n\r\n2019-07-12 15:38:29,840 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n2019-07-12 15:38:29.856975: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-12 15:38:29.860863: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-07-12 15:38:29.974060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:29.976417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:29.977100: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x97d2a40 executing computations on platform CUDA. Devices:\r\n2019-07-12 15:38:29.977112: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-07-12 15:38:29.977117: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-07-12 15:38:29.978574: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696330000 Hz\r\n2019-07-12 15:38:29.979175: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x984ed80 executing computations on platform Host. Devices:\r\n2019-07-12 15:38:29.979185: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-12 15:38:29.980287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:29.980881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\n2019-07-12 15:38:29.980907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:29.981642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:02:00.0\r\n2019-07-12 15:38:29.981842: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-12 15:38:29.982531: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-12 15:38:29.983173: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-07-12 15:38:29.983363: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-07-12 15:38:29.984149: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-07-12 15:38:29.984728: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-07-12 15:38:29.985920: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-12 15:38:29.985961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:29.986728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:29.987487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:29.988224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:29.988948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1\r\n2019-07-12 15:38:33.252621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-12 15:38:33.252666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 \r\n2019-07-12 15:38:33.252672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y \r\n2019-07-12 15:38:33.252677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N \r\n2019-07-12 15:38:33.253003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:33.253769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:33.254223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:33.254958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10054 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-07-12 15:38:33.255392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-07-12 15:38:33.255840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10054 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2019-07-12 15:38:33.714112: F ./tensorflow/core/kernels/random_op_gpu.h:227] Non-OK-status: CudaLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: invalid configuration argument\r\nAborted (core dumped)\r\n```", "comments": ["I tried reproducing issue with the provided code ,but i am unable to reproduce .Will it be possible to provide minimal code snippet which can reproduce the reported issue.Thanks", "@ravikyram Thanks. I was able to solve the issue. I think it is compatibility issue between tensorflow-gpu and cuda.\r\n\r\n- CUDA 10.1 and Tensorflow-gpu 1.14: No Error\r\n- CUDA 9.2 and Tensorflow-gpu 1.14: CudaLaunchKernel error.\r\n- CUDA 9.2 and Tensorflow-gpu 1.12: No Error.", "@xinsu626 \r\nI am closing this issue as it was resolved.Thanks!", "Sorry for commenting on a closed issue, but reg.\r\n\r\n> CUDA 10.1 and Tensorflow-gpu 1.14: No Error\r\n\r\nDid you compile 1.14 manually against CUDA 10.1? As per the documentation and from experience\r\n\r\n```\r\n2019-07-19 21:26:32.698745: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 21:26:32.698819: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 21:26:32.698888: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 21:26:32.698928: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 21:26:32.698996: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 21:26:32.699039: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 21:26:32.701819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-19 21:26:32.701833: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n\r\nneither 1.14 nor the nightly, nor the nightly-tf2-preview, work with CUDA 10.1...\r\n\r\n@ravikram could you please reopen this?\r\n\r\nMany thanks!", "@skeydan Hi, \r\n\r\nSorry I just checked my server and the CUDA version is 10.0(`Cuda compilation tools, release 10.0, V10.0.130`) instead of 10.1.\r\n\r\nI didn't compile 1.14 manually. I used pip to install it. `python3.6 -m pip install tensorflow-gpu`. I also ran the code below with CUDA 10.0 and tensorflow 1.14, and there was no error.\r\n\r\n```\r\nimport numpy as np\r\nfrom keras.layers import Input, Bidirectional, LSTM, Subtract, Lambda, Multiply, concatenate, Dense\r\nfrom keras.utils import multi_gpu_model\r\nfrom keras.models import Model\r\nimport keras.backend as backend\r\n\r\nx1 = np.random.rand(100, 128, 3072)\r\nx2 = np.random.rand(100, 128, 3072)\r\ny = np.random.rand(100)\r\n\r\nleft = Input(shape=(128, 3072), dtype='float32', name='Input-Left')\r\nright = Input(shape=(128, 3072), dtype='float32', name='Input-Right')\r\nlstm = Bidirectional(LSTM(units=768,\r\n                          activation='tanh'),\r\n                     name='Bidirectional-LSTM')\r\nl_lstm = lstm(left)\r\nr_lstm = lstm(right)\r\nsubtracted = Subtract(name='Subtract')([l_lstm, r_lstm])\r\nabs_subtracted = Lambda(function=backend.abs)(subtracted)\r\nmul = Multiply(name='multiplication')([l_lstm, r_lstm])\r\nconcat = concatenate([abs_subtracted, mul])\r\noutput = Dense(units=1)(concat)\r\nmodel = Model(inputs=[left, right],\r\n              outputs=output)\r\nmodel = multi_gpu_model(model, gpus=2)\r\nmodel.compile(loss='mean_squared_error',\r\n              optimizer='Adam',\r\n              metrics=['acc'])\r\nmodel.fit(x=[x1, x2],\r\n          y=y,\r\n          epochs=10)\r\n```\r\n \r\n\r\n\r\n\r\n", "@skeydan \r\nCan you please go through the comments of xinsu626 and let us know still if you want us to reopen this issue. Thanks!", "@ravikyram thanks for asking, @xinsu626 thanks for clarifying!\r\n\r\nWe have a similar-looking issue here https://github.com/rstudio/keras/issues/843 and I'm now passing on the recommendation to use TF 1.14 with CUDA 10.0 to evade the problem. \r\n\r\nIf  it's okay with you, I might ask to reopen at a later time in case that issue persists even in  the recommended configuration. Thanks!", "Had the same issue with 4 workers, each with 2 Tesla K80s, on the AI Platform. Not sure whether this is a CUDA problem, since it is not reproducible: launching another job worked well. ", "> _\u8bf7\u786e\u4fdd\u8fd9\u662f\u4e00\u4e2a\u9519\u8bef\u3002\u6839\u636e\u6211\u4eec\u7684[GitHub\u653f\u7b56](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md)\uff0c\u6211\u4eec\u4ec5\u89e3\u51b3[GitHub\u4e0a\u7684](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md)\u4ee3\u7801/\u6587\u6863\u9519\u8bef\uff0c\u6027\u80fd\u95ee\u9898\uff0c\u529f\u80fd\u8bf7\u6c42\u548c\u6784\u5efa/\u5b89\u88c5\u95ee\u9898\u3002\u6807\u8bb0\uff1abug_template_\r\n> \r\n> **\u7cfb\u7edf\u4fe1\u606f**\r\n> \r\n> * \u6211\u662f\u5426\u7f16\u5199\u4e86\u81ea\u5b9a\u4e49\u4ee3\u7801\uff08\u4e0e\u4f7f\u7528TensorFlow\u63d0\u4f9b\u7684\u80a1\u7968\u793a\u4f8b\u811a\u672c\u76f8\u53cd\uff09\uff1a\r\n> * \u64cd\u4f5c\u7cfb\u7edf\u5e73\u53f0\u548c\u53d1\u884c\u7248\uff08\u4f8b\u5982\uff0cLinux Ubuntu 16.04\uff09\uff1aLinux Ubuntu 16.04\r\n> * \u5982\u679c\u95ee\u9898\u53d1\u751f\u5728\u884c\u52a8\u88c5\u7f6e\u4e0a\uff0c\u5219\u884c\u52a8\u88c5\u7f6e\uff08\u4f8b\u5982iPhone 8\uff0cPixel 2\uff0cSamsung Galaxy\uff09\uff1a\r\n> * \u4ece\uff08\u6e90\u6216\u4e8c\u8fdb\u5236\uff09\u5b89\u88c5TensorFlow\uff1a\u4e8c\u8fdb\u5236\r\n> * TensorFlow\u7248\u672c\uff08\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\uff09\uff1a1.14.0\r\n> * Python\u7248\u672c\uff1a3.6\r\n> * Bazel\u7248\u672c\uff08\u5982\u679c\u4ece\u6e90\u4ee3\u7801\u7f16\u8bd1\uff09\uff1a\r\n> * GCC /\u7f16\u8bd1\u5668\u7248\u672c\uff08\u5982\u679c\u4ece\u6e90\u4ee3\u7801\u7f16\u8bd1\uff09\uff1a\r\n> * CUDA / cuDNN\u7248\u672c\uff1a9.2\uff0cV9.2.148\r\n> * GPU\u578b\u53f7\u548c\u5185\u5b58\uff1a\u4e24\u4e2aGPU\uff0cGTX 1080 Ti\uff0c\u6bcf\u4e2a11178MiB\r\n> \r\n> \u60a8\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u7684\u73af\u5883\u6355\u83b7\r\n> [\u811a\u672c](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\u6536\u96c6\u4e00\u4e9b\u6b64\u7c7b\u4fe1\u606f\u3002\r\n> \u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u83b7\u53d6TensorFlow\u7248\u672c\uff1a1. TF 1.0\uff1a`python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`2. TF 2.0\uff1a`python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n> \r\n> **\u63cf\u8ff0**\r\n> \u6211\u6b63\u5728\u5c1d\u8bd5\u5728\u6211\u7684GPU\u4e2d\u8fd0\u884c\u4ee5\u4e0b\u793a\u4f8b\u4ee3\u7801\uff08keras tensorflow backend\uff09**\u7684\u5f53\u524d\u884c\u4e3a**\r\n> \r\n> ```\r\n> left = Input(shape=(128, 3072), dtype='float32', name='Input-Left')\r\n> right = Input(shape=(128, 3072), dtype='float32', name='Input-Right')\r\n> lstm = Bidirectional(LSTM(units=768,\r\n>                           activation='tanh'),\r\n>                       name='Bidirectional-LSTM')\r\n> l_lstm = lstm(left)\r\n> r_lstm = lstm(right)\r\n> subtracted = Subtract(name='Subtract')([l_lstm, r_lstm])\r\n> abs_subtracted = Lambda(function=backend.abs)(subtracted)\r\n> mul = Multiply(name='multiplication')([l_lstm, r_lstm])\r\n> concat = concatenate([abs_subtracted, mul])\r\n> output = Dense(units=1)(concat)\r\n> model = Model(inputs=[left, right],\r\n>               outputs=output)\r\n> model = multi_gpu_model(model, gpus=2)\r\n> model.compile(loss='mean_squared_error',\r\n>               optimizer='Adam',\r\n>               metrics=['acc'])\r\n> import numpy as np\r\n> x1 = np.random.rand(100, 128, 3072)\r\n> x2 = np.random.rand(100, 128, 3072)\r\n> y = np.random.rand(100)\r\n> model.fit(x = [x1, x2],\r\n>          y=y,\r\n>          epochs=10)\r\n> ```\r\n> \r\n> **\u63cf\u8ff0\u9884\u671f\u7684\u884c\u4e3a**\r\n> \r\n> \u9884\u671f\u5b83\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c\u800c\u4e0d\u4f1a\u51fa\u73b0\u9519\u8bef\u3002\r\n> \r\n> **\u91cd\u73b0\u95ee\u9898\u7684\u4ee3\u7801**\r\n> \u63d0\u4f9b\u53ef\u91cd\u73b0\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u8fd9\u662f\u4ea7\u751f\u95ee\u9898\u6240\u5fc5\u9700\u7684\u6700\u4f4e\u8981\u6c42\u3002\r\n> \r\n> **\u5176\u4ed6\u4fe1\u606f/\u65e5\u5fd7**\r\n> \u5305\u62ec\u6709\u52a9\u4e8e\u8bca\u65ad\u95ee\u9898\u7684\u4efb\u4f55\u65e5\u5fd7\u6216\u6e90\u4ee3\u7801\u3002\u5982\u679c\u5305\u62ec\u56de\u6eaf\uff0c\u8bf7\u5305\u62ec\u5b8c\u6574\u7684\u56de\u6eaf\u3002\u5927\u578b\u65e5\u5fd7\u548c\u6587\u4ef6\u5e94\u9644\u52a0\u3002\r\n> \r\n> \u5f53\u6211\u5728GPU\u4e0a\u8fd0\u884c\u4ee3\u7801\u65f6\uff0c\u51fa\u73b0\u4ee5\u4e0b\u9519\u8bef\uff0c\u4f46\u662f\uff0c\u5982\u679c\u6211\u5378\u8f7dtensorflow-gpu\uff0c\u5219\u6ca1\u6709\u9519\u8bef\uff0c\u4f46\u662f\u6211\u53ea\u80fd\u4f7f\u7528cpu\u3002\r\n> \r\n> ```\r\n> Using TensorFlow backend.\r\n> 2019-07-12 15:38:28,543 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n> \r\n> 2019-07-12 15:38:28,552 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n> \r\n> 2019-07-12 15:38:28,552 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\r\n> \r\n> 2019-07-12 15:38:29,759 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\r\n> \r\n> 2019-07-12 15:38:29,837 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\r\n> \r\n> 2019-07-12 15:38:29,840 From /home/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n> 2019-07-12 15:38:29.856975: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2019-07-12 15:38:29.860863: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n> 2019-07-12 15:38:29.974060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:29.976417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:29.977100: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x97d2a40 executing computations on platform CUDA. Devices:\r\n> 2019-07-12 15:38:29.977112: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n> 2019-07-12 15:38:29.977117: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n> 2019-07-12 15:38:29.978574: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696330000 Hz\r\n> 2019-07-12 15:38:29.979175: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x984ed80 executing computations on platform Host. Devices:\r\n> 2019-07-12 15:38:29.979185: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n> 2019-07-12 15:38:29.980287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:29.980881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\n> name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\n> pciBusID: 0000:01:00.0\r\n> 2019-07-12 15:38:29.980907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:29.981642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\n> name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\n> pciBusID: 0000:02:00.0\r\n> 2019-07-12 15:38:29.981842: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n> 2019-07-12 15:38:29.982531: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n> 2019-07-12 15:38:29.983173: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n> 2019-07-12 15:38:29.983363: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n> 2019-07-12 15:38:29.984149: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n> 2019-07-12 15:38:29.984728: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n> 2019-07-12 15:38:29.985920: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-07-12 15:38:29.985961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:29.986728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:29.987487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:29.988224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:29.988948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1\r\n> 2019-07-12 15:38:33.252621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-07-12 15:38:33.252666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 \r\n> 2019-07-12 15:38:33.252672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y \r\n> 2019-07-12 15:38:33.252677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N \r\n> 2019-07-12 15:38:33.253003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:33.253769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:33.254223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:33.254958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10054 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n> 2019-07-12 15:38:33.255392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-07-12 15:38:33.255840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10054 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n> 2019-07-12 15:38:33.714112: F ./tensorflow/core/kernels/random_op_gpu.h:227] Non-OK-status: CudaLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: invalid configuration argument\r\n> Aborted (core dumped)\r\n> ``when I explore multi_gpu_model in keras, I have a same problem, can you tell me how to settle this .", "i am also getting the same error and not able to resolve it if I m running the following code:\r\nimport tensorflow as tf\r\nimport numpy as np\r\nmodel=tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(64,activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(64,activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(10,activation='softmax'))\r\nmodel.compile(optimzer='Adam',\r\n             loss='categorical_crossentropy',\r\n             metrics=['accuracy'])\r\ndata=np.random.random((1000,32))\r\nlabels=np.random.random((1000,10))\r\nmodel.fit(data,labels,batch_size=32,epochs=3)<-----------     \r\n\r\n\"\"   after this I m getting error::\"\"\"\r\n\r\n2019-11-28 11:04:06.964863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56\r\npciBusID: 0000:01:00.0\r\n2019-11-28 11:04:06.971063: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-28 11:04:06.976492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-28 11:04:06.979274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-28 11:04:06.984074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-11-28 11:04:06.986746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-11-28 11:04:06.992165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 3023 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-11-28 11:04:08.220421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56\r\npciBusID: 0000:01:00.0\r\n2019-11-28 11:04:08.227433: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-28 11:04:08.233568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-28 11:04:08.235936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-28 11:04:08.241661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-11-28 11:04:08.250926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-11-28 11:04:08.256356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 3023 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-11-28 11:04:16.908225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56\r\npciBusID: 0000:01:00.0\r\n2019-11-28 11:04:16.915702: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-28 11:04:16.921282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-28 11:04:16.927152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56\r\npciBusID: 0000:01:00.0\r\n2019-11-28 11:04:16.947245: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-28 11:04:16.952123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-28 11:04:16.954761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-28 11:04:16.962555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-11-28 11:04:16.977982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-11-28 11:04:16.996614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3023 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-11-28 11:04:17.374983: **F .\\tensorflow/core/kernels/random_op_gpu.h:227] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: invalid device function**", "please help if anyone can resolve this \r\ni  using Tensorflow version =2.0\r\nhave installed tensorfloe-gpu with pip\r\nCUDA version is 10.0\r\npython version is 3.6x", "Same error here, my environment:\r\ntensorflow-gpu 2.0 installed with pip\r\nCUDA version is 9.2/10.0\r\npython version is 3.6.9", " tf-nightly-gpu - install. they fixed that. ", "After reinstall tensorflow-gpu==2.0.0 in a clean environment, the issue disappears."]}, {"number": 30664, "title": "tf.data.Dataset.window encountered `AttributeError: '_VariantDataset' object has no attribute 'numpy'`", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): n/a\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0b1\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```\r\nubuntu@ubuntu:/v# python\r\nPython 2.7.15+ (default, Nov 27 2018, 23:36:35) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.version.VERSION\r\n'2.0.0-beta1'\r\n>>> x = tf.data.Dataset.range(5)\r\n>>> y = tf.data.Dataset.range(5).window(2)\r\n>>> for i in x:\r\n...   print(i.numpy())\r\n... \r\n0\r\n1\r\n2\r\n3\r\n4\r\n>>> for i in y:\r\n...   print(i.numpy())\r\n... \r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\nAttributeError: '_VariantDataset' object has no attribute 'numpy'\r\n>>> \r\n```\r\n**Describe the expected behavior**\r\n\r\nIn the above example `for i in y:print(i.numpy())` should produce result without AttributeError.\r\n```\r\nfor i in y:\r\n...   print(i.numpy())\r\n... \r\n0\r\n1\r\n2\r\n3\r\n4\r\n```\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.version.VERSION\r\nx = tf.data.Dataset.range(5)\r\ny = tf.data.Dataset.range(5).window(2)\r\nfor i in x:\r\n  print(i.numpy())\r\nfor i in y:\r\n  print(i.numpy())\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["/cc @jsimsa @mrry ", "This behavior is expected. `window` combines a finite sequence of elements of the input dataset into a single element of type dataset. The dataset object does not have `numpy`. If you would like to iterate over the element of `y`, you should do the following:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.version.VERSION\r\nx = tf.data.Dataset.range(5)\r\ny = tf.data.Dataset.range(5).window(2)\r\nfor i in x:\r\n  print(i.numpy())\r\nfor i in y:\r\n  for j in i:\r\n    print(j.numpy())\r\n```", "@jsimsa Thanks for the explanation \ud83d\udc4d !"]}, {"number": 30663, "title": "Fix incorrect example in tf.data.Dataset.window", "body": "This fix fixes an incorrect example in tf.data.Dataset.window\r\n\r\nThe previous example:\r\n```\r\ntf.data.Dataset.from_tensor_slices((range(4), range(4)).window(2)\r\n```\r\n\r\ndoes not have close parentheses, and should be\r\n\r\n```\r\ntf.data.Dataset.from_tensor_slices((range(4), range(4))).window(2)\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 30662, "title": "[R1.14] Fix incorrect get_link_flags on Mac", "body": "**NOTE: This is against R1.14 and is cherry-picked from #30656**\r\n\r\n\r\nThis fix tries to address the issue raised in #30633 where\r\n`tf.sysconfig.get_link_flags` on mac returned\r\n'-l:libtensorflow_framework.1.dylib' which is not valid\r\nfor ld on macOS.\r\n\r\nThis fix changes to `-ltensorflow_framework.1`\r\n\r\nThis fix fixes #30633.\r\nThis fix also fixed #30564.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 30661, "title": "[WIP] Adding generic TRT plugin support", "body": "This PR enables generic plugin support for TFTRT, allowing custom TRT plugins to be included TRT conversion process, increasing TFTRT coverage.", "comments": ["@aaroey I created this for you to review. Please let me know what you think", "@samikama thank you , can you please add some test cases.", "@samikama thank you. @aaroey can you please review this PR . ?", "@samikama Could you write a unit test for this or a python test? You might be able to take advantage of the cast plugin from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2tensorrt/plugin/plugin_cast.cu.cc\r\n", "Can one of the admins verify this patch?", "@samikama could you help to resolve the conflicts?", "@samikama Could you please address the reviewer comments and resolve conflicts? Thanks!", "@aaroey I was working on sorting out merge conflict and I noticed there were some changes made on of TRT plugins are initialized. Current changes, especially passing logger to plugin initialization function causing ownership or duplication problems. In current schema, caller of Converter::Create() owns the logger and it is passed down to plugins. However plugins library will require initialization without presence of converter which in turn require another copy of logger. I believe passing the logger to converter then to the plugins is not a good choice. How should we solve this?\r\n\r\nThanks,\r\nSami", "As per private discussions I am closing this. If anybody would require this functionality, you can contact me."]}, {"number": 30660, "title": "[XLA:GPU][ROCm] refactor XLA nvptx_compiler module to extract platform-neutral logic", "body": "- Rename `nvptx_compiler` to `gpu_compiler`. `gpu_compiler` would form the base class for `nvptx_compiler` and upcoming `amdgpu_compiler`.\r\n- Extract NVPTX-specific logic into `nvptx_compiler`.\r\n- Disable targets which directly import CUDA-only headers such as `cholesky_thunk`, `cusolver_context` within bazel scripts.\r\n\r\n@thomasjoerg The PR is highly dependent to #30238 and #30326 so I think it'd be better to get those 2 PRs finalized before we thoroughly review this one. I'm posting this PR to initiate discussion.\r\n\r\nMuch of the HLO optimization pipeline is the same between NVPTX and AMDGPU, while some are platform-dependent:\r\n- convolution canonicalization:\r\n  - `CusolverRewriter` is only available on CUDA.\r\n  - `CudnnFusedConvRewriter` is only available on CUDA. On AMD ROCm platform different fusion kernels are supported so the rewriter would be different.\r\n  - `CudnnConvPadForTensorCores` is only available on NV Volta.\r\n- post layout assignment: \r\n  - `GemmRewriter` and `GemmAlgorithmPicker` are tailored for Cublas.\r\n  - `CudnnConvAlgorithmPicker` is only available on CUDA. On AMD ROCm platform a different pass `MIOpenConvAlgorithmPicker` would be introduced in subsequent PRs.", "comments": ["apparently recent changes in MLIR has changed the implementation quite a bit. let me rebase and adjust to the new codes. ", "Inviting @timshen91 as this PR sits on top of #30238. I'm still revising the PR due to recent introduction of `mlir_gpu` module in TensorFlow.", "I'm not faimilar with MLIR-related changes. I guess I'll defer the review to Thomas.", "Failures suggest an environment issue. `cudnn_conv_algorithm_picker.h` is present in TensorFlow mainline but CI `Linux GPU` claims the header couldn't be found.", "@thomasjoerg a gentle ping.", "@thomasjoerg I still couldn't get the PR to build properly. We'll always run into this error:\r\n\r\n```\r\n./tensorflow/compiler/xla/service/gpu/nvptx_compiler.h:26:10: fatal error: tensorflow/compiler/xla/service/gpu/gpu_compiler.h: No such file or directory\r\n #include \"tensorflow/compiler/xla/service/gpu/gpu_compiler.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\nBut if we add `:gpu_compiler` into either `:nvptx_compiler` or `:nvptx_compiler_impl` we produce cyclical dependency and fails bazel.", "@thomasjoerg sorry for all the forced updates to this PR. Now `Linux GPU` test targets looks better. Instead of letting `:gpu_compiler_impl` depending on `:nvptx_compiler_impl` or `:amdgpu_compiler_impl` I reversed the order.", "@thomasjoerg I noticed `Linux GPU` tests are failing because #31146 was reverted due to failing XLA CUDA. Any chance I can take a look at those failures?", "@thomasjoerg a gentle ping. since #31146 was rolled back this PR fails on `Linux GPU`. may I understand the best process to get both (#31146, #30660) PRs merged? since the test results for \"XLA for CUDA\" is not visible to me I could only rely on information from you to help steer me forward.\r\n\r\n"]}, {"number": 30659, "title": "PLEASE add cdist", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0b1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\ncalculating distance matrices efficiently with tensorflow is a huge pain involving reading tons of stack overflow threads and re-implementing the same stuff. the solutions on stack overflow only cover euclidean distances and give MxM matrices even if you want city-block distance and MxMxD tensors ... it is extremely frustrating to experiment with optimal transport theory with tensorflow when such an obvious thing as CDIST is missing. \r\n\r\n**Will this change the current api? How?**\r\nYes, it will be easy to calculate distance matrices\r\n\r\n**Who will benefit with this feature?**\r\nanyone doing molecular dynamics, optimal transport, wasserstein, shape matching, graph theory, etc etc\r\n\r\n**Any Other info.**\r\nPDIST is also potentially useful\r\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html\r\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\r\n\r\nBlock-wise version would be IDEAL as huge distance matrices often overflow gpu memory", "comments": ["here is L1 (cityblock / manhattan) works for N,D x M,D and does NOT reduce sum (so it will be N,M,D output shape)\r\n```\r\ndef get_distances(a, b):\r\n    return tf.math.abs(tf.expand_dims(a, axis=1) - tf.expand_dims(b, axis=0))\r\n```", "https://github.com/ftonolini45/Variational_Sparse_Coding/blob/master/models/vae_utils.py\r\n\r\n```\r\ndef cartesian(arrays, out=None):\r\n    \"\"\"\r\n    Generate a cartesian product of input arrays.\r\n    FROM http://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays/1235363#1235363\r\n    Parameters\r\n    ----------\r\n    arrays : list of array-like\r\n        1-D arrays to form the cartesian product of.\r\n    out : ndarray\r\n        Array to place the cartesian product in.\r\n    Returns\r\n    -------\r\n    out : ndarray\r\n        2-D array of shape (M, len(arrays)) containing cartesian products\r\n        formed of input arrays.\r\n    Examples\r\n    --------\r\n    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\r\n    array([[1, 4, 6],\r\n           [1, 4, 7],\r\n           [1, 5, 6],\r\n           [1, 5, 7],\r\n           [2, 4, 6],\r\n           [2, 4, 7],\r\n           [2, 5, 6],\r\n           [2, 5, 7],\r\n           [3, 4, 6],\r\n           [3, 4, 7],\r\n           [3, 5, 6],\r\n           [3, 5, 7]])\r\n    \"\"\"\r\n\r\n    arrays = [np.asarray(x) for x in arrays]\r\n    dtype = arrays[0].dtype\r\n\r\n    n = np.prod([x.size for x in arrays])\r\n    if out is None:\r\n        out = np.zeros([n, len(arrays)], dtype=dtype)\r\n\r\n    m = n / arrays[0].size\r\n    out[:,0] = np.repeat(arrays[0], m)\r\n    if arrays[1:]:\r\n        cartesian(arrays[1:], out=out[0:m,1:])\r\n        for j in xrange(1, arrays[0].size):\r\n            out[j*m:(j+1)*m,1:] = out[0:m,1:]\r\n    return out\r\n```\r\nthis could be used to assemble a list of indices with tf.range and then we could do\r\n```\r\ndef cdist(a, b, metric):\r\n    N = tf.shape(a)[1]\r\n    M = tf.shape(b)[1]\r\n    indices = cartesian(tf.range(N), tf.range(M))\r\n    distances = tf.vectorized_map(lambda ij: metric(ij), indices)\r\n    return tf.reshape(distances, (N,M))\r\n```\r\n@agarwal-ashish did vectorized_map, might be able to help make this op blazing fast\r\nnot sure if we'd need to use tf.map_fn instead of vectorized_map on GPU\r\nalso not sure how to do blocks, maybe with tf.slice?", "https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\r\neuclidean:\r\n```\r\ndef pairwise_dist (A, B):  \r\n  \"\"\"\r\n  Computes pairwise distances between each elements of A and each elements of B.\r\n  Args:\r\n    A,    [m,d] matrix\r\n    B,    [n,d] matrix\r\n  Returns:\r\n    D,    [m,n] matrix of pairwise distances\r\n  \"\"\"\r\n  with tf.variable_scope('pairwise_dist'):\r\n    # squared norms of each row in A and B\r\n    na = tf.reduce_sum(tf.square(A), 1)\r\n    nb = tf.reduce_sum(tf.square(B), 1)\r\n    \r\n    # na as a row and nb as a co\"lumn vectors\r\n    na = tf.reshape(na, [-1, 1])\r\n    nb = tf.reshape(nb, [1, -1])\r\n\r\n    # return pairwise euclidean difference matrix\r\n    D = tf.sqrt(tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 0.0))\r\n  return D\r\n```\r\nEDIT : this fails on my rig, segmentation fault ", "docs for desired method have link to source: \r\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html", "these both work for us now, might require some tf.squeeze action for other applications \r\n\r\n```\r\n# @tf.function\r\n# def get_distances(a, b):  # L1\r\n#     print(\"tracing get_distances\")\r\n#     return B.sum(B.abs(tf.expand_dims(a, 0) - tf.expand_dims(b, 1)), axis=-1))\r\n\r\n@tf.function\r\ndef get_distances(a, b):  # L2\r\n    print(\"tracing get_distances\")\r\n    return B.sqrt(B.sum(B.square(tf.expand_dims(a, 0) - tf.expand_dims(b, 1)), axis=-1))\r\n```\r\nhighly cool if fast distances were supported by TF team, also Sinkhorn / OT and Higher Order SVD, Normalized Compression Distance could be good loss for many many applications, but there's not a clear way to compress tensors for this metric in TF 2.0", "Maybe this belongs in tf probability of tf addons, but unclear if it belongs in core TF given how easy it is to implement with TF.", "@alextp note that this is not easy to implement at all. The version provided by bionicles uses much more memory than necessary to compute cdist as for pairwise distances between two sets of N and M objects in needs an array NxM. This is quadratic memory scaling, and it blows up very easily.\r\n\r\nImagine you had 100 images 256x256 pixels each, and you had a batch of 20 such sequences, which is fairly reasonable. If you wanted to compute the CDIST of the images in the sequence, the forward + backward pass of this operation would not fit on your GPU. However, it is in fact only necessary to store 100 x 256 x 256 x 20 values, which is well below a gigabyte. This is what scipy implements, and it far from easy for an average user.", "I believe cdist can be implemented as a composition of existing TF operations (efficient cdist amounts to a batch matmul under the hood, right?), so it can live in addons.\r\n\r\n@rmlarsen do you know if I'm being confused here?", "I think that's right, for L2 distances a batch matmul will do (but for other common distances it can be more tricky). I just wanted to note that it is rather non-trivial to implement for an average user, especially those not aware of what the efficient implementation is, and it would be helpful to have it since this thread seems to have concluded that the implementation is trivial.", "One related question is if XLA could fuse reductions with cwise operations to avoid the blowup of memory.", "This is the feature I've been waiting for years... Even if it could be easily implemented by users, although I didn't find it very easy, to include this feature in the official library will definitely benefit a lot of average users like me. I'm working on graphs and calculating similarity matrices using TF gave me quite a bit of pain... "]}, {"number": 30657, "title": "No TRTEngineOps after Conversion (Tiny Yolov3)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0 / 7.6.0\r\n- GPU model and memory: Tesla V100 32GB\r\n\r\n**Describe the current behavior**\r\nI am trying to convert a Tiny Yolov3 frozen graph into a frozen graph with some operations replaced with TRTEngineOps so that they are run with TensorRT. My graph has many [nodes that are supported by TF-TRT](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops) yet none are simplified into a TRTEngineOp.\r\n\r\nAfter conversion, this is the count of different `node.op` in the resulting frozen graph:\r\n```\r\n{\r\n    \"Placeholder\": 1,\r\n    \"Const\": 84,\r\n    \"Mul\": 7,\r\n    \"Transpose\": 26,\r\n    \"Conv2D\": 13,\r\n    \"FusedBatchNorm\": 11,\r\n    \"LeakyRelu\": 11,\r\n    \"MaxPool\": 6,\r\n    \"ResizeNearestNeighbor\": 1,\r\n    \"Identity\": 4,\r\n    \"BiasAdd\": 2,\r\n    \"ConcatV2\": 5,\r\n    \"Reshape\": 2,\r\n    \"SplitV\": 3,\r\n    \"Sigmoid\": 6,\r\n    \"Exp\": 2,\r\n    \"Add\": 4,\r\n    \"Sub\": 2\r\n}\r\n```\r\n\r\nAnd, information regarding the conversion process:\r\n```\r\ngraph_size(MB)(native_tf): 34.0\r\ngraph_size(MB)(trt): 34.0\r\nnum_nodes(native_tf): 300\r\nnum_nodes(tftrt_total): 190\r\nnum_nodes(trt_only): 0\r\ntime(s) (trt_conversion): 8.8426\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect some groups of nodes / subgraphs to be converted into TRTEngineOps so that they can run in TensorRT. According to the [supported ops](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops), the following operators could / should have been somewhat simplified into TRTEngineOps: `Const`, `Mul`, `Conv2D`, `FusedBatchNorm`, `MaxPool`, `Identity`, `BiasAdd`, `ConcatV2`, `Reshape`, `Sigmoid`, `Exp`, `Add`, and `Sub`. In fact, this is nearly all the operators, save for `Placeholder`, `Transpose`, `LeakyRelu`, `ResizeNearestNeighbor`, and `SplitV`.\r\n\r\n**Code to reproduce the issue**\r\n(functionality used from [example here](https://github.com/tensorflow/tensorrt/tree/master/tftrt/examples/object_detection))\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\ndef optimize_model(frozen_graph,\r\n                   use_trt=True,\r\n                   force_nms_cpu=True,\r\n                   replace_relu6=True,\r\n                   remove_assert=True,\r\n                   is_dynamic_op=True,\r\n                   precision_mode='FP32',\r\n                   max_batch_size=1,\r\n                   minimum_segment_size=1,\r\n                   max_workspace_size_bytes=2 << 32,\r\n                   maximum_cached_engines=100,\r\n                   calib_images_dir=None,\r\n                   num_calib_images=None,\r\n                   calib_batch_size=1,\r\n                   calib_image_shape=None,\r\n                   output_path=None):\r\n    # apply graph modifications\r\n    if force_nms_cpu:\r\n        frozen_graph = f_force_nms_cpu(frozen_graph)\r\n    if replace_relu6:\r\n        frozen_graph = f_replace_relu6(frozen_graph)\r\n    if remove_assert:\r\n        frozen_graph = f_remove_assert(frozen_graph)\r\n    # get input names\r\n    output_names = ['inputs', 'output_boxes']\r\n\r\n    # optionally perform TensorRT optimization\r\n    if use_trt:\r\n        graph_size = len(frozen_graph.SerializeToString())\r\n        num_nodes = len(frozen_graph.node)\r\n        start_time = time.time()\r\n\r\n        converter = trt.TrtGraphConverter(\r\n            input_graph_def=frozen_graph,\r\n            nodes_blacklist=output_names,\r\n            max_workspace_size_bytes=max_workspace_size_bytes,\r\n            precision_mode=precision_mode,\r\n            minimum_segment_size=minimum_segment_size,\r\n            is_dynamic_op=is_dynamic_op,\r\n            maximum_cached_engines=maximum_cached_engines,\r\n            max_batch_size=max_batch_size)\r\n        frozen_graph = converter.convert()\r\n\r\n        end_time = time.time()\r\n        print(\"graph_size(MB)(native_tf): %.1f\" % (float(graph_size)/(1<<20)))\r\n        print(\"graph_size(MB)(trt): %.1f\" %\r\n            (float(len(frozen_graph.SerializeToString()))/(1<<20)))\r\n        print(\"num_nodes(native_tf): %d\" % num_nodes)\r\n        print(\"num_nodes(tftrt_total): %d\" % len(frozen_graph.node))\r\n        print(\"num_nodes(trt_only): %d\" % len([1 for n in frozen_graph.node if str(n.op)=='TRTEngineOp']))\r\n        print(\"time(s) (trt_conversion): %.4f\" % (end_time - start_time))\r\n\r\n    return frozen_graph\r\n\r\ninput_graph_path = '/path/to/frozen_graph.pb'\r\n\r\nwith tf.io.gfile.GFile(input_graph_path, 'rb') as f:\r\n    orig_graph_def = tf.compat.v1.GraphDef()\r\n    orig_graph_def.ParseFromString(f.read())\r\n\r\nfrozen_graph_def = optimize_model(orig_graph_def,\r\n                                  precision_mode=PRECISION,\r\n                                  max_batch_size=64,\r\n                                  is_dynamic_op=False)\r\n\r\nfor node in frozen_graph_def.node:\r\n    print(node.op, node.name)\r\n```\r\n\r\n**Other info / logs**\r\nClick [here](https://drive.google.com/file/d/1Bgj9h6TJLwedtrhnritRs9eYm_iczG4v/view?usp=sharing) for the graph in question.\r\n\r\nAdditionally, regardless of which precision of `FP32`, `FP16`, and `INT8` I use for precision, the size of the resulting models is `exactly` the same number of bytes. I observe no inference speed-up either.\r\n", "comments": ["Apparently there is some issues with TF-TRT in 1.14, @bananabowl for more information.\r\n\r\nIf you try with tf-nightly-gpu, the output will look like:\r\n```\r\n2019-07-22 09:20:39.761959: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: tf_graph\r\n2019-07-22 09:20:39.762029: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 171 nodes (-128), 186 edges (-130), time = 107.078ms.\r\n2019-07-22 09:20:39.762049: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 223 nodes (52), 238 edges (52), time = 38.567ms.\r\n2019-07-22 09:20:39.762065: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 223 nodes (0), 238 edges (0), time = 44.786ms.\r\n2019-07-22 09:20:39.762081: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 31 nodes (-192), 40 edges (-198), time = 13152.8809ms.\r\n2019-07-22 09:20:39.762097: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 23 nodes (-8), 36 edges (-4), time = 69.685ms.\r\ngraph_size(MB)(native_tf): 34.0\r\ngraph_size(MB)(trt): 61.6\r\nnum_nodes(native_tf): 299\r\nnum_nodes(tftrt_total): 23\r\nnum_nodes(trt_only): 6\r\ntime(s) (trt_conversion): 15.2821\r\nPlaceholder inputs\r\nConst detector/tiny-yolo/Conv_6/weights\r\nConst detector/tiny-yolo/Const\r\nConst detector/tiny-yolo/split/split_dim\r\nConst detector/tiny-yolo/ResizeNearestNeighbor/size\r\nConst detector/tiny-yolo/Const_1\r\nConst detector/tiny-yolo/split_1/split_dim\r\nConst Const\r\nConst split/split_dim\r\nConst output_boxes/axis\r\nTRTEngineOp TRTEngineOp_0\r\nIdentity detector/tiny-yolo/Conv_6/weights/read\r\nConv2D detector/tiny-yolo/Conv_6/Conv2D\r\nTRTEngineOp detector/tiny-yolo/TRTEngineOp_3\r\nSplitV detector/tiny-yolo/split\r\nResizeNearestNeighbor detector/tiny-yolo/ResizeNearestNeighbor\r\nTRTEngineOp detector/tiny-yolo/TRTEngineOp_4\r\nSplitV detector/tiny-yolo/split_1\r\nTRTEngineOp detector/tiny-yolo/TRTEngineOp_6\r\nSplitV split\r\nTRTEngineOp TRTEngineOp_1\r\nTRTEngineOp TRTEngineOp_2\r\nConcatV2 output_boxes\r\n```", "According to my experience, model optimized directly with the tf default installation package does not have 'TRTEngineOp', which means that the optimization is not successful. According to NVIDIA's official documentation, you need to use TensorFlow container or compile TensorFlow with TensorRT through source code. The latter one I tried is ok(cuda10+tf1.14). Through experiments, QPS of the classification model (inception_v3) for FP16 is increased by 3 times. However, for the OCR model, because the input is not fixed, the optimization is not successful.", "> Apparently there is some issues with TF-TRT in 1.14, @bananabowl for more information.\r\n> \r\n> If you try with tf-nightly-gpu, the output will look like:\r\n> \r\n> ```\r\n> 2019-07-22 09:20:39.761959: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: tf_graph\r\n> 2019-07-22 09:20:39.762029: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 171 nodes (-128), 186 edges (-130), time = 107.078ms.\r\n> 2019-07-22 09:20:39.762049: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 223 nodes (52), 238 edges (52), time = 38.567ms.\r\n> 2019-07-22 09:20:39.762065: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 223 nodes (0), 238 edges (0), time = 44.786ms.\r\n> 2019-07-22 09:20:39.762081: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 31 nodes (-192), 40 edges (-198), time = 13152.8809ms.\r\n> 2019-07-22 09:20:39.762097: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 23 nodes (-8), 36 edges (-4), time = 69.685ms.\r\n> graph_size(MB)(native_tf): 34.0\r\n> graph_size(MB)(trt): 61.6\r\n> num_nodes(native_tf): 299\r\n> num_nodes(tftrt_total): 23\r\n> num_nodes(trt_only): 6\r\n> time(s) (trt_conversion): 15.2821\r\n> Placeholder inputs\r\n> Const detector/tiny-yolo/Conv_6/weights\r\n> Const detector/tiny-yolo/Const\r\n> Const detector/tiny-yolo/split/split_dim\r\n> Const detector/tiny-yolo/ResizeNearestNeighbor/size\r\n> Const detector/tiny-yolo/Const_1\r\n> Const detector/tiny-yolo/split_1/split_dim\r\n> Const Const\r\n> Const split/split_dim\r\n> Const output_boxes/axis\r\n> TRTEngineOp TRTEngineOp_0\r\n> Identity detector/tiny-yolo/Conv_6/weights/read\r\n> Conv2D detector/tiny-yolo/Conv_6/Conv2D\r\n> TRTEngineOp detector/tiny-yolo/TRTEngineOp_3\r\n> SplitV detector/tiny-yolo/split\r\n> ResizeNearestNeighbor detector/tiny-yolo/ResizeNearestNeighbor\r\n> TRTEngineOp detector/tiny-yolo/TRTEngineOp_4\r\n> SplitV detector/tiny-yolo/split_1\r\n> TRTEngineOp detector/tiny-yolo/TRTEngineOp_6\r\n> SplitV split\r\n> TRTEngineOp TRTEngineOp_1\r\n> TRTEngineOp TRTEngineOp_2\r\n> ConcatV2 output_boxes\r\n> ```\r\n\r\n@aaroey Thank you. Updating to `tf-nightly-gpu==1.15.0.dev20190718` fixed the issue of no TRTEngineOps being created/used. Closing as this specific issue is fixed."]}, {"number": 30656, "title": "Fix incorrect get_link_flags on Mac", "body": "This fix tries to address the issue raised in #30633 where `tf.sysconfig.get_link_flags` on mac returned\r\n'-l:libtensorflow_framework.1.dylib' which is not valid for ld on macOS.\r\n\r\nThis fix changes to `-ltensorflow_framework.1`\r\n\r\nThis fix fixes #30633.\r\n\r\nThis fix also fixed #30564.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@mihaimaruseac Thanks for the review. I will create another PR against R1.14 and cherry-pick once this PR is merged.", "@mihaimaruseac @yongtang Hmmm this commit broke our linking to TF2 preview macos whl. There it is named `libtensorflow_framework.2.dylib` as I would have thought should be expected.  Is there a mismatch in what we're expecting across versions here?\r\n\r\nSee: https://github.com/tensorflow/addons/issues/353", "@seanpmorgan see #30633 and  #30564 for some background information.", "> @seanpmorgan see #30633 and #30564 for some background information.\r\n\r\nThanks was just going through these. So my question would be why did our bazel build work prior to this using the custom-op template:\r\nhttps://github.com/tensorflow/addons/blob/master/tf_dependency/BUILD.tpl#L11\r\n`TF_SHARED_LIBRARY_NAME` = `libtensorflow_framework.2.dylib` \r\n\r\nDid you run into this error building with bazel? I'd prefer not to have to do a `if == Dawrin` string edit, at least not without understanding why", "I guess a more reasonable thing to do would use the macos `ld` to find the library name... but is there a way to do that in the bazel build / should there be an updated procedure on custom-op?", "I believe this string edit is a mis-use of the link flag:\r\nhttps://github.com/tensorflow/custom-op/blob/master/configure.sh#L94\r\n\r\nI'll file an issue/PR on custom-op. Thanks!"]}, {"number": 30655, "title": "About Quantization Full Integer: Test with Yolo Tiny v2", "body": "Hello, \r\n\r\nWe tested the integer quantization in the scenarios of the table below, but we did not succeed in testing with Yolo Tiny v2, we would like to know what kind of optimization there is in mobilenet, inception and resnet so that we can apply in Yolo so that I can quantize and obtain a smaller inference time for the quantized model.\r\n\r\n![image](https://user-images.githubusercontent.com/8125226/61136270-b11e1100-a499-11e9-8d52-efb148f6ab10.png)\r\n\r\n\r\n", "comments": ["Sorry for the delayed response.\r\n\r\n1. Try if post-training integer quantization works now.\r\n2. If 1 doesn't work, try not passing representative dataset and only enable default optimization. This gives you a weight-only quantized model. You should be able to see some speedup with that. More details:\r\nhttps://www.tensorflow.org/lite/performance/post_training_quantization", "@Izabellaaaq \r\nIf this is still an issue?\r\nCould you please try with the latest TF v2.6.0 and refer to this[ link ](https://www.tensorflow.org/lite/performance/post_training_quantization) ?Please let us know if it helps ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30655\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30655\">No</a>\n"]}, {"number": 30654, "title": "[mlir] enable Affine Dialect in tf-opt", "body": "`tf-opt --help` shows Affine dialect passes, but it doesn't\r\nrecognize the Affine dialect because AffineDialectRegistration\r\nis not linked in.", "comments": ["Thanks. Yes, I know `mlir-opt` works, but as I said `tf-opt --help` shows Affine passes. That was a bit confusing. "]}, {"number": 30653, "title": "tf.data.Dataset.map() ignores eager execution", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.5 (18F132)\r\n- TensorFlow installed from (source or binary): tensorflow==2.0.0b1 from https://pypi.org/\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nBy default, eager execution should be enabled in TF 2.0; so each tensor's value can be accessed by calling `.numpy()`.\r\n\r\nWhen a function relying on accessing a tensor's value is passed as a parameter to `tf.data.Dataset.map()`, it seems that internally the tensor is no longer an `EagerTensor` and accessing its value fails.\r\n\r\n**Describe the expected behavior**\r\n\r\nI'm not sure whether this behavior is intended or not but either way it should be document and/or fixed.\r\n\r\nPurpose: text-based operations on `tf.data.Dataset`, such as custom splitting, filtering, etc. ideally without the need to write a custom operation.\r\n\r\n**Code to reproduce the issue**\r\n\r\nMWE to reproduce the behavior in colab:\r\n\r\n```python\r\n!pip install tensorflow-gpu==2.0.0b1\r\n!pip install tensorflow-datasets==1.0.2\r\n\r\nimport tensorflow as tf\r\n\r\n\r\n# transform a string tensor to upper case\r\ndef upper_case_fn(t: tf.Tensor) -> tf.Tensor:\r\n    print(type(t))\r\n    return tf.constant(t.numpy().decode('utf-8').upper())\r\n\r\n\r\n# the same with tf op\r\ndef upper_case_tf(t: tf.Tensor) -> tf.Tensor:\r\n    print(type(t))\r\n    return tf.strings.upper(t)\r\n\r\n\r\n# sanity check\r\nc1 = upper_case_fn(tf.constant('casing_fn'))\r\nprint(c1)\r\n\r\nc2 = upper_case_tf(tf.constant('casing_tf'))\r\nprint(c2)\r\n\r\n# dataset\r\nd = tf.data.Dataset.from_tensor_slices([\r\n    tf.constant('hello'),\r\n    tf.constant('world')\r\n])\r\n\r\n# working with OP\r\nd2 = d.map(lambda x: upper_case_tf(x))\r\nfor _ in d2:\r\n    print(_)\r\n\r\n# failing with Python fn\r\nd2 = d.map(lambda x: upper_case_fn(x))\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\n```\r\nInstalling collected packages: tensorflow-datasets\r\nSuccessfully installed tensorflow-datasets-1.0.2\r\n<class 'tensorflow.python.framework.ops.EagerTensor'>\r\ntf.Tensor(b'CASING_FN', shape=(), dtype=string)\r\n<class 'tensorflow.python.framework.ops.EagerTensor'>\r\ntf.Tensor(b'CASING_TF', shape=(), dtype=string)\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\ntf.Tensor(b'HELLO', shape=(), dtype=string)\r\ntf.Tensor(b'WORLD', shape=(), dtype=string)\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\n\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-1-44edb2261a09> in <module>()\r\n     36 \r\n     37 # failing with Python fn\r\n---> 38 d2 = d.map(lambda x: upper_case_fn(x))\r\n\r\n11 frames\r\n\r\n<ipython-input-1-44edb2261a09> in upper_case_fn(t)\r\n      8 def upper_case_fn(t: tf.Tensor) -> tf.Tensor:\r\n      9     print(type(t))\r\n---> 10     return tf.constant(t.numpy().decode('utf-8').upper())\r\n     11 \r\n     12 \r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n", "comments": ["My bad, this **is** documented:\r\n\r\n```\r\n    Note that irrespective of the context in which `map_func` is defined (eager\r\n    vs. graph), tf.data traces the function and executes it as a graph. To use\r\n    Python code inside of the function you have two options:\r\n...\r\n    2) Use `tf.py_function`, which allows you to write arbitrary Python code but\r\n    will generally result in worse performance than 1).\r\n```\r\n\r\nClosing.", "Nevertheless, it's not clear how to actually do it.... several attemps in this direction failed:\r\n\r\n```python\r\n# failing with Python fn\r\nd2 = d.map(lambda x:\r\n           tf.py_function(\r\n               func=upper_case_fn(x),\r\n               inp=[x],\r\n               Tout=tf.string\r\n           )\r\n           )\r\n```", "Would this be resolved by using [`tf.numpy_function`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/numpy_function)? Adding @jsimsa for visibility. \ud83d\ude42 ", "The following works:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# transform a string tensor to upper case\r\ndef upper_case_fn(t):\r\n    return t.numpy().upper()\r\n\r\n# dataset\r\nd = tf.data.Dataset.from_tensor_slices([\r\n    tf.constant('hello'),\r\n    tf.constant('world')\r\n])\r\n\r\nd = d.map(lambda x: tf.py_function(upper_case_fn, [x], tf.string))\r\nfor _ in d:\r\n    print(_)\r\n```", "Is there a simper way to access raw values?\r\n\r\nIn my case i had to:\r\n\r\n```\r\n        def get_label(file_path):\r\n            # convert the path to a list of path components\r\n            parts = file_path.numpy().decode().split(os.path.sep)\r\n            # The second to last is the class-directory\r\n            return [str(self.label_dict[parts[-1]])]\r\n```\r\n\r\non top of calling tf.py_function.\r\n\r\nThis looks a bit circuitous.", "\"tf.py_function\" has different behavior and does not replace the need for eager execution for tf.data.Dataset.map functions.  My purpose for using tf.data is to accelerate loading data.  eager execution allows me to inspect tensor values and computation with the debuger.   tf.numpy_function, instrad, converts inputs from tensors to numpy arrays and outputs from numpy arrays to tensors.  I cannot see the tensor values and verify the logic with tf.numpy_function because the tensors are no longer there.  \r\n\r\nPlease, instead, use @tf.function to enable graph vs eager execution of tf.data.Dataset.map functions consistent with tensorflow 2 behavior", "There is no eager execution of tf.data.\r\n\r\nIn TF 2 eager mode, tf.data input pipeline graph is constructed eagerly, an iterator for the input pipeline graph is created eagerly, and then, the \"give me the next element\" is (repeatedly) executed eagerly. The \"give me the next element\" op executes the input pipeline graph (and often this execution in fact happens asynchronously ahead time so that by the time data is requested, it has already been precomputed).\r\n\r\nIn other words, given how tf.data works, executing user-defined functions passed to tf.data transformations eagerly is not trivially possible. For that to make sense, there would need to be no asynchrony in the input pipeline and the tf.data (mostly C++) implementation would either need to be updated to support switching between C++ and Python execution (for which the current mechanism is `tf.py_function`) or have alternative Python backend which would be used for this \"eager\" mode.\r\n\r\nThe tf.data team has no plans to support this.\r\n\r\n", "Thank you for your explanation.  The execution pipelining has certainly speed my training and eliminated software bottlenecks compared with my previous python dataset processing.\r\n", "Following code worked:\r\n\r\n    def parse_str(str_tensor):\r\n        raw_string = str_tensor.numpy().decode(\"utf-8\") \r\n    \r\n        # play with raw string\r\n        raw_string = 'AAA'+raw_string     \r\n        return raw_string\r\n    \r\n\r\nCall parse function:\r\n\r\n\r\n    def tf_pre_processing(row):\r\n      return tf.py_function(parse_str, [row['context']], [tf.string])\r\n    \r\n    \r\n    train = t.map(tf_pre_processing).batch(1).take(1)\r\n    \r\n    list(train)\r\n\r\n", "@jsimsa Thanks for this explanation.\r\n\r\nGiven this current limitation, do you have any tips regarding how to best debug a `tf.data` pipeline?\r\nTypically with any other TensorFlow module I would activate eager mode (`tf.config.run_functions_eagerly(True)`) and use breakpoints. Of course without eager mode it's not possible to breakpoint.", "If you would like to debug using breakpoints, my suggestion is to:\r\n\r\n1) remove any `prefetch` or `num_parallel_calls` arguments in `map` or `interleave` from your input pipeline\r\n2) wrap all functions passed into tf.data transformations that accept user-defined functions (such as `map`) in `tf.py_function`)\r\n\r\nThanks to 1) the input pipeline will be executing fully synchronously and 2) will allow you to use breakpoints.", "@jsimsa : Your advice on how to debug is greatly appreciated! However, it doesn't seem to work for me :(. Using tensorflow 2.4.1, pycharm and windows and your code example:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ntf.data.experimental.enable_debug_mode() # using tensorflow 2.5.0rc0\r\ntf.config.run_functions_eagerly(True)\r\n\r\n\r\ndef upper_case_fn(t):\r\n    x = t.numpy().upper()  # BREAKPOINT HERE\r\n    return x\r\n\r\n\r\n# dataset\r\nd = tf.data.Dataset.from_tensor_slices([tf.constant(\"hello\"), tf.constant(\"world\")])\r\nd = d.map(lambda x: tf.py_function(upper_case_fn, [x], tf.string))\r\nfor _ in d:\r\n    print(_)\r\n```\r\n\r\n\r\nResult: The code does not stop at the specified breakpoint. Any advice?\r\n\r\n// EDIT: \r\nBy now I also discovered that tensorflow 2.5.0 advertises `https://www.tensorflow.org/api_docs/python/tf/data/experimental/enable_debug_mode`. However, even then the breakpoint isn't hit. Probably worth a new issue, opened https://github.com/tensorflow/tensorflow/issues/48429"]}, {"number": 30652, "title": "Failed to convert tensorflow frozen graph to pbtxt file", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):1.14\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 4\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: No GPU used\r\n\r\n**Describe the current behavior**\r\nI want to extract pbtxt file given an input of tensorflow frozen inference graph. In order to do this I am using the below script :\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n#from google.protobuf import text_format\r\nfrom tensorflow.python.platform import gfile\r\n\r\ndef converter(filename): \r\n  with gfile.FastGFile(filename,'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    tf.import_graph_def(graph_def, name='')\r\n    tf.train.write_graph(graph_def, 'pbtxt/', 'protobuf.pbtxt', as_text=True)\r\n    print(graph_def)\r\n  return\r\n\r\n\r\n#converter('ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb')  # here you can write the name of the file to be converted\r\n# and then a new file will be made in pbtxt directory.\r\n\r\nconverter('ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb')\r\n```\r\n\r\nAs an example, I am using ssd mobilenet architecture. Using the above code I get the output as pbtxt but I cannot use it. For reference see the image below\r\n\r\n<img width=\"1440\" alt=\"Screenshot 2019-07-12 at 6 34 05 PM\" src=\"https://user-images.githubusercontent.com/17012391/61133402-06c6dd80-a4db-11e9-81e8-e38e02d329c9.png\">\r\n\r\n> RIGHT: Image of original pbtxt file of mobile-net architecture \r\n\r\n> LEFT: Image of pbtxt file obtained by using above script.\r\n\r\nWhen I use The official pbtxt on the RIGHT I get correct results. But, I do not get any prediction when I use LEFT pbtxt which I generated using above script\r\n\r\nI am using these predictions on open cv DNN module\r\n\r\n`tensorflowNet = cv2.dnn.readNetFromTensorflow('ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb', 'pbtxt/protobuf.pbtxt')`\r\n\r\nHow do I convert mobilenet frozen inference graph into proper pbtxt format so that I can get inference ?\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nIt is expected that the above script produces pbtxt file similar to the one provided by google for its mobilenet ssd model.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n#from google.protobuf import text_format\r\nfrom tensorflow.python.platform import gfile\r\n\r\ndef converter(filename): \r\n  with gfile.FastGFile(filename,'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    tf.import_graph_def(graph_def, name='')\r\n    tf.train.write_graph(graph_def, 'pbtxt/', 'protobuf.pbtxt', as_text=True)\r\n    print(graph_def)\r\n  return\r\n\r\n\r\n\r\n# and then a new file will be made in pbtxt directory.\r\n\r\nconverter('ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb')\r\n```\r\n\r\nReferences: https://gist.github.com/Arafatk/c063bddb9b8d17a037695d748db4f592\r\n", "comments": []}, {"number": 30651, "title": "Add one more flag for Bazel compatibility.", "body": "This also adds the 3 compatibility flags on windows builds, as I missed doing that in #30583", "comments": []}, {"number": 30650, "title": "Updated curl to 7.65.1", "body": "Updated curl to latest version 7.65.1\r\n\r\nSigned-off-by: Choong, Yin Thong <yin.thong.choong@intel.com>", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30650) for more info**.\n\n<!-- need_sender_cla -->", "@YinThong please sign CLA", "Hi,\r\nI already get cooperate CLA signed. How should i proceed with this PR?", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30650) for more info**.\n\n<!-- cla_yes -->", "Please also resolve conflicts.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30650) for more info**.\n\n<!-- ok -->", "This change is to update the CURL to latest version because version 7.60.0 have below CVE issue.\r\nCVE-2018-16842", "@YinThong can you check build failures ?", "@rthadur i no good on the CI flow and log. However, base on the log seem i am missing on urlapi.h on the include/curl. Already added in third_party/curl.BUILD", "[Patch resend]\r\nCurrent CI showing fail on MacOS. After checking the log because cannot found darwinssl.c. Curl already renamed darwinssl to sectransp. \r\nhttps://github.com/curl/curl/commit/dc5ac786d90d38d8ef5fd2c9dc211486204efad5\r\n", "@penpornk  OK. Already find out the fix and push it as a single commit. Let's see the CI result now.", "Still got a CI build failed (MacOS Python2 and CC). However, i cannot retrieve the detail info. Please advice.", "@YinThong All tests had passed in our latest test run (before I clicked approve and the tests were automatically rerun). So this should be unrelated to the PR. :) \r\n\r\nThe PR has been pulled, just having a problem with internal tests. I'll let you know if we need to make further modifications. "]}, {"number": 30649, "title": "tf.distribute.MirroredStrategy incompatible with tf.estimator training when defining tf.train.Scaffold with saver ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**: \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0/7.1\r\n- GPU model and memory: TitanXp 12G x 4\r\n\r\n**Describe the current behavior**\r\nWhen I use `tf.estimator` together with `tf.distribute.MirroredStrategy()` for single worker multiple GPUs training, I meet the following error if I try to define `tf.train.Scaffold` for `tf.estimator.EstimatorSpec()` to configure the saver parameters. Everything works fine JUST I remove the scafflold and the multiple gpu training for estimator is referred this [tutorial](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/guide/distribute_strategy.ipynb#scrollTo=_098zB3vVhuV).\r\n```\r\n...\r\nFile \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 126, in _require_cross_replica_or_default_context_extended\r\n    raise RuntimeError(\"Method requires being in cross-replica context, use \"\r\nRuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()\r\n```\r\n\r\n**Code to reproduce the issue**\r\nHere is my minimum snippet of code to reproduce this error.\r\n```python\r\nimport tensorflow as tf \r\nfrom tensorflow.python.keras.applications import MobileNetV2\r\n\r\nl = tf.keras.layers\r\n\r\ndef input_fn(): \r\n    dataset = tf.data.Dataset.from_tensor_slices({\"feature\": tf.random_normal(shape=(1, 224, 224, 3), dtype=tf.float32),\r\n                                                  \"label\": tf.random.uniform(shape=[1], minval=0, maxval=2, dtype=tf.int32)})\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(2)\r\n    return dataset\r\n\r\ndef model_fn(features, labels, mode):\r\n    input_tensor = features['feature']\r\n    label = features['label']\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        model = MobileNetV2(input_shape=(224, 224, 3), classes=2, weights=None)\r\n        output = model(input_tensor)\r\n\r\n        loss = tf.losses.sparse_softmax_cross_entropy(label, output)\r\n        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss, global_step=tf.train.get_global_step())\r\n        # define scaffold\r\n        saver = tf.train.Saver(\r\n            sharded=True,\r\n            keep_checkpoint_every_n_hours=1,\r\n            save_relative_paths=True)\r\n        tf.add_to_collection(tf.GraphKeys.SAVERS, saver)\r\n        scaffold = tf.train.Scaffold(saver=saver)\r\n        # remove scaffold this code could work\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, scaffold=scaffold)\r\n\r\n# multiple gpu configuration for estimator    \r\ndevices = [\"/device:GPU:0\", \"/device:GPU:1\"]\r\nstrategy = tf.distribute.MirroredStrategy(devices=devices)\r\n\r\nconfig = tf.estimator.RunConfig(model_dir=\"test_multi_gpu\",)\r\n                                train_distribute=strategy)\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\nestimator.train(input_fn=input_fn, steps=1000)\r\n```\r\n\r\n**Other info / logs**\r\nThe total error log is here:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_multi_gpus.py\", line 45, in <module>\r\n    estimator.train(input_fn=input_fn, steps=1000)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1156, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1219, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1299, in _actual_train_model_distributed\r\n    self.config))\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1555, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 693, in _call_for_each_replica\r\n    fn, args, kwargs)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 195, in _call_for_each_replica\r\n    coord.join(threads)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 911, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1146, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"test_multi_gpus.py\", line 34, in model_fn\r\n    save_relative_paths=True)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 825, in __init__\r\n    self.build()\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 837, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 875, in _build\r\n    build_restore=build_restore)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 497, in _build_internal\r\n    per_device = self._GroupByDevices(saveables)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 404, in _GroupByDevices\r\n    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 404, in <genexpr>\r\n    pydev.canonical_name(spec.tensor.device) for spec in saveable.specs)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object.py\", line 52, in tensor\r\n    return self._tensor() if callable(self._tensor) else self._tensor\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\", line 1358, in tensor\r\n    return strategy.extended.read_var(sync_on_read_variable)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 768, in read_var\r\n    return replica_local_var._get_cross_replica()  # pylint: disable=protected-access\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\", line 1424, in _get_cross_replica\r\n    axis=None)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 832, in reduce\r\n    return super(StrategyV1, self).reduce(reduce_op, value, axis)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 552, in reduce\r\n    _require_cross_replica_or_default_context_extended(self._extended)\r\n  File \"/data/fanzong/miniconda3/envs/tf_cuda10/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 126, in _require_cross_replica_or_default_context_extended\r\n    raise RuntimeError(\"Method requires being in cross-replica context, use \"\r\nRuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()\r\n```\r\n", "comments": ["This error seems to be due to a redundant initialization of Saver when using MirroredStrategy for multi-gpu training since TF estimator would initialize `Saver` based on its `RunConfig`. In this case, commenting saver inside the EstimatorSpec could solve this problem. BUT if anyone has some ideas about the detailed mechanism of this conflict. ", "I also meet this problem, have you had any idea of solving it? @CasiaFan ", "I fix this bug when training mobilenetv2_ssd in tensorflow object detection API with TF1.15, by add `inplace_batchnorm_update: true` in pipeline.config. I hope this can help anyone else.", "> This error seems to be due to a redundant initialization of Saver when using MirroredStrategy for multi-gpu training since TF estimator would initialize `Saver` based on its `RunConfig`. In this case, commenting saver inside the EstimatorSpec could solve this problem. BUT if anyone has some ideas about the detailed mechanism of this conflict.\r\n\r\n-------------------------------------------------------------------------------------------\r\nhahaha kaobi\r\nI also meet it \r\nremove scaffold and saver in EstimatorSpec can fix it \r\nit is caused by keras + estimator haha", "This [thread](https://github.com/tensorflow/models/issues/5421#issuecomment-510821659) may help. I'm not sure whether you are meeting this error in tf models repo. @ZhuLingfeng1993 ", "\"What's supported now?\r\nIn TF 2.0 release, there is limited support for training with Estimator using all strategies except TPUStrategy. **_Basic training and evaluation should work, but a number of advanced features such as scaffold do not yet work._** There may also be a number of bugs in this integration.\"\r\n\r\nQutoed from link below:\r\nhttps://www.tensorflow.org/guide/distributed_training#whats_supported_now_3\r\n@CasiaFan", "@wf-hit Yep, maybe it's time to turn to the Keras type. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30649\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30649\">No</a>\n"]}, {"number": 30648, "title": "Improving the documentation of learning_rate_scheduler.py", "body": "I have removed the 'global_step' from the usage example of various learning rate scheduler as it is no longer in use.\r\n \r\nThis is in reference to https://github.com/tensorflow/tensorflow/issues/30437\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30648) for more info**.\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30648) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30648) for more info**.\n\n<!-- ok -->"]}, {"number": 30647, "title": "Unresolved External Symbols Windows C++Tensorflow v1.14.0", "body": "### System information\r\n- Windows 10\r\n- Built from source\r\n- Tensorflow v1.14.0\r\n- Bazel v0.25.2\r\n- MSVC 14.16.27023\r\n- CUDA 10.0 Cudnn 7.6.0\r\n- GTX 1060\r\n\r\n### Describe the problem\r\nI have built tenosrflow c++ library using the following commands \r\n\r\n```bash\r\nbazel build //tensorflow:tensorflow_cc.lib\r\n```\r\n\r\n```bash\r\nbazel build //tensorflow:tensorflow_cc.dll\r\n```\r\n\r\nhowever, after including the necessary headers and building/compiling my c++ code, I got the following 20 unresolved externals. \r\n\r\n### Source code / logs\r\nIncluded tensorflow headers: \r\n```\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/image_ops.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/str_util.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n\r\n```\r\nMissing external symbols:\r\n\r\n```\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Operation::Operation(class tensorflow::Node *)\" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@Z) referenced in function \"public: __cdecl tensorflow::Input::Input(struct tensorflow::Input::Initializer const &)\" (??0Input@tensorflow@@QEAA@AEBUInitializer@01@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z) referenced in function \"public: __cdecl tensorflow::Input::Input(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: class tensorflow::Status __cdecl tensorflow::Scope::ToGraphDef(class tensorflow::GraphDef *)const \" (?ToGraphDef@Scope@tensorflow@@QEBA?AVStatus@2@PEAVGraphDef@2@@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"private: class tensorflow::Scope __cdecl tensorflow::Scope::WithOpNameImpl(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?WithOpNameImpl@Scope@tensorflow@@AEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z) referenced in function \"public: class tensorflow::Scope __cdecl tensorflow::Scope::WithOpName<char const *>(char const *)const \" (??$WithOpName@PEBD@Scope@tensorflow@@QEBA?AV01@PEBD@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"class tensorflow::Output __cdecl tensorflow::ops::Const(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::DecodeBmp::DecodeBmp(class tensorflow::Scope const &,class tensorflow::Input)\" (??0DecodeBmp@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::DecodeGif::DecodeGif(class tensorflow::Scope const &,class tensorflow::Input)\" (??0DecodeGif@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::DecodeJpeg::DecodeJpeg(class tensorflow::Scope const &,class tensorflow::Input,struct tensorflow::ops::DecodeJpeg::Attrs const &)\" (??0DecodeJpeg@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::DecodePng::DecodePng(class tensorflow::Scope const &,class tensorflow::Input,struct tensorflow::ops::DecodePng::Attrs const &)\" (??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::ResizeBilinear::ResizeBilinear(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0ResizeBilinear@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::ExpandDims::ExpandDims(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0ExpandDims@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::Placeholder::Placeholder(class tensorflow::Scope const &,enum tensorflow::DataType)\" (??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::Squeeze::Squeeze(class tensorflow::Scope const &,class tensorflow::Input)\" (??0Squeeze@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::Cast::Cast(class tensorflow::Scope const &,class tensorflow::Input,enum tensorflow::DataType)\" (??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::Div::Div(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Div@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::Subtract::Subtract(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Subtract@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z) referenced in function \"class tensorflow::Status __cdecl ReadTensorFromImageFile(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,int,int,float,float,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)\" (?ReadTensorFromImageFile@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@HHMMPEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ) referenced in function \"class tensorflow::Status __cdecl LoadGraph(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::Session,struct std::default_delete<class tensorflow::Session> > *)\" (?LoadGraph@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VSession@tensorflow@@U?$default_delete@VSession@tensorflow@@@std@@@4@@Z)\r\n1>main.obj : error LNK2019: unresolved external symbol \"class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)\" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z) referenced in function \"class tensorflow::Status __cdecl LoadGraph(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::Session,struct std::default_delete<class tensorflow::Session> > *)\" (?LoadGraph@@YA?AVStatus@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VSession@tensorflow@@U?$default_delete@VSession@tensorflow@@@std@@@4@@Z)\r\n1>C:\\Sources\\Projects\\UDAE_cc_interface\\x64\\Debug\\UDAE_cc_interface.exe : fatal error LNK1120: 20 unresolved externals\r\n```\r\n\r\nI'm using Microsoft Visual Studio and linked tensorflow_cc.lib library and included the headers .. \r\nWhat are the other libraries I should link against other than the generated tensorflow c++ library, protobuf, abseil and eigen? I tried to export these missing symbols but didn't work .. \r\n", "comments": ["Please, go through the issue #24885 and see if that helps you. Thanks!", "I went through different related open issues including the one you referenced, but it didn\u2019t work out. I added the missing unreferenced symbols to a def file then exported them to the .lib and .dll files but also didn\u2019t work out. \r\n\r\nIf I use CMake to try to link missing dependencies, I must fix a lot of errors and wrong paths because it is not supported... \r\n\r\nCan you list the necessary libraries and headers I should link against other than the generated c++ library? ", "I encountered the same issue. The solution/workaround for version 1.14 version, probable works for 2.0 as well, is to modify  tensorflow\\tools\\def_file_filter\\def_file_filter.py.tpl to add those missing symbols. For instance around line 130.\r\n\r\nThe long term solution is to modify the rules in this file to include more critical symbols.", "I added the unresolved externals in the `tensorflow\\tools\\def_file_filter\\def_file_filter.py.tpl` file:\r\n\r\n\r\n    # Header for the def file.\r\n    if args.target:\r\n      def_fp.write(\"LIBRARY \" + args.target + \"\\n\")\r\n    def_fp.write(\"EXPORTS\\n\")\r\n    def_fp.write(\"\\t ??1OpDef@tensorflow@@UEAA@XZ\\n\")\r\n\r\n    def_fp.write(\"\\t ??0Operation@tensorflow@@QEAA@PEAVNode@1@@z\\n\")\r\n    def_fp.write(\"\\t ??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@z\\n\")\r\n    def_fp.write(\"\\t ??1Scope@tensorflow@@QEAA@XZ\\n\")\r\n    def_fp.write(\"\\t ?NewRootScope@Scope@tensorflow@@sa?AV12@XZ\\n\")\r\n    def_fp.write(\"\\t ?ToGraphDef@Scope@tensorflow@@qeba?AVStatus@2@PEAVGraphDef@2@@z\\n\")\r\n    def_fp.write(\"\\t ?WithOpNameImpl@Scope@tensorflow@@aeba?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@z\\n\")\r\n    def_fp.write(\"\\t ?Const@ops@tensorflow@@ya?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@z\\n\")\r\n    def_fp.write(\"\\t ??0DecodeBmp@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@z\\n\")\r\n    def_fp.write(\"\\t ??0DecodeGif@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@z\\n\")\r\n    def_fp.write(\"\\t ??0DecodeJpeg@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@z\\n\")\r\n    def_fp.write(\"\\t ??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@z\\n\")\r\n    def_fp.write(\"\\t ??0ResizeBilinear@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\\n\")\r\n    def_fp.write(\"\\t ??0ExpandDims@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\\n\")\r\n    def_fp.write(\"\\t ??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@z\\n\")\r\n    def_fp.write(\"\\t ??0Squeeze@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@z\\n\")\r\n    def_fp.write(\"\\t ??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@z\\n\")\r\n    def_fp.write(\"\\t ??0Div@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\\n\")\r\n    def_fp.write(\"\\t ??0Subtract@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\\n\")\r\n    def_fp.write(\"\\t ??0SessionOptions@tensorflow@@QEAA@XZ\\n\")\r\n    def_fp.write(\"\\t ?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@z\\n\")\r\n\r\nwhich are the 20 unresolved external symbols, but while building the library it threw the following error:\r\n\r\n\r\n ```\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Professional/VC/Tools/MSVC/14.16.27023/bin/HostX64/x64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -defaultlib:advapi32.lib -DEFAULTLIB:advapi32.lib -ignore:4221 -ignore:4221 -ignore:4221 -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll-2.params /DEF:bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_filtered_def_file.def /ignore:4070\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow_cc.dll.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow_cc.dll.exp\r\nlibmemory_optimizer.a(memory_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nlibpin_to_host_optimizer.a(pin_to_host_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nlibutils.a(utils.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nlibcollective_param_resolver_distributed.a(collective_param_resolver_distributed.o) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function \"public: __cdecl tensorflow::CollGroupParams::CollGroupParams(void)\" (??0CollGroupParams@tensorflow@@QEAA@XZ)\r\nlibbatch_kernels.lo(batch_kernels.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nlibcaptured_function.a(captured_function.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nlibarithmetic_optimizer.a(arithmetic_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nlibtfprof_show.a(tfprof_show.o) : warning LNK4217: locally defined symbol TF_NewStatus imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\nlibtfprof_show.a(tfprof_show.o) : warning LNK4217: locally defined symbol TF_DeleteStatus imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\nlibtfprof_show.a(tfprof_show.o) : warning LNK4217: locally defined symbol TF_GetCode imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\nlibtfprof_show.a(tfprof_show.o) : warning LNK4217: locally defined symbol TF_Message imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\nlibpin_to_host_optimizer.a(pin_to_host_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nlibutils.a(utils.o) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nlibarithmetic_optimizer.a(arithmetic_optimizer.o) : warning LNK4217: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported in function \"private: bool __cdecl tensorflow::grappler::`anonymous namespace'::ReorderCastLikeAndValuePreserving::NodeIsOnCpuOrGpu(class tensorflow::NodeDef const *)const \" (?NodeIsOnCpuOrGpu@ReorderCastLikeAndValuePreserving@?A0x1806e22c@grappler@tensorflow@@AEBA_NPEBVNodeDef@4@@Z)\r\nlibauto_mixed_precision.a(auto_mixed_precision.o) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nliblayout_optimizer.a(layout_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nlibmemory_optimizer.a(memory_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nlibcudnn_plugin.lo(cuda_dnn.o) : warning LNK4217: locally defined symbol ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z (public: class stream_executor::Stream & __cdecl stream_executor::Stream::ThenBlasGemm(enum stream_executor::blas::Transpose,enum stream_executor::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class stream_executor::DeviceMemory<float> const &,int,class stream_executor::DeviceMemory<float> const &,int,float,class stream_executor::DeviceMemory<float> *,int)) imported in function \"public: virtual bool __cdecl stream_executor::gpu::CudnnSupport::DoMatMul(class stream_executor::Stream *,class stream_executor::DeviceMemory<float> const &,class stream_executor::DeviceMemory<float> const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::DeviceMemory<float> *)\" (?DoMatMul@CudnnSupport@gpu@stream_executor@@UEAA_NPEAVStream@3@AEBV?$DeviceMemory@M@3@1AEBVBatchDescriptor@dnn@3@2PEAV53@@Z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@z\" (??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0DecodeBmp@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@z\" (??0DecodeBmp@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0DecodeGif@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@z\" (??0DecodeGif@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0DecodeJpeg@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@z\" (??0DecodeJpeg@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@z\" (??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@z\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0Operation@tensorflow@@QEAA@PEAVNode@1@@z\" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@z\" (??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0Squeeze@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@z\" (??0Squeeze@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"?Const@ops@tensorflow@@ya?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@z\" (?Const@ops@tensorflow@@ya?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"?NewRootScope@Scope@tensorflow@@sa?AV12@XZ\" (?NewRootScope@Scope@tensorflow@@sa?AV12@XZ)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@z\" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"?ToGraphDef@Scope@tensorflow@@qeba?AVStatus@2@PEAVGraphDef@2@@z\" (?ToGraphDef@Scope@tensorflow@@qeba?AVStatus@2@PEAVGraphDef@2@@z)\r\nlibtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"?WithOpNameImpl@Scope@tensorflow@@aeba?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@z\" (?WithOpNameImpl@Scope@tensorflow@@aeba?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@z)\r\nbazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll : fatal error LNK1120: 14 unresolved externals\r\nTarget //tensorflow:tensorflow_cc.dll failed to build\r\n```\r\n\r\nIt went from 20 to 14 unresolved externals. \r\nIt didn't work out .. ", "Having the same issue here: https://github.com/tensorflow/tensorflow/issues/30552", "Hi, I have been experiencing the same problems, has anyone found a solution?", "I am also having the same issue, the solution with the patch and editing `def_file_filter.py.tpl` file only seems to fix the problem in 1.13. Has anyone found a workaround for 1.14?", "I can confirm that, for the system described in [0], a simple application like [1] runs when including the missing symbols in `def_file_filter.py.tpl` and building `//tensorflow:libtensorflow_cc.so` again. For this case, I had to include the lines shown in [2] after `def_fp.write(\"\\t ??1OpDef@tensorflow@@UEAA@XZ\\n\")`, however, more symbols will be needed when developing more complex applications.\r\n\r\n@gunan, it would be great if all TensorFlow symbols were exported without the need of manually including them in the `def_file_filter.py.tpl` file!\r\n\r\n\r\n[0] **System information**\r\n```\r\nWindows 10\r\nBuilt from source\r\nBranch r2.0\r\nBazel v0.24.1\r\nVisual studio build tools v15.9.28307.812\r\nCUDA support No\r\n```\r\n\r\n[1] **Sample code**\r\n```\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/cc/ops/math_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n\r\nint main() {\r\n\tusing namespace tensorflow;\r\n\tusing namespace tensorflow::ops;\r\n\tScope root = Scope::NewRootScope();\r\n\t //Matrix A = [3 2; -1 0]\r\n\tauto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });\r\n\t// Vector b = [3 5]\r\n\tauto b = Const(root, { {3.f, 5.f} });\r\n\t// v = Ab^T\r\n\tauto v = MatMul(root.WithOpName(\"v\"), A, b, MatMul::TransposeB(true));\r\n\tstd::vector<Tensor> outputs;\r\n\tClientSession session(root);\r\n\t// Run and fetch v\r\n\tTF_CHECK_OK(session.Run({ v }, &outputs));\r\n\t// Expect outputs[0] == [19; -3]\r\n\tLOG(INFO) << outputs[0].matrix<float>();\r\n\treturn 0;\r\n}\r\n```\r\n\r\n[2] **Added lines to `def_file_filter.py.tpl`:**\r\n```\r\n    def_fp.write(\"\\t ??1Scope@tensorflow@@QEAA@XZ\\n\")\r\n    def_fp.write(\"\\t ?NewRootScope@Scope@tensorflow@@SA?AV12@XZ\\n\")\r\n    def_fp.write(\"\\t ??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z\\n\")\r\n    def_fp.write(\"\\t ?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z\\n\")\r\n    def_fp.write(\"\\t ??0MatMul@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1AEBUAttrs@012@@Z\\n\")\r\n    def_fp.write(\"\\t ??0ClientSession@tensorflow@@QEAA@AEBVScope@1@@Z\\n\")\r\n    def_fp.write(\"\\t ??1ClientSession@tensorflow@@QEAA@XZ\\n\")\r\n    def_fp.write(\"\\t ?Run@ClientSession@tensorflow@@QEBA?AVStatus@2@AEBV?$vector@VOutput@tensorflow@@V?$allocator@VOutput@tensorflow@@@std@@@std@@PEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@5@@Z\\n\")\r\n    def_fp.write(\"\\t ?WithOpNameImpl@Scope@tensorflow@@AEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z\\n\")\r\n    \r\n```", "@stbnps We initially tried exporting every single symbol.\r\nBut that makes TF unlinkable.\r\nWindows has a limit on the number of shared library symbol exports, which is 2^16\r\nhttps://social.msdn.microsoft.com/Forums/vstudio/en-US/caec290c-31cd-4133-b11c-1c7997ffbecb/how-to-split-a-dll-into-several-dlls-to-overcome-import-library-symbol-count-restriction?forum=vclanguage\r\nTherefore, the ask to export everything without def_file_filter is simply infeasible.\r\n\r\nI would like to point out that some of the additions you have there are also not OK, and wont link on linux. For example, MatMul and Const op symbols are exported from a different library (or even hidden by now, not sure). Mostly the header files under `tensorflow/core/framework`, plus eigen and stream executor headers are OK to be linked from custom op libraries.\r\nAny code under `tensorflow/core/ops` and `tensorflow/core/kernels` is definitely not supposed to be used by custom ops.", "I had a very similar error and I was able to fix this by manually building tensor flow 1.14 with Bazel, and using the TF_EXPORT macro on symbols that I wanted to be visible. \r\n\r\nI wrote down my solution here: https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows\r\n\r\nIt starts at the very beginning of the process of building and linking Tensorflow on Windows, so you may need to scroll down a while before you get to the part you are interested in (fixing the missing symbols)\r\nI am still trying to improve this document so if you find it hard to follow, or needs improvement you can email ashley.tharp@gmail.com to correspond. ", "Thank you very much for your guide @sitting-duck !\r\nI am happy to accept any changes you propose to make this better into TF.\r\n", "> Thank you very much for your guide @sitting-duck !\r\n> I am happy to accept any changes you propose to make this better into TF\r\n\r\nAre you able to apply the fix? It is a lot if info in the guide, but basically all you have to do is add TF_EXPORT where you need it and recompile. \r\n\r\nPlease let me know if it worked for you, or if my directions are not clear. \r\n\r\nThe process should not take too long. \r\nYou can also reach out to my gmail, ashley.tharp@gmail.com and I can try to help you apply the steps.\r\n\r\n", "> > Thank you very much for your guide @sitting-duck !\r\n> > I am happy to accept any changes you propose to make this better into TF\r\n> \r\n> Are you able to apply the fix? It is a lot if info in the guide, but basically all you have to do is add TF_EXPORT where you need it and recompile.\r\n> \r\n> Please let me know if it worked for you, or if my directions are not clear.\r\n> \r\n> The process should not take too long.\r\n> You can also reach out to my gmail, [ashley.tharp@gmail.com](mailto:ashley.tharp@gmail.com) and I can try to help you apply the steps.\r\n\r\n\r\n\r\n> > Thank you very much for your guide @sitting-duck !\r\n> > I am happy to accept any changes you propose to make this better into TF\r\n> \r\n> Are you able to apply the fix? It is a lot if info in the guide, but basically all you have to do is add TF_EXPORT where you need it and recompile.\r\n> \r\n> Please let me know if it worked for you, or if my directions are not clear.\r\n> \r\n> The process should not take too long.\r\n> You can also reach out to my gmail, [ashley.tharp@gmail.com](mailto:ashley.tharp@gmail.com) and I can try to help you apply the steps.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThank you so much, I can solve the error ! *^^*\r\nBy the way, You'd better tell people exactly where the file is located.\r\nexample : \r\nC:\\tensorflow-master\\tensorflow\\core\\public\\session.h\r\n\r\n\r\n\r\n\r\n", "> Thank you so much, I can solve the error ! _^^_\r\n> By the way, You'd better tell people exactly where the file is located.\r\n> example :\r\n> C:\\tensorflow-master\\tensorflow\\core\\public\\session.h\r\n\r\n@eehoeskrap I went ahead and applied your suggestion for session.h. I did not do this for every file people are saying they can't find the symbol for, but perhaps as I go along I can make exact error message to solution stack overflow QAs. This is great suggestion, thanks for telling me.", "Same Issue! TF 1.14 Unresolved External Symbols. ", "@sheirving  are you able to fix your problem using this guide?\r\nI wrote down my solution here: https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows\r\n\r\nI try to clear my inbox every day. You can contact me at ashley.tharp@gmail.com if you need help using the guide.\r\n", "> @sheirving are you able to fix your problem using this guide?\r\n> I wrote down my solution here: https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows\r\n> \r\n> I try to clear my inbox every day. You can contact me at [ashley.tharp@gmail.com](mailto:ashley.tharp@gmail.com) if you need help using the guide.\r\n\r\nThankyou very much. This problem has troubled me for several days. I  have tried to adopt your solution, but it doesn't seem to work for me (system information: TF r1.14, Windows: 10.0, Bazel: 0.24.1, Bazel build with 25 unresolved external symblos). Fortunately, I finally solved the problem with above mentioned \"Added lines to def_file_filter.py.tpl\".\r\n\r\nThankyou very much!", "> @stbnps We initially tried exporting every single symbol.\r\n> But that makes TF unlinkable.\r\n> Windows has a limit on the number of shared library symbol exports, which is 2^16\r\n> https://social.msdn.microsoft.com/Forums/vstudio/en-US/caec290c-31cd-4133-b11c-1c7997ffbecb/how-to-split-a-dll-into-several-dlls-to-overcome-import-library-symbol-count-restriction?forum=vclanguage\r\n> Therefore, the ask to export everything without def_file_filter is simply infeasible.\r\n> \r\n> I would like to point out that some of the additions you have there are also not OK, and wont link on linux. For example, MatMul and Const op symbols are exported from a different library (or even hidden by now, not sure). Mostly the header files under `tensorflow/core/framework`, plus eigen and stream executor headers are OK to be linked from custom op libraries.\r\n> Any code under `tensorflow/core/ops` and `tensorflow/core/kernels` is definitely not supposed to be used by custom ops.\r\n\r\n**Build Configuration:**\r\n\r\nWindows 10, VS2017\r\nBazel 0.25.2\r\nTensorflow; 1.14, cpu\r\nJIT: No\r\nRcom: No\r\nCUDA:No\r\ninline override for less compile time: No\r\n\r\nMy cpu build is successfull. I have linked the temsorflow.lib(this is for c++, I see tensorflow_cc.lib for c++ evrery other thread, followed @sitting-duck steps ) in Linker settings and other required directory paths .\r\n@gunan So what is the workaround for the unresolved external symbols in tf 1.14 c++ ?\r\n@sitting-duck I tried your way of TF_EXPORTS and probably screwed up the build. Did a fresh install. I think finding filenames each time for someone who has 50 unresolved external symbols is painful. Any suggestions on that ?", "@mohapatras I have seen in this thread\r\nhttps://github.com/tensorflow/tensorflow/issues/23542#issuecomment-564148147\r\nanother developer mentioning a way he was able to do it using def_file_filter.py. \r\nI have not tested this method myself so I cannot confirm or deny it, but perhaps this thread will give you some hint you can use to find that solution. \r\n\r\nAs for myself, I read that symbol name collisions cause a crash during run time and they are very hard to debug and chase down because there are many many many symbols in many third party libs. \r\nI want to avoid that, so I chose to manually expose only what I use one by one since I also include many many other third party .libs in my projects. \r\n\r\nI hope that helps explain :) Good luck!", "> > @sheirving are you able to fix your problem using this guide?\r\n> > I wrote down my solution here: https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows\r\n> > I try to clear my inbox every day. You can contact me at [ashley.tharp@gmail.com](mailto:ashley.tharp@gmail.com) if you need help using the guide.\r\n> \r\n> Thankyou very much. This problem has troubled me for several days. I have tried to adopt your solution, but it doesn't seem to work for me (system information: TF r1.14, Windows: 10.0, Bazel: 0.24.1, Bazel build with 25 unresolved external symblos). Fortunately, I finally solved the problem with above mentioned \"Added lines to def_file_filter.py.tpl\".\r\n> \r\n> Thankyou very much!\r\n\r\nHi @sheirving \r\n\r\nI have the same problem as unresolved symbols. Then I tried to add those lines to def_file_filter.py.tpl as said above. But I am getting dll build error when I compile for the tensorflow.dll using Bazel. I wonder, how can you manage this problem. \r\nPlease let us know. Thanks.", "All you want to have on Windows, I mean - fast way to rebuild TF's import library with all missing symbols is here https://github.com/tensorflow/tensorflow/issues/23542. ", "@hashishoya  \r\nIs this still an issue", "> @hashishoya\r\n> Is this still an issue\r\n\r\nwell, I haven't tried this anymore as I shifted to other C++ inference APIs like TensorRT, which have their own problems \ud83d\ude06 but easier to manage and solve. \r\n\r\nAnw, I guess the provided @sitting-duck 's solution might be working well, so I'm closing this issue and would re-open it later if needed .. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30647\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30647\">No</a>\n", "I am trying to add TF_EXPORT into array_ops.h file from tensorflow 1.15 version after bazel build success.\r\nBut the file has been generated by bazel, and is read only attribute, so I forced to make it writable and added TF_EXPORT.\r\nAnd I tried to compile array_ops.cc file again by deleting the array_ops.o object file, those actions have been done well and I could get a new tensorflow_cc.dll well. But I could find that the array_ops.cc file has been changed back to original one, so there is no TF_EXPORT and the new DLL has no symbols which I want.\r\n\r\nHow can I add the TF_EXPORT macro into a file which will be generated by bazel at each compilation time?    \r\n\r\n\r\n\r\n", "I believe you cannot fix those symbol this way.\n\nThere is a Python script way to expose symbols I am vaguely aware of that\nyou may be able to try. I have not tried this myself.\n\n\nOn Thu, Jul 16, 2020 at 6:01 PM jeffhwang02 <notifications@github.com>\nwrote:\n\n> I am trying to add TF_EXPORT into array_ops.h file from tensorflow 1.15\n> version after bazel build.\n> But the file has been generated by bazel, and is read only attribute, so I\n> forced to make it writable and add TF_EXPORT.\n> And I tried to compile array_ops.cc file again by deleting the array_ops.o\n> object file, those actions have been done well and I could get a new\n> tensorflow_cc.dll well. But I could find that the array_ops.cc file has\n> been changed to original one back, so there is no TF_EXPORT and the new DLL\n> has no symbols of it.\n>\n> How can I add the TF_EXPORT macro into a file which will be generated by\n> bazel at each compilation time?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30647#issuecomment-659722211>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAJ23ZVGLNB55QB6QVMQNZLR36BENANCNFSM4ICJCEGA>\n> .\n>\n"]}, {"number": 30646, "title": "Filling shuffle buffer, this happens before the beginning of every epoch. Is there a way to avoid it?", "body": "I am following [this tutorial](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches), however I am using my own dataset stored as a csv file consisting 1265800 elements. My question is:\r\nBefore the beginning of every epoch, it shows `Filling up shuffle buffer (this may take a while)`. I think it means that it is shuffling the dataset before feeding it to the model for training. Is there a way to not to shuffle this before every epoch because it takes time before proceeding to the next epoch.\r\nAlso, this behaviour is not seen if I run the example model on Google Colab. I also read [this issue](https://github.com/tensorflow/tensorflow/issues/29957) but didn't help.\r\nThank you for your inputs.", "comments": ["Shuffling is required to Train the Model with as many different Samples (Versatile data) as possible. Since your data is huge, you can use [tf.data.experimental.shuffle_and_repeat](https://www.tensorflow.org/api_docs/python/tf/data/experimental/shuffle_and_repeat), which combines the best of good performance and varied Samples. Refer this [TF Link](https://www.tensorflow.org/guide/performance/datasets#repeat_and_shuffle) for more info.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi @rmothukuru , thank you for your suggestion. Please excuse me I could not test it earlier and respond here. \r\nI followed your suggestion and instead of using `Dataset.shuffle` as used in the [link](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches), I am using `Dataset.apply(tf.data.experimental.shuffle_and_repeat())` from [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/shuffle_and_repeat) but the problem still is there. What still happening is, it is still filling up the buffer and for that it is consuming the CPU RAM memory which after sometime fills up and everything stops responding then I have to kill every process. My questions are,\r\nIs this a normal behaviour and if so, then how can one shuffle a huge dataset like this? Or do I need more CPU RAM so that it can be used for filling up the shuffle buffer?", "I tried to train the model without using `shuffle()` function. It does not fill up the buffer but on the start of the model training, it throws `(GPU_0_bfc) ran out of memory trying to allocate 1.37GiB (rounded to 1467428096)`. I am not sure on how to troubleshoot this problem, I tried various batch sizes also like 1,2, 128,256,512, 1024.\r\n**UPDATE**\r\nAs a workaround, I avoided GPU memory error by lowering the embedding dimension value from 1000 to 100 in Embedding layer of the model. I have raised an [issue](https://github.com/tensorflow/tensorflow/issues/31162) regarding this behaviour.\r\nAlthough, I still can not use `shuffle()` function as it is filling up the CPU memory. ", "Hello @rmothukuru , any updates on the issue? Because, I still can not shuffle my data before feeding it to the mode for training. :(", "@rishabhsahrawat Is this still an issue.? I noticed you have opened multiple issues on this or similar topic. If this was covered in other opened issues, then please close this issue. Thanks!", "Yes, this has been answered [here](https://github.com/tensorflow/tensorflow/issues/32376). ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30646\">No</a>\n"]}, {"number": 30645, "title": "How to convert TFmodel to TFLite Model????", "body": "System information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): Command Line\r\nTensorFlow version: 1.14.0\r\nPython version: 3.7.3\r\nInstalled using virtualenv? pip? conda?: conda\r\nBazel version (if compiling from source): 0.28.0\r\nGCC/Compiler version (if compiling from source): No\r\nCUDA/cuDNN version: NO\r\nGPU model and memory:NO\r\n\r\nI found different ways to convert Tensorflow model to Tensorflow Lite model\r\nusing Bazel\r\nusing Toco\r\nusing tflite_convert.py\r\n\r\nAll these method give error\r\nI need a proper suggestion to convert TFModel to TFLiteModel\r\nThe error says \r\n\r\n**tflite_convert.py: error: --input_arrays and --output_arrays are required with --graph_def_file**\r\n\r\n**ValueError: Invalid tensors 'input' were found.**\r\n\r\nThank you\r\n\r\n", "comments": ["i have the same problem plz anyone help!", "You need to specify name of input layer, for example I used Netron to know it", "> You need to specify name of input layer, for example I used Netron to know it\r\n\r\nI installed Netron, It asked for model to use. I selected my model. It is displaying a big graph.\r\nI m not able find a solution. How to use the value in my code\r\n", "I updated the code with \r\n--input_array=image_tensor\u2014output_array=output\r\nNow a new error has arrived \r\n**tflite_convert.py: error: the following arguments are required: --output_file**", "You have to give the output file name with path in the --output_file parameter\r\nEg: --output_file=/Documents/my_Lite_graph.tflite", "> You have to give the output file name with path in the --output_file parameter\r\n> Eg: --output_file=/Documents/my_Lite_graph.tflite\r\n \r\nI Solved it thank you KauthamMurugan\r\n\r\n", "**toco: error: --input_arrays and --output_arrays are required with --graph_def_file**\r\n\r\nthis is my code\r\n\r\ntoco --graph_def_file=/Users/deepak/Documents/Tensorflow/workspace/training_demo/trained-inference-graphs/output_inference_graph_v1/frozen_inference_graph.pb--input_array=Identity_6 \u2014output_array=TensorArrayWriteV3 --output_file=/Users/deepak/Desktop/TFlite/frozen_inference_graph.tflite --output_format=TFLITE --input_shape=1,224,224,3--inference_type=FLOAT --inference_input_type=FLOAT\r\n\r\nI need Help!!!!", "```toco``` is deprecated can you please use ```tflite_convert``` instead?. See https://www.tensorflow.org/lite/convert/cmdline_examples#command-line_tools_", "Hi, Thanks for the feedback\r\n\r\nI'm trying with tflite_convert.py \r\nwith this code, kindly verify my code. Even this gives me error\r\n\r\n**cd /Applications/anaconda3/envs/tensorflow_cpu/lib/python3.7/site-packages/tensorflow/lite/python\r\n\r\npython ./tflite_convert.py --graph_def_file=/Users/deepak/Documents/Tensorflow/workspace/training_demo/trained-inference-graphs/output_inference_graph_v1/frozen_inference_graph.pb --input_array=input\u2014output_array= final_result --output_file=/Users/deepak/Desktop/TFlite/frozen_inference_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,300,300,3 --inference_type=FLOAT --input_data_type=FLOAT**\r\n\r\n**ValueError: Invalid tensors 'input' were found.**\r\n", "Hi,\r\n\r\nI'm wondering if the input tensor name you specified is correct. Could you try the tf graph summary tool[1] which will list the input/output tensors of a graph?\r\n\r\n[1]https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/summarize_graph_main.cc", "Hi, I am able to convert tensorflow model to tensorflowlite model through this code\r\n\r\n**tflite_convert --output_file=/Users/deepak/Desktop/Quantize/undertaker.tflite --graph_def_file=/Users/deepak/Desktop/Quantize/tflite_graph.pb --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --input_shapes=1,300,300,3 --inference_type=FLOAT  --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops**\r\n\r\nMy model is **55.3 Mb** size\r\nAfter this process my model turn into .tflite file\r\nThe size of that file is **54.9 Mb**\r\nI need to **Quantize my model!!!!**\r\n\r\nI tried to add **--inference_type= QUANTIZED_UINT8**\r\nThis doesn't work for me but \r\n**--inference_type= FLOAT**\r\nthis one is working.....\r\n\r\n**<<<<**HOW TO CONVERT MY TENSORFLOW MODEL TO TENSORFLOW LITE MODEL BY QUANTIZING**>>>>**\r\n\r\nI need to reduce my model's size as it is been trained to use for mobile device........\r\n\r\n**Thanks Tensorflow!!!!!!!!**", "Could you post the error here when you specify  --inference_type= QUANTIZED_UINT8?\r\n\r\nIn the meantime, you can checkout post-training quantization here and give it a try:\r\nhttps://www.tensorflow.org/lite/performance/post_training_quantization", "Thank you \r\nI solved it. But the accuracy has gone done\r\nI used this code to Convert TFModel to TFlite Model\r\n\r\nset the python location in the terminal\r\n**cd /Applications/anaconda3/envs/tensorflow_cpu/lib/python3.7/site-packages/tensorflow/lite/python**\r\n\r\nThen add this code to Convert the Model to a Quantized one!!!\r\n\r\n**tflite_convert --output_file=$(Output file Location_ name it with outputfile.tflite)\r\n--graph_def_file=$(Input file Location_ name it with inputfile.pb)\r\n--input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \r\n--input_shapes=1,300,300,3 \r\n--inference_type=QUANTIZED_UINT8  \r\n--mean_values=128 \r\n--std_dev_values=128 \r\n--change_concat_input_ranges=false \r\n--allow_custom_ops \r\n--default_ranges_min=0 \r\n--default_ranges_max=255\r\n**\r\n\r\nThis code will convert the model to a quantized one, which reduced my model from 55Mb to 13Mb of size....\r\n\r\nIf you want a model with float inference type then, change the inference type to **FLOAT**\r\nbut the model will not reduce its size..\r\n\r\nThanks GITHUB!!!!\r\nHope this will be useful for Who will Search for \r\n\"How to convert TFModel to TFLite Model\"\r\n", "@Deepak-HpyAda \r\nHow you get the output paramters ?"]}, {"number": 30644, "title": "When trying to convert .pb to tflite ", "body": "System information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: No\r\nTensorFlow installed from (source or binary): Command Line\r\nTensorFlow version: 1.14.0\r\nPython version: 3.7.3\r\nInstalled using virtualenv? pip? conda?: conda\r\nBazel version (if compiling from source): 0.28.0\r\nGCC/Compiler version (if compiling from source): No\r\nCUDA/cuDNN version: NO\r\nGPU model and memory: NO\r\n/tensorflow/lite/python\r\nI m using tflite_convert.py from this folder /tensorflow/lite/python.\r\nI run the code in Command Line as:\r\npython ./tflite_convert.py --graph_def_file=/Users/deepak/Documents/Tensorflow/workspace/training_demo/trained-inference-graphs/output_inference_graph_v1/frozen_inference_graph.pb --output_file=/Users/deepak/Desktop/TFlite/frozen_inference_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,300,300,3 --input_array=Mul\u2014output_array= final_result--inference_type=FLOAT --input_data_type=FLOAT\r\n\r\nI get\r\n\r\nusage: tflite_convert.py [-h] --output_file OUTPUT_FILE\r\n(--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)\r\n[--output_format {TFLITE,GRAPHVIZ_DOT}]\r\n[--inference_type {FLOAT,QUANTIZED_UINT8}]\r\n[--inference_input_type {FLOAT,QUANTIZED_UINT8}]\r\n[--input_arrays INPUT_ARRAYS]\r\n[--input_shapes INPUT_SHAPES]\r\n[--output_arrays OUTPUT_ARRAYS]\r\n[--saved_model_tag_set SAVED_MODEL_TAG_SET]\r\n[--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\r\n[--std_dev_values STD_DEV_VALUES]\r\n[--mean_values MEAN_VALUES]\r\n[--default_ranges_min DEFAULT_RANGES_MIN]\r\n[--default_ranges_max DEFAULT_RANGES_MAX]\r\n[--post_training_quantize]\r\n[--drop_control_dependency]\r\n[--reorder_across_fake_quant]\r\n[--change_concat_input_ranges {TRUE,FALSE}]\r\n[--allow_custom_ops] [--target_ops TARGET_OPS]\r\n[--dump_graphviz_dir DUMP_GRAPHVIZ_DIR]\r\n[--dump_graphviz_video]\r\ntflite_convert.py: error: --input_arrays and --output_arrays are required with --graph_def_file\r\n\r\n\r\nplz, help.", "comments": ["NO documentation works properly! Searched Complete GOOGLE but left with nothing>>>\r\n**THANK YOU**", "**Tensorflow** you have to reply to this.. As your documents are not understandable.\r\nAnd after each code you say. This type of error will be raised. Then whats the use....\r\n\r\n\r\n", "You need to pass the input tensor names and output tensor names. See examples in this blog post:\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193"]}, {"number": 30643, "title": "Convert TFModel to TFLite Model", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Command Line\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.28.0\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: NO\r\n- GPU model and memory:NO\r\n\r\n/tensorflow/lite/python\r\nI m using tflite_convert.py from this folder /tensorflow/lite/python.\r\nI run the code in Command Line as:\r\npython ./tflite_convert.py --graph_def_file=/Users/deepak/Documents/Tensorflow/workspace/training_demo/trained-inference-graphs/output_inference_graph_v1/frozen_inference_graph.pb --output_file=/Users/deepak/Desktop/TFlite/frozen_inference_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,300,300,3 --input_array=Mul\u2014output_array= final_result--inference_type=FLOAT --input_data_type=FLOAT\r\n\r\nI get\r\n\r\nusage: tflite_convert.py [-h] --output_file OUTPUT_FILE\r\n                         (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)\r\n                         [--output_format {TFLITE,GRAPHVIZ_DOT}]\r\n                         [--inference_type {FLOAT,QUANTIZED_UINT8}]\r\n                         [--inference_input_type {FLOAT,QUANTIZED_UINT8}]\r\n                         [--input_arrays INPUT_ARRAYS]\r\n                         [--input_shapes INPUT_SHAPES]\r\n                         [--output_arrays OUTPUT_ARRAYS]\r\n                         [--saved_model_tag_set SAVED_MODEL_TAG_SET]\r\n                         [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\r\n                         [--std_dev_values STD_DEV_VALUES]\r\n                         [--mean_values MEAN_VALUES]\r\n                         [--default_ranges_min DEFAULT_RANGES_MIN]\r\n                         [--default_ranges_max DEFAULT_RANGES_MAX]\r\n                         [--post_training_quantize]\r\n                         [--drop_control_dependency]\r\n                         [--reorder_across_fake_quant]\r\n                         [--change_concat_input_ranges {TRUE,FALSE}]\r\n                         [--allow_custom_ops] [--target_ops TARGET_OPS]\r\n                         [--dump_graphviz_dir DUMP_GRAPHVIZ_DIR]\r\n                         [--dump_graphviz_video]\r\n**tflite_convert.py: error: --input_arrays and --output_arrays are required with --graph_def_file**\r\n\r\nI need help to solve this error\r\n", "comments": ["duplicate #30645 "]}, {"number": 30642, "title": "\u2018scatter_nd_update\u2019 doesn't work with string", "body": "**System information**\r\nI Reproduced this issue in newest [tensorflow official docker image](https://hub.docker.com/r/tensorflow/tensorflow).\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.14.0\r\n- Python version:2.7.15+\r\n\r\n**Describe the current behavior**\r\nIn my model, I need to maintain an extremely long 2-D variable tensor\uff0cwhich has several columns and many rows, and its dtype is string. In every training step, I need to update only several individual rows of that tensor. [tf.scatter_nd_update](https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update) meets my requirements perfectly,\r\nexcept that it doesn't work with string in fact. As a contrast,  [tf.scatter_nd](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) does work. Since the document doesn't mention that `ref` can't be string, I think it may be a bug.\r\n\r\n**Describe the expected behavior**\r\nI hope `tf.scatter_nd_update` support string `ref`,and I really need this feature in my project. So if it can't be fixed quickly, **any walk-around suggestions (include modify some source code) is also welcome.**  \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nref = tf.Variable([\u2018qq\u2019,\u2019ww\u2019,\u2019ee\u2019,\u2019rr\u2019,\u2019\u2019,\u2019\u2019,\u2019\u2019,\u2019\u2019])\r\nindices = tf.constant([[4], [3], [1] ,[7]])\r\nupdates = tf.constant(['aa', 'dd', 'cc', 'bb'])\r\nupdate = tf.scatter_nd_update(ref, indices, updates)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.initialize_all_variables())\r\n    print(sess.run(update))\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ScatterNdUpdate' used by node ScatterNdUpdate (defined at <stdin>:1) with these attrs: [_class=[\"loc:@Variable\"], use_locking=true, Tindices=DT_INT32, T=DT_STRING]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]\r\n\r\n\t [[ScatterNdUpdate]]\r\n```\r\n", "comments": ["Could reproduce the issue with TF version 1.14.Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30642\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30642\">No</a>\n"]}, {"number": 30641, "title": "No OpKernel was registered to support Op 'GatherV2' with these attrs.", "body": "Hi there! Facing with this problem after training pytorch model, converting to pb file and runing on android with tensorflow 1.13.1. Can anybody guide me what I am doing wrong?\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the [Github ](https://github.com/tensorflow/tensorflow/issues/new/choose)new issue template.\r\nAlso, can you elaborate more on your issue.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@ravikyram I trained unet model on pytorch (1.0), converted it to onnx using api provided by pytorch which was following by converting to pb using tensorflow (1.13). And finally, after trying to run it on android (tensorflow also 1.13) it gave this error. I didnt compile anything, just used provided api. Other details: Ubuntu 18 x64.\r\n\r\n        val graph = tensorFlowInference.graph()\r\n        graph.operations().forEach {\r\n            println(it.name())\r\n        }\r\n        tensorFlowInference.feed(inputName, imageNormalizedPixels,\r\n                1L, imageSize, imageSize, COLOR_CHANNELS.toLong())\r\n        tensorFlowInference.run(arrayOf(outputName))\r\n        tensorFlowInference.fetch(outputName, results)", "@cruigo93 \r\nJust to verify did you convert .pb model into TF lite to run it on android. Thanks!", "Just used pb without converting into tflite", "Please, go through the below links if it helps you.Thanks!\r\nhttps://www.tensorflow.org/lite\r\nhttps://www.tensorflow.org/lite/examples\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30640, "title": "DEBUG: /home/nd/.cache/bazel/_bazel_nd/905339168ec5fb0a3da01fdaab718117/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12 (trying to install)\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.19\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: 9/7 (trying to install) \r\n- GPU model and memory: Nvidia Quardo K4000\r\n\r\n**Describe the problem**\r\nWhile running tensorflow with backend i was getting problem ` Ignoring visible gpu device (device: 0, name: Quadro K4000, pci bus id: 0000:05:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.` So to make it compatible i followed tutorial from [here](https://medium.com/@mccann.matt/compiling-tensorflow-with-cuda-3-0-support-42d8fe0bf3b5). After running instruction `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` getting error which is mentioned in attached in image. Please help me to solve the problem. \r\n![Error](https://user-images.githubusercontent.com/30615882/61120986-c60b9c00-a4bb-11e9-85dd-1db2d53bc099.png)\r\n\r\n\r\n", "comments": ["@hiteshnitetc Just to verify, Did you follow the steps mentioned in the official [Tensorflow](https://www.tensorflow.org/install/source) Website.Thanks!  ", "@gadagashwini hello. No i donot follow steps given in official website. I follow steps from [here](https://medium.com/@mccann.matt/compiling-tensorflow-with-cuda-3-0-support-42d8fe0bf3b5)", "@hiteshnitetc It would be better if you follow instructions mentioned in the official website. Thanks!", "Ok, But i have to install cuda and cudnn also, and there to install these,\ninstructions are not given\n\nOn Tue, Jul 16, 2019 at 3:17 PM gadagashwini <notifications@github.com>\nwrote:\n\n> @hiteshnitetc <https://github.com/hiteshnitetc> It would be better if you\n> follow instructions mentioned in the official website. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30640?email_source=notifications&email_token=AHJSSSR4BHB7WMREPOMDKY3P7WKJHA5CNFSM4ICGPYX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2AJLGA#issuecomment-511743384>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHJSSSS5HY456ZSDSMKH2ALP7WKJHANCNFSM4ICGPYXQ>\n> .\n>\n\n\n-- \nHitesh Tekchandani\nPhD Research Scholar\nNIT Raipur\nIndia\n", "@hiteshnitetc To install cuda and cudnn, follow the steps mentioned [here](https://www.tensorflow.org/install/source#install_gpu_support_optional_linux_only). Thanks!", "@gadagashwini Ok, thnx", "@hiteshnitetc Were you able to solve this issue? Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30639, "title": "tf.while_loop with tf.keras.layers.LSTM broken", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): the july 12 p36 gpu 2.0 nightly preview\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10/7\r\n- GPU model and memory: 3 GeForce GTX w/8 GB \r\n\r\n**Describe the current behavior**\r\n\r\nFirst, I want to mention that the LSTM not working with distributed strategies is already being looked into here: https://github.com/tensorflow/tensorflow/issues/29189 -- I wanted to highlight this as a separate issue, because it likely has a different source...\r\n\r\nBasically, when dynamically decoding a sequence with an LSTM and tf.while_loop, the code breaks (see logs below for more detail). This does not happen with an RNN(LSTMCell) configuration, but the LSTM is the only CuDNN access point, aside from GRU (which also does not work in this configuration).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code should use the optimized CuDNN LSTM implementation and behave as the RNN(LSTMCell) approach i.e. not fail.\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/jkamalu/tensorflow_bugs/blob/master/LSTMGraphPlacement.py\r\n\r\n**Other info / logs**\r\n\r\n2019-07-12 11:37:10.386140: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1558] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-07-12 11:38:30.548248: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 1 of node se_q3/seq_encoder/while/body/_195/TensorListPushBack_49 was passed int32 from se_q3/seq_encoder/while/body/_195/decoder_c/lstm_3/StatefulPartitionedCall:9 incompatible with expected variant.\r\n2019-07-12 11:38:37.853257: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 1 of node se_q3/seq_encoder/while/body/_195/TensorListPushBack_49 was passed int32 from se_q3/seq_encoder/while/body/_195/decoder_c/lstm_3/StatefulPartitionedCall:9 incompatible with expected variant.\r\n2019-07-12 11:38:39.689929: W tensorflow/core/common_runtime/process_function_library_runtime.cc:672] Ignoring multi-device function optimization failure: Invalid argument: Input 1 of node se_q3/seq_encoder/while/body/_195/TensorListPushBack_77 was passed int32 from se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall:9 incompatible with expected variant.\r\n2019-07-12 11:38:45.280991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-12 11:38:45.755520: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at partitioned_function_ops.cc:113 : Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal\r\n2019-07-12 11:38:45.755562: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal\r\n\t [[{{node se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall}}]]\r\n\t [[If_9/else/_2424/gradients/while_grad/while_grad/body/_11561/gradients/TensorArrayV2Read/TensorListGetItem_grad/TensorListLength/TensorListPopBack/_1920]]\r\n2019-07-12 11:38:45.755854: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is while_20/exit/_94 , and the dst node is while_0_RetVal\r\n\t [[{{node se_q3/seq_encoder/while/body/_195/decoder_c/lstm_2/StatefulPartitionedCall}}]]\r\n[I 11:38:49.971 NotebookApp] Saving file at /SEQ3_LSTM_CUDA.ipynb\r\n", "comments": ["@jkamalu ,\r\nThank you for bringing this up.\r\nIn the code mentioned by you in the link https://github.com/jkamalu/tensorflow_bugs/blob/master/LSTMGraphPlacement.py , we could not find tf.while_loop. If the issue is with tf.while_loop, Can you please provide the code with tf.while_loop. Thanks", "@anush-o Thanks for the reply. I'm going off of the docs, which say that a for loop which iterates over a tensor is converted to a tf.while_loop with @tf.function.\r\n\r\nSee: https://www.tensorflow.org/beta/guide/effective_tf2\r\n\r\n> for/while -> tf.while_loop (break and continue are supported)\r\n\r\nWith that understanding, the tf.while_loop is the post-conversion of the python for loop I have that iterates over the tf.constant(...) in the code I linked.", "Thanks for reporting the issue, the while loop placement issue should be solved by ca7acecce5066f518b775da339b6258fafd3db23 and f0fd2bed4d5358ebdb8bae5243c1a86fd7967f27. Could u try the code again with the tf-2.0-nightly builds?\r\n\r\nThanks.", "@jkamalu As @qlzh727 mentioned, it was resolved. I ran your code in `tf-nightly-gpu-2.0-preview ` and I don't see any error. Please check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/5e4a2c43934f1f126563b47e3dcc6797/tf_30639_lstm.ipynb). \r\n\r\nI am closing this issue as it was resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30639\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30639\">No</a>\n"]}, {"number": 30638, "title": "CUDA10.1 with tensorflow-gpu==2.0.0b1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution:Linux manjaro (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip install tensorflow-gpu==2.0.0-b1\r\n- TensorFlow version:2.0.0b1\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:rtx2070\r\n\r\n\r\n\r\n**Describe the problem**\r\ntf.test.is_gpu_available():\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-12 16:39:38.345749: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-12 16:39:38.345852: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-12 16:39:38.345953: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-12 16:39:38.346052: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-12 16:39:38.346153: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["Try install the cudatoolkit (2nd latest version) and cudnn via conda", "I am having the same issue under the same conditions except for:\r\nOS: arch\r\nGPU: GT 710", "> Try to install the cudatoolkit (2nd latest version) and cudnn via conda\r\n\r\nWell, I have tried this, but it didn't work. I have installed the cudatoolkit-10.1.168 and cudnn-7.6.0-cuda10.1_0 in m the server.", "> \r\n> \r\n> > Try to install the cudatoolkit (2nd latest version) and cudnn via conda\r\n> \r\n> Well, I have tried this, but it didn't work. I have installed the cudatoolkit-10.1.168 and cudnn-7.6.0-cuda10.1_0 in m the server.\r\n\r\nInstall the cudatookit 10.0.130.\r\nI tried 10.1.168 few weeks before, had some errors", "As of now, TensorFlow does not support CUDA 10.1, thus you need to install CUDA 10.0, as indicated in the software requirements section of [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu).", "> As of now, TensorFlow does not support CUDA 10.1, thus you need to install CUDA 10.0, as indicated in the software requirements section of https://www.tensorflow.org/install/gpu.\r\n\r\nYea, I have installed the ubuntu18 on the server and use the cuda10.0 for tensorflow-gpu==2.0b1, it works so far so good.", "@MuseMamba \r\nCan we close this issue as it was resolved.Thanks!", "I have the same problem. \r\nTake 'libcurand.so.10.0' as example, in my case, logs said\r\n`I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64`\r\nHowever in ` /usr/local/cuda-10.1/lib64`, there are libcurand.so, libcurand.so.10, libcurand.10.1.1.243. and the last two are actually linked to libcurand.so, so I just create a link named libcurand.so.10.0 and link it to libcurand.so, no warnings except 'libcublas', since no libcublas.so under this dir. \r\nAccording to [cyberwillis](https://icl.cs.utk.edu/magma/forum/viewtopic.php?f=2&t=2573), libcublas is may already installed in some where. you just need to find it and link to it\r\nyou can find it with this `ldconfig -p | grep cublas`\r\nFinally, no warning at all!", "Can't bloody nvidia just open source everything. Those managers should be hanged. Wasted way too much time on this.", "> Can't bloody nvidia just open source everything. Those managers should be hanged. Wasted way too much time on this.\r\n\r\nTo be fair, this issue has barely anything to do with CUDA not being open-source, as it is a mere version compatibility issue. That being said..."]}, {"number": 30637, "title": "R1.14", "body": "pull code", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30637) for more info**.\n\n<!-- need_sender_cla -->", "Please reopen with latest changes , this seems to be having lot of conflicts."]}]