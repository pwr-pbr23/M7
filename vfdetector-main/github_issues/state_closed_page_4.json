[{"number": 55444, "title": "what is the order when I call tf.keras.Layers.BatchNormation.get_weights() ? my tensorflow version is 2.2.0?", "body": "<em>Please make sure that this is an issue related to keras.\r\ntag:keras_template</em>\r\n\r\n**Important Notice**\r\n\r\nPlease note that `tf.keras` code was moved entirely to\r\n[keras-team/keras](https://github.com/keras-team/keras) repository\r\n\r\nYou can open any code/doc bugs, performance issues, and feature requests\r\n in [keras-team/keras](https://github.com/keras-team/keras/issues) repository\r\n\r\n`tf.keras` related issues opened in\r\n[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) repository may\r\nnot get attention as [keras-team/keras](https://github.com/keras-team/keras)\r\nrepository is dedicated for the development of `keras` code\r\n", "comments": ["sorry, I mean BatchNormalization. Does different version of tensorflow get different result?", "I got this from this link(https://github.com/tensorflow/models/blob/master/official/vision/beta/projects/yolo/modeling/layers/nn_blocks.py#:~:text=gamma%2C%20beta%2C%20moving_mean%2C%20moving_variance%20%3D%20self.bn.get_weights()) but I have no idea if the version can cause the different result.", "@daeing ,\r\nWhen we call tf.keras.Layers.BatchNormation.get_weights() the order of the list is `[gamma, beta, mean, variance]`.\r\n\r\nAlso trainable weights does not change. Batchnormalization  maintains non-trainable weights, which are updated via layer updates (i.e. not through backprop): the mean and variance vectors.\r\n\r\nIf we want to disable weight updates we simply call the layer with the argument training=False in the functional API, which disables the Keras learning phase for layer \r\n\r\n**`x = BatchNormalization()(x, training=False)`**\r\n\r\nBatch norm simply shifts and scales the data by a fixed amount derived from the exponential moving averages. This should be fixed at test time and indepdent of the batch contents.", "Also please test your code in latest stable version 2.8 and let us know if you are facing same issue.Thanks!", "> @daeing , When we call tf.keras.Layers.BatchNormation.get_weights() the order of the list is `[gamma, beta, mean, variance]`.\r\n> \r\n> Also trainable weights does not change. Batchnormalization maintains non-trainable weights, which are updated via layer updates (i.e. not through backprop): the mean and variance vectors.\r\n> \r\n> If we want to disable weight updates we simply call the layer with the argument training=False in the functional API, which disables the Keras learning phase for layer\r\n> \r\n> **`x = BatchNormalization()(x, training=False)`**\r\n> \r\n> Batch norm simply shifts and scales the data by a fixed amount derived from the exponential moving averages. This should be fixed at test time and indepdent of the batch contents.\r\n\r\nI test my code on tensorflow2.2, the order is [gamma, beta, mean, variance] as you methoned. I know the gamma beta are trainable params, mean and variance are no-trainable params. Because I want bn layer to set_weights() correctly, so I ask this question. Many thanks!!!", "@daeing ,\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55444\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55444\">No</a>\n"]}, {"number": 55443, "title": "Create `tensorflow_issue_template.yaml`", "body": "Create generic template for all type of issues and auto label based on issue type selection", "comments": ["There are still templates left. Can you delete all of them in a single commit?", "sorry Mihai could not do in one commit , forgot to fork the repo instead created a branch."]}, {"number": 55440, "title": "Tensorflow 2.8.0 | Error Message: ImportError: SystemError. | Tensorflow package error?", "body": "Greetings Tensorflow Support Team.\r\nHi, I got this error recently on my anaconda environment on my Windows 10 Machine.\r\n### Error Message:\r\n\r\n> ImportError: SystemError: <built-in method __contains__ of dict object at 0x000001E3DD963A48> returned a result with an error set\r\n\r\n### System Information\r\n\r\n> Windows 10\r\n> Anaconda Navigator 2.1.2\r\n> Python: 3.7.11\r\n> Numpy: 1.21.5\r\n> Nvidia RTX 3060 Laptop GPU\r\n> Nvidia CUDA: 11.2\r\n> cuDNN Version: 8.1\r\n\r\n### I got this error when upgrade it to Tensorflow 2.8.0 (Installed using pip)\r\n`pip install tensorflow`\r\n\r\nError Log:\r\n```\r\n(shapes_tf2) C:\\Users\\User>python -c \"import tensorflow as tf;print(tf.__version__)\"\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"D:\\VirtualEnvironment\\anaconda3\\envs\\shapes_tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\VirtualEnvironment\\anaconda3\\envs\\shapes_tf2\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 37, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"D:\\VirtualEnvironment\\anaconda3\\envs\\shapes_tf2\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python.client import pywrap_tf_session\r\n  File \"D:\\VirtualEnvironment\\anaconda3\\envs\\shapes_tf2\\lib\\site-packages\\tensorflow\\python\\client\\pywrap_tf_session.py\", line 19, in <module>\r\n    from tensorflow.python.client._pywrap_tf_session import *\r\nImportError: SystemError: <built-in method __contains__ of dict object at 0x000001E3DD963A48> returned a result with an error set\r\n\r\n(shapes_tf2) C:\\Users\\User>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"D:\\VirtualEnvironment\\anaconda3\\envs\\shapes_tf2\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\VirtualEnvironment\\anaconda3\\envs\\shapes_tf2\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 37, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"D:\\VirtualEnvironment\\anaconda3\\envs\\shapes_tf2\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python.client import pywrap_tf_session\r\n  File \"D:\\VirtualEnvironment\\anaconda3\\envs\\shapes_tf2\\lib\\site-packages\\tensorflow\\python\\client\\pywrap_tf_session.py\", line 19, in <module>\r\n    from tensorflow.python.client._pywrap_tf_session import *\r\nImportError: SystemError: <built-in method __contains__ of dict object at 0x000001D6FFDF3A48> returned a result with an error set\r\n```\r\n### But when I'm downgrade it to 2.7.0, it runs normal. (Installed using pip)\r\n`pip install tensorflow==2.7.0`\r\n```\r\n(shapes_tf2) C:\\Users\\User>python -c \"import tensorflow as tf;print(tf.__version__)\"\r\n2.7.0\r\n\r\n(shapes_tf2) C:\\Users\\User>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n2022-03-31 02:42:33.973041: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-03-31 02:42:34.357936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3493 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\r\ntf.Tensor(95.8436, shape=(), dtype=float32)\r\n```\r\n\r\nAny solutions for this (other than downgrade it?).\r\nI'm not having any problem for downgrade it. I'm here to report the error I got.\r\nThank you in advance.\r\n\r\nMarch, 31st 2022\r\n\r\nAldy", "comments": ["@aldyhelnawan ,\r\nCan you please try to upgrade the python and confirm if you are facing same issue.\r\n`python3 -m venv --system-site-packages ./venv`\r\nAlso please check this [issue1](https://github.com/tensorflow/tensorflow/issues/55338#issuecomment-1075646598) and [issue2](https://github.com/tensorflow/tensorflow/issues/55377) with similar error.Thanks!", "@tilakrayal. Hi, thank you to response my issue.\r\n\r\nFor the issue, yes. issue1 and issue2 are similar to my issue.\r\nFor the suggestions you provide, **upgrade the python version** on my **anaconda venv** is **work** for me.\r\n\r\nI'm update the python version using: \r\n`conda update python`\r\nAnd the python version updated to version 3.7.13\r\n```\r\n(shapes_tf2) C:\\Users\\User>conda list python\r\n# packages in environment at D:\\VirtualEnvironment\\anaconda3\\envs\\shapes_tf2:\r\n#\r\n# Name                    Version                   Build  Channel\r\nipython                   7.31.1           py37haa95532_0\r\nipython_genutils          0.2.0              pyhd3eb1b0_1\r\nopencv-python             4.5.5.64                 pypi_0    pypi\r\npython                    3.7.13               h6244533_0\r\npython-dateutil           2.8.2              pyhd3eb1b0_0\r\npython-git                2018.2.1                 pypi_0    pypi\r\n```\r\n\r\nThen I upgrade the tensorflow back to version 2.8.0 using:\r\n`pip install --upgrade tensorflow`\r\n\r\nLast, I tested the tensorflow again and it works. This is the result:\r\n```\r\n(shapes_tf2) C:\\Users\\User>python -c \"import tensorflow as tf;print(tf.__version__)\"\r\n2.8.0\r\n\r\n(shapes_tf2) C:\\Users\\User>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n2022-03-31 22:34:44.834772: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-03-31 22:34:45.752188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3497 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\r\ntf.Tensor(1209.3511, shape=(), dtype=float32)\r\n```\r\n\r\n\r\nOnce again, thank you for your support.\r\n\r\n", "@aldyhelnawan ,\r\nGlad the suggestion worked for you to resolve the issue, please feel free to move this to closed status.", "@tilakrayal. Ok, status closed. Thank you. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55440\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55440\">No</a>\n", "@aldyhelnawan ,\r\nAre you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 55439, "title": "[ROCm] Rocm bef executable fixes", "body": "/cc @chsigg @cheshire @jurahul @hanbinyoon ", "comments": []}, {"number": 55438, "title": "model is loaded as Loader._recreate_base_user_object.<locals>._UserObject", "body": "\r\npython == 3.8\r\ntensorflow (gpu) == 'v2.8.0-rc1-32-g3f878cff5b6'\r\nkeras == 2.8\r\nwindows 10\r\n\r\nI am saving the model as both an h5 and keras format with\r\n\r\n```\r\nmodel.save('model.h5')\r\nmodel.save(model_location)\r\ntype(model)   \r\n```\r\n```\r\nkeras.engine.functional.Functional\r\n```\r\n\r\nThen, I am loading the model with \r\n\r\n```\r\nmodel = keras.models.load_model(model_location)\r\n# or \r\nmodel = tf.saved_model.load(model_location) # results in the same\r\ntype(model)\r\n```\r\n\r\n```<class 'tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject'>```\r\n\r\nThere is something up with the loading process that results in the following error when calling `model.predict(X)` \r\n\r\n```AttributeError: '_UserObject' object has no attribute 'predict'```\r\n\r\n```\r\ntf.saved_model.contains_saved_model(model_location)\r\n\r\ntrue\r\n```\r\nHowever, loading the same model saved as an h5 file \r\n```\r\nmodel = keras.models.load_model('model.h5')\r\ntype(model)\r\n```\r\n```\r\nkeras.engine.functional.Functional\r\n```\r\nworks. \r\n\r\n\r\nThanks in advance!\r\n\r\n", "comments": ["@darrahts ,\r\nModel.save() will save  keras type model. tf.keras.models.load_model will load it back as keras model. \r\n\r\ntf.saved_model.save will save  generic model. tf.saved_model.load will load it back either type, as generic _UserObject model.\r\nAlso can you please look at this [comment](https://github.com/tensorflow/models/issues/8990#issuecomment-1069733488) from the issue with the similar error.Thanks", "@tilakrayal,\r\n\r\nThanks for the comment. The point of this is, why is `keras.models.load_model` not returning the model? It is returning the same thing as `tf.saved_model.load`. ", "@darrahts ,\r\nIn order to reproduce the issue reported here, could you please provide the complete code you are using. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@darrahts This issue doesn't exist on my Linux machine. **Please try with the latest version of tensorflow gpu on windows** and let me know if the issue still persists . Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55438\">No</a>\n"]}, {"number": 55437, "title": "runpath not including //tensorflow/python for _dtensor_device.so", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): manylinux2014\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): git HEAD\r\n- Python version: 3.7.x\r\n- Bazel version (if compiling from source): 5.0.0\r\n- GCC/Compiler version (if compiling from source): 10.2.1\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nauditwheel fails with message about cannot find _pywrap_tensorflow_internal.so\r\n\r\n**Describe the expected behavior**\r\n\r\nauditwheel passes\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nauditwheel repair -w ../wheels/ tensorflow-pkg/tensorflow_aarch64-2.9.0-cp38-cp38-linux_aarch64.wh\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nIssue introduced with https://github.com/tensorflow/tensorflow/commit/fd94c2628c344a288ee194bcdbfe36239f5d4cdd\r\n", "comments": ["@cfRod @nSircombe ", "$ auditwheel repair -w ../wheels/ tensorflow-pkg/tensorflow_aarch64-2.9.0-cp38-cp38-linux_aarch64.whl\r\nINFO:auditwheel.main_repair:Repairing tensorflow_aarch64-2.9.0-cp38-cp38-linux_aarch64.whl\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/auditwheel\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/opt/_internal/pipx/venvs/auditwheel/lib/python3.9/site-packages/auditwheel/main.py\", line 59, in main\r\n    rval = args.func(args, p)\r\n  File \"/opt/_internal/pipx/venvs/auditwheel/lib/python3.9/site-packages/auditwheel/main_repair.py\", line 161, in execute\r\n    out_wheel = repair_wheel(\r\n  File \"/opt/_internal/pipx/venvs/auditwheel/lib/python3.9/site-packages/auditwheel/repair.py\", line 74, in repair_wheel\r\n    raise ValueError(\r\nValueError: Cannot repair wheel, because required library \"_pywrap_tensorflow_internal.so\" could not be located\r\n", "Running auditwheel with --verbose gives a lot more detail, in particular this section about _dtensor_device shows that tensorflow/python is missing from runpath and so gets a null path for _pywrap_tensorflow_internal.so\r\n\r\n    \"tensorflow/dtensor/python/_dtensor_device.so\": {\r\n        \"interp\": null,\r\n        \"path\": \"tensorflow/dtensor/python/_dtensor_device.so\",\r\n        \"realpath\": \"tensorflow/dtensor/python/_dtensor_device.so\",\r\n        \"needed\": [\r\n            \"libtensorflow_framework.so.2\",\r\n            \"_pywrap_tensorflow_internal.so\",\r\n            \"libstdc++.so.6\",\r\n            \"libm.so.6\",\r\n            \"libgcc_s.so.1\",\r\n            \"libc.so.6\"\r\n        ],\r\n        \"rpath\": [],\r\n        \"runpath\": [\r\n            \"/tmp/tmpp3a0nu2y/tensorflow/dtensor/python\",\r\n            \"/tmp/tmpp3a0nu2y/tensorflow/dtensor\",\r\n            \"/tmp/tmpp3a0nu2y/tensorflow\"\r\n        ],\r\n        \"libs\": {\r\n            \"libtensorflow_framework.so.2\": {\r\n                \"realpath\": \"/tmp/tmpp3a0nu2y/tensorflow/libtensorflow_framework.so.2\",\r\n                \"path\": \"/tmp/tmpp3a0nu2y/tensorflow/libtensorflow_framework.so.2\",\r\n                \"needed\": [\r\n                    \"libdl.so.2\",\r\n                    \"libm.so.6\",\r\n                    \"librt.so.1\",\r\n                    \"libpthread.so.0\",\r\n                    \"libstdc++.so.6\",\r\n                    \"libgcc_s.so.1\",\r\n                    \"libc.so.6\",\r\n                    \"ld-linux-aarch64.so.1\"\r\n                ]\r\n            },\r\n            \"libdl.so.2\": {\r\n                \"realpath\": \"/lib64/libdl-2.17.so\",\r\n                \"path\": \"/lib64/libdl.so.2\",\r\n                \"needed\": [\r\n                    \"libc.so.6\",\r\n                    \"ld-linux-aarch64.so.1\"\r\n                ]\r\n            },\r\n            \"libc.so.6\": {\r\n                \"realpath\": \"/lib64/libc-2.17.so\",\r\n                \"path\": \"/lib64/libc.so.6\",\r\n                \"needed\": [\r\n                    \"ld-linux-aarch64.so.1\"\r\n                ]\r\n            },\r\n            \"ld-linux-aarch64.so.1\": {\r\n                \"realpath\": \"/lib64/ld-2.17.so\",\r\n                \"path\": \"/lib/ld-linux-aarch64.so.1\",\r\n                \"needed\": []\r\n            },\r\n            \"libm.so.6\": {\r\n                \"realpath\": \"/lib64/libm-2.17.so\",\r\n                \"path\": \"/lib64/libm.so.6\",\r\n                \"needed\": [\r\n                    \"libc.so.6\"\r\n                ]\r\n            },\r\n            \"librt.so.1\": {\r\n                \"realpath\": \"/lib64/librt-2.17.so\",\r\n                \"path\": \"/lib64/librt.so.1\",\r\n                \"needed\": [\r\n                    \"libpthread.so.0\",\r\n                    \"libc.so.6\"\r\n                ]\r\n            },\r\n            \"libpthread.so.0\": {\r\n                \"realpath\": \"/lib64/libpthread-2.17.so\",\r\n                \"path\": \"/lib64/libpthread.so.0\",\r\n                \"needed\": [\r\n                    \"libc.so.6\",\r\n                    \"ld-linux-aarch64.so.1\"\r\n                ]\r\n            },\r\n            \"libstdc++.so.6\": {\r\n                \"realpath\": \"/lib64/libstdc++.so.6.0.19\",\r\n                \"path\": \"/lib64/libstdc++.so.6\",\r\n                \"needed\": [\r\n                    \"libm.so.6\",\r\n                    \"libc.so.6\",\r\n                    \"ld-linux-aarch64.so.1\",\r\n                    \"libgcc_s.so.1\"\r\n                ]\r\n            },\r\n            \"libgcc_s.so.1\": {\r\n                \"realpath\": \"/lib64/libgcc_s-4.8.5-20150702.so.1\",\r\n                \"path\": \"/lib64/libgcc_s.so.1\",\r\n                \"needed\": [\r\n                    \"libc.so.6\"\r\n                ]\r\n            },\r\n            \"_pywrap_tensorflow_internal.so\": {\r\n                \"realpath\": null,\r\n                \"path\": null,\r\n                \"needed\": []\r\n            }\r\n        }\r\n    },\r\n", "Compare the above with the entry for _pywrap_mlir.so which gets a path for _pywrap_tensorflow_internal.so\r\n\r\n    \"tensorflow/python/_pywrap_mlir.so\": {\r\n        \"interp\": null,\r\n        \"path\": \"tensorflow/python/_pywrap_mlir.so\",\r\n        \"realpath\": \"tensorflow/python/_pywrap_mlir.so\",\r\n        \"needed\": [\r\n            \"libtensorflow_framework.so.2\",\r\n            \"_pywrap_tensorflow_internal.so\",\r\n            \"libm.so.6\",\r\n            \"libdl.so.2\",\r\n            \"libpthread.so.0\",\r\n            \"libstdc++.so.6\",\r\n            \"libgcc_s.so.1\",\r\n            \"libc.so.6\",\r\n            \"ld-linux-aarch64.so.1\"\r\n        ],\r\n        \"rpath\": [],\r\n        \"runpath\": [\r\n            \"/tmp/tmpp3a0nu2y/tensorflow/python\",\r\n            \"/tmp/tmpp3a0nu2y/tensorflow\"\r\n        ],\r\n        \"libs\": {\r\n            \"libtensorflow_framework.so.2\": {\r\n                \"realpath\": \"/tmp/tmpp3a0nu2y/tensorflow/libtensorflow_framework.so.2\",\r\n                \"path\": \"/tmp/tmpp3a0nu2y/tensorflow/libtensorflow_framework.so.2\",\r\n                \"needed\": [\r\n                    \"libdl.so.2\",\r\n                    \"libm.so.6\",\r\n                    \"librt.so.1\",\r\n                    \"libpthread.so.0\",\r\n                    \"libstdc++.so.6\",\r\n                    \"libgcc_s.so.1\",\r\n                    \"libc.so.6\",\r\n                    \"ld-linux-aarch64.so.1\"\r\n                ]\r\n            },\r\n            \"libdl.so.2\": {\r\n                \"realpath\": \"/lib64/libdl-2.17.so\",\r\n                \"path\": \"/lib64/libdl.so.2\",\r\n                \"needed\": [\r\n                    \"libc.so.6\",\r\n                    \"ld-linux-aarch64.so.1\"\r\n                ]\r\n            },\r\n            \"libc.so.6\": {\r\n                \"realpath\": \"/lib64/libc-2.17.so\",\r\n                \"path\": \"/lib64/libc.so.6\",\r\n                \"needed\": [\r\n                    \"ld-linux-aarch64.so.1\"\r\n                ]\r\n            },\r\n            \"ld-linux-aarch64.so.1\": {\r\n                \"realpath\": \"/lib64/ld-2.17.so\",\r\n                \"path\": \"/lib/ld-linux-aarch64.so.1\",\r\n                \"needed\": []\r\n            },\r\n            \"libm.so.6\": {\r\n                \"realpath\": \"/lib64/libm-2.17.so\",\r\n                \"path\": \"/lib64/libm.so.6\",\r\n                \"needed\": [\r\n                    \"libc.so.6\"\r\n                ]\r\n            },\r\n            \"librt.so.1\": {\r\n                \"realpath\": \"/lib64/librt-2.17.so\",\r\n                \"path\": \"/lib64/librt.so.1\",\r\n                \"needed\": [\r\n                    \"libpthread.so.0\",\r\n                    \"libc.so.6\"\r\n                ]\r\n            },\r\n            \"libpthread.so.0\": {\r\n                \"realpath\": \"/lib64/libpthread-2.17.so\",\r\n                \"path\": \"/lib64/libpthread.so.0\",\r\n                \"needed\": [\r\n                    \"libc.so.6\",\r\n                    \"ld-linux-aarch64.so.1\"\r\n                ]\r\n            },\r\n            \"libstdc++.so.6\": {\r\n                \"realpath\": \"/lib64/libstdc++.so.6.0.19\",\r\n                \"path\": \"/lib64/libstdc++.so.6\",\r\n                \"needed\": [\r\n                    \"libm.so.6\",\r\n                    \"libc.so.6\",\r\n                    \"ld-linux-aarch64.so.1\",\r\n                    \"libgcc_s.so.1\"\r\n                ]\r\n            },\r\n            \"libgcc_s.so.1\": {\r\n                \"realpath\": \"/lib64/libgcc_s-4.8.5-20150702.so.1\",\r\n                \"path\": \"/lib64/libgcc_s.so.1\",\r\n                \"needed\": [\r\n                    \"libc.so.6\"\r\n                ]\r\n            },\r\n            \"_pywrap_tensorflow_internal.so\": {\r\n                \"realpath\": \"/tmp/tmpp3a0nu2y/tensorflow/python/_pywrap_tensorflow_internal.so\",\r\n                \"path\": \"/tmp/tmpp3a0nu2y/tensorflow/python/_pywrap_tensorflow_internal.so\",\r\n                \"needed\": [\r\n                    \"libtensorflow_framework.so.2\",\r\n                    \"libpthread.so.0\",\r\n                    \"libm.so.6\",\r\n                    \"libdl.so.2\",\r\n                    \"librt.so.1\",\r\n                    \"libstdc++.so.6\",\r\n                    \"libgcc_s.so.1\",\r\n                    \"libc.so.6\",\r\n                    \"ld-linux-aarch64.so.1\"\r\n                ]\r\n            }\r\n        }\r\n    },\r\n", "It looks like the runpath is composed of the location of the target shared object and all its parents to the tensorflow root. However _dtensor_device.so has a dependency on _pywrap_tensorflow_internal.so which is located in tensorflow/python. But _dtensor_device.so is located in tensorflow/dtensor/python so its runpath does not include tensorflow/python as it is not a parent and hence the failure to locate _pywrap_tensorflow_internal.so.", "This seems to have been resolved by https://github.com/tensorflow/tensorflow/commit/95754274f34169fc7e59ba292a7970dfc9adc772", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55437\">No</a>\n"]}, {"number": 55434, "title": "Updates mkl_aarch64 build to use oneDNN v2.6.", "body": null, "comments": ["Hi @penpornk - I notice there's no r2.9 branch yet, is there any chance we can get this in before the cut?", "> I think we are waiting to see the nightly test results and then cut at a commit from yesterday. But we can try to get this in anyway.\r\n\r\nRighto, thanks @penpornk."]}, {"number": 55433, "title": "tensorflow-macos version 2.8.0 cannot import keras", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nmacOS 12.3\r\n`x86_64` and `arm64` versions\r\ntensorflow-macos 2.8.0 installed via pypi\r\n* https://pypi.org/project/tensorflow-macos/#files\r\n* https://files.pythonhosted.org/packages/e5/6c/05e158fd3a729c3d11720468b170bfa18db140e0091452e5bec2976e0f3d/tensorflow_macos-2.8.0-cp38-cp38-macosx_11_0_x86_64.whl\r\n\r\n**Describe the current behavior**\r\nThese `keras` imports don't work:\r\n```bash\r\npython3.8 -c 'import tensorflow.keras'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.keras'\r\n```\r\n\r\n```bash\r\npython3.8 -c 'import tensorflow.keras.datasets.mnist'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.keras'\r\n```\r\n\r\n```bash\r\npython3.8 -c 'from tensorflow import keras as keras ; keras.datasets.mnist.load_data'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\", line 58, in __getattr__\r\n    module = self._load()\r\n  File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\", line 41, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 973, in _find_and_load_unlocked\r\n```\r\n\r\n```bash\r\npython3.8 -c 'from tensorflow.python import keras as keras ; keras.datasets.mnist.load_data'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'tensorflow.python.keras' has no attribute 'datasets'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nAll `keras` imports above should work.\r\n", "comments": ["@essandess To use keras ,could you please make sure that you have installed tensorflow packages successfully in your system, please check the [Build from source](https://www.tensorflow.org/install/source#macos) for macOS and refer to the tested build configurations.\r\nThanks!", "@sushreebarsa `tensorflow-macos` is provided as a pre-built `.whl` by Tensorflow on pypi: https://pypi.org/project/tensorflow-macos/#files. It is installed correctly.\r\n\r\nAlso please note that the previous version 2.7.0 works correctly: https://pypi.org/project/tensorflow-macos/2.7.0/\r\n\r\ncc: @kulinseth (`tensorflow-macos` maintainer on pypi)", "@essandess, \r\nThanks for reporting this issue.\r\nI could reproduce the issue with `Tensorflow-macos 2.8`. on macOS Monterey 12.3.\r\n\r\nI resolved through.\r\n\r\n```\r\nbrew install python3\r\npython3 -m vent tf\r\nsource tf/bin/activate\r\npython3 -m pip install -U pip\r\npip install tensorflow\r\n```", "> I could reproduce the issue with `Tensorflow-macos 2.8`. on macOS Monterey 12.3.\r\n> \r\n> I resolved through.\r\n\r\nThis does not resolve the issue. It simply installs a different build of Tensorflow that does not support `tensorflow-metal` and macOS GPU support.", "> @sushreebarsa `tensorflow-macos` is provided as a pre-built `.whl` by Tensorflow on pypi: https://pypi.org/project/tensorflow-macos/#files. It is installed correctly.\r\n> \r\n> Also please note that the previous version 2.7.0 works correctly: https://pypi.org/project/tensorflow-macos/2.7.0/\r\n> \r\n> cc: @kulinseth (`tensorflow-macos` maintainer on pypi)\r\n\r\nI ran the commands:\r\n`python3.8 -c 'import tensorflow.keras'` and `python3.8 -c 'import tensorflow.keras.datasets.mnist'` . They both ran fine with :\r\n```\r\n(tensorflow-metal) $ python3.8 -c 'import tensorflow.keras.datasets.mnist'\r\nInit Plugin\r\nInit Graph Optimizer\r\nInit Kernel\r\n```\r\nDo you have the right version of keras installed ?\r\n```\r\nkeras                         2.8.0rc0\r\ntensorflow-macos              2.8.0\r\ntensorflow-metal              0.4.0\r\n```\r\n\r\nThese are the relevant packages in pip list.", "> Do you have the right version of keras installed ?\r\n\r\nThat's the issue\u2014thank you! Please see https://github.com/macports/macports-ports/pull/14459.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55433\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55433\">No</a>\n"]}, {"number": 55431, "title": "[TF-TRT] Slice Instance_OP Bug Fix and Cleaning", "body": "This PR increase code readability around the `op_instance` mechanism and fix a name collision bug inside `Converter::DynamicReshape`.", "comments": ["@bixia1 for review.\r\nCC: @christopherbate and @tfeher \r\n", "Original Error:\r\n\r\n```bash\r\n2022-03-29 19:38:35.717118: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger 4: [network.cpp::validate::2647] Error Code 4: Internal Error (Repeated layer name: TRTEngineOp_000_002/StatefulPartitionedCall/mrcnn/multilevel_crop_and_resize/stack-slice_2:SLICE (layers must have distinct names))\r\n```"]}, {"number": 55430, "title": "Add ndmin check", "body": "Added ndmin check to allow maximum 32 ndmin to make same behavior as numpy.\r\nCurrently it is crashing when very large ndmin is passed.\r\nCloses:  https://github.com/tensorflow/tensorflow/issues/55292", "comments": []}, {"number": 55428, "title": "Converter for LogicalNot operation", "body": "- Converter for LogicalNot operation is using the same base class (ConvertUnaryImpl) as other Unary operations.\r\n- The special converter for Rsqrt was removed and now for this operation regular Unary Op converter is used.\r\n- New templated class implemented for testing ConvertUnary, ConvertBooleanUnary, ConvertActivation.\r\n- A new check and corresponding subtests were added to Validate: at least 1 dimension is required for input of any Unary and UnaryBoolean operation. (Similar subtest for ConvertActivation operations is blocked and after refactoring of ConvertActivation it will be activated)", "comments": ["Replacement for [PR#55229](https://github.com/tensorflow/tensorflow/pull/55229)", "[fail1.log](https://github.com/tensorflow/tensorflow/files/8374305/fail1.log)\r\n", "> [fail1.log](https://github.com/tensorflow/tensorflow/files/8374305/fail1.log)\r\nFIXED\r\n\r\nSorry, while resolving a merge conflict online, I didn't notice that the line `template <typename T>` disappeared from \r\n```\r\ntemplate <typename T>\r\nclass OpConverter_BinaryTest\r\n```", "@drivanov build errors:\r\nIn file included from [third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:16](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc?l=16&ws=tap-prod-presubmit/180112651&snapshot=2):\r\nIn file included from [./third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h?l=26&ws=tap-prod-presubmit/180112651&snapshot=2):\r\nIn file included from [./third_party/tensorflow/compiler/tf2tensorrt/convert/op_converter.h:25](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/op_converter.h?l=25&ws=tap-prod-presubmit/180112651&snapshot=2):\r\nIn file included from [./third_party/tensorflow/compiler/tf2tensorrt/convert/weights.h:22](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/weights.h?l=22&ws=tap-prod-presubmit/180112651&snapshot=2):\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:98](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h?l=98&ws=tap-prod-presubmit/180112651&snapshot=2):30: error: no matching function for call to 'DebugString'\r\n    StrAppend(&tmp_s, StrCat(DebugString(el), \", \"));\r\n                             ^~~~~~~~~~~\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:1706](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc?l=1706&ws=tap-prod-presubmit/180112651&snapshot=2):58: note: in instantiation of function template specialization 'tensorflow::tensorrt::DebugString<bool>' requested here\r\n                       << \", Received Input Tensor: \" << DebugString(values);\r\n                                                         ^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:1888](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc?l=1888&ws=tap-prod-presubmit/180112651&snapshot=2):7: note: in instantiation of function template specialization 'tensorflow::tensorrt::convert::ParameterizedOpConverterTestBase::AddTestTensor<bool>' requested here\r\n      AddTestTensor(\"input\", p.input_dims, input_tf_type, input_values);\r\n      ^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:7057](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc?l=7057&ws=tap-prod-presubmit/180112651&snapshot=2):3: note: in instantiation of function template specialization 'tensorflow::tensorrt::convert::OpConverter_UnaryTest<bool>::RunTests<nvinfer1::UnaryOperation>' requested here\r\n  RunTests(\"LogicalUnary\", ops_to_test, *UnaryBooleanOperationMap(), op_map,\r\n  ^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/trt_parameters.h:46](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/trt_parameters.h?l=46&ws=tap-prod-presubmit/180112651&snapshot=2):8: note: candidate function not viable: no known conversion from 'const std::__bit_iterator<std::vector<bool>, true, 0>::reference' (aka 'const std::__bit_const_reference<std::vector<bool>>') to 'const tensorflow::tensorrt::TrtPrecisionMode' for 1st argument\r\nstring DebugString(const TrtPrecisionMode mode);\r\n       ^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:95](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h?l=95&ws=tap-prod-presubmit/180112651&snapshot=2):8: note: candidate template ignored: could not match 'vector' against '__bit_const_reference'\r\nstring DebugString(const std::vector<CType>& vector) {\r\n       ^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:86](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h?l=86&ws=tap-prod-presubmit/180112651&snapshot=2):8: note: candidate template ignored: requirement 'std::is_arithmetic<std::__bit_const_reference<std::vector<bool, std::allocator<bool>>>>::value' was not satisfied [with CType = std::__bit_const_reference<std::vector<bool>>]\r\nstring DebugString(const CType& el) {", "@bixia1 : I don't see these compilation issues on my side. To make things easier, I just created [PR#55542](https://github.com/tensorflow/tensorflow/pull/55542) and am closing it."]}, {"number": 55426, "title": "Bazel build error. Warning on retrieving an archive which is not found and then fails.", "body": "Hi fellow developers,\r\n\r\nI am trying to build Tensorflow from source on Windows using the tutorial on the website (https://www.tensorflow.org/install/source_windows) but I have an issue when i get to the bazel build of tensorflow\r\nI am using this command to build since I have cuda and cudnn installed on my computer. (When i did the ./configure it found them and I could specify the compute capability for example) :\r\n`bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nHere is the trace starting a little before it could not found an archive and then fails.\r\n```\r\nINFO: Repository local_config_cuda instantiated at:\r\n  C:/users/me/desktop/ia/tensorflow-master/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/me/desktop/ia/tensorflow-master/tensorflow/workspace2.bzl:870:19: in workspace\r\n  C:/users/me/desktop/ia/tensorflow-master/tensorflow/workspace2.bzl:96:19: in _tf_toolchains\r\nRepository rule cuda_configure defined at:\r\n  C:/users/me/desktop/ia/tensorflow-master/third_party/gpus/cuda_configure.bzl:1448:33: in <toplevel>\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/8b8793a6d3d528e9b190ab7f9b85fea2f1aadc2c.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1401, column 38, in _cuda_autoconf_impl\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1239, column 56, in _create_local_cuda_repository\r\n                host_compiler_includes + _cuda_include_path(\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 364, column 32, in _cuda_include_path\r\n                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + \"/include\"))\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/remote_config/common.bzl\", line 290, column 19, in realpath\r\n                return execute(repository_ctx, [bash_bin, \"-c\", \"realpath \\\"%s\\\"\" % path]).stdout.strip()\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\n/usr/bin/bash: line 1: realpath: command not found\r\nERROR: C:/users/me/desktop/ia/tensorflow-master/WORKSPACE:15:14: fetching cuda_configure rule //external:local_config_cuda: Traceback (most recent call last):\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1401, column 38, in _cuda_autoconf_impl\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1239, column 56, in _create_local_cuda_repository\r\n                host_compiler_includes + _cuda_include_path(\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 364, column 32, in _cuda_include_path\r\n                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + \"/include\"))\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/remote_config/common.bzl\", line 290, column 19, in realpath\r\n                return execute(repository_ctx, [bash_bin, \"-c\", \"realpath \\\"%s\\\"\" % path]).stdout.strip()\r\n        File \"C:/users/me/desktop/ia/tensorflow-master/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\n/usr/bin/bash: line 1: realpath: command not found\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\me\\desktop\\ia\\tensorflow-master\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\me\\desktop\\ia\\tensorflow-master\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Repository command failed\r\n/usr/bin/bash: line 1: realpath: command not found\r\n```\r\n\r\nI am really sorry if it appears to be a problem on my side but since I can't retrieve the archive I think it is best to start here first.\r\nThank you for your help and your job.\r\n\r\n", "comments": ["Problem solved here :\r\nhttps://discuss.tensorflow.org/t/error-building-tensorflow-2-8-in-windows-10/7984/3\r\n\r\nSorry for the inconvenience", "@GosuPaper ,\r\nPlease refer this link [1](https://github.com/tensorflow/tensorflow/issues/54407) and [2](https://github.com/tensorflow/tensorflow/issues/53614) with the similar error.It helps.Thanks!", "> Problem solved here : https://discuss.tensorflow.org/t/error-building-tensorflow-2-8-in-windows-10/7984/3\r\n> \r\n> Sorry for the inconvenience\r\n\r\nHere is the answer to my problem @tilakrayal . Sorry for the incovenience and thanks for your help", "@GosuPaper ,\r\nGlad the issue is resolved for you, please feel free to move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55426\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55426\">No</a>\n"]}, {"number": 55425, "title": "`pfor`/`tf.vectorize_map`: Improve feedback/hints messages when it `fallback_to_while_loop`", "body": "**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\nProbably if I can talk with a codeowner of this namepace\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs we are adding [a root cause message](https://github.com/tensorflow/tensorflow/pull/55192) on the cause we are going internally to rely on `fallback_to_while_loop`  it would be nice to have a more clear feedback  string to the user on what kind of action is required (e.g. refactoring his function, open a new ticket with a code gist on TF github, etc..).\r\n\r\nSee more at https://github.com/tensorflow/tensorflow/pull/55192#issuecomment-1081306349 /cc @wangpengmit \r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nDevelopers that partially fail to fully `tf.vectorize_map` their functions\r\n**Any Other info.**\r\n", "comments": ["@bhack, Can we close this issue, since associated PR got merged. Thanks!", "@gadagashwini This is something else and it has no associated PR. \r\nPlease check the mentioned @wangpengmit's comment at https://github.com/tensorflow/tensorflow/pull/55192#issuecomment-1081306349"]}, {"number": 55424, "title": "TFLite allocate tensors fails: (CONCATENATION) failed to prepare ", "body": "### 1. System information\r\n\r\nThere is a problem in the pytorch model -> tflite quantization process, the model is a multi-input multi-output structure\r\n\r\nThe model inference process is shown in the code\r\n\r\n`    def forward(self, input_x: torch.Tensor, tdnn_s0, tdnn_s1, tdnn_s1_2):\r\n         model_in = input_x\r\n\r\n        conv1_inputs = torch.cat([tdnn_s0, model_in], 2) \r\n\r\n        conv1 = self.norm[0](self.relu(self.convs[0](conv1_inputs)).permute(0, 2, 1)).permute(0, 2, 1)\r\n\r\n        output_conv1 = conv1\r\n\r\n        conv2_inputs = torch.cat([tdnn_s1, conv1], 2) \r\n        conv2 = self.convs[1](conv2_inputs) \r\n        output_conv2 = conv2\r\n\r\n        conv2_2_inputs = torch.cat([tdnn_s1_2, conv2], 2) \r\n        conv2_2 = self.norm[1](self.relu(self.convs[2](conv2_2_inputs)).permute(0, 2, 1)).permute(0, 2, 1)\r\n\r\n        conv2_2 = torch.add(conv2_inputs[:,:,:-1], conv2_2)\r\n\r\n        dense_input = conv2_2.permute(0, 2, 1)\r\n\r\n        output = self.dense(dense_input)\r\n\r\n        return  output, output_conv1, output_conv2`\r\n\r\nWhen I quantize the model I get an error like the following, but when I comment out the quantization in the code, it works; I don't yet know where the problem is\r\n\r\n2022-03-29 20:37:26.144668: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format.\r\n2022-03-29 20:37:26.144820: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency.\r\n2022-03-29 20:37:26.144875: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored change_concat_input_ranges.\r\n2022-03-29 20:37:26.146905: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: tmp\r\n2022-03-29 20:37:26.149434: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\r\n2022-03-29 20:37:26.149493: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: tmp\r\n2022-03-29 20:37:26.158658: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n2022-03-29 20:37:26.159847: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2200000000 Hz\r\n2022-03-29 20:37:26.178324: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: tmp\r\n2022-03-29 20:37:26.187430: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 40531 microseconds.\r\n2022-03-29 20:37:26.210737: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\ncalib [[1, 40, 4], [1, 40, 2], [1, 160, 1], [1, 20, 1]]\r\nTraceback (most recent call last):\r\n  File \"export_flow_layer2.py\", line 207, in <module>\r\n    main()\r\n  File \"export_flow_layer2.py\", line 180, in main\r\n    tflite_model_quant = converter.convert()\r\n  File \"/home/storage06/lidongbo/tools/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 921, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/storage06/lidongbo/tools/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 521, in _calibrate_quantize_model\r\n    calibrated = calibrate_quantize.calibrate(\r\n  File \"/home/storage06/lidongbo/tools/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 173, in calibrate\r\n    self._calibrator.Prepare([list(s.shape) for s in sample])\r\nRuntimeError: tensorflow/lite/kernels/concatenation.cc:80 t->dims->data[d] != t0->dims->data[d] (160 != 40)Node number 0 (CONCATENATION) failed to prepare .\r\n\r\n### 2. Code\r\n``` \r\n      def representative_data_gen():\r\n            for item in range(2):\r\n                feat = torch.zeros(1, 40, 4, dtype=torch.float)\r\n                tdnn_s0 = torch.zeros(1, 40, 2, dtype=torch.float)\r\n                tdnn_s1 = torch.zeros(1, 160, 1, dtype=torch.float)\r\n                tdnn_s1_2 = torch.zeros(1, 20, 1, dtype=torch.float)\r\n                yield [feat.numpy(), tdnn_s0.numpy(), tdnn_s1.numpy(), tdnn_s1_2.numpy()]\r\n\r\n        converter = tf.lite.TFLiteConverter.from_saved_model(args.pb_model)\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.representative_dataset = representative_data_gen\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n        converter.inference_input_type = tf.int8\r\n        converter.inference_output_type = tf.int8\r\n        tflite_model_quant = converter.convert()\r\n\r\n        with open(args.tflite, 'wb') as f:\r\n            f.write(tflite_model_quant)\r\n```", "comments": ["Hi @ldb1026 ! I think you should use ONNX to convert Pytorch models to TF model/TF lite models . Attaching relevant threads [1](https://stackoverflow.com/questions/61679908/pytorch-convert-2d-cnn-model-to-tflite), [2](https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/pytorch-to-tensorflow-lite-for-deploying-on-arm-ethos-u55-and-u65) for reference . Please revert back with a Colab gist for further assistance. Thank you!", "hi  @mohantym  \r\n\r\nhttps://colab.research.google.com/drive/19-TLv1SEtZWcE6Vzb1Nig8JSjwL6uICq#scrollTo=L_PMowpPmaFx\r\n\r\nI have successfully converted the model to a pb model, it's just that I get an error during quantization\r\n\r\nThe problem I encountered is that when I use the code to convert the pb to tflite model\r\nif I quantify the configuration, like this\r\n\r\n```\r\n        converter = tf.lite.TFLiteConverter.from_saved_model(args.pb_model)\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.representative_dataset = representative_data_gen\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n        converter.inference_input_type = tf.int8\r\n        converter.inference_output_type = tf.int8\r\n        tflite_model_quant = converter.convert()\r\n```\r\n\r\nthere will be corresponding problems\uff0c that is to  paper  \r\n\r\n> RuntimeError: tensorflow/lite/kernels/concatenation.cc:80 t->dims->data[d] != t0->dims->data[d] (20 != 40)Node number 0 (CONCATENATION) failed to prepare.\r\n\r\nbut if it is not quantized, the conversion will be successful.\r\n```\r\n        converter = tf.lite.TFLiteConverter.from_saved_model(args.pb_model)\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        tflite_model_quant = converter.convert()\r\n```\r\n\r\nIs this problem caused by the fact that the model cannot recalculate the shape due to operator collapse?\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55424\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55424\">No</a>\n", "The problem is solved, the reason is that the pb model is a multi-input model, and the data returned by the function onverter.representative_dataset is not in the correct order", " @ldb1026 ! I did not have access to above colab notebook but  suspected the same (order or concatenation of inputs as single input upon how model was trained) in the representative dataset aside suggesting to remove the .numpy() operation.  Shall we move this to closed status then?", "The problem is solved, it can be closed, thank you", "@ldb1026 ! Thanks for confirmation. Moving to closed status as it was marked resolved from this[ comment](https://github.com/tensorflow/tensorflow/issues/55424#issuecomment-1082821296).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55424\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55424\">No</a>\n"]}, {"number": 55423, "title": "Customize TF tensor multiplication", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIs there a way o customize/modify the tensor/matrix multiplication operation used through out tensorflow? Let's say whenever I am executing a forward pass on a given input, involving a sequence of tensor multiplications defined by the architecture of the neural net, how would I proceed to change the very definition of how the multiplication is carried out? \r\nWhich source file do I have to edit to achieve this? Maybe smewhere in `tensorflow/python/ops`  ?\r\n\r\nThank you for your help.\r\n", "comments": ["I tried to find the relevant source file myself, and also modify and rebuild the whole TF package from source. No success so far.\r\nWould this source file, at the heart of the matmul operation lie somewhere in `tensorflow/c` or even `tensorflow/core` ? Any help would be greatly appreciated. Thank you.", "@fedlucchetti,\r\n\r\nDoes [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matmul_op_impl.h) help you? Thanks!", "@chunduriv \r\nThis is exactly what I was looking for. Thank you so much.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 55422, "title": "Parameter init_epoch can not be identified when calling model.train()", "body": "Hi,\r\n    I am trying to implement incremental training (when the model trained is stopped due to other reasons, I can retrain the model from the latest checkpoint). There, I used the parameter 'init_epoch' when calling model.train(). However, parameter 'init_epoch' can not be identified in Tensorflow2.4.0.\r\n   I want to know which version of Tensorflow support parameter 'init_epoch'?\r\n\r\n![scr](https://user-images.githubusercontent.com/12910533/160596038-09c582fa-334f-4426-bb5a-8d014398e617.png)\r\n\r\n", "comments": ["@v3551G In order to expedite the trouble-shooting process,could you please provide the code and share the error in  text format  instead of the screenshot which will help  us to analyze the issue?Thanks!", "Thank you. The parameter is misspelled, and the correct parameter should be 'initial_epoch'", "@v3551G Currently the BackupAndRestore callback only supports eager mode. In graph mode, consider using  Save/Restore Model as mentioned [here](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras), and by providing initial_epoch in [Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).\r\nWe recommend you to use TF v2.4 or later versions as older versions are not actively supported.\r\nPlease let us know if it helps?\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 55421, "title": "How can I take CPU variable as input of a custom GPU operation?", "body": "I want to hold embedding in DRAM and make a HBM cache for it.\r\nSo I registered a GPU only operation, and I want to it take a CPU resource variable as input.\r\n\r\n```c++\r\nREGISTER_KERNEL_BUILDER(Name(\"MyLookup\")\r\n                            .Device(DEVICE_GPU)\r\n                            .HostMemory(\"embeddings\"),\r\n                        MyLookupOp);\r\n\r\nREGISTER_OP(\"MyLookup\")\r\n    .Attr(\"E: {float16, float32, float64}\")\r\n    .Attr(\"K: {int32, int64}\")\r\n    .Input(\"embeddings: resource\")\r\n    .Input(\"keys: K\")\r\n    .Output(\"vecs: E\")\r\n    .SetShapeFn(LookupShapeInfer);\r\n```\r\n\r\n```python\r\n  with tf.device('/device:CPU:0'):\r\n    var = tf.get_variable(shape=[VOC, EMB_SIZE],\r\n                           dtype=np.float32,\r\n                           initializer=tf.ones_initializer(),\r\n                           name='EMB')\r\n  keys = tf.random.uniform(shape=[BATCH_SIZE, KEY_NUM],\r\n                           minval=0,\r\n                           maxval=VOC,\r\n                           dtype=tf.int64)\r\n  ret = lib.my_lookup(var, keys)\r\n  print(ret)\r\n```\r\n\r\nBut I got exception complaining no CPU OpKernel available.\r\n\r\n```\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'MyLookup' OpKernel for 'CPU' devices compatible with node {{node HorovodEmbeddingLookupCss}}\r\n        .  Registered:  device='GPU'\r\n [Op:HorovodEmbeddingLookupCss]\r\n```\r\n\r\nHow can I take CPU variable as input of a custom GPU operation?", "comments": ["@tilakrayal hello, could you please give me some advice? thank u.", "@nrailgun ,\r\nIn order to expedite the trouble-shooting process, could you please provide a complete code and the TensorFlow version you are using.\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 55420, "title": "Fix typos in data_adapter.py", "body": " Fix typos in data_adapter.py ", "comments": ["It looks like your PR relates to the Keras component. Please submit it to the github.com/keras-team/keras repository instead. Thankyou.\r\n@fchollet, @qlzh727"]}, {"number": 55416, "title": "Fixes for TF v2.8.0 oneDNN containers and remove support for Ubuntu 16.04", "body": null, "comments": []}, {"number": 55415, "title": "Removing the executor file and link jax to use tpu_api_dlsym_initializer", "body": "Because tpu_profiler dependency already pulls in the full tpu_api_dlsym_initializer dependency, so there's no point maintaining this smaller executor dependency.", "comments": ["@skye could you please take a look at this PR(draft)? it is dependent on [this PR](https://github.com/tensorflow/tensorflow/pull/55413). ", "LGTM, my only suggestion to mention that the tpu_profiler dependency already pulls in the full tpu_api_dlsym_initializer dependency, so there's no point maintaining this smaller executor dependency. I'll approve this for real once the previous change is in + this is rebased.", "@michaelbanfield could you please review this PR? It is changing where JAX is loading the libtpu, and removing executor file which was the light weight version of tpu_api_dlsym_initializer. ", "@sshahrokhi Can you please resolve conflicts? Thank you!", "> @sshahrokhi Can you please resolve conflicts? Thank you!\r\n\r\nIt is funny that there were commits exactly to the same files I was working on, between the few hours between my PR and your response. Anyhow I rebased it again and the conflict should have been resolved, thanks! "]}, {"number": 55413, "title": "Refactoring tpu_api_dlsym_initializer", "body": "We want to remove the static initializer of the TPU for JAX, which stays in the tpu_api_dlsym_initializer file. However, The functions and links in this file are needed to be called and linked in the correct places if we remove the static initializer. Therefore, we are only keeping the static initializer in this file, and have the tpu_initializer_helper file to have the rest of the functions and links. So there is no need to link this file in any place that does not need the static initializer. This PR does not change the behavior of anything, and is only refactoring functions. ", "comments": ["@skye could you please review this PR? ", "Marking \"ready to pull\" so I can run internal tests, not actually ready for submission.", "@sshahrokhi Can you please address Ubuntu Sanity errors? Thank you!", "@gbaned  the build seems to fail because of a mismatch in bazel version, I guess can we run them again to see if that is resolved? ", "@skye is out this week, @michaelbanfield  could you please review this? ", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 55412, "title": "[TF-TRT] Fix two problems in python tests.", "body": "This PR fixes two problems in the test.\r\n\r\n1. `saved_model.load.load` only exists with `bazel run ...`\r\n2. Enable multi GPU support for `binary_tensor_weight_broadcast_test.py`", "comments": []}, {"number": 55410, "title": "Use `_shape` annotation in ReadVariableOp shape inference if found", "body": "This PR is a workaround to fix the shape inference of `ReadVariableOp` nodes when a graph gets resource inputs and we can't infer the shapes of the variables. The nodes need to be annotated beforehand with an attribute `_shape`. This is only a temporary solution, ideally, shape inference should handle resource inputs (similarly to how it can get shapes of the inputs when `assume_valid_feeds == true`).\r\n\r\nThis is a clone of #54591 which couldn't be merged due to CLA issues.", "comments": ["cc @bixia1 "]}, {"number": 55409, "title": "Increase timeout on join for preemption test", "body": "The use of less powerful machines for testing can result in //tensorflow/python/distribute/failure_handling:failure_handler_test failing with an internal timeout while waiting for threads to join(). Add a timeout of 300s to override the default 200s in this case.", "comments": ["@cfRod @nSircombe ", "How close are you to the new 300 timeout (i.e. is 300 enough?)"]}, {"number": 55408, "title": "[oneDNN] Softmax with oneDNN library.", "body": "This PR enables softmax forward op using oneDNN library. It uses in-place computation whenever possible. It improves performance of BERT/Transformer like models wherein the softmax is used for self-attention.\r\n\r\nThe following are performance data on some micro-benchmarks and BERT inference. The performance data were collected on Intel Xeon CPUs using Eigen ThreadPool.\r\n\r\n## Microbenchmarks\r\n\r\nTensor Dims | Numeric Type | Intra-op Threads | HW | oneDNN (ns) | Eigen (ns) | Speedup\r\n-- | -- | -- | -- | -- | -- | --\r\n1x16x384x384 | FLOAT32 | 4 | Xeon 28 core | 435505 | 849837 | 1.95x\r\n16x16x384x384 | FLOAT32 | 4 | Xeon 28 core | 30608250 | 39503349 | 1.29x\r\n32x1008 | FLOAT32 | 1 | Xeon 28 core | 28455 | 33078 | 1.16x\r\n128x1008 | FLOAT32 | 1 | Xeon 28 core | 83065 | 123516 | 1.49x\r\n32x1008 | FLOAT32 | 4 | Xeon 28 core | 30585 | 32165 | 1.05x\r\n128x1008 | FLOAT32 | 4 | Xeon 28 core | 41348 | 127743 | 3.09x\r\n1x16x384x384 | BFLOAT16 | 4 | Xeon 26 core | 430448 | 867489 | 2.02x\r\n16x16x384x384 | BFLOAT16 | 4 | Xeon 26 core | 17430198 | 25598959 | 1.47x\r\n32x1008 | BFLOAT16 | 1 | Xeon 26 core | 31498 | 49853 | 1.58x\r\n128x1008 | BFLOAT16 | 1 | Xeon 26 core | 91812 | 159045 | 1.73x\r\n32x1008 | BFLOAT16 | 4 | Xeon 26 core | 29836 | 49224 | 1.65x\r\n128x1008 | BFLOAT16 | 4 | Xeon 26 core | 43047 | 122662 | 2.85x\r\n\r\n\r\nThis PR improves performance by 12% for some models that use softmax.\r\n", "comments": []}, {"number": 55407, "title": "TensorBoard Dev doesn't update upon delete operation", "body": "Hi,\r\n\r\nI'm using TensorBoard dev to log my training metrics. My TensorBoard version is `2.8.0`.\r\n\r\nI log my data using the command\r\n```\r\ntensorboard dev upload --logdir=my/dir/of/logs\r\n```\r\n\r\nHowever, I noticed that once a version is created, and is apparent on TensorBoard dev, even if I delete it locally, it still exist on the online version. This behavior doesn't happen with local TensorBoard.\r\n\r\nIs this behavior expected? is there a way to correct this?\r\n\r\nThank you in advance.", "comments": ["@de-gozaru Could you please post this issue on Tensorboard [repo](https://github.com/tensorflow/tensorboard/issues) to get the right help there?\r\nThanks!", "Thank you for your response, I'll do that!", "@de-gozaru Thank you for the quick response!\r\nPlease move this issue to closed status if you have posted this issue in that repo?\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55407\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55407\">No</a>\n"]}, {"number": 55406, "title": "Develop upstream sync 220328", "body": "https://github.com/ROCmSoftwarePlatform/frameworks-internal/issues/1334", "comments": []}, {"number": 55404, "title": "Fix training accuracy on AArch64 in FC layers", "body": "This patch fixes training accuracy on AArch64. When training on AArch64 using Compute Library as backend in oneDNN we need to inform Compute Library\u2019s fully connected layer that weights are not constant so that they can be updated. This information now is propagated from MklDnnMatMulFwdPrimitive as a new parameter.", "comments": []}, {"number": 55403, "title": "ValueError: Python inputs incompatible with input_signature", "body": "I'm working with this On-Device Training with TensorFlow Lite\r\nhttps://www.tensorflow.org/lite/examples/on_device_training/overview#setup\r\n\r\nHowever, I customized the model. But, after customization, the following error is coming\r\nValueError: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    tf.Tensor(\r\n[[ 9.       17.229916]\r\n [15.       10.601508]\r\n [15.       10.517618]\r\n [ 7.       26.893026]\r\n [15.       10.572687]\r\n [ 7.       27.393883]\r\n [15.        9.859101]\r\n [14.       11.724992]\r\n [12.       13.421819]\r\n [ 7.       28.229916]], shape=(10, 2), dtype=float32),\r\n    tf.Tensor([1. 1. 1. 0. 1. 0. 1. 1. 1. 0.], shape=(10,), dtype=float32))\r\n  input_signature: (\r\n    TensorSpec(shape=(10, 2), dtype=tf.float32, name=None),\r\n    TensorSpec(shape=(10, 1), dtype=tf.float32, name=None)).\r\n\r\nPlease find the code here\r\nhttps://colab.research.google.com/drive/1jLM8BWuB7VRDMJHxKUDAIBR17MHqYSKk?usp=sharing\r\n\r\n\r\n", "comments": ["Hi @harshayelchuri ! I think you have already resolved the issue by changing these lines. \r\n`TensorSpec(shape=(10, 2), dtype=tf.float32, name=None) ,TensorSpec(shape=(10, 1), dtype=tf.float32, name=None)`\r\nto\r\n`TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)`", "Yes I resolved the issue, But the final code is this:\r\n @tf.function(input_signature=[tf.TensorSpec([None, 2], tf.float64),tf.TensorSpec([None], tf.float64)])\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 55401, "title": "Removes third_party/mkl_dnn/onednn-acl-bf16.patch", "body": "Since the release of oneDNN 2.5, the onednn-acl-bf16.patch\r\npatch is no longer required to enable bf16 for matmul and ip.", "comments": ["@penpornk - some tidying up I missed the _last_ time we updated the oneDNN version for the AArch64 build!"]}]