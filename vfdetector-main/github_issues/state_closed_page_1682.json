[{"number": 2443, "title": "ImportError: cannot import name grid_rnn", "body": "Docker - Tensor Flow Latest \n### Steps to reproduce\n\nWhen typing:    from tensorflow.contrib import grid_rnn\n\ni have this message:\nImportError: cannot import name grid_rnn\n\nHow can download the contrib part into tensorFlow Docker (in windows) ?\n\nThanks\n", "comments": ["`tf.contrib.grid_rnn` is only available in the nightly builds of TensorFlow (and it will be in version 0.9).  We don't currently publish nightly docker images, but you can modify the [`Dockerfile`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile) by replacing these lines:\n\n```\n# Install TensorFlow CPU version.\nENV TENSORFLOW_VERSION 0.8.0\nRUN pip --no-cache-dir install \\\n    http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-${TENSORFLOW_VERSION}-cp27-none-linux_x86_64.whl\n```\n\n...with:\n\n```\n# Install TensorFlow CPU version.\nRUN pip --no-cache-dir install \\\n    http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n```\n\nFollow the Docker instructions for [building a new image on Windows](https://docs.docker.com/windows/step_four/) to complete the process.\n", "I am using this command to download tensorflow....\ndocker run -it b.gcr.io/tensorflow/tensorflow\nI cannot change the docker file on tensorflow side....\n\nWhat command should I use to install a local dockerfile ?\n\nThanks, regards\n\n> On May 21, 2016, at 04:40, Derek Murray notifications@github.com wrote:\n> \n> tf.contrib.grid_rnn is only available in the nightly builds of TensorFlow (and it will be in version 0.9). We don't currently publish nightly docker images, but you can modify the Dockerfile by replacing these lines:\n> \n> # Install TensorFlow CPU version.\n> \n> ENV TENSORFLOW_VERSION 0.8.0\n> RUN pip --no-cache-dir install \\\n>     http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-${TENSORFLOW_VERSION}-cp27-none-linux_x86_64.whl\n> ...with:\n> \n> # Install TensorFlow CPU version.\n> \n> RUN pip --no-cache-dir install \\\n>     http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n> Follow the Docker instructions for building a new image on Windows to complete the process.\n> \n> \u2015\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n", "Here's a step-by-step tutorial for using Docker on Windows with a custom Dockerfile:\n\nhttps://docs.docker.com/windows/step_four/\n", "ok, Thanks\nBut, I got this message when builiding the docker:\n\nStep 7 : COPY jupyter_notebook_config.py /root/.jupyter/\nlstat jupyter_notebook_config.py: no such file or directory\n\nFrom which folder should I build the docker ?\n"]}, {"number": 2442, "title": "Add piecewise constant op", "body": "This should close https://github.com/tensorflow/tensorflow/issues/2432\n\nThis is more general than for learning-rate decay, but at the same time it didn't feel like it should be a native math op, so I kept it in `tensorflow/python/training/learning_rate_decay.py`.\n", "comments": ["Can one of the admins verify this patch?\n", "The tests run, but I just noticed that `piecewise_constant` isn't available as `tf.train.piecewise_constant`, and I don't know how to fix this as I don't see any relevant entries in `tensorflow/python/training/__init__.py` or `tensorflow/python/__init__.py`. I'll fix if you advise.\n", "@mrry current solution is to\n- Accept lists of `Tensor`s\n- Make sure that all `Tensor`s in `boundaries` have the same `dtype` as `x` (with test)\n- Make sure that all `Tensor`s in `values` have the same `dtype` (with test)\n\nI haven't made sure that `boundaries`'s entries are strictly increasing. Not sure of the canonical way to do this since `boundaries` is a `Tensor`.\n", "I think we can defer adding the strictly-increasing assertions for now. Thanks for this!\n", "@tensorflow-jenkins test this please.\n", "Sure, glad to contribute.\n\nList vs. generator fixed. Also caught/fixed a bug in the tests, as the different-`values`-dtypes `ValueError` wasn't actually getting tested.\n", "@tensorflow-jenkins test this please.\n", "ok to merge ?\n", "learning_rate_decay_test.py appears to be a binary file...\n", "LGTM (modulo binary file concerns).\n", "Can you please elaborate? Is this something I need to fix on my end? I edited the files over an sshfs mount, so I'm wondering if that caused problems.\n", "I'm not sure -- if you look at the file diffs, I can't even see the contents of the test code\n", "Hi, from my previous experience I found that it would have been accidentally made an executable. Changing it using chmod should hopefully solve the issue.\n", "The problem was that I somehow ended up appending a null character to the end of learning_rate_decay_test.py.\n\nI tried to reset to the previous commit, but the null character remained. Instead I reset back to your original source, copied over all changes to the most recent, and cleanly committed once.\n\nSorry for the confusion.\n", "Cool, looks good.  One more test and we can merge.\n\n@tensorflow-jenkins test this please\n"]}, {"number": 2441, "title": "Understanding cause of bad performance", "body": "I am training a convnet with multilple gpus and was using the cifar10 model as an example. It computes the gradients in every tower, stitches them and averages them. But that is mathematically equivalent to defining `total_loss = tf.add_n(tower_loss_list)` so I thought I would just do that.\nBut this implementation with 2 gpus runs at 1/2 the speed of the single gpu implementation. I am guessing the reason is that the gradient ops placement is very bad, and forces a lot data to be moved around.\nDo you also think that is the reason?\nIf so, what could be done to improve the automatic device assignment?\n", "comments": ["Did you change from having one `Optimizer` per tower to a single `Optimizer`? In that case, it's possible that all of the gradients are being computed on a single GPU (most likely `/gpu:0`), because the `colocate_gradients_with_ops` argument to `Optimizer.minimize()`/`Optimizer.compute_gradients()` defaults to `False`, and so the gradient ops will default to running on a single device.  You could try switching that option to `True` to see if it makes a difference.\n", "Thanks for that! Yes I switched to having a single `Optimizer`.\nSetting `colocare_gradients_with_ops` to true improved the performance by 50%. But still, 2 GPUs run at 3/4 the speed of 1.\n", "If you `tf.add_n` the losses together, you fuse the two different backward passes into a single backward pass which can no longer be parallelized across multiple GPUs (TensorFlow currently does not split single ops across multiple GPUs).  Thus, one of the GPUs is likely running idle during the backward pass.\n", "That is not convincing. Of course the gradient of `tf.add_n()` runs as a single op, but the backpropagation should continue independently throughout the nodes in each separate GPU.\nThis is evidenced by the fact that all GPUs run at the same level -- 30% or so.\n", "@cgel: Apologies for the brief hallucination: what I said above is of course wrong. \n", "@vrv: Do you have any guesses as to the problem?\n", "No guesses, it would be useful to use the timeline.py and chrome trace viewer to see where there may be gaps in execution. \n", "Without more information there's not much we can do -- closing for now but feel free to reopen if you have more information!\n", "How to interpret the gaps or blanks between the ops in the timeline?"]}, {"number": 2440, "title": "Experimental makefile support", "body": "I'm working on automatically generating a makefile from our Bazel dependencies, to make it easier to cross-compile for mobile platforms, and build on systems that can't handle Bazel's resource requirements or dependencies. Supports OS X, Ubuntu, iOS, and Android currently.\n", "comments": ["@petewarden, does the makefile also generate the ops files in tensorflow/cc/ops?\n\nI checked the makefile but didn't find it,\n", "@cg31 No, it's focused on the C++ core needed to load and run pre-trained inference models, which doesn't include the generated cc ops and headers. In this it's similar to the Android libraries we build with Bazel. We haven't defined this core subset of files explicitly before, so I'm hoping to document what we're doing a bit more.\n\nIt would also be possible to add the cc ops to the makefile, but I'm trying to keep it as minimal as possible since my focus is porting to resource-limited platforms.\n", "@petewarden Thanks for your contributions on this: the makefile works as advertised. I am running on OS X and can use this to produce the libtensorflow-core.a. I can build and run the example benchmark tool and it works.\n\nNow I am trying to create a tool in my development paradigm (which utilizes CMake) that links against this lib. I can load a graph in (using ReadBinaryProto), but when I try to create a TF session like this:\n\n> tensorflow::GraphDef reinspect_graph_def;\n> tensorflow::Status graph_load_status = ReadBinaryProto(tensorflow::Env::Default(), reinspect_graph_file, &reinspect_graph_def);\n> ...\n> tensorflow::SessionOptions sess_opts;\n> std::unique_ptrtensorflow::Session session_reinspect(tensorflow::NewSession(sess_opts));\n> tensorflow::Status session_create_status = session_reinspect->Create(reinspect_graph_def);\n\nI get this run time error:\n\n> E tensorflow/core/common_runtime/session.cc:58] Not found: No session factory registered for the given session options: {target: \"\" config: } Registered factories are {}.\n> Segmentation fault: 11\n\nAny idea what us causing that error? My tool links against libtensorflow-core.a as well as the protobuf library that is installed on my machine. Perhaps I need to link against some other libs?\n", "@timbrucks thanks for the update on your progress! Unfortunately that's a problem I know well, and the summary is that the linker is stripping out global constructors it mistakenly thinks aren't needed.\n\nThe longer version is that most components in TF are registered using C++ macros that look like REGISTER_KERNEL(\"SomeOp\", SomeKernelClass).\n\nUnder the hood, those create a global variable of a C++ class that registers the component with the main registry as part of its constructor. The problem is, that global is just an implementation detail, and is never referenced by any code. This leads many linkers to conclude that it's not needed and can be removed, even though its constructor has an important side-effect.\n\nThe only reliable solution we have is to force the linker to avoid stripping any code from the library, even if it thinks it's not used. There are a variety of different ways to specify that, depending on your platform. On OS X/iOS you use -force_load with the path to the static library. On Ubuntu, we use --whole-archive:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L124\n\nIn your case, you should be able to pick the correct linker flags for the platform and it will avoid stripping the global that does the factory registration that's causing your error. Hopefully the linker flags in the makefile should help there.\n", "@petewarden thanks for the very clear explanation, I was pretty confused. Per your suggestion I was able to get the linker to stop stripping code from the library. I now have the TF core code integrated into my development paradigm (ahead of schedule too ... I did not think I could get it done before TF arrived at version 1.0).\n\nIf anyone is trying to do something similar via CMake, the step that overcame the linker issues was adding \"-all_load\" within the target_link_libraries() call in the CMakeLists.txt file\n", "@timbrucks you're not considering how slow we will be getting to 1.0. :)\n", "This looks useful. Does it work?\n\nIf so, should we create a build for it?\n", "We should. Its main purpose is to create an iOS library, for which we could\ncreate a build on the Mac.\n\nOn Thu, Jun 2, 2016 at 10:53 PM Jan Prach notifications@github.com wrote:\n\n> This looks useful. Does it work?\n> \n> If so, should we create a build for it?\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2440#issuecomment-223496634,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_UFDiO5H3CEANTW5aR_H8luBttf_ks5qH8FPgaJpZM4IjXMh\n> .\n", "I thought it is there for ios. More pain to maintain macs. Ok, I'll leave macs to caisq or gunan ;-)\n", "thank you"]}, {"number": 2439, "title": "Plots in tensorboard overlay when change horizontal axis", "body": "## Problem\n\nWhen using tensorboard, if the axis is changed from 'step' to 'relative' and back to 'step' quickly a new graph is created and overlays the original one.  This looks confusing as in the images below.  Note that the issue can be cleared by refreshing the page.\n\nSmall graph, both of the graphs shown here have an extra graph overlayed.\n![alt text](https://www.breadboardkiller.com.au/img/misc/tensorboard_issue_2.png)\nIf the graph is expanded, the old graph stays small and the new graph is enlarged showing the double graph issue more clearly.\n![alt text](https://www.breadboardkiller.com.au/img/misc/tensorboard_issue_1.png)\n### Environment info\n\nOperating System: Fedora release 23\nTensorflow Version: 0.7.1\n### Steps to reproduce\n1. Run tensorboard from a directory with some tensorflow summary files.\n2. Open tensorboard in on the 'Events' page.\n3. Quickly change the 'Horizontal Axis' from 'Step' to 'Relative' and back to 'Step' (ie. all within < 1 second)\n4. Graphs are now reproduced as described above.\n### Workaround\n\nJust refresh the page if you accidentally do this.  \nWould be worth fixing if somebody is aware of the code responsible and it's an easy enough fix.\n", "comments": ["Thanks for the report. Because there's an easy workaround, and it seems to come up pretty rarely, I'm not going to prioritize this right now.\n", "Closing due to inactivity. If this is still happening, please feel to open an issue in our new repository at https://github.com/tensorflow/tensorboard/issues. Thanks!"]}, {"number": 2438, "title": "Add quick link to check compute capability", "body": "The same link also appears under \"Configure TensorFlow's canonical view of Cuda libraries\", but people might overlook it.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2437, "title": "Loaded cudnn library: 4007 but source was compiled against 4004", "body": "when I install the tensorflow from souces in my ubuntu14.04 system, a cudnn version does not match  problem happened.\nOmit some fine steps, the following is my installation steps:\n### 1.Configure the cuda installation,  CUDA 7.5 and cudnn v4.0.7  have been chosen:\n\nroot@node4:/home/zhouzhiqiang/tensor_5_9/tensorflow# ./configure \nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 4.0.7\nPlease specify the location where cuDNN 4.0.7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \nSetting up Cuda include\nSetting up Cuda lib64\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\n### 2. Create the pip package and install\n\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n\nsudo pip install /tmp/tensorflow_pkg/tensorflow-0.8.0-py2-none-any.whl\n### After the installation, The following example was carried out:\n\npython tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py\n### Log for printing:\n\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:286] Loaded cudnn library: 4007 but source was compiled against 4004.  If using a binary install, upgrade your cudnn library to match.  If building from sources, make sure the library loaded matches the version you specified during compile configuration.\nW tensorflow/stream_executor/stream.cc:301] attempting to perform DNN operation using StreamExecutor without DNN support\nTraceback (most recent call last):\n  File \"tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py\", line 236, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py\", line 232, in main\n    run_benchmark()\n  File \"tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py\", line 221, in run_benchmark\n    time_tensorflow_run(sess, pool5, \"Forward\")\n  File \"tensor_5_9/tensorflow/tensorflow/models/image/alexnet/alexnet_benchmark.py\", line 177, in time_tensorflow_run\n    _ = session.run(target)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 332, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 572, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 652, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 672, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InternalError: cuDNN launch failure : input shape([128,3,225,225]) filter shape([11,11,3,64])\n         [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 4, 4, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable/read, conv1/weights/read)]]\n### Print notes that cudnn version  does not match, but in reality,  I haven't installed cudnn v4.0.4\uff0c and  cudnn v4.0.7 is configured  at installation time. Does anyone know why?\n", "comments": ["No one has ever met??\n", "If you grep for \"CUDNN_PATCHLEVEL\" in your \"/usr/local/cuda/include/cudnn.h\", what does it say? \n", "@zheng-xq \n\n### CUDNN_PATCHLEVEL in cudnn.h described as follows:\n\n#if !defined(CUDNN_H_)\n#define CUDNN_H_\n\n#define CUDNN_MAJOR      4\n#define CUDNN_MINOR      0\n#define CUDNN_PATCHLEVEL 7\n\n#define CUDNN_VERSION    (CUDNN_MAJOR \\* 1000 + CUDNN_MINOR \\* 100 + CUDNN_PATCHLEVEL)\n", "@zheng-xq \nwhen I run the following example:\nbazel build -c opt  --config=cuda  //tensorflow/cc:tutorials_example_trainer\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n\nit can be executed correctly, print as shown below:\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \n", "If you grep third_party/gpus/cuda/include/cudnn.h, do you see the same thing? You could also try to set LD_LIBRARY_PATH to point to the right library. \n", "Another possibility is that you are using an older TensorFlow binary. Please check around and make sure you didn't have those. Also look under /tmp/tensorflow_pkg as well. \n", "@zheng-xq \n\nyeah, seems I don't  have the right to replace the install file,  it is ok after recompiled. thank you !\n", "I am having same issue: I am having same issue and tried everything.\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:347] Loaded runtime CuDNN library: 5004 (compatibility version 5000) but source was compiled with 5103 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\nF tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \nAborted (core dumped)\n\nI tried uninstall and install several times with cuda 7.5 and 8. I tried cudann 4, 5, 5.1. NOthing worked. \nWhat else I can do?\n", "@oakkas, the issue was that your code was compiled with Cudnn 5.1, but the loaded Cudnn library was 5.0. They might not be completely binary compatible. That is why stream-executor complains. \n\nDo you have multiple Cudnn library installed on your machine? Please make sure the one you pointed out to with \"configure\" is the one that is actually loaded. You can set LD_LIBRARY_PATH to manually make sure. \n"]}, {"number": 2436, "title": "Typo fix in master_session.cc comment ", "body": "Just a typo fix.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for the fix!\n"]}, {"number": 2435, "title": "sync with supported data types in cwise_op_*.", "body": "This is an updated version of PR #2244 after the comp128 support from @ibab and based on comments from @girving and @zheng-xq.\n\nI created additional UNARY macros as suggested and used the corresponding macros for each op.\n\nAll python tests passed including `compat:backwards_compatibility_test`.\n\n``` bash\nbazel test //tensorflow/python/...\n\n...\n\n//tensorflow/python:pooling_ops_3d_test                                  PASSED in 20.1s\n\nExecuted 1 out of 196 tests: 196 tests pass.\n\n$ bazel test //tensorflow/core/ops/compat:backwards_compatibility_test\nINFO: Found 1 test target...\nTarget //tensorflow/core/ops/compat:backwards_compatibility_test up-to-date:\n  bazel-bin/tensorflow/core/ops/compat/backwards_compatibility_test\nINFO: Elapsed time: 1.988s, Critical Path: 1.48s\n//tensorflow/core/ops/compat:backwards_compatibility_test                PASSED in 0.1s\n\nExecuted 1 out of 1 tests: 1 test passes.\n```\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@googlebot done already!\n", "@hunkim: The commit author is currently \"Your name\" (you can see this if you do `git log`).  The CLA check won't pass until you fix the author to your email address. \n", "I recently had the same issue, fixed it by following instructions at https://help.github.com/articles/changing-author-info/ to rewrite the commit history with correct author name/email\n", "Thanks for fixing this.  The file change looks good, but we have to edit the history before the test passes, right?  I suppose I can find out: Jenkins, test this please.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@yaroslavvb Thanks! It is fixed. @girving It seems the Jenkins tests for this PR stopped. :-)\n", "Jenkins, test this please.  I think it stopped since you rewrote the commit: it isn't smart enough to notice that the tree didn't change.\n", "@josh11b: Does it make sense to you that the backwards compatibility test is passing with this change?  It's specifically removing registered dtypes.  The change is good, but it's unnerving me that history surgery wasn't required. \n", "@girving @josh11b It seems `//tensorflow/core/ops/compat:backwards_compatibility_test` does not work properly. I added this for op name: \"Sin\" to make the test fail:\n\n``` diff\n--- a/tensorflow/core/ops/compat/ops_history.v0.pbtxt\n+++ b/tensorflow/core/ops/compat/ops_history.v0.pbtxt\n@@ -16883,7 +16883,8 @@ op {\n         type: DT_DOUBLE\n         type: DT_INT32\n         type: DT_COMPLEX64\n-        type: DT_INT64\n+        type: DT_STRING\n+        type: DT_SHOULD_FAIL\n       }\n     }\n   }\n```\n\nI think the test should fail, but it passes:\n\n``` bash\nINFO: Elapsed time: 3.883s, Critical Path: 0.63s\n//tensorflow/core/ops/compat:backwards_compatibility_test                PASSED in 0.6s\n```\n\nDid I miss something?\n", "Hi there! I was wondering if this pull request adds support for complex number types to [Tensor Transformation Ops](https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#tensor-transformations) to the GPU.\n\nI was testing this [code](https://github.com/ska-sa/montblanc/blob/867afab028e59c6675ef39d7e89b15fe2b250401/montblanc/tensorflow/rime_ops/test_b_sqrt.py#L114) on last night's GPU build and ran into\n\n``` python\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'pack_7': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n     [[Node: pack_7 = Pack[N=4, T=DT_COMPLEX64, _device=\"/device:GPU:0\"](Complex_4, Complex_5, Complex_6, Complex_7)]]\n```\n\nfor complex64 and\n\n``` python\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'pack_7': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n     [[Node: pack_7 = Pack[N=4, T=DT_COMPLEX128, _device=\"/device:GPU:0\"](Complex_4, Complex_5, Complex_6, Complex_7)]]\n```\n\nfor complex128.\n\nLet me know I should open a separate issue/PR for this.\n\n**EDIT:** _On second thoughts, implementing generic packing and transposes on a GPU is not a trivial operation, I might be expecting too much from these ops on the GPU, for all types._\n", "@sjperkins I think creating a separate issue/PR would be better to keep this PR small.\n", "@hunkim: There was a bug in the backwards compatibility check which Josh fixed.  We'll let you know when that's checked in and pushed.  It's probably easiest to wait until after the test is fixed to update your change for it.  Thank you for uncovering yet another bug!\n\n@sjperkins: Yes, that should be a separate PR.  I think it's orthogonal to this one, so it could happen at any time. \n", "@girving Cool! \n", "@girving is this ready to test / merge?\n", "@vrv: No, a change which Josh submitted internally on Friday is still not external, so we can't make progress.  Is it possible to speed up the lag when externalizing internal changes?\n", "I have the external push out for testing.\n", "Ok, merged the push.  rebase and try again?\n", "Let me know if there is anything I can do on my side.\n", "@hunkim: The internal change fixing the test is finally out, so if you rebase the backwards compatibility test should fail again.  Can you verify that and then do the necessary history surgery to make it pass?\n", "@girving Updated tensorflow/core/ops/compat/ops_history.v0.pbtxt, and my local test passed!\n\n``` bash\nINFO: Found 1 test target...\nTarget //tensorflow/core/ops/compat:backwards_compatibility_test up-to-date:\n  bazel-bin/tensorflow/core/ops/compat/backwards_compatibility_test\nINFO: Elapsed time: 1.267s, Critical Path: 0.01s\n//tensorflow/core/ops/compat:backwards_compatibility_test       (cached) PASSED in 0.4s\n```\n", "Looks good!  You'll need to squash before submit to avoid a commit with failing tests.  Jenkins, test this please, however.\n", "@hunkim: Can you squash?\n", "We can squash for them when merging now.\n\nOn Tue, May 24, 2016, 9:20 AM Geoffrey Irving notifications@github.com\nwrote:\n\n> @hunkim https://github.com/hunkim: Can you squash?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2435#issuecomment-221324950\n", "@vrv: Ah, thanks Vijay!  I'll merge as soon as we do a quick check whether this will break an internal-only test.  @hunkim: No need to squash. \n", "@girving cool!\n", "@hunkim: Thank for the fix! \n"]}, {"number": 2434, "title": "Branch 122741738", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 2433, "title": "Feature needed: configuration file", "body": "Is there any configuration file which can be used to configure the default behavior such as which gpu device to use and so on? Just like the .theanorc file in package Theano? If there isn't, I hope some one can add this feature in the future code.\n", "comments": ["This sounds like an potentially useful feature, but it won't be trivial to develop a good version of this. Assigning to @martinwicke for prioritization.\n", "This does look like a useful feature. There are a number of such configuration options exposed as Environment variables, with appropriate refactoring it would be straightforward to expose them as \"check environment, if not set, read from config file, if not found, apply default\" in a uniform manner. \n", "That said, this is not going to be worked on right now, if someone wants to pick it up, I'd be delighted to get a PR.\n", "@martinwicke I can't find any documents which explain which Environment variables can be set to what in TF, can you please show me a link to some related documents or just give me a heuristic example?\n", "There are a bunch. The best (and maybe only) way to find out is to grep for `getenv`. I know this is a sad answer.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 2432, "title": "Feature Request: Staircase function", "body": "Right now for learning rates we have `exponential_decay`, which is useful but doesn't handle fine-tuned scheduling. For example, regardless of whether we did this manually or with `exponential_decay`, there'd be boilerplate code to use a learning rate of 1.0 for 100000 steps, 0.5 for the next 10000 steps, and 0.1 for all remaining steps.\n\nThis could be handled with a staircase function, maybe with usage like this:\n\n```\nboundaries = [100000, 110000]\nvalues = [1.0, 0.5, 0.1]\nlearning_rate = staircase(global_step, boundaries, values)\n```\n\nThis would create a `Tensor` that evaluates to 1.0 when `x <= 100000`, 0.5 when `x > 100000` and `x <= 110000`, and 0.1 when `x > 110000`.\n\nShouldn't be hard to build using `tf.case`.\n\nIs this of interest or too specific?\n", "comments": ["Update: here is the code without the necessary error checks, conversions, tests that'd be necessary for a pull request. Can do this if we think it might be accepted.\n\n```\ndef staircase(x, boundaries, values):\n    \"\"\" Staircase function.\n\n    Arguments:\n        x: A 0-D Tensor.\n        boundaries: A 1-D NumPy array with strictly increasing entries.\n        values: A 1-D NumPy array that specifies the values for the intervals\n            defined by `boundaries`. (It should therefore have one more entry\n            than `boundaries`.)\n\n    Returns: A 0-D Tensor. Its value is `values[0]` when `x <= boundaries[0]`,\n        `values[1]` when `x > boundaries[0]` and `x <= boundaries[1]`, ..., and\n        values[-1] when `x > boundaries[-1]`.\n    \"\"\"\n\n    pred_fn_pairs = {}\n    pred_fn_pairs[x <= boundaries[0]] = lambda: tf.constant(values[0])\n    pred_fn_pairs[x > boundaries[-1]] = lambda: tf.constant(values[-1])\n    for lower, upper, value in zip(boundaries[:-1], boundaries[1:], values[1:-1]):\n        pred_fn_pairs[(x > lower) & (x <= upper)] = lambda: tf.constant(value)\n\n    return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]), exclusive=True)\n```\n", "This seems like it could be a useful contribution!\n", "Btw, for very cheap use cases such as adjusting learning rate and other hyper-parameters, what I did is simply to compute the value in python and assign it to the corresponding variable in TF, once a while. This should be easy enough and have negligible overhead.\n"]}, {"number": 2431, "title": "Better support for breaking up too-large operations", "body": "Unless I'm missing something, the ability to automatically break up calculations that don't fit on a device does not seem to be part of the TensorFlow API. This surprises me since one of the very first things users (at least naive ones like me) encounter in readily accessible GPU machines (e.g. AWS EC2 instances) are memory crashes on GPUs. \n\nTo avoid these errors users have to either give up on GPUs for large parts of their calculations (again, unless I'm missing something) or hand code some form of batching to avoid the crashes (if they can figure out where they're happening, which isn't always easy with such errors).\n\nShouldn't TensorFlow do this automatically \"under the hood\", breaking up too-large calculations and merging them, to avoid code like that below?\n\n---\n\n_From a [related question](http://stackoverflow.com/q/37327312/656912) asked on SO:_\n\nI'm puzzled about how to efficiently assign my TensorFlow operations and variables to devices. It's clear that, at least for my implementation of a basic convolutional neural network, placing as many operations as possible on a GPU is desirable. But the GPU I currently have access to has limited memory and results in many warnings of the form\n\n> `Ran out of memory trying to allocate 2.60GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.`\n\nand occasional crashes for certain specific operations, like\n\n> ```\n> Ran out of memory trying to allocate 83.74MiB.  See logs for memory state.\n> Resource exhausted: OOM when allocating tensor with shape[28000,1,28,28]\n> ```\n\nThese can be avoided by placing variables on CPUs, but in my implementation, this results in training epochs taking 10 times as long to compute. \n\nClearly the ideal policy is to identify specific chunks of code that generate errors, and attempt to place only those on CPUs. But it is unclear to me how to do this, because those calculations can't be isolated from others that require GPU placement to achieve efficiencies.\n\nFor example, simply generating predictions on a test set with something like\n\n```\nevals = sess.run(tf.argmax(y, 1), feed_dict={x: use_x_all})\n```\n\nwhere `x` is a `tf.placeholder` of inputs to my model, and `y` are the output activations of my network, produces the above error when `use_x_all` is a large array (here with `28000` examples). Attempting to put this calculation alone on a CPU fails, presumably because the network evaluation producing `y` is on the GPU.\n\nBecause of this I (seem to) need to resort to approaches like\n\n```\nuse_x_all, _ = data_loader.stack_data(use_data, as_cols=False)\nuse_x_split = np.split(use_x_all, splits)\nfor use_x in use_x_split:\n    # ...\n    evals_part = sess.run(tf.argmax(y, 1), feed_dict={x: use_x})\n    # accumulate evals\n```\n\nwhich clearly doesn't scale.\n\nIs there a better way? Specifically:\n- Is there a way to place calculations like the one above on a CPU and still have those calculations for the same graph (e.g. training) run on a GPU?\n\nor, alternatively\n- Is there an idiom (like batching) that can be more easily applied in such situations to reduce the memory demands of such calculations?\n", "comments": ["Why does your `np.split(...)` approach not scale? This is an easy way to proceed if your dataset fits in host memory. For larger datasets, you can use the standard reader pipeline to read batches of input at a time: e.g. see the `batch_inputs()` function in the [Inception model](https://github.com/tensorflow/models/blob/dc7791d01c9a6b1fcc40e9e2c1ca107cbd982027/inception/inception/image_processing.py#L407). You could also try [`tf.train.batch()` and related functions](https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#batching-at-the-end-of-an-input-pipeline) to control how the inputs are batched.\n\nAs to why TensorFlow doesn't do this automatically, there are clearly [lots of different ways to do batching](https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#batching-at-the-end-of-an-input-pipeline), and TensorFlow can't reliably infer the user's intent. Therefore, we provide higher level libraries to allow users to build the appropriate input pipelines. \n", "@mrry \u2014 At least in this context, it's not an issue of the many ways to do batching, but a simple matter of  sequencing: doing something in parts rather than all at once (as in the hand-coded version). What doesn't scale is that I need to ferret out every place where this happens, and write code like I did above.\n\nBut I see the problem: its not easily generalized, as you say. The data might need shuffling, there could be various ways to combine the results, and not all of it may fit in memory at once, etc.\n\nSo I guess as long as I'm \"doing it right\", I'm not worried; and the question boils down to that: Is it idiomatic to be manually breaking up and reassembling data fed to TF operations as needed when they result in calculations that are too big for the hardware?\n", "@mrry \u2014 If I've got that right, it would be extremely helpful to see how I might rewrite the full code example in [my SO question](http://stackoverflow.com/q/37327312/656912) to take advantage of some of the related TF API (e.g., `train.batch` and `batch_join`, perhaps).\n", "@orome I've written a tool called [hypercube](https://github.com/ska-sa/hypercube) to reason about problem sizes and memory requirements that you may find useful.\n", "This is mostly questions and requests for help rather than an issue, so I'm going to close to keep the issue tracker focused.\n", "@girving: More of a feature request. What's the home for those?\n", "@orome: Unfortunately I think the feature request to have TensorFlow automatically shard ops is too broad to leave as a Github issue. \n", "@girving: Not saying it should be here; but wondering where the home for general discussion of features requests is (where they can evolve to more specific issues).\n", "discuss@tensorflow.org is one option, but on second thought leaving this as an issue is fine.\n", "@girving: It's certainly within the scope of what's spelled out in the issue submission template.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Hey, I'd like to calculate a matrix multiplication with one sample tensor which does not fit into memory.\r\nNow I thought about batching the samples and summing up the results.\r\n\r\nIn OpenMP this would be a simple omp parallel reduction.\r\nIn Tensorflow, it is a rather big bunch of code to write a dedicated input pipeline, create a while loop, assign-adding the values, ...\r\nAlso, the user has to decide how big the batches are allowed to be.\r\n\r\nHave there been any updates on simplifying this?\r\nI'd love something like a map-reduce option which would automatically decide which batch size it can use."]}, {"number": 2430, "title": "Documentation describes Python 3 Anaconda installation but no support for current Anaconda Python 3 (i.e. Python 3.5)", "body": "Anaconda uses Python 3.5 as its version of Python 3.\n\nIf you follow the [instructions ](https://storage.googleapis.com/tensorflow/linux/cpu/)to install Tensorflow via Anaconda you'll fail with the error \n\n> not a supported wheel on this platform\n\npresumably because the wheel is made for Python 3.4 instead of 3.5\n", "comments": ["Workaround is to build from source or rename the wheel file.\n", "Renaming the wheel file with py3-none in place of the version information worked for me.\n", "With PR #2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: \nhttps://github.com/tensorflow/tensorflow/\n\nLinux Python 3.5 whl files will also be included in future releases.\n"]}, {"number": 2429, "title": "TypeError: List of Tensors when single Tensor expected when using tf.pack()", "body": "I  get a confusion problem. I can run  code well on my computer. But when I copy the code to the cluster and run it. I get the error: `TypeError: List of Tensors when single Tensor expected.` on this line `outputs     =   tf.pack(outputs)`.  The input of pack function should be list. That is so wired. Why? The version of TF on cluster is 0.8.\n", "comments": ["What is the version of TF on your computer? (The nightly build contains additional support for automatically packing nested lists of tensors, which will not work with older versions.)\n", "@mrry  Hi mrry, the version of TF on my computer is 0.7, it works fine on tf.pack(outputs). But the version 0.8 on cluster can not. Can you tell me what  I can do to solve the problem on cluster please?\n", "Can you share what the value of `outputs` is?\n", "@mrry \nHere is the code:\n\n```\nw_x2h   = tf.get_variable(\"w_x2h\", [word_dim, 2*hidden_dim])\nb_x2h    = tf.get_variable(\"b_x2h\", [2*hidden_dim])\nself.x     = tf.placeholder(tf.float32, [None, sentence_length, word_dim])  \nself.real_length  = tf.placeholder(tf.int64, [None])  \nself.istate_fw      = tf.placeholder(tf.float32, [None, 2*hidden_dim])\nself.istate_bw     = tf.placeholder(tf.float32, [None, 2*hidden_dim])\n\ndef RNN(x,  istate_fw, istate_bw,   real_length,  w_x2h, b_x2h):\n            x = tf.transpose(x, [1, 0, 2]) \n            x = tf.reshape(x, [-1, word_dim]) \n            x = tf.matmul(x, w_x2h) + b_x2h\n\n            lstm_fw_cell   = rnn_cell.BasicLSTMCell(hidden_dim, forget_bias=1.0)\n            lstm_bw_cell   = rnn_cell.BasicLSTMCell(hidden_dim, forget_bias=1.0)\n\n            x = tf.split(0, sentence_length, x) # sentence_length * (batch_size, hidden_dim)\n            outputs = rnn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, initial_state_fw=istate_fw, initial_state_bw=istate_bw, sequence_length=real_length)\n\n            return outputs\n\noutputs  =  RNN(self.x,  self.istate_fw, self.istate_bw, self.real_length, w_x2h, b_x2h)\n```\n", "It looks like you are using `rnn.bidirectional_rnn()`. The return type of that function changed from [a single tensor in 0.7](https://github.com/tensorflow/tensorflow/blob/657f1d987aca211e775acbccf719e67864a0be70/tensorflow/python/ops/rnn.py#L323) to [a tuple of three tensors in 0.8](https://github.com/tensorflow/tensorflow/blob/4b7bc3174ed67b4a0eb1803537c9d00f132e9ae7/tensorflow/python/ops/rnn.py#L373).\n\nThis explains the type error when using two different versions of TensorFlow.\n\nWhen running on 0.8, you should probably change the last line of your code snippet to ignore the two new return values as follows:\n\n``` python\noutputs, _, _ = RNN(self.x, self.istate_fw, self.istate_bw, self.real_length, w_x2h, b_x2h)\n```\n", "@mrry Thank you so much!!!\n"]}, {"number": 2428, "title": "GloVe implementation for Tensorflow", "body": "I think [GloVe](http://nlp.stanford.edu/projects/glove/) can be an useful embedding model for NLP.\nThe patch would be very similar to the current /models/embedding/word2vec \n", "comments": ["We're moving models over to their own repository. Feel free to contribute this to the https://github.com/tensorflow/models repository (or open an issue over there).\n", "As a brief update, I just noticed that someone has published an implementation of this model on GitHub: https://github.com/GradySimon/tensorflow-glove\n", "Yes, but is far from being mergeable.\nI was thinking to also add a faster implementation with specialized tf ops.\n\nThere is also the reference code implemented in C that potentially could be\nwrapped in one single op in user space.\n\nWhat would be the preferred way to be part of the model library?\n\nOn Thursday, May 19, 2016, Derek Murray notifications@github.com wrote:\n\n> As a brief update, I just noticed that someone has published an\n> implementation of this model on GitHub:\n> https://github.com/GradySimon/tensorflow-glove\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2428#issuecomment-220445588\n\n## \n\nLinkedIn: http://linkedin.com/in/fmilo\nTwitter: @fabmilo\n\n## Github: http://github.com/Mistobaan/\n\nSimplicity, consistency, and repetition - that's how you get through. (Jack\nWelch)\nPerfection must be reached by degrees; she requires the slow hand of time\n(Voltaire)\nThe best way to predict the future is to invent it (Alan Kay)\n"]}, {"number": 2427, "title": "Error on retraining flowers", "body": "Creating bottleneck at /tmp/bottleneck/roses/8949720453_66e8304c30.jpg.txt\nTraceback (most recent call last):\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 827, in <module>\n    tf.app.run()\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 754, in main\n    bottleneck_tensor)\n  File \"/home/ubuntu/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 676, in add_final_training_ops\n    bottleneck_input = tf.placeholder_with_default(\nAttributeError: 'module' object has no attribute 'placeholder_with_default'\n", "comments": ["Which version of TensorFlow are you running? You will need to upgrade to version 0.8 for that example code to work.\n", "Thanks @mrry \n"]}, {"number": 2426, "title": "Supporting negative reduction indices.", "body": "The reduce operations do not support negative indices as opposed to their numpy equivalents. In particular, the following code fails with the error message `ValueError: Invalid reduction dimension -1 for input with 2 dimensions`.\n\n``` python\nimport tensorflow as tf\nimport numpy as np\n\n# Create reference values in numpy\nx = np.random.normal(0, 1, (40, 50))\ny = np.sum(x, axis=-1)\n\n# Try to reproduce the same in tensorflow\nsess = tf.InteractiveSession()\ntf_x = tf.constant(x)\ntf_y = tf.reduce_sum(x, -1)\nnp.testing.assert_allclose(tf_y.eval(), y)\nsess.close()    \n```\n\nThe code that checks the reduction indices and throws the exception appears to know about the dimensionality of the vector (taken from [_ReductionShape](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py#L1610)).\n\n``` python\nfor reduction_index in reduction_indices:\n    if reduction_index < 0 or reduction_index >= input_shape.ndims:\n        raise ValueError(\"Invalid reduction dimension %d for input with %d dimensions\" % (reduction_index, input_shape.ndims))\n```\n\nI have thus started using the following function to support negative indices.\n\n``` python\ndef _reduction_indices(input_tensor, reduction_indices):\n    # Get the shape and convert indices to a list\n    input_shape = input_tensor.get_shape()\n    reduction_indices = np.ravel(reduction_indices)\n\n    # Convert the indices\n    return [(input_shape.ndims + index) if index < 0 else index for index in reduction_indices]\n```\n\nWould this be worth integrating into the main repository or is the lack of support for tensors with completely unknown shape problematic?\n", "comments": ["Thanks for the suggestion - this looks like it would be better to implement in the C++ code (with a corresponding relaxation to shape function. @keveman is cooking up a patch as we speak.\n", "Great news, thanks for implementing this @keveman. On a related note, do you know whether other functions such as `scatter_update` are going to get axis support to simplify things like [this](http://stackoverflow.com/questions/37307519/packing-array-into-lower-triangular-of-a-tensor)?\n"]}, {"number": 2425, "title": "Initializing saver after variables are initialized and queue runners are started causes inconsistent behaviour.", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:  Mac OsX (El Capitain)\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.8.0\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1.  Run the CNN example provided by tensorflow using a filename queue\n2.  Initialize variable and queue runners\n3.  Initialize a saver \n### What have you tried?\n1.  Tried running the exact same code with a saver that saves no values and no saver.  Simply initializing the saver at the wrong time is sufficient to cause weird behaviour.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nApp will crash intermittently somewhere completely unrelated in code (usually native).\n\nI know this is user error, but it would have been really helpful if an exception had been raised.  I wasted a lot of time trying to hunt down a mystery error when it turned out it was the saver all along.  This can be confusing to beginners such as myself.\n", "comments": ["The solution is to start the queue runners after you have finished building the graph (which includes the saver). Does that work for you?\n", "Like I said, I understand this is user error.  I just don't see why this shouldn't be disallowed.  As a beginner it's really confusing to see unrelated crashes and there isn't an easy way to debug native code.  \n\nIt's an easy mistake to make and the only feedback is random crashing.\n", "We've gone back and forth on disallowing it&mdash;for example, we could call `tf.Graph.finalize()` in the `tf.train.start_queue_runners()` method, but that would break a lot of code that _mostly_ works. However, it seems reasonable to log an ominous warning, to discourage this kind of code. At least it should help debugging. I'll prepare a change.\n"]}, {"number": 2424, "title": "couldn't find \"libtensorflow_demo.so\"", "body": "I got this error message after ran on Android 5\n\n```\n05-19 13:21:33.731 27653-27653/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main\n                                                                     Process: org.tensorflow.demo, PID: 27653\n                                                                     java.lang.UnsatisfiedLinkError: com.android.tools.fd.runtime.IncrementalClassLoader$DelegateClassLoader[DexPathList[[dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_9-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_8-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_7-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_6-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_5-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_4-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_3-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_2-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_1-classes.dex\", dex file \"/data/data/org.tensorflow.demo/files/instant-run/dex/slice-slice_0-classes.dex\"],nativeLibraryDirectories=[/vendor/lib, /system/lib, /vendor/lib, /system/lib]]] couldn't find \"libtensorflow_demo.so\"\n                                                                         at java.lang.Runtime.loadLibrary(Runtime.java:366)\n                                                                         at java.lang.System.loadLibrary(System.java:989)\n                                                                         at org.tensorflow.demo.TensorflowClassifier.<clinit>(TensorflowClassifier.java:47)\n                                                                         at org.tensorflow.demo.TensorflowImageListener.<init>(TensorflowImageListener.java:56)\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.<init>(CameraConnectionFragment.java:445)\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.newInstance(CameraConnectionFragment.java:266)\n                                                                         at org.tensorflow.demo.CameraActivity.onCreate(CameraActivity.java:33)\n                                                                         at android.app.Activity.performCreate(Activity.java:6289)\n                                                                         at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1119)\n                                                                         at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2646)\n                                                                         at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2758)\n                                                                         at android.app.ActivityThread.access$900(ActivityThread.java:177)\n                                                                         at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1448)\n                                                                         at android.os.Handler.dispatchMessage(Handler.java:102)\n                                                                         at android.os.Looper.loop(Looper.java:145)\n                                                                         at android.app.ActivityThread.main(ActivityThread.java:5942)\n                                                                         at java.lang.reflect.Method.invoke(Native Method)\n                                                                         at java.lang.reflect.Method.invoke(Method.java:372)\n                                                                         at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1389)\n                                                                         at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1184)\n```\n\nP.S: BTW i'm using android studio for this project (converted into gradle project based).\n\nPlease advice. Thank you.\n", "comments": ["Can you check and see if libtensorflow_demo.so is in the apk, and built for the correct target architecture? Just unzip -v to see the contents. It sounds like the native libraries (the .cc code) are either not getting built, or not getting packaged into the apk properly.\n\nGradle support is outside the scope of Tensorflow github issues, but I would suggest looking for differences between [this](https://github.com/miyosuda/TensorFlowAndroidDemo/tree/master/jni-build) known working Gradle/Android build of Tensorflow and yours to see if you can spot the difference.\n", "Hi, were you able to resolve the issue?\n", " Closing, please reopen if problem persists.\n", "I am having this problem as well -- https://github.com/tensorflow/tensorflow/issues/2530\n", "I encountered the same error when moving my project to Android Studio.\nIt can be resolved simply by extracting libtensorflow_demo.so from the APK built by bazel and copying it to the Android Studio project to app/src/main/jniLibs/armeabi-v7a/ .\n", "You can find full instructions for building the Tensorflow demo in Android studio here: https://github.com/tensorflow/tensorflow/issues/3444\n\nBazel is still required, though.\n"]}, {"number": 2423, "title": "More random ops", "body": "Tensorflow currently doesn't support too many random operations. It would be nice to have things like bernoulli, batched normal (the existing normal only takes scalars), beta, gamma, and so on.\n", "comments": ["I'm starting to work on this.\n", "+1.\nIt'd be great to be able to sample from gamma distributions in TF. That would make it possible to sample from many other popular distributions, such as:\n- Inverse-Gamma (just the inverse of a gamma r.v.)\n- Beta/Dirichlet (which are just normalized gamma random vectors)\n- Student t (sample from a normal whose variance is sampled from an inverse-gamma)\n- Gumbel (location-scale transformation of a log-transformed inverse-gamma)\n\nIf the Poisson distribution were also implemented, that would add\n- Negative binomial (sample from a Poisson whose mean is sampled from a gamma)\n", "I'm working on a new truncated normal function with minval and maxval parameters, and batching. Some of the distributions you mentioned are already implemented [in Python](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distributions/python/ops). We could add Gumbel and Poisson distributions there too. I'm not sure if we need beta or negative binomial distributions if they can be calculated easily from the other ones, but it wouldn't hurt to have a Python implementation of them too.\n", "Do you know where I can find the `gen_random_ops` file? It seems like the Gamma sampler uses it, and I can't track how it's implemented.\n", "Hi Dustin,\n\nSorry for the delay. gen_random_ops is generated automatically in the build process from the ops defined in [random_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/random_ops.cc). The Python code itself is generated by [python_op_gen.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/python_op_gen.cc), but each function is just a wrapper for a C++ function that's defined under tensorflow/core/kernels/.\n", "Any chance we could get a categorical distribution? This would make things like sampling from RNN language models much simpler.\n", "@vladfi1 Do you need something that's not offered by tf.multinomial?\n", "@ringw I think `tf.multinomial` is good, but I don't see a way to sample batches of normals (or other continuous distributions).\n", "fyi, there are random ops available in sample methods from `tf.contrib.distributions`. although in general, i'm not sure what the plan is for `tf.random_normal([])` vs `distributions.Normal(mu=0.0, sigma=1.0).sample()`\n", "I'm currently working on adding batched parameters to tf.truncated_normal, but currently it's significantly slower than the current implementation (with hard-coded parameters). If we can't match the performance of the current version, then we might want to add samplers with batches (and maybe other bells and whistles) to tf.contrib.distributions, and only add ops for common parameters or use cases that can be hard-coded for speed.\n", "It looks like there are now lots of random ops in `tf.contrib.distributions`, but `Binomial.sample()` is not implemented whereas `Bernoulli.sample()` and `Multinomial.sample()` are both implemented. Are there any plans for `Binomial.sample()`?", "@ebrevdo, could you comment?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "I will probably get back to the improved truncated normal sampling soon, but otherwise I'm not actively working on new random ops.", "This is a very open-ended issue.  If there is a specific random sampling op you want; please open an issue for it.  We've implemented a bunch of new samplers since this issue went in."]}, {"number": 2422, "title": "Why not allow feeding and fetching the same tensor?", "body": "Currently this generates `InvalidArgumentError: Placeholder_2:0 is both fed and fetched.`\n\nIt doesn't seem like a particularly useful thing but why not trivially allow this behavior? We might as well allow users to fetch any tensor in the graph.\n", "comments": ["The answer here is that it would require extra code to be written and tested, and&mdash;as you point out&mdash;it's not particularly useful :).\n"]}, {"number": 2421, "title": "SKLearn examples not working in the latest release", "body": "I've been trying to run some examples from the provided SKFlow examples located at \n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/skflow\n\nAnd most fail with dependency failures such as: \n\nAttributeError: 'module' object has no attribute 'datasets'\n\nFor:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/text_classification_builtin_rnn_model.py\n\nI looked around the source code and some of this modules seem to have been moved around, maybe this examples should be updated or completely removed, I tried adapting the code to work however I ended copying big chunks of source code into the file to make it work.\n\nusing tensorflow 0.8\n", "comments": ["The skflow examples in the master branch have changed to use a new API that isn't present in the 0.8 release. There are two options:\n1. Download the examples from the `r0.8` branch: https://github.com/tensorflow/tensorflow/tree/r0.8/tensorflow/examples/skflow\n2. Install a [nightly version of TensorFlow](https://github.com/tensorflow/tensorflow#installation) and use the examples from the master branch.\n", "Hey, if you dont want to re-install the nightly build, you can manually download the same dataset.  I've documented ez steps here:  http://stackoverflow.com/questions/37206459/tensorflow-examples-all-fail-due-to-attributeerror-module-object-has-no-attri\n"]}, {"number": 2420, "title": "TypeError: an integer is required executing skflow text classification examples", "body": "Using a nightly build I'm unable to execute the text classification examples in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/skflow (I have tried text_classification.py, text_classification_cnn.py and text_classification_builtin_rnn_model.py).\n\n`$ sudo pip3 install --upgrade http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl \n\n$ python3 -c \"import tensorflow; print(tensorflow.**version**)\"\n0.8.0\n\n$ python3 rnn.py\nTotal words: 822383\nTraceback (most recent call last):\n  File \"rnn.py\", line 68, in <module>\n    classifier.fit(X_train, y_train, logdir='/tmp/tf_examples/word_rnn')\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 270, in fit\n    feed_params_fn=self._data_feeder.get_feed_params)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/trainer.py\", line 49, in train\n    feed_dict = feed_dict_fn()\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py\", line 324, in _feed_dict_fn\n    out.itemset((i, self.y[sample]), 1.0)\nTypeError: an integer is required`\n", "comments": ["i think this part hasn't been finished yet. You can modify out.itemset((i, self.y[sample]), 1.0) ->> out.itemset((i, int(self.y[sample])), 1.0). Convert the type of y_train and y_test to numpy array. Besides, it has not implemented continue_training function.  I test cnn, rnn and character-rnn and character-cnn. Both character-level model cant achieve a baseline method.\n", "Thanks @alrojo for the upcoming fix!\n"]}, {"number": 2419, "title": "Push changes from internal: CL 122636949", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@tensorflow-jenkins, test this please.\n"]}, {"number": 2418, "title": "R0.8", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "We have been seeing this type of misoperation PRs from a couple of GH users recently. Not sure what might be the cause of those.\n", "@caisq: In that case I'll close this request, because it seems spurious.\n", "@caisq @mrry Seems like some of theses changes are not on master\n"]}, {"number": 2417, "title": "Add more changes for complex128 support", "body": "This PR contains all ops from @hunkim's #2263, rebased on `master` with a few adjustments.\nI've extended all relevant tests, and added a few more ops that were missing from the PR above.\nWith this PR, support for `complex128` is almost finished (documentation changes and a few ops are missing).\n", "comments": ["Can one of the admins verify this patch?\n", "Looks good! Thanks!!\n", "Can you add a `complex128` version of the the binary op version of `testComplex64Basic` in `cwise_ops_test.py`?\n", "Looks like `complex128` `tf.conj` needs a test too (copy the `complex64` one).\n", "And `complex128` `tf.select` and `tf.pow` (see testOverload for the latter).  Finally `testOverloadComparisons` should test both `complex64` and `complex128`.\n", "One more: `sparse_tensor_dense_matmul` needs to test `complex128`.  That looks like it; otherwise the testing is good.\n", "Thanks for the comments!\n\n`testOverloadComparisons` doesn't test for `complex64` because we can't define `greater/less` ops for it.\nThe second part of `testOverloadComparisons` only seems to operate on booleans.\n(And, strangely, tests `tf.equal` and `tf.not_equal` instead of the overloaded operators).\nShould I add `_EQ` and `_NEQ` lambdas to test the overloading for `==` and `!=`?\nThen it would make sense to add `complex64` and `complex128` to the test.\n\nAlso, what should I do about the ordering of types in various macros and type lists that @hunkim rightfully pointed out?\nI've tried to always append `complex128` at the end, as that's what has seemingly been done for previous types.\n", "Unfortunately, TensorFlow doesn't overload the `==` and `!=` operators: they are the same as `is` for `Tensor` objects in Python.  However, it's worth refactoring that routine so that we do test `equal` and `not_equal` for the two complex types.\n\nPutting `complex64` and `complex128` next to each other is good as he suggests.\n", "Go ahead and add new commits resolving the comments to make it easier to review, and we can squash at the end once everything looks good.\n", "Ah, right, `==` and `!=` aren't overloaded.\nIn this case, would it make more sense to extend `testTensorCompareTensor` and friends?\n(Which also test `tf.equal` and `tf.not_equal`, but don't contain `complex64` and `complex128` at the moment)\n\n**edit:** Ah, `testTensorCompareTensor` also already tests for `complex64` at the bottom\n", "Yep, `testTensorCompareTensor` sounds good.\n", "I think all comments have been addressed and the tests are passing for me on the CPU.\n", "Looks great!  Jenkins, test this please.\n", "Looks like one of the tests failed.\nThis one was `(cached) PASSED` when I ran the tests, so maybe `bazel` didn't pick up the changes.\nWill fix the problem and push again in a moment.\n\n**edit:** Ah, this is a GPU test failure\n", "Yep, looks like transpose complex128 needs to be registered on GPU.\n", "I'm having some problems with extending the GPU kernel for `transpose` to `complex128`.\nCalling `__ldg` on a `complex128*` doesn't compile for me, which means that I can't call\n\n```\ninternal::Transpose<Device, complex128>(d, in, perm, out)\n```\n\nas I did with the CPU version.\nThe call to `__ldg` happens here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/transpose_functor_gpu.cu.cc#L40\nShould I just define a template instance for `complex128` that doesn't use the `__ldg` intrinsic?\n", "Seems like I'm able to call `__ldg` on  `float4`, which should have the right size.\nIs that a correct solution, or did I miss something?\nI've pushed a commit that uses `float4` and makes the `transpose` tests run.\n", "@ibab: What's the error message with `__ldg` on `complex128`?  We do want to preserve `__ldg` semantics here since it can improve performance, so we probably want to write a little wrapper around __ldb in `cuda_kernel_helper.h` that calls `__ldg` twice for `complex128`.\n\n@zffchen78: You recently refactored transpose to be lower compile time; do you have suggestions for how to add `complex128`?  \n\n@ibab: If you're unable to run GPU tests, it's going to be hard to iterate, so it may be worth splitting the changes out of this PR.  I could take over that bit of it if you want.\n", "@ibab: Cool, `float4` sounds perfect.  I'll take a look. \n", "Jenkins, test this please.\n", "Looks like we have some conflicts now.\nThey're very simple in case you want to solve this on your end, otherwise I can push my rebased version.\n", "@ibab: Please push your rebased and squashed version (reduced to one commit).\n", "@girving: Okay, it's pushed.\n", "Woot.  Jenkins, test this please.\n", "Merged.  Thanks again for the contributions!  This will be great to have for applications of TensorFlow outside standard deep nets.\n", "@girving @hunkim: Thanks for reviewing the code!\n"]}, {"number": 2416, "title": "Memory error running tensorflow code on GPU", "body": "### Environment info\n\nOperating System: Scientific Linux release 7.2 (Nitrogen)\n\nInstalled version of CUDA and cuDNN: \n$ ll /usr/local/cuda/lib/libcud*\n\n```\n-rw-r--r-- 1 root root 185K Mar 18 15:29 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root   16 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root   19 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 305K Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 545K Mar 18 15:29 /usr/local/cuda/lib/libcudart_static.a\n```\n\nInstalled version 0.8.0 with anaconda.\n\nI'm getting a MemoryError when I try to run a tensorflow script on a server with GPU support. The same code, with the same inputs, runs without problems on my local machine, which is CPU only and has 8 GB of RAM. \n\nI tried to allocate up to 64 GB to run the script, and the same problem occurred. Here's the stacktrace:\n\n```\nTraceback (most recent call last):\n  File \"src/train.py\", line 93, in <module>\n    learning_rate=args.rate, l2_constant=args.l2)\n  File \"/hltsrv0/rocha/rte-lstm/src/rte_lstm.py\", line 189, in __init__\n    gradients, v = zip(*optimizer.compute_gradients(self.loss))\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 241, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 485, in gradients\n    in_grads = control_flow_ops.tuple(in_grads)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1784, in tuple\n    tpl.append(with_dependencies([gate], t))\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1664, in with_dependencies\n    return _Identity(output_tensor, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 114, in _Identity\n    return array_ops.identity(data, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 609, in identity\n    return _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1165, in __init__\n    self._recompute_node_def()\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1318, in _recompute_node_def\n    self._control_inputs])\nMemoryError\n```\n\nFrom the stack trace, I see that the error happens sometime during the gradient computation, but I have no idea why it only happens when I run the code in the server. \n\nBy the way, I'm not experienced in GPU computation. I just tried running the same code in an environment with the cuda libraries and tensorflow with GPU support installed.\n", "comments": ["This doesn't look like the error is related to the GPU: the `MemoryError` is raised while you are **building** your graph, and before anything will be transferred to the GPU.\n\nIt's difficult to tell what's going wrong here without seeing your `train.py` code. One possibility is that your server has a 32-bit version of Python installed for some reason. Can you try running the following snippet and reporting the result?\n\n``` python\nimport sys\nprint(\"%x\" % sys.maxsize, sys.maxsize > 2**32)\n```\n", "It's 64.\n\n```\nIn [2]: print(\"%x\" % sys.maxsize, sys.maxsize > 2**32)\n('7fffffffffffffff', True)\n```\n\nMy `train.py` code is somewhat big. I will try to isolate the problem and post back.\n", "Ok, I tried some snippets via the interpreter and got a segfault as soon as I started a session:\n\n```\nIn [2]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n\nIn [6]: sess = tf.Session()\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:05:00.0\nTotal memory: 11.25GiB\nFree memory: 11.16GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:06:00.0\nTotal memory: 11.25GiB\nFree memory: 11.16GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:09:00.0\nTotal memory: 11.25GiB\nFree memory: 11.16GiB\nSegmentation fault (core dumped)\n```\n\nI also tried `tf.InteractiveSession` with the same result.\n\nI suppose that something could be wrong with CUDA libraries version... but I have no idea where to begin.\n", "My problem seems to be related to #152. I tried to set `CUDA_VISIBLE_DEVICES=1`, as indicated in that issue, since I'm using a shared server. This made python (but not ipython) work with a simple statement after `-c`, but the interactive shell still results in a segfault:\n\n```\n$ CUDA_VISIBLE_DEVICES=1 python -c \"import tensorflow as tf;tf.InteractiveSession()\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:06:00.0\nTotal memory: 11.25GiB\nFree memory: 11.16GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0)\n```\n\n```\n$ CUDA_VISIBLE_DEVICES=1 python \nPython 2.7.11 |Anaconda 4.0.0 (64-bit)| (default, Dec  6 2015, 18:08:32) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n>>> import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n>>> sess = tf.InteractiveSession()\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:06:00.0\nTotal memory: 11.25GiB\nFree memory: 11.16GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0)\nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\nAborted (core dumped)\n```\n\nAny suggestions about what might be wrong?\n", "@zheng-xq: Do you have any ideas what could be causing an exception/segfault in the GPU device construction code path?\n", "The original `MemoryError` is probably caused by trying to allocate more memory than the machine has available.  In order to debug the segfaults, we'd have to get more information, such as stack traces or information from an attached debugger.\n", "@erickrf, one possibility is that with a shared server, some other processes are using that GPU. Could you run nvidia-smi and make sure no other is using that GPU? \n\nA few experiments to try: \n1. Allocate the server to yourself, if only temporarily, and see if the problem goes away. \n2. Add stack trace or printf near here and see what is causing the GPU construction to fail. \n   https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L587\n", "Automatically closing due to lack of recent activity. Please reopen if it is still an issue.\n", "So I finally could look again into this problem.\n\n@zheng-xq , I could not allocate the server just for myself, but here is the output of `nvidia-smi` at the time of a new similar `MemoryError`:\n\n```\n+------------------------------------------------------+                       \n| NVIDIA-SMI 352.39     Driver Version: 352.39         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:05:00.0     Off |                    0 |\n| N/A   32C    P8    46W / 149W |   3501MiB / 11519MiB |     89%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   69C    P0   172W / 149W |   3800MiB / 11519MiB |     91%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 0000:09:00.0     Off |                    0 |\n| N/A   46C    P0    91W / 149W |   3500MiB / 11519MiB |     84%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 0000:0A:00.0     Off |                    0 |\n| N/A   47C    P8    30W / 149W |     22MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     56514    C   python                                        3477MiB |\n|    1     43200    C   python                                        3776MiB |\n|    2     57034    C   python                                        3476MiB |\n+-----------------------------------------------------------------------------+\n```\n\nSo one of the GPU's was completely free.\nI could not recompile tensorflow with the printf statements yet, because I don't have root access in this server and installing all the necessary requirements is quite difficult.\n\nBut I also tried running the following dummy script:\n\n```\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\nx = tf.constant(1, shape=[100, 100, 100])\ny = tf.constant(2, shape=[100, 100, 100])\nz = x + y\n\nprint(z.eval())\n```\n\nCuriously, running the above script gives me different outputs depending on the amount of RAM allocated, and none of them is a Python `MemoryError`.\n- With 8GB or less:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:05:00.0\nTotal memory: 7.92GiB\nFree memory: 7.79GiB\nSegmentation fault (core dumped)\n```\n\ngdb says it was terminated with SIGSEGV. Stack:\n\n```\n#0  0x00002abe9c8880bc in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00002abe9c623258 in tensorflow::GPUMachineManager() () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00002abe9c6214d0 in tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds(std::vector<int, std::allocator<int> >*) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00002abe9c621d50 in tensorflow::BaseGPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00002abe9c7fa0d6 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00002abe9c5e5b71 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00002abe9c81c6d7 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00002abe9c7eb0e1 in TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00002abe9ba4ec3b in _wrap_TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00002abe9219fe15 in call_function (oparg=<optimized out>, pp_stack=0x7ffdfdfbd588) at Python/ceval.c:4350\n#10 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#11 0x00002abe921a0a2e in PyEval_EvalCodeEx (co=0x2abeee194cb0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=4, kws=0x2abf45186d48, kwcount=0, \n    defs=0x2abeee0932e8, defcount=3, closure=0x0) at Python/ceval.c:3582\n#12 0x00002abe9219fa55 in fast_function (nk=<optimized out>, na=4, n=<optimized out>, pp_stack=0x7ffdfdfbd7a8, func=0x2abeee08ccf8) at Python/ceval.c:4446\n#13 call_function (oparg=<optimized out>, pp_stack=0x7ffdfdfbd7a8) at Python/ceval.c:4371\n#14 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#15 0x00002abe921a0a2e in PyEval_EvalCodeEx (co=0x2abeee0953b0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=1, kws=0x0, kwcount=0, defs=0x2abeee0934c8, \n    defcount=3, closure=0x0) at Python/ceval.c:3582\n#16 0x00002abe9211c3a1 in function_call (func=0x2abeee098aa0, arg=0x2abe91f6ca50, kw=0x0) at Objects/funcobject.c:526\n#17 0x00002abe920ecd23 in PyObject_Call (func=0x2abeee098aa0, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546\n#18 0x00002abe920ff4bf in instancemethod_call (func=0x2abeee098aa0, arg=0x2abe91f6ca50, kw=0x0) at Objects/classobject.c:2602\n#19 0x00002abe920ecd23 in PyObject_Call (func=0x2abef7f5ecd0, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546\n#20 0x00002abe92159320 in slot_tp_init (self=0x2abf4517e2d0, args=0x2abe91eb2050, kwds=0x0) at Objects/typeobject.c:5715\n#21 0x00002abe9214fdd8 in type_call (type=<optimized out>, args=0x2abe91eb2050, kwds=0x0) at Objects/typeobject.c:745\n#22 0x00002abe920ecd23 in PyObject_Call (func=0x30dac40, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546\n#23 0x00002abe9219df54 in do_call (nk=<optimized out>, na=<optimized out>, pp_stack=0x7ffdfdfbddc8, func=0x30dac40) at Python/ceval.c:4568\n#24 call_function (oparg=<optimized out>, pp_stack=0x7ffdfdfbddc8) at Python/ceval.c:4373\n#25 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n(continues to vanilla python calls)\n```\n- With 10GB:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:05:00.0\nTotal memory: 8.92GiB\nFree memory: 8.10GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:06:00.0\nTotal memory: 8.92GiB\nFree memory: 7.50GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:09:00.0\nTotal memory: 8.92GiB\nFree memory: 8.12GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 3 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:0a:00.0\nTotal memory: 8.92GiB\nFree memory: 8.40GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:0a:00.0)\nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\nAborted (core dumped)\n```\n\ngdb says the program was terminated with SIGABRT and the following stack: \n\n```\n#0  0x00002b80422765f7 in raise () from /lib64/libc.so.6\n#1  0x00002b8042277ce8 in abort () from /lib64/libc.so.6\n#2  0x00002b8054de59d5 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6\n#3  0x00002b8054de3946 in ?? () from /lib64/libstdc++.so.6\n#4  0x00002b8054de3973 in std::terminate() () from /lib64/libstdc++.so.6\n#5  0x00002b8054de3b93 in __cxa_throw () from /lib64/libstdc++.so.6\n#6  0x00002b8054e38e30 in std::__throw_system_error(int) () from /lib64/libstdc++.so.6\n#7  0x00002b8054e3a559 in std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>) () from /lib64/libstdc++.so.6\n#8  0x00002b804be074b0 in tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::string const&, std::function<void ()>) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00002b804bdf1338 in tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00002b804bdf1f2f in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00002b804bdf205a in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, std::string const&, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00002b804ba71f98 in tensorflow::(anonymous namespace)::NewThreadPool(tensorflow::SessionOptions const&) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00002b804ba77abc in tensorflow::DirectSession::DirectSession(tensorflow::SessionOptions const&, tensorflow::DeviceMgr const*) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00002b804ba77bb5 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x00002b804bcae6d7 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#16 0x00002b804bc7d0e1 in TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#17 0x00002b804aee0c3b in _wrap_TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#18 0x00002b8041631e15 in call_function (oparg=<optimized out>, pp_stack=0x7fffb2412cd8) at Python/ceval.c:4350\n#19 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#20 0x00002b8041632a2e in PyEval_EvalCodeEx (co=0x2b809d626cb0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=4, kws=0x2b810c4c0d48, kwcount=0, \n    defs=0x2b809d5252e8, defcount=3, closure=0x0) at Python/ceval.c:3582\n#21 0x00002b8041631a55 in fast_function (nk=<optimized out>, na=4, n=<optimized out>, pp_stack=0x7fffb2412ef8, func=0x2b809d51ecf8) at Python/ceval.c:4446\n#22 call_function (oparg=<optimized out>, pp_stack=0x7fffb2412ef8) at Python/ceval.c:4371\n#23 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#24 0x00002b8041632a2e in PyEval_EvalCodeEx (co=0x2b809d5273b0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=1, kws=0x0, kwcount=0, defs=0x2b809d5254c8, \n    defcount=3, closure=0x0) at Python/ceval.c:3582\n(continues to vanilla python calls)\n```\n- With 12GB it works. \n\nNow, I have no idea how much RAM is required to have tensorflow running in GPU's, but 12GB seems definitely a huge amount for such a basic script. \n", "Could you set the CUDA_VISIBLE_DEVICES environmental variable to limit the GPU that is visible to TensorFlow? \n\nTensorFlow by default grabs all the GPUs visible to it, and grabs all the memory for suballocation. The user can configure how much memory it should ask. But 8G is definitely enough. The segfault is a bit troubling. If CUDA_VISIBLE_DEVICES doesn't fix any of this problem, we can ask the stream-executor team to investigate further.\n", "Ok, I ran `CUDA_VISIBLE_DEVICES=3 python test.py` (I checked with `nvidia-smi` that GPU 3 was not running anything), with 8GB of RAM allocated. Segfault again:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:0a:00.0\nTotal memory: 7.92GiB\nFree memory: 7.79GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:0a:00.0)\nSegmentation fault (core dumped)\n```\n\nEDIT: I tried running it again allocating different amounts of RAM, and sometimes it crashed even with 16GB.\n", "If you build tensorflow with `-c dbg --config=cuda` and run it under GDB, it will show which line in `StreamExecutor::DeviceMemoryUsage` causes the SEGSERV\n", "@yaroslavvb Well, it seems I cannot build tensorflow in this server, since it doesn't have internet access (and the other machines I can use which have internet don't have GPUs).\n", "@erickrf You don't need to have GPU to build for GPU support, `--config=cuda` should work regardless\n", "@yaroslavvb Ok, so I finally compiled tensorflow in another machine. How can I install it in the server without internet access? I tried copying the whole repository with the compiled files, but it still searches pypi for some packages when I run `python setup.py develop`.\n", "Hi @erickrf , do you solve your problem? I have the same issue as you had. \r\nWhen I run sess = tf.Session(), there is Segmentation fault (core dumped). And when I try to only allocate very small GPU memory, it is ok.", "@stephenjia I found out that in the server I was working with, I had to allocate 512 GB of RAM for GPU computation. It definitely sounds like an overkill but that solved it.", "@erickrf  Thank you for your reply. But I remember you said without other operations, you just run \r\n\" \r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\n\"\r\nIt would give you an error of \"\r\nTotal memory: 11.25GiB\r\nFree memory: 11.16GiB\r\nSegmentation fault (core dumped)\"\r\nI have the same problem and am a little confused about this.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing as the issue seems to have been solved. @erickrf please feel free to reopen the issue still exists.", "I am facing the same issue.\r\nTried both tensorflow-gpu and tensorflow(cpu only) and got MemoryError.\r\nI don't know how to fix this. \r\nAlso, erickrf commented that giving 512GB of RAM to GPU solved issue, how to do that on server?\r\n ", "I had the same problem, but it was due to the fact that i had more than one program of python opened at a time. I was using Jupiter  notebook in my case, going to the running tab and closing the running programs that i don't need helped me solve this issue.\r\n\r\n(https://user-images.githubusercontent.com/34263377/54976592-327e9000-4fde-11e9-9b89-d5cec2b4d799.png)\r\n", "Try installing without caching: pip install --no-cache-dir tensorflow"]}, {"number": 2415, "title": "Feature Request: atrous convolution with stride > 1", "body": "I'd like access to strides in atrous_conv2d for learning multi-scale raw audio filters. Using a stride of 1 is computationally infeasible, and as the goal is classification resolution does not need to be preserved.\n", "comments": ["@gpapan: Assigning you based on your contribution of the original op.\n", "@chris-scott, it is a bit tricky to write an efficient implementation for general strides and rate based on the current approach. Note however that if rate is a multiple of stride, i.e., `rate = k * stride` for some positive k, then:\n\n```\natrous_conv(input, rate=rate, stride=stride)\n```\n\nis identical to\n\n```\natrous_conv(subsample(input, stride), rate=k, stride=1)\n```\n\nCan you adopt something like that in your project?\n", "@gpapan \nThank you - I didn't appreciate adding strides was non-trivial. I can take a downsampling approach as you suggest.\n", "Hi @gpapan ,\r\n\r\nDo you have plan implementing atrous conv with general strides?\r\n\r\nThank you"]}, {"number": 2414, "title": "how to run lstm on multi gpus?", "body": "I run a example of lstm on multi gpus using codes in `models/image/cifar10`, but it turns out that Attempting to use uninitialized value lstm/LSTMCell/W_0. I have read the issues 1390, so I have a question if the implementation of lstmcell is ok. when I want to store variables on cpu, and run ops on gpu, the implementation of lstmcell do initialization of `w` in blackbox, then variables and ops must be in the same place, cpu or gpu, it's not a good idea, right?\n", "comments": ["I'm closing this because it's a question that would be more appropriately handled on Stack Overflow:\n\n> GitHub issues are for bugs / installation problems / feature requests.  \n> For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\n> To make bugs and feature requests more easy to find and organize, we close issues that are deemed\n> out of scope for GitHub Issues and point people to StackOverflow.\n"]}]