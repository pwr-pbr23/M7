[{"number": 30606, "title": "Additional command line arguments needed for tflite conversion of a very simple model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Docker image tensorflow/tensorflow:latest-gpu\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.7.15+\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce RTX 2080 Ti\r\n\r\nI am trying to deploy a simple, custom fully-quantized model with Coral Edge TPU support. When trying to convert the frozen graph into a tflite model, I run into errors, unless I provide more command line arguments. Considering how simple the model is (e.g. using very basic layers), I did not expect to run into problems.\r\n\r\n**Code to reproduce the issue**\r\n\r\nI create and train a model that fits the function f(x) = 2 x - 1 with the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nprint(tf.__version__)\r\n\r\ntrain_input = np.array([ -1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\r\ntrain_truth = np.array([ -3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\r\n\r\ndef build_keras_model():\r\n\treturn keras.models.Sequential([\r\n\t\tkeras.layers.Dense(units=1, input_shape=[1]),\r\n\t])\r\n\r\n### train the model\r\ntrain_graph = tf.Graph()\r\ntrain_sess = tf.Session(graph=train_graph)\r\n\r\nkeras.backend.set_session(train_sess)\r\n\r\nwith train_graph.as_default():\r\n\tkeras.backend.set_learning_phase(1)\r\n\ttrain_model = build_keras_model()\r\n\r\n\ttf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)\r\n\ttrain_sess.run(tf.global_variables_initializer())\r\n\r\n\ttrain_model.compile(\r\n\t\toptimizer='sgd',\r\n\t\tloss='mean_squared_error'\r\n\t)\r\n\ttrain_model.fit(train_input, train_truth, epochs=250)\r\n\r\n\tsaver = tf.train.Saver()\r\n\tsaver.save(train_sess, 'linear.ckpt')\r\n```\r\n\r\nAfterwards, I create an eval graph and freeze the model with the help of this script:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\ndef build_keras_model():\r\n\treturn keras.models.Sequential([\r\n\t\tkeras.layers.Dense(units=1, input_shape=[1]),\r\n\t])\r\n\r\n# eval\r\neval_graph = tf.Graph()\r\neval_sess = tf.Session(graph=eval_graph)\r\n\r\nkeras.backend.set_session(eval_sess)\r\n\r\nwith eval_graph.as_default():\r\n\tkeras.backend.set_learning_phase(0)\r\n\teval_model = build_keras_model()\r\n\r\n\ttf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\r\n\r\n\teval_graph_def = eval_graph.as_graph_def()\r\n\tsaver = tf.train.Saver()\r\n\tsaver.restore(eval_sess, 'linear.ckpt')\r\n\r\n\tfrozen_graph_def = tf.graph_util.convert_variables_to_constants(\r\n\t\teval_sess,\r\n\t\teval_graph_def,\r\n\t\t[eval_model.output.op.name]\r\n\t)\r\n\r\n\twith open('frozen_model.pb', 'wb') as f:\r\n\t\tf.write(frozen_graph_def.SerializeToString())\r\n```\r\n\r\nWhen I try to convert the frozen graph to a tflite model, using the following command\r\n\r\n```\r\ntflite_convert \\\r\n--output_file=model.tflite \\\r\n--graph_def_file=frozen_model.pb \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--input_arrays=dense_input \\\r\n--output_arrays=dense/BiasAdd \\\r\n--mean_values=0 \\\r\n--std_dev_values=255\r\n```\r\nI run into an error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 503, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 499, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 193, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py\", line 898, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 404, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-07-11 15:54:26.995204: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 7 operators, 12 arrays (0 quantized)\r\n2019-07-11 15:54:26.995314: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 7 operators, 12 arrays (0 quantized)\r\n2019-07-11 15:54:26.995465: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (1 quantized)\r\n2019-07-11 15:54:26.995497: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 2 operators, 5 arrays (1 quantized)\r\n2019-07-11 15:54:26.995512: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 2 operators, 5 arrays (1 quantized)\r\n2019-07-11 15:54:26.995528: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 2 operators, 5 arrays (1 quantized)\r\n2019-07-11 15:54:26.995558: F tensorflow/lite/toco/graph_transformations/quantize.cc:149] Array dense/BiasAdd does not have MinMax information, and is not a constant array. Cannot proceed with quantization.\r\nAborted (core dumped)\r\n```\r\n\r\nThis error vanishes, when I provide two additional command line arguments with any value:\r\n`--default_ranges_min=-128 --default_ranges_max=127`\r\n\r\nFirst and foremost, shouldn't a simple model like this be convertible without these args? Also, I am still only guessing on how to determine the default ranges as well as the values for mean_values and std_dev_values. This should be added to the documentation for clarity.", "comments": ["@DocDriven ,\r\nThe arguments, `--default_ranges_min and --default_ranges_max` are required as per the documentation provided in this [Tensorflow Website](https://www.tensorflow.org/lite/convert/cmdline_examples#use_dummy-quantization_to_try_out_quantized_inference_on_a_float_graph_)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@rmothukuru \r\nI have read the docs, but they do not answer my questions. Is there any detailed documentation what these arguments are doing exactly? Maybe you could expand the description of the linked website.\r\n\r\nAlso, the important question for me is, why is the conversion without these arguments possible from the h5 file format, but I do need them when passing a frozen graph def to the converter?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30606\">No</a>\n"]}, {"number": 30605, "title": "Share tf.distribute.strategy scopes across multiple sets of GPUs for multithreaded data gather and training", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14/2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn some RL models, an agent will gather data in one thread/process while an update loop in a second thread calculates the gradients/updates the model. Often for model-based RL, the time spent performing forward passes through the model can be large, so this has many advantages if it can be a non-blocking procedure in parallel with the update calls. It would be useful to be able to use the new tf.distribute strategies to share weights across multiple *sets* of GPUs (for instance, one GPU performs continual forward passes through the model while the remaining 7 on the node perform the updates on the model) across threads/processes. In the current parameter server model, I can simply pass the tf.Server configuration and build the graph in multiple workers, sharing the same set of variables on the parameter server.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["What you're describing sounds like one way to do model parallelism and distribution strategy doesn't support that yet. So for now, you would have to manually copy the variables from one thread/process to another to achieve the above.\r\n\r\nHere is the response from @tomhennigan who works with folks who do RL with TF:\r\n\r\nFor single machine multi-threaded training you can simply use eager mode to read parameter values from the learner in your actor threads. I suspect at some point you might want to scale your learner beyond one GPU, using MirroredStrategy to do that should hardly change the pseudocode below (add the strategy scope around model construction and using strategy.run to parallel apply the training step).\r\n\r\n```\r\nlearner = MyModel()\r\n\r\ndef learner():\r\n  while True:\r\n    train(learner, ...)\r\n\r\ndef actor():\r\n  actor = MyModel()\r\n\r\n  while True:\r\n    if should_copy_from_learner:\r\n      for l, a in zip(learner.variables, actor.variables):\r\n        a.assign(l)\r\n\r\n    act(actor, ...)\r\n\r\nrun_in_thread(learner)\r\nrun_in_thread(actor)\r\n```", "Thanks for the idea! I'm currently building my model as a subclass of tf.Module containing keras layers across my actor and learner thread. Is this paradigm incompatible with autograph? I'd like to use autograph for two reasons: one, my model uses graph convolutions which seem to perform significantly more slowly in eager execution. Two, I've had difficulty getting the distributed strategy to work without decorating my train loop in tf.function... I've tried the nightly, and I get an error about invalid datatypes (int64 vs int32, even though none of my inputs or casts are an int64) in a SplitV node, but the trace isn't very informative. Everything works fine with a tf.function decorator.", "\"I've had difficulty getting the distributed strategy to work without decorating my train loop in tf.function\" - can you share a code snippet to help us repro this issue? You should be able to use tf.distribute strategy without tf.function without errors. ", "> \"I've had difficulty getting the distributed strategy to work without decorating my train loop in tf.function\" - can you share a code snippet to help us repro this issue? You should be able to use tf.distribute strategy without tf.function without errors.\r\n\r\nEdit: I was storing a value in the graph used for loss normalization as a int64. This was triggering an aggregation issue with NCCL all reduce. Looking at the stack trace, is there a better way to expose specific ops that are causing the issue for future reference? (See below:). The same code containing the tf.Variable with an integer value was not causing an issue in autograph.\r\n\r\n```\r\n2019-07-24 11:40:07.065064: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\nTraceback (most recent call last):\r\n  File \"../physics/scripts/train_interaction.py\", line 26, in <module>\r\n    main()\r\n  File \"../physics/scripts/train_interaction.py\", line 19, in main\r\n    train_worker.distributed_train()\r\n  File \"/home/mjlbach/Repositories/tf2.0_box_physics/physics/physics/scripts/worker.py\", line 246, in distributed_train\r\n    train_res = distributed_train_step(\r\n  File \"/home/mjlbach/Repositories/tf2.0_box_physics/physics/physics/scripts/train_step.py\", line 26, in distributed_train_step\r\n    train_step, args=(inputs, world_model, world_optimizer)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 708, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1710, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 708, in _call_for_each_replica\r\n    fn, args, kwargs)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 195, in _call_for_each_replica\r\n    coord.join(threads)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 189, in _call_for_each_replica\r\n    **merge_kwargs)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/values.py\", line 825, in merge_fn\r\n    v = _apply_aggregation(strategy, value, self._aggregation, self)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/values.py\", line 747, in _apply_aggregation\r\n    return strategy.extended.reduce_to(reduce_op, value, destinations)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1388, in reduce_to\r\n    return self._reduce_to(reduce_op, value, destinations)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 750, in _reduce_to\r\n    reduce_op, value, destinations=destinations)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 272, in reduce\r\n    destinations)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 695, in reduce_implementation\r\n    return self._batch_all_reduce(reduce_op, [per_replica_value])[0]\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 715, in _batch_all_reduce\r\n    dense_results = self._do_batch_all_reduce(reduce_op, dense_values)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 740, in _do_batch_all_reduce\r\n    self._agg_small_grads_max_group)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 641, in _pack_tensors\r\n    device_grad_packs = tensor_packer.pack(device_grads)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/distribute/cross_device_ops.py\", line 547, in pack\r\n    grad_packs = array_ops.split(concat_grads, split_sizes)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1564, in split\r\n    value=value, size_splits=size_splits, axis=axis, num_split=num, name=name)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 9566, in split_v\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'SplitV' OpKernel for GPU devices compatible with node {{node SplitV}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_INT64, Tlen=DT_INT32, num_split=1\r\n\t.  Registered:  device='XLA_CPU'; Tlen in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_GPU'; Tlen in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_CPU_JIT'; Tlen in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_GPU_JIT'; Tlen in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_VARIANT]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_VARIANT]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_RESOURCE]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_RESOURCE]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_STRING]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_STRING]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_BOOL]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_BOOL]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_COMPLEX128]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_COMPLEX128]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_COMPLEX64]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_COMPLEX64]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_DOUBLE]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_DOUBLE]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_FLOAT]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_FLOAT]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_BFLOAT16]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_BFLOAT16]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_HALF]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_HALF]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_INT8]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_INT8]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_UINT8]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_UINT8]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_INT16]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_INT16]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_UINT16]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_UINT16]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_INT32]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_INT32]\r\n  device='CPU'; Tlen in [DT_INT64]; T in [DT_INT64]\r\n  device='CPU'; Tlen in [DT_INT32]; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]; Tlen in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]; Tlen in [DT_INT32]\r\n  device='GPU'; Tlen in [DT_INT64]; T in [DT_BFLOAT16]\r\n  device='GPU'; Tlen in [DT_INT32]; T in [DT_BFLOAT16]\r\n  device='GPU'; Tlen in [DT_INT64]; T in [DT_COMPLEX128]\r\n  device='GPU'; Tlen in [DT_INT32]; T in [DT_COMPLEX128]\r\n  device='GPU'; Tlen in [DT_INT64]; T in [DT_COMPLEX64]\r\n  device='GPU'; Tlen in [DT_INT32]; T in [DT_COMPLEX64]\r\n  device='GPU'; Tlen in [DT_INT64]; T in [DT_DOUBLE]\r\n  device='GPU'; Tlen in [DT_INT32]; T in [DT_DOUBLE]\r\n  device='GPU'; Tlen in [DT_INT64]; T in [DT_FLOAT]\r\n  device='GPU'; Tlen in [DT_INT32]; T in [DT_FLOAT]\r\n  device='GPU'; Tlen in [DT_INT64]; T in [DT_HALF]\r\n  device='GPU'; Tlen in [DT_INT32]; T in [DT_HALF]\r\n [Op:SplitV] name: split\r\n```\r\n\r\nI'm also showing a huge performance regression and increase in memory using in the model, even with the @tf.function decorator... I'm a little at a loss about the best way to profile what is causing the slowdown."]}, {"number": 30604, "title": "Calling .map on a tf.data.Dataset causes unrelated imports to fail", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-6008-gd883916ee4 2.0.0-dev20190711 \r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nWhen `.map` is called on a `tf.data.Dataset`, some available imports are getting lost. I've stumbled upon this being unable to import `tf.summary.create_file_writer` after mapping a function onto a dataset, even though I can import it in ipython. Several other methods of `tf.summary` are lost as well. I didn't check whether any other tensorflow models apart form `summary` loose imports.\r\n\r\n**Describe the expected behavior**\r\n\r\nCalling `.map` on a `tf.data.Dataset` should leave available imports intact.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.summary.create_file_writer)\r\n\r\ndef map_fun(x):\r\n    return x\r\n\r\nds = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])\r\n\r\nds = ds.map(map_fun)\r\n\r\nprint(tf.summary.create_file_writer)\r\n```\r\n\r\n**Other info / logs**\r\nOutput of the above code snippet:\r\n```\r\n<function create_file_writer_v2 at 0x13180d9d8>\r\n2019-07-11 17:12:16.193865: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA                                                  \r\n2019-07-11 17:12:16.208592: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc08db4f5b0 executing computations on platform Host. Devices:                                                                     \r\n2019-07-11 17:12:16.208619: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>                                                                                            \r\nTraceback (most recent call last):\r\n  File \"bug_test.py\", line 12, in <module>\r\n    print(tf.summary.create_file_writer)\r\nAttributeError: module 'tensorflow_core.summary' has no attribute 'create_file_writer'\r\n```", "comments": ["@BeWe11 I executed the given code on Colab with Tensorflow 2.0.0-dev20190711 but i did not get any error. The below is the result\r\n\r\n```\r\n> 2.0.0-dev20190711\r\n> <function create_file_writer_v2 at 0x7f037d67dea0>\r\n> <function create_file_writer_v2 at 0x7f037d67dea0>\r\n```\r\nCan you please try once and let us know. Thanks!", "After reinstallation of tensorflow, the problem has disappeared. Thanks!"]}, {"number": 30603, "title": "release TF Java 1.14.0 (except tensorflow-android)", "body": "There are some changes to how tensorflow-android is released, I'm still\r\nworking out what to do with that artifact.", "comments": []}, {"number": 30602, "title": "FFT ops consume a lot of GPU RAM", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 19.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')\r\n- **Python version**: 2.7.16\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 10.0 / 7.6.1.34\r\n- **GPU model and memory**: RTX 2080ti 11GB\r\n- **Exact command to reproduce**:\r\n`import tensorflow as tf`\r\n`tf.enable_eager_execution()`\r\n`d = tf.cast(tf.random_uniform((512,), dtype=tf.float32), dtype=tf.complex64)`\r\n`D = tf.fft(d)`\r\n`d = tf.ifft(D)`\r\n\r\n### Describe the problem\r\nThe output of the above code is an error produced by the line `d = tf.ifft(D)`. The error is: `tensorflow.python.framework.errors_impl.InternalError: BlasScal failed : in.shape=[512] [Op:IFFT]`\r\n\r\nThe following code works as expected:\r\n`import tensorflow as tf`\r\n`tf.enable_eager_execution()`\r\n`d = tf.cast(tf.random_uniform((512,), dtype=tf.float32), dtype=tf.complex64)`\r\n`D = tf.fft(d)`\r\n`with tf.device(\"/cpu:0\"):`\r\n`    d = tf.ifft(D)`", "comments": ["@tportenier \r\n\r\nI tried reproducing the issue and i am not getting any error. Please, find the below screenshot for your reference.Can you cross check and let us know if the issue still persists.\r\n\r\n![IFFT](https://user-images.githubusercontent.com/51902062/61119333-34e6f600-a4b8-11e9-98d2-bc82983686a1.png)\r\n", "Indeed I did not provide a sufficient minimum example, please excuse. The attached script 'error.txt' produces the error while 'no_error.txt' and 'no_error2.txt' work fine.\r\n\r\n[error.txt](https://github.com/tensorflow/tensorflow/files/3386309/error.txt)\r\n[no_error.txt](https://github.com/tensorflow/tensorflow/files/3386311/no_error.txt)\r\n[no_error2.txt](https://github.com/tensorflow/tensorflow/files/3386315/no_error2.txt)\r\n\r\n", "@tportenier still i am not able see any error. Please, find the below screenshot.Please, let me know where i am missing. Thanks!\r\n![testnew](https://user-images.githubusercontent.com/51902062/61203099-10cd2400-a707-11e9-81fc-d45fb73d43be.png)\r\n", "I do not see any discrepancy between your screenshot and the code that causes the error on my side, unfortunately. Nevertheless, it causes the error here on my side.", "After more investigation, the error seems to be caused by an out-of-memory problem, related to the issue discussed here #9489\r\n\r\nWhile the following triggers the error:\r\n\r\nimport tensorflow as tf\r\n\r\n#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\r\n\r\nwith tf.device('/gpu:0'):\r\n    with tf.Session() as sess:#config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\r\n        k = tf.ones((71, 71, 1, 1), tf.float32)\r\n\r\n        data = tf.ones((1, 512, 256, 1))\r\n\r\n        kernel = tf.Variable(initial_value=k, trainable=False, dtype=tf.float32)\r\n        data_convolved = tf.nn.conv2d(data, filter=kernel, strides=(1, 1, 1, 1), padding='SAME')\r\n\r\n        d = tf.cast(tf.random_uniform((512,), dtype=tf.float32), dtype=tf.complex64)\r\n        D = tf.fft(d)\r\n        d = tf.ifft(D)\r\n\r\n        sess.run(tf.initialize_all_variables())\r\n\r\n        d_out, data_convolved_out = sess.run([d, data_convolved])\r\n\r\n\r\nthis one runs without error:\r\n\r\nimport tensorflow as tf\r\n\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\r\n\r\nwith tf.device('/gpu:0'):\r\n    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\r\n        k = tf.ones((71, 71, 1, 1), tf.float32)\r\n\r\n        data = tf.ones((1, 512, 256, 1))\r\n\r\n        kernel = tf.Variable(initial_value=k, trainable=False, dtype=tf.float32)\r\n        data_convolved = tf.nn.conv2d(data, filter=kernel, strides=(1, 1, 1, 1), padding='SAME')\r\n\r\n        d = tf.cast(tf.random_uniform((512,), dtype=tf.float32), dtype=tf.complex64)\r\n        D = tf.fft(d)\r\n        d = tf.ifft(D)\r\n\r\n        sess.run(tf.initialize_all_variables())\r\n\r\n        d_out, data_convolved_out = sess.run([d, data_convolved])\r\n", "@tportenier \r\n\r\nI am seeing only warning message but no error.Please, see below screenshot\r\n\r\n![test2](https://user-images.githubusercontent.com/51902062/61277196-505f4300-a7cf-11e9-95f1-bade92c4bedd.png)\r\n", "One thing I see is that you seem to use python3 whereas I use python2.7. I attach the full output when running the scrpit 'error3.txt'.\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/3396068/log.txt)\r\n[error3.txt](https://github.com/tensorflow/tensorflow/files/3396073/error3.txt)\r\n\r\n\r\n", "@tportenier Even i used python2.7. Thanks!", "@tportenier Thanks for sharing your investigation. Limiting GPU memory is recommended approach to overcome GPU OOM.\r\nhttps://www.tensorflow.org/beta/guide/using_gpu#limiting_gpu_memory_growth", "@ymodak I still consider this a bug, since eating up 11GB of memory for doing ifft1D on a signal of that size seems unreasonable...", "@tportenier the FFT ops allocate a workspace which is by default 4GB:\r\nhttps://github.com/tensorflow/tensorflow/blob/63b93ebc184daf0fc30b914d7e504d3cc14914b0/tensorflow/core/kernels/fft_ops.cc#L555\r\n\r\nCould you please try setting the environment variable `TF_CUFFT_WORKSPACE_LIMIT_IN_MB` to something smaller (e.g. `256`) to see if this improves your issue?\r\n\r\nI totally agree that a 512-point FFT should not consume 11GB of RAM. Is your GPU totally unused other than TensorFlow when you run your repro example? \r\n\r\nI tried your repro in a Google Colab with a K80 GPU and using `nvgpu.gpu_info` suggested the memory usage was 167 MB immediately after running the ops, but this does not tell us what the peak usage was.\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nwith tf.device('/gpu:0'):\r\n  d = tf.cast(tf.random_uniform((512,), dtype=tf.float32), dtype=tf.complex64)\r\n  D = tf.fft(d)\r\n  d = tf.ifft(D)\r\nnvgpu.gpu_info()\r\n# [{'index': '0',\r\n#  'mem_total': 11441,\r\n#  'mem_used': 167,\r\n#  'mem_used_percent': 1.4596626169041167,\r\n#  'type': 'Tesla K80',\r\n#  'uuid': 'GPU-9bb2a5ef-ef52-f010-c3f5-9c482cb8db57'}]\r\n```\r\n\r\n", "Tentatively renaming the issue to reflect the discussion. I believe the issue is related to the cuFFT workspace default of 4GB.", "@tportenier gentle ping -- would love to know if the above helps! :)", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "@rryan I am having the above problem for the fft2d of a `[35,640,368]` tensor. In don't have a reproducible example yet, but here is the error I got (even after trying the proposed fixes):\r\n\r\n```\r\nInternalError:  BlasScal failed : in.shape=[35,640,368]\r\n\t [[node model/ifft_masked_1/IFFT2D (defined at /volatile/home/Zaccharie/workspace/fastmri-reproducible-benchmark/fastmri_recon/helpers/nn_mri.py:70) ]] [Op:__inference_function_4679]\r\n\r\nFunction call stack:\r\nfunction\r\n```", "Unfortunately, even for a batch size of 1, I get this error.\r\n\r\nMy minimal failing example is basically:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.signal.fft_ops import ifft2d\r\nifft2d(tf.zeros((1, 640, 368), dtype='complex64'))\r\n```\r\nGiving the error:\r\n```\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-3-7b8e3073d0a0> in <module>\r\n----> 1 ifft2d(tf.zeros((1, 640, 368), dtype='complex64'))\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_spectral_ops.py in ifft2d(input, name)\r\n    679         raise\r\n    680     except _core._NotOkStatusException as e:\r\n--> 681       _ops.raise_from_not_ok_status(e, name)\r\n    682   # Add nodes to the TensorFlow graph.\r\n    683   try:\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6604   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6605   # pylint: disable=protected-access\r\n-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6607   # pylint: enable=protected-access\r\n   6608 \r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInternalError: BlasScal failed : in.shape=[1,640,368] [Op:IFFT2D]\r\n\r\n```\r\n\r\nSo it's not very informative. \r\n\r\nHowever, the warnings are:\r\n```\r\n2020-01-22 11:56:15.597585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-22 11:56:15.597890: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-01-22 11:56:15.597903: W tensorflow/stream_executor/stream.cc:2041] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n```\r\n\r\nSo I guess it's actually a problem with blas not being loaded correctly.", "Ok it was indeed a blas problem, following [this advice](https://github.com/tensorflow/tensorflow/issues/9489#issuecomment-562394257), I got rid of the error.\r\n\r\nInterestingly, the convolutions were working, only the fft was impacted."]}, {"number": 30601, "title": "Tensorflow 2.0/1.14 GPU build fails in debug mode", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda? \r\n- Bazel version (if compiling from source):0.25.2\r\n- GCC/Compiler version (if compiling from source):5.4.0\r\n- CUDA/cuDNN version:10.0/7\r\n- GPU model and memory: p40 \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\ntensorflow 1.14 gpu build doesn't succeed in debug mode\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build --config=opt --config=cuda --compilation_mode=dbg //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n1 error detected in the compilation of \"/tmp/tmpxft_000029e6_00000000-6_spacetodepth_op_gpu.cu.cpp1.ii\".\r\nERROR: /home/core/tensorflow/tensorflow/core/kernels/BUILD:4483:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/spacetodepth_op_gpu.cu.pic.o' was not created\r\nERROR: /home/core/tensorflow/tensorflow/core/kernels/BUILD:4483:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 394.154s, Critical Path: 51.10s\r\nINFO: 4640 processes: 4640 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["Pasting another log:\r\n\r\nINFO: From Compiling tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.cc:\r\nexternal/com_google_absl/absl/strings/string_view.h(495): error: constexpr function return is non-constant", "Can you also test with 1.13 to see if this is a regression?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "This solved the problem:\r\n\r\nbazel build --config cuda --strip=never -c dbg --copt=\"-DNDEBUG\" //tensorflow/tools/pip_package:build_pip_package\r\n", "Glad it resolved. "]}, {"number": 30600, "title": "TFTRT: Can I specify the computation precision for a specific layer?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n1.14\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSupport blacklist of ops that cannot use lower precision when converting a graph into tftrt with precision mode of fp16 or int8. Or provide a way to specify a subgraph that cannot be optimized by tftrt.\r\n\r\n**Who will benefit with this feature?**\r\nAll users that need a mixed precision of their model and want to optimize the model by tftrt.\r\n\r\n**Any Other info.**\r\n\r\nIn my product I'm developing a model that needs full precision for a few layers (which include elementwise ops and conv ops). The layers need full precision because they are doing some math computation that cannot sacrifice precision. I also want to use tftrt to optimize the graph because most of other layers can use precision as low as fp16 or int8. However, i cannot find a way in tftrt to specify a blacklist of ops that cannot use lower precision. And there is no way to specify a subgraph that cannot be optimized by tftrt. Tensorrt supports specifying a precision for a layer but my model uses many ops that don't exist in tensorrt so I cannot convert it into tensorrt.\r\n\r\nThank you.\r\n\r\n@trevor-m ", "comments": ["the only way is thru tf.cast?", "@motionlife  I think tftrt will automatically convert all conv ops to use precision mode specified in the graph conversion api, and there is no way to disable tftrt for a subgraph. How can tf.cast solve this?", "I believe now it'll fallback to fp16 if int8 is not available, see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L1581-L1588). Does this work for you @x10000year ?", "@aaroey Yes, that's definitely useful! Then I can force some layer to use fp16. It will be better if there's a way to force some layer to use fp32.", "@pooyadavoodi would you please take a look? Thanks.", "This feature is currently not supported by TF-TRT.\r\nSince TensorRT supports this already, we can consider adding this feature.\r\nIt would require an API such that users can provide a list of ops/subgraphs with their desired precision. \r\nWe don't currently have a plan to add this feature.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 30599, "title": "Error(i.e. fatal error C1189: #error:  STL1001: Unexpected compiler version, expected MSVC 19.20 or newer) while building Tensorflow with MSVC 2019 on windows ", "body": "System information\r\nOS Platform : Windows 10 Pro\r\nTensorFlow version: r1.3\r\nPython version: Python 3.6.8 :: Anaconda, Inc.\r\nInstalled using virtualenv: conda\r\nBazel version : 0.26.1\r\nMSVC 19\r\n\r\nconfiguration :\r\nPlease specify the location of python. [Default is C:\\Users\\zen2_microsoft\\Anaconda3\\envs\\tensorflow-1.13.1-without-mkl\\python.exe]:\r\nFound possible Python library paths:\r\nC:\\Users\\zen2_microsoft\\Anaconda3\\envs\\tensorflow-1.13.1-without-mkl\\lib\\site-packages\r\nPlease input the desired Python library path to use. Default is [C:\\Users\\zen2_microsoft\\Anaconda3\\envs\\tensorflow-1.13.1-without-mkl\\lib\\site-packages]\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: N\r\n\r\nError Description :\r\n\r\n C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.16.27023/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/spectrogram.obj /c tensorflow/lite/kernels/internal/spectrogram.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.21.27702\\include\\yvals_core.h(352): fatal error C1189: #error:  STL1001: Unexpected compiler version, expected MSVC 19.20 or newer.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1719.835s, Critical Path: 17.80s\r\nINFO: 18 processes: 18 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["I meet the problem too,do you have sloved this problem??", "@ChaoyiXie \r\nI didn't find solution . facing different issue after VS update 16.2 #31608 . Let me know in case you find something . ", "I ran into this problem when there were multiple versions of visual studio installed. When I  removed the extra visual studio installations (or the command line tools for 2017 or 2015 that you can install with vs 2019) it worked OK for me.\r\n\r\n@laszlocsomor @meteorcloudy this looks like a bug in bazel VS detection, maybe?", "@gunan , yes, this is a bug in Bazel, which is already fixed from Bazel 0.27.0\r\nSee https://github.com/bazelbuild/bazel/pull/7686#issuecomment-492796469", "See https://github.com/tensorflow/tensorflow/issues/30599#issuecomment-522457370\r\n\r\n<strike>@gunan , the command looks legit. \r\n\r\nI have VC 2019 Community (14.20.27508) installed, and my `yvals_core.h` looks like:\r\n```\r\n   337  #if _MSC_VER < 1920 // Coarse-grained, not inspecting _MSC_FULL_VER\r\n   338  #error STL1001: Unexpected compiler version, expected MSVC 19.20 or newer.\r\n   339  #endif // ^^^ old MSVC ^^^\r\n```\r\n\r\nFor that particular installation, `_MSC_VER` is `1921`.\r\n\r\nIt looks like @mayadav1 is getting a bad `_MSC_VER`.\r\n\r\n@mayadav1 : Could you please run Bazel with `--verbose_failures` and paste the envvars here too? It will print the failing command, but also all the envvars Bazel set for it.</strike>", "See https://github.com/tensorflow/tensorflow/issues/30599#issuecomment-522457370\r\n\r\n<strike>@mayadav1 : Could you please try something else as well?\r\n\r\n1. Create a new directory, for example `c:\\foo`\r\n2. Create an empty `WORKSPACE` file there (if you create it with Notepad then make sure to remove the `.txt` extension)\r\n3. Create a `BUILD` file (again, watch that there's no `.txt` extension) with this content:\r\n    ```\r\n    cc_binary(\r\n        name = \"msvc_ver\",\r\n        srcs = [\"msvc_ver.cc\"],\r\n    )\r\n    ```\r\n4. Create a `msvc_ver.cc` file with this content:\r\n    ```\r\n    #include <stdio.h>\r\n    #define _T(x) #x\r\n    #define T(x) _T(x)\r\n    int main() {\r\n      printf(\"_MSC_VER = (%s)\\n\", T(_MSC_VER));\r\n      return 0;\r\n    }\r\n    ```\r\n5. Open `cmd.exe`, then:\r\n    ```\r\n    cd c:\\foo\r\n\r\n    bazel run //:msvc_ver\r\n    ```\r\n\r\nWhat's the output?</strike>", "You can now use more recent bazel versions with TF, therefore, this issue is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30599\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30599\">No</a>\n", "> You can now use more recent bazel versions with TF, therefore, this issue is resolved.\r\n\r\nAs if we didn't have strict version limitations of Bazel to use for every single TensorFlow version. Upgrade to Bazel 0.27.0 rules out TensorFlow 1.15 (which needs <= 0.26.1) and older."]}, {"number": 30598, "title": "gfile for S3 uploads corrupt data when called concurrently", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nUbuntu 16.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n\r\nAmazon DLAMI\r\n- TensorFlow version (use command below):\r\n1.13.1\r\n- Python version:\r\n3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI have code that writes .npz files from multiple python processes. python code is single-threaded, and each process writes a separate .npz file. The files are sometimes corrupted on S3. I checked a sha256 checksum of the data in memory, and they differ from the checksum of the written data on S3.\r\n\r\n**Describe the expected behavior**\r\n\r\nthe contents should be identical.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI can create a repro if needed.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe reason seems to be that the gfile writer doesn't create a unique tempfile when it is invoked within a millisecond.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L213\r\n\r\nHere, the writer creates a tempfile. and the corresponding aws-sdk-cpp code is:\r\n\r\nhttps://github.com/aws/aws-sdk-cpp/blob/master/aws-cpp-sdk-core/source/platform/linux-shared/FileSystem.cpp#L262\r\n\r\nThe latter code only has a millisecond granularity, so if gfile is called within the same millisecond, it creates the same tempfile and they overwrite each other.\r\n\r\nAlso, the \"XXXXXX\" part in s3_file_system.c doesn't do anything, although it's not the cause of this problem.\r\n\r\nI created a separate issue in\r\n\r\nhttps://github.com/aws/aws-sdk-cpp/issues/1192\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@yasushi-saito Provide us the full minimal code snippet which replicates mentioned issue. It will indeed help us to move faster.Thanks!\r\n", "Try the attached file. You need to change test_path. With CONCURRENCY > 1, I consistency get file corruption. With CONCURRENCY=1, I don't see an error.\r\n\r\n[gfilebug.txt](https://github.com/tensorflow/tensorflow/files/3387404/gfilebug.txt)\r\n", "Any news on this issue? It seems that the aws-sdk fixed the issue on version 1.7.153\r\nhttps://github.com/aws/aws-sdk-cpp/issues/1192#issuecomment-518383782\r\n", "As now, almost a year has passed since this issue was opened. @jvishnuvardhan do you know if this issue fixed on a newer tf release?", "@tomasferraro Sorry for missing this issue. Can you please check the issue with recent versions like `TF1.15.2` and `TF2.2`. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30598\">No</a>\n"]}, {"number": 30597, "title": "Deprecated warning removed from contrib timeseries", "body": "The following deprecated warnings are removed from contrib/timeseries examples\r\n```\r\nThe name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\nThe name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\nThe name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\n```\r\n", "comments": ["@siju-samuel Can you please check Ubuntu Sanity errors? Thanks!"]}, {"number": 30596, "title": "Very bad performance using Gradient Tape", "body": "System information\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Ubuntu 18.04.2\r\nTensorFlow installed from (source or binary): binary pip\r\nTensorFlow version (use command below): 2.0.0-beta1\r\nPython version: 3.6.8\r\nCUDA/cuDNN version: 10.0/7\r\nGPU model and memory: Tesla K80\r\n\r\nI'm trying to learn Tensorflow 2.0, so I build a toy model and trained it using keres .fit \r\nmethod, everything worked well. \r\nBut when I tried to implement the training loop from scratch, the training is happening very very slowly. Keras .fit method trained the model in 1 min 41 secs while the training code I've written taking more than 8 mins to train!!!\r\n\r\nBelow is my model definition:\r\n```python\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.AveragePooling2D())\r\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\r\nmodel.add(layers.AveragePooling2D())\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(units = 120, activation = 'relu'))\r\nmodel.add(layers.Dense(units = 84, activation = 'relu'))\r\nmodel.add(layers.Dense(units = 10, activation = 'softmax'))\r\n```\r\nBelow I'm defining loss, optimizer and accuracy:\r\n```python\r\noptimizer = tf.keras.optimizers.Adam()\r\nobjective = tf.keras.losses.SparseCategoricalCrossentropy()\r\nmetric = tf.keras.metrics.SparseCategoricalAccuracy()\r\n```\r\n\r\nAnd Below is my training loop:\r\n```python\r\n%%time\r\nwith tf.device('gpu:0'):\r\n    for epoch in range(20):\r\n        cumulative_loss = 0.0\r\n        metric.reset_states()\r\n        for images, labels in dataset:\r\n            with tf.GradientTape() as tape:\r\n                predictions = model(images, training=True)\r\n                loss = objective(labels, predictions)\r\n\r\n            grads = tape.gradient(loss, model.trainable_variables)\r\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n            cumulative_loss += loss\r\n            metric.update_state(labels, predictions)\r\n\r\n        print(\"Epoch: {} Loss: {} Accuracy: {}\".format(epoch, cumulative_loss.numpy()/(batch + 1), metric.result()))\r\n```\r\nI'm runnig my notebook in Google Colab", "comments": ["Hey, this is just a wild guess, but maybe you need to wrap your train loop inside a `tf.function`?", "Looks like the code is incomplete.Can you please provide full code snippet to reproduce it on our environment.Thanks!", "Here is the full code.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nimport time\r\n\r\nprint(tf.test.is_gpu_available()) # prints True\r\nprint(tf.__version__) # prints '2.0.0-beta1'\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\nx_train.shape, x_test.shape\r\n\r\nx_train = tf.cast(x_train, tf.float32)\r\nx_train = tf.expand_dims(x_train, axis = 3)/255.0\r\ny_train = tf.cast(y_train, tf.float32)\r\n\r\nx_test = tf.cast(x_test, tf.float32)\r\nx_test = tf.expand_dims(x_test, axis = 3)/255.0\r\ny_test = tf.cast(y_test, tf.float32)\r\n\r\nprint('x_train.shape:', x_train.shape) # x_train.shape: (60000, 28, 28, 1)\r\nprint('y_train.shape:', y_train.shape) # y_train.shape: (60000,)\r\n\r\nprint('x_test.shape:', x_test.shape) # x_test.shape: (10000, 28, 28, 1)\r\nprint('y_test.shape:', y_test.shape) # y_test.shape: (10000,)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\ndataset = dataset.shuffle(60000).batch(64)\r\n\r\nprint(dataset) # <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.float32)>\r\n\r\n#checking shape of features and labels\r\nfor x, y in dataset.take(1): # take(1) takes first batch of the datase\r\n    print(x.shape) # (64, 28, 28, 1)\r\n    print(y.shape) # (64,)\r\n\r\n#counting number of batches\r\nfor batch, (features, labels) in enumerate(dataset):\r\n    pass\r\nprint('batches:', batch + 1) # batches: 938\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.AveragePooling2D())\r\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\r\nmodel.add(layers.AveragePooling2D())\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(units = 120, activation = 'relu'))\r\nmodel.add(layers.Dense(units = 84, activation = 'relu'))\r\nmodel.add(layers.Dense(units = 10, activation = 'softmax'))\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\nobjective = tf.keras.losses.SparseCategoricalCrossentropy()\r\nmetric = tf.keras.metrics.SparseCategoricalAccuracy()\r\n\r\nstart = time.time()\r\n\r\nwith tf.device('gpu:0'):\r\n    for epoch in range(20):\r\n        cumulative_loss = 0.0\r\n        metric.reset_states()\r\n        for images, labels in dataset:\r\n            with tf.GradientTape() as tape:\r\n                predictions = model(images, training=True)\r\n                loss = objective(labels, predictions)\r\n\r\n            grads = tape.gradient(loss, model.trainable_variables)\r\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n            cumulative_loss += loss\r\n            metric.update_state(labels, predictions)\r\n\r\n        print(\"Epoch: {} Loss: {} Accuracy: {}\".format(epoch, cumulative_loss.numpy()/(batch + 1), metric.result()))\r\nprint(f'Took: {(time.time() - start)/60.0:.5f}')\r\n```", "> Hey, this is just a wild guess, but maybe you need to wrap your train loop inside a `tf.function`?\r\n\r\nYes, I've tested it, and that didn't improve the performance at all!!!! And it's still nowhere near as tf.keras's .fit() method,\r\n\r\nI've compared the same code using @tf.function with other frameworks: MXNet and Pytorch, and both of them are way faster than Tensorflow, MXNet is about 8x, and Pytorch is about 7.5x.\r\n\r\nI am pretty sure that I am doing something wrong with my Tensorflow code. Please help me out.", "Please somebody help", ">> Hey, this is just a wild guess, but maybe you need to wrap your train loop inside a tf.function?\r\n\r\n> Yes, I've tested it, and that didn't improve the performance at all!!!!\r\n\r\nThis is odd. I replicated your issue on my system, and while I see a huge difference between `model.fit` and the manual training loop, wrapping the latter using `tf.function` does decrease its runtime...\r\n\r\nThe code I used, based on yours (and reducing the number of epochs to 10 for pure convenience):\r\n\r\n```\r\ndef training():\r\n    # Note: I did not specify to use the GPU, but on my system it does by default.\r\n    for epoch in range(10):\r\n        cumulative_loss = 0.0\r\n        metric.reset_states()\r\n        for images, labels in dataset:\r\n            with tf.GradientTape() as tape:\r\n                predictions = model(images, training=True)\r\n                loss = objective(labels, predictions)\r\n \r\n            grads = tape.gradient(loss, model.trainable_variables)\r\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n            cumulative_loss += loss\r\n            metric.update_state(labels, predictions)\r\n        tf.print(f'Epoch: {epoch}')\r\n        tf.print(cumulative_loss / (batch + 1))\r\n        tf.print(metric.result())\r\n\r\n\r\ndef fit_model():\r\n    model.compile(optimizer, objective, [metric])\r\n    model.fit(dataset, epochs=10)\r\n\r\n\r\ndef timeit(func):\r\n    def timed():\r\n        start = time.time()\r\n        func()\r\n        print(f'Took: {(time.time() - start):.5f}')\r\n    return timed\r\n\r\n# Timings will, of course, vary depending on the computer.\r\ntimeit(training)()               # 123 seconds\r\ntimeit(tf.function(training))()  # 58 seconds\r\ntimeit(fit_model)()              # 33 seconds\r\n```\r\n\r\nNow, that being said, there is still a significant difference between GradientTape-based and keras-wrapped fitting runtimes, and I do not know what can be done about it. To me it really feels like the logical and sad consequence of using Eager execution, but hopefully someone can prove me wrong and point out an optimization trick that will help you...", "I wasn't using `tf.print`, after using it I got an almost 1.9x speed up from `@tf.function` vs without `@tf.function`. But still far behind than `tf.keras's .fit`, which surprisingly also works as eager execution according to Tensorflow Team. \r\nAnyway hoping to see some performance improvement for `tf.GradientTape` soon", "@robieta do you know what could be happening here to make the fit version different from the manual version, with tf.function turned on?", "I'm not able to reproduce the rank order. (I'm running on a P100, so more powerful than a K80 but in the same ballpark. I'm also using `tf.enable_v2_behavior` rather than the beta install, but I don't think that should matter.) I see 14 ms / step for pure eager, 2.5 ms / step after wrapping the train step in a tf.function, and 4 ms / step with model.fit. (Expected since custom training loop is more barebones than model.fit) Can you try the following?\r\n\r\n```\r\n@tf.function\r\ndef train_step(images, labels, metric):\r\n  with tf.GradientTape() as tape:\r\n      predictions = model(images, training=True)\r\n      loss = objective(labels, predictions)\r\n\r\n  grads = tape.gradient(loss, model.trainable_variables)\r\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n  metric.update_state(labels, predictions)\r\n  return loss\r\n\r\noverall_st = time.time()\r\nwith tf.device('gpu:0'):\r\n    # # Uncomment for model.fit version\r\n    # model.compile(loss=objective, optimizer=optimizer)\r\n    # model.fit(dataset, epochs=20)\r\n    # \"\"\"\r\n    for epoch in range(20):\r\n        start = time.time()\r\n        cumulative_loss = 0.0\r\n        metric.reset_states()\r\n        for i, [images, labels] in enumerate(dataset):\r\n            cumulative_loss += train_step(images, labels, metric)\r\n\r\n        print(\"Epoch: {} Loss: {} Accuracy: {}\".format(epoch, cumulative_loss.numpy()/(batch + 1), metric.result()))\r\n        print('{:.5f} ms / step'.format((time.time() - start) * 1000 / (i + 1)))\r\n    # \"\"\"\r\nprint(\"overall time: {:.1f} sec\".format(time.time() - overall_st))\r\n```\r\n\r\nP.S. I appreciate the use of fstrings. (Even if I had to remove it because I'm on an older python version.)", "Sorry for late reply, @robieta thanks, it worked. And as of now I'm getting better performance using tf.function(55.secs) as compared to tf.keras .fit(64.8 secs), probably because .fit does some initialization before training starts.\r\nBut unfortunately in eager execution (without tf.function) its taking 270 secs.\r\nAnd if we compare this result with MXNet and Pytorch, it turns out Tensorflow eager execution is over 8x slower than MXNet and 7.5x slower than Pytorch.\r\nI've no problem using tf.function but there are some tricky parts over using it, that I don't like, for instance my version of train function definition using tf.function doesn't work, while your's works like champ.\r\nAs of now I've got the working solution so I'm closing the issue after next reply, but it'd be great if eager execution is a bit faster than what it is.\r\n\r\nThanks for help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30596\">No</a>\n", "> Sorry for late reply, @robieta thanks, it worked. And as of now I'm getting better performance using tf.function(55.secs) as compared to tf.keras .fit(64.8 secs), probably because .fit does some initialization before training starts.\r\n> But unfortunately in eager execution (without tf.function) its taking 270 secs.\r\n> And if we compare this result with MXNet and Pytorch, it turns out Tensorflow eager execution is over 8x slower than MXNet and 7.5x slower than Pytorch.\r\n> I've no problem using tf.function but there are some tricky parts over using it, that I don't like, for instance my version of train function definition using tf.function doesn't work, while your's works like champ.\r\n> As of now I've got the working solution so I'm closing the issue after next reply, but it'd be great if eager execution is a bit faster than what it is.\r\n> \r\n> Thanks for help.\r\n\r\nHi, I was also training a tf.keras model. The loss reduced very quickly when calling model.fit but it plateaued very quickly when I updated the weights manually using tf.GradientTape(). Could I know how you figured out the reason? I created a [reproducible example](https://colab.research.google.com/drive/1aIRqi_x-YGAFtZCWL4gkOPkW_8sSlZ_N) in Colab. Could you have a look? It would take 2 minutes to reproduce.\r\n\r\n", "Please file a separate bug for your problem.\n\nOn Sun, Jan 26, 2020 at 10:33 AM Xiaokang Wang <notifications@github.com>\nwrote:\n\n> Sorry for late reply, @robieta <https://github.com/robieta> thanks, it\n> worked. And as of now I'm getting better performance using\n> tf.function(55.secs) as compared to tf.keras .fit(64.8 secs), probably\n> because .fit does some initialization before training starts.\n> But unfortunately in eager execution (without tf.function) its taking 270\n> secs.\n> And if we compare this result with MXNet and Pytorch, it turns out\n> Tensorflow eager execution is over 8x slower than MXNet and 7.5x slower\n> than Pytorch.\n> I've no problem using tf.function but there are some tricky parts over\n> using it, that I don't like, for instance my version of train function\n> definition using tf.function doesn't work, while your's works like champ.\n> As of now I've got the working solution so I'm closing the issue after\n> next reply, but it'd be great if eager execution is a bit faster than what\n> it is.\n>\n> Thanks for help.\n>\n> Hi, I was also training a tf.keras model. The loss reduced very quickly\n> when calling model.fit but it plateaued very quickly when I updated the\n> weights manually using tf.GradientTape(). Could I know how you figured out\n> the reason?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30596?email_source=notifications&email_token=AAABHRKRVI5GO3CNNJL57QLQ7XJQHA5CNFSM4IBANHP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJ52S6Q#issuecomment-578529658>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLIJW7NCL3LPFGQW2TQ7XJQHANCNFSM4IBANHPQ>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 30595, "title": "TFTRT calib_graph/infer_graph has different output", "body": "I am doing tftrt convert to accelerate tf model.\r\nfirst, get calib_graph\r\ncalibration_graph = trt.create_inference_graph()\r\n\r\nsecond run calib_graph\r\nthe output is a tensor with batch_size = 16, output like this\r\n\r\nthird, I get the int8 graph\r\nint8Graph = trt.calib_graph_to_infer_graph(calibration_graph)\r\n('length and output: ', [array([-0.999994  ,  0.99682164,  0.9992819 ,  0.95060456, -0.8617101 ,\r\n        0.9993279 , -0.994945  ,  0.99960554, -0.99968964,  0.99943244,\r\n        0.9990337 ,  0.9889959 ,  0.91563284,  0.99274397,  0.99358046,\r\n        0.9925939 ], dtype=float32)])\r\n\r\nBut when I run the int8Graph using the same input data as step2, I get output like this, \r\n('length and output: ', array([0.952391  , 0.94719326, 0.9581959 , 0.86539173, 0.92513335,\r\n       0.97020185, 0.88829505, 0.97703993, 0.87482417, 0.9612503 ,\r\n       0.95443654, 0.8331325 , 0.9141325 , 0.92734456, 0.94608974,\r\n       0.9845928 ], dtype=float32))\r\n\r\nIt's really confused, any one met this problem?\r\n\r\n\r\nby the way, I run origin pb with the same data, I get \r\n('length and output: ', array([-0.999994  ,  0.99682164,  0.9992819 ,  0.9506048 , -0.8617081 ,\r\n        0.9993279 , -0.9949451 ,  0.99960554, -0.99968964,  0.99943244,\r\n        0.9990337 ,  0.9889959 ,  0.9156331 ,  0.99274397,  0.99358046,\r\n        0.9925939 ], dtype=float32)),\r\nsame as my calib_graph result.\r\n\r\nSo, what's wrong with the int8_graph???\r\n", "comments": ["@CLIsVeryOK Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary? Thanks!\r\n", "@gadagashwini  I use ubuntu 16.04/1080Ti GPU, tf1.13, installed a binary", "@CLIsVeryOK Will it be possible to provide us the complete code to reproduce the issue. Thanks!", "@gadagashwini  please give me your email, I send you the data and code in zip", "Hi @CLIsVeryOK, would you please provide the code and data via github? Thanks.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30595\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30595\">No</a>\n"]}, {"number": 30594, "title": "CUDA_ERROR_OUT_OF_MEMORY AND UNABLE TO LOAD CUDA", "body": " Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-11 12:42:11.226602: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416\r\n2019-07-11 12:42:11.399046: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAborted (core dumped)\r\n", "comments": ["Hi,\r\nThis is an error that usually shows when the GPU resources are already locked by another process. Do you by any chance have two parallel Python instances in which Tensorflow (or PyTorch, etc.) is being used?\r\nIf so, you can retain one of those instances from using the GPU by first exporting the environment variable CUDA_VISIBLE_DEVICES to -1 (or 1 if you have multiple GPUs and want to use the second one, etc.).\r\nIf that does not answer your issue, could you please give some more information as to the system you are using? (OS, TF version, etc.)", "yes!! thanks @pandrey-fr that could be a reason as multiple programs are executing on server by different members!!", "You are welcome! I am not a sysadmin, but I believe in such cases a good practice can be to have users queue their jobs that require access to the GPU, and automatically prevent access to it in other cases. At any rate, if you want to use tensorflow without accessing the GPU, `export CUDA_VISIBLE_DEVICES=-1` before launching python and you should be fine :)", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "did anyone find the solution to this problem?\r\n\r\nYour CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-01 14:26:33.107400: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz\r\n2019-08-01 14:26:33.107815: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55714b51c9f0 executing computations on platform Host. Devices:\r\n2019-08-01 14:26:33.107827: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-01 14:26:33.215296: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 8335458304\r\n2019-08-01 14:26:33.215397: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\nAborted (core dumped)", "@anil-bit Is it possible that, as was the case for the author of this issue, you already have an instance of python / tensorflow opened that reserves the entire GPU?\r\n\r\nOn a unix system you can check which programs take memory on your GPU using the `nvidia-smi` command in a terminal.\r\n\r\nTo disable the use of GPUs by tensorflow (which should be a workaround), you can run (in 2.0) `tf.config.set_visible_devices([], 'GPU')` ; I do not have the 1.14 equivalent in mind at the moment but you can easily look it up.", "@anil-bit I think you were running some program over GPU and if you have forcefully stooped it and then you are again executing it over the same GPU then this problem arises as the GPU memory was already locked by the previous program. To solve this issue terminate the terminal and again open the terminal and execute the code. Hope this helps!!"]}, {"number": 30593, "title": "(Aborted (core dumped)) - Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for MapAccumulate", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Stock example script. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: On my laptop,\r\n- TensorFlow installed from (source or binary): From source for c++ application. \r\n- TensorFlow version (use command below): v1.8.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.15.1\r\n- GCC/Compiler version (if compiling from source): g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: None.\r\n\r\n\r\n**Describe the current behavior**\r\nI have a [Standalone c++ build](https://medium.com/@tomdeore/standalone-c-build-tensorflow-opencv-6dc9d8a1412d) process for ubuntu and it works fine with my own code as well as with the c++ [example code](https://www.tensorflow.org/guide/extend/cc) on tensorflow. \r\n\r\nBut when i run the same code on CentOS platform. I get following runtime error message and the executable core dumps:\r\n\r\n```\r\n2019-07-11 10:20:42.027023: F tensorflow/core/framework/function.cc:1329] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for MapAccumulate\r\nAborted (core dumped)\r\n```\r\nMy compilation command is as: \r\n\r\n```\r\ng++ -std=gnu++11 -Wl,-rpath='$ORIGIN/lib' -fPIC -Iinclude -Llib  example.cc -ltensorflow_cc -ltensorflow_framework   -o example\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nPlease follow the steps provided on [blog](https://medium.com/@tomdeore/standalone-c-build-tensorflow-opencv-6dc9d8a1412d). \r\n", "comments": ["@milinddeore Can you please confirm whether you are getting runtime error with your code or c++ code on tensorflow ,with CentOS platform.Thanks!", "@ravikyram I got this issues using both my code as well as with c++ code on Tensorflow.  ", "@milinddeore Sorry for missing this issue. Can you please check with most recent version of TF like `TF1.15.2` or `TF2.2` (preferred). Thanks!\r\n\r\nIf this was already resolved for you, please close the issue. Thanks!", "@jvishnuvardhan Thanks for your reply. I'm also facing this issue on TF1.12.0, when I'm trying to include a deps of `//tensorflow/core/kernels:lookup`. \r\n\r\nDo you have any idea how to avoid this core issue? ", "@icoffeebeans can you please check with latest version and let us know? Thanks!", "1.12 is not supported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I am closing this issue as `TF1.12` is not supported. Please use recent stable versions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30593\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30593\">No</a>\n", "the same coredump as @milinddeore "]}, {"number": 30592, "title": "Macros definition for using MKL from Eigen", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution Centos 7.2 (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1 Release\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: Build from source, no conda\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nIn tensorflow/tensorflow/tensorflow.bzl, it defines the macros used for using MKL from Eigen as below:\r\n`if_mkl([\"-DINTEL_MKL=1\", \"-DEIGEN_USE_VML\"])`\r\n\r\nHowever, in Eigen's documentation, the macro is:\r\n`-DEIGEN_USE_MKL_VML`\r\n \r\nIs this intentional? I tried to read both tensorflow's and Eigen's source code but did not find a reason of that. \r\nThanks.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["-DEIGEN_USE_VML is to use pure Eigen (w/o mkl) while -DEIGEN_USE_MKL_VML is enabling the use of inbuilt Intel MKL through Eigen", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30592\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30592\">No</a>\n"]}, {"number": 30591, "title": "Consistent use of else after return (pylint no-else-return)", "body": "While working on some pylint warnings in `tensorflow/python/keras/backend.py` I noticed that two different ways of \"if else return\" styles are used.\r\n\r\n1. `if condition: return 0`\r\n    `return 1`\r\n2. `if condition: return 0`\r\n    `else: return 1`\r\n\r\nIs changing the behaiviour to one of the variants considered good practice?\r\nI would like to clear things up (and improve the pylint score :D ).\r\n\r\n", "comments": ["Both behaviours appear in the PEP 8 style guide (look up \"be consistent in return statements.\" on [this page](https://www.python.org/dev/peps/pep-0008/#programming-recommendations)), so I do not believe there is a preferred one _per se_, at least not in this guide.\r\n\r\nThat being said, in my humble not-a-tensorflow-developer opinion, you should (in accordance with PEP-8 principles) use the alternative that feels best as to readability and comprehensibility based on context. Parsimony would ask to do as pylint suggests, getting rid of useless `else` statements, but there might be cases in which this statement actually improves the reader's comprehension of what is happening, in which case you should probably keep it. If you worry about pylint scores, you can always add a pylint command in comment on those specific cases.", "Alright, thanks for clarifying!"]}, {"number": 30590, "title": "add i386 architecture", "body": "in order to build a fully compatible universal framework for ios, the `i386` architecture needs to be in the fat library archive.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30590) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30590) for more info**.\n\n<!-- ok -->", "Question: Is i386 used in any real use cases? \r\n\r\nI remember that we dropped i386 support because XCode has problem compiling `thread_local` with i386 starting from XCode 10.1. `thread_local` was used by Eigen, which is a library used by TensorFlow Lite. \r\n\r\nI'm not sure if anything changed. Are you able to run the script with newest XCode?", "@mlostekk Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "Can one of the admins verify this patch?", "well, the i386 is still required if you build a universal framework (e.g. with carthage) for the simulator. \r\n\r\nhttps://stackoverflow.com/questions/28033635/why-does-the-ios-simulator-require-i386-and-x86-64-symbols-even-though-im-on-an", "It builds fine for me without addressing the `thread_local` issue. Though it appears when building the full tensorflow package. I could also provide a PR for this if needed.", "Just created a extra PR for fixing the `thread_loacal` issue for i386 builds. \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/32166"]}, {"number": 30589, "title": "Train models under the director '/mxnet/example/gluon/'", "body": "I conduct distributed training on almost all the models provided under the directory _'/mxnet/example/gluon'_, however, when running the models with the asynchronous mechanism (--kvstore dist_async), none of the models is convergent. \r\n For instance, I train the _resnet18_v1_ with _**cifar10**_ dataset, the training accuracy is around 0.09 under the asynchronous mechanism. Nevertheless, training with the synchronous mechanism can achieve the target accuracy.\r\nI do not know the reason why the _asynchronous SGD_ is not applicable to these models, and what should I do to fix this? ", "comments": ["This is not for Tensorflow and I am sorry"]}, {"number": 30588, "title": "[mlir] fix two trivial typos in comments", "body": "", "comments": []}, {"number": 30587, "title": "keras define a trainable weights as layer", "body": "Hi~I have some problems in use tf.keras to build model. Now I want to define a trainbale weight B with shape(64, 128), then do tensor matrix, how to define a trainable weight B in keras similar to `tf.get_variable`\r\n```\r\nA = Input(shape=(128,))\r\nout = tf.keras.layers.Lambda(lambda x: tf.matmul(x[0], x[1], transpose_b=True))([A, B])\r\nmodel = Model(inputs=A, outputs=out)\r\n```\r\nI hope someone can help me.It's very grateful!", "comments": ["Hi,\r\n\r\nYou could simply create a random Tensor, then make it trainable using `tf.Variable`:\r\n```\r\ninputs = tf.keras.Input((128,))\r\nweights = tf.Variable(tf.random.normal((64, 128)))\r\noutput = tf.matmul(inputs, tf.transpose(outputs))\r\nmodel = tf.keras.Model(inputs, output)\r\n```\r\n\r\nThat being said, you probably have better do something like:\r\n```\r\ninputs = tf.keras.Input((128,))\r\noutput = tf.keras.layers.Dense(64, use_bias=False)(inputs)\r\nmodel = tf.keras.Model(inputs, output)\r\n# And if you want to access the weights:\r\nweights = model.layers[-1].weights[0]\r\n```\r\nNote that you can `tf.transpose` the `weights` tensor if you really want it in `(64, 128)` shape. In this version, you can also pass additional arguments to `tf.keras.layers.Dense` to use a specific initialization scheme.\r\n\r\nI hope this helps. By the way, no offence, but this question would be better asked on a questions answering website (e.g. StackOverflow) than on a GitHub Issues tracker.", "@pandrey-fr  Sorry to trouble you! Thanks for you answer. I still have some question.Can you help me? In your code, I change something.\r\n```\r\ninputs = tf.keras.Input((128,))\r\nweights = tf.Variable(tf.random.normal((64, 128)))\r\noutput = tf.keras.layers.Lambda(lambda x: tf.matmul(x, tf.transpose(weights)))(inputs)\r\nmodel = tf.keras.Model(inputs, output)\r\n```\r\nthe  outputs:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_10 (InputLayer)        (None, 128)               0         \r\n_________________________________________________________________\r\nlambda_2 (Lambda)            (None, 64)                0         \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n```\r\nthe defined weights is not trainable. \r\nIn addition, I know Dense can get trained matrix weights and bias. But if  I want add a bias, I can't use Dense.\r\nHowever, I have to use add_weights in custome layer, for example:\r\n```\r\nclass Bias(keras.layers.Layer):\r\n    def build(self, input_shape):\r\n        self.bias = self.add_weight(shape=(64, 128), initializer='zeros', dtype=tf.float32, name='x')\r\n        self.built = True\r\n\r\n    def call(self, inputs):\r\n        return inputs + self.bias\r\n\r\ninputs = Input(shape=(64, 128))\r\noutputs = Bias()(inputs)\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.summary()\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_11 (InputLayer)        (None, 64, 128)           0         \r\n_________________________________________________________________\r\nbias_5 (Bias)                (None, 64, 128)           8192      \r\n=================================================================\r\nTotal params: 8,192\r\nTrainable params: 8,192\r\nNon-trainable params: 0\r\n```\r\nIs there any more easily method to define a trainable variable \uff1f", "> @pandrey-fr Sorry to trouble you\r\n\r\nNo worries, I am happy to try to help you - but I think this discussion would be more suited on Stack than here, where it might take some visibility from actual issues which TF developers (of which I am not part - I am just a random TF user) have to deal with.\r\n\r\n> the defined weights is not trainable\r\n\r\nThe variable is defined as trainable, but keras won't make it so in the model, for some reason... The same happens with `tf.keras.backend.variable`, which does not make much sense to me but must be related to how keras builds the model's graph.\r\n\r\nIndeed, you would need to write it as a custom layer - or, in this case, use `tf.keras.layers.Dense` (with `use_bias=False` is you do not want to use a bias). Same goes for adding a bias - however unpractical it may seem for something as simple as that, I think your code is the proper workaround.\r\n\r\nThere may be a better way, but at the moment I do not see it. I would encourage you to copy/paste your question on a more suited website where there is a higher probability that someone with a better idea would read and answer it :)\r\n", "@pandrey-fr  It's very grateful! I will question on stackoverflow to look for other answers. Thank you~", "You are very welcome! (and if I may, you probably should close this issue, and optionally post the answer here afterwards if you manage to get a good one)"]}, {"number": 30586, "title": "[LITE] Deprecated tf.app.run removed from lite files.", "body": "The below deprecation warning is removed from all lite files.\r\n` The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.`\r\n", "comments": []}, {"number": 30585, "title": "Add should_finialize(state) to tf.data.experimental.Reducer", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrent behavior: tf.data.experimental.Reducer will not finalize until the end of a dataset.\r\nWanted feature: I don't want to reduce the whole dataset. I hope that the Reducer could decide when to finalize based on its state. For example, add the following function inside Reducer and call the function after reduce_func().\r\ndef should_finialize(state):\r\n  return True   --> stop reducing anymore, call finalize_func, and return results.\r\n  return False  --> continue reduce.\r\n\r\n**Will this change the current api? How?**\r\nNo. The default should_finialize always returns False. If anyone wants to change this behavior, he needs to override this function.\r\n\r\n**Who will benefit with this feature?**\r\nThe tf.data input pipeline will be more versatile with this new feature. It is possible to do more operations without calling a python generator or py_func. For example, we need several different samples from the same class to be in one batch for tf.contrib.losses.metric_learning.triplet_semihard_loss. The labels of a batch may look like the following:\r\n0, 0, 0, 0,\r\n1, 1, 1, 1,\r\n...\r\n15, 15, 15, 15\r\nThere are 64 (=16*4) samples in this batch. I cannot find a pure tf.data way to form batches in this way. But with the new feature, we are able to do so.\r\n\r\n**Any Other info.**\r\nIt seems that Jiri Simsa @jsimsa is very familiar with tf.data.experimental.Reducer.", "comments": ["I believe that you should be able to achieve this with existing transformations. In particular, I would direct your attention to [group_by_reducer](https://www.tensorflow.org/api_docs/python/tf/data/experimental/group_by_reducer), [group_by_window](https://www.tensorflow.org/api_docs/python/tf/data/experimental/group_by_window), [scan](https://www.tensorflow.org/api_docs/python/tf/data/experimental/scan) and [take_while](https://www.tensorflow.org/api_docs/python/tf/data/experimental/take_while).\r\n\r\nIf you provide me with a concrete example of input / output behavior, I can provide you a solution based on existing transformations.", "Thanks for your help.\r\nLet's suppose I have a dataset composed of (image, label).\r\nimage, label\r\nim0, 0\r\nim1, 0\r\nim2, 1\r\nim3, 1\r\nim4, 1\r\nim5, 2\r\n\r\nI want to generate a batch containing 4 samples. The first two are sharing the same label, and the last two are sharing the same label. This can be down by the following code, which is what I am using now.\r\n```\r\nds = ds.shuffle(6).repeat()\r\nds = ds.apply(tf.data.experimental.group_by_window(\r\n  lambda im, label: label,\r\n  lambda key, data: data.batch(2), \r\n  2))\r\nds = ds.apply(tf.data.experimental.unbatch())\r\nds = ds.batch(4)\r\n```\r\n\r\nBut there is a problem with this code. The first and second samples may be exactly the same one, which is what I want to **avoid** now. I don't want the same sample appearing more than once in each window.\r\n\r\nLets' suppose the shuffled stream are\r\nepoch 0: im0, im1, im2, im3, im4, im5\r\nepoch 1: im0, im1, im4, im2, im3, im5\r\nepoch 2: im0, im1, im2, im3, im4, im5\r\nThe generated batch should be\r\nbatch 0: im0, im1, im2, im3\r\nbatch 1: im0, im1, im4, im4\r\nbatch 2: im2, im3, im5, im5\r\n...\r\n\r\nThe third and fourth samples are exactly the same in batch 1 and batch 2. They will not generate any loss for margin-based losses.\r\n\r\nWhat I am expecting is\r\nbatch 0: im0, im1, im2, im3\r\nbatch 1: im0, im1, im4, ~~im4~~ im2  (when the second im4 is adding to a window, do not add it)\r\nbatch 2: im0, im1, im3, im2\r\nbatch 3: im3, im4, ...\r\n\r\nThe window for im5 will not be filled forever.", "You can achieve the desired effect as follows:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.enable_v2_behavior()\r\n\r\ndata = (['a', 'b', 'c', 'd', 'e', 'f'], \r\n        np.array([0, 0, 1, 1, 1, 2], dtype=np.int64))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(data).shuffle(6)\r\n\r\ndataset = dataset.apply(tf.data.experimental.group_by_window(\r\n    lambda x, y: y,\r\n    lambda key, x: x.batch(2),\r\n    window_size=2,\r\n))\r\ndataset = dataset.unbatch().batch(4, drop_remainder=True)\r\ndataset = dataset.repeat()\r\n\r\nfor elem in dataset.take(3):\r\n  x, y = elem\r\n  print(x.numpy(), y.numpy())\r\n```", "Great! Thank you very much."]}, {"number": 30584, "title": "Disable some more tests before patch release", "body": "They seem to fail so rather than debugging just disable them as they shouldn't be influenced by patch.", "comments": []}, {"number": 30583, "title": "Changes to build files for patch release", "body": "Add `set -ex` to build scripts as well as bazel flags for incompatible versions.\r\n\r\nAlso change `Dockerfile` for Debian-based docker container", "comments": []}, {"number": 30582, "title": "Run buildifier on files from #30573", "body": "Sanity build fails on these two files", "comments": []}, {"number": 30581, "title": "Restoring only a PART of the graph from warm_start_from in Estimator", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen restoring a TF estimator, we can use warm_start_from. However, that means all variables will be restored from the checkpoint. (see https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\r\nHowever, what if my model_fn is changed with some more variables being added? In that case, we can't use warm_start_from.\r\nWe hope those newly added variables can be initialized by some default initializer while others are loaded from the checkpoint. That way, the estimator can be incrementally defined. Much more flexible!\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nBERT users and Wide-n-deep estimator users.\r\n**Any Other info.**\r\n", "comments": ["Restoring from all variables is the default behavior. But it's possible to only restore select variables with WarmStartSettings (https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings).", "> Restoring from all variables is the default behavior. But it's possible to only restore select variables with WarmStartSettings (https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings).\r\n\r\nThat method assumes the graph of the estimator is not being modified (identical graph), right?\r\nLet's say, I firstly train one estimator with all parameters being in the scope of \"**Round1**/\"; then I modify the graph of the estimator by adding some more trainable variables in the scope of \"**Round2**/\".\r\nIn that case, the second graph contains, but bigger than the first graph. Can I use WarmStartSettings to load variables in \"**Round1**/\" from the 1st estimator, and then randomly initialize newly added variables in \"**Round2**/\"?\r\n\r\nMaybe I should use a **different** model_dir when instantiating the new estimator? Otherwise, the **WarmStartSettings** will be blocked and the new estimator will simply **compare and load** all variables from the model specified in model_dir.\r\n\r\nThanks.", "No it doesn't assume that at all. You can use `var_name_to_prev_var_name` and pass a dictionary like {'Round2/my_var': 'Round1/my_var'} as long as they are the shape. You can also filter which variables to restore with the `vars_to_warm_start` argument. This question probably belongs on StackOverflow, also.\r\n\r\nAnd yes, if you're doing a partial restore, you should be using new model directory.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 30580, "title": "TPU has XLA compilation issue on TF 1.14", "body": "I am getting an issue with using XLA on the cloud TPU on tensorflow version 1.14\r\n\r\n**System information**\r\n- Using Google's cloud TPU with Tensorflow 1.14\r\n```v1.14.0-rc1-22-gaf24dc91b5 1.14.0```\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3379601/tf_env.txt)\r\nSystem info (sanity check log message removed, full `tf_env.txt` attached above):\r\n```\r\n\r\n== check python ===================================================\r\npython version: 3.5.3\r\npython branch: \r\npython build version: ('default', 'Sep 27 2018 17:25:39')\r\npython compiler version: GCC 6.3.0 20170516\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1 SMP Sat Jun 22 23:33:41 PDT 2019\r\nos release version: 4.14.111+\r\nos platform: Linux-4.14.111+-x86_64-with-debian-9.9\r\nlinux distribution: ('debian', '9.9', '')\r\nlinux os distribution: ('debian', '9.9', '')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='cs-6000-devshell-vm-cb1acb17-794a-49c5-8346-cd612beb1d0d', release='4.14.111+', version='#1 SMP Sat Jun 22 23:33:41 PDT 2019', machine='x86_64', processor='')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy (1.16.4)\r\nprotobuf (3.8.0)\r\ntensorflow (1.14.0)\r\ntensorflow-estimator (1.14.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 1.14.0\r\ntf.version.GIT_VERSION = v1.14.0-rc1-22-gaf24dc91b5\r\ntf.version.COMPILER_VERSION = 4.8.5\r\nSanity check: -- EDITED OUT ---\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 1.14.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python3.5/dist-packages\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 5, 3, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\nBuild label: 0.27.1\r\nBuild time: Tue Jul 2 17:49:35 2019 (1562089775)\r\nBuild timestamp: 1562089775\r\nBuild timestamp as int: 1562089775\r\n\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run my model (which is a partial implementation of Transformer) using XLA, I get an error message.\r\n\r\nThe error message is the following:\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0710 13:59:32.655435 139705705027328 deprecation_wrapper.py:119] From issue.py:24: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\r\n\r\nW0710 13:59:32.655846 139705705027328 deprecation.py:323] From issue.py:25: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_types(dataset)`.\r\nW0710 13:59:32.655990 139705705027328 deprecation.py:323] From issue.py:26: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_shapes(dataset)`.\r\nW0710 13:59:32.663649 139705705027328 deprecation_wrapper.py:119] From issue.py:68: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nW0710 13:59:32.707169 139705705027328 deprecation_wrapper.py:119] From issue.py:85: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\r\n\r\nW0710 13:59:32.714751 139705705027328 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0710 13:59:33.743603 139705705027328 deprecation_wrapper.py:119] From issue.py:205: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\r\n\r\nW0710 13:59:35.527529 139705705027328 deprecation_wrapper.py:119] From issue.py:206: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nW0710 13:59:35.536509 139705705027328 deprecation_wrapper.py:119] From issue.py:208: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\r\n\r\nW0710 13:59:35.547362 139705705027328 deprecation_wrapper.py:119] From issue.py:30: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2019-07-10 13:59:35.547951: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-10 13:59:35.799823: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2019-07-10 13:59:35.800045: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5608f773ae90 executing computations on platform Host. Devices:\r\n2019-07-10 13:59:35.800062: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nW0710 13:59:35.811006 139705705027328 deprecation_wrapper.py:119] From issue.py:31: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\n2019-07-10 13:59:36.203915: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-07-10 13:59:36.572778: W tensorflow/core/framework/allocator.cc:107] Allocation of 67141632 exceeds 10% of system memory.\r\n2019-07-10 13:59:36.861743: W tensorflow/core/framework/allocator.cc:107] Allocation of 67141632 exceeds 10% of system memory.\r\n2019-07-10 13:59:37.125698: W tensorflow/core/framework/allocator.cc:107] Allocation of 67141632 exceeds 10% of system memory.\r\n2019-07-10 13:59:37.551537: W tensorflow/core/framework/allocator.cc:107] Allocation of 67141632 exceeds 10% of system memory.\r\nW0710 13:59:39.391257 139705705027328 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_types(iterator)`.\r\nW0710 13:59:39.444635 139705705027328 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_shapes(iterator)`.\r\nW0710 13:59:39.445140 139705705027328 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_classes(iterator)`.\r\n2019-07-10 13:59:44.506360: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at xla_ops.cc:343 : Invalid argument: Detected unsupported operations when trying to compile graph cluster_13546192731870215987[] on XLA_CPU_JIT: TemporaryVariable (No registered 'TemporaryVariable' OpKernel for XLA_CPU_JIT devices compatible with node {{node gradients/AddN_13/tmp_var}}\r\n  .  Registered:  device='CPU'\r\n){{node gradients/AddN_13/tmp_var}}\r\n  This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_13546192731870215987[] on XLA_CPU_JIT: TemporaryVariable (No registered 'TemporaryVariable' OpKernel for XLA_CPU_JIT devices compatible with node {{node gradients/AddN_13/tmp_var}}\r\n  .  Registered:  device='CPU'\r\n){{node gradients/AddN_13/tmp_var}}\r\n  This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\n   [[cluster]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"issue.py\", line 221, in <module>\r\n    sys.exit(main(sys.argv))\r\n  File \"issue.py\", line 217, in main\r\n    hlo = get_hlo(transformer_model_fn, transformer_input_fn)\r\n  File \"issue.py\", line 33, in get_hlo\r\n    sess.run(loss)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_13546192731870215987[] on XLA_CPU_JIT: TemporaryVariable (No registered 'TemporaryVariable' OpKernel for XLA_CPU_JIT devices compatible with node {{node gradients/AddN_13/tmp_var}}\r\n  .  Registered:  device='CPU'\r\n){{node gradients/AddN_13/tmp_var}}\r\n  This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\n   [[cluster]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect there to be no error message!\r\n\r\n\r\n**Code to reproduce the issue**\r\nReproduce with `python3 issue.py`\r\n\r\n[issue.py.zip](https://github.com/tensorflow/tensorflow/files/3379603/issue.py.zip)\r\nissue.py is the following (also zipped above):\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.compiler.xla import compile\r\n\r\nHIDDEN_SIZE = 2048\r\nFILTER_SIZE = 8196\r\nNUM_HEADS = 32\r\nD_K = HIDDEN_SIZE // NUM_HEADS\r\nD_K_ROOT = D_K ** (-0.5)\r\nWORD_LEN = 512 \r\n\r\n\r\ndef get_hlo(model_fn, input_fn):\r\n\r\n    def xla_model_fn(features, labels):\r\n        spec = model_fn(features, labels, mode=\"train\", params=None)\r\n        with tf.control_dependencies([spec.train_op]):\r\n            return tf.identity(spec.loss, name=spec.loss.op.name)\r\n\r\n    train_ds = input_fn().repeat()\r\n\r\n    iterator = tf.data.Iterator.from_structure(\r\n        train_ds.output_types,\r\n        train_ds.output_shapes,\r\n    )\r\n    loss, = compile(xla_model_fn, inputs=iterator.get_next())\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(iterator.make_initializer(train_ds))\r\n        sess.run(loss)\r\n\r\n\r\ndef transformer_input_fn():\r\n    feat_shape = [1, WORD_LEN, HIDDEN_SIZE]\r\n    labels_shape = [1, 1, 1, WORD_LEN]\r\n\r\n    model_inputs = [\r\n        tf.image.convert_image_dtype(\r\n            tf.reshape(\r\n                tf.constant(\r\n                    np.random.uniform(size=feat_shape)\r\n                ),\r\n                feat_shape,\r\n            ),\r\n            tf.float32,\r\n        ),\r\n    ]\r\n\r\n    model_expected = [\r\n        tf.image.convert_image_dtype(\r\n            tf.reshape(\r\n                tf.constant(np.random.uniform(size=labels_shape)),\r\n                labels_shape,\r\n            ),\r\n            tf.float32,\r\n        ),\r\n    ]\r\n\r\n    dataset = (model_inputs, model_expected)\r\n    dataset = tf.data.Dataset.from_tensor_slices(dataset)\r\n    return dataset\r\n\r\n\r\ndef transformer_model_fn(features, labels, mode, params):\r\n    layer_norm_scale = tf.get_variable(\r\n        \"layer_norm_scale\",\r\n        [HIDDEN_SIZE],\r\n        initializer=tf.ones_initializer(),\r\n    )\r\n    layer_norm_bias = tf.get_variable(\r\n        \"layer_norm_bias\",\r\n        [HIDDEN_SIZE],\r\n        initializer=tf.zeros_initializer(),\r\n    )\r\n\r\n    def layer_normalization(net):\r\n        mean = tf.reduce_mean(net, axis=[-1], keepdims=True)\r\n        mean = tf.multiply(mean, -1)\r\n        diff = tf.add(net, mean)\r\n        var = tf.reduce_mean(tf.square(diff), axis=[-1], keepdims=True)\r\n        var = tf.add(var, 1e-6)\r\n        net = tf.multiply(diff, tf.rsqrt(var))\r\n        return tf.add(tf.multiply(net, layer_norm_scale), layer_norm_bias)\r\n\r\n    def transformer_fc(net, size, use_bias=True):\r\n        return tf.keras.layers.Dense(\r\n            size,\r\n            activation=None,\r\n            use_bias=use_bias,\r\n        )(net)\r\n\r\n    def feed_forward(net):\r\n        net = transformer_fc(net, FILTER_SIZE)\r\n        net = tf.nn.relu(net)\r\n        net = tf.nn.dropout(net, rate=0.1)\r\n        net = transformer_fc(net, HIDDEN_SIZE)\r\n        return net\r\n\r\n    def multi_headed_attention(q, k, v, bias):\r\n        def split_heads(net):\r\n            net = tf.reshape(\r\n                net,\r\n                [1, WORD_LEN, NUM_HEADS, D_K],\r\n            )\r\n            net = tf.transpose(net, [0, 2, 1, 3])\r\n            return net\r\n\r\n        def combine_heads(net):\r\n            net = tf.transpose(net, [0, 2, 1, 3])\r\n            net = tf.reshape(\r\n                net, [1, WORD_LEN, HIDDEN_SIZE]\r\n            )\r\n            return net\r\n\r\n        q = transformer_fc(q, HIDDEN_SIZE, use_bias=False)\r\n        k = transformer_fc(k, HIDDEN_SIZE, use_bias=False)\r\n        v = transformer_fc(v, HIDDEN_SIZE, use_bias=False)\r\n\r\n        q = split_heads(q)\r\n        k = split_heads(k)\r\n        v = split_heads(v)\r\n\r\n        q = tf.multiply(q, D_K_ROOT)\r\n        x = tf.matmul(q, k, transpose_b=True)\r\n        x = tf.add(x, bias)\r\n\r\n        x = tf.nn.softmax(x)\r\n        x = tf.nn.dropout(x, rate=0.1)\r\n        x = tf.matmul(x, v)\r\n\r\n        x = combine_heads(x)\r\n        x = transformer_fc(x, HIDDEN_SIZE, use_bias=False)\r\n        return x\r\n\r\n    def encoder_block(net, enc_bias):\r\n        residual = layer_normalization(net)\r\n        residual = multi_headed_attention(\r\n            residual, residual, residual, enc_bias\r\n        )\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        residual = layer_normalization(net)\r\n        residual = feed_forward(residual)\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        return layer_normalization(net)\r\n\r\n    def encode(enc_input, enc_bias):\r\n        net = tf.nn.dropout(enc_input, rate=0.1)\r\n        net = encoder_block(net, enc_bias) \r\n        return net\r\n\r\n    def decoder_block(net, enc_output, dec_bias, enc_dec_bias):\r\n        residual = layer_normalization(net)\r\n        residual = multi_headed_attention(\r\n            residual, residual, residual, dec_bias\r\n        )\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        residual = layer_normalization(net)\r\n        residual = multi_headed_attention(\r\n            residual, enc_output, enc_output, enc_dec_bias\r\n        )\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        residual = layer_normalization(net)\r\n        residual = feed_forward(residual)\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        return layer_normalization(net)\r\n\r\n    def decode(dec_input, enc_output, dec_bias, enc_dec_bias):\r\n        net = tf.nn.dropout(dec_input, rate=0.1)\r\n        net = decoder_block(net, enc_output, dec_bias, enc_dec_bias)\r\n        return net\r\n\r\n    labels = tf.stop_gradient(labels)\r\n\r\n    enc_input = features\r\n    dec_input = features\r\n    enc_bias = labels\r\n    enc_dec_bias = labels\r\n    dec_bias = labels\r\n    target = features\r\n\r\n    enc_output = encode(enc_input, enc_bias)\r\n    dec_output = decode(dec_input, enc_output, dec_bias, enc_dec_bias)\r\n\r\n    net = dec_output\r\n\r\n    loss = tf.reduce_mean(\r\n        tf.nn.softmax_cross_entropy_with_logits_v2(\r\n            labels=target,\r\n            logits=net,\r\n        )\r\n    )\r\n    train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\r\n    global_step = tf.train.get_or_create_global_step()\r\n\r\n    update_global_step = tf.assign(\r\n        global_step, global_step + 1, name=\"update_global_step\"\r\n    )\r\n    return tf.estimator.EstimatorSpec(\r\n        mode, loss=loss, train_op=tf.group(train_step, update_global_step)\r\n    )\r\n\r\n\r\ndef main(args):\r\n    hlo = get_hlo(transformer_model_fn, transformer_input_fn)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main(sys.argv))\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe issue can actually be avoided if the `issue.py` file is modified slightly:\r\n\r\nchange\r\n```\r\nHIDDEN_SIZE = 2048\r\nFILTER_SIZE = 8196 \r\nNUM_HEADS = 32\r\n```\r\nto\r\n```\r\nHIDDEN_SIZE = 2048 // 4\r\nFILTER_SIZE = 8196 // 4\r\nNUM_HEADS = 32 // 4\r\n```\r\non lines `7-9`\r\n\r\nNote that this fix works on the Google cloud TPU, may not work on other machines?\r\n\r\nThe issue was also reproduced on multiple tf versions:\r\n\r\n` tensorflow/tensorflow:1.13.0rc0-py3`: Was able to reproduce the issue\r\n` tensorflow/tensorflow:1.13.0rc1-py3`: Was able to reproduce the issue\r\n` tensorflow/tensorflow:1.13.0rc2-py3`: Was able to reproduce the issue\r\n` tensorflow/tensorflow:1.13.1-py3`: Was able to reproduce the issue\r\n` tensorflow/tensorflow:1.14.0-py3`: Was able to reproduce the issue\r\n` tensorflow/tensorflow:2.0.0a0-py3`: No module named 'tensorflow.contrib' import error\r\n\r\nAll of these tf versions were run on vanilla tf docker images. The fix above where I divide the numbers by 4 also fixes the issue on all those versions as well.\r\n\r\ntf version 2 and higher was not tested because of significant API changes, for this I need to spend some time creating a new python file with new API calls to get the issue reproduced\r\n\r\nImages:\r\n\r\nIssue reproduced using issue.py:\r\n<img width=\"1081\" alt=\"tpu_err\" src=\"https://user-images.githubusercontent.com/44978436/61009735-92e4d380-a328-11e9-9701-aed9d921ecde.png\">\r\n\r\nIssue avoided by dividing variables by 4  (expected behavior):\r\n<img width=\"1430\" alt=\"tf_no_issue\" src=\"https://user-images.githubusercontent.com/44978436/61009751-9f692c00-a328-11e9-9e2c-9c2417211360.png\">\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0710 15:25:47.346601 140584685344512 deprecation_wrapper.py:119] From issue.py:24: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\r\n\r\nW0710 15:25:47.347033 140584685344512 deprecation.py:323] From issue.py:25: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_types(dataset)`.\r\nW0710 15:25:47.347194 140584685344512 deprecation.py:323] From issue.py:26: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_shapes(dataset)`.\r\nW0710 15:25:47.356488 140584685344512 deprecation_wrapper.py:119] From issue.py:68: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nW0710 15:25:47.417571 140584685344512 deprecation_wrapper.py:119] From issue.py:85: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\r\n\r\nW0710 15:25:47.425961 140584685344512 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0710 15:25:48.473849 140584685344512 deprecation_wrapper.py:119] From issue.py:205: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\r\n\r\nW0710 15:25:49.911786 140584685344512 deprecation_wrapper.py:119] From issue.py:206: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nW0710 15:25:49.917340 140584685344512 deprecation_wrapper.py:119] From issue.py:208: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\r\n\r\nW0710 15:25:49.928683 140584685344512 deprecation_wrapper.py:119] From issue.py:30: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2019-07-10 15:25:49.929245: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-10 15:25:49.939914: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2019-07-10 15:25:49.940074: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5563e2d4be50 executing computations on platform Host. Devices:\r\n2019-07-10 15:25:49.940087: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nW0710 15:25:49.940532 140584685344512 deprecation_wrapper.py:119] From issue.py:31: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\n2019-07-10 15:25:50.432006: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nW0710 15:25:50.529966 140584685344512 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_types(iterator)`.\r\nW0710 15:25:50.530351 140584685344512 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_shapes(iterator)`.\r\nW0710 15:25:50.530480 140584685344512 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_classes(iterator)`.\r\n2019-07-10 15:26:03.872235: W tensorflow/core/framework/allocator.cc:107] Allocation of 123777024 exceeds 10% of system memory.\r\n```", "comments": ["I tried on Colab with TPU and without TPU using tensorflow version 1.14. I am able to reproduce the issue. Thanks! ", "This fails because the XLA cluster has a `TemporaryVariable` op which XLA does not support.  I'm not familiar enough with the TF APIs here to immediately point out where it is coming from, but I'd suggest changing your model to use resource variables to see if that fixes the issue.", "@sanjoy\r\nI enabled resource variables by:\r\n- placing `tf.enable_resource_variables()` in the beginning of my `transformer_model_fn`\r\n- placing `tf.enable_resource_variables()` right before I compile the XLA\r\n- passing in `use_resource=True` for `tf.get_variable`\r\n\r\n I still get the same issue. The `TemporaryVariable` I believe is a part of the gradient calculation for `reduce_sum`, I need `reduce_sum` for this model. What I find an issue here is that `TemporaryVariable` doesn't appear (the issue is avoided) if I reduce the `HIDDEN_SIZE`, `FILTER_SIZE` and `NUM_HEADS` by a factor of 4.\r\n\r\nCode:\r\n```import sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.compiler.xla import compile\r\n\r\nHIDDEN_SIZE = 2048\r\nFILTER_SIZE = 8196\r\nNUM_HEADS = 32\r\nD_K = HIDDEN_SIZE // NUM_HEADS\r\nD_K_ROOT = D_K ** (-0.5)\r\nWORD_LEN = 512 \r\n\r\n\r\ndef get_hlo(model_fn, input_fn):\r\n\r\n    def xla_model_fn(features, labels):\r\n        spec = model_fn(features, labels, mode=\"train\", params=None)\r\n        with tf.control_dependencies([spec.train_op]):\r\n            return tf.identity(spec.loss, name=spec.loss.op.name)\r\n\r\n    train_ds = input_fn().repeat()\r\n\r\n    iterator = tf.data.Iterator.from_structure(\r\n        train_ds.output_types,\r\n        train_ds.output_shapes,\r\n    )\r\n    tf.enable_resource_variables()\r\n    loss, = compile(xla_model_fn, inputs=iterator.get_next())\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(iterator.make_initializer(train_ds))\r\n        sess.run(loss)\r\n\r\n\r\ndef transformer_input_fn():\r\n    feat_shape = [1, WORD_LEN, HIDDEN_SIZE]\r\n    labels_shape = [1, 1, 1, WORD_LEN]\r\n\r\n    model_inputs = [\r\n        tf.image.convert_image_dtype(\r\n            tf.reshape(\r\n                tf.constant(\r\n                    np.random.uniform(size=feat_shape)\r\n                ),\r\n                feat_shape,\r\n            ),\r\n            tf.float32,\r\n        ),\r\n    ]\r\n\r\n    model_expected = [\r\n        tf.image.convert_image_dtype(\r\n            tf.reshape(\r\n                tf.constant(np.random.uniform(size=labels_shape)),\r\n                labels_shape,\r\n            ),\r\n            tf.float32,\r\n        ),\r\n    ]\r\n\r\n    dataset = (model_inputs, model_expected)\r\n    dataset = tf.data.Dataset.from_tensor_slices(dataset)\r\n    return dataset\r\n\r\n\r\ndef transformer_model_fn(features, labels, mode, params):\r\n    tf.enable_resource_variables()\r\n    layer_norm_scale = tf.get_variable(\r\n        \"layer_norm_scale\",\r\n        [HIDDEN_SIZE],\r\n        initializer=tf.ones_initializer(),\r\n        use_resource=True,\r\n    )\r\n    layer_norm_bias = tf.get_variable(\r\n        \"layer_norm_bias\",\r\n        [HIDDEN_SIZE],\r\n        initializer=tf.zeros_initializer(),\r\n        use_resource=True,\r\n    )\r\n\r\n    def layer_normalization(net):\r\n        mean = tf.reduce_mean(net, axis=[-1], keepdims=True)\r\n        mean = tf.multiply(mean, -1)\r\n        diff = tf.add(net, mean)\r\n        var = tf.reduce_mean(tf.square(diff), axis=[-1], keepdims=True)\r\n        var = tf.add(var, 1e-6)\r\n        net = tf.multiply(diff, tf.rsqrt(var))\r\n        return tf.add(tf.multiply(net, layer_norm_scale), layer_norm_bias)\r\n\r\n    def transformer_fc(net, size, use_bias=True):\r\n        return tf.keras.layers.Dense(\r\n            size,\r\n            activation=None,\r\n            use_bias=use_bias,\r\n        )(net)\r\n\r\n    def feed_forward(net):\r\n        net = transformer_fc(net, FILTER_SIZE)\r\n        net = tf.nn.relu(net)\r\n        net = tf.nn.dropout(net, rate=0.1)\r\n        net = transformer_fc(net, HIDDEN_SIZE)\r\n        return net\r\n\r\n    def multi_headed_attention(q, k, v, bias):\r\n        def split_heads(net):\r\n            net = tf.reshape(\r\n                net,\r\n                [1, WORD_LEN, NUM_HEADS, D_K],\r\n            )\r\n            net = tf.transpose(net, [0, 2, 1, 3])\r\n            return net\r\n\r\n        def combine_heads(net):\r\n            net = tf.transpose(net, [0, 2, 1, 3])\r\n            net = tf.reshape(\r\n                net, [1, WORD_LEN, HIDDEN_SIZE]\r\n            )\r\n            return net\r\n\r\n        q = transformer_fc(q, HIDDEN_SIZE, use_bias=False)\r\n        k = transformer_fc(k, HIDDEN_SIZE, use_bias=False)\r\n        v = transformer_fc(v, HIDDEN_SIZE, use_bias=False)\r\n\r\n        q = split_heads(q)\r\n        k = split_heads(k)\r\n        v = split_heads(v)\r\n\r\n        q = tf.multiply(q, D_K_ROOT)\r\n        x = tf.matmul(q, k, transpose_b=True)\r\n        x = tf.add(x, bias)\r\n\r\n        x = tf.nn.softmax(x)\r\n        x = tf.nn.dropout(x, rate=0.1)\r\n        x = tf.matmul(x, v)\r\n\r\n        x = combine_heads(x)\r\n        x = transformer_fc(x, HIDDEN_SIZE, use_bias=False)\r\n        return x\r\n\r\n    def encoder_block(net, enc_bias):\r\n        residual = layer_normalization(net)\r\n        residual = multi_headed_attention(\r\n            residual, residual, residual, enc_bias\r\n        )\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        residual = layer_normalization(net)\r\n        residual = feed_forward(residual)\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        return layer_normalization(net)\r\n\r\n    def encode(enc_input, enc_bias):\r\n        net = tf.nn.dropout(enc_input, rate=0.1)\r\n        net = encoder_block(net, enc_bias) \r\n        return net\r\n\r\n    def decoder_block(net, enc_output, dec_bias, enc_dec_bias):\r\n        residual = layer_normalization(net)\r\n        residual = multi_headed_attention(\r\n            residual, residual, residual, dec_bias\r\n        )\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        residual = layer_normalization(net)\r\n        residual = multi_headed_attention(\r\n            residual, enc_output, enc_output, enc_dec_bias\r\n        )\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        residual = layer_normalization(net)\r\n        residual = feed_forward(residual)\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        return layer_normalization(net)\r\n\r\n    def decode(dec_input, enc_output, dec_bias, enc_dec_bias):\r\n        net = tf.nn.dropout(dec_input, rate=0.1)\r\n        net = decoder_block(net, enc_output, dec_bias, enc_dec_bias)\r\n        return net\r\n\r\n    labels = tf.stop_gradient(labels)\r\n\r\n    enc_input = features\r\n    dec_input = features\r\n    enc_bias = labels\r\n    enc_dec_bias = labels\r\n    dec_bias = labels\r\n    target = features\r\n\r\n    enc_output = encode(enc_input, enc_bias)\r\n    dec_output = decode(dec_input, enc_output, dec_bias, enc_dec_bias)\r\n\r\n    net = dec_output\r\n\r\n    loss = tf.reduce_mean(\r\n        tf.nn.softmax_cross_entropy_with_logits_v2(\r\n            labels=target,\r\n            logits=net,\r\n        )\r\n    )\r\n    train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\r\n    global_step = tf.train.get_or_create_global_step()\r\n\r\n    update_global_step = tf.assign(\r\n        global_step, global_step + 1, name=\"update_global_step\"\r\n    )\r\n    return tf.estimator.EstimatorSpec(\r\n        mode, loss=loss, train_op=tf.group(train_step, update_global_step)\r\n    )\r\n\r\n\r\ndef main(args):\r\n    hlo = get_hlo(transformer_model_fn, transformer_input_fn)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main(sys.argv))\r\n\r\n```", "I have the same warnings so i want to know if you have solved the warnings in your outputs: **(One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.** ", "@tangjie77wd Not sure, I don't know how to solve that warning", "> @sanjoy\r\n> I enabled resource variables by:\r\n> \r\n> * placing `tf.enable_resource_variables()` in the beginning of my `transformer_model_fn`\r\n> * placing `tf.enable_resource_variables()` right before I compile the XLA\r\n> * passing in `use_resource=True` for `tf.get_variable`\r\n> \r\n> I still get the same issue. The `TemporaryVariable` I believe is a part of the gradient calculation for `reduce_sum`, I need `reduce_sum` for this model. What I find an issue here is that `TemporaryVariable` doesn't appear (the issue is avoided) if I reduce the `HIDDEN_SIZE`, `FILTER_SIZE` and `NUM_HEADS` by a factor of 4.\r\n\r\nI see.\r\n\r\nUnfortunately I'm not familiar enough with the Python API side of things to help you here.  Maybe @alextp can help, or find someone else who can help?", "> @tangjie77wd Not sure, I don't know how to solve that warning\r\n\r\nThanks anyway.I am trying other solutions.", "@alextp\r\nThe fundamental issue here is that by changing the hidden size of the model we introduce a `TemporaryVariableOp` into the model which in turn causes XLA to fail because XLA doesn't support this op. Having the same fundamental network and just changing the hidden size of a model shouldn't introduce different types of operations into the network, I would expect this `TemporaryVariableOp` to be avoided at all costs to get the most XLA functionality.\r\n\r\nThis model is an incomplete `Transformer` model from the paper `Attention Is All You Need`, which is by no means a niche model.", "Maybe we should then extend the tf-xla bridge to support temporary variables with known shapes?", "That would also be a good solution", "No,i do not change the input size of the model since the size of my input image is the same with the requirement size of the model.", "> Maybe we should then extend the tf-xla bridge to support temporary variables with known shapes?\r\n\r\nI'm reverse engineering from the kernel implementation but isn't `TemporaryVariable` a ref variable?  If yes, I don't think we can support it in the tf-xla bridge (without a lot of work!).\r\n\r\nI think it is a ref variable because the kernel creates an instance of `TmpVar` and calls `context->set_output_ref(0, &tmp_var->mu, &tmp_var->val);`.", "It currently is a ref variable, but one could in principle make the same\nAPI work with resource variables.\n\nBut now I'm confused: TemporaryVariable is not a public symbol in the TF\nAPI; how did you come to have a graph with it? (and if you have a graph\nwhich depends on non-public APIs I think you're on your own)\n\nOn Tue, Jul 23, 2019 at 8:53 AM Sanjoy Das <notifications@github.com> wrote:\n\n> Maybe we should then extend the tf-xla bridge to support temporary\n> variables with known shapes?\n>\n> I'm reverse engineering from the kernel implementation but isn't\n> TemporaryVariable a ref variable? If yes, I don't think we can support it\n> in the tf-xla bridge (without a lot of work!).\n>\n> I think it is a ref variable because the kernel creates an instance of\n> TmpVar and calls context->set_output_ref(0, &tmp_var->mu, &tmp_var->val);.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30580?email_source=notifications&email_token=AAABHROH4RHGLTYD4X6WY7LQA4SPTA5CNFSM4H77IJK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2TRXGQ#issuecomment-514268058>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROO5IECHNCURC3RDG3QA4SPTANCNFSM4H77IJKQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp @sanjoy \r\n\r\nSo it seems we can try supporting TemporaryVariableOp with known sizes on XLA, but if that is not possible for any reason, is it still possible to try and avoid using TemporaryVariableOp when possible (i.e. for this model I am using changing the hidden size introduces the op)?\r\n", "Can you provide a minimal example of a model which has temporary variable?\n\nOn Thu, Jul 25, 2019 at 2:03 PM Elyas Mehtabuddin <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> @sanjoy <https://github.com/sanjoy>\n>\n> So it seems we can try supporting TemporaryVariableOp with known sizes on\n> XLA, but if that is not possible for any reason, is it still possible to\n> try and avoid using TemporaryVariableOp when possible (i.e. for this model\n> I am using changing the hidden size introduces the op)?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30580?email_source=notifications&email_token=AAABHRJ25NFADVTPL47JZOTQBIIKZA5CNFSM4H77IJK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22X6WA#issuecomment-515211096>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKVF3VRABM6T2EPM2LQBIIKZANCNFSM4H77IJKQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp \r\n\r\nWhen I posted this issue I wanted it to be resolved as quickly as possible, I spent as much time on my end actually trying to make the smallest example to reproduce the issue. The original model I had was the full Transformer model, I was able to strip it down to what it is right now and still reproduce the issue. However, if I remove any part of the model now the issue completely goes away (which is really annoying). Like if I remove the layer normalization layer or if I completely remove the encoder or decoder the issue is gone. I actually had a lot of trouble even getting it to reproduce to the script I have above, the original model was many many more lines and files.\r\n\r\nTLDR: The example above is actually the best minimal example I could come up with at the moment.", "Ok, looking at this more carefully it seems that the error is coming from the aggregation method accumulate_n in the optimizer.\r\n\r\nSo if you replace \r\n\r\n```\r\n    train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\r\n```\r\n\r\nwith\r\n\r\n\r\n```\r\n    train_step = tf.train.GradientDescentOptimizer(0.0001, aggregation_method=tf.AggregationMethod.ADD_N).minimize(loss)\r\n```\r\nthings should work.\r\n\r\nWhat I don't understand is why TF seems to be defaulting to accumulate_n in your case.", "@alextp \r\nSounds like good news! However I wasn't able to fix it on the TPU machine I am working on. Its on tf 1.14.0.\r\n\r\n```\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.0001, aggregation_method=tf.AggregationMethod.ADD_N).minimize(loss)\r\n\r\n```\r\ngives me an error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"issue.py\", line 220, in <module>\r\n    hlo = get_hlo(transformer_model_fn, transformer_input_fn)\r\n  File \"issue.py\", line 30, in get_hlo\r\n    loss, = compile(xla_model_fn, inputs=iterator.get_next())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 110, in compile\r\n    return _compile_internal(computation, inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 352, in _compile_internal\r\n    outputs = computation(*computation_inputs)\r\n  File \"issue.py\", line 18, in xla_model_fn\r\n    spec = model_fn(features, labels, mode=\"train\", params=None)\r\n  File \"issue.py\", line 210, in transformer_model_fn\r\n    train_step = tf.train.GradientDescentOptimizer(0.0001, aggregation_method=tf.AggregationMethod.ADD_N).minimize(loss)\r\nTypeError: __init__() got an unexpected keyword argument 'aggregation_method'\r\n```\r\n\r\nIt looks like `aggregation_method` is a parameter for `minimize` not the gradient descent constructor?\r\n\r\nI changed it to:\r\n```\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss, aggregation_method=tf.AggregationMethod.ADD_N)\r\n\r\n```\r\n\r\nDoing that gave me back my original issue I had.\r\nIs `aggregation_method` a new parameter for the gradient descent optimizer constructor in a later tf 1.14 / 2 version? Maybe I am doing something obviously wrong here?", "> I have the same warnings so i want to know if you have solved the warnings in your outputs: **(One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set. If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU. To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.**\r\n\r\nHas already been solved!", "aggregation_method has been an argument to minimize since ~forever\n\n\nI'm sorry, I don't understand what's your original issue here\n\nOn Wed, Jul 31, 2019 at 1:53 PM Elyas Mehtabuddin <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp>\n> Sounds like good news! However I wasn't able to fix it on the TPU machine\n> I am working on. Its on tf 1.14.0.\n>\n> train_step = tf.train.GradientDescentOptimizer(0.0001,\n> aggregation_method=tf.AggregationMethod.ADD_N).minimize(loss)\n>\n> gives me an error:\n>\n> Traceback (most recent call last):\n>   File \"issue.py\", line 220, in <module>\n>     hlo = get_hlo(transformer_model_fn, transformer_input_fn)\n>   File \"issue.py\", line 30, in get_hlo\n>     loss, = compile(xla_model_fn, inputs=iterator.get_next())\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 110, in compile\n>     return _compile_internal(computation, inputs)\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py\", line 352, in _compile_internal\n>     outputs = computation(*computation_inputs)\n>   File \"issue.py\", line 18, in xla_model_fn\n>     spec = model_fn(features, labels, mode=\"train\", params=None)\n>   File \"issue.py\", line 210, in transformer_model_fn\n>     train_step = tf.train.GradientDescentOptimizer(0.0001, aggregation_method=tf.AggregationMethod.ADD_N).minimize(loss)\n> TypeError: __init__() got an unexpected keyword argument 'aggregation_method'\n>\n> It looks like aggregation_method is a parameter for minimize not the\n> gradient descent constructor?\n>\n> I changed it to:\n> train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss,\n> aggregation_method=tf.AggregationMethod.ADD_N)\n>\n> Doing that gave me back my original issue I had.\n> Is aggregation_method a new parameter for the gradient descent optimizer\n> constructor in a later tf 1.14 / 2 version? Maybe I am doing something\n> obviously wrong here?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30580?email_source=notifications&email_token=AAABHRPHN4F7IOTS2TEMCYTQCH3UVA5CNFSM4H77IJK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3IQG5A#issuecomment-517014388>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLZR7IG4CHJJ26PGN3QCH3UVANCNFSM4H77IJKQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp \r\nSorry, what I meant to say was that adding `aggregation_method=tf.AggregationMethod.ADD_N` as a parameter to thee `minimize()` function did NOT resolve my issue.", "@elyasmehtabuddin this is very weird because nothing in the TF public API should be using accumulate_n (and indeed when I ran your code I couldn't reproduce the error) but the error is coming from a call to accumulate_n.\r\n\r\nCan you monkey-patch tf to suss out the problem? Something like\r\n\r\n```\r\nfrom tensorflow.python.ops import math_ops\r\nmath_ops.accumulate_n = lambda *a, **k: assert False\r\n```\r\n\r\non the top of your file should give a stack trace of the code trying to use accumulate_n.", "> @elyasmehtabuddin this is very weird because nothing in the TF public API should be using accumulate_n (and indeed when I ran your code I couldn't reproduce the error) but the error is coming from a call to accumulate_n.\r\n> \r\n> Can you monkey-patch tf to suss out the problem? Something like\r\n> \r\n> ```\r\n> from tensorflow.python.ops import math_ops\r\n> math_ops.accumulate_n = lambda *a, **k: assert False\r\n> ```\r\n> \r\n> on the top of your file should give a stack trace of the code trying to use accumulate_n.\r\n\r\nI wasn't able to hit that assertion, my code to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.compiler.xla import compile\r\nfrom tensorflow.python.compiler.xla import jit\r\n\r\nfrom tensorflow.python.ops import math_ops\r\ndef fail_me(*a, **k):\r\n    assert False\r\n    return 0\r\nmath_ops.accumulate_n = fail_me\r\n\r\n\r\nHIDDEN_SIZE = 2048\r\nFILTER_SIZE = 8196\r\nNUM_HEADS = 32\r\nD_K = HIDDEN_SIZE // NUM_HEADS\r\nD_K_ROOT = D_K ** (-0.5)\r\nWORD_LEN = 512 \r\n\r\n\r\ndef get_hlo(model_fn, input_fn):\r\n\r\n    def xla_model_fn(features, labels):\r\n        spec = model_fn(features, labels, mode=\"train\", params=None)\r\n        with jit.experimental_jit_scope():\r\n            with tf.control_dependencies([spec.train_op]):\r\n                return tf.identity(spec.loss, name=spec.loss.op.name)\r\n\r\n    train_ds = input_fn().repeat()\r\n\r\n    iterator = tf.data.Iterator.from_structure(\r\n        train_ds.output_types,\r\n        train_ds.output_shapes,\r\n    )\r\n    tf.enable_resource_variables()\r\n    loss, = compile(xla_model_fn, inputs=iterator.get_next())\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(iterator.make_initializer(train_ds))\r\n        sess.run(loss)\r\n\r\n\r\ndef transformer_input_fn():\r\n    feat_shape = [1, WORD_LEN, HIDDEN_SIZE]\r\n    labels_shape = [1, 1, 1, WORD_LEN]\r\n\r\n    model_inputs = [\r\n        tf.image.convert_image_dtype(\r\n            tf.reshape(\r\n                tf.constant(\r\n                    np.random.uniform(size=feat_shape)\r\n                ),\r\n                feat_shape,\r\n            ),\r\n            tf.float32,\r\n        ),\r\n    ]\r\n\r\n    model_expected = [\r\n        tf.image.convert_image_dtype(\r\n            tf.reshape(\r\n                tf.constant(np.random.uniform(size=labels_shape)),\r\n                labels_shape,\r\n            ),\r\n            tf.float32,\r\n        ),\r\n    ]\r\n\r\n    dataset = (model_inputs, model_expected)\r\n    dataset = tf.data.Dataset.from_tensor_slices(dataset)\r\n    return dataset\r\n\r\n\r\ndef transformer_model_fn(features, labels, mode, params):\r\n    tf.enable_resource_variables()\r\n    layer_norm_scale = tf.get_variable(\r\n        \"layer_norm_scale\",\r\n        [HIDDEN_SIZE],\r\n        initializer=tf.ones_initializer(),\r\n        use_resource=True,\r\n    )\r\n    layer_norm_bias = tf.get_variable(\r\n        \"layer_norm_bias\",\r\n        [HIDDEN_SIZE],\r\n        initializer=tf.zeros_initializer(),\r\n        use_resource=True,\r\n    )\r\n\r\n    def layer_normalization(net):\r\n        mean = tf.reduce_mean(net, axis=[-1], keepdims=True)\r\n        mean = tf.multiply(mean, -1)\r\n        diff = tf.add(net, mean)\r\n        var = tf.reduce_mean(tf.square(diff), axis=[-1], keepdims=True)\r\n        var = tf.add(var, 1e-6)\r\n        net = tf.multiply(diff, tf.rsqrt(var))\r\n        return tf.add(tf.multiply(net, layer_norm_scale), layer_norm_bias)\r\n\r\n    def transformer_fc(net, size, use_bias=True):\r\n        return tf.keras.layers.Dense(\r\n            size,\r\n            activation=None,\r\n            use_bias=use_bias,\r\n        )(net)\r\n\r\n    def feed_forward(net):\r\n        net = transformer_fc(net, FILTER_SIZE)\r\n        net = tf.nn.relu(net)\r\n        net = tf.nn.dropout(net, rate=0.1)\r\n        net = transformer_fc(net, HIDDEN_SIZE)\r\n        return net\r\n\r\n    def multi_headed_attention(q, k, v, bias):\r\n        def split_heads(net):\r\n            net = tf.reshape(\r\n                net,\r\n                [1, WORD_LEN, NUM_HEADS, D_K],\r\n            )\r\n            net = tf.transpose(net, [0, 2, 1, 3])\r\n            return net\r\n\r\n        def combine_heads(net):\r\n            net = tf.transpose(net, [0, 2, 1, 3])\r\n            net = tf.reshape(\r\n                net, [1, WORD_LEN, HIDDEN_SIZE]\r\n            )\r\n            return net\r\n\r\n        q = transformer_fc(q, HIDDEN_SIZE, use_bias=False)\r\n        k = transformer_fc(k, HIDDEN_SIZE, use_bias=False)\r\n        v = transformer_fc(v, HIDDEN_SIZE, use_bias=False)\r\n\r\n        q = split_heads(q)\r\n        k = split_heads(k)\r\n        v = split_heads(v)\r\n\r\n        q = tf.multiply(q, D_K_ROOT)\r\n        x = tf.matmul(q, k, transpose_b=True)\r\n        x = tf.add(x, bias)\r\n\r\n        x = tf.nn.softmax(x)\r\n        x = tf.nn.dropout(x, rate=0.1)\r\n        x = tf.matmul(x, v)\r\n\r\n        x = combine_heads(x)\r\n        x = transformer_fc(x, HIDDEN_SIZE, use_bias=False)\r\n        return x\r\n\r\n    def encoder_block(net, enc_bias):\r\n        residual = layer_normalization(net)\r\n        residual = multi_headed_attention(\r\n            residual, residual, residual, enc_bias\r\n        )\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        residual = layer_normalization(net)\r\n        residual = feed_forward(residual)\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        return layer_normalization(net)\r\n\r\n    def encode(enc_input, enc_bias):\r\n        net = tf.nn.dropout(enc_input, rate=0.1)\r\n        net = encoder_block(net, enc_bias) \r\n        return net\r\n\r\n    def decoder_block(net, enc_output, dec_bias, enc_dec_bias):\r\n        residual = layer_normalization(net)\r\n        residual = multi_headed_attention(\r\n            residual, residual, residual, dec_bias\r\n        )\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        residual = layer_normalization(net)\r\n        residual = multi_headed_attention(\r\n            residual, enc_output, enc_output, enc_dec_bias\r\n        )\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        residual = layer_normalization(net)\r\n        residual = feed_forward(residual)\r\n        residual = tf.nn.dropout(residual, rate=0.1)\r\n        net = tf.add(residual, net)\r\n\r\n        return layer_normalization(net)\r\n\r\n    def decode(dec_input, enc_output, dec_bias, enc_dec_bias):\r\n        net = tf.nn.dropout(dec_input, rate=0.1)\r\n        net = decoder_block(net, enc_output, dec_bias, enc_dec_bias)\r\n        return net\r\n\r\n    labels = tf.stop_gradient(labels)\r\n\r\n    enc_input = features\r\n    dec_input = features\r\n    enc_bias = labels\r\n    enc_dec_bias = labels\r\n    dec_bias = labels\r\n    target = features\r\n\r\n    enc_output = encode(enc_input, enc_bias)\r\n    dec_output = decode(dec_input, enc_output, dec_bias, enc_dec_bias)\r\n\r\n    net = dec_output\r\n\r\n    loss = tf.reduce_mean(\r\n        tf.nn.softmax_cross_entropy_with_logits_v2(\r\n            labels=target,\r\n            logits=net,\r\n        )\r\n    )\r\n    # train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\r\n    train_step = tf.train.GradientDescentOptimizer(\r\n        0.0001\r\n    ).minimize(loss, aggregation_method=tf.AggregationMethod.ADD_N)\r\n    global_step = tf.train.get_or_create_global_step()\r\n\r\n    update_global_step = tf.assign(\r\n        global_step, global_step + 1, name=\"update_global_step\"\r\n    )\r\n    return tf.estimator.EstimatorSpec(\r\n        mode, loss=loss, train_op=tf.group(train_step, update_global_step)\r\n    )\r\n\r\nhlo = get_hlo(transformer_model_fn, transformer_input_fn)\r\n```", "@ezhulenev do you think a graph rewrite pass could be adding the temporary variable here before the tfxla bridge sees the code? I'm very confused.\r\n\r\n@elyasmehtabuddin I'm specially confused because I run your code on colab and see no errors. Can you give me a colab to reproduce this on colab.research.google.com ?", "@alextp Sure thing!\r\n\r\nhttps://colab.research.google.com/drive/1sy0E7UHTngQ-mivhiOMb0v9xFB7WvaFE\r\n\r\n![Screen Shot 2019-08-05 at 1 41 49 PM](https://user-images.githubusercontent.com/44978436/62493786-d3e4d200-b786-11e9-94bf-85e642d62edf.png)\r\n", "1. XLA encapsulates subgraph into a function\r\n2. Grappler optimizes function library, and memory optimizer adds \"tmp_var\" node\r\n\r\nCurrently XLA-encapsulated functions are not marked with any attributes, and I guess there might be more potential problems in other optimizers. I think XLA should mark such functions with some attribute, so Grappler can skip them.", "@alextp Hey it appears I screwed up the permissions with the link for the google collab, this one should work:\r\n\r\nhttps://colab.research.google.com/drive/1sy0E7UHTngQ-mivhiOMb0v9xFB7WvaFE", "@sanjoy I think this boils down to a problem in the interaction between the tf-xla bridge and grappler, then.\r\n\r\nI think the workaround is to disable the grappler arithmetic optimizer here. @ezhulenev can you put instructions here?", "@alextp @sanjoy @ezhulenev Any updates on this?", "I think this should be fixed, but I'm not sure if it will be available in any of the v1 releases.", "@ezhulenev Thank you for pushing the commit, will test and see how it goes!", "Sorry guys I was on vacation.\r\n\r\nI compiled tf 1.13 from scratch and installed the whl, the issue was there.\r\nI then cherry-picked commit `471b73c238709fb796929eb412f1dab763b3f8cc` and added those changes manually to a new branch I created based off of 1.13.\r\nThen I compiled this custom whl and installed it, the issue went away.\r\n\r\nIssue is resolved! Thanks!!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30580\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30580\">No</a>\n"]}, {"number": 30579, "title": "Choosing device on NNAPI", "body": "I would like to call specifically into the NPU of a device. Is there a way to call into NNAPI from the application level that will allow such behavior?\r\n\r\n\r\n", "comments": ["@abrarmatin,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!"]}, {"number": 30578, "title": "[ROCm] Adding ROCm support for the bias op", "body": "This PR adds ROCm support for the bias op\r\n\r\nMost of the changes in this PR are trivial, save one. \r\n\r\nThe `BiasGradNCHW_SharedAtomics` GPU kernel has a ROCm specific implementation, because the CUDA version of the same does not seem to work correctly on ROCm (even though it appears to be functionally the same)\r\n\r\nplease review and merge. thanks.\r\n\r\n----------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg \r\n\r\n", "comments": []}, {"number": 30577, "title": "TensorflowLiteSwift fails pod validation", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution: MacOS 10.14.5\r\n- TensorflowLiteSwift Pod version: 0.2.0\r\n- Commit: 477447155b\r\n- Cocoapods version: 1.7.3\r\n- Xcode version: 10.2.1\r\n\r\n**Describe the problem**\r\n\r\nWhen running the command `pod spec lint`, tests fail because `i386` architecture cannot be found. This happens not only on my machine, but also on our private pod system. Failing validation means we cannot use the pod for our apps without turning off the tests. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. cd to TensorflowLiteSwift directory\r\n2. run `pod spec lint --verbose` \r\n\r\n**Any other info / logs**\r\n\r\n[here's the output](https://github.com/tensorflow/tensorflow/files/3378927/log.txt)\r\n\r\nI was able to resolve this issue by upping the the iOS deployment target in the podspec from 9.0 to 11.0. I believe `pod spec lint` wants to compile a 32 bit version of TensorFlowLiteC for iOS 9 and 10, but the TensorFlowLiteC binary only contains `x86_64` and no `i386`. A more robust solution might be to compile for `i386`. Or it may be possible to configure the podspec to only execute tests on arm and `x86_64`. ", "comments": ["Is this still a problem for you? When I run `pod spec lint --verbose --allow-warnings TensorFlowLiteSwift.podspec` in the `tensorflow/lite/experimental/swift` directory, it succeeds on my side.\r\n\r\nA few more things to note.\r\n* The i386 target is now included in the `TensorFlowLiteC` nightly binaries (as of 29e6eafad33725f3c1b8f0dae54c702df804e03d).\r\n* The `--allow-warnings` flag will be needed due to a swift version related warning.\r\n* If the missing `i386` still causing you a problem, you could also use `--skip-import-validation` to pass the lint, although it's not ideal.\r\n\r\nPlease let me know.", "We turned off the tests for this pod as a short-term workaround. Also, since we only support the last 2 versions of iOS, since iOS 13 is out we'll drop support for iOS 10 which should also fix the problem. ", "Sounds good. Let me close this bug for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30577\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30577\">No</a>\n"]}]