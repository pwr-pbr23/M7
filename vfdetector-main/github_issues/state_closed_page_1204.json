[{"number": 17055, "title": "Merge branch 1.6 back to master", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "We do want some of the changes, for example saver.py revert breaks checkpoint loading code, and it does not seem to be in master.\r\n\r\nAnyway, how did these branches diverge this much? I did not even touch most of these files myself, I just merged the branch.", "OK, retrying, PTAL."]}, {"number": 17054, "title": "Branch 185878562", "body": "", "comments": []}, {"number": 17053, "title": "[Feature Request] Support grayscale images for classifier on Android", "body": "How can one modify ClassifierActivity.java (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java) to convert images obtained from the camera to grayscale and then pass them to the inferenceInterface?\r\n\r\n**Have I written custom code:** I want to use the ClassifierActivity with another neural network that was trained on grayscale images\r\n**OS Platform and Distribution:** N/A\r\n**TensorFlow installed from:** pip\r\n**TensorFlow version:** 1.2.0\r\n**Python version:** 3.6\r\n**Bazel version (if compiling from source):** N/A\r\n**GCC/Compiler version (if compiling from source):** N/A\r\n**CUDA/cuDNN version:** N/A\r\n**GPU model and memory:** N/A\r\n**Exact command to reproduce:** N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17052, "title": "Updated roadmap", "body": "", "comments": []}, {"number": 17051, "title": "Indentation fix", "body": "", "comments": []}, {"number": 17050, "title": "Distributed FIFOQueue with shared_name is not shared", "body": "### System information\r\n- **Environment**:\r\nShared Cluster\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRHEL Server 7.2\r\n- **TensorFlow installed from (source or binary)**:\r\npip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**:\r\nv1.5.0-0-g37aa430d84 1.5.0\r\n- **Python version**:\r\n3.6\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0\r\n- **GPU model and memory**:\r\nN/A -- GPU not allocated\r\n\r\nI am attempting to use a `FIFOQueue` to signal the parameter servers to shut down on a multi-machine shared cluster, based on [this](https://stackoverflow.com/questions/39810356/shut-down-server-in-tensorflow/40186129#40186129) [example](https://gist.github.com/yaroslavvb/ea1b1bae0a75c4aae593df7eca72d9ca). After some testing, I believe that `shared_name` simply doesn't seem to do anything--even after removing the `dequeue()` operations, the number of elements in the `FIFOQueue` don't correlate to the number of workers.\r\n\r\nMinimum Reproducible Code\r\n```\r\n# for example\r\ncluster = tf.train.ClusterSpec({\r\n    'ps': ['192.168.1.1:36598'],\r\n    'worker': ['192.168.1.2:40596', '192.168.1.3:47324', '192.168.1.4:38923']\r\n})\r\n# ... #\r\nserver = tf.train.Server(cluster, job_name=job_name, task_index=task)\r\n\r\n# using server.join() causes cluster management headaches\r\n# use a FIFOQueue to tell the parameter server to shutdown\r\nif job_name == 'ps':\r\n    with tf.device('/job:ps/task:%d' % task):\r\n        queue = tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue')\r\n    with tf.Session(server.target) as sess:\r\n        sess.run(queue.dequeue())\r\n        print('ps %d: quitting' % task)\r\n\r\n# MonitoredTrainingSession with FinalOpsHook not shown\r\nelif job_name == 'worker':\r\n    with tf.device('/job:worker/task:%d' % task):\r\n        with tf.name_scope('done_queue'):\r\n            queue = tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue')\r\n    with tf.Session(server.target) as sess:\r\n        _, size = sess.run([queue.enqueue(1), queue.size()])\r\n        print('Worker:%d sending done to ps:%d [elements=%d]' % (task, 0, size))\r\n```", "comments": ["Whew. How silly.\r\nThe problem is that the queues are all assigned to different devices--one on `/job:ps` and the others on their individual `/job:worker` tasks.\r\nI've produced a minimal working example:\r\n```\r\nimport tensorflow as tf\r\nimport threading\r\n\r\ndef main(job_name, task):\r\n    cluster = tf.train.ClusterSpec({\r\n        'ps': ['localhost:22222', 'localhost:22223'],\r\n        'worker': ['localhost: 22224','localhost: 22225','localhost: 22226']\r\n    })\r\n\r\n    # Create and start a server for the local task\r\n    server = tf.train.Server(cluster, job_name=job_name, task_index=task)\r\n\r\n    if job_name == 'ps':\r\n        with tf.device('/job:ps/task:%d' % task):\r\n            queue = tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue%d' % task)\r\n        with tf.Session(server.target) as sess:\r\n            for i in range(cluster.num_tasks('worker')):\r\n                sess.run(queue.dequeue())\r\n                print('ps:%d received done from worker:%d' % (task, i))\r\n            print('ps:%d quitting' % task)\r\n    elif job_name == 'worker':\r\n        # queue needs to be visible to /job:ps\r\n        queues = []\r\n        for i in range(cluster.num_tasks('ps')):\r\n            with tf.device('/job:ps/task:%d' % i):\r\n                queues.append(tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue%d' % i))\r\n        with tf.Session(server.target) as sess:\r\n            for i in range(cluster.num_tasks('ps')):\r\n                _, size = sess.run([queues[i].enqueue(task), queues[i].size()])\r\n                print('Worker:%d sending done to ps:%d [elements=%d]' % (task, i, size))\r\n\r\nif __name__ == '__main__':\r\n    threads = [\r\n        threading.Thread(target=main, args=('ps', 0)),\r\n        threading.Thread(target=main, args=('ps', 1)),\r\n        threading.Thread(target=main, args=('worker', 0)),\r\n        threading.Thread(target=main, args=('worker', 1)),\r\n        threading.Thread(target=main, args=('worker', 2))]\r\n    for thread in threads:\r\n        thread.start()\r\n    for thread in threads:\r\n        thread.join()\r\n```\r\nIt's simple to change to work with MonitoredTrainingSession using FinalOpsHook, although it certainly isn't the prettiest way to go about doing things.", "@illeatmyhat (excellent username, BTW) have you resolved your own bug, or is there a remaining issue that you need help with? ", "Yes, the issue is resolved. It can be closed now.\r\nThanks"]}, {"number": 17049, "title": "Fix MKL build break on Windows", "body": "Eigen with MKL and MKL-ML only, no MKL-DNN.\r\n\r\nWindows 10 1709\r\n\r\nMicrosoft Visual Studio 2017 15.4.0\r\n\r\nIntel Parallel Studio XE Cluster Edition for Windows 2018 Update 1\r\n\r\n    98% tests passed, 7 tests failed out of 387\r\n    \r\n    Total Test time (real) = 4031.00 sec\r\n    \r\n    The following tests FAILED:\r\n             40 - C:/Users/User/Source/Repos/tensorflow/tensorflow/python/debug/lib/session_debug_file_test.py (Failed)\r\n             42 - C:/Users/User/Source/Repos/tensorflow/tensorflow/python/debug/lib/stepper_test.py (Failed)\r\n             84 - C:/Users/User/Source/Repos/tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py (OTHER_FAULT)\r\n            165 - C:/Users/User/Source/Repos/tensorflow/tensorflow/python/kernel_tests/matrix_solve_ls_op_test.py (Failed)\r\n            182 - C:/Users/User/Source/Repos/tensorflow/tensorflow/python/kernel_tests/pooling_ops_test.py (OTHER_FAULT)\r\n            328 - C:/Users/User/Source/Repos/tensorflow/tensorflow/contrib/factorization/python/ops/gmm_ops_test.py (Failed)\r\n            353 - C:/Users/User/Source/Repos/tensorflow/tensorflow/python/keras/_impl/keras/layers/convolutional_recurrent_test.py (Failed)\r\n\r\n\r\nfull build log and test log attached\r\n\r\n[msbuild.zip](https://github.com/tensorflow/tensorflow/files/1728463/msbuild.zip)\r\n[pytest.zip](https://github.com/tensorflow/tensorflow/files/1728466/pytest.zip)\r\n\r\n", "comments": []}, {"number": 17048, "title": "Tensorflow or cuda not giving back gpu memory after session closes", "body": "I am tying to install tensorflow correctly and I am getting memory allocation erros.\r\nI am using:\r\n\r\nUbuntu 16.04\r\ntf = 1.5.0 from pip install tensorflow-gpu\r\nCUDA 9.0\r\nCUDNN 7.0.5\r\n\r\nstarting python in a command terminal and running the following commands:\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nsess.close()\r\n\r\nIf I start a session it is fine the first time it says total memory: 7.72Gib free Memory: 7.50GiB\r\n\r\nThe next time in the same terminal I start python again ti says freeMemory: 279.44MiB\r\n\r\nand finally if I start again it says:\r\n\r\nfreeMemory 122.50MiB\r\n\r\nfailed to allocate 72.50M from device: CUDA_ERROR_OUT_OF_MEMORY\r\n\r\nWhat can I do to fix this? \r\n\r\nI have pasted the entire sequence below:\r\n\r\nteves@teves:~$ python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n2018-02-15 11:06:55.708721: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-02-15 11:06:55.846202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-15 11:06:55.846656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.50GiB\r\n2018-02-15 11:06:55.846685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n>>> sess.close()\r\n>>> \r\n[1]+  Stopped                 python\r\nteves@teves:~$ python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n2018-02-15 11:07:34.144528: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-02-15 11:07:34.351426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-15 11:07:34.351699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 279.44MiB\r\n2018-02-15 11:07:34.351732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n>>> sess.close()\r\n>>> \r\n[2]+  Stopped                 python\r\nteves@teves:~$ python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n2018-02-15 11:08:43.527818: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-02-15 11:08:43.877301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-15 11:08:43.877577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 122.50MiB\r\n2018-02-15 11:08:43.877610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-02-15 11:08:44.047980: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 72.50M (76021760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n>>> tf.__version__\r\n'1.5.0'\r\n>>> \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: pip install tensorflow-gpu\r\nTensorFlow version 1.5.0\r\nBazel version: 0.9.0\r\nCUDA/cuDNN version: cuda 9.0 cudnn 7.0.5\r\nGPU model and memory: gtx 1070 8gb\r\nExact command to reproduce\r\nopen pthon \r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nsess.close()\r\nclose python\r\nrepeat three times. get out of memory error\r\n\r\nI think it has something to do with the nvidia driver. I went down to the 390.12 version from 390.3 and the problem got better. I am able to close the terminal and open a new one now and i do not get the issue. However if I dont close the terminal but close the python session and start a new one I still get the error.", "By default TensorFlow allocates GPU memory for the lifetime of the process, not the lifetime of the session object. More details at: https://www.tensorflow.org/programmers_guide/using_gpu#allowing_gpu_memory_growth \r\n\r\nThus, if you want memory to be freed, you'll have to exit the Python interpreter, not just close the session.\r\n\r\nHope that helps.", "I'm currently building out a workflow using Celery to perform a set of image analytics in parallel. I ran into this issues since the celery daemon, once started, never released GPU resources after it completed processing images that used TensorFlow. As a result, I quickly depleted all of my GPU resources. My interim solution to the problem was to wrap my analytic in a Pool that spawns a single Process. It's not pretty, but it at least solves the problem in releasing GPU resources if you have a long running process like a celery daemon.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom multiprocessing import Pool\r\n\r\n\r\ndef _process(image):\r\n    sess = tf.Session()\r\n    sess.close()\r\n\r\n   \r\ndef process_image(image):\r\n    with Pool(1) as p:\r\n        return p.apply(_process, (image,))\r\n```\r\n\r\n", "The problem is that even if I exit python in the terminal (ctl -Z) and\nrestart another python process the memory is still occupied. I have to\nclose the terminal for the memory to release.\n\nOn Fri, Feb 23, 2018 at 12:41 PM, Nathan Douglas <notifications@github.com>\nwrote:\n\n> I'm currently building out a workflow using Celery to perform a set of\n> image analytics in parallel. I ran into this issues since the celery\n> daemon, once started, never released GPU resources after it completed\n> processing images that used TensorFlow. As a result, I quickly depleted all\n> of my GPU resources. My interim solution to the problem, was to wrap my\n> analytic in a Pool that spawns a single Process. It's not pretty, but it at\n> least solves the problem in releasing GPU resources if you have a long\n> running process like a celery daemon.\n>\n> import tensorflow as tf\n> from multiprocessing import Pool\n>\n> def _process(image):\n>     sess = tf.Session()\n>     sess.close()\n>\n>    def process_image(image):\n>     with Pool(1) as p:\n>         return p.apply(_process, (image,))\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-368082470>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsMvwR_3Po8S7ANCWN0MJNkZIOfGm9wks5tXvhGgaJpZM4SHHWg>\n> .\n>\n", "When you're \"Ctrl+Z\"ing, you're not killing the python process, you're just putting it into the background. So it doesn't release the memory. You want to exit the python interpreter to kill the process.", "Oh I see. Just to clarify in case you weren't aware, Ctrl-Z will put the python process in the background which isn't the same as doing Ctrl-D from the python terminal. If you open 2 terminals and in one run `watch nvidia-smi` and in the other you run your python terminal to start a tensorflow session, you'll see your python process allocate space and then drop off with Ctrl-D, where as the Ctrl-Z command won't release those resources.", "Thanks Nathan. Yes with ctrl-D it works properly.\n\nOn Fri, Feb 23, 2018 at 5:43 PM, Nathan Douglas <notifications@github.com>\nwrote:\n\n> Oh I see. Just to clarify in case you weren't aware, Ctrl-Z will put the\n> python process in the background which isn't the same as doing Ctrl-D from\n> the python terminal. If you open 2 terminals and in one run watch\n> nvidia-smi and in the other you run your python terminal to start a\n> tensorflow session, you'll see your python process allocate space and then\n> drop off with Ctrl-D, where as the Ctrl-Z command won't release those\n> resources.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-368159242>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsMv3ZK9ZS3jEyWbTIF5TwnAFQ3i_tPks5tXz8CgaJpZM4SHHWg>\n> .\n>\n", "Closing this as things are working as intended.\r\n", "I'm having a similar issue. I want to run indefinite training cycles back to back, because I'm using an evolutionary algorithm to optimize the hyperparameters. However, over time, the GPU \"waits\" for greater and greater times between training cycles (afterburner visualization):\r\n\r\n![image](https://user-images.githubusercontent.com/37384864/37435942-dffbfd7c-27bb-11e8-8e83-7b6b2c0dbc3e.png)\r\n\r\nI assume this is a memory issue in a similar vein. Any ideas of a workaround? I found an attempt at a solution using Keras to reset GPU memory after each run, and have tried this, but it has not solved the problem (it may have somewhat shortened the lag between training runs, but not enough to constitute a fix). [Keras \"fix\"](http://forums.fast.ai/t/tip-clear-tensorflow-gpu-memory/1979)\r\nWhat do you suppose is going on here? I.e. what may be the source of the problem, and resources that could help me address it?\r\n\r\nThanks!", "I use [numba ](http://numba.pydata.org/numba-doc/0.13/CUDADevice.html)to release the gpu. With tensorflow I can not find a effect method.", "```\r\nimport tensorflow as tf\r\nfrom numba import cuda\r\n\r\na = tf.constant([1.0,2.0,3.0],shape=[3],name='a')\r\nb = tf.constant([1.0,2.0,3.0],shape=[3],name='b')\r\nwith tf.device('/gpu:1'):\r\n    c = a+b\r\n\r\nTF_CONFIG = tf.ConfigProto(\r\ngpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.1),\r\n  allow_soft_placement=True)\r\n\r\nsess = tf.Session(config=TF_CONFIG)\r\nsess.run(tf.global_variables_initializer())\r\ni=1\r\nwhile(i<1000):\r\n        i=i+1\r\n        print(sess.run(c))\r\n\r\nsess.close()\r\ncuda.select_device(1)\r\ncuda.close()\r\nwith tf.device('/gpu:1'):\r\n    c = a+b\r\n\r\nTF_CONFIG = tf.ConfigProto(\r\ngpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5),\r\n  allow_soft_placement=True)\r\n\r\nsess = tf.Session(config=TF_CONFIG)\r\n\r\nsess.run(tf.global_variables_initializer())\r\nwhile(1):\r\n        print(sess.run(c))\r\n```", "@androidbeepboop : What you're alluding to seems different from the issue the original author of this issue had. Could you please file a separate issue with all details (and ideally a minimal, complete, verifiable example)?", "@TanLingxiao were you able to find any other method? numba is a great way with the drawback being that once you run cuda.close(), you can no longer run your process again in the same process/session. As alluded above and in other threads, Was hoping that tensorflow has config option to free GPU Memory after the processing ends.", "This solution can be use: [`https://stackoverflow.com/questions/15197286/how-can-i-flush-gpu-memory-using-cuda-physical-reset-is-unavailable`]\r\nsudo fuser -v /dev/nvidia* \r\nYour output will look something like this:\r\n ```\r\n                     USER        PID  ACCESS COMMAND\r\n/dev/nvidia0:        root      1256  F...m  Xorg\r\n                     username   2057  F...m  compiz\r\n                     username   2759  F...m  chrome\r\n                     username   2777  F...m  chrome\r\n                     username   20450 F...m  python\r\n                     username   20699 F...m  python\r\n\r\n```\r\nsudo kill -9 PID\r\n", "I've elaborated on the trick used by @nathandouglas to create a decorator that will transparently launch your tf training/prediction function in a subprocess. It also takes care of hiding CUDA devices from TF so that it only uses the desired number of devices (you can easily remove that functionality if you don't need it). You'll need to wrap any code that creates a session to ensure that your GPU memory is released as it seems that TF carries out all the CUDA initialisation and device memory allocation when the first session is created. For example:\r\n```\r\n@RunAsCUDASubprocess(num_gpus=1)\r\ndef train_model():\r\n  ...\r\n\r\ntrain_model()\r\n```\r\n\r\nhttps://gist.github.com/ed-alertedh/85dc3a70d3972e742ca0c4296de7bf00\r\n", "> I've elaborated on the trick used by @nathandouglas to create a decorator that will transparently launch your tf training/prediction function in a subprocess. It also takes care of hiding CUDA devices from TF so that it only uses the desired number of devices (you can easily remove that functionality if you don't need it). You'll need to wrap any code that creates a session to ensure that your GPU memory is released as it seems that TF carries out all the CUDA initialisation and device memory allocation when the first session is created. For example:\r\n> \r\n> ```\r\n> @RunAsCUDASubprocess(num_gpus=1)\r\n> def train_model():\r\n>   ...\r\n> \r\n> train_model()\r\n> ```\r\n> https://gist.github.com/ed-alertedh/85dc3a70d3972e742ca0c4296de7bf00\r\n\r\nThis solution is giving me this error:\r\nTypeError: can't pickle SwigPyObject objects\r\nany clue?", "@calannap due to the way the `multiprocessing` module works you need to make sure that the arguments passed into your wrapped function can be pickled and unpickled. As a workaround try creating the swig objects inside your function (if possible).", "@asimshankar I think there needs to be a way to get around this in the python instance. e.g. if I want to make a python script which consecutively runs several models with different params.\r\n\r\nI have tried manually calling garbage collection but still resource usage increases:\r\n\r\n\r\n```python\r\nimport gc, resource\r\nfrom .my.module import run_model\r\n\r\ndef wrapper_run_model(params)\r\n   run_model(params)\r\n   print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\r\n   gc.collect() \r\n\r\n\r\n# my/module\r\ndef run_model(params):\r\n    with sess as tf.Session():\r\n        # do stuff\r\n        # sess.close() <--- shouldn't need to add this as called when exiting with statement, \r\n        # but tried just in case it helps\r\n```\r\n\r\n\r\n\r\nIf I can not free resource from python, then how should I launch a series of experiments?\r\n\r\n@nathandouglas  I also tried applying your solution above, but I still see resources increasing until it is all allocated and throws OSError", "> The problem is that even if I exit python in the terminal (ctl -Z) and restart another python process the memory is still occupied. I have to close the terminal for the memory to release.\r\n> [\u2026](#)\r\n> On Fri, Feb 23, 2018 at 12:41 PM, Nathan Douglas ***@***.***> wrote: I'm currently building out a workflow using Celery to perform a set of image analytics in parallel. I ran into this issues since the celery daemon, once started, never released GPU resources after it completed processing images that used TensorFlow. As a result, I quickly depleted all of my GPU resources. My interim solution to the problem, was to wrap my analytic in a Pool that spawns a single Process. It's not pretty, but it at least solves the problem in releasing GPU resources if you have a long running process like a celery daemon. import tensorflow as tf from multiprocessing import Pool def _process(image): sess = tf.Session() sess.close() def process_image(image): with Pool(1) as p: return p.apply(_process, (image,)) \u2014 You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub <[#17048 (comment)](https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-368082470)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AJsMvwR_3Po8S7ANCWN0MJNkZIOfGm9wks5tXvhGgaJpZM4SHHWg> .\r\n\r\nFor me Ctrl+Shift + C kills the python process and frees the gpus", "Using numba causes Segmentation fault:\r\nhttps://stackoverflow.com/questions/58792739/tensorflow-model-wrapper-that-can-release-gpu-resources\r\n\r\nThis example also fails https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-413082016\r\n\r\n```\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\n1.14.0\r\n```\r\n\r\n```\r\n2019-11-11 17:25:57.239503: E tensorflow/stream_executor/cuda/cuda_driver.cc:674] failed to memset memory: CUDA_ERROR_INVALID_VALUE: invalid argument\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to memcopy into scratch buffer for device 0\r\n\t [[{{node _SOURCE}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test_numba.py\", line 32, in <module>\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to memcopy into scratch buffer for device 0\r\n\t [[{{node _SOURCE}}]]\r\n```", "try tf.keras.backend.clear_session() as it will del the model that has been occupying memory.\r\nnot explicitly flush memory till empty but at least reducing model in memory.\r\n\"If you are creating many models in a loop, this global state will consume an increasing amount of memory over time, and you may want to clear it. Calling clear_session() releases the global state: this helps avoid clutter from old models and layers, especially when memory is limited.\""]}, {"number": 17047, "title": "Failing assertion when building with MKL and using Xception", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nv1.6.0-rc0-19-gecec1d8\r\n- **Python version**: \r\nPython 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\nBazel 0.8.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\n1. Get the Dockerfile from here: https://gist.github.com/abiro/c155a8107879d9c4e0368f3c3e94ea81\r\n2. Build with the MKL library: `docker build -t tf_mkl -f Dockerfile.devel-cpu-mkl .`\r\n3. Build without the MKL library: `docker build -t tf_nomkl -f Dockerfile.devel-cpu-mkl --build-arg MKL_FLAG=0 .`\r\n3. Run image built with the MKL library: `docker run tf_mkl`\r\nResult: error (see output below)\r\n4. Run image built without the MKL library: `docker run tf_nomkl`\r\nResult: no error\r\n\r\n### Describe the problem\r\nWhen building TensorFlow 1.6 with the MKL library, inference with the Xception model results in a failing assertion and the program exits. See the error message and the code below please. The Dockerfile linked above contains the source code and can be used to easily reproduce the problem.\r\n\r\n### Source code / logs\r\n#### Source code\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.keras.applications.xception.Xception(weights=\"xception_weights.h5\")\r\nx.predict(np.zeros((1, 299, 299, 3)), batch_size=1)\r\n```\r\n\r\n#### Logs\r\nStep 4 from above outputs the following:\r\n>2018-02-15 15:04:26.610461: F tensorflow/core/kernels/mkl_input_conversion_op.cc:448] Check failed: tf_input.CheckReorderToOpMem( memory::primitive_desc(output_mkl_md, cpu_engine), tensor_out, &net) == true (0 vs. 1)\r\nAborted (core dumped)\r\n\r\n#### Misc\r\nThe host machine used to build and run the Docker containers was an EC2 c4.2xlarge instance with an Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz processor.\r\n\r\nRelated to #16982", "comments": ["@gunan can you please take a look?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@wei-v-wang : Is this related to the investigation in #16982 ?\r\n(CC @tatianashp )", "@asimshankar Sorry for the delayed answer.  It could be. I am working with my colleague to work on a fix for these. Hopefully the turnaround time will be better in short future. ", "@abiro  @asimshankar  @bignamehyp @gunan \r\n\r\nFYI: please try the latest commit e.g. https://github.com/tensorflow/tensorflow/commit/a175841eb549f069ac205fb32bf55314a387fe6d\r\n\r\nI tested a private version similar to the above commit and did not encounter the issue anymore (but was indeed seeing this with some old public commits, sorry for the inconveniences). \r\n\r\n investigate_17047]$ python p.py  \r\n/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-04-27 18:43:23.264129: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n----\r\ni.e. no error. \r\n\r\n@abiro could you please confirm? Thank you for reporting! \r\n", "Thanks for looking into this!\r\n\r\nRunning the code from the bug report in a Docker image built with MKL from commit eeab2c8 results in the following output:\r\n\r\n```\r\n2018-05-08 15:23:04.589899: E tensorflow/core/framework/op_kernel.cc:1242] \r\nOpKernel ('op: \"_MklConv2DWithBiasBackpropBias\" device_type: \"CPU\" \r\nconstraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } label: \"MklOp\"') for \r\nunknown op: _MklConv2DWithBiasBackpropBias\r\n\r\n2018-05-08 15:23:06.760291: W tensorflow/core/util/tensor_slice_reader.cc:95] \r\nCould not open ./xception_weights.h5: Data loss: not an sstable (bad magic number): \r\nperhaps your file is in a different file format and you need to use a different restore operator?\r\n```\r\n\r\nThe checksum of the weights file matches the checksum in the [source file.](https://github.com/tensorflow/tensorflow/blob/eeab2c867faa0f10dfea8635d1e87009844f902e/tensorflow/python/keras/_impl/keras/applications/xception.py#L318)\r\n\r\nThe second message is reproducible when building the image from the bug report without MKL (this was built from commit 6ba9573). It is also reproducible when running the code in the `tensorflow/tensorflow:nightly-devel-py3` image that was built from commit 714f3c4, so I think it's safe to say that the second message is not related to MKL. I'm not sure however how to interpret the first message.\r\n\r\nSorry about the different commits, the oldest one is from the 4th of May.", "@abiro Thank you very much for following up and trying the latest commits and for reporting out the results. \r\nAs we know TensorFlow has a large amount of commits each day. May I suggest using the tagged release versions, e.g. v1.7.0 tag and v1.8.0 tag. I have tested these tf commits, and both did not have the issues reported in this thread. The above \"MklConv2DWithBiasBackpropBias\" seems to be a new issue with latest code - we will investigate. But to unblock your work, may I suggest you try the **v180 (https://github.com/tensorflow/tensorflow/commit/93bc2e2072e0daccbcff7a90d397b704a9e8f778)** or **v170 (https://github.com/tensorflow/tensorflow/commit/024aecf414941e11eb643e29ceed3e1c47a115ad)**? \r\nPlease see below for my results: \r\n\r\n[ investigate_17047]$ python p.py   \r\n/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-05-08 13:07:20.992102: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2018-05-08 13:07:21.012545: I tensorflow/core/common_runtime/process_util.cc:63] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n[investigate_17047]$ cat p.py  \r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.keras.applications.xception.Xception(weights=\"xception_weights.h5\")\r\nx.predict(np.zeros((1, 299, 299, 3)), batch_size=1)\r\n\r\n[investigate_17047]$ ls xception_weights.h5  -hl\r\n-rw-r--r-- 1 weiwang weiwang 88M May 24  2017 xception_weights.h5 \r\n\r\n\r\nOr TF v170:\r\n\r\n investigate_17047]$ export PYTHONPATH=~/shared_big/TF_Public_v170                                                                                                                                                                               \r\n[investigate_17047]$ python p.py                                                                                                                                                                                                                 \r\n/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-05-08 13:08:58.230349: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n", "Thank you @wei-v-wang! I consider the issue resolved."]}, {"number": 17046, "title": "Problem compiling on mac os x TF 1.6", "body": "Hello I am on Mac Os X\r\n\r\nDarwin fcamacbook.dyndns.cern.ch 17.4.0 Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64 x86_64\r\n\r\nand I have the latest gcc\r\n\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 9.0.0 (clang-900.0.39.2)\r\nTarget: x86_64-apple-darwin17.4.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\nFound CUDA installation: /usr/local/cuda, version 8.0\r\n\r\nI have python 3.6.4 from homebrew\r\n\r\nWhen I try to compile the master of TF from github with mkl support and -march=native I have\r\n\r\n\r\nERROR: /usr/local/tensorflow/tensorflow/core/BUILD:1574:1: C++ compilation of rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal' failed (Exit 1)\r\nclang: error: unsupported option '-fopenmp'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\nThanks for help. \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks for your message. I am sorry I was not clearer, but I thought all the info you are asking was in my posting:\r\n\r\nOS platform and distribution:\r\n-----------------------------\r\nDarwin fcamacbook.dyndns.cern.ch 17.4.0 Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64 x86_64\r\n\r\naka \r\n\r\nmacOS High Sierra version 10.13.3 (17D47)\r\n\r\nTensorFlow installed from & Version\r\n------------------------------------\r\n\r\nmaster from github\r\n\r\nbazel version\r\n-------------\r\n\r\nBuild label: 0.10.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jan 10 02:02:06 +50057 (1517480013726)\r\nBuild timestamp: 1517480013726\r\nBuild timestamp as int: 1517480013726\r\n\r\nCUDA/cuDNN version & GPU model and memory\r\n-------------------------------------------------\r\n\r\nI am not using CUDA / GPU\r\n\r\nExact command to reproduce\r\n------------------------------\r\n\r\nConfigure TF with mkl support & \r\n\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n", "As the message said you were using the clang from Xcode, which doesn\u2019t support OpenMP. What you should do is either disabling OpenMP or using clang or gcc with OpenMP ", "Hello,\r\n  thanks for your answer. In fact support is there but a bit complex to get at. See this interesting article\r\n\r\nhttps://iscinumpy.gitlab.io/post/omp-on-high-sierra/\r\n\r\nOtherwise, any suggestion on how to tell bazel to change compiler? But then, should I compile the whole of python with the new compiler? ", "Reply to myself... telling bazel to use a different complier seems to be a project in itself\r\n\r\nhttps://github.com/bazelbuild/bazel/wiki/About-the-CROSSTOOL\r\n\r\n", "change `$PATH` and/or `$CC` could help. E.g., the gcc I used is `/opt/local/bin/gcc-mp-6`, so I add `/opt/local/bin` to my `$PATH` and do something like\r\n`\r\nCC=gcc-mp-6 bazel build --config .....\r\n`", "MKL support for macos is still not available.\r\nI am blocked on a few bazel bugs.\r\nPlease watch #10685 for updates.", "Please let me know if this is the wrong place, but I'm also trying to compile TF on macOS and having trouble with changing the compiler (trying to use MKL, but also just to compile with openMP and native arch extensions). Setting CC and CXX doesn't work, because the CROSSTOOL in the project is configured to use `xcrun` which uses the system clang with no openMP support. I can only see the CROSSTOOL in the generated bazel files (after starting a build)\u2014I can't work out where this all is configured.", "if Xcode is installed, you need BAZEL_USE_CPP_ONLY_TOOLCHAIN=1, see this [stackoverflow entry](https://stackoverflow.com/questions/47624689/how-to-set-c-compiler-on-os-x-with-bazel)"]}, {"number": 17045, "title": "Tensorflow 1.5.0 breaking previously built models", "body": "Hello!\r\n\r\nI've recently updated to tensorflow version 1.5.0, and suddenly receive an error, that I can't decipher, for code that worked before (in version 1.4.1): `Cannot use 'transducer_training/while/rnn/strided_slice' as input to 'gradients/transducer_training/while/rnn/while/Select_1_grad/Select/f_acc' because 'transducer_training/while/rnn/strided_slice' is in a while loop`\r\n\r\nI've also tried using the softmax_cross_entropy_with_logits function, but that still produced the same error. Here's the [stackoverflow](https://stackoverflow.com/questions/48713335/tensorflow-strided-slice-slicing-error-with-while-loop) post, in case its a coding mistake on my part.\r\nThe model is a seq2seq variation.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.5.0 (previous 1.4.1)\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Copy, paste and run the code\r\n\r\n\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\r\nfrom tensorflow.python.layers import core as layers_core\r\n\r\n# NOTE: Time major\r\n\r\n# ---------------- Constants Manager ----------------------------\r\nclass ConstantsManager(object):\r\n    def __init__(self, input_dimensions, input_embedding_size, inputs_embedded, encoder_hidden_units,\r\n                 transducer_hidden_units, vocab_ids, input_block_size, beam_width):\r\n        assert transducer_hidden_units == encoder_hidden_units, 'Encoder and transducer have to have the same amount' \\\r\n                                                                'of hidden units'\r\n        self.input_dimensions = input_dimensions\r\n        self.vocab_ids = vocab_ids\r\n        self.E_SYMBOL = len(self.vocab_ids)\r\n        self.vocab_ids.append('E_SYMBOL')\r\n        self.GO_SYMBOL = len(self.vocab_ids)\r\n        self.vocab_ids.append('GO_SYMBOL')\r\n        self.vocab_size = len(self.vocab_ids)\r\n        self.input_embedding_size = input_embedding_size\r\n        self.inputs_embedded = inputs_embedded\r\n        self.encoder_hidden_units = encoder_hidden_units\r\n        self.transducer_hidden_units = transducer_hidden_units\r\n        self.input_block_size = input_block_size\r\n        self.beam_width = beam_width\r\n        self.batch_size = 1  # Cannot be increased, see paper\r\n        self.log_prob_init_value = 0\r\n\r\n# ----------------- Model ---------------------------------------\r\n\r\n\r\nclass Model(object):\r\n    def __init__(self, cons_manager):\r\n        self.var_list = []\r\n        self.cons_manager = cons_manager\r\n        self.max_blocks, self.inputs_full_raw, self.transducer_list_outputs, self.start_block, self.encoder_hidden_init,\\\r\n            self.trans_hidden_init, self.logits, self.encoder_hidden_state_new, \\\r\n            self.transducer_hidden_state_new, self.train_saver = self.build_full_transducer()\r\n\r\n        self.targets, self.train_op, self.loss = self.build_training_step()\r\n\r\n    def build_full_transducer(self):\r\n        with tf.variable_scope('transducer_training'):\r\n\r\n            embeddings = tf.Variable(tf.random_uniform([self.cons_manager.vocab_size,\r\n                                                        self.cons_manager.input_embedding_size], -1.0, 1.0),\r\n                                     dtype=tf.float32,\r\n                                     name='embedding')\r\n            # Inputs\r\n            max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')  # total amount of blocks to go through\r\n            if self.cons_manager.inputs_embedded is True:\r\n                input_type = tf.float32\r\n            else:\r\n                input_type = tf.int32\r\n            inputs_full_raw = tf.placeholder(shape=(None, self.cons_manager.batch_size,\r\n                                                    self.cons_manager.input_dimensions), dtype=input_type,\r\n                                             name='inputs_full_raw')  # shape [max_time, 1, input_dims]\r\n            transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,\r\n                                                     name='transducer_list_outputs')  # amount to output per block\r\n            start_block = tf.placeholder(dtype=tf.int32, name='transducer_start_block')  # where to start the input\r\n\r\n            encoder_hidden_init = tf.placeholder(shape=(2, 1, self.cons_manager.encoder_hidden_units), dtype=tf.float32,\r\n                                                 name='encoder_hidden_init')\r\n            trans_hidden_init = tf.placeholder(shape=(2, 1, self.cons_manager.transducer_hidden_units), dtype=tf.float32,\r\n                                               name='trans_hidden_init')\r\n\r\n            # Temporary constants, maybe changed during inference\r\n            end_symbol = tf.get_variable(name='end_symbol',\r\n                                         initializer=tf.constant_initializer(self.cons_manager.vocab_size),\r\n                                         shape=(), dtype=tf.int32)\r\n\r\n            # Turn inputs into tensor which is easily readable#\r\n\r\n            inputs_full = tf.reshape(inputs_full_raw, shape=[-1, self.cons_manager.input_block_size,\r\n                                                             self.cons_manager.batch_size,\r\n                                                             self.cons_manager.input_dimensions])\r\n\r\n            # Outputs\r\n            outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)\r\n\r\n            init_state = (start_block, outputs_ta, encoder_hidden_init, trans_hidden_init)\r\n\r\n            # Initiate cells, NOTE: if there is a future error, put these back inside the body function\r\n            encoder_cell = tf.contrib.rnn.LSTMCell(num_units=self.cons_manager.encoder_hidden_units)\r\n            transducer_cell = tf.contrib.rnn.LSTMCell(self.cons_manager.transducer_hidden_units)\r\n\r\n            def cond(current_block, outputs_int, encoder_hidden, trans_hidden):\r\n                return current_block < start_block + max_blocks\r\n\r\n            def body(current_block, outputs_int, encoder_hidden, trans_hidden):\r\n\r\n                # --------------------- ENCODER ----------------------------------------------------------------------\r\n                encoder_inputs = inputs_full[current_block]\r\n                encoder_inputs_length = [tf.shape(encoder_inputs)[0]]\r\n                encoder_hidden_state = encoder_hidden\r\n\r\n                if self.cons_manager.inputs_embedded is True:\r\n                    encoder_inputs_embedded = encoder_inputs\r\n                else:\r\n                    encoder_inputs = tf.reshape(encoder_inputs, shape=[-1, self.cons_manager.batch_size])\r\n                    encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\r\n\r\n                # Build model\r\n\r\n                # Build previous state\r\n                encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)\r\n                encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, self.cons_manager.encoder_hidden_units])\r\n                encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, self.cons_manager.encoder_hidden_units])\r\n                encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)\r\n\r\n                #   encoder_outputs: [max_time, batch_size, num_units]\r\n                encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(\r\n                    encoder_cell, encoder_inputs_embedded,\r\n                    sequence_length=encoder_inputs_length, time_major=True,\r\n                    dtype=tf.float32, initial_state=encoder_hidden_state_t)\r\n\r\n                # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.\r\n                encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)\r\n                encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new,\r\n                                                      shape=[2, -1, self.cons_manager.encoder_hidden_units])\r\n\r\n                # --------------------- TRANSDUCER --------------------------------------------------------------------\r\n                encoder_raw_outputs = encoder_outputs\r\n                # Save/load the state as one tensor, use encoder state as init if this is the first block\r\n                trans_hidden_state = tf.cond(current_block > 0, lambda: trans_hidden, lambda: encoder_hidden_state_new)\r\n                transducer_amount_outputs = transducer_list_outputs[current_block - start_block]\r\n\r\n                # Model building\r\n                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\r\n                    embedding=embeddings,\r\n                    start_tokens=tf.tile([self.cons_manager.GO_SYMBOL],\r\n                                         [self.cons_manager.batch_size]),  # TODO: check if this looks good\r\n                    end_token=end_symbol)  # vocab size, so that it doesn't prematurely end the decoding\r\n\r\n                attention_states = tf.transpose(encoder_raw_outputs,\r\n                                                [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]\r\n\r\n                attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n                    self.cons_manager.encoder_hidden_units, attention_states)\r\n\r\n                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n                    transducer_cell,\r\n                    attention_mechanism,\r\n                    attention_layer_size=self.cons_manager.transducer_hidden_units)\r\n\r\n                projection_layer = layers_core.Dense(self.cons_manager.vocab_size, use_bias=False)\r\n\r\n                # Build previous state\r\n                trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)\r\n                trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, self.cons_manager.transducer_hidden_units])\r\n                trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, self.cons_manager.transducer_hidden_units])\r\n                trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)\r\n\r\n                decoder = tf.contrib.seq2seq.BasicDecoder(\r\n                    decoder_cell, helper,\r\n                    decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),\r\n                    output_layer=projection_layer)\r\n\r\n                outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\r\n                                                                                            output_time_major=True,\r\n                                                                                            maximum_iterations=transducer_amount_outputs)\r\n                logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]\r\n                decoder_prediction = outputs.sample_id  # For debugging\r\n\r\n                # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.\r\n                transducer_hidden_state_new = tf.concat(\r\n                    [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],\r\n                    axis=0)\r\n                transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,\r\n                                                         shape=[2, -1, self.cons_manager.transducer_hidden_units])\r\n\r\n\r\n                # Note the outputs\r\n                outputs_int = outputs_int.write(current_block - start_block, logits)\r\n\r\n                return current_block + 1, outputs_int, encoder_hidden_state_new, transducer_hidden_state_new\r\n\r\n            _, outputs_final, encoder_hidden_state_new, transducer_hidden_state_new = \\\r\n                tf.while_loop(cond, body, init_state, parallel_iterations=1)\r\n\r\n            # Process outputs\r\n            outputs = outputs_final.concat()\r\n            logits = tf.reshape(\r\n                outputs,\r\n                shape=(-1, 1, self.cons_manager.vocab_size))  # And now its [max_output_time, batch_size, vocab]\r\n\r\n            # For loading the model later on\r\n            logits = tf.identity(logits, name='logits')\r\n            encoder_hidden_state_new = tf.identity(encoder_hidden_state_new, name='encoder_hidden_state_new')\r\n            transducer_hidden_state_new = tf.identity(transducer_hidden_state_new, name='transducer_hidden_state_new')\r\n\r\n        train_saver = tf.train.Saver()  # For now save everything\r\n\r\n        return max_blocks, inputs_full_raw, transducer_list_outputs, start_block, encoder_hidden_init,\\\r\n            trans_hidden_init, logits, encoder_hidden_state_new, transducer_hidden_state_new, train_saver\r\n\r\n    def build_training_step(self):\r\n        targets = tf.placeholder(shape=(None,), dtype=tf.int32, name='targets')\r\n        targets_one_hot = tf.one_hot(targets, depth=self.cons_manager.vocab_size, dtype=tf.float32)\r\n\r\n        targets_one_hot = tf.Print(targets_one_hot, [targets], message='Targets: ', summarize=10)\r\n        targets_one_hot = tf.Print(targets_one_hot, [tf.argmax(self.logits, axis=2)], message='Argmax: ', summarize=10)\r\n\r\n        stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=targets_one_hot,\r\n                                                                         logits=self.logits)\r\n        loss = tf.reduce_mean(stepwise_cross_entropy)\r\n        train_op = tf.train.AdamOptimizer().minimize(loss)\r\n        return targets, train_op, loss\r\n\r\n\r\nconstants_manager = ConstantsManager(input_dimensions=1, input_embedding_size=11, inputs_embedded=False,\r\n                                     encoder_hidden_units=100, transducer_hidden_units=100, vocab_ids=[0, 1, 2],\r\n                                     input_block_size=1, beam_width=5)\r\nmodel = Model(cons_manager=constants_manager)\r\n```\r\nI can try and make a smaller fail case if needed.\r\n\r\nThanks!\r\nNikita", "comments": ["Any updates? @ebrevdo could you take a look?", "This is the same issue as described in #17199", "Let's close one of these bugs as duplicate.\n\nOn Sat, Feb 24, 2018, 8:36 AM nikita68 <notifications@github.com> wrote:\n\n> This is the same issue as described in #17199\n> <https://github.com/tensorflow/tensorflow/issues/17199>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17045#issuecomment-368240594>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim637HnMPacDWU_v-kHHDHbShUUAeks5tYDqBgaJpZM4SG_9N>\n> .\n>\n"]}, {"number": 17044, "title": "Hello, I am interested in collaborating with your project, serving as a translator to Spanish since it is my native language. I can translate any document in .md", "body": "\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Yes I can update it, and no I don't have experience in the field, I'm interested in contributing by translating over a thousand words texts... if interested let me know..", "Thank you for volunteering! I'm not sure what our multi-lingual plans are. @MarkDaoust ?", "my intention is to translate the dc to spanish so it gets the attention of the spanish speaking people, let me know if you guys approve it so we can start working on it!, cheers!", "We're seeing more and more people eager to contribute to translations (for example #16871).\r\n\r\nBut we don't have infrastructure to support this. We're looking into it.\r\n\r\n+@ewilderj \r\n+@wolffg", "FYI we just released our [ML course](https://developers.google.com/machine-learning/crash-course/?hl=es-419)."]}, {"number": 17043, "title": "seg fault in 1.6rc0 and master on skylake cpu (avx512 related probably)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBuilt from source\r\n- **TensorFlow version (use command below)**:\r\nv1.6.0-rc0-19-gecec1d8 1.6.0-rc1\r\nmaster\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.1.85/7.0.5.15\r\n- **GPU model and memory**:\r\nGeForce GTX 1080 Ti\r\ntotalMemory: 10.91GiB freeMemory: 8.36GiB\r\n\r\n### Describe the problem\r\n\r\nIf tf1.6 is compiled with `--march=native` then running inference on a large model ends up with crash with cryptic stack trace, 100% reproducible, no matter with or without CUDA (whether CUDA_VISIBLE_DEVICES=\"\" or absent).\r\n\r\nIf compiled **without** avx512 support (i.e. `-O3 -msse4.2 -mavx2 -mfma` only), then everything works fine.\r\nTF emits a warning though:\r\n```\r\n2018-02-15 14:03:54.237530: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n```\r\n\r\nI checked another issue with 64 byte alignment (https://github.com/tensorflow/tensorflow/issues/15588), but it does not help.\r\n\r\nHere is a crash trace, if it may help.\r\n```\r\n(gdb) bt\r\n#0  0x00007fdea3c5c9d5 in Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 48, 16, 0, false, false>::operator()(float*, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer> const&, long, long, long, long) () from /home/zbr/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007fdea3cd0fc4 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 48, 16, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 48, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool) ()\r\n   from /home/zbr/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fdea06232d1 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /home/zbr/.local/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fdea06210e7 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/zbr/.local/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fde96afbc80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#5  0x00007fded77d76ba in start_thread (arg=0x7fdeb87d8700) at pthread_create.c:333\r\n#6  0x00007fded750d41d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n(gdb)\r\n```", "comments": ["I tried this patch for avx512 alignment, but it doe not help\r\n\r\n```\r\n--- a/tensorflow/core/framework/allocator.h\r\n+++ b/tensorflow/core/framework/allocator.h\r\n@@ -67,13 +67,8 @@ struct AllocatorStats {\r\n // device memory.\r\n class Allocator {\r\n  public:\r\n-#ifdef EIGEN_VECTORIZE_AVX512\r\n   // Align to 64 byte boundary.\r\n   static constexpr size_t kAllocatorAlignment = 64;\r\n-#else\r\n-  // Align to 32 byte boundary.\r\n-  static constexpr size_t kAllocatorAlignment = 32;\r\n-#endif\r\n```", "```\r\n(gdb) x/i $rip\r\n=> 0x7f77386da175 <_ZN5Eigen8internal13gemm_pack_lhsIflNS0_26TensorContractionSubMapperIflLi1ENS_15TensorEvaluatorIKNS_17TensorReshapingOpIKNS_6DSizesIlLi2EEEKNS_9TensorMapINS_6TensorIKfLi4ELi1ElEELi16ENS_11MakePointerEEEEENS_16ThreadPoolDeviceEEENS_5arrayIlLm1EEESK_Li16ELb1ELb0ELi0ESC_EELi48ELi16ELi0ELb0ELb0EEclEPfRKSL_llll+485>:\tvmovaps %zmm0,-0x40(%r10)\r\n(gdb) \r\n```", "`-0x40(%r10)` - this looks definitely like 64byte alignment issue", "another trace:\r\n```\r\nvmovaps %zmm2,-0xc0(%rdx)\r\np/x $rdx\r\n$1 = 0x7f7b9400f8a0\r\n```\r\n\r\n0x7f7b9400f8a0 - 0xc0 = 140168740796384, which is aligned to 32 and not 64.\r\nSO far I changed every occurence of EIGEN_MAX_ALIGN_BYTES, but it leaks somewhere else.\r\n```\r\n\tmodified:   tensorflow/c/c_api.cc\r\n\tmodified:   tensorflow/core/common_runtime/gpu/gpu_device.cc\r\n\tmodified:   tensorflow/core/framework/allocator.h\r\n\tmodified:   tensorflow/core/framework/tensor.h\r\n\tmodified:   tensorflow/core/kernels/ops_util.h\r\n\tmodified:   tensorflow/python/lib/core/py_func.cc\r\n```", "Here is a problem description to date:\r\n* external eigen is being compiled with the native instruction set, which implies using avx512 on skylake cpus\r\n* external eigen somehow misses the fact that `EIGEN_VECTORIZE_AVX512` has to be defined, probably because it does not see `__AVX512F__` although it sees `__AVX2__` define (how?)\r\n* since there is no `EIGEN_VECTORIZE_AVX512`, eigen sets 32-byte alignment for avx2\r\n* since native instructions are used, `vmovaps` is injected, which faults on unaligned (32-byte aligned) chunks allocated via internal eigen compiler\r\n\r\nRudely adding `--copt=-DEIGEN_VECTORIZE_AVX512` ends up with error in `Eigen/src/Core/arch/SSE/PacketMath.h:260` and other places where compiler does not see correct instructions anymore, probably setting up this define breaks detection of the other options.\r\n\r\nChanging `EIGEN_IDEAL_MAX_ALIGN_BYTES` in `Eigen/src/Core/util/Macros.h` to 64 works fine.", "Following patch fixes segmentation fault, but network produces garbage outputs, disabling avx512 fixes this\r\n\r\n```\r\ndiff --git a/third_party/eigen.BUILD b/third_party/eigen.BUILD\r\nindex 07bb664..b88d9ca 100644\r\n--- a/third_party/eigen.BUILD\r\n+++ b/third_party/eigen.BUILD\r\n@@ -64,6 +64,7 @@ cc_library(\r\n         # This define (mostly) guarantees we don't link any problematic\r\n         # code. We use it, but we do not rely on it, as evidenced above.\r\n         \"EIGEN_MPL2_ONLY\",\r\n+       \"EIGEN_MAX_ALIGN_BYTES=64\"\r\n     ],\r\n     includes = [\".\"],\r\n     visibility = [\"//visibility:public\"],\r\n```", "This can also be a gcc bug (there is even no skylake march option in this compiler), in this case there should be a way to disable avx512 support for older compilers.\r\n\r\nI've sent a mail to eigen devel list describing this problem.", "I'm having the same issue. Is compiling without avx512 the best option at the moment? Could you please explain the \"-O3 -msse4.2 -mavx2 -mfma\" options for me? Seems to fix the issue.", "When you specify only these options, compiler will not use other instructions available on your processor, in particular it will not use AVX512 instructions which require additional support from TF and/or Eigen library. Also, avx512 support can be broken on older compilers and this is likely the case for ubuntu 16.04 and its eldery 5.4.0 gcc", "Upgrading to gcc 6.3.0 does not fix this issue, neither with allocator alignment nor with eigen one. Patch below fixes misalignment segfault, but network still produces garbage. Removing avx512 instructions (i.e. only using avx2, sse4.2 and mfma) fixes the problem.\r\n\r\n```\r\ndiff --git a/tensorflow/core/framework/allocator.h b/tensorflow/core/framework/allocator.h\r\nindex 5a95d3a..4e5bc66 100644\r\n--- a/tensorflow/core/framework/allocator.h\r\n+++ b/tensorflow/core/framework/allocator.h\r\n@@ -67,13 +67,8 @@ struct AllocatorStats {\r\n // device memory.\r\n class Allocator {\r\n  public:\r\n-#ifdef EIGEN_VECTORIZE_AVX512\r\n   // Align to 64 byte boundary.\r\n   static constexpr size_t kAllocatorAlignment = 64;\r\n-#else\r\n-  // Align to 32 byte boundary.\r\n-  static constexpr size_t kAllocatorAlignment = 32;\r\n-#endif\r\n \r\n   virtual ~Allocator();\r\n \r\ndiff --git a/third_party/eigen.BUILD b/third_party/eigen.BUILD\r\nindex 07bb664..ffe8a43 100644\r\n--- a/third_party/eigen.BUILD\r\n+++ b/third_party/eigen.BUILD\r\n@@ -64,6 +64,7 @@ cc_library(\r\n         # This define (mostly) guarantees we don't link any problematic\r\n         # code. We use it, but we do not rely on it, as evidenced above.\r\n         \"EIGEN_MPL2_ONLY\",\r\n+       \"EIGEN_MAX_ALIGN_BYTES=64\",\r\n     ],\r\n     includes = [\".\"],\r\n     visibility = [\"//visibility:public\"],\r\n```", "avx512 support is still not official. But We would greatly appreciate any fixes, if you can pinpoint the problem.", "I'll add to this old post and say I'm seeing intermittent segfaults and system lockups simply importing Tensorflow compiled using march=native on a SkylakeX (i7-7940).  Recompiling with march=\"haswell\" eliminates the issue.  While I can't say for sure this is an AVX-512 issue, that seem like a top candidate given the limited difference in flags.\r\n\r\nBecause of the intermittent nature, this was difficult to diagnose and it wasn't obvious that recompiling with different flags would fix the issue.  If AVX-512 isn't working/supported it would be helpful if a warning appeared in the docs or configure script to alert people of the potential danger.\r\n\r\n**Have I written custom code:** No\r\n**OS Platform and Distribution:** Ubuntu 18.04.1\r\n**TensorFlow:** Build from source\r\n**TensorFlow version:** 1.12rc1\r\n**Python version:** 3.6.6\r\n**Bazel version**: 0.18.0\r\n**GCC/Compiler version:** 7.3.0\r\n **CUDA/cuDNN version:** 10.0\r\n**GPU model and memory:** TitanX(maxwell) 12GB", "I don't think we still test or verify TF on skylake, so this is still expected.\r\nWe will probably switch to using MKL before we switch to compiling with avx512.", "It might be worth retrying the AVX512 builds on master.  The recent Eigen updates have pulled a number of AVX512 specific fixes into TensorFlow master that were not present in 1.12.  In particular, #21265, #20373, #18676 are now fixed.  #21265 is the most significant of these.  Prior to the fix for #21265, tensor contraction was broken on AVX512 builds and many of the examples simply didn't work.\r\n\r\nFrom the standpoint of the unit tests (1), the only failures specific to the AVX512 builds I see on master right now are the embedding_ops tests (see #21676).  The issue here is with the test cases themselves and not the underlying code.  There's a PR pending ( #21677 ) that fixes these unit tests.\r\n\r\n(1) The unit tests as run with bazel test --config opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/... -//tensorflow/lite/...\r\n", "Looks good for me, I disabled unit tests and XLA which do not compile, so far I do not see crashes I previously observed in tests in like 0-30 seconds after the start.\r\n\r\nAs a side note, NN inference performance dropped by 10% compared to 1.11 release (manually specifying all flags instead of --march=native). This could or could not be related to xla/avx512 or master build.", "No, the problem persists. And actually it has been made quite different: code does not crash anymore, but the output result is heavily wrong (compared to gpu calculation or cpu with avx512 turned off).\r\n\r\nThe good thing is that running rather large network produce different output each time, so this can be a sign or test, if the output would have been the same (but wrong) it could've been much harder to catch.\r\n\r\nThis is basically a large classification cnn which produces slightly different (within several percents) results with each run for the same input.", "@bioothod Are you testing on the latest version of master or on a release branch?", "The latest commit was 8855358, so it is master of Dec 10", "So you should have the latest changes.  Could you possibly run\r\n\r\nbazel test --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:conv_ops_test\r\n\r\non your local setup to see whether it passes or not?", "4 tests have failed, logs have been attached, but they tested cuda\r\n\r\n```\r\nINFO: Analysed target //tensorflow/python/kernel_tests:conv_ops_test (0 packages loaded).\r\nINFO: Found 1 test target...\r\nFAIL: //tensorflow/python/kernel_tests:conv_ops_test (shard 3 of 4) (see /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_3_of_4/test.log)\r\nFAIL: //tensorflow/python/kernel_tests:conv_ops_test (shard 2 of 4) (see /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_2_of_4/test.log)\r\nFAIL: //tensorflow/python/kernel_tests:conv_ops_test (shard 4 of 4) (see /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_4_of_4/test.log)\r\n\r\nFAILED: //tensorflow/python/kernel_tests:conv_ops_test (Summary)\r\n      /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_3_of_4/test.log\r\n      /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_2_of_4/test.log\r\n      /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_4_of_4/test.log\r\n      /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_1_of_4/test.log\r\nFAIL: //tensorflow/python/kernel_tests:conv_ops_test (shard 1 of 4) (see /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_1_of_4/test.log)\r\nTarget //tensorflow/python/kernel_tests:conv_ops_test up-to-date:\r\n  bazel-bin/tensorflow/python/kernel_tests/conv_ops_test\r\nINFO: Elapsed time: 2.130s, Critical Path: 1.89s\r\nINFO: 4 processes: 4 local.\r\nINFO: Build completed, 1 test FAILED, 5 total actions\r\n//tensorflow/python/kernel_tests:conv_ops_test                           FAILED in 4 out of 4 in 1.9s\r\n  Stats over 4 runs: max = 1.9s, min = 1.7s, avg = 1.8s, dev = 0.1s\r\n  /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_3_of_4/test.log\r\n  /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_2_of_4/test.log\r\n  /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_4_of_4/test.log\r\n  /home/zbr/.cache/bazel/_bazel_zbr/fca2d89a068104ec3ae07b293c9e9c36/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/conv_ops_test/shard_1_of_4/test.log\r\n```\r\n\r\nAttached in the same order as above\r\n[test.log](https://github.com/tensorflow/tensorflow/files/2672199/test.log)\r\n[test.log](https://github.com/tensorflow/tensorflow/files/2672201/test.log)\r\n[test.log](https://github.com/tensorflow/tensorflow/files/2672202/test.log)\r\n[test.log](https://github.com/tensorflow/tensorflow/files/2672203/test.log)\r\n\r\n\r\n", "Hi @bioothod !It seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Attaching relevant threads for reference. [link](https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17043\">No</a>\n"]}, {"number": 17042, "title": "fixed typo in docstring for unchanged shape method", "body": "", "comments": []}, {"number": 17041, "title": "Saving the model throws \"op_kernel.cc\" No such file or directory error", "body": "```\r\nEpoch: 50 Train Perplexity: 37.819\r\nEpoch: 50 Valid Perplexity: 60.281\r\nTest Perplexity: 44.138\r\nSaving model to output.\r\n2018-02-15 14:05:42.473937: W tensorflow/core/framework/op_kernel.cc:1198] Not found: ; No such file or directory\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory\r\n\t [[Node: save_1/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Model/Model/RNN/multi_rnn_cell/cell_0/lstm_cell/bias/Adam/_109, Model/Model/RNN/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1/_111, Model/Model/RNN/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam/_113, Model/Model/RNN/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1/_115, Model/Model/embedding/Adam/_117, Model/Model/embedding/Adam_1/_119, Model/Model/softmax_b/Adam/_121, Model/Model/softmax_b/Adam_1/_123, Model/Model/softmax_w/Adam/_125, Model/Model/softmax_w/Adam_1/_127, Model/RNN/multi_rnn_cell/cell_0/lstm_cell/bias/_129, Model/RNN/multi_rnn_cell/cell_0/lstm_cell/kernel/_131, Model/embedding/_133, Model/global_step, Model/softmax_b/_135, Model/softmax_w/_137, Train/Model/Variable/_139, Train/Model/beta1_power/_141, Train/Model/beta2_power/_143)]]\r\n```\r\n### System information\r\n- **Have I written custom code**: [PTB model](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py) from official RNN tutorial (using custom data and hyperparams slightly modified)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux 4.14.16-1 Manjaro\r\n- **TensorFlow installed from**: pip\r\n- **TensorFlow version**: 1.5.0\r\n- **Python version**: 3.6 \r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: NVIDIA GTX 1070 Mobile (8GB, driver 384.111)\r\n- **Exact command to reproduce**: just running the training file `main.py` with default FLAG params\r\n\r\n### Describe the problem\r\nTraining goes fine and I can observe loss decreasing. I can also run Tensorboard on the log dir and see the model graph. When the max_epoch is reached the code throws the above mentioned error.\r\n\r\n### Source code / logs\r\n--\r\n", "comments": ["Check checkpoint dir path is provided?", "Yes, that was the issue. Thank you."]}, {"number": 17040, "title": "conv1d doc string misnames first argument", "body": "The docs say `conv2d()`'s first argument is `input` which is maybe how the docstring for `conv1d()` ended up saying `input` instead of `value` when describing the `filters` argument.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17039, "title": "Docs fix r1.5", "body": "Docs Cherry-pick\r\n\r\nAdd blank lines after HTML  blocks for compatibility with new markdown parser.\r\n\r\nPiperOrigin-RevId: 185554969\r\n\r\n", "comments": []}, {"number": 17038, "title": "Docs fix r1.6", "body": "Docs Cherry-pick\r\n\r\nAdd blank lines after HTML  blocks for compatibility with new markdown parser.\r\nFix cuDNN64 dll name in windows install instructions.\r\n\r\nPiperOrigin-RevId: 185554969", "comments": []}, {"number": 17037, "title": "Bug: tf.contrib.learn.Experiment.continuous_train_and_eval does not release GPU memory switching between training and evaluation phases", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0rc0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**: 1080, 8GB\r\n- **Exact command to reproduce**: -\r\n\r\nThe documentation of [`continuous_train_and_eval`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment#continuous_train_and_eval) says:\r\n\r\n> the resources (e.g., memory) used by training will be released before evaluation (train_and_evaluate takes double resources)\r\n\r\nFrom the execution logs, however, this doesn't seem to be the case:\r\n\r\n**Starting training**:\r\n```\r\n2018-02-15 11:06:37.745151: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1208] Found device 0 with properties:\r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.835\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.60GiB\r\n2018-02-15 11:06:37.764258: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1308] Adding visible gpu devices: 0\r\n2018-02-15 11:06:39.084140: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6381 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n```\r\n\r\n**Switching from training to evaluation**:\r\n```\r\n2018-02-15 09:12:54.871561: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1308] Adding visible gpu devices: 0\r\n2018-02-15 09:12:54.871717: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 175 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n```\r\n\r\n**Switching back from evaluation to training**:\r\n```\r\n2018-02-14 19:56:50.513663: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1308] Adding visible gpu devices: 0\r\n2018-02-14 19:56:50.513819: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 175 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n```\r\n\r\nIt seems the function's loop fails to release memory after the first training phase is complete, so the subsequent devices only have 175MBs of memory available. Interestingly, training does experience a reduction in speed, but not as high as I would have expected (with 6GBs of memory available I have about 1.1 steps/second, with 175MBs it's 1.0 steps/second).\r\n\r\nI'm training the standard `inception_resnet_v2` from `slim`'s model zoo (batch size is 32, if that information is of any use).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Edited the original post", "GPU memory allocator still holds the memory. It didn't released back to OS. You can verify by checking the number of training params in your training. \r\n\r\nself.num_trainable_params = np.sum([\r\n        np.prod(var.get_shape().as_list()) for var in tf.trainable_variables()\r\n    ])\r\n    tf.logging.info(\r\n        'number of trainable params: {}'.format(self.num_trainable_params))", "If the allocator doesn't release the memory, I assume the new session created when running evaluation won't have access to it (at least that's what I infer from the log messages). Doesn't this break the specification of `continuous_train_and_eval`? It's supposed to release the memory before running evaluation (if fact, that's the core difference between this and `train_and_eval`)", "@xiejw can you please take a look at this issue?\r\n@GPhilo continuous_train_and_eval is still in experimental phase so there is no official support for it. ", "I am removing myself from this thread as I am not an expert to debug GPU memory issue. \r\n\r\nWhat experiment. continuous_train_and_eval  does is just calling Estimator.train and Estimator.evaluate, which both launches isolated Session. ", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17036, "title": "Add clean_dep to tf_cc_test.", "body": "", "comments": ["@gunan will you merge this? I don't want to interfere with releases.", "Sure, I will take care of the merge once we get all the tests passing."]}, {"number": 17035, "title": "Add clean_dep to tf_cc_test.", "body": "", "comments": []}, {"number": 17034, "title": "Quantized graphs produce unusable output on Android", "body": "When running a quantized graph on linux/ios/android the first 2 are correct, android though produces only garbage output. Does only occur with newer trained graphs (TF 1.4/5 i suppose), never had this issue before.\r\n\r\n`softfp` problem?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I am seeing this in iOS", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing because the template is not filled out.\r\n\r\n@joelteply if you still have this issue, please file a new issue with the issue template filled out."]}, {"number": 17033, "title": "cuda error on import", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Linux Fedora 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: v1.4.0-19-ga52c8d9, 1.4.1\r\n- **Python version**: 2.7\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: cuda_8.0.61,  cudnnv5\r\n- **GPU model and memory**: GeForce GTX TITAN X ,   12207MiB\r\n- **Exact command to reproduce**:\r\n\r\nWe have a computer cluster that some of the machines have GPU and others don't. I have installed tensorflow-gpu-1.4 from wheel file in a virtualenv in a folder on the file-server which means that it is accessible on all the machines in the cluster. \r\nMy program is a distributed software which means that some of the tasks are done on all the cluster nodes. (data generation and configuration) and the machine learning part is only done on the machines with GPU. I activate the aforementioned virtualenv before running the servers on nodes of the cluster so all the nodes are running inside the same virtual environment.  \r\nOn the machines that have a GPU when I import tensorflow everything works fine, but when I import the tensorflow on the machines that do not have the gpu (and Cuda is not installed on them) I get following error:   \r\n\r\n> In [1]: import tensorflow as tf\r\n> ImportError                               Traceback (most recent call last)\r\n> <ipython-input-1-64156d691fe5> in <module>()\r\n> ----> 1 import tensorflow as tf\r\n> \r\n> /virtualenv/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()\r\n>      22 \r\n>      23 # pylint: disable=wildcard-import\r\n> ---> 24 from tensorflow.python import *\r\n>      25 # pylint: enable=wildcard-import\r\n>      26 \r\n> \r\n> /virtualenv/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()\r\n>      47 import numpy as np\r\n>      48 \r\n> ---> 49 from tensorflow.python import pywrap_tensorflow\r\n>      50 \r\n>      51 # Protocol buffers\r\n> \r\n> /virtualenv/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n>      70 for some common reasons and solutions.  Include the entire stack trace\r\n>      71 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n> ---> 72   raise ImportError(msg)\r\n>      73 \r\n>      74 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n> \r\n> ImportError: Traceback (most recent call last):\r\n>   File \"virtualenv/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"/virtualenv/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"/virtualenv/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> ImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n\r\nI am aware that tensorflow-gpu is statically linked to the Cuda libraries and I installed a local version of Cuda using the runfile in a folder on the file-server (accessible to all the nodes) and added its path to the LD_LIBRARY_PATH and PATH and now importing tf gives me the following error:\r\n\r\n> 2018-02-15 01:41:58.519693: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU \r\n\r\n> supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n> 2018-02-15 01:41:58.519972: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUresult(-1)\r\n> 2018-02-15 01:41:58.520007: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: maserati\r\n> 2018-02-15 01:41:58.520017: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: maserati\r\n> 2018-02-15 01:41:58.520140: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\n> 2018-02-15 01:41:58.520167: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  384.111  Tue Dec 19 23:51:45 PST 2017\r\n> GCC version:  gcc version 7.2.1 20170915 (Red Hat 7.2.1-2) (GCC) \r\n> \"\"\"\r\n> 2018-02-15 01:41:58.520191: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.111.0\r\n\r\nAnd when i run a hello world script it gives me the following error (which clearly means it cant run anything on gpu because of previous error):\r\n\r\n> In [5]: with tf.device(\"/GPU:0\"):\r\n>    ...:     hello = tf.constant('Hello, TensorFlow!')\r\n>    ...:     sess = tf.Session()\r\n>    ...:     print(sess.run(hello))\r\n>    ...:     \r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> <ipython-input-5-4fc1d9ca141a> in <module>()\r\n>       2     hello = tf.constant('Hello, TensorFlow!')\r\n>       3     sess = tf.Session()\r\n> ----> 4     print(sess.run(hello))\r\n>       5 \r\n> \r\n> /virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n>     887     try:\r\n>     888       result = self._run(None, fetches, feed_dict, options_ptr,\r\n> --> 889                          run_metadata_ptr)\r\n>     890       if run_metadata:\r\n>     891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n> \r\n> /virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n>    1118     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n>    1119       results = self._do_run(handle, final_targets, final_fetches,\r\n> -> 1120                              feed_dict_tensor, options, run_metadata)\r\n>    1121     else:\r\n>    1122       results = []\r\n> \r\n> /virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n>    1315     if handle is None:\r\n>    1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n> -> 1317                            options, run_metadata)\r\n>    1318     else:\r\n>    1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n> \r\n> /virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n>    1334         except KeyError:\r\n>    1335           pass\r\n> -> 1336       raise type(e)(node_def, op, message)\r\n>    1337 \r\n>    1338   def _extend_graph(self):\r\n> \r\n> InvalidArgumentError: Cannot assign a device for operation 'Const_1': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\r\n> \t [[Node: Const_1 = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: Hello, TensorFlow!>, _device=\"/device:GPU:0\"]()]]\r\n> \r\n> Caused by op u'Const_1', defined at:\r\n>   File \"/virtualenv/bin/ipython\", line 11, in <module>\r\n>     sys.exit(start_ipython())\r\n>   File \"/virtualenv/lib/python2.7/site-packages/IPython/__init__.py\", line 119, in start_ipython\r\n>     return launch_new_instance(argv=argv, **kwargs)\r\n>   File \"/virtualenv/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n>     app.start()\r\n>   File \"/virtualenv/lib/python2.7/site-packages/IPython/terminal/ipapp.py\", line 355, in start\r\n>     self.shell.mainloop()\r\n>   File \"/virtualenv/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 493, in mainloop\r\n>     self.interact()\r\n>   File \"/virtualenv/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 484, in interact\r\n>     self.run_cell(code, store_history=True)\r\n>   File \"/virtualenv/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n>     interactivity=interactivity, compiler=compiler, result=result)\r\n>   File \"/virtualenv/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n>     if self.run_code(code, result):\r\n>   File \"/virtualenv/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n>     exec(code_obj, self.user_global_ns, self.user_ns)\r\n>   File \"<ipython-input-5-4fc1d9ca141a>\", line 2, in <module>\r\n>     hello = tf.constant('Hello, TensorFlow!')\r\n>   File \"/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 214, in constant\r\n>     name=name).outputs[0]\r\n>   File \"/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n>     op_def=op_def)\r\n>   File \"/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n> \r\n> InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Const_1': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\r\n> \t [[Node: Const_1 = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: Hello, TensorFlow!>, _device=\"/device:GPU:0\"]()]]\r\n\r\nI dont wan't/ can't install cuda on non-gpu machines is there any work around for this issue?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "yes", "This is a duplicate of  #2175\r\nSolution there... i.e.\r\nAt start of your code:\r\n```\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n```\r\nClosing for now. Thanks!\r\n\r\n\r\n"]}, {"number": 17032, "title": "Fix warnings in tf.contrib.bayesflow.monte_carlo.expectation", "body": "This fix fixes several warnings in tf.contrib.bayesflow.monte_carlo.expectation\r\nby switching to keepdims for tf.reduce_mean.\r\n", "comments": ["great"]}, {"number": 17031, "title": "program crashes on tf1.5 when creating a tensor object with CAPI", "body": "The following function is used to create a tensor object feeded into session's run function for inference at evaluation.    The evaluation test is wrapped in a loop for maximum 500 iterations, it crashes at iteration of 200-300 each time with log \"Process finished with exit code 139 (interrupted by signal 11: SIGSEGV\".   It works well in previous versions.  Thanks for any correction.\r\n\r\n```\r\nstatic void Deallocator(void *data, size_t, void *arg) {\r\n  tensorflow::cpu_allocator()->DeallocateRaw(data);\r\n  *reinterpret_cast<bool *>(arg) = true;\r\n}\r\n\r\nTF_Tensor *DlTensorUtil<float>::Feature2TF_Tensor(float feature[], int64_t dims[], int nDims, TF_DataType type) {\r\n  size_t len = 1;\r\n  for (int i = 0; i < nDims; i++)\r\n    len *= dims[i];\r\n  len *= sizeof(float);\r\n  bool deallocator_called = false;\r\n  TF_Tensor *ts = TF_NewTensor(type, dims, nDims, feature, len, &Deallocator, &deallocator_called);\r\n  return ts;\r\n}\r\n\r\nvoid DlTensorUtil<float>::Feature2Tensor(float feature[], int64_t dims[], int nDims, TF_DataType type, Tensor &out_tensor) {\r\n  TF_Tensor *tftensor = Feature2TF_Tensor(feature, dims, nDims, type);\r\n  out_tensor = tensorflow::TensorCApi::MakeTensor(tftensor->dtype, tftensor->shape, tftensor->buffer);\r\n}\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.3\r\n- **TensorFlow installed from (source or binary)**: From Source\r\n- **TensorFlow version (use command below)**:  1.5\r\n- **Python version**:  Not used\r\n- **Bazel version (if compiling from source)**:  0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 8.0/6.1\r\n- **GPU model and memory**: gtx1080/8g\r\n- **Exact command to reproduce**:", "comments": ["Please provide the log file. Thanks", "\"Process finished with exit code 139 (interrupted by signal 11: SIGSEGV\" is met", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 83 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "the issue dismissed.\n\nOn Sat, Jun 2, 2018 at 3:23 PM, Alfred Sorten Wolf <notifications@github.com\n> wrote:\n\n> We are closing this issue for now due to lack of activity. Please comment\n> if this is still an issue for you. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17031#issuecomment-394065691>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AfJbA6R513Dm8G4s-vHexKWP1og9VBJoks5t4j2GgaJpZM4SGevR>\n> .\n>\n"]}, {"number": 17030, "title": "fix SRU call return type", "body": "Because many other cells , such as BasicLSTMCell \u3001 LSTMCell \u3001NASCell \u3001 LayerNormBasicLSTMCell , return h and stateTuple with c and h when call . I think sru should return the same type as lstm .", "comments": ["It's a bit off-topic but I've found that doing `U = math_ops.matmul(inputs, self._kernel)` inside the ` def call(self, inputs, state):` is not helpful to achieve the 10x speed up mentioned in the paper. I believe the matrix multiply should happen outside of the `call`.\r\n", "I don't think it's clearer to return the output h as part of the recurrent state as the essential trick of the SRU is that only c takes part in the recurrence. This pr makes it look like h is used for the next timestep.", "@marcbelmont achieve the 10x speed up need cuda code implement SRU , not python code . see more in https://github.com/taolei87/sru .", "@carlthome I agree with you as only c takes part in the recurrence . So you advice is no need for change thr origin code ? I just think it should be compatible with lstm , so  that user only change lstm to sru with no other experiment code change . Maybe I should close this pr .", "@ekelsen is this PR good now?", "IMHO this should not be merged because there's no clear benefit to the breaking change and it's also less mathematically correct to pass around h in the state when it's not actually part of the recurrence.\r\n\r\nThere are many other RNN cells that also doesn't return a state tuple (GRU for example) so you'd still have to check what state actually is anyway (or use tf.contrib.framework.nest).", "@carlthome  OK \uff0c I close this pr ."]}, {"number": 17029, "title": "Fixes #16152", "body": " Fixes #16152. Please note that I have tested this fix only on Ubuntu 16.04 with python 3.5. It should in theory work for both python 2+ and python 3+ but I have not tested it on all major versions of python.", "comments": ["Python 2 has no `inspect.getfullargspec` so this PR will not work on Python 2.", "Re @KarlJack47 please address sanxiyn@'s comments or close the PR?", "Sorry @protoget I have been busy with classes. I will close this pull request and try to implement a fix for the issue mentioned by @sanxiyn and then reopen a new pull request. I have some ideas around a fix but don't have the time to implement it right now.", "By the way thanks noticing the issue @sanxiyn "]}, {"number": 17028, "title": "MLP prediction is 3~4x slower than theano/pytorch", "body": "(Moving from keras-team/keras#9388)\r\nFor a simple 2-hidden-layer MLP, TF is 3~4x slower than pytorch and theano. This is only for **prediction**, not training, and it is only on **CPU**. Please see this [GitHub gist](https://gist.github.com/JiaweiZhuang/c3350f7a89db3d5a98c6a2c0228ceea9/eb4eec9056b02b1ac2e0e039f646347c02885309) for timing.\r\n\r\nFor reproducibility, the test was done on AWS EC2 c5.large. Two different builds were tested and showed similar results. One is the pre-built [AWS deep learning AMI](https://aws.amazon.com/machine-learning/amis/) (ami-e07e779a). Another is installing conda on a fresh Ubuntu machine and then installing TF with `pip`.\r\n\r\nMy questions are:\r\n1. Is this performance difference expected? I assume that TF shouldn't be that slow. \r\n2. @fchollet suggested that the TF installation was broken. If so, what's the correct way to install TensorFlow to ensure good performance? I also tried [other installation methods on the official docs](https://www.tensorflow.org/install/install_linux), as such native Python and pip, but didn't get better performance.\r\n\r\n### System information\r\n- **Have I written custom code**: Almost all built-in functions\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04 \r\nTest env 1: AWS deep learning AMI Ubuntu Version (ami-e07e779a)\r\nTest env 2: AWS base Ubuntu AMI (ami-66506c1c)\r\n- **TensorFlow installed from**: \r\nTest env 1: TensorFlow/Keras/PyTorch are all provided by that AMI.\r\nTest env 2: Installed from binary, i.e. `pip install tensorflow` and `pip install keras`. PyTorch was installed by `conda install pytorch`.\r\n- **TensorFlow version**: 1.5.0\r\n- **Python version**: 3.6\r\n- **Bazel version**: N/A (from binary)\r\n- **CUDA/cuDNN version**: CPU-only\r\n- **GPU model and memory**: CPU-only\r\n- **Exact command to reproduce**: Please follow this [GitHub gist](https://gist.github.com/JiaweiZhuang/c3350f7a89db3d5a98c6a2c0228ceea9/eb4eec9056b02b1ac2e0e039f646347c02885309).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version", "For TF, the code you have is suboptimal. Modifying the following should help:\r\n\r\nReplace:\r\nX_tf = tf.placeholder(tf.float32, shape=(None, n_feature), name=\"X\")\r\nwith:\r\nX_tf = tf.Variable(X)\r\n\r\nAnd change the next cell to:\r\n\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    out.eval()  # Certain optimization passes are run on the first eval for a new graph\r\n    %timeit out.eval()\r\n", "> For TF, the code you have is suboptimal. Modifying the following should help:\r\n\r\nThanks, but your suggestion further slows down TF by 2x... Please see this [updated gist](https://gist.github.com/JiaweiZhuang/c3350f7a89db3d5a98c6a2c0228ceea9/05823858b19620a0367716459eb01f6e153ea27f)", "Looks like there's a fair bit of noise in how I have been trying this. One issue that does seem to hold:\r\n- For TF 1.5 and before the default CPU binary isn't optimized e.g. isn't built with AVX etc.\r\n- TF 1.6.0rc0 includes AVX (could still be better for the AMI you mention if built with the best options).\r\n\r\nCreated a [colab notebook](https://drive.google.com/file/d/1VTNncnaN0_4u6VT7hmiPd6WvjXohO_1H/view?usp=sharing) with some of your GIST  and running there (with TF 1.6rc) seems to give similar results. Note: It doesn't match your numbers for either PyTorch or TF, and there was some noise.\r\n\r\nSee [Optimizing for CPU](https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu) in the performance guide for more details.", "Thanks for pointing out the AVX issue! 1.6rc1 does increase the performance by 40% (from 600 ms to 360 ms), but it is still 2~3x slower than PyTorch (130 ms) in my test environment (EC2 c5.large). Please see this [updated Gist](https://gist.github.com/JiaweiZhuang/c3350f7a89db3d5a98c6a2c0228ceea9/e5371704fa09be2f530a4e503a027fd82891f3a1) for details.\r\n\r\nTensorFlow is installed by:\r\n```\r\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.6.0rc1-cp36-cp36m-linux_x86_64.whl\r\n```", "There doesn't seem to be an inherent performance difference even though we don't benchmark the specific case you mention. You may get better performance following the instructions in [Optimizing for CPU](https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu) or you can try installing the [TensorFlow binary from Intel](https://anaconda.org/intel/tensorflow) just to try it out (Note it is still on TF 1.4). \r\nCopying their instructions here:\r\n`pip install -i https://pypi.anaconda.org/intel/simple tensorflow`\r\nor\r\n`conda install -c intel tensorflow` \r\n\r\nPlease continue to look at the [performance guide](https://www.tensorflow.org/performance/performance_guide) to get the best performance for TensorFlow.", "> you can try installing the TensorFlow binary from Intel just to try it out\r\n\r\nInstalling Intel's tensorflow binary does remove the performance difference. Now tensorflow is as fast as pytorch, although Keras still adds a significant overhead (3x slow down). The timing is available in the [updated Gist](https://gist.github.com/JiaweiZhuang/c3350f7a89db3d5a98c6a2c0228ceea9).\r\n\r\nThanks again for the help! Let me close this issue."]}, {"number": 17027, "title": "Fix __shared__ complex<T> undefined behavior", "body": "std::complex<T> has a non-empty constructor (zero assignment) that is not\r\ncompatible with CUDA __shared__ memory. This fixes current reliance on\r\nundefined behavior (and removes an unnecessary run-time initialization).", "comments": ["Does this have any performance implications?  Did you run the reduction benchmarks?", "Did not check performance of reductions, but changes are expected to slightly improve performance, if anything. With the old code the compiler was generating unnecessary instructions for each thread to independently zero the shared memory region.", "Nagging Assignee @ekelsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tfboyd can you look into getting this merged?", "@zheng-xq  Can you have someone escort this PR through the merge process.  Email sent to you as well just now.  ", "@reedwm, please help move this PR through the merge process. Thanks!"]}, {"number": 17026, "title": "Fix #16152", "body": "Please note that I have tested this fix only on Ubuntu 16.04 with python 3.5. It should in theory work for both python 2+ and python 3+ but I have not tested it on all major versions of python.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}]