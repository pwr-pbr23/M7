[{"number": 23760, "title": "tensorflow lite: error when convert from keras model .h5", "body": "When I try convert the keras model to tflite, this error appears:\r\n`\r\nTypeError: ('Keyword argument not understood:', 'negative_slope')`\r\n\r\nBut only with some models like mobilenet and mobilenetv2. When I convert my inceptionv3 and nasnetmobile is Ok. All the models that Im trying to convert was retrained and fine tuned.\r\n\r\nCode:\r\n`import numpy as np`\r\n`import tensorflow as tf`\r\n`converter = tf.contrib.lite.TocoConverter.from_keras_model_file('models/mobilenetv2_tune.h5')`\r\n`tflite_model = converter.convert()`\r\n`open(\"mobilenetv2_model.tflite\", \"wb\").write(tflite_model)`\r\n\r\n**Keras version**: 2.2.4\r\n**Tensorflow version**: 1.10.1\r\n\r\n\r\n\r\n", "comments": ["I solved the problem using the tf-nightly-gpu, probably is some bug with the incompatibility of versions between keras and tensorflow.", "That' great. I will close this issue since its resolved."]}, {"number": 23759, "title": "longer latency after post-training quantization", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI modified the example script a bit to print inference latency\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nGoogle Colaboratory with CPU\r\n- **TensorFlow installed from (source or binary)**:\r\n! pip install -U tf-nightly==1.13.0.dev20181027\r\n- **TensorFlow version (use command below)**:\r\ntf-nightly-1.13.0.dev20181027\r\n- **Python version**:\r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nThe post-training quantization documentation claims that it provides 3x lower latency: https://www.tensorflow.org/lite/performance/post_training_quantization\r\n\r\nBut when I try with the example script: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb\r\nI found that quantized model is apparently slower than original one.\r\n(You can see the logs below. Quantized model provides longer latency).\r\n\r\nWhy is the experiment result different from the documentation claim?\r\nShould I try some other example/environment?\r\n\r\n\r\n### Source code / logs\r\nThe original one with floating-point:\r\nAccuracy after 500 images: 0.970000 average time: 0.002977\r\nAccuracy after 1000 images: 0.960000 average time: 0.003007\r\nAccuracy after 1500 images: 0.952667 average time: 0.003022\r\nAccuracy after 2000 images: 0.951500 average time: 0.003026\r\nAccuracy after 2500 images: 0.949600 average time: 0.003026\r\nAccuracy after 3000 images: 0.954000 average time: 0.003024\r\nAccuracy after 3500 images: 0.956857 average time: 0.003029\r\nAccuracy after 4000 images: 0.954000 average time: 0.003031\r\nAccuracy after 4500 images: 0.954222 average time: 0.003032\r\nAccuracy after 5000 images: 0.954200 average time: 0.003038\r\nAccuracy after 5500 images: 0.957091 average time: 0.003043\r\nAccuracy after 6000 images: 0.958500 average time: 0.003047\r\nAccuracy after 6500 images: 0.959692 average time: 0.003050\r\nAccuracy after 7000 images: 0.960714 average time: 0.003050\r\nAccuracy after 7500 images: 0.962400 average time: 0.003051\r\nAccuracy after 8000 images: 0.964375 average time: 0.003052\r\nAccuracy after 8500 images: 0.965059 average time: 0.003051\r\nAccuracy after 9000 images: 0.966889 average time: 0.003052\r\nAccuracy after 9500 images: 0.968000 average time: 0.003052\r\nAccuracy after 10000 images: 0.966700 average time: 0.003054\r\ntotal time: 30.540090\r\n0.9667\r\n\r\n\r\nWith post-training quantization:\r\nAccuracy after 500 images: 0.970000 average time: 0.004113\r\nAccuracy after 1000 images: 0.959000 average time: 0.004096\r\nAccuracy after 1500 images: 0.950667 average time: 0.004093\r\nAccuracy after 2000 images: 0.950000 average time: 0.004084\r\nAccuracy after 2500 images: 0.948000 average time: 0.004077\r\nAccuracy after 3000 images: 0.952667 average time: 0.004081\r\nAccuracy after 3500 images: 0.955714 average time: 0.004103\r\nAccuracy after 4000 images: 0.953000 average time: 0.004125\r\nAccuracy after 4500 images: 0.953111 average time: 0.004133\r\nAccuracy after 5000 images: 0.953200 average time: 0.004130\r\nAccuracy after 5500 images: 0.956182 average time: 0.004126\r\nAccuracy after 6000 images: 0.957667 average time: 0.004132\r\nAccuracy after 6500 images: 0.958923 average time: 0.004137\r\nAccuracy after 7000 images: 0.960000 average time: 0.004142\r\nAccuracy after 7500 images: 0.961600 average time: 0.004146\r\nAccuracy after 8000 images: 0.963625 average time: 0.004149\r\nAccuracy after 8500 images: 0.964353 average time: 0.004148\r\nAccuracy after 9000 images: 0.966222 average time: 0.004145\r\nAccuracy after 9500 images: 0.967368 average time: 0.004140\r\nAccuracy after 10000 images: 0.966100 average time: 0.004136\r\ntotal time: 41.360908\r\n0.9661\r\n\r\nWe can see after post-training quantization, the latency is apparently longer......\r\nBut in the documentation, it claims to reduce latency....\r\n", "comments": ["Same problem here. I trained mobilenet on keras on my own dataset and after convert to tflite and use on the application, the FPS was around 250ms and with the post-training quantization was about 450ms.\r\n\r\n**Keras:** 2.2.4\r\n**tf-nightly-gpu**: '1.13.0-dev20181110'", "@mengwanguc did you find any solution?", "Same problem.", "Hi all, thanks for the issue!\r\nWe may need to optimize some kernels that your models are using. We experienced up to 3x on models we optimized, but there are many ops that may still need extra attention :)\r\n\r\nWe are looking into this. To help us reproduce, could you provide the following:\r\n(1) Which model are you running?\r\n(2) What device/architecture are you running on?\r\n\r\nAlso, if possible could you provide your TFLite float and TFLite quantized model for reproducing? Feel free to direct email me.\r\n", "Hi @suharshs \r\n\r\n(1). I'm using the tensorflow official MNIST model: https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py\r\n(2). I'm running on Google Colab with CPU.\r\n       It's Intel(R) Xeon(R) CPU @ 2.30GHz with ~12.7G RAM.\r\n\r\nHere the generated TFLite models :\r\nTFLite float: https://drive.google.com/file/d/146fFE4jEXaTLfgXWK3ByQ0XOxIYqa8I1/view?usp=sharing\r\nTFLite quantized: https://drive.google.com/file/d/1dcl9hWzcKNSvoOQBqgTOy3QLsvJLz010/view?usp=sharing\r\n\r\nThanks,\r\nMeng\r\n", "@suharshs \r\nActually this is the Google Colab notework I used: https://colab.research.google.com/drive/1ki6VI522w8oUCdoTu4azPZ2_dSZMPHtB", "Same problem here. I used mobilenet v1.", "If you happen to have access to an android phone, can you try to benchmark the models with this tool?\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark\r\n\r\nThe numbers we published were all based on results running on phones instead of desktops. I tried to reproduce the issue with the mnist_model.tflite and mnist_model_quant.tflite linked above. Here's what I got:\r\n\r\nfloat model: avg=5694.7us\r\nquant model: avg=3106.09us", "Hi @liyunlu0618, thanks!\r\nI tried the benchmark on Google Pixel 2, and yes, the quant model is much faster!\r\n\r\nBut... It's so fast that it confuses me... Here are my results:\r\nfloat model: avg=27312.6us\r\nquant model: avg=2727.6us\r\n\r\nThe speed up is ~10x.....\r\nI also tried the pre-trained mobilenet_v1_1.0_224 model, and the result is similar: the quant model is 9x faster.\r\nI thought it should be 3-4x, as published.\r\nWhy is my quant model so fast?\r\n\r\nAlso, could you explain why quant model is much slower than float model on desktop CPU?\r\nI guess it's because the speed-up of integer arithmetic require special/optimized instructions/kernels, while such optimizations are not done on desktop CPU? Am I correct?", "OK I guess the 10x speed-up is caused by num_threads.\r\n\r\nIn my previous experiment, I followed the benchmark document, and set the num_threads to 4. The speed-up was 10x.\r\n\r\nI changed num_threads to 1, and the results are similar to yours:\r\nfloat model: avg=5347.93us\r\nquant model: avg=2734.78us\r\n\r\nLooks like quant model does better for multi-threading?\r\nI have some guess: is that because quant model requires less memory access, thus when running with multiple threads, the queueing for shared memory (model weights) takes less time?", "Hi @mengwanguc \r\nThanks for the detailed investigation! You explanation about the behavior running on desktop is correct. Our kernels are optimized for mobile CPU only.\r\n\r\nIn terms of the multi-threaded float vs. quant performance difference, it's more likely the case that the floating point code path is not as optimized. We're actively looking into improving the performance of multi-threaded floating point inference."]}, {"number": 23758, "title": "ImportError: cannot import name 'cloud'", "body": "The latest build is giving me a weird Import Error on a module, even though I added extra stuff to the necessary __init__.py. Build command and trace below:\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): Master branch\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Code to reproduce the issue**\r\n```bash \r\nwget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel_0.15.0-linux-x86_64.deb && \\\r\n    sudo dpkg -i bazel_0.15.0-linux-x86_64.deb && \\\r\n    sudo apt-get install -f && \\\r\n    rm bazel_0.15.0-linux-x86_64.deb && \\\r\n    pip uninstall -y tensorflow-tensorboard tfp-nightly tensorflow_estimator tb-nightly tf-nightly tensorflow && \\\r\n    cd /opt && \\\r\n    git clone --recursive   https://github.com/tensorflow/tensorflow.git && \\\r\n    cd /opt/tensorflow && \\\r\n    sed -i 's/2018\\.0\\.3\\.20180406/2019\\.0\\.1\\.20180928/g' tensorflow/contrib/cmake/external/mkl.cmake && \\\r\n    sed -i 's/v0\\.14/v0\\.17\\-rc/g' tensorflow/contrib/cmake/external/mkl.cmake && \\\r\n    sed -i 's/3063b2e4c943983f6bf5f2fb9a490d4a998cd291/21fb5f2af1dd14e132af4f1b79160977ee487818/g' tensorflow/contrib/cmake/external/mkldnn.cmake && \\\r\n    ln -s /usr/lib/libmkldnn.so /usr/lib/libiomp5.so /usr/lib/libmklml_gnu.so  /usr/lib/libmklml_intel.so $MKLROOT/lib/intel64_lin && \\\r\n    ln -s /usr/include/mkldnn* $MKLROOT/include && \\\r\n    ln -s $MKLROOT/lib/intel64_lin/* $MKLROOT/lib && \\\r\n    ln -s /usr/lib /usr/lib/lib && \\\r\n    echo \"\" > /usr/local/lib/license.txt && \\\r\n    echo \"\" > /usr/local/include/license.txt && \\\r\n    echo \"from tensorflow.contrib import cloud\" >> tensorflow/contrib/__init__.py && \\\r\n    echo \"from tensorflow.contrib import *\" >> tensorflow/contrib/__init__.py && \\\r\n    TF_MKL_ROOT=/usr/lib  \\\r\n    TF_MKL_DOWNLOAD=0 \\\r\n    USE_DEFAULT_PYTHON_LIB_PATH=1 \\\r\n    TF_NEED_MKL=1 \\\r\n    TF_NEED_JEMALLOC=1 \\\r\n    TF_NEED_GCP=0 \\\r\n    TF_NEED_HDFS=0 \\\r\n    TF_ENABLE_XLA=1 \\\r\n    TF_NEED_MPI=0 \\\r\n    TF_NEED_GDR=0 \\\r\n    TF_NEED_S3=1 \\\r\n    TF_NEED_KAFKA=0 \\\r\n    TF_SET_ANDROID_WORKSPACE=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_MKL_ENABLED=\"true\" \\\r\n    CI_BUILD_PYTHON=/opt/conda/bin/python \\\r\n    PYTHON_BIN_PATH=/opt/conda/bin/python \\\r\n    PYTHON_LIB_PATH=/opt/conda/lib/python3.6/site-packages \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    CC_OPT_FLAGS=\"-msse4.2 -msse4.1 -mavx -msse2 -msse3 -mfpmath=sse -lmkl_gf_lp64 -Wl,--start-group -lmkldnn -lmklml_intel -lmkl_gnu_thread -lmkl_core -Wl,--end-group -dl -lpthread -lm \" \\\r\n    /bin/bash ./configure \\\r\n    && \\\r\n    bazel build \\\r\n    --config=mkl --config=opt \\\r\n    --config=noaws --config=nogcp --config=verbs --config=noignite --config=nokafka \\\r\n    --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n    --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3  \\\r\n    --copt=-O3 --copt=-mfpmath=both \\\r\n    --copt=\"-DMKL_LP64\" \\\r\n    --copt=\"-fPIC\" \\\r\n    --linkopt=\"-lmkl_gf_lp64\" \\\r\n    --linkopt=\"-lmkl_gnu_thread\" \\\r\n    --linkopt=\"-dl\" \\\r\n    --linkopt=\"-ldl\" \\\r\n    --linkopt=\"-lpthread\" \\\r\n    --linkopt=\"-lmkl_core\" \\\r\n    --linkopt=\"-lm\" \\\r\n    --linkopt=\"-lmkl_rt\" \\\r\n    --linkopt=\"-lmkldnn\" \\\r\n    tensorflow/tools/pip_package:build_pip_package && \\\r\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \\\r\n    pip install --no-deps /tmp/pip/tensorflow-*.whl && \\\r\n    cd /opt && rm -rf /opt/tensorflow /tmp/* && \\\r\n    python -c \"import tensorflow as tf; hello = tf.constant('Hello, TensorFlow!'); sess = tf.Session(); print(sess.run(hello))\" && \\\r\n\r\n### Build TFP after this ###\r\n\r\n    python -c \"import tensorflow_probability\"\r\n\r\n```\r\n\r\n** Trace **\r\n```python \r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_probability/__init__.py\", line 78, in <module>\r\n    from tensorflow_probability.python import *  # pylint: disable=wildcard-import\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py\", line 22, in <module>\r\n    from tensorflow_probability.python import distributions\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/distributions/__init__.py\", line 66, in <module>\r\n    from tensorflow_probability.python.distributions.multivariate_student_t import MultivariateStudentTLinearOperator\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/distributions/multivariate_student_t.py\", line 25, in <module>\r\n    from tensorflow_probability.python import math\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/math/__init__.py\", line 22, in <module>\r\n    from tensorflow_probability.python.math.diag_jacobian import diag_jacobian\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_probability/python/math/diag_jacobian.py\", line 24, in <module>\r\n    tfe = tf.contrib.eager\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/opt/conda/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 30, in <module>\r\n    from tensorflow.contrib import cloud\r\nImportError: cannot import name 'cloud'\r\n\r\n```", "comments": ["I see the same issue. How did you resolve it?", "Oh I didn't know this issue popped up again. It was solved automatically on my end when I built from Master using Bazel 0.15. I have a few lines in my build call that I add to in the inits, but I haven't built in a recent while. I'll reopen this if my build fails again. ", "Ooff can confirm that it's indeed back. Reopening.", "Working on Bazel 0.18.0 btw", "which commit did you build the above version?\r\nIt looks like I made a mistake in our build files, which excluded cloud capabilities by mistake.\r\nIt is fixed in the last few days.", "I just built on whatever the latest commit was on Master 3 hours ago.", "I pulled the fresh master and was able to rebuild it.\n\nOn Tue, Nov 20, 2018, 4:41 PM Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> which commit did you build the above version?\n> It looks like I made a mistake in our build files, which excluded cloud\n> capabilities by mistake.\n> It is fixed in the last few days.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23758#issuecomment-440438308>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEY95TXX5IuyW83YaDYVjS7aFJIzH413ks5uxHb7gaJpZM4Ye0JD>\n> .\n>\n", "Closing then! Thanks!\r\n", "@gunan Could you please point to commit which resolves` ImportError: cannot import name cloud` issue?\r\nI have observed this error on test cases failures for v1.12.0", "I just did \"git checkout master\" to make sure I have that fix. I also think\nit may be a good idea to just delete \"tensorflow\" folder after a failed\nbuild and pull a fresh one. I think \"bazel clean\" does not clean up\neverything.\n\nOn Thu, Nov 22, 2018 at 12:48 PM Nayana Thorat <notifications@github.com>\nwrote:\n\n> @gunan <https://github.com/gunan> Could you please point to commit which\n> resolves ImportError: cannot import name cloud issue?\n> I have observed this error on test cases failures for v1.12.0\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23758#issuecomment-441093672>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEY95RT2nfUsQDAlkGVqww8lMswjkGnhks5uxuN_gaJpZM4Ye0JD>\n> .\n>\n", "@phalexo Thanks for your inputs.\r\n I wanted to know which commit fixes this issue in master so if possible I can apply that patch to v1.12.0 code and rerun test cases. ", "I think the same commits mentioned in https://github.com/tensorflow/tensorflow/issues/23771 should fix this\r\n", "@gunan reopening because it's back again.. :(", "I think you need to build with GCP support on x86 system to avoid running into this problem\r\non ppc, we will slve this by disabling access to tf.contrib.cloud as described in #23976", "Aha! I just started the bulid with GCP, will report back! THanks!", "Built all good, thanks! Closing again."]}, {"number": 23757, "title": "Fixed technical issues with README.md", "body": "Improved the README.md file to adhere to more technical writing standards. Concision was improved, errors were fixed, and formality is increased\r\n\r\nFixes #23756", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@claytonjian Can you solve the merge conflicts to move ahead with the PR? Thanks", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 23756, "title": "Fix technical issues with README.md", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n- Doc Link:\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Closing this issue since its obsolete and [README.md ](https://github.com/tensorflow/tensorflow/blob/master/README.md) has been updated. Thanks!"]}, {"number": 23755, "title": "Windows 10 Failed to load the native TensorFlow runtime", "body": "Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\r\n\r\nSystem information\r\n\r\nOS Platform and Distribution Windows 10 64 bit\r\nTensorFlow installed from (source or binary): pip install tensorflow\r\nTensorFlow version: 1.12.0\r\nPython version:3.6.5\r\nGPU model and memory: Geforce 150mx 4gb\r\nDescribe the problem\r\nI'm unable to import keras using \"Tensorflow backend\"\r\nwhen I try to import \"from keras.models import Sequential\" I get the error shown bellow\r\n\r\nThese are the steps that I did when I installed the libraries\r\n1- conda create -n tensorflow python=3.5 anaconda.\r\n\r\n2- activate tensorflow.\r\n\r\n3- conda install theano.\r\n\r\n4- conda install mingw libpython.\r\n\r\n5-pip install tensorflow.\r\n\r\n6- pip install keras.\r\n\r\n7- conda update --all\r\n\r\n8- pip install tensorflow-gpu\r\n\r\nOther info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nError shown:\r\n\r\n`from keras.models import Sequential\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\nFile \"\", line 1, in \r\nfrom keras.models import Sequential\r\n\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras_init_.py\", line 3, in \r\nfrom . import utils\r\n\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\utils_init_.py\", line 6, in \r\nfrom . import conv_utils\r\n\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in \r\nfrom .. import backend as K\r\n\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\backend_init_.py\", line 89, in \r\nfrom .tensorflow_backend import *\r\n\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in \r\nimport tensorflow as tf\r\n\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow_init_.py\", line 24, in \r\nfrom tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\n\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python_init_.py\", line 49, in \r\nfrom tensorflow.python import pywrap_tensorflow\r\n\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in \r\nraise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\Users\\MHD\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\nreturn _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions. Include the entire stack trace\r\nabove this error message when asking for help.`\r\n", "comments": ["Couple of things I would like to check. I see that you are using Python 3.X\r\nTherefore you need to use pip3 to install TensorFlow and Keras\r\npip3 install tensorflow\r\npip3 install keras\r\nAlso are you able to start a TensorFlow session successfully?\r\nWhat's the output of following in your python console?\r\n  import tensorflow as tf", "I have installed python 3 but when I checked now I found that I have folder named `python_dateutil-2.7.5.dist-info`\r\nalso I tried    to import tensorflow from the console and I got this error:\r\n`import tensorflow as tf\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`", "I installed tensorflow on my gpu and I didn't install CUDA Tool kit 9.0 , CUDNN and Tensorflow-GPU 1.5 \r\nmy question is: could that be the reason?", "Yes you need to install those drivers for [GPU support](https://www.tensorflow.org/install/gpu) and update [environment variables](https://www.tensorflow.org/install/gpu#windows_setup).", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "conda create -n tfdeeplearning python=3.5\r\nconda install jupyter\r\nconda install numpy\r\nconda install pandas\r\nconda install scikit-learn\r\nconda install matplotlib\r\npip install --upgrade tensorflow\r\ngive me error \r\nImportError                               Traceback (most recent call last)\r\n~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\n~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nKindly help .", "> conda create -n tfdeeplearning python=3.5\r\n> conda install jupyter\r\n> conda install numpy\r\n> conda install pandas\r\n> conda install scikit-learn\r\n> conda install matplotlib\r\n> pip install --upgrade tensorflow\r\n> give me error\r\n> ImportError Traceback (most recent call last)\r\n> ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in ()\r\n> 57\r\n> ---> 58 from tensorflow.python.pywrap_tensorflow_internal import *\r\n> 59 from tensorflow.python.pywrap_tensorflow_internal import **version**\r\n> \r\n> ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in ()\r\n> 27 return _mod\r\n> ---> 28 _pywrap_tensorflow_internal = swig_import_helper()\r\n> 29 del swig_import_helper\r\n> \r\n> ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n> 23 try:\r\n> ---> 24 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> 25 finally:\r\n> \r\n> ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py in load_module(name, file, filename, details)\r\n> 242 else:\r\n> --> 243 return load_dynamic(name, filename, file)\r\n> 244 elif type_ == PKG_DIRECTORY:\r\n> \r\n> ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py in load_dynamic(name, path, file)\r\n> 342 name=name, loader=loader, origin=path)\r\n> --> 343 return _load(spec)\r\n> 344\r\n> \r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> ImportError Traceback (most recent call last)\r\n> in ()\r\n> ----> 1 import tensorflow as tf\r\n> \r\n> ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow__init__.py in ()\r\n> 22\r\n> 23 # pylint: disable=g-bad-import-order\r\n> ---> 24 from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\n> 25\r\n> 26 from tensorflow._api.v1 import app\r\n> \r\n> ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python__init__.py in ()\r\n> 47 import numpy as np\r\n> 48\r\n> ---> 49 from tensorflow.python import pywrap_tensorflow\r\n> 50\r\n> 51 # Protocol buffers\r\n> \r\n> ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in ()\r\n> 72 for some common reasons and solutions. Include the entire stack trace\r\n> 73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n> ---> 74 raise ImportError(msg)\r\n> 75\r\n> 76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n> \r\n> ImportError: Traceback (most recent call last):\r\n> File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in \r\n> from tensorflow.python.pywrap_tensorflow_internal import *\r\n> File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in \r\n> _pywrap_tensorflow_internal = swig_import_helper()\r\n> File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n> _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py\", line 243, in load_module\r\n> return load_dynamic(name, filename, file)\r\n> File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py\", line 343, in load_dynamic\r\n> return _load(spec)\r\n> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions. Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> Kindly help .\r\n\r\nDid you found any solution; I am going through the same issue since yesterday?", "I also have the same problem \r\nI have python 3.6.8\r\ncpu version \r\nmy operating system is windows 10 64bit\r\n\r\n ```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n```\r\n`During handling of the above exception, another exception occurred:\r\n`\r\n```\r\nTraceback (most recent call last):\r\n  File \"flow\", line 4, in <module>\r\n    from darkflow.cli import cliHandler\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\Downloads\\YOLO_Object_Detection-master\\darkflow\\cli.py\", line 3, in <module>\r\n    from .net.build import TFNet\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\Downloads\\YOLO_Object_Detection-master\\darkflow\\net\\build.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\\u062d\u0633\u064a\u0646\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n```\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "> > conda create -n tfdeeplearning python=3.5\r\n> > conda install jupyter\r\n> > conda install numpy\r\n> > conda install pandas\r\n> > conda install scikit-learn\r\n> > conda install matplotlib\r\n> > pip install --upgrade tensorflow\r\n> > give me error\r\n> > ImportError Traceback (most recent call last)\r\n> > ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in ()\r\n> > 57\r\n> > ---> 58 from tensorflow.python.pywrap_tensorflow_internal import *\r\n> > 59 from tensorflow.python.pywrap_tensorflow_internal import **version**\r\n> > ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in ()\r\n> > 27 return _mod\r\n> > ---> 28 _pywrap_tensorflow_internal = swig_import_helper()\r\n> > 29 del swig_import_helper\r\n> > ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n> > 23 try:\r\n> > ---> 24 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> > 25 finally:\r\n> > ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py in load_module(name, file, filename, details)\r\n> > 242 else:\r\n> > --> 243 return load_dynamic(name, filename, file)\r\n> > 244 elif type_ == PKG_DIRECTORY:\r\n> > ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py in load_dynamic(name, path, file)\r\n> > 342 name=name, loader=loader, origin=path)\r\n> > --> 343 return _load(spec)\r\n> > 344\r\n> > ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> > During handling of the above exception, another exception occurred:\r\n> > ImportError Traceback (most recent call last)\r\n> > in ()\r\n> > ----> 1 import tensorflow as tf\r\n> > ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow__init__.py in ()\r\n> > 22\r\n> > 23 # pylint: disable=g-bad-import-order\r\n> > ---> 24 from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\n> > 25\r\n> > 26 from tensorflow._api.v1 import app\r\n> > ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python__init__.py in ()\r\n> > 47 import numpy as np\r\n> > 48\r\n> > ---> 49 from tensorflow.python import pywrap_tensorflow\r\n> > 50\r\n> > 51 # Protocol buffers\r\n> > ~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in ()\r\n> > 72 for some common reasons and solutions. Include the entire stack trace\r\n> > 73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n> > ---> 74 raise ImportError(msg)\r\n> > 75\r\n> > 76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n> > ImportError: Traceback (most recent call last):\r\n> > File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in\r\n> > from tensorflow.python.pywrap_tensorflow_internal import *\r\n> > File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in\r\n> > _pywrap_tensorflow_internal = swig_import_helper()\r\n> > File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n> > _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> > File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py\", line 243, in load_module\r\n> > return load_dynamic(name, filename, file)\r\n> > File \"C:\\Users\\ibr\\Anaconda3\\envs\\tfdeeplearning\\lib\\imp.py\", line 343, in load_dynamic\r\n> > return _load(spec)\r\n> > ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> > Failed to load the native TensorFlow runtime.\r\n> > See https://www.tensorflow.org/install/errors\r\n> > for some common reasons and solutions. Include the entire stack trace\r\n> > above this error message when asking for help.\r\n> > Kindly help .\r\n> \r\n> Did you found any solution; I am going through the same issue since yesterday?\r\n\r\nSorry nothing new with me in my PC working but I am trying to install It in my Laptop give me this error Failed to load the native TensorFlow runtime.", "I have the same problem\r\n\r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\princ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "@RajdeepPy @ibrahimabob @osama12sy @Saran-nns @ymodak @MohammadDabbas\r\nok so i did a research on this erro and i found that tensorflow only support python 3.5x & 3.6x so download and install one of these 2 versions the install the packages you need and run the program. for me it worked", "thank you so much ", "@RajdeepPy @ibrahimabob @osama12sy @Saran-nns @ymodak @MohammadDabbas\r\n# Current stable release for CPU-only\r\npip install tensorflow\r\n\r\n# Preview nightly build for CPU-only (unstable)\r\npip install tf-nightly\r\n\r\n# Install TensorFlow 2.0 Alpha\r\npip install tensorflow==2.0.0-alpha0\r\n", "@osama12sy \r\nI have faced the same problem, did you solve it?\r\nplease help", "@osama12sy \r\n", "Sorry no\n\nOn 1 Feb 2020 Sat at 6:21 PM AmA <notifications@github.com> wrote:\n\n> @osama12sy <https://github.com/osama12sy>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23755?email_source=notifications&email_token=AFHXYERIJI754ZZX6UPF5G3RAWHOTA5CNFSM4GD24FE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKQ7NTQ#issuecomment-581039822>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFHXYESF2U2KQMTEQYGT6YTRAWHOTANCNFSM4GD24FEQ>\n> .\n>\n", "Having the same problem and literally checked every website and found nothing!  ", "Same issue... any suggestion?", "Please open new issues and fill in the template. Also, search for similar issues, they might have been already solved.\r\n\r\nFor Windows, please make sure the following are all checked:\r\n\r\n1. your CPU supports AVX\r\n1. you have installed the latest MSVC redistributable from Microsoft\r\n1. you have a 64 bits Python\r\n1. `pip` has been updated to the last version (`pip install --upgrade pip`)\r\n\r\nLocking conversation to prevent further comments hiding solutions"]}, {"number": 23753, "title": "Can't Enable Eager Execution [tensorflow_gpu==1.12.0]", "body": "I'm following the [Tensorflow Eager Execution Guide](https://www.tensorflow.org/guide/eager) and can't  enable eager execution.\r\n\r\nSystem info:\r\n\r\n> tensorflow_gpu 1.12.0 (std binary)\r\npython 3.6.6\r\nlinux Mint 19\r\nkernel 4.15.0-38\r\nnvidia geforce 1050ti\r\n\r\nSteps to reproduce:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ntf.executing_eagerly()\r\n# False\r\n```", "comments": ["I was able to run the code snippet successfully using TF 1.10, 1.11 and 1.12. May be you can try running it in [Google Colab](https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/guide/eager.ipynb).", "Thanks for the input.\r\n\r\nI didn't include jupyter `1.0.0` and IPython `7.1.1` versions in my software stack. \r\n\r\nI can run eager execution on colab, but not locally using a jupyter notebook. With such a simple setup, there aren't many opportunities to go astray. \r\n\r\nIt has to have something to do with how jupyter is interacting with tf. Now I'm thinking it's probably on their side. I'll look deeper into the issue and file one there if I can't pinpoint it.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23752, "title": "Fix #22455.", "body": "RdmaAdapter is allocated outside of RdmaMgr so that ibv_device is available to the static visitor registration.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@yanivbl6, @byronyi please review.", "Thanks for the patch.\r\nI tried running it and the build issue is in did fixed, but verbs itself still fails (tried the standard benchmark).\r\n\r\nMy view was that it is better to have Verbs fails during build than later  while there is WIP on fixing it, because it can save users from getting some frustrating errors that are harder to track. \r\nBut perhaps I was wrong and it's better to have partial fixes, so different people trying to work on this will not have to face the same issues. Any thoughts on the matter?", "Sorry to leave verbs hanging there for this long time; unfortunately many of us are busying migrating existing codebase to be the newly curated tensorflow/networking repo, in which verbs/gdr/mpi will reside instead of in current form, i.e. in the contrib folder. The contrib projects will be removed in TF 2.0 and tensorflow/networking will be the place for the verbs networking plugin. \r\n\r\nI will suggest you to use older versions of TF where verbs still works.", "@yanivbl6 if possible, I would like to get verbs working again sooner than later.  I agree that getting the verbs code to compile and link is misleading if it still fails at runtime.  In that sense, this patch is only a partial fix.  (As an aside, it seems perhaps the gdr code is also failing for me at runtime.)  Is there any hope of getting verbs to work again prior to the new networking repo?  After your comments, I also tried running the default cnn benchmark.  When I turned on runtime logging, it seems none of the GPU host or device allocations are getting registered by the visitors.\r\n\r\n@byronyi where can I find the new networking repo?  More information?  Is there a release date?  We have customers asking for both verbs as well as the latest versions of tensorflow, so going back to the last known working version (1.8?) isn't a great option.", "@jeffdaily AFAIK GDR works at the latest master branch. Please open a separated issue if you met any problems.", "As for the networking repo, we haven\u2019t had any set timeline. It is expected to be released at the same time of TF 2.0. You could join the mailing list and get latest information: https://groups.google.com/a/tensorflow.org/forum/?nomobile=true#!forum/networking", "@yanivbl6 which \"standard benchmark\" do you use for evaluating verbs?", "I usually do distributed resnet50 training, 2ps+ 2workers from tensorflow/benchmarks.\r\n\r\nI definitely intend to bring verbs to at least a working condition before the networking repo. So far I have been unsuccessful. I can see that some seemingly important components were added in the [last few days ](https://github.com/tensorflow/tensorflow/blame/236fc65a99cd27acbd7a425cc7ebab1d3571b9d2/tensorflow/core/common_runtime/threadpool_device_factory.cc#L54) so I will give another try tomorrow.\r\nPlease open an issue if you have problems with GDR. I had some ease of mind knowing that there is an functional alternative, and if it is not working it will most likely be easier to fix.", "I am not familiar with the code. Please find another reviewer. Thanks. ", "@yanivbl6 I used the default tf cnn benchmark model with 1ps + 1worker and grpc+verbs was running correctly from this patch.  What error(s) were you experiencing?\r\n\r\n*edit* -- It turns out I was running CPU-only.  I still need to confirm GPU grpc+verbs.", "@jeffdaily could you address `clang-format` checks? the other failures on MacOS and Windows don't seem to be related to this PR.", "@whchung Done.", "Nagging Reviewer @poxvoculi: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "I believe the issue has been fixed in #24250 without this PR. @jeffdaily @dksb could you close this?", "This is fixed in #24250."]}, {"number": 23751, "title": "Windows 10: Failed to load native runtime.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution Windows 10 64 bit\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 1.12.0\r\n- Python version:3.6.5\r\n- GPU model and memory: Geforce 150mx 4gb\r\n\r\n**Describe the problem**\r\nI'm unable to import keras using _\"Tensorflow backend\"_\r\nwhen I try to import _\"from keras.models import Sequential\"_  I get the error shown bellow\r\n\r\n**These are the steps that I did when I installed the libraries**\r\n1- conda create -n tensorflow python=3.5 anaconda.\r\n\r\n2- activate tensorflow.\r\n\r\n3- conda install theano.\r\n\r\n4- conda install mingw libpython.\r\n\r\n5-pip install tensorflow.\r\n\r\n6- pip install keras.\r\n\r\n7- conda update --all\r\n\r\n8-  pip install tensorflow-gpu\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\nError shown:\r\n\r\n`from keras.models import Sequential\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-5-9c5e0a19b646>\", line 1, in <module>\r\n    from keras.models import Sequential\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\MHD\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n\r\n", "comments": ["Duplicate of #23755 "]}, {"number": 23750, "title": "values in the output tensor", "body": "Hi, i have a frozen_graph pretrained model. and i what to know values in the output tensor in this model. How can i print it? maybe who know.", "comments": ["First you will have to load the [Graph](https://www.tensorflow.org/guide/extend/model_files#graphdef) from the saved files and later use [NodeDef](https://www.tensorflow.org/guide/extend/model_files#nodes) to extract the data from the Graph.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23749, "title": "Export train-test split in raster/vector format", "body": "I want to classify an image using support vector machine algorithm. Then, I imported samples in raster format and applied train-test split to assign 70% for training and 30% for testing. I want to export the samples after being split to vector or raster format. Is it possible to perform such things?\r\n\r\nThanks for your help.\r\n", "comments": ["You can explore ArcPy module to import/export data in raster format in Python, if haven't already. ", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23748, "title": "tensorflow.keras Dense layers complain if the input is a sparse Input layer.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nOSX Mojave 10.14.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nna\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.10.0 (v1.10.0-rc1-19-g656e7a2b34)\r\n- Python version:\r\n3.5\r\n- Bazel version (if compiling from source):\r\nNA\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nSee below, which I ran on a OSX Mojave Macbook Pro (Early 2015), ipython running python 3.5, tensorflow 1.10.0:\r\n\r\n```\r\nIn [1]: from tensorflow.keras.models import Model\r\n//anaconda/envs/dssm/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\r\n  return f(*args, **kwds)\r\n\r\nIn [2]: from tensorflow.keras.layers import Input, Dense\r\n\r\nIn [3]: i = Input((4,), sparse=True)\r\n\r\nIn [4]: d = Dense(4)(i)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-0fb73bda26dc> in <module>\r\n----> 1 d = Dense(4)(i)\r\n\r\n//anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    718\r\n    719         # Check input assumptions set before layer building, e.g. input rank.\r\n--> 720         self._assert_input_compatibility(inputs)\r\n    721         if input_list and self._dtype is None:\r\n    722           try:\r\n\r\n//anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in _assert_input_compatibility(self, inputs)\r\n   1408           spec.min_ndim is not None or\r\n   1409           spec.max_ndim is not None):\r\n-> 1410         if x.shape.ndims is None:\r\n   1411           raise ValueError('Input ' + str(input_index) + ' of layer ' +\r\n   1412                            self.name + ' is incompatible with the layer: '\r\n\r\nAttributeError: 'SparseTensor' object has no attribute 'shape'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIf I were using normal Keras, I'd expect no errors trying to do the above and for the model to compile subsequently without issue.\r\n\r\n**Code to reproduce the issue**\r\nSee the code in my snippet above.\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["@omalleyt12 -- I recall you worked on something similar recently; can you comment on what is expected here?\r\n\r\nNote that in 1.12 the above gives a different error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-0347ad7938f4> in <module>()\r\n      2 from tensorflow.keras.layers import Input, Dense\r\n      3 i = Input(shape=(4,), sparse=True)\r\n----> 4 d = Dense(4)(i)\r\n\r\ngoogle3/third_party/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    532       if not self.built:\r\n    533         # Build layer if applicable (if the `build` method has been overridden).\r\n--> 534         self._maybe_build(inputs)\r\n    535         # We must set self.built since user defined build functions are not\r\n    536         # constrained to set self.built.\r\n\r\ngoogle3/third_party/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   1592     # Only call `build` if the user has manually overridden the build method.\r\n   1593     if not hasattr(self.build, '_is_default'):\r\n-> 1594       self.build(input_shapes)\r\n   1595 \r\n   1596 \r\n\r\ngoogle3/third_party/tensorflow/python/keras/layers/core.py in build(self, input_shape)\r\n    928     input_shape = tensor_shape.TensorShape(input_shape)\r\n    929     if tensor_shape.dimension_value(input_shape[-1]) is None:\r\n--> 930       raise ValueError('The last dimension of the inputs to `Dense` '\r\n    931                        'should be defined. Found `None`.')\r\n    932     last_dim = tensor_shape.dimension_value(input_shape[-1])\r\n\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```", "@karmel @omalleyt12 I looked into this issue last week. There are several things that need to be fixed. \r\nOne is that sparse.placeholder could not recognize `(None, 4)`. It will always converts to `(None, None)` so that the error of:\r\n```\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```\r\n\r\nI created a PR #24048 to fix the above shape inforamtion issue first.\r\n\r\nThere might be some other places that need to be fixed to make @JamesGlooTeam example work though.\r\n\r\n(Note: as pointed out by @omalleyt12, the error is based in 1.12 and is different from the original error posted by @JamesGlooTeam )", "This is because for SparseTensors, the `shape` is actually a Tensor. Tensors can't have `None` values.\r\n\r\nYou can get around this on the `Input` layer by defining your `batch_size`:\r\n\r\n```python\r\nx = keras.Input(batch_size=10, shape=(4,), sparse=True)\r\n```\r\nHowever, `Dense` layers (and most layers in general it seems) don't support sparse inputs, so you would need to subclass Layer in order to call `tf.sparse.sparse_dense_matmul` on your inputs, or create a Lambda layer to convert your sparse inputs to dense.", "Ideally, the `shape` of a `SparseTensor` would probably be a `TensorShape`, but I think that would be a pretty substantial rewrite of a lot of the sparse ops in order to allow `None` values to flow through", "@omalleyt12 The PR #24048 propose to convert `(None, 4)` to `(-1, 4)` for the shape tensor then the information could be preserved without making drastic changes.", "Hi,\r\nI got the same issue, if the input is sparse then even when I apply the Dense layer I got an error.\r\nThat's too bad because to train a neural network, specifying sparse=True in the input layer makes the learning phase 5 times faster ...\r\n\r\nAnyone has a solution for that ?\r\n\r\nThanks a lot.", "> @omalleyt12 -- I recall you worked on something similar recently; can you comment on what is expected here?\r\n> \r\n> Note that in 1.12 the above gives a different error:\r\n> \r\n> ```\r\n> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-4-0347ad7938f4> in <module>()\r\n>       2 from tensorflow.keras.layers import Input, Dense\r\n>       3 i = Input(shape=(4,), sparse=True)\r\n> ----> 4 d = Dense(4)(i)\r\n> \r\n> google3/third_party/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n>     532       if not self.built:\r\n>     533         # Build layer if applicable (if the `build` method has been overridden).\r\n> --> 534         self._maybe_build(inputs)\r\n>     535         # We must set self.built since user defined build functions are not\r\n>     536         # constrained to set self.built.\r\n> \r\n> google3/third_party/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n>    1592     # Only call `build` if the user has manually overridden the build method.\r\n>    1593     if not hasattr(self.build, '_is_default'):\r\n> -> 1594       self.build(input_shapes)\r\n>    1595 \r\n>    1596 \r\n> \r\n> google3/third_party/tensorflow/python/keras/layers/core.py in build(self, input_shape)\r\n>     928     input_shape = tensor_shape.TensorShape(input_shape)\r\n>     929     if tensor_shape.dimension_value(input_shape[-1]) is None:\r\n> --> 930       raise ValueError('The last dimension of the inputs to `Dense` '\r\n>     931                        'should be defined. Found `None`.')\r\n>     932     last_dim = tensor_shape.dimension_value(input_shape[-1])\r\n> \r\n> ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n> ```\r\n\r\ni got the same error in tf1.13. Anyones has a quick local fix solution? \r\nThanks a lot!", "SparseTensors aren't supported in Dense layer currently, although this is something we are considering. You can implement a custom layer that calls `tf.sparse.sparse_dense_matmul` on your SparseTensor", "@omalleyt12 Is there some progress on this issue?\r\n\r\nI'd like to build a large sparse logistic regression model with Keras and having a dense layer supporting sparse input in Keras would be quite cool.\r\n- Should I wait for such a feature landing in Keras or should I implement my own layer?\r\n- Is there already an existing snippet with such a layer somewhere?", "Maybe, the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out : https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1", "@SharoneDayan \r\n> Maybe, the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out : https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1\r\n\r\nOn TensorFlow 2.0.0-rc0 I get \"ValueError: The two structures don't have the same nested structure.\" trying your DenseLayerForSparse layer. Using sparse inputs as to regular Dense gives the \"ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\" There doesn't seem to be any simple fix to this that doesn't involve in-depth understanding of TF2 internals..\r\n\r\nFixing the batch_size to all inputs as @omalleyt12 suggests gives the same \"two structures don't have the same nested structure\" error, and it seems that SparseTensorSpec gets the training dataset size as its first axis, whereas the SparseTensor -structure gets the batch_size as its first axis. Same when trying the DenseLayerSparse workaround.\r\n\r\nSparse inputs have worked out of the box for Keras so far, as in every other DL backend. These are a common use case with document and graph data, so it's strange that this feature is not working so close to TF2 full release, especially since SparseTensors are one of the advertised new features.", "@anttttti \r\n\r\nPossibly related:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/31999", "Can confirm that updating to >=1.14 breaks most code dealing with sparse data, which is especially relevant for graphs and text as @anttttti mentioned. \r\n\r\nThis does not seem to be an issue that can be easily worked around, and the only stable solution is to use Keras 2.2.5 (not 2.3, because that's broken too since it was made to be similar to `tf.keras`). \r\nThis seems like a huge step back and effectively makes it impossible to use sparse tensors in a Keras model.", "For tensorflow 2.0 (not rc):\r\n\r\nx = Input(shape=(32,), sparse=True)\r\ny = Dense(1, activation='sigmoid')(x)\r\n\r\nmodel = Model(x, y)\r\n\r\nI am still getting:\r\n\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n\r\nAnyone know how to resolve?", "Hi @kechan, did you figure out the solution? I'm running into the same issue too.", "@quangkevin \r\nNo. I haven't looked into it again. I thought i just use Masking for my particular model and move on. But try install tf.nightly and see if the issue is still there. it appears this \"bug\" has been opened for a long while.", "> This is because for SparseTensors, the `shape` is actually a Tensor. Tensors can't have `None` values.\r\n> \r\n> You can get around this on the `Input` layer by defining your `batch_size`:\r\n> \r\n> ```python\r\n> x = keras.Input(batch_size=10, shape=(4,), sparse=True)\r\n> ```\r\n> \r\n> However, `Dense` layers (and most layers in general it seems) don't support sparse inputs, so you would need to subclass Layer in order to call `tf.sparse.sparse_dense_matmul` on your inputs, or create a Lambda layer to convert your sparse inputs to dense.\r\n\r\nHi omalleyt12,\r\n\r\nI have tried using the tf.sparse.sparse_dense_matmul() function to convert my sparse tensor into a dense one. But it still cannot fit into a dense layer:\r\n\r\n for p, dim in zip(adjacency_powers, dim_per_power):\r\n    net_p = adj_times_x(sparse_adjacency, x, p)\r\n\r\n    with tf.variable_scope('r%i_l%i_p%s' % (replica, layer_id, str(p))):\r\n      layer = tf.layers.Dense(\r\n          dim,\r\n          kernel_regularizer=kernel_regularizer,\r\n          activation=None, use_bias=False)\r\n      net_p = layer.apply(net_p)\r\n\r\ndef adj_times_x(adj, x, adj_pow=1):\r\n  \"\"\"Multiplies (adj^adj_pow)*x.\"\"\"\r\n  for i in range(adj_pow):\r\n    x = tf.sparse_tensor_dense_matmul(adj, x)\r\n  return x\r\n\r\nand the error is still \"ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\"\r\n\r\nAny suggestions regarding this?\r\n\r\nThanks", "This is not fixed in 2.1.0 :(\r\nThis is a major blocker for the [Spektral](https://danielegrattarola.github.io/spektral/) library. Can we get some attention on this issue?", "@cgarciae -- can you file a new bug with a minimal repro in 2.x? This bug is very old, and it's hard to tell exactly what condition set doesn't work here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23748\">No</a>\n", "Hi @karmel, \r\n\r\nI just submitted the fix for this. It should be synced out shortly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23748\">No</a>\n"]}, {"number": 23747, "title": "Feature Request - Deeplab TFLITE Android Application", "body": "<em>Feature request, requesting an android application for Deeplab tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow 1.12 [tensorflow-lite-1.12 ]\r\n- Model Information: Deeplab V3 MobilenetV2\r\n- Are you willing to contribute it (Yes/No): No\r\n- Android API Version: API 24 (Android Nougat)\r\n\r\n\r\n\r\n**Feature Current Behavior**\r\nThere is no mobile application to test out the working of Deeplab tflite model in Android or IOS. This seems as a direct need for developers and it will be helpful for knowing the parsing mechanism for tflite where we get semantic predictions as an output, as there is an unclear way of parsing the specific data type in android and ios(prediction?) as it involves pixel data.\r\n\r\n**Current API: Need of change**\r\nWe are in need of new API model to help in with parsing the model input. Could be released as a subsequent fix. \r\n\r\n**Beneficiaries**\r\nPeople who are developing camera applications can directly benefit from this as it involves playing with segmentation on android devices.\r\n\r\n**Other information.**\r\nWe are trying building the application tweaking with available applications. We are currently facing up some issues. Hereby with, we are attaching the issue links.\r\n\r\n[https://stackoverflow.com/questions/53228969/unable-to-test-and-deploy-a-deeplabv3-mobilenetv2-tensorflow-lite-segmentation-m](https://stackoverflow.com/questions/53228969/unable-to-test-and-deploy-a-deeplabv3-mobilenetv2-tensorflow-lite-segmentation-m)\r\n\r\n[https://stackoverflow.com/questions/53236290/unable-to-load-tflite-deeplab-segmentation-model-in-android-application-error](https://stackoverflow.com/questions/53236290/unable-to-load-tflite-deeplab-segmentation-model-in-android-application-error)", "comments": ["+ @jdduke \r\n\r\n@SanthoshRajendiran : We have demo apps: \r\nOne example:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo\r\nThe 2 issues that you reference seem to be due to model conversion.\r\nCan you attach your TF lite model with this issue", "I believe @suharshs has looked into Deeplab support with TensorFlow Lite.", "Thanks for the prompt response @shashishekhar @jdduke. I have gone through the app link you have provided. That is specific for Image Classification, I think it cannot be directly used for image segmentation. \r\n\r\nWith response to your query, I am attaching here both the frozen graph (From the official repo - both 8MB and 3MB) and the tflite we created from the 8MB model. We have not been able to convert the 3MB Model. Will be helpful if you could support us with the conversion command for 3MB model.\r\n\r\nThe command for conversion to tflite, is given in the stackoverflow link provided above\r\n\r\n[deeplab_tflite_issues.zip](https://github.com/tensorflow/tensorflow/files/2598408/deeplab_tflite_issues.zip)\r\n", "Hello guys. Waiting for your reply.. Any improvements as of now. Do you need any help from our side??", "I have the same issue with you on model convert. for the real time segmentation you can ref to https://github.com/tantara/JejuNet, seems that project has successfully converted the model to tflite. but the accuracy is not that good.", "Hi @kismeter . Ya. I happened to see the code in Jejunet, but could not find it useful with relevance to the provided deeplab model. It seems that the model is tweaked in various stages to make it run on the device. Anyway, in Jejunet, only tflite is available, no references to Deeplab or pb file. I am in need of converting pb file to tflite and deploying it on the device.", "I am also facing the same issue. Specifically I'm trying to convert\r\n\r\nhttp://download.tensorflow.org/models/deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz\r\n\r\ninto a tflite file. Optimally also quantization but before that I already ran onto this problem\r\n\r\nusing this:\r\n```\r\n\r\ntflite_convert --output_file model/test2.tflite\r\n --graph_def_file deeplabv3_mnv2_cityscapes_train_2018_02_05/deeplabv3_mnv2_cityscapes_train/frozen_inference_graph.pb  \r\n\r\n--input_arrays ImageTensor \r\n\r\n--output_arrays SemanticPredictions \r\n\r\n--input_shapes=1,513,513,3\r\n```\r\n\r\nI get \r\n\r\n`Check failed: array.data_type == array.final_data_type Array \"ImageTensor\" has mis-matching actual and final data types (data_type=Uint8, final_data_type=Float).`\r\n\r\nany help would be highly appreciated.\r\n\r\n", "Hi @normandra \r\nWe are going through the same process. Actually, we have been able to overcome the same issue you are facing, by providing the parameters: inference-input type and inference type. For the conversion command, do check out the stack overflow link mentioned above. Actually, we are facing some other issue in tflite conversion as discussed above and are currently waiting for @suharshs to reply back. \r\n\r\nIn your case, if you have not explicitly mentioned the inference type parameter. by default it will take the input type as inference input type. By default, FLOAT becomes the inference type (You can set it to QUANTIZED UINT8 also). Actually, updates in tflite conversion. forces on usage of Quantized UINT8 and Float data types.", "@tensorflowbutler @suharshs Any updates on the feature request???", "watching this...", "Thanks for the info @SanthoshRajendiran . I already tried specifying the input / inference type and I seem to be getting either the same error or the Nonetype error. \r\n\r\nEDIT: Okay now I'm facing the exact same problem you are ( ByteBuffer is not a valid flatbuffer model ).\r\nI guess we can only wait now.", "Hi sorry for the delay, thanksgiving holidays and such. We will take a look into reproducing your issue. Thanks!", "@sandeepngupta  @normandra \r\nHi I found it is easy to solve the problem. In my case tf.image.ResizeMethod.NEAREST_NEIGHBOR and the slice operator are not supported, therefore when exporting the model with export_model.py, I do not include the two operators. Below is how I modify the code to export proper '.pb'.\r\n```\r\n    # Crop the valid regions from the predictions.\r\n    # enable tflite for exporting tflite model\r\n    if not FLAGS.tflite:\r\n        semantic_predictions = tf.slice(\r\n            predictions[common.OUTPUT_TYPE],\r\n            [0, 0, 0],\r\n            [1, resized_image_size[0], resized_image_size[1]])\r\n\r\n    # Resize back the prediction to the original image size.\r\n    def _resize_label(label, label_size):\r\n      # Expand dimension of label to [1, height, width, 1] for resize operation.\r\n      label = tf.expand_dims(label, 3)\r\n      resized_label = tf.image.resize_images(\r\n          label,\r\n          label_size,\r\n          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\r\n          align_corners=True)\r\n      return tf.squeeze(resized_label, 3)\r\n    if FLAGS.tflite:\r\n        semantic_predictions = tf.cast(predictions[common.OUTPUT_TYPE], tf.uint8)\r\n        semantic_predictions = tf.squeeze(semantic_predictions)\r\n    else:\r\n        semantic_predictions = _resize_label(semantic_predictions, image_size)\r\n\r\n    semantic_predictions = tf.identity(semantic_predictions, name=_OUTPUT_NAME)\r\n```\r\n\r\nThe operators not supported can be implemented in your Java or Python code.", "Thanks for sharing @melody-rain ,\r\n\r\ndo you mind on elaborating how to implement the unsupported ops in java / python ? I'm hit with a segenv in my android wrapper so I'm not sure what to do there. Testing in python for some weird reason regardless of what my input tensor is set to i get the same output.\r\n\r\n![whitenoise](https://user-images.githubusercontent.com/8277940/49375664-53586800-f705-11e8-807d-0b4d6d745e1a.jpg)\r\n\r\n\r\nEDIT: I guess not exactly the same everytime but very similiar", "@normandra \r\nslice op is used to crop the image, so you can use python's slice. \r\nresize can also be replace with opencv's resize. ", "Okay but at what point would I implement those? My output from the model currently is a 513x513 tensor pictured above.", "Seems like there are number of subproblems in this issue. I'm trying to convert the 3MB mobilenetv2_dm05_coco_voc_trainval model and see what may be missing", "For the 3MB model, I found the cause of the TOCO error. I'm working with internal engineers to figure out what the proper fix is.", "I am also running into issues with converting the frozen graphs, although they're a bit different depending on which graph i try to convert to tensorflow lite. When converting the graph found from http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz via TOCO with the command:\r\n```\r\ntflite_convert \\\r\n  --output_file=test.lite \\\r\n  --graph_def_file=frozen_inference_graph.pb \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=SemanticPredictions \\\r\n  --input_shapes=1,513,513,3 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --inference_type=FLOAT \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=128\r\n```\r\nI get no issues in the conversion. But when i try and implement it in my app on Android I get the `java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/contrib/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 32)Node number 30 (DEPTHWISE_CONV_2D) failed to prepare.` error.\r\n\r\nHowever when i try to retrain the model on my local machine with `tensorflow 1.12` using the `local_test_mobilenetv2.sh` script in the tensorflow-models repo for Deeplab i get the following error when trying to convert the exported graph with the same `tflite_convert` command: \r\n```\r\n2018-12-07 10:29:51.195230: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 453, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 342, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2018-12-07 10:29:53.090897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-12-07 10:29:53.104921: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 811 operators, 1236 arrays (0 quantized)\r\n2018-12-07 10:29:53.131609: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 801 operators, 1217 arrays (0 quantized)\r\n2018-12-07 10:29:53.160096: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 801 operators, 1217 arrays (0 quantized)\r\n2018-12-07 10:29:53.186542: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:624] Check failed: input_shape.dims().size() == op->size.size() (4 vs. 3)\r\n\r\nNone\r\n```\r\n\r\nThis is the same error i get when i try to run the `tflite_convert` command on the frozen graph from http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainaug_2018_10_01.tar.gz. \r\n\r\nAlso just a side note: I tried running the `tflite_convert` command on the frozen graph from http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz just to see what would happen, and the conversion seemed to work. But i got to the same error point when actually running inference in the app with the model with nearly the same error message `java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/contrib/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 64)Node number 33 (DEPTHWISE_CONV_2D) failed to prepare.` Just a difference of `SizeOfDimension(filter, 3) (0 != 64)` vs ` SizeOfDimension(filter, 3) (0 != 32)` in the mobilenetsv2 model.\r\n\r\nI'm relatively new to TensorFlow/ML so forgive me for any extraneous info or misuse of terminology. I'm just trying to start out with trying to implement the Deeplabv3+ model on Android with a pretrained model for now. Not sure if there is a workaround? I looked at https://github.com/dailystudio/ml/tree/master/deeplab implementation of the Deeplab model on Android, but it appears to use the soon to be deprecated version Tensorflow Mobile API as opposed to Tensorflow Lite. I also am unsure as to whether a model that is trained with the current version of Tensorflow would work with this demo app (I'll soon try) since this codebase is from May (not sure which version was used to generate the graph that was used in the demo).", "So I did follow @melody-rain's advice and added their code into `export_model.py` and ran the `tflite_convert` command on the frozen graph. Had to make some other adjustments to my java code and got the model to successfully run in my android app without crashing/raising errors. The resulting graph for the tflite conversion appears to produce something... i'm just not sure what.\r\n\r\nhere is an inference result from the frozen graph without the lite conversion, which i ran via python on my Macbook Pro:\r\n![test-something](https://user-images.githubusercontent.com/6912634/49668614-b7906a00-fa2c-11e8-99af-1c1a32a9d71b.jpeg)\r\n\r\nand here is the same inference result from the graph with the conversion, which i ran through my android app on a Samsung Galaxy Tab A:\r\n![tf-pixpic_1544210110162](https://user-images.githubusercontent.com/6912634/49668457-27522500-fa2c-11e8-8f62-1e1ed761d5c8.jpg)\r\n\r\nIts an improvement from not working at all, however i'm not sure what exactly the results from the lite conversion means. The results from the non converted model are pretty decent. Not sure if my conversion from the output results in the java code is working incorrectly or if it is just an issue with the conversion. ", "@SirNeuman: Just to confirm, have you tried without enabling quantization during conversion (i.e., using the float path)?", "Sorry. I was actually messing up my input image when i resized it for running through the inference on mobile. It actually works quite well now. This is a different input image running for inference, but you can see that the results actually make a bit more sense and is close to what i'm trying to achieve:\r\n![tf-quantized-20181210_104010](https://user-images.githubusercontent.com/6912634/49747841-4b06ac80-fc72-11e8-877c-c911b11189cf.jpg)\r\n\r\n\r\nI still need to implement the java code for scaling the output image...\r\n\r\n@jdduke: I assume you mean changing `--inference_input_type=QUANTIZED_UINT8` to `--inference_input_type=FLOAT` in my conversion command? I just went ahead and tried it to see what the results would be like. I had to change the `input_image` when exporting the model from type `uint8` to `float32` to get the conversion to run. I then also had to update my convertBitmapToByteBuffer method when running the image through the model in the app. I ended up getting it to run without errors and ended up with pretty similar results (not sure if it's more accurate but it definitely runs inference faster?). Just to note this produces an 8.5mb lite graph as opposed to a 2.2mb lite graph that my original conversion produced:\r\n![tf-20181210_104010](https://user-images.githubusercontent.com/6912634/49747875-5bb72280-fc72-11e8-8b00-9c529cbe27e8.jpg)\r\n\r\nHere's with the unconverted (non-lite) model for reference:\r\n![api-test-2](https://user-images.githubusercontent.com/6912634/49747909-71c4e300-fc72-11e8-8dca-db19465b7e1f.jpeg)\r\n\r\nAlso the time for inference with the original uint8 type was 10335 ms, while the time for inference with the float type was 4455ms on my Galaxy Samsung Tab A. Which does seem strange to me that the quantized version performs worse, but once again I'm relatively new to this so perhaps i'm either doing something wrong or i'm misunderstanding the changes i'm making.\r\n", "> For the 3MB model, I found the cause of the TOCO error. I'm working with internal engineers to figure out what the proper fix is.\r\n\r\nHello @alanchiao , we are keen to know the response from your internal engineers team for the TOCO error. How long do you think you need to provide a solution to successfully convert 3MB pb model into tflite? Thanks for your continued support.", "> Sorry. I was actually messing up my input image when i resized it for running through the inference on mobile. It actually works quite well now. This is a different input image running for inference, but you can see that the results actually make a bit more sense and is close to what i'm trying to achieve:\r\n> ![tf-quantized-20181210_104010](https://user-images.githubusercontent.com/6912634/49747841-4b06ac80-fc72-11e8-877c-c911b11189cf.jpg)\r\n> \r\n> I still need to implement the java code for scaling the output image...\r\n> \r\n> @jdduke: I assume you mean changing `--inference_input_type=QUANTIZED_UINT8` to `--inference_input_type=FLOAT` in my conversion command? I just went ahead and tried it to see what the results would be like. I had to change the `input_image` when exporting the model from type `uint8` to `float32` to get the conversion to run. I then also had to update my convertBitmapToByteBuffer method when running the image through the model in the app. I ended up getting it to run without errors and ended up with pretty similar results (not sure if it's more accurate but it definitely runs inference faster?). Just to note this produces an 8.5mb lite graph as opposed to a 2.2mb lite graph that my original conversion produced:\r\n> ![tf-20181210_104010](https://user-images.githubusercontent.com/6912634/49747875-5bb72280-fc72-11e8-8b00-9c529cbe27e8.jpg)\r\n> \r\n> Here's with the unconverted (non-lite) model for reference:\r\n> ![api-test-2](https://user-images.githubusercontent.com/6912634/49747909-71c4e300-fc72-11e8-8dca-db19465b7e1f.jpeg)\r\n> \r\n> Also the time for inference with the original uint8 type was 10335 ms, while the time for inference with the float type was 4455ms on my Galaxy Samsung Tab A. Which does seem strange to me that the quantized version performs worse, but once again I'm relatively new to this so perhaps i'm either doing something wrong or i'm misunderstanding the changes i'm making.\r\n\r\n@SirNeuman Did you include any custom implementations for Slice and Resize operators which were excluded in export_model.py file?\r\nCan we please know the Samsung Tab A device specifications, as we are equally surprised to know the long inference times?", "@Srinivas-Introtuce: since the day I provided the last response, I've been trying to verify the below analysis and fix with a SWE from another team and unfortunately they haven't gotten back. Provided it is correct, it would expect it to take at most two days after, including a day to a day and a half for code review. \r\n\r\nIf you're not familiar with or interested in TOCO internals, please ignore the following:\r\n\r\nIn a build of the latest source code, I saw the following error: Check failed: input_shape.dims().size() == op->size.size() (4 vs. 3) at propagate_fixed_sizes.cc:625. \r\n\r\npropagate_fixed_sizes is a graph transformation that takes the shapes of the model input tensors and propagates them throughout the graph to compute the shapes of each op's input and output tensors. \r\n\r\nThe error is thrown for the Slice Op that follows an Argmax Op. For this graph, the argmax input shape is [1, 513, 513, 21] and the computed output shape from propate_fixed_sizes.cc is [1, 513, 513, 1]. This input shape goes to the slice op (input_shape.dims().size() = 4 with the 4 elements), with the existing size param = [1, 256, 256] (op->size.size = 3 with the 3 elements), leading to the 4 != 3 error.\r\n\r\nTypically, for argmax, the output shape would be [1, 513, 513], but I'm guessing that the 1 was appended to the end of the output shape since TOCO's in-memory representation works with 4D tensors. The fix would to be to modify the Slice operator's in-memory representation to match the Argmax behavior.", "> The error is thrown for the Slice Op that follows an Argmax Op. For this graph, the argmax input shape is [1, 513, 513, 21] and the computed output shape from propate_fixed_sizes.cc is [1, 513, 513, 1]. This input shape goes to the slice op (input_shape.dims().size() = 4 with the 4 elements), with the existing size param = [1, 256, 256] (op->size.size = 3 with the 3 elements), leading to the 4 != 3 error.\r\n\r\nHi @alanchiao - Thank you very much for the response. Yes myself and my team got fair understanding of the error based on your detailed description. We are looking forward to receive the respective fix and waiting to successfully convert 3mb pb file into tflite. Again, thank you for the continued support.", "@Srinivas-Introtuce yeah. i just cropped and resized the image before/after running the image through the graph in our android application code. \r\n\r\nMy GalaxyTab A specs are:\r\nModel Sm-T580\r\n1.6GHz Octa Core Processor\r\n2GB RAM\r\nAndroid 8.1\r\n", "Finally after fiddling around I was able to make my implementation work as well.\r\n\r\nInterestingly enough my result is also similar to @SirNeuman 's where the float model performed faster than the quantized one.\r\n\r\nOn a HTC U11 (Snapdragon 835)\r\nhttps://www.gsmarena.com/htc_u11-8630.php\r\n\r\nwith the input_size set to 300x300 I was able to reach:\r\n~470ms on the float model (8.5 mb)\r\n~830ms on the Quantized model (2.2 mb)", "If you don't mind sharing your converted .tflite model (either here or sending to me directly), we'd be happy to dive into performance issues.\r\n\r\nOtherwise, it might be helpful if you could profile the model per [these instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) (under the \"Profiling model operators\" section). Thanks!", "@normandra , @Srinivas-Introtuce : to make sure, when you say quantized model, you are referring to models generated using the post_training_quantize flag right?  \r\n\r\nFor performance, are you measuring with single-threaded performance? It makes a difference between with post_training_quantize, we currently don't support multithreading whereas we do for the float model. This is because in practice on a user's phone with various other applications running, multithreading is often slower than single threaded performance due to the contention.", "Yes, I will upload my converted tflite tomorrow\r\n\r\nOn Dec 13, 2018 7:27 PM, alanchiao <notifications@github.com> wrote:\r\n\r\n@normandra<https://github.com/normandra> , @Srinivas-Introtuce<https://github.com/Srinivas-Introtuce> : to make sure, when you say quantized model, you are referring to models generated using the post_training_quantize flag right?\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/23747#issuecomment-447069814>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AH5PtKHVKa2e-dreNqYBt8IDOwFCkUsCks5u4pwMgaJpZM4YeDVJ>.\r\n\r\n", "So here are the two models:\r\n\r\n[models.zip](https://github.com/tensorflow/tensorflow/files/2679973/models.zip)\r\n\r\nSo I used the tip from @melody-rain to remove the unsupported ops and to convert the models.\r\n\r\nTo convert the model I used:\r\n\r\n`tflite_convert --output_file 300_quantized.tflite --graph_def_file 300.pb --input_arrays ImageTensor --output_arrays SemanticPredictions --input_shapes=1,300,300,3 --inference_input_type=QUANTIZED_UINT8 --inference_type=FLOAT --mean_values=128 --std_dev_values=128 --post_training_quantize\r\n`\r\n\r\nand for the other without the --post_training_quantize flag", "Hi @normandra ,\r\n\r\nWe tried running your tflite model on OnePlus 3 (Snapdragon 820) and the inference time was around 2000 ms (both for single and double threaded execution) for 2.2 Mb model. How did you measure the execution time (is it just the running time for 'tflite.run()' of tflite intrepreter)?Also did you use nnapi (HTC U11 seems to support Android Pie) and how many threads were used in your case?\r\n\r\n(So far the best performance that we found in terms of speed, is around 150 ms  using [Jejunet](https://github.com/tantara/JejuNet) with input size of 256*256 and and INT64 output )", "@anilsathyan7 I used 4 Threads. NNAPI doesn't seem to make any difference whatsoever on this device. To test the runtime I have made an app similiar to the one used in Jejunet which again is similiar to the one from the tflite / tfmobile demo provided by the tensorflow team.\r\n\r\nSomething to note about Jejunet is that its a modified version of deeplab. Some layers are missing there (Batchnormalization for example) presumably to boost the speed. It also seem that the model was trained from scratch. ", "@alanchiao I did test the quantized model on a single threaded run, and the quantized model did not perform better there.", "Hello @alanchiao . Any updates on the feature request?? And thanks @normandra for the information.", "@melody-rain \r\nI followed your code and modified the export_model.py file accordingly\r\n\r\nbut got an error\r\n```INFO:tensorflow:Prepare to export model to: /home/jayanthl/Documents/ai/tensorflow_models/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set_mobilenetv2/export/frozen_inference_graph.pb\r\nINFO:tensorflow:Exported model performs single-scale inference.\r\nWARNING:tensorflow:From /home/jayanthl/Documents/ai/tensorflow_models/models/research/deeplab/core/feature_extractor.py:160: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nWARNING:tensorflow:From /home/jayanthl/.pythonvenv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nTraceback (most recent call last):\r\n  File \"/home/jayanthl/Documents/ai/tensorflow_models/models/research/deeplab/export_model.py\", line 166, in <module>\r\n    tf.app.run()\r\n  File \"/home/jayanthl/.pythonvenv/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/jayanthl/Documents/ai/tensorflow_models/models/research/deeplab/export_model.py\", line 142, in main\r\n    semantic_predictions = tf.cast(predictions[common.OUTPUT_TYPE], tf.uint8)\r\n  File \"/home/jayanthl/.pythonvenv/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 618, in _slice_helper\r\n    _check_index(s)\r\n  File \"/home/jayanthl/.pythonvenv/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 516, in _check_index\r\n    raise TypeError(_SLICE_TYPE_ERROR + \", got {!r}\".format(idx))\r\nTypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'semantic'```", "@SirNeuman \r\nCan you please post the tflite_convert command which you used to convert the model ?", "@melody-rain \r\nif i use python 3.6.8 i get the following error\r\n```INFO:tensorflow:Prepare to export model to: /home/jayanthl/Documents/ai/tensorflow_models/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set_mobilenetv2/export/frozen_inference_graph.pb\r\nINFO:tensorflow:Exported model performs single-scale inference.\r\nTraceback (most recent call last):\r\n  File \"/home/jayanthl/Documents/ai/tensorflow_models/models/research/deeplab/export_model.py\", line 166, in <module>\r\n    tf.app.run()\r\n  File \"/home/jayanthl/.source_pythonvenv/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/jayanthl/Documents/ai/tensorflow_models/models/research/deeplab/export_model.py\", line 142, in main\r\n    semantic_predictions = tf.cast(predictions[common.OUTPUT_TYPE], tf.uint8)\r\n  File \"/home/jayanthl/.source_pythonvenv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 491, in _slice_helper\r\n    end.append(s + 1)\r\nTypeError: must be str, not int\r\n```\r\n\r\n@SirNeuman \r\ncan you please post the export_model.py file ?\r\n\r\n*** Update ***\r\nIt was my fault, figured it out.", "@Jayanth-L \r\n\r\nChange\r\n```\r\n    predictions = tf.cast(predictions[common.OUTPUT_TYPE], tf.float32)\r\n```\r\nto:\r\n```\r\n    if not FLAGS.tflite:\r\n        semantic_predictions = tf.slice(\r\n            predictions[common.OUTPUT_TYPE],\r\n            [0, 0, 0],\r\n            [1, resized_image_size[0], resized_image_size[1]])\r\n\r\n```", "@SanthoshRajendiran : starting to code the fix now. Apologize for the delay", "Hi guys, you guys might need to see this post.\r\nhttps://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/rzLivfAUGLk/EOZN2LyWCgAJ\r\n\r\nOne guy release tf-lite model of segmentation", "I took a look at it. Aside from the final Reshape layer that model is pretty much identical with the one from JeJunet. So I guess to speed up the model we probably have to remove the BatchToSpaceNd + Mul + Add blocks in the feature extraction part. Some advice to do so would be great.", "@Srinivas-Introtuce @SanthoshRajendiran : after [this commit](https://github.com/tensorflow/tensorflow/commit/1d0552e94d4e091ce03248675ac92c1e1e0accae), you'll be able to convert the 3MB Deeplab TF graphdef to a TFLite model. I unfortunately don't have the cycles right now to hook everything up into the demo app.", "We tried the official tensorflow experimental gpu backend for inference on andorid by following the following links:-\r\n1. https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7\r\n2. https://www.tensorflow.org/lite/performance/gpu\r\n\r\nThe speed up for mobilenet v1 and mobilenet v2 was around '2x' when compared to float model\r\non OnePlus3(Snapdrahon 820, Adreno 530).Here are the benchmarks for mobilenet v2:-\r\n\r\nQuantized CPU: 80 ms\r\nFloat CPU: 80ms\r\nFloat GPU:40ms\r\n\r\nHowever when we tried the given deeplab segmentation model, the time taken for a frame was around\r\n500ms in CPU float and 400ms in GPU on this device.This is still less than jejunet quantized inference(150ms).\r\n\r\nWe hope these are some other techniques for improving  the performance\r\n1.Training using a single class.(application specific)\r\n2.Converting output from 1*256*256*21 to 1*65536 (jejunet style)\r\n3.Removing some layers or operators (as per previous comments)\r\n\r\nOur target is 30 FPS on android.Are there any other ways the improve the performance?\r\n\r\n", "@anilsathyan7 Did you implement android code with deeplabv3_257_mv_gpu.tflite? I did try but time taken 900ms.\r\n\r\nI had a problem to inference. Can you see my code and give me comment? \r\n\r\nPlease reference below.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/25193\r\n\r\nthanks.", "Added iOS example, waiting for [pull request 25785 approve](https://github.com/tensorflow/tensorflow/pull/25785).  \r\n\r\n![real-time](https://github.com/VolodymyrPavliukevych/DeepLabApp/raw/master/real_time.gif)\r\n\r\n![static-image](https://github.com/VolodymyrPavliukevych/DeepLabApp/raw/master/static_images.gif)\r\n", "> I am also running into issues with converting the frozen graphs, although they're a bit different depending on which graph i try to convert to tensorflow lite. When converting the graph found from http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz via TOCO with the command:\r\n> \r\n> ```\r\n> tflite_convert \\\r\n>   --output_file=test.lite \\\r\n>   --graph_def_file=frozen_inference_graph.pb \\\r\n>   --input_arrays=ImageTensor \\\r\n>   --output_arrays=SemanticPredictions \\\r\n>   --input_shapes=1,513,513,3 \\\r\n>   --inference_input_type=QUANTIZED_UINT8 \\\r\n>   --inference_type=FLOAT \\\r\n>   --mean_values=128 \\\r\n>   --std_dev_values=128\r\n> ```\r\n> I get no issues in the conversion. But when i try and implement it in my app on Android I get the `java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/contrib/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 32)Node number 30 (DEPTHWISE_CONV_2D) failed to prepare.` error.\r\n> \r\n\r\nHow did you resolve this issue @SirNeuman ? I am having the same problem trying to inference Deeplab v3+ ResNet 101 variant in tflite format. Conversion is succesful with no errors and the model works fine before conversion. I tried to adjust the export_model file according to melody-rain's tips, but it did not fix the problem.", "Could someone provide the command to convert deeplab pb file into a fully quantized tflite, along with some insights on the mean, std_dev_values and min-max ranges.", "for quantized runtime I believe you also need to do quantization aware training, see https://github.com/tensorflow/tensorflow/issues/20867#issue-341770125", "> With response to your query, I am attaching here both the frozen graph (From the official repo - both 8MB and 3MB) and the tflite we created from the 8MB model. We have not been able to convert the 3MB Model. Will be helpful if you could support us with the conversion command for 3MB model.\r\n\r\n\r\n@SanthoshRajendiran . Can you attach here a link (wget \"https:/...\") to small_frozen_inference_graph.pb file whose size is 3mb I am having hard time locating it on tensorflows official website", "could tflite use gpu of phone?", "\r\n> > With response to your query, I am attaching here both the frozen graph (From the official repo - both 8MB and 3MB) and the tflite we created from the 8MB model. We have not been able to convert the 3MB Model. Will be helpful if you could support us with the conversion command for 3MB model.\r\n> \r\n> @SanthoshRajendiran . Can you attach here a link (wget \"https:/...\") to small_frozen_inference_graph.pb file whose size is 3mb I am having hard time locating it on tensorflows official website\r\n\r\n\r\n[http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainaug_2018_10_01.tar.gz](http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainaug_2018_10_01.tar.gz)\r\n\r\nThese models are available in [deeplab model zoo page](https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md)", "> could tflite use gpu of phone?\r\n\r\nYes. It can use the Phone GPU, using TFLite GPU Delegate. Make sure that the operators in your TFLite model are supported by TFLite GPU delegate.", "@SanthoshRajendiran  can the checkpoints derived from this 3mb model-directory be used to custom train your own data-set ?\r\n\r\n> http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainaug_2018_10_01.tar.gz\r\n\r\nI used similar approach and used checkpoints from the 8mb model directory to train my own dataset without any trouble\r\n\r\nThe command I use in both the cases is this:\r\n\r\n```\r\npython3 deeplab/train.py \\\r\n    --logtostderr \\\r\n    --train_split=\"train\" \\\r\n    --training_number_of_steps=1000 \\\r\n    --model_variant=\"mobilenet_v2\" \\\r\n    --output_stride=16 \\\r\n    --train_crop_size=600 \\\r\n    --train_crop_size=450 \\\r\n    --train_batch_size=1 \\\r\n    --dataset=\"testset\" \\\r\n    --tf_initial_checkpoint=deeplab/deeplabv3_mnv2_dm05_pascal_trainaug/model.ckpt\\\r\n    --train_logdir=deeplabdeeplab/dm05_pascalvoc/ \\\r\n    --dataset_dir=deeplab/datasets/CamVid/tfrecord \\\r\n    --initialize_last_layer=False \\\r\n    --last_layers_contain_logits_only=False \\\r\n    --fine_tune_batch_norm=False\r\n```\r\n\r\nHowever for 3mb model I get an error:\r\n\r\n> ```python\r\n>     sess = tf.Session()\r\n>     with sess.as_default():\r\n>         tensor = tf.range(10)\r\n>         print_op = tf.print(tensor)\r\n>         with tf.control_dependencies([print_op]):\r\n>           out = tf.add(tensor, tensor)\r\n>         sess.run(out)\r\n>     ```\r\n> Additionally, to use tf.print in python 2.7, users must make sure to import\r\n> the following:\r\n> \r\n>   `from __future__ import print_function`\r\n> \r\n> INFO:tensorflow:Initializing model from path: deeplab/deeplabv3_mnv2_dm05_pascal_trainaug/model.ckpt\r\n> Traceback (most recent call last):\r\n>   File \"deeplab/train.py\", line 500, in <module>\r\n>     tf.app.run()\r\n>   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n>     _sys.exit(main(argv))\r\n>   File \"deeplab/train.py\", line 467, in main\r\n>     ignore_missing_vars=True)\r\n>   File \"/home/ubuntu/ajinkya/models/research/deeplab/utils/train_utils.py\", line 174, in get_model_init_fn\r\n>     ignore_missing_vars=ignore_missing_vars)\r\n>   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 686, in assign_from_checkpoint\r\n>     % (ckpt_name, str(ckpt_value.shape), str(var.get_shape())))\r\n> ValueError: Total size of new array must be unchanged for MobilenetV2/Conv/weights lh_shape: [(3, 3, 3, 16)], rh_shape: [(3, 3, 3, 32)]\r\n> \r\n\r\nAny suggestion on how do I resolve this", "You need to include one more parameter, in command you use for training 3MB model.. Depth multiplier is used in this case.. And so, you have to include an additional parameter.. \r\n\r\n--depth_multiplier=0.5\r\n\r\n", "thank you that worked\r\n", "@SanthoshRajendiran did you have any luck running tflite model on android. I converted the frozen inference graph to tflite graph using \r\n\r\n> tflite_convert \\\r\n>   --output_file=test.lite \\\r\n>   --graph_def_file=frozen_inference_graph_3mbvoc.pb \\\r\n>   --input_arrays=ImageTensor \\\r\n>   --output_arrays=SemanticPredictions \\\r\n>   --input_shapes=1,600,450,3 \\\r\n>   --inference_input_type=QUANTIZED_UINT8 \\\r\n>   --inference_type=FLOAT \\\r\n>   --mean_values=128 \\\r\n>   --std_dev_values=128\r\n\r\nWhen I ran it on android I get:\r\n\r\n`Caused by: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: third_party/tensorflow/lite/kernels/unpack.cc:54 NumDimensions(input) > 1 was not true.Node number 4 (UNPACK) failed to prepare.`\r\n\r\nAny idea whats happening here, and how to resolve it ?", "@ajinkya933 would you mind sharing the .tflite model you've converted? And/or the saved model you used for conversion?", "@jdduke saved model used for conversion is available [here](https://drive.google.com/file/d/1_pEkX6BZZjyrHkytiN83rueOniNa1POX/view?usp=sharing). tflite model that I have converted is available [here](https://drive.google.com/file/d/1x-tPCGjGcirxeMpjee_NE3MkiQuBYX0p/view?usp=sharing). Please let me know if you cannot access the links. Training set Images width =600, height =450", "The model is to detect credit card from images. I am resizing the image to 513X385 in android studio and running inference on this resized image. I tried putting --input_shapes=1,450,600,3 \\, and --input_shapes=1,513,385,3 \\ It removes the above error in both cases but it dosent seem to produce any output. This is very strange as the \" .pb\" file for the same model is working perfectly alright using tf-mobile on android. I want inference to be faster in milliseconds than tfmobile thats why I am trying tflite\r\n", "Thanks for the repro steps, we're investigating internally.", "@jdduke thanks, this may add a more clear picture. Please take a look at image below:\r\n\r\n![Screenshot from 2019-04-23 14-42-20](https://user-images.githubusercontent.com/17012391/56710895-f443d000-6745-11e9-815d-1ffe673a1c9e.png)\r\n\r\nThe graph on left is my [tflight-graph](https://drive.google.com/file/d/1uWcOCAHBcYwwf0yQSIJLlrctFNSr91oq/view). and the graph on right is googles [tflight-graph](https://drive.google.com/file/d/1wJ9RMFuZqs_TOW27ZPK6feToqmnNbepw/view).\r\n\r\nHow is googles tflight-graph created from \" .pb\" file? how can I replicate this process so that my graph on left looks like googles graph on the right? My assumption is that if I create graph same as google it may give me semantic predictions on my custom trained objects properly. This assumption my be incorrect.\r\n", "Google's tflite graph does not have post processing as well as pre-processing. In case if you want to replicate the same, strip your frozen graph from sub_7 to ResizeBilinear_2, and then do the tflite conversion.", "@SanthoshRajendiran   I researched, there isn't a clean way to remove nodes from a graph, so removing a subgraph isn't practical", "Hope you would have gone through graph transformations, there you can do all the required stuff to meet your needs. ", "@VolodymyrPavliukevych \r\nsir, if you don't mind I want to ask you about your app , using tflite deeplab model on ios application,is there any blog should I follow?", "Hi, I meet the same problems like this when I'm trying to convert frozen_graph to tfLite:\r\n\r\n`Traceback (most recent call last):\r\n  File \"tfdm.py\", line 12, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/zhoukeyang/anaconda3/envs/tf2/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 453, in convert\r\n    **converter_kwargs)\r\n  File \"/home/zhoukeyang/anaconda3/envs/tf2/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 342, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/zhoukeyang/anaconda3/envs/tf2/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2019-06-24 19:49:16.463291: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 852 operators, 1282 arrays (0 quantized)\r\n2019-06-24 19:49:16.486382: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 842 operators, 1263 arrays (0 quantized)\r\n2019-06-24 19:49:16.513770: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 842 operators, 1263 arrays (0 quantized)\r\n2019-06-24 19:49:16.537679: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 102 operators, 256 arrays (0 quantized)\r\n2019-06-24 19:49:16.539015: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 102 operators, 256 arrays (0 quantized)\r\n2019-06-24 19:49:16.539768: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:624] Check failed: input_shape.dims().size() == op->size.size() (4 vs. 3)\r\nAborted (core dumped)`\r\n\r\nCould you please give me some advice about fixing it? Thanks a lot.", "@SirNeuman \r\n\r\n> tflite_convert \\\r\n  --output_file=test.lite \\\r\n  --graph_def_file=frozen_inference_graph.pb \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=SemanticPredictions \\\r\n  --input_shapes=1,513,513,3 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --inference_type=FLOAT \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=128\r\n\r\n> Sorry. I was actually messing up my input image when i resized it for running through the inference on mobile. It actually works quite well now. This is a different input image running for inference, but you can see that the results actually make a bit more sense and is close to what i'm trying to achieve:\r\n\r\nI want to make sure if is it work for you on android with 513 as a crop_size??\r\nbecause when i export and convert to tflite with 257 it works fine for me but when I changed to 513 or higher value it doesn't predict anything \r\nknowing that i need to use 3041 as crop size\r\ndid you now what i suppose to do ??", "> @SirNeuman\r\n> \r\n> > tflite_convert \r\n> >   --output_file=test.lite \r\n> >   --graph_def_file=frozen_inference_graph.pb \r\n> >   --input_arrays=ImageTensor \r\n> >   --output_arrays=SemanticPredictions \r\n> >   --input_shapes=1,513,513,3 \r\n> >   --inference_input_type=QUANTIZED_UINT8 \r\n> >   --inference_type=FLOAT \r\n> >   --mean_values=128 \r\n> >   --std_dev_values=128\r\n> \r\n> > Sorry. I was actually messing up my input image when i resized it for running through the inference on mobile. It actually works quite well now. This is a different input image running for inference, but you can see that the results actually make a bit more sense and is close to what i'm trying to achieve:\r\n> \r\n> I want to make sure if is it work for you on android with 513 as a crop_size??\r\n> because when i export and convert to tflite with 257 it works fine for me but when I changed to 513 or higher value it doesn't predict anything\r\n> knowing that i need to use 3041 as crop size\r\n> did you now what i suppose to do ??\r\n\r\n@essalahsouad Does your model works fine? Could you please share your model and provide your way of converting to tflite?", "@alexkartsev \r\nno ,unfortunately my model does not predict correctly \r\ni used the model for an ios application TensorFlow Lite GPU delegate\r\nand i still have a bad segmentation mobile\r\n my tflite conversion is:\r\n`tflite_convert ----output_format=TFLITE --inference_type=FLOAT --inference_input_type=FLOAT --input_arrays=sub_2 --input_shapes=1,257,257,3 --output_arrays=ResizeBilinear_2 --output_file=frozen.tflite --graph_def=rozen.pb --mean_values=128 --std_dev_values=127 --allow_custom_ops --post_training_quantize`\r\n", "@ajinkya933 \r\n\r\n> The graph on left is my tflight-graph. and the graph on right is googles tflight-graph.\r\n> How is googles tflight-graph created from \" .pb\" file? how can I replicate this process so that my graph on left looks like googles graph on the right? My assumption is that if I create graph same as google it may give me semantic predictions on my custom trained objects properly. This assumption my be incorrect.\r\n\r\ni have the same problem as you, i even don(t know if this is a problem my model contains sub_2 instead f sub_7 and ResizeBilinear_2 instead of ResizeBilinear \r\ni tried to use the converted tflite model on ios with tflite gpu delegate but i got a slow resualt compared to the original deeplab model \r\n`nference_input_type=FLOAT --input_arrays=sub_2 --input_shapes=1,257,257,3 --output_arrays=ResizeBilinear_2 --output_file=frozen.tflite --graph_def=rozen.pb --mean_values=128 --std_dev_values=127 --allow_custom_ops --post_training_quantize`\r\ndid you have any idea?\r\n", "@SanthoshRajendiran \r\n> Google's tflite graph does not have post processing as well as pre-processing. In case if you want to replicate the same, strip your frozen graph from sub_7 to ResizeBilinear_2, and then do the tflite conversion.\r\n\r\ni don't get it what do you mean by that ??\r\nmy trained model contains sub_2 instead of sub_7 and resizeBilinear_2 instead of resizeBilinear_3\r\nand i converted to tflite for integrated it on mobile but the result was slow comparing to the original  TensorFlow model\r\ni don't know if the problem was because of my model architecture ??", "@melody-rain \r\nCould you please share how to properly run export_model.py. I'm trying to avoid the issue with tf.image.ResizeMethod.NEAREST_NEIGHBOR.\r\nAlso I'm trying to export it with checkpoint trained on ade20k dataset.\r\nHere the command I use to get .pb file\r\n```\r\npython models-master/research/deeplab/export_model.py \\\r\n  --logtostderr \\\r\n  --checkpoint_path=./deeplabv3_mnv2_ade20k_train_2018_12_03/model.ckpt \\\r\n  --export_path=./deeplabv3_mnv2_ade20k_train_2018_12_03/frozen_inference_graph_2.pb \\\r\n  --num_classes=151 \\\r\n  --crop_size=257 \\\r\n  --crop_size=257 \\\r\n  --model_variant=\"mobilenet_v2\" \\\r\n  --output_stride=16 \\\r\n  --dataset=\"ade20k\" \\\r\n  --inference_scales=1.0\r\n```\r\nHere the command I use to convert it to .tflite\r\n```\r\ntflite_convert \\\r\n  --output_file=./deeplabv3_mnv2_ade20k_train_2018_12_03/deeplabv3_mnv2_ade20k_2.tflite \\\r\n  --graph_def_file=./deeplabv3_mnv2_ade20k_train_2018_12_03/frozen_inference_graph_2.pb \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=SemanticPredictions \\\r\n  --input_shape=1,257,257,3 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --inference_type=FLOAT \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=127\r\n```\r\nI'm getting the following error when I run on ios device:\r\n```\r\ntensorflow/lite/kernels/unpack.cc:39 NumDimensions(input) > 1 was not true.\r\nNode number 4 (UNPACK) failed to prepare.\r\n```\r\nWhat I'm doing wrong here?", "Finally, I converted the model using the following command:\r\n```\r\ntflite_convert \\\r\n  --output_file=./deeplabv3_mnv2_ade20k.tflite \\\r\n  --graph_def_file=./frozen_inference_graph.pb \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=ExpandDims_1 \\\r\n  --input_shapes=1,257,257,3 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --inference_type=FLOAT \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=127\r\n```\r\nExpandDims_1 as output_arrays also works quite good", "> Google's tflite graph does not have post processing as well as pre-processing. In case if you want to replicate the same, strip your frozen graph from sub_7 to ResizeBilinear_2, and then do the tflite conversion.\r\n\r\n@SanthoshRajendiran \r\nHi SanthoshRajendiran. Thans so much!   \r\nFollowing your suggestion above, I have solved this problem: tensorflow/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 32) Node number 30 (DEPTHWISE_CONV_2D) failed to prepare.\r\nhope to help more people!", "> > @SirNeuman\r\n> > > tflite_convert\r\n> > > --output_file=test.lite\r\n> > > --graph_def_file=frozen_inference_graph.pb\r\n> > > --input_arrays=ImageTensor\r\n> > > --output_arrays=SemanticPredictions\r\n> > > --input_shapes=1,513,513,3\r\n> > > --inference_input_type=QUANTIZED_UINT8\r\n> > > --inference_type=FLOAT\r\n> > > --mean_values=128\r\n> > > --std_dev_values=128\r\n> > \r\n> > \r\n> > > Sorry. I was actually messing up my input image when i resized it for running through the inference on mobile. It actually works quite well now. This is a different input image running for inference, but you can see that the results actually make a bit more sense and is close to what i'm trying to achieve:\r\n> > \r\n> > \r\n> > I want to make sure if is it work for you on android with 513 as a crop_size??\r\n> > because when i export and convert to tflite with 257 it works fine for me but when I changed to 513 or higher value it doesn't predict anything\r\n> > knowing that i need to use 3041 as crop size\r\n> > did you now what i suppose to do ??\r\n> \r\n> @essalahsouad Does your model works fine? Could you please share your model and provide your way of converting to tflite?\r\n\r\nI've found a great example for iOS application with using semantic segmentation, you can try it\r\nhttps://github.com/makeml-app/MakeML-Nails", "@fly-butterfly thanks for the reply\r\nI've already resolved my issue, but this app looks promising. did you get a pretty result from makeml app?", "> @fly-butterfly thanks for the reply\r\n> I've already resolved my issue, but this app looks promising. did you get a pretty result from makeml app?\r\n\r\nyes, I've used it and got the tflite model.\r\nthere are not a lot of custom settings, but you can create the model without code at all. btw you can markup images there ", "@fly-butterfly \r\nsorry for my question but I want to know if is it for free?", "> @fly-butterfly\r\n> sorry for my question but I want to know if is it for free?\r\n\r\nlabeling images - yes\r\ntraining - one month ago was subscription about 10$ for unlimited trainings on their GPUs, but I had promo code for 5$, can try to find it if you want", "> Google's tflite graph does not have post processing as well as pre-processing. In case if you want to replicate the same, strip your frozen graph from sub_7 to ResizeBilinear_2, and then do the tflite conversion.\r\n\r\n@SanthoshRajendiran Thank you! It worked perfectly for me.\r\nJust in case anyone else is interested, I had code that worked with the model from the ios samples: `deeplabv3_257_mv_gpu.tflite` and I wanted to have a model to run on a higher resolution image.\r\nI downloaded the model `deeplabv3_mnv2_dm05_pascal_trainval` from Deeplab model zoo, and converted it to tflite using:\r\n```\r\ntflite_convert \\\r\n    --output_file=./deeplabv3_513.tflite \\\r\n    --graph_def_file=frozen_inference_graph.pb \\\r\n    --input_arrays=sub_7 \\\r\n    --output_arrays=ResizeBilinear_2 \\\r\n    --input_shapes=1,513,513,3 \\\r\n    --inference_type=FLOAT\r\n```\r\nThen the same code that worked for `deeplabv3_257_mv_gpu.tflite` worked on the converted model, just with input image of 513x513 instead 257x257\r\n\r\n", "> > Google's tflite graph does not have post processing as well as pre-processing. In case if you want to replicate the same, strip your frozen graph from sub_7 to ResizeBilinear_2, and then do the tflite conversion.\r\n> \r\n> @SanthoshRajendiran Thank you! It worked perfectly for me.\r\n> Just in case anyone else is interested, I had code that worked with the model from the ios samples: `deeplabv3_257_mv_gpu.tflite` and I wanted to have a model to run on a higher resolution image.\r\n> I downloaded the model `deeplabv3_mnv2_dm05_pascal_trainval` from Deeplab model zoo, and converted it to tflite using:\r\n> \r\n> ```\r\n> tflite_convert \\\r\n>     --output_file=./deeplabv3_513.tflite \\\r\n>     --graph_def_file=frozen_inference_graph.pb \\\r\n>     --input_arrays=sub_7 \\\r\n>     --output_arrays=ResizeBilinear_2 \\\r\n>     --input_shapes=1,513,513,3 \\\r\n>     --inference_type=FLOAT\r\n> ```\r\n> \r\n> Then the same code that worked for `deeplabv3_257_mv_gpu.tflite` worked on the converted model, just with input image of 513x513 instead 257x257\r\n\r\nThanks so much. Simply stripping unsupported nodes away when doing TF-Lite conversion really helps. Your solution is so simple that it makes me angry at myself, really. I wasted days trying to modify export_model.py, load graph_def up, exporting to TF-Lite, tweak things here and there only to end up running into a new problem after fixing another one.", "Buenas he intentado entrenar mi propio modelo y lo he convertido a TFLITE pero todas mis pruebas Dan error, alguien podr\u00eda ayudarme con mi aplicaci\u00f3n quiero poder entrenar y compilar mi modelo Deeplab en TFLITE con \u00e9xito ", "If anyone is trying to convert a frozen.pb model into tflite , [this](https://github.com/PINTO0309/PINTO_model_zoo/tree/master/01_deeplabv3/) repo have some frozen model model along with converted tflite models", "https://www.tensorflow.org/lite/models/segmentation/overview Is this helpful?", "@SanthoshRajendiran the increasing the resolution to 513x513 work for me. Any idea how to make it work for 1025x1025?\r\nwhen I have change the command 513 to 1025 it's giving below error\r\n`ValueError: The shape of tensor 'sub_7' cannot be changed from (1, 513, 513, 3) to [1, 1025, 1025, 3].`", "> > Google's tflite graph does not have post processing as well as pre-processing. In case if you want to replicate the same, strip your frozen graph from sub_7 to ResizeBilinear_2, and then do the tflite conversion.\r\n> \r\n> @SanthoshRajendiran Thank you! It worked perfectly for me.\r\n> Just in case anyone else is interested, I had code that worked with the model from the ios samples: `deeplabv3_257_mv_gpu.tflite` and I wanted to have a model to run on a higher resolution image.\r\n> I downloaded the model `deeplabv3_mnv2_dm05_pascal_trainval` from Deeplab model zoo, and converted it to tflite using:\r\n> \r\n> ```\r\n> tflite_convert \\\r\n>     --output_file=./deeplabv3_513.tflite \\\r\n>     --graph_def_file=frozen_inference_graph.pb \\\r\n>     --input_arrays=sub_7 \\\r\n>     --output_arrays=ResizeBilinear_2 \\\r\n>     --input_shapes=1,513,513,3 \\\r\n>     --inference_type=FLOAT\r\n> ```\r\n> \r\n> Then the same code that worked for `deeplabv3_257_mv_gpu.tflite` worked on the converted model, just with input image of 513x513 instead 257x257\r\n\r\n@ValYouW @Dr-Champ would you mind sharing you converted tflite model with 513x513 input?\r\nThank you!", "@josefgrunig I uploaded one to [this](https://github.com/ValYouW/tflite-win-c) repo which also includes a working sample in c++...", "> @josefgrunig I uploaded one to [this](https://github.com/ValYouW/tflite-win-c) repo which also includes a working sample in c++...\r\n\r\nThank you @ValYouW, \r\nwhen using your tflite model on the mobile example I see it is missing the 4th dimension in the output:\r\n\r\n`Error occurred when initializing ImageSegmenter: Output tensor is expected to have 4 dimensions, found 3.`\r\n\r\nCan I ask from which frozen graph you created this model from? \r\n\r\nMeanwhile, I managed to get **tflite_convert** working too and successfully converted this graph http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainval_2018_10_01.tar.gz into a 513x513 tflite model with metadata (I use TensorFlow Lite Task Library)\r\n\r\nI am trying to obtain the same quality of the deprecated TensorFlow Mobile (not Lite) model: some how the lite version lost accuracy for a faster computation time, GPU compatibility (which is not my priority here). Here is the comparison of the two models: https://github.com/dailystudio/ml/tree/master/deeplab\r\n\r\nThank you\r\n", "@josefgrunig It depends what you choose as your last layer (OutputArrays) when converting, if you choose `ResizeBilinear_2` then the output layer will be 4 dimensions, where the 4th dimension is 21 values which is the score per \"class\", and you will have to select the class with the highest probability yourself. If you choose `ArgMax` as the output layer, then the output is 3 dimensions where the 3rd dimension is the class with highest probability (which is what I used in the my project mentioned above).\r\n\r\nAs for quality, I did a quick test (using cpu) and was able to get similar results to TF_MOBILE (I dilate and blur the mask a bit):\r\n![image](https://user-images.githubusercontent.com/4539636/112859437-83d52480-90bb-11eb-8710-99ad910185c7.png)\r\n\r\nNOTE: The model you are using is `dm05` (depth multiplier = 0.5) which is less accurate (but smaller and probably faster), the model I have in my repo is with \"depth multiplier=1\"", "Seems promising @ValYouW, will go through the full conversion process again starting from a frozen graph with dm=1. \r\nUnfortunately cannot find the .pb file you started from in your repo. Can you point me at the right path?\r\nIs there a better dataset than another for recognising portrait figures from the background?\r\nThank you for your support, I still have a lot to learn.", "@josefgrunig I believe I took it here from [here](https://github.com/tensorflow/models/blob/71943914beaa3a0a74c073657193f7e31a3b1b0e/research/deeplab/g3doc/model_zoo.md), it is the `mobilenetv2_coco_voc_trainval` model.\r\nI once wrote a [blog post](https://www.thecodingnotebook.com/2020/05/converting-to-tensorflow-lite-models.html) on the conversion process (which uses TF 1.x).\r\n\r\nIf you find something better for portraits would appreciate if you can share.", "> @josefgrunig I believe I took it here from [here](https://github.com/tensorflow/models/blob/71943914beaa3a0a74c073657193f7e31a3b1b0e/research/deeplab/g3doc/model_zoo.md), it is the `mobilenetv2_coco_voc_trainval` model.\r\n> I once wrote a [blog post](https://www.thecodingnotebook.com/2020/05/converting-to-tensorflow-lite-models.html) on the conversion process (which uses TF 1.x).\r\n> \r\n> If you find something better for portraits would appreciate if you can share.\r\n\r\nThank you @ValYouW for pointing me in the right direction!\r\nStarting from mobilenetv2_coco_voc_trainval dataset and converting into a tflite model I am getting good results. Also the resulting segmentation masks is no more binary, but somehow smoothed on the edges. Is is due to the depth multiplier?\r\nThank you again, its the best result I have seen so far on mobile.", "@josefgrunig As far as I understand the model output is not a \"mask\" per-se, but just a matrix with the detected classId per pixel (i.e whether it belongs to a dog/horse/person etc). It is up to you to create an image mask from it....", "@ValYouW I understood the reason for the blurred mask image: its the small output size of the mask image. Converting the mobilenetv2_coco_voc_trainval model I get an output mask of 65*65 pixels only, while my target is to reach 513x513. The mobilenetv2_dm05_coco_voc_trainval model has an output shape of 513x513 but a lower depth multiplier. Are you sure your model comes from the mobilenetv2_coco_voc_trainval frozen graph? Looking at your blog post seems you used mobilenetv2_dm05_coco_voc_trainval. Thank you", "It was long time ago but I'm quite positive... you can find the converted model in [this](https://github.com/ValYouW/tflite-win-c) repo (file: deeplabv3_mnv2_pascal.tflite)\r\n\r\nWhen you inspect your converted model in Netron, what do you see as output size?\r\nThis is what I get in Netron:\r\n![image](https://user-images.githubusercontent.com/4539636/114108176-40f52700-98db-11eb-9be3-674ca56f17e0.png)\r\n", "I confirm that your model is 513x513, but as last node it uses ArgMax while I need a 4 dimensional output (ResizeBilinear_2).\r\nFor this reason I started from the frozen graph and converted the model to tflite, but I get a 65x65 output\r\n<img width=\"487\" alt=\"Screenshot 2021-04-09 at 09 17 17\" src=\"https://user-images.githubusercontent.com/32868530/114144340-83d3f080-9915-11eb-871a-964c6fc8b8dc.png\">\r\n\r\nYour model seems also to be working on 65x65 and being scaled before output to 513x513:\r\n<img width=\"1016\" alt=\"Screenshot 2021-04-09 at 09 20 04\" src=\"https://user-images.githubusercontent.com/32868530/114144525-b67de900-9915-11eb-8179-94f799fb0749.png\">\r\n\r\nIts it possible to change the output size while converting the frozen graph? if both graph work on 65x65 would it give an effective quality improvement? \r\n\r\nThank you for your time @ValYouW \r\n", "I'm really not familiar with the model architecture... You can use ResizeBilinear_3 as output which has 4 dimensions ( 1x513x513x21 ) but I really don't know if it will improve quality, but it still has nothing todo with the \"blur\". The model doesn't output an \"image\", but just numbers, it's up to you to build the mask image... So using ResizeBilinear_3 as your output layer you'll be able to build a binary mask (no blur) of size 513X513\r\n(BTW, not sure this discussion is related to this thread, please feel free to contact me via email).", "Hi @SanthoshRajendiran ! Can you check this [example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/android) on Deeplab TFLite android application ? ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> It was long time ago but I'm quite positive... you can find the converted model in [this](https://github.com/ValYouW/tflite-win-c) repo (file: deeplabv3_mnv2_pascal.tflite)\r\n> \r\n> When you inspect your converted model in Netron, what do you see as output size? This is what I get in Netron: ![image](https://user-images.githubusercontent.com/4539636/114108176-40f52700-98db-11eb-9be3-674ca56f17e0.png)\r\n\r\nHello guys, I am exporting model using the export_model.py file on the deeplab code on github. On netron, my model output is [1, 256, 256] as [ValYouW](https://github.com/ValYouW) got. I need to get [1,256,256,1]. How do I add expand my output in the export_file.py? Thank you", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23746, "title": "\"Invalid loop structure\" for Cleverhans saliency map attack on Keras model with Dropout layers", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Fedora 29\r\n- TensorFlow installed from: source\r\n- TensorFlow version: v1.11.0-0-gc19e29306c\r\n- Python version: 3.6.6\r\n- Bazel version: 0.19.1- (@non-git)\r\n- GCC/Compiler version: 7.3.0\r\n- CUDA/cuDNN version: 9.2\r\n- GPU model and memory: GeForce GTX 1050 3.95GiB\r\n\r\n**Describe the current behavior**\r\n- Prepared & trained a Keras Sequential model based on [this example script](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py), which contains Dropout layers\r\n- Passed the trained model to a [KerasModelWrapper](https://github.com/tensorflow/cleverhans/blob/master/cleverhans_tutorials/mnist_tutorial_keras_tf.py), a class provided by the [Cleverhans](https://github.com/tensorflow/cleverhans) library\r\n- Instantiated a [SaliencyMapMethod](https://github.com/tensorflow/cleverhans/blob/master/cleverhans_tutorials/mnist_tutorial_jsma.py) attack object on the wrapped model, and attempted to craft an adversarial example with it\r\n- TensorFlow encounters this error:\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: Invalid loop structure: Mismatched parent frames for \"while_1/while_context\": \"while_1/while_context\" vs \"\". The node giving this error: {{node while_1/gradients/while_1/model_4/dropout_1/cond/mul_grad/Shape/Enter}}This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n```\r\n- Removing the Dropout layers from the Keras model prevents this error from occuring.\r\n\r\n**Describe the expected behavior**\r\nWant the `generate_np` method of the SaliencyMapMethod object to return an adversarial example for the wrapped Keras model.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nfrom keras.activations import relu, softmax\r\n\r\nimport cleverhans\r\nfrom cleverhans.attacks import SaliencyMapMethod\r\nfrom cleverhans.utils_keras import KerasModelWrapper\r\n\r\nimport numpy as np\r\n\r\ndataset = keras.datasets.cifar10\r\nnum_classes = 10\r\n\r\n(train_images, train_labels), (test_images, test_labels) = dataset.load_data()\r\ninput_shape = train_images.shape[1:]\r\n\r\ntrain_images = train_images.astype('float32') / 255.0\r\ntest_images  = test_images.astype('float32') / 255.0\r\n\r\ntrain_labels = keras.utils.to_categorical(train_labels, num_classes)\r\ntest_labels  = keras.utils.to_categorical(test_labels, num_classes)\r\n\r\nmodel = Sequential([\r\n    Conv2D(32, (3,3), padding='same', input_shape=input_shape, activation=relu),\r\n    Conv2D(32, (3,3), activation=relu),\r\n    MaxPooling2D(pool_size=(2,2)),\r\n    Dropout(0.25),\r\n\r\n    Conv2D(64, (3,3), padding='same', activation=relu),\r\n    Conv2D(64, (3,3), activation=relu),\r\n    MaxPooling2D(pool_size=(2, 2)),\r\n    Dropout(0.25),\r\n\r\n    Flatten(),\r\n    Dense(512, activation=relu),\r\n    Dropout(0.5),\r\n    Dense(num_classes, activation=softmax)\r\n])\r\n\r\nmodel.compile(optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\r\n              loss=keras.losses.categorical_crossentropy,\r\n              metrics=[keras.metrics.categorical_accuracy])\r\n\r\nmodel.fit(train_images, train_labels)\r\n\r\njsma = SaliencyMapMethod(KerasModelWrapper(model), keras.backend.get_session())\r\njsma_params = {'theta': 1., 'gamma': 0.1,\r\n               'clip_min': 0., 'clip_max': 1.,\r\n               'y_target': None}\r\n\r\n# Generate adversarial examples for first test sample\r\nsample_ind = 0\r\nsamples = test_images[sample_ind:(sample_ind + 1)]\r\ncurrent_class = int(np.argmax(test_labels[sample_ind]))\r\nfor target in cleverhans.utils.other_classes(num_classes, current_class):\r\n    one_hot_target = np.zeros((1, num_classes), dtype=np.float32)\r\n    one_hot_target[0, target] = 1\r\n    jsma_params['y_target'] = one_hot_target\r\n    # Error happens here\r\n    adversarial_images = jsma.generate_np(samples, **jsma_params)\r\n    break\r\n```\r\n\r\n**Other info / logs**\r\n- Keras version: 2.2.4 (from pip)\r\n- Cleverhans version: 2.0.0-fe079e17fb307c71a4a413d8bd408d98 (from source, commit [0b22b1c](https://github.com/tensorflow/cleverhans/commit/0b22b1c523cd06efd93d3999c5118986819ac98e))\r\n- Full traceback of the error when running the above code:\r\n```\r\nTraceback (most recent call last):\r\n  File \"~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1292, in _do_call\r\n    return fn(*args)\r\n  File \"~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1277, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1367, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Invalid loop structure: Mismatched parent frames for \"while/while_context\": \"while/while_context\" vs \"\". The node giving this error: {{node while/gradients/while/model_1/dropout_1/cond/mul_grad/Shape/Enter}}This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"crash.py\", line 62, in <module>\r\n    adversarial_images = jsma.generate_np(samples, **jsma_params)\r\n  File \"~/cleverhans/cleverhans/attacks.py\", line 203, in generate_np\r\n    return self.sess.run(x_adv, feed_dict)\r\n  File \"~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Invalid loop structure: Mismatched parent frames for \"while/while_context\": \"while/while_context\" vs \"\". The node giving this error: {{node while/gradients/while/model_1/dropout_1/cond/mul_grad/Shape/Enter}}This is an internal bug, please file a bug report with instructions on how to reproduce the error.\r\n```\r\n", "comments": ["Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I apologize for the delay in response. I think this issue is better suited for [tensorflow/cleverhans repo](https://github.com/tensorflow/cleverhans/issues). Please post it on cleverhans repo. Thanks!"]}, {"number": 23745, "title": "CMake build update", "body": "I am trying to separate changes in cmake and TF core code changes in two PRs regarding [this old PR](https://github.com/tensorflow/tensorflow/pull/16480)\r\n\r\nseveral new features are added in this contrib:\r\n- better CMake GUI experience\r\n- better linux build setting\r\n- abseil_cpp update to match latest tf dependency requirement\r\n- only find python interpreter if user want to build python binding\r\n- TF is still on 9.0, few weeks ago someone changes min version to 9.1 (https://www.tensorflow.org/install/source#tested_source_configurations)\r\n- MSVC CMake GUI build instruction added in readme\r\n- CMake packaging, can generate shared libraries on all platforms. C++ api can be used via cmake instructions in CMakeLists.txt: \r\n ```cmake\r\nfind_package(Tensorflow REQUIRED)\r\ninclude_directories(${TENSORFLOW_INCLUDE_DIRS})\r\n```\r\n- new tf ops added\r\n- improved windows gpu build compatibility", "comments": ["@jackyko1991 do you also plan to add CMake for tflite?", "@nicolarrosia I am not sure if tf lite cmake should be keep standalone or attach to current cmake. \r\n\r\nIt seems fine to first write c API and core for lite build. "]}, {"number": 23744, "title": "Changed the Makefile version of protobuf to 3.6.0", "body": "Fixed #22536 ", "comments": ["Did you mean to merge this into `master`?", "The issue was reported to r1.10. Should I merge it to master as well? I am new to this community", "It should be only to master. We don't usually re-use release-specific branches after the release is over.", "@angersson  Will create new pull request then"]}, {"number": 23743, "title": "ckpt file only has .meta file , no .index file or .data file in every step to save", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Liunx centos7.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:1.11\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda9 cndnn7\r\n- GPU model and memory:M40 12GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI use MonitoredTrainingSession to train distribute tf  on 3 machines(1 ps , 2 workers), and it worked ,but the ckpt file  only has .meta file , no .index file or .data file in every step to save. The max train step was 1000,no errors in the training time.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nthe file like this\r\n-rw-r--r-- 1 root root      693 11\u6708 14 22:10 checkpoint\r\n-rw-r--r-- 1 root root 17226852 11\u6708 14 22:12 events.out.tfevents.1542203421.node58\r\n-rw-r--r-- 1 root root  9268881 11\u6708 14 21:50 graph.pbtxt\r\n-rw-r--r-- 1 root root  4858196 11\u6708 14 22:10 model.ckpt-1000.meta\r\n-rw-r--r-- 1 root root  4858196 11\u6708 14 22:02 model.ckpt-601.meta\r\n-rw-r--r-- 1 root root  4858196 11\u6708 14 22:04 model.ckpt-701.meta\r\n-rw-r--r-- 1 root root  4858196 11\u6708 14 22:06 model.ckpt-801.meta\r\n-rw-r--r-- 1 root root  4858196 11\u6708 14 22:08 model.ckpt-901.meta\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\npart of the log \r\nINFO:tensorflow:global_step/sec: 0.876005\r\nINFO:tensorflow:Saving checkpoints for 501 into /data0/users/haha/tensorflow-test/MonitoredTrainingSession_test/log/model.ckpt.\r\n2018-11-14 22:54:46.889383 : global step = 500, loss = 1.45 (12.6 examples/sec; 1.265sec/batch)\r\n2018-11-14 22:54:59.885239 : global step = 510, loss = 1.13 (12.3 examples/sec; 1.300sec/batch)\r\n2018-11-14 22:55:09.314735 : global step = 520, loss = 0.88 (17.0 examples/sec; 0.943sec/batch)\r\n2018-11-14 22:55:23.053285 : global step = 530, loss = 1.01 (11.6 examples/sec; 1.374sec/batch)\r\n\r\n```\r\nthe code \r\n```\r\ndef main(_):\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.reset_default_graph()\r\n    # 1.cluster spec\r\n    worker_spec = FLAGS.workerSpec.split(',')\r\n    ps_spec = FLAGS.psSpec.split(',')\r\n    print('cluster has %d workers, %d ps' % (len(worker_spec), len(ps_spec)))\r\n    cluster = tf.train.ClusterSpec({'worker': worker_spec,\r\n                                    'ps': ps_spec})\r\n    if FLAGS.job_name == 'ps':\r\n        server.join()\r\n    elif FLAGS.job_name == 'worker':\r\n        if FLAGS.gpu_num > 0:\r\n            worker_device = \"/job:worker/task:{}/gpu:{}\".format(FLAGS.task_index, FLAGS.task_index % FLAGS.gpu_num)\r\n        else:\r\n            worker_device = \"/job:worker/task:{}/cpu:0\".format(FLAGS.task_index)\r\n\r\n        # 2. data  https://www.tensorflow.org/programmers_guide/datasets\r\n        with tf.device(tf.train.replica_device_setter(\r\n                                                      worker_device=worker_device,\r\n                                                      cluster=cluster)):\r\n            filenames = get_tfrecord_filenames(FLAGS.dataset_dir)\r\n            tf.logging.info(\"dataset %s:\" % filenames)\r\n            dataset = tf.data.TFRecordDataset(filenames)\r\n            dataset = dataset.map(parse_single_image)\r\n            dataset = dataset.shuffle(buffer_size=1000)\r\n            dataset = dataset.batch(FLAGS.batch_size)\r\n            dataset = dataset.repeat(FLAGS.epochs)\r\n            iterator = dataset.make_initializable_iterator()\r\n          \r\n            batch = iterator.get_next()\r\n            image_file_name, img_batch, label_batch = batch\r\n            print(img_batch, label_batch)\r\n\r\n            # 3. model\r\n            with slim.arg_scope(inception_v3_arg_scope()):\r\n                logits, _ = inception_v3(img_batch, 9)\r\n            predicts = tf.nn.softmax(logits)\r\n          \r\n            loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=tf.one_hot(label_batch, depth=9, on_value=1, off_value=0))\r\n\r\n            global_step = tf.train.get_or_create_global_step()\r\n            opt = tf.train.AdamOptimizer(FLAGS.lr)\r\n\r\n            if FLAGS.sync_replicas:\r\n                if FLAGS.replicas_to_aggregate is None:\r\n                    replicas_to_aggregate = len(worker_spec)\r\n                else:\r\n                    replicas_to_aggregate = FLAGS.replicas_to_aggregate\r\n                opt = tf.train.SyncReplicasOptimizer(opt,\r\n                                                     replicas_to_aggregate,\r\n                                                     total_num_replicas=len(worker_spec),\r\n                                                     #replica_id=FLAGS.task_index,\r\n                                                     name=\"sync_replicas_optimizer\")\r\n\r\n                sync_replicas_hook = opt.make_session_run_hook(is_chief=(FLAGS.task_index == 0), num_tokens=0)\r\n\r\n            train_op = opt.minimize(loss, global_step=global_step)\r\n\r\n        class _DatasetInitializerHook(tf.train.SessionRunHook):\r\n            def __init__(self, data_iterator):\r\n                super(_DatasetInitializerHook, self).__init__()\r\n                self._iterator = data_iterator\r\n\r\n            def begin(self):\r\n                self._initializer = self._iterator.initializer\r\n\r\n            def after_create_session(self, session, coord):\r\n                del coord\r\n                session.run(self._initializer)\r\n\r\n        class _LogHook(tf.train.SessionRunHook):\r\n            def __init__(self, global_step, loss, log_frequency, batch_size):\r\n                super(_LogHook, self).__init__()\r\n                self.loss = loss\r\n                self.global_step = global_step\r\n                self.log_frequency = log_frequency\r\n                self.batch_size = batch_size\r\n\r\n            def begin(self):\r\n                self.start_time = time.time()\r\n\r\n            def before_run(self, run_context):\r\n                return tf.train.SessionRunArgs([global_step,loss])\r\n\r\n            def after_run(self,\r\n                          run_context,  # pylint: disable=unused-argument\r\n                          run_values):\r\n                global_step_value, loss_value = run_values.results\r\n                if global_step_value % self.log_frequency == 0:\r\n                    current_time = time.time()\r\n                    duration = current_time - self.start_time\r\n                    self.start_time = current_time\r\n                    img_per_sec = self.log_frequency * self.batch_size / duration\r\n                    sec_per_batch = float(duration / self.log_frequency)\r\n                    format_str = ('%s : global step = %d, loss = %.2f (%.1f imgs/sec; %.3fsec/batch)')\r\n                    print(format_str % (datetime.now(), global_step_value, loss_value, img_per_sec, sec_per_batch))\r\n\r\n        # 4.train\r\n        saver=tf.train.Saver()\r\n        scaffold = tf.train.Scaffold(saver=saver)\r\n\r\n        hooks = [tf.train.StopAtStepHook(last_step=FLAGS.steps),\r\n                 tf.train.NanTensorHook(loss),\r\n                 _LogHook(global_step ,loss, FLAGS.log_frequency, FLAGS.batch_size),\r\n                 _DatasetInitializerHook(iterator),\r\n                 sync_replicas_hook]\r\n\r\n        config = tf.ConfigProto(allow_soft_placement=True,\r\n                                log_device_placement=False,\r\n                                device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.task_index])\r\n        config.gpu_options.allow_growth = True\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                               is_chief=(FLAGS.task_index == 0),\r\n                                               checkpoint_dir=FLAGS.train_dir,\r\n                                               scaffold=scaffold,\r\n                                               hooks=hooks,\r\n                                               save_checkpoint_steps=100,\r\n                                               stop_grace_period_secs=5,\r\n                                               config=config) as mon_sess:\r\n\r\n            try:\r\n                while not mon_sess.should_stop():\r\n                    mon_sess.run(train_op)\r\n            except Exception as e:\r\n                print(e)\r\n\r\n```\r\nSo,anyone to help me ? thank you ! ", "comments": ["I found the same code that I run in one machine , the log files are normal. Three kinds of log (.index /.data/.meta) all had. Anyone to help me, thank you!\r\n```\r\ndef main(_):\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.reset_default_graph()\r\n\r\n    # 1. data  https://www.tensorflow.org/programmers_guide/datasets\r\n\r\n    filenames = get_tfrecord_filenames(FLAGS.dataset_dir)\r\n    tf.logging.info(\"dataset %s:\" % filenames)\r\n    dataset = tf.data.TFRecordDataset(filenames)\r\n    dataset = dataset.map(parse_single_image)\r\n    dataset = dataset.shuffle(buffer_size=1000)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(FLAGS.batch_size)\r\n\r\n    iterator = dataset.make_initializable_iterator()\r\n    # iterator = dataset.make_one_shot_iterator()\r\n    batch = iterator.get_next()\r\n    image_file_name, img_batch, label_batch = batch\r\n    print(img_batch, label_batch)\r\n\r\n    # 3. model\r\n    with slim.arg_scope(inception_v3_arg_scope()):\r\n        logits, _ = inception_v3(img_batch, FLAGS.class_num)\r\n    # predicts = tf.nn.softmax(logits)\r\n    # coross_mean = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label_batch)\r\n    # loss = tf.reduce_mean(coross_mean)\r\n\r\n    loss = tf.losses.softmax_cross_entropy(logits=logits,\r\n                                           onehot_labels=tf.one_hot(label_batch, depth=FLAGS.class_num,\r\n                                                                    on_value=1,\r\n                                                                    off_value=0))\r\n\r\n    global_step = tf.train.get_or_create_global_step()\r\n    opt = tf.train.AdamOptimizer(FLAGS.lr)\r\n\r\n    train_op = opt.minimize(loss, global_step=global_step)\r\n\r\n    class _DatasetInitializerHook(tf.train.SessionRunHook):\r\n        def __init__(self, data_iterator):\r\n            super(_DatasetInitializerHook, self).__init__()\r\n            self._iterator = data_iterator\r\n\r\n        def begin(self):\r\n            self._initializer = self._iterator.initializer\r\n\r\n        def after_create_session(self, session, coord):\r\n            del coord\r\n            session.run(self._initializer)\r\n\r\n    class _LogHook(tf.train.SessionRunHook):\r\n        def __init__(self, global_step, loss, log_frequency, batch_size):\r\n            super(_LogHook, self).__init__()\r\n            self.loss = loss\r\n            self.global_step = global_step\r\n            self.log_frequency = log_frequency\r\n            self.batch_size = batch_size\r\n\r\n        def begin(self):\r\n            # self.step = -1\r\n            self.start_time = time.time()\r\n\r\n        def before_run(self, run_context):\r\n            # self.step += 1\r\n            return tf.train.SessionRunArgs([global_step, loss])\r\n\r\n        def after_run(self,\r\n                      run_context,  # pylint: disable=unused-argument\r\n                      run_values):\r\n            global_step_value, loss_value = run_values.results\r\n            if global_step_value % self.log_frequency == 0:\r\n                current_time = time.time()\r\n                duration = current_time - self.start_time\r\n                self.start_time = current_time\r\n                imgs_per_sec = self.log_frequency * self.batch_size / duration\r\n                sec_per_batch = float(duration / self.log_frequency)\r\n                format_str = ('%s : global step = %d, loss = %.2f (%.1f imgs/sec; %.3fsec/batch)')\r\n                tf.logging.info(\r\n                    format_str % (datetime.now(), global_step_value+1, loss_value, imgs_per_sec, sec_per_batch))\r\n\r\n    # 4.train\r\n    hooks = [tf.train.StopAtStepHook(last_step=FLAGS.steps),\r\n             tf.train.NanTensorHook(loss),\r\n             _LogHook(global_step, loss, FLAGS.log_frequency, FLAGS.batch_size),\r\n             _DatasetInitializerHook(iterator)]\r\n\r\n    config = tf.ConfigProto(allow_soft_placement=True,\r\n                            log_device_placement=False,\r\n                            device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.task_index])\r\n    config.gpu_options.allow_growth = True\r\n\r\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=FLAGS.train_dir,\r\n                                           # scaffold=scaffold,\r\n                                           hooks=hooks,\r\n                                           save_checkpoint_steps=100,\r\n                                           config=config,\r\n                                           stop_grace_period_secs=5) as mon_sess:\r\n\r\n        try:\r\n\r\n            # mon_sess.run(iterator.initializer)\r\n            # mon_sess = tf_debug.LocalCLIDebugWrapperSession(mon_sess)\r\n            while not mon_sess.should_stop():\r\n                try:\r\n                    mon_sess.run(train_op)\r\n                except tf.errors.OutOfRangeError:\r\n                    break\r\n        except Exception as e:\r\n            print(e)\r\n```", "If the logs say these files are created, they probably are. Do all of the workers/parameter servers see the same filesystem?", "When I trained distribute ,only the chief worker has log files, and all workers print logs while in the training,ps no log nor log files.\r\n \r\nsingle machine train  log is below\r\n```\r\nINFO:tensorflow:Saving checkpoints for 9301 into /data0/users/gaolei7/tensorflow-test/MonitoredTrainingSession_test/log1/model.ckpt.\r\nINFO:tensorflow:2018-11-20 00:00:29.895015 : global step = 9300, loss = 0.01 (40.0 imgs/sec; 1.601sec/batch)\r\nINFO:tensorflow:global_step/sec: 0.624138\r\nINFO:tensorflow:Saving checkpoints for 9401 into /data0/users/gaolei7/tensorflow-test/MonitoredTrainingSession_test/log1/model.ckpt.\r\nINFO:tensorflow:2018-11-20 00:03:10.261891 : global step = 9400, loss = 0.09 (39.9 imgs/sec; 1.604sec/batch)\r\nINFO:tensorflow:global_step/sec: 0.623877\r\nINFO:tensorflow:Saving checkpoints for 9501 into /data0/users/gaolei7/tensorflow-test/MonitoredTrainingSession_test/log1/model.ckpt.\r\nINFO:tensorflow:2018-11-20 00:05:50.339205 : global step = 9500, loss = 0.03 (40.0 imgs/sec; 1.601sec/batch)\r\nINFO:tensorflow:global_step/sec: 0.627477\r\nINFO:tensorflow:Saving checkpoints for 9601 into /data0/users/gaolei7/tensorflow-test/MonitoredTrainingSession_test/log1/model.ckpt.\r\nINFO:tensorflow:2018-11-20 00:08:29.960465 : global step = 9600, loss = 0.10 (40.1 imgs/sec; 1.596sec/batch)\r\nINFO:tensorflow:global_step/sec: 0.643929\r\nINFO:tensorflow:Saving checkpoints for 9701 into /data0/users/gaolei7/tensorflow-test/MonitoredTrainingSession_test/log1/model.ckpt.\r\nINFO:tensorflow:2018-11-20 00:11:05.013845 : global step = 9700, loss = 0.13 (41.3 imgs/sec; 1.551sec/batch)\r\nINFO:tensorflow:global_step/sec: 0.6288\r\nINFO:tensorflow:Saving checkpoints for 9801 into /data0/users/gaolei7/tensorflow-test/MonitoredTrainingSession_test/log1/model.ckpt.\r\nINFO:tensorflow:2018-11-20 00:13:44.280198 : global step = 9800, loss = 0.01 (40.2 imgs/sec; 1.593sec/batch)\r\nINFO:tensorflow:global_step/sec: 0.62375\r\nINFO:tensorflow:Saving checkpoints for 9901 into /data0/users/gaolei7/tensorflow-test/MonitoredTrainingSession_test/log1/model.ckpt.\r\nINFO:tensorflow:2018-11-20 00:16:24.657066 : global step = 9900, loss = 0.09 (39.9 imgs/sec; 1.604sec/batch)\r\nINFO:tensorflow:Saving checkpoints for 10000 into /data0/users/gaolei7/tensorflow-test/MonitoredTrainingSession_test/log1/model.ckpt.\r\n\r\n```\r\nand the log file list is below\r\n```\r\n-rw-r--r-- 1 root root       705 11\u6708 20 00:19 checkpoint\r\n-rw-r--r-- 1 root root  13883010 11\u6708 20 00:19 events.out.tfevents.1542628331.node58\r\n-rw-r--r-- 1 root root   8094880 11\u6708 19 19:52 graph.pbtxt\r\n-rw-r--r-- 1 root root 271841312 11\u6708 20 00:19 model.ckpt-10000.data-00000-of-00001\r\n-rw-r--r-- 1 root root     32294 11\u6708 20 00:19 model.ckpt-10000.index\r\n-rw-r--r-- 1 root root   4190714 11\u6708 20 00:19 model.ckpt-10000.meta\r\n-rw-r--r-- 1 root root 271841312 11\u6708 20 00:08 model.ckpt-9601.data-00000-of-00001\r\n-rw-r--r-- 1 root root     32294 11\u6708 20 00:08 model.ckpt-9601.index\r\n-rw-r--r-- 1 root root   4190714 11\u6708 20 00:08 model.ckpt-9601.meta\r\n-rw-r--r-- 1 root root 271841312 11\u6708 20 00:11 model.ckpt-9701.data-00000-of-00001\r\n-rw-r--r-- 1 root root     32294 11\u6708 20 00:11 model.ckpt-9701.index\r\n-rw-r--r-- 1 root root   4190714 11\u6708 20 00:11 model.ckpt-9701.meta\r\n-rw-r--r-- 1 root root 271841312 11\u6708 20 00:13 model.ckpt-9801.data-00000-of-00001\r\n-rw-r--r-- 1 root root     32294 11\u6708 20 00:13 model.ckpt-9801.index\r\n-rw-r--r-- 1 root root   4190714 11\u6708 20 00:13 model.ckpt-9801.meta\r\n-rw-r--r-- 1 root root 271841312 11\u6708 20 00:16 model.ckpt-9901.data-00000-of-00001\r\n-rw-r--r-- 1 root root     32294 11\u6708 20 00:16 model.ckpt-9901.index\r\n-rw-r--r-- 1 root root   4190714 11\u6708 20 00:16 model.ckpt-9901.meta\r\n\r\n```\r\nI had checked part of the tf source code, and I want to know why. Please help me, thanks!", "I'm confused. Didn't you say the issue was that you only had `.meta` files? It looks like everything's there.", " When I trained distribute mode with three machine, the log only has .meta in the chief worker. While I was trained with one machine ,three kinds of log all have. ", "Are there checkpoint files on any of the other machines?", "I found when I set the 'FLAGS.train_dir' was  hdfs path, the log files have .meta, .data and .index.  Thank you.\r\n"]}, {"number": 23742, "title": "Add example to import_meta_graph docstring", "body": "Based on the super insightful example by @mrry on Stackoverflow (https://stackoverflow.com/a/38834095/1063607), which many users suggested should be in the official documentation.", "comments": ["Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you change this to be a PR to the `master` branch? Our import-to-internal system doesn't handle changes to old release branches.", "Yeah sure, will be done in <24h, I'm in the middle of moving to a new flat\n:D\n\nV.\n\nOn Fri, Nov 30, 2018, 8:16 PM Austin Anderson <notifications@github.com>\nwrote:\n\n> Can you change this to be a PR to the master branch? Our\n> import-to-internal system doesn't handle changes to old release branches.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/23742#issuecomment-443308411>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsvvLhY_Gfys5x39elf9VFnStgk_DRtks5u0YQjgaJpZM4Yd3mI>\n> .\n>\n", "@angersson It was hard to change this to point to the master branch, because of the merge conflicts. I couldn't figure out how to merge the master into this branch, so I created a new PR, which is also cleaner, I guess. \r\n\r\nPR: https://github.com/tensorflow/tensorflow/pull/24173", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 23741, "title": "Fixed link path \"apis.md\"", "body": "Improvement of link disconnection by document movement.", "comments": ["Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@PINTO0309 Can you make the requested changes? Thanks!"]}, {"number": 23740, "title": "No TF import library to link custom operators on Windows", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, 64 bits\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary / pip\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nThe import library tensorflow\\python\\pywrap_tensorflow_internal.lib is missing. I tried to regenerate it myself from _pywrap_tensorflow_internal.pyd to build my custom operators, but\r\ntf.load_op_library throws the exception tensorflow.python.framework.errors_impl.NotFoundError and does not find the library given in argument. The exception appears in TF_LoadLibrary.\r\n\r\n**Describe the expected behavior**\r\n\r\nIn TF 1.8 the import library tensorflow\\python\\pywrap_tensorflow_internal.lib is present, but not in TF 1.12. This library is required to build custom operators.\r\n\r\ntf.load_op_library is working correctly with TensorFlow 1.8 on my configuration. The same custom operators rebuilt for TF 1.12 are not found by tf.load_op_library, although the paths of the dll given to tf.load_op_library are correct.\r\n\r\n**Other info / logs**\r\n\r\nEither there is a problem with tf.load_op_library in TF 1.12 contrary to TF 1.8, or there is a mistake in my process to generate the import library from the pyd file (the process is normally used for dll, not pyd).\r\n\r\nIn all cases, the dll of the custom operator generated with the provided import library in TF 1.8 depends on _pywrap_tensorflow_internal.pyd, whereas the dll generated with my own import library for TF 1.12 depends on pywrap_tensorflow_internal.dll which is a non existing file.\r\n\r\nThe simplest way to tackle this issue first seems to include again pywrap_tensorflow_internal.lib in the pip package to build custom operators on Windows.", "comments": ["I solved this issue by changing the way I generate the import library pywrap_tensorflow_internal.lib. Now the dll of my custom operators depend on _pywrap_tensorflow_internal.pyd instead of pywrap_tensorflow_internal.dll.\r\n\r\nI still let the issue opened as the import library pywrap_tensorflow_internal.lib should be included in the pip package as in TF 1.8.", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@meteorcloudy @gunan This sounds like a potential regression from the switch to Bazel. Does one of you know how to add this file back?", "I can send a change to add this file back", "Note that `_pywrap_tensorflow_internal.lib` still depends on `_pywrap_tensorflow_internal.pyd` instead of `_pywrap_tensorflow_internal.dll`. Because the DLL name is embedded in the import library.", "\"Note that _pywrap_tensorflow_internal.lib still depends on _pywrap_tensorflow_internal.pyd\"\r\n\r\nThat is the expected behavior. It does not work if _pywrap_tensorflow_internal.lib depends on _pywrap_tensorflow_internal.dll as this file does not exist, that was my error when I generated the import library myself.", "I cannot reopen the issue, but the issue persists, the file is still missing in the pip package.\r\n\r\nCould you please reintroduce the library _pywrap_tensorflow_internal.lib in the pip pickage ?"]}, {"number": 23739, "title": "Update README.md", "body": "improving readability. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 23738, "title": "Fix a TypeError", "body": "We cannot concatenate string and FailedPreConditionError", "comments": ["I hope that these builds are not failing because of this change? ", "Nagging Assignee @wt-huang: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23737, "title": "tf.load_op_library behaves different in version 1.12 vs 1.11", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): -\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): gcc 6.5 for custom ops\r\n- CUDA/cuDNN version: 9.0/7.3.1\r\n- GPU model and memory: Titan X with 10GB\r\n\r\n**Describe the current behavior**\r\nWhen I use tf.load_op_library to load my library with two ops, only one of them appears in the loaded module.\r\n\r\n**Describe the expected behavior**\r\nBoth ops should appear in the loaded module. Works in version 1.11.", "comments": ["Do you have them both in the same so file? What are they called?", "They both should end up in the same shared object. The build script:\r\n\r\n```\r\nTF_CFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\r\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\r\n\r\nnvcc \\\r\n  -DNDEBUG \\\r\n  -ccbin=g++-6 \\\r\n  -std=c++11 -c -o extract_volume_patches_op_gpu.cu.o \\\r\n  ../core/kernels/extract_volume_patches_op_gpu.cu.cc \\\r\n  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\r\n\r\nnvcc \\\r\n  -DNDEBUG \\\r\n  -ccbin=g++-6 \\\r\n  -std=c++11 -c -o integrate_volume_patches_op_gpu.cu.o \\\r\n  ../core/kernels/integrate_volume_patches_op_gpu.cu.cc \\\r\n  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\r\n\r\ng++-6 \\\r\n  -std=c++11 -shared -o volume_patches.so \\\r\n  ../core/ops/extract_volume_patches.cc \\\r\n  ../core/ops/integrate_volume_patches.cc \\\r\n  ../core/kernels/extract_volume_patches_op.cc \\\r\n  ../core/kernels/integrate_volume_patches_op.cc \\\r\n  extract_volume_patches_op_gpu.cu.o \\\r\n  integrate_volume_patches_op_gpu.cu.o \\\r\n  ${TF_CFLAGS[@]} -fPIC -lcudart ${TF_LFLAGS[@]} \\\r\n  -L /opt/cuda/lib64/\r\n```", "I think I saw some similar problem recently. It's not clear what is going wrong. Did you build tensorflow from source? If not, could you try that?\r\n\r\n@allenlavoie may have more ideas.", "Does it reproduce in a simple case, like doubling up the registration in [one of our custom op tests](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/ackermann_op.cc)?\r\n\r\n@annarev could this be a side effect of the API generation changes? \r\n\r\nCC @gunan @yifeif who are interested in custom ops", "I can't think of recent api generation changes that could cause it. Api generation should not impact custom ops.\r\n\r\n@christiantinauer What is the name of the op that doesn't load? I see tf.extract_volume_patches in 1.12 but not in 1.11:\r\nhttps://www.tensorflow.org/api_docs/python/tf/extract_volume_patches\r\n\r\nCould there be a possible name conflict:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/load_library.py#L47\r\n\r\nedit: fixed typo\r\n\r\n\r\n\r\n", "@annarev, @hsgkim actually it is a naming conflict as tf.extract_volume_patches is not loaded in version 1.12. Unfortunately the new op is not mentioned in the release notes (I checked before upgrading).\r\n\r\nThe implementation in 1.12 is nearly the same as mine. Nevertheless I also implemented the gradient for the op (integrate_volume_patches). If there is interest in the implementation I can share.\r\n\r\nThanks for the support.", "Glad you figured it out. Thanks for looping back!"]}, {"number": 23736, "title": "Change the dependent @nsync version, resulting in a significant increase in the .so size", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow version:1.8.0\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):0.17.2\r\n- GCC/Compiler version (if compiling from source):gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5)\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want compile tensorflow1.8.0 to target cpu arrch64\uff0cAnd got the error as issue #13115.\r\n```\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/nsync/BUILD:401:13: Configurable attribute \"copts\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @nsync//:android_arm\r\n @nsync//:android_arm64\r\n @nsync//:android_armeabi\r\n @nsync//:android_x86_32\r\n @nsync//:android_x86_64\r\n @nsync//:clang_macos_x86_64\r\n @nsync//:gcc_linux_aarch64\r\n @nsync//:gcc_linux_ppc64\r\n @nsync//:gcc_linux_x86_64_1\r\n @nsync//:gcc_linux_x86_64_2\r\n @nsync//:ios_x86_64\r\n @nsync//:msvc_windows_x86_64.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n```\r\nthe issue tells me to add `\"//conditions:default\": [],` in the depend nsync. It does work.\r\nBut I was asked not to modify any open source binaries\uff0cso\uff0cI use tensorflow as an external repository and use the following rule to change tensorflow1.8.0's depend nsync version.\r\n```\r\nlocal_repository(\r\n\tname=\"org_tensorflow\",\r\n\tpath=\"../tmp\",\r\n    repo_mapping = {\"@nsync\" : \"@nsync_1.20.1\"},\r\n)\r\n```\r\n```\r\ntf_http_archive(\r\n    name = \"nsync_1.20.1\",\r\n    urls = [\r\n        \"https://mirror.bazel.build/github.com/google/nsync/archive/1.20.1.tar.gz\",\r\n        \"https://github.com/google/nsync/archive/1.20.1.tar.gz\",\r\n    ],\r\n    sha256 = \"692f9b30e219f71a6371b98edd39cef3cbda35ac3abc4cd99ce19db430a5591a\",\r\n    strip_prefix = \"nsync-1.20.1\",\r\n)\r\n```\r\n\r\nThis works for me\uff0cBut\uff0cthis resulting in a significant increase in the .so size. By default nsync it was 50M and when I change change the denpend nsync version\uff0cthe .so file was 160M.\r\n\r\nIs this normal? How should I do now? It's been bothering me for a long time, hoping to give me some advice!\r\n", "comments": []}, {"number": 23734, "title": "Object_detection model supports IOS?", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:MAC 10.14\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:Phone 8\r\n- **TensorFlow installed from (source or binary)**:bazel \r\n- **TensorFlow version (use command below)**:1.10.0\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:NO\r\n- **CUDA/cuDNN version**:NO\r\n- **GPU model and memory**:NO\r\n- **Exact command to reproduce**:NO\r\n\r\n\r\n### Source code / logs\r\nHow to support IOS for object detection\uff1f\uff1f\uff1f\r\nmodel:ssd_mobilenet_v1_", "comments": ["We're actively working on a port of the existing Android object detection sample to iOS.", "@jdduke - where? \r\n\r\nwould love to contribute", "It hasn't yet been staged in the public repo. We're hoping to push the updated samples very early Q1 (or sooner)."]}, {"number": 23733, "title": "Memory leak in tf.train.Example/tf.train.Features/tf.gfile", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- TensorFlow installed from (source or binary): repository\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: cuda 10, cudnn 7\r\n- GPU model and memory: nvidia 1080ti\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe system goes OOM and I have 64 GiB of memory.\r\n\r\n**Describe the expected behavior**\r\n\r\nI should be able to create a dataset converting files I read from Google Cloud to local tfrecords.\r\n\r\nI'm debugging the memory usage of python and as you can see below, the usage is around 670K - hence the memory allocation should be outside of python, and the only library I'm using is Tensorflow.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport subprocess\r\nimport multiprocessing\r\nfrom glob import glob\r\nimport tensorflow as tf\r\nfrom tensorflow import keras as k\r\nimport json\r\nimport os\r\nimport logging\r\nfrom typing import Pattern, List, Tuple, Callable\r\nimport linecache\r\nimport tracemalloc\r\n\r\n\r\ndef display_top(snapshot, key_type=\"lineno\", limit=10):\r\n    snapshot = snapshot.filter_traces(\r\n        (\r\n            tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\r\n            tracemalloc.Filter(False, \"<unknown>\"),\r\n        )\r\n    )\r\n    top_stats = snapshot.statistics(key_type)\r\n\r\n    print(\"Top %s lines\" % limit)\r\n    for index, stat in enumerate(top_stats[:limit], 1):\r\n        frame = stat.traceback[0]\r\n        # replace \"/path/to/module/file.py\" with \"module/file.py\"\r\n        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\r\n        print(\r\n            \"#%s: %s:%s: %.1f KiB\" % (index, filename, frame.lineno, stat.size / 1024)\r\n        )\r\n        line = linecache.getline(frame.filename, frame.lineno).strip()\r\n        if line:\r\n            print(\"    %s\" % line)\r\n\r\n    other = top_stats[limit:]\r\n    if other:\r\n        size = sum(stat.size for stat in other)\r\n        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\r\n    total = sum(stat.size for stat in top_stats)\r\n    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\r\n\r\n\r\ntracemalloc.start()\r\n\r\nCACHE = os.path.join(os.getcwd(), \".data\")\r\nROWS_PER_RECORD = 32\r\nSPLITS = (\"train\", \"validation\", \"test\")\r\n\r\nLOG = logging.Logger(__name__)\r\nLOG.setLevel(logging.INFO)\r\n\r\n\r\ndef _int64_feature(value):\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n\r\ndef _float_feature(value):\r\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n\r\n\r\ndef _bytes_feature(value):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\n\r\ndef rows_in_tfrecord(path):\r\n    \"\"\"Count the number of elements in a tfrecord file.\r\n    Args:\r\n        path: The path of the TFRecord file\r\n    Return:\r\n        The number of examples in the TFRecord\r\n    \"\"\"\r\n    return sum(1 for _ in tf.io.tf_record_iterator(path))\r\n\r\n\r\ndef _is_dir(path: str):\r\n    \"\"\"Determines if path is a directory only looking at the filename.\r\n    It's ugly, but is the only way to get a decent speed.\r\n    Args:\r\n        path: The input path\r\n    Return:\r\n        value: True if the path looks like a directory\r\n    \"\"\"\r\n    return path.endswith(\"/\") or not \".\" in path.split(\"/\")[-1]\r\n\r\n\r\ndef list_dir(root):\r\n    \"\"\"Recursively list a directory and returns the list of all the files.\r\n    Args:\r\n        root: The path of the root directory\r\n    Return:\r\n        List of all the files found in this directory and its subfolders.\r\n    \"\"\"\r\n    ret = []\r\n    for full_path in tf.gfile.Glob(os.path.join(root, \"*\")):\r\n        if _is_dir(full_path):\r\n            return ret + list_dir(full_path)\r\n        else:\r\n            ret.append(full_path)\r\n    return ret\r\n\r\n\r\ndef _gs2tfrecord(gs_paths: List[str], patterns: List[Pattern], cache: str = CACHE):\r\n    \"\"\"Fetch images from google cloud storage, conver to tf_record and\r\n    properly handle cache.\r\n\r\n    Args:\r\n        gs_paths: list of paths on google cloud storage\r\n        patterns: list of compiled regex to fiter the data on each gs_paths\r\n        cache: local cache/dataset folder\r\n    \"\"\"\r\n    gcloud_auth = os.path.join(\r\n        os.path.expanduser(\"~\"),\r\n        \".config\",\r\n        \"gcloud\",\r\n        \"application_default_credentials.json\",\r\n    )\r\n    if not os.path.exists(gcloud_auth):\r\n        subprocess.check_call([\"gcloud\", \"auth\", \"application-default\", \"login\"])\r\n\r\n    meta_file = os.path.join(cache, \"meta.json\")\r\n    dataset_path = os.path.join(cache, \"dataset\")\r\n    if not os.path.exists(cache):\r\n        os.makedirs(dataset_path)\r\n        for split in SPLITS:\r\n            os.makedirs(os.path.join(dataset_path, split))\r\n        meta = {\"local_files\": {path: [] for path in gs_paths}}\r\n    else:\r\n        with open(meta_file, \"r\") as fp:\r\n            meta = json.load(fp)\r\n\r\n    for remote_path, pattern in zip(gs_paths, patterns):\r\n        remote_files = set()\r\n        for name in list_dir(remote_path):\r\n            if pattern.search(name.replace(remote_path, \"\")):\r\n                remote_files.add(name)\r\n\r\n        local_files = set(meta[\"local_files\"][remote_path])\r\n\r\n        diff = remote_files - local_files\r\n        LOG.warn(\"New remote elements: {}\".format(len(diff)))\r\n        if diff:\r\n            # Check the current tfrecords, find the last one created\r\n            # Fill the empty spaces in this one (rewriting it completely)\r\n            # And create the other ones\r\n            record_id = sum(\r\n                len(glob(os.path.join(dataset_path, split, \"*.tfrecord\")))\r\n                for split in SPLITS\r\n            )\r\n\r\n            last_tf_record = os.path.join(dataset_path, \"train\", \"1.tfrecord\")\r\n            for split in SPLITS:\r\n                last_path = os.path.join(dataset_path, split, f\"{record_id}.tfrecord\")\r\n                if os.path.exists(last_path):\r\n                    last_tf_record = last_path\r\n                    break\r\n\r\n            examples = []\r\n            if os.path.exists(last_tf_record):\r\n                rows_in_last = rows_in_tfrecord(last_tf_record)\r\n                if rows_in_last < ROWS_PER_RECORD:\r\n                    # read all rows in the last tfrecord\r\n                    # we're going to add the last one\r\n                    # and recreate it\r\n                    for row in tf.io.tf_record_iterator(last_tf_record):\r\n                        example = tf.train.Example()\r\n                        examples.append(example.ParseFromString(row))\r\n                else:\r\n                    record_id += 1\r\n\r\n            # fetch new files and update the tfrecords\r\n            tot_new_elements = len(diff)\r\n            for idx, new_element in enumerate(diff):\r\n                LOG.warn(f\"File: {new_element}\")\r\n                with tf.gfile.GFile(new_element, \"rb\") as fp:\r\n                    img = fp.read()\r\n                remote_meta = os.path.join(os.path.realpath(new_element), \"meta.json\")\r\n                if not tf.gfile.Exists(remote_meta):\r\n                    example_meta = {\r\n                        \"count\": _int64_feature(1),\r\n                        \"feature1\": _float_feature(1076),\r\n                        \"feature2\": _float_feature(100),\r\n                        \"f3\": _float_feature(1076),\r\n                        \"f4\": _float_feature(100),\r\n                        \"f5\": _bytes_feature(\"mango\".encode(\"UTF-8\")),\r\n                        \"f7\": _bytes_feature(\r\n                            \"banana\".encode(\"UTF-8\")\r\n                        ),\r\n                    }\r\n                else:\r\n                    with tf.gfile.GFile(remote_meta, \"r\") as fp:\r\n                        example_meta = json.loads(fp.read())\r\n\r\n                # TODO: check if image is an old style image and convert to new format\r\n                example_meta[\"img\"] = _bytes_feature(img)\r\n                examples.append(\r\n                    tf.train.Example(features=tf.train.Features(feature=example_meta))\r\n                )\r\n\r\n                if len(examples) == ROWS_PER_RECORD or idx == tot_new_elements - 1:\r\n                    # write (always in the training dataset) and reset examples buffer\r\n                    filename = os.path.join(\r\n                        dataset_path, \"train\", f\"{record_id}.tfrecord\"\r\n                    )\r\n                    with tf.io.TFRecordWriter(filename) as writer:\r\n                        for example in examples:\r\n                            writer.write(example.SerializeToString())\r\n                    # Reset example buffer\r\n                    examples.clear()\r\n                    # Increment tfrecord id\r\n                    record_id += 1\r\n                    LOG.warn(f\"TFRecord: {filename} written\")\r\n                    snapshot = tracemalloc.take_snapshot()\r\n                    display_top(snapshot)\r\n                    # break\r\n\r\n```\r\n\r\nJust use the `_gs2tfrecord` function. I'm using it to fetch data from Google Cloud Storage, but since I'm using `tf.gfile` a local path can be used too.\r\n\r\nMy guess is that leak can be somewhere in `tf.train.Example` or in `tf.train.Feature` since those are the only 2 functions I call in a loop.\r\n\r\n**UPDATE**\r\n\r\nI've created another function that just download the images from Google Cloud and store them locally. It goes out of memory in the same way.\r\n\r\nMaybe the leak is in `tf.gfile`?\r\n\r\nHere's the function\r\n```python\r\ndef _gs2folder(\r\n    gs_paths: List[str], patterns: List[Pattern], names: List[str], cache: str = CACHE\r\n):\r\n    \"\"\"Fetch images from google cloud storage, conver to tf_record and\r\n    properly handle cache.\r\n\r\n    Args:\r\n        gs_paths: list of paths on google cloud storage\r\n        patterns: list of compiled regex to fiter the data on each gs_paths\r\n        names: list of dataset name\r\n        cache: local cache/dataset folder\r\n    \"\"\"\r\n    assert len(gs_paths) == len(patterns) and len(patterns) == len(names)\r\n\r\n    gcloud_auth = os.path.join(\r\n        os.path.expanduser(\"~\"),\r\n        \".config\",\r\n        \"gcloud\",\r\n        \"application_default_credentials.json\",\r\n    )\r\n    if not os.path.exists(gcloud_auth):\r\n        subprocess.check_call([\"gcloud\", \"auth\", \"application-default\", \"login\"])\r\n\r\n    for remote_path, pattern, name in zip(gs_paths, patterns, names):\r\n        meta_file = os.path.join(cache, name, \"meta.json\")\r\n        dataset_path = os.path.join(cache, name, \"original\")\r\n        if not os.path.exists(cache):\r\n            os.makedirs(dataset_path)\r\n            meta = {\"local_files\": {path: [] for path in gs_paths}}\r\n        else:\r\n            with open(meta_file, \"r\") as fp:\r\n                meta = json.load(fp)\r\n\r\n        remote_files = set()\r\n        for file_name in list_dir(remote_path):\r\n            if pattern.search(file_name.replace(remote_path, \"\")):\r\n                remote_files.add(file_name)\r\n\r\n        local_files = set(meta[\"local_files\"][remote_path])\r\n        diff = remote_files - local_files\r\n        LOG.warning(\"New remote elements: {}\".format(len(diff)))\r\n        last_id = len(local_files) + 1\r\n        if diff:\r\n            # fetch new files and store them\r\n            tot_new_elements = len(diff)\r\n\r\n            for idx, new_element in enumerate(diff):\r\n                LOG.warning(f\"File: {new_element}\")\r\n                with tf.gfile.GFile(new_element, \"rb\") as fp:\r\n                    img_bytes = fp.read()\r\n\r\n                image = Image.open(io.BytesIO(img_bytes))\r\n                image = np.array(image)\r\n                if image.shape[-1] == 4:\r\n                    # remove transparency\r\n                    image = image[..., :3]\r\n                remote_meta = os.path.join(os.path.realpath(new_element), \"meta.json\")\r\n                if not tf.gfile.Exists(remote_meta):\r\n                    example_meta = {\r\n                        \"count\": 1\r\n                    }\r\n                else:\r\n                    with tf.gfile.GFile(remote_meta, \"r\") as fp:\r\n                        example_meta = json.loads(fp.read())\r\n\r\n                image = Image.fromarray(image)\r\n                # Just save the image file with its meta info\r\n                image.save(os.path.join(dataset_path, f\"{last_id}.png\"))\r\n                with open(os.path.join(dataset_path, f\"{last_id}.json\"), \"w\") as fp:\r\n                    json.dump(example_meta, fp)\r\n                last_id += 1\r\n\r\n        meta[\"local_files\"][remote_path] = remote_files\r\n\r\n    with open(meta, \"w\") as fp:\r\n        json.dump(meta, fp)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nThis is the output of a `display_top` invocation:\r\n\r\n```\r\nTop 10 lines\r\n#1: python3.7/linecache.py:137: 246.9 KiB\r\n    lines = fp.readlines()\r\n#2: util/compat.py:80: 150.0 KiB\r\n    return bytes_or_text.decode(encoding)\r\n#3: datasets/floorplans.py:143: 64.2 KiB\r\n    diff = remote_files - local_files\r\n#4: datasets/floorplans.py:139: 32.0 KiB\r\n    remote_files.add(name)\r\n#5: internal/python_message.py:475: 21.6 KiB\r\n    self._oneofs = {}\r\n#6: internal/python_message.py:425: 15.5 KiB\r\n    result = message_type._concrete_class()\r\n#7: python3.7/tracemalloc.py:185: 13.4 KiB\r\n    self._frames = tuple(reversed(frames))\r\n#8: internal/python_message.py:472: 12.0 KiB\r\n    self._fields = {}\r\n#9: internal/python_message.py:1402: 10.3 KiB\r\n    self._parent_message_weakref = weakref.proxy(parent_message)\r\n#10: internal/python_message.py:1155: 5.1 KiB\r\n    for field, value in list(self._fields.items()):  # dict can change size!\r\n228 other: 104.0 KiB\r\nTotal allocated size: 675.1 KiB\r\n```", "comments": ["Update: probably the leak is in `tf.gfile` since I can see that the memory grows every time I run the line:\r\n\r\n```python\r\n                with tf.gfile.GFile(new_element, \"rb\") as fp:\r\n                    img_bytes = fp.read()\r\n```\r\nIn the loop.\r\n\r\nI've also tried to change it to:\r\n\r\n```python\r\nfp = tf.gfile.GFile(new_element, \"rb\")\r\nimg_bytes = fp.read()\r\nfp.close()\r\nfp = None\r\n```\r\n\r\nBut every time I `read()` the memory is never released", "Another update: the problem is related to the tensorflow package I'm using, that's the one shipped by the Archlinux devs.\r\n\r\nIf I create a virtualenv with python3.6 and I use the tensorflow package installed via pip, it works correctly.", "@galeone : Thanks for all the details. I wanted to confirm the state after all your digging and investigation.\r\n\r\nIs it accurate to say that there is no problem observed in the official TensorFlow release binaries and you're only seeing this in a package built by the Archlinux folks?", "@asimshankar Yes, I confirm.\r\nIf I create a virtualenv with everything official inside, there is no leak.\r\n\r\nHowever, compiling tensorflow is straightforward, hence probably is something is worth investigating, since in future Tensorflow could be used with CUDA 10 and python 3.7 (and those are the only differences between the official and the Archlinux version)", "Update: the same happen with Tensorflow installed from conda\n\nOn Tue, Nov 20, 2018, 07:57 Asim Shankar <notifications@github.com wrote:\n\n> @galeone <https://github.com/galeone> : Thanks for all the details. I\n> wanted to confirm the state after all your digging and investigation.\n>\n> Is it accurate to say that there is no problem observed in the official\n> TensorFlow release binaries and you're only seeing this in a package built\n> by the Archlinux folks?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23733#issuecomment-440164185>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AICZDHYP41NCNOScE9tk9eetFP2xRzqCks5uw6fRgaJpZM4YdiX7>\n> .\n>\n", "@galeone : With \"conda\" - still Python 3.7? (Just trying to understand the environment)", "No, with Python 3.6.7 and tensorflow-gpu (1.12) installed using conda into the conda environement.\r\nHere's the detail:\r\n\r\n- cuda toolkit: 9.2\r\n- cudnn: 7.2.1\r\n- conda version: 4.3.27\r\n- cupti: 9.2.148\r\n- cuda version: 9.0.176", "The same problem exists on the official Cloud Deep Learning VM Image (TF 1.12, CUDA 10). It runs out of memory within a few minutes when training data on gs://. Runs perfectly fine when the bucket is mounted to the local fs.", "**Edit**: this bug isn't reproducible for some reason, I'm keeping it here so the rest of the thread still makes sense, but feel free to skip ahead.\r\n\r\nIs there any update on this? I'm running out of memory with a simple \r\n\r\n```\r\ndef generate_writer(self):\r\n        tfrecord_fname = ''.join([random.choice(string.ascii_letters + string.digits) for n in xrange(9)]) + '.tfrecord'\r\n        opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\r\n        writer = tf.python_io.TFRecordWriter(tfrecord_fname, options=opts)\r\n        return writer, tfrecord_fname\r\n\r\nwriter, fname = self.generate_writer()\r\ni = 0\r\nwhile True:\r\n            sample = {\r\n                'video': Feature(float_list=FloatList(value=(e['video'].reshape(-1)))),\r\n                'video_shape': Feature(int64_list=Int64List(value=(e['video'].shape))),\r\n                'tx_path': Feature(bytes_list=BytesList(value=wrap(e['tx_path']))),\r\n                'tx_type': Feature(bytes_list=BytesList(value=wrap(e['tx_type']))),\r\n                'item_scanned': Feature(int64_list=Int64List(value=(e['item_scanned'])))\r\n            }\r\n            feature = tf.train.Features(feature=sample)\r\n            example = tf.train.Example(features=feature)\r\n            writer.write(example.SerializeToString())\r\n\r\n            i += 1\r\n\r\n            if i % 10 == 0:\r\n                writer.close()\r\n                writer, fname = self.generate_writer()\r\n```\r\n\r\n(of course in practice I iterate and fetch various elements `e` rather than serialize the same example over and over, but this has nothing to do with the memory leak)\r\n\r\nObserved in Ubuntu 16, both tensorflow 1.12 and 1.13", "Thanks for the simple example. I'll take a look today.", "I tried to look into this problem today.\r\n\r\n@urimend I cannot reproduce the memory leaks from your sample code using either tensorflow 1.12 installed from python3 virtualenv or anaconda. Can you please provide more details about this issue? Does the size of e['video'] matter? I tried setting e['video'] to numpy.zeros(10**5).\r\n\r\n@galeone I cannot reproduce the memory leaks you reported in the following code with tensorflow 1.12 with ananconda.\r\n```\r\nwith tf.gfile.GFile(new_element, \"rb\") as fp:\r\n      img_bytes = fp.read()\r\n```\r\nI have some trouble making tensorflow-gpu run in my machine. Can you please check to see if you can reproduce the memory leak using tensorflow instead of tensorflow-gpu?", "@jing-dong - you meant @urimerhav :)", "For me, it is reproduced for almost every scenario. Just try to read a few thousands of files directly from gs:// using tf.gfile.GFile with the \"with\" statement. You will see a memory leak right away.\r\n\r\nFor me the important part was is that it should be tf 1.12 built for CUDA 10. It did not repro for a CPU build installed using pip. This problem exists on the official Cloud Deep Learning VM Image (TF 1.12, CUDA 10).\r\n\r\nI will try to repro it on the official v1.13.1 to see if it is still an issue.\r\n\r\n", "@jing-dong I'm sorry. I relaunched my aws instance from the same image and now I can't reproduce my error. I guess I'm retracting my bug report since I'm not able to reproduce", "For me, it also didn't repro today with the official tensorflow-gpu 1.13.1", "Update: So this problem _is_ reproducible for me, with ubuntu 16 python 2.7 and tflow (both cpu and gpu) 1.13.1.\r\n\r\nThe only thing I can add that might be going on here, is that I'm fetching the data from an s3 path, though I really can't imagine how that would effect anything. ", "The problem is still present in the 2 different environments I reported (python 3.7 - cuda 10 - archlinux build *and* python 3.6 - tensorflow-gpu 1.12 - conda env). When I read a file, using GFile, from Google Cloud storage there's a memory leak that saturates all the system memory.\r\nIt happens both when using context manager or when manually opening and closing the files (I thought the leak could be caused by a wrong implementation of the context manager that doesn't close the file when exiting the context - but it is not the case).", "@galeone do you happen to know of a version with python 2.7 where the memory leak doesn't occur? ", "@urimerhav no, never used python 2.x with Tensorflow; never tried", "@urimerhav Can you please confirm that the memory leaks only occur when you try to read from s3 path by trying to read from local FS or gcs path (gs://) if that is available to you? If the memory leak only happens when reading the s3 paths, it may be because the s3 file reader in TF contains a leak.\r\n\r\n@galeone Would it be easy for you to try to experiment with tensorflow (without gpu) to see if you can reproduce the leaks there? As I said earlier, I have trouble running tensorflow-gpu in my environment.  Also, just to confirm, the code that produces the memory leak for you is the following, right?\r\n```\r\nwhile True:\r\n    with tf.gfile.GFile(gs_path, \"rb\") as fp:\r\n        img_bytes = fp.read()\r\n```", "Any update on this bug? We're seeing some sort of memory leak after moving from TF 1.1.9 to TF 1.1.3 on Python 2.7\r\n", "I confirm that the bug still exists in TF 1.13. Env: the latest NVIDIA TF 1.13 container running on Google Gloud, python2 (nvcr.io/nvidia/tensorflow:19.03-py2)", "@ygoncharov  @quirkyllama can you confirm that python3 doesn't have that memory leak?\r\n\r\n", "@urimerhav with this setup https://github.com/tensorflow/tensorflow/issues/23733#issuecomment-444046925 the leak is still present and is Python 3", "Thanks @galeone \r\n\r\nI'm willing to use just about any TF distribution that will rid me of this bug, so far it seems like the only report on a working setup is 1.1.9 on python 2?", "Reading the comments looks like yes, 1.1.9 py2 works - but you can try with some new tensorflow release that support Python 2 to see if the leak is still there or is gone", "@galeone @ygoncharov @urimend Can you please describe a python environment and a minimum code snippet that reproduces this memory leak? I will try to see if I can reproduce this in a GCloud VM and go from there.", "Since I have not heard back regarding this issue, I will close it for now. Please feel free to reopen the issue with a setup to reproduce the problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23733\">No</a>\n", "@jing-dong this thread contains the exact environment (see my post; the latest NVIDIA TF 1.13 container running on Google Gloud, docker image: nvcr.io/nvidia/tensorflow:19.03-py2) and the code (see @urimerhav  post) to reproduce the issue.\r\n\r\nFor easier repro please try the following:\r\n1. Env: nvcr.io/nvidia/tensorflow:19.03-py2 running on GC v100\r\n2. Create a GS bucket with ~100k files\r\n3. Read all these files one by one using tf.gfile.Open from gs:// and write them to the local FS\r\n-> your program will hit OOM \r\n\r\nThis is a critical issue that appears across a few TF release builds in many environment, not sure why it was closed.", "Thanks @ygoncharov. I will reopen the issue and look into reproducing the problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23733\">No</a>\n", "@ygoncharov, I am able to reproduce the problem as you described. Thanks for the detailed instructions. One strange thing I noted is that the memory leak problem does not happen if I use tensorflow-gpu (1.13.1) installed on the host without using the nivida docker image for both python2 and python3. Can you please confirm that is the case? If so, the problem probably comes from how Nvidia built the docker image.", "@jing-dong Glad that you were able to reproduce the issue. Indeed it seems the issue does not repro in some environments, but I cannot pinpoint the exact reason for that. I don't think that the problem is in the docker image since I (and it seems other people in this thread) had this problem in a docker-less environment before. ", "@ygoncharov I tried the following setup that seems to work:\r\n1. Env: nvcr.io/nvidia/tensorflow:19.02-py2 running on GC v100  (note the version is 19.02)\r\n2. Inside the docker container, create python virtualenv\r\n3. pip install tensorflow-gpu==1.13.1\r\n4. The test script runs without memory leak\r\n\r\nCan you please try this out to see if this works for you?\r\n\r\nThe default installed tensorflow in the docker image also has the memory leak issue as you noticed. It suggests that the problem comes from how nvidia built the tensorflow for the docker image. If you can reproduce the memory leak issue without using the nvidia docker image, we can investigate further on this. If not, can you please report the problem to nvidia to investigate further?", "@ygoncharov There seems to be a related problem reported in the nvidia developer forum:\r\n\r\nhttps://devtalk.nvidia.com/default/topic/1046985/docker-and-nvidia-docker/tensorflow-model-memory-issues-with-new-docker-version-19-01/", "@jing-dong sounds good, I will try those steps to verify your conclusion", "@ygoncharov How does the workaround work for you? Were you able to reproduce the memory leak outside of the nvidia docker image?", "@ygoncharov I will close this ticket for now. If you still have any issues, please feel free to re-open this ticket.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23733\">No</a>\n", "@jing-dong I did a similar experiment: run the script on an official GC deep learning image and then pip installed tensorflow-gpu==1.13.1 in a clean virtualenv. I got the same result: memory leak in the first case and no memory leak in the second.", "Hi everybody, I'm joining to the memory leaking club.\r\n\r\nEnvironment:\r\nDocker container base: FROM python:3.7-stretch\r\nTensorflow: conda install -c conda-forge tensorflow  (tried pip install tensorflow=1.13.1 as well) \r\n= using CPU\r\n\r\nUse case:\r\nRunning loading from GCS using GFile in callback on message from pubsub (sending messages 1 by one for testing) - I can see that data is not released,\r\n\r\nFile1:\r\n      9    196.6 MiB    196.6 MiB   @profile\r\n    10                                             def read(filename):\r\n    11    196.6 MiB      0.0 MiB            with tf.gfile.GFile(filename, 'rb') as dicom_file_obj:\r\n    12    327.9 MiB    131.3 MiB              return dicom_file_obj.read()\r\nFile 2:\r\n    11    328.0 MiB      0.0 MiB       with tf.gfile.GFile(filename, 'rb') as dicom_file_obj:\r\n    12    715.7 MiB    387.8 MiB           return dicom_file_obj.read()\r\nFile 3:\r\n    11    715.8 MiB      0.0 MiB       with tf.gfile.GFile(filename, 'rb') as dicom_file_obj:\r\n    12    973.6 MiB    257.8 MiB           return dicom_file_obj.read()\r\n \r\nand just grows with every file, first I though it it's pubsub problem but I removed everything from the pubsub callback and I only read the file and return and finish. Memory still grows.\r\n\r\nAny ideas?\r\n\r\nFollow up info:\r\n\r\nPython3.6-stretch works without problems, so it's combination of TF1.13.1 and Python3.7\r\n", "@Luxas98 Can you please post a minimal Dockerfile and sample script here that I can use to reproduce the problem? I will try to see what I can do from there.\r\n\r\n@ygoncharov Can you please clarify what the \"official GC deep learning image\" refers to? Is it  nvcr.io/nvidia/tensorflow:19.03-py2 that you used earlier?", "@jing-dong no, it was not a docker image. It is an official Google Cloud image preconfigured for Tensorflow from https://cloud.google.com/deep-learning-vm/\r\n\r\nThe link to the current marketplace image is below, but please note that in my test I used the previous version (the one that came with tensorflow 1.12).\r\nhttps://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning?_ga=2.212830999.-2046381496.1540473489", "@jing-dong Hi, check example of leak and no leak in https://github.com/Luxas98/tensorflow-gfile-memory-leak \r\n\r\nPlease let me know when you are done so I can remove the service account secrets and gs files :-)", "@Luxas98 Thanks for the simple examples for reproducing the memory leaks. I am able to reproduce the same problem as you described. I will look into the problem and update here if there is any findings.", "Setting the following environment variable prevents the memory leak problem in Python 3.7 for me.\r\nexport ENV GCS_READ_CACHE_MAX_SIZE_MB=0\r\n\r\nCan you please give it a try and see if this works for you? In the mean time, I will look into the root cause for the problem.", "We identified the root cause of the memory leak problem and will cherry-pick the fix to TF 1.14 soon. For earlier versions, you can continue to use \"export GCS_READ_CACHE_MAX_SIZE_MB=0\" as the workaround for this problem.", "The fix has been cherry-picked to TF 1.14 (https://github.com/tensorflow/tensorflow/commit/66d4f9e6d8f65c56eda1091384c4f0967abd0fdf). Please feel free to re-open the ticket if there are further issues related to this bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23733\">No</a>\n", "Great!\n\nOn Fri, Apr 26, 2019, 22:41 Jing Dong <notifications@github.com> wrote:\n\n> We identified the root cause of the memory leak problem and will\n> cherry-pick the fix to TF 1.14 soon. For earlier versions, you can continue\n> to use \"export GCS_READ_CACHE_MAX_SIZE_MB=0\" as the workaround for this\n> problem.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23733#issuecomment-487192970>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACAJSDHQFFR2QISXCTGOVODPSNSIVANCNFSM4GDWEX5Q>\n> .\n>\n", "Thank you for fixing this. I can confirm `export ENV GCS_READ_CACHE_MAX_SIZE_MB=0` fixes the leak for me in `1.13.1` and it works in `1.14.0` without setting that.\r\n\r\nI spent a day tracking down the memory leak since it does not show up in python profiling with `tracemalloc`. Is there any way to profile how much memory tensorflow is using to help with isolating future memory leaks?", "One tool that I found helpful is the heap profiler in Gperftools (https://gperftools.github.io/gperftools/heapprofile.html). You may want to try it out next time when you suspect there are memory leaks.", "Thank you @jing-dong I will try that next time :pray: ", "Is there a way to pass this flag when running on TPUs?"]}, {"number": 23732, "title": "Instance of 'CheckpointState' has no 'model_checkpoint_path' member", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Package based PIP installation\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nSo I am trying to get checkpoint path, but I am unable to. Error:\r\nInstance of 'CheckpointState' has no 'model_checkpoint_path' member\r\n\r\n**Describe the expected behavior**\r\nNo error and able to get checkpoint path.\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndef restore(s, save_instance):\r\n    checkpoint = tensorflow.train.get_checkpoint_state(os.path.dirname(configuration.CHECKPOINT_PATH + \"/checkpoint\"))\r\n    if checkpoint and checkpoint.model_checkpoint_path:\r\n        print(\"Loading...\")\r\n        save_instance.restore(s, checkpoint.model_checkpoint_path)\r\n    else:\r\n        print(\"New Instance being created...\")\r\n```\r\n\r\n", "comments": ["Is there going to be a follow-up? ", "I can't reproduce. Could you put an end-to-end example together?\r\n\r\nHere's what works for me:\r\n\r\nimport tensorflow as tf\r\n\r\n```\r\ndef restore(save_path):\r\n    checkpoint = tf.train.get_checkpoint_state(save_path)\r\n    if checkpoint and checkpoint.model_checkpoint_path:\r\n        print(\"Loading... {}\".format(checkpoint.model_checkpoint_path))\r\n    else:\r\n        print(\"New Instance being created...\")\r\n\r\ntf.train.update_checkpoint_state(\"/tmp/\", model_checkpoint_path=\"blah\")\r\nrestore(\"/tmp/\")\r\n```\r\n\r\nPrints:\r\n\r\n> Loading... /tmp/blah", "I do the exact same thing and it doesn't work. Is there a file in your \"/tmp/ directory before you run this? ", "I removed `/tmp/checkpoint` and re-ran. I'm using Python 3.6; I don't have 3.7 installed. I'm also on Linux, but we have similar unit tests which run on Windows. \r\n\r\nDoes it work for you on Python 3.6? Presumably not using `\"/tmp/\"` since that isn't meaningful on Windows.", "\uff1f\r\n", "Closing as not reproducible, but please do follow up if it's still an issue."]}, {"number": 23731, "title": "Error when using tf.metrics.mean_iou() in eval_metric_ops of tf.estimator.EstimatorSpec()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:_no_\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  _Linux Ubuntu 16.04.5 LTS_\u3002\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:_no_\r\n- **TensorFlow installed from (source or binary)**:_binary_\r\n- **TensorFlow version (use command below)**:_1.11.0_\r\n- **Python version**:_Python 3.5.2_\r\n- **Bazel version (if compiling from source)**:_None_\r\n- **GCC/Compiler version (if compiling from source)**:_None_\r\n- **CUDA/cuDNN version**:_CUDA-9.1/cuDNN-7.0.5_\r\n- **GPU model and memory**: _GTX 1080Ti 11GB, 2 same cards_\r\n- **Exact command to reproduce**:_below_\r\n\r\n### Supplementary material\r\nI just pulled the tensorflow's offical docker image and RUN some `pip install ...` instructions.\r\nThe tag of the iamge is `1.11.0-devel-gpu-py3`.\r\n\r\n### Describe the problem\r\nWhen I use the code below\r\n\r\n```\r\naccuracy = tf.metrics.accuracy(valid_labels, valid_preds)\r\nmean_iou = tf.metrics.mean_iou(valid_labels, valid_preds, params['num_classes'])\r\nprint(mean_iou)\r\neval_metrics = {'px_accuracy': accuracy, 'mean_iou': mean_iou}\t\r\n......\r\nreturn tf.estimator.EstimatorSpec(\r\n\t\tmode=mode,\r\n\t\tpredictions=None,\r\n\t\tloss=loss,\r\n\t\ttrain_op=train_op,\r\n\t\teval_metric_ops=eval_metrics)\r\n```\r\n\r\nIt will raise `TypeError`\r\n\r\n```\r\nTypeError: eval_metric_ops[mean_iou] must be Operation or Tensor, \r\ngiven: <tf.Variable 'mean_iou/AssignAddVariableOp' shape=(21, 21) dtype=float64>\r\n```\r\n\r\nAnd the result of  `print` is\r\n\r\n```\r\n(<tf.Tensor 'mean_iou/Select_1:0' shape=() dtype=float32>, \r\n<tf.Variable 'mean_iou/AssignAddVariableOp' shape=(21, 21) dtype=float64>)\r\n```\r\n\r\nI google the error and find the similar issue\r\nhttps://github.com/tensorflow/tensorflow/issues/20418\r\n\r\nSo I modify the code\r\n```\r\naccuracy = tf.metrics.accuracy(valid_labels, valid_preds)\r\nmean_iou = tf.metrics.mean_iou(valid_labels, valid_preds, params['num_classes'])\r\nmean_iou = (mean_iou[0], tf.to_float(mean_iou[1]))\r\nprint(mean_iou)\r\neval_metrics = {'px_accuracy': accuracy, 'mean_iou': mean_iou}\t\r\n```\r\nThen the code works normally.\r\n\r\nSo I think there may be something wrong in `tf.metrics.mean_iou`.\r\n\r\nA strange thing is that the original code(no `tf.to_float()`) used to work normally......\r\nAfter I add `distribution strategy`  to the config of the estimator and do some other changes , it broke down. \r\n\r\n", "comments": ["Mustafa, can you make this check in Estimator be less false-positive-friendly? It should take anything convert_to_tensor is happy with, not just things which are exactly tensors or ops.", "Hey @pavithrasv, could you please relax the assertion as Alex recommended?\r\nthanks"]}, {"number": 23730, "title": "Multi GPU, GPU to GPU communication stops", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: NVIDIA DOCKER Image\r\n- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NVIDIA DOCKER Image\r\n- **GPU model and memory**: Titan Xp (8 pcs.)\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n\r\nHi, I'm trying to utilize 8 gpus for my GAN training. I use 8 pieces of Titan Xp. Detail of gpus are\r\n```bash\r\nGPU | Bus-Id\r\n0 | 00000000:1B:00.0\r\n1 | 00000000:1C:00.0\r\n2 | 00000000:1D:00.0\r\n3 | 00000000:1E:00.0\r\n4 | 00000000:3D:00.0\r\n5 | 00000000:3F:00.0\r\n6 | 00000000:40:00.0\r\n7 | 00000000:41:00.0\r\n```\r\n\r\nAfter I build model, I use this code to sync every device. Variables in each device has same prefix such as 'Tower_0', 'Tower_1', ..., 'Tower_7' and the 'Tower_0' is the main one.\r\n```python\r\n# Creating ops\r\ndef device_sync_op(prefix, main_idx):\r\n    import re\r\n    all_vars = tf.global_variables() + tf.local_variables()\r\n    var_by_name = dict([(v.name, v) for v in all_vars])\r\n    post_init_ops = []\r\n    match_re = r\"{}\\d+\".format(prefix.format(\"\"))\r\n    regex = re.compile(match_re)\r\n    main_tower_name = prefix.format(main_idx)\r\n    for v in all_vars:\r\n        tower_name = regex.search(v.name)\r\n        if tower_name is None:\r\n            continue\r\n\r\n        if main_tower_name == tower_name:\r\n            # no need to copy to main tower\r\n            continue\r\n\r\n        tower_name = tower_name.group()\r\n\r\n        copy_from = var_by_name.get(v.name.replace(tower_name, main_tower_name))\r\n        if v.name == copy_from:\r\n            continue\r\n\r\n        if copy_from is not None:\r\n            post_init_ops.append(v.assign(copy_from.read_value()))\r\n        else:\r\n            UserWarning(\"Cannot find {} in the graph!\".format(v.name.replace(tower_name, main_tower_name)))\r\n\r\n    return tf.group(*post_init_ops, name=\"sync_variables_from_main_tower\")\r\n\r\n# This is the code I use to sync every devices\r\nsync_op = device_sync_op(\"Tower_%d\", 0)\r\nsess.run(sync_op)\r\n```\r\n\r\nThis code works perfectly, if I utilize only 2 gpus from different PCI BUS group. As you can see above, GPU 0-3, GPU 4-7 have sequent PCI BUS addresses. So I call each group as PCI BUS group for easy explanation. In other words, if I use gpus (0, 4), (0,5), (0, 6), (0, 7), (1, 4), ..., (3, 7), the code works.\r\nHowever, if I try to use 2 gpus from same group such as (0, 1), (0, 2), (0, 3), ..., (6, 7), it just doesn't response. I have to kill the process using **kill -9** or reboot computer at the worst. I cannot utilize more than 2 gpus for my model because of this problem.\r\n\r\nIs there any options I have to enable or drivers to install more to solve this problem? Or is it impossible to make each device communicate each other?\r\n\r\nWaiting for your wise solutions.", "comments": ["Found solutions.\r\nIt was due to ACSCtl function of Supermicro.\r\nhttps://github.com/NVIDIA/nccl/issues/19", "What\u2019s you motherboard/system? Some Supermicro PCIe systems are known to have this bug. I have met the same issue and upgrading BIOS firmware resolves this issue.", "> What\u2019s you motherboard/system? Some Supermicro PCIe systems are known to have this bug. I have met the same issue and upgrading BIOS firmware resolves this issue.\r\n\r\nYes, it was Supermicro bug. I updated BIOS and I fixed my problem!\r\n\r\nThanks for your concerning."]}, {"number": 23729, "title": "pybullet.error: createCollisionShape failed.", "body": "It is on Ubuntu 16.04LTS, and it shows this error:\r\n\r\n![image](https://user-images.githubusercontent.com/15700681/48466407-c1a1bd00-e820-11e8-80f4-695690415552.png)\r\n", "comments": ["It doesn't look like an error on TensorFlow's end. Can you please share your bullet-test.py script?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "ok\uff0c I have solved it. thank you!", "@JobSmith how did you solve it ?", "For anyone still looking GEOM_BOX requires the halfExtents argument, even though in the quickstart guide it claims that there is a default. So `p.createCollisionShape(p.GEOM_BOX)` won't work, but `p.createCollisionShape(p.GEOM_BOX, halfExtents=[1, 1, 1])` will"]}]