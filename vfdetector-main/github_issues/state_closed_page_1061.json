[{"number": 21455, "title": "multi-hot representation and sequence_categorical_column_with_identity", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **mobile device**: N/A\r\n- **Exact command to reproduce**: See code below\r\n\r\n### Describe the problem\r\n`sequence_feature_columns` are great, but they are not compatible with the multi-hot representation of a categorical column. I want to multi-hot represent a feature of shape `[batch, time, dim]` with `indicator_column` and `sequence_categorical_column_with_identity`, but when using the `sequence_input_layer` it produces a `TypeError.` Are you familiar with this problem and do you have a solution for this or am I missing something? Thank you very much for any help!\r\n\r\n### Source code / logs\r\nA simple example that produces the error:\r\n```python\r\nx = tf.placeholder(tf.int32, [None, None, 2])\r\ncolumn = tf.feature_column.indicator_column(tf.contrib.feature_column.sequence_categorical_column_with_identity('x', 3))\r\ninput_layer, sequence_length = tf.contrib.feature_column.sequence_input_layer({'x': x}, [column])\r\n```\r\n```python\r\nTypeError: sequence item 2: expected str instance, Tensor found\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "Thanks @neuencer for the report. We are actively working on sequence feature columns, and it would be great to get more detail about your use-case as well.\r\n\r\nFor the immediate term, the issue seems to be that sequence feature columns currently expect SparseTensors as inputs. (This matches the expected output if you are parsing your features from TFRecords.) For the above example, it works if you change just the first line and make `x` a SparseTensor:\r\n\r\n```\r\nx = tf.SparseTensor(\r\n    indices=[[0, 0]], values=tf.placeholder(tf.int32, [1], name='x_holder'),\r\n    dense_shape=[1, 1])\r\ncolumn = tf.feature_column.indicator_column(tf.contrib.feature_column.sequence_categorical_column_with_identity('x', 3))\r\ninput_layer, sequence_length = tf.contrib.feature_column.sequence_input_layer({'x': x}, [column])\r\n\r\nwith tf.Session() as sess:\r\n  print(sess.run(input_layer, feed_dict={'x_holder:0': [2]}))\r\n\r\n>>> [[[ 0.  0.  1.]]]\r\n```\r\n\r\nFor the short term, you can perhaps feed the expect input in as SparseTensors. But I understand that may seem unnatural-- can you tell us a little bit more about what your input data looks like, and how you want it to feed in to the sequence feature columns?", "Thanks for your respond @karmel! But I realized my post probably wasn't specific enough (sorry for that). My issue is with the multi-hot representation that usually happens by using the indicator column (see: [tf.feature_column.indicator_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column)). If my input is `[\"bob\", \"wanda\"]` or `[0, 2]`, I want to get a `[1, 0, 1]` vector and not `[[1, 0, 0], [0, 0, 1]]`. Hope that helps.  More about my use case and data shape follows below.\r\n\r\nI'm implementing a poker bot using a recurrent neural network and wanted to try out feature columns for the input layer for learning purposes. The features that are related to my problem or the multi-hot representation are the player's hole cards and the community board.\r\nThe Shape of hole cards: (batch, time, 2).\r\nThe Shape of the board: (batch, time, 5).", "Thanks, @neuencer , but I don't quite follow-- the correct one-hot representation of sequential data should be in the shape [batch, time, one hot dimensionality]. Is what you want a count of classes across all timesteps for a given example? Can you sum along the timestep axis?", "Thanks, @karmel, I think the important thing is that I talk about multi-hot encoding. I don't want to count along the time axis, I want to count one feature(i.e. hole card 1) and the second feature (i.e. hole card 2) with the same classes together, while these two features are represented in one string tensor of shape `[batch, time, 2]`. I know I could just use two separate columns for it, but it's unnecessary because the network would have to learn one relationship twice.\r\nSo I have a Tensor `hole_cards` of shape `[batch, time, 2]`, which looks like this `hole_cards[0][0][:] = ['Ah', 'Kd']`. So, for example, if the class space is `[Ah, Ad, Kh, Kd]` I want to transform `['Ah', 'Kd']` to `[1,0,1,0]`. And then having an additional time and batch axis. So the combined column would have shape `[batch, time, 4]` for this example or 52 instead of 4 for a complete poker deck. ", "Ah, I see. In that case, I think your best options might be:\r\n* Handle the transformations when parsing the data, produce the multi-hot column, and pass in to a sequence numeric column. Since the data is already multi-hot, sequence numerical is appropriate.\r\n* Pass the data through as an indicator column, and handle to summation/collapsing in your model code before passing through to the RNN/other layers.\r\n* Create a mutli-hot indicator column that meets the needs here, and contribute it back to TensorFlow for others to use.\r\n\r\nThoughts?", "I found a quick hack in the feature_column.py. But to be honest, the solution is pretty dumb, but it works and I couldn't come up with something better. I just deleted the part that caused the error. So I changed the `_get_sparse_tensors` of `_SequenceCategoricalColumn` to just `return _CategoricalColumn.IdWeightPair(inputs.get(self), None)`.\r\n\r\nSo yes, this shouldn't be the solution, but I couldn't see how the official `_get_sparse_tensors` is useful in any way. So please correct me if I'm wrong.", "The sparse tensor transformation is important for other downstream steps (ie, converting into an embedding). Since those steps don't apply here, removing that line works for you, but likely would not work for the general case. ", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "_get_sparse_tensors is a private function used to transform input data into the expected format. I'm glad you found a workaround; closing this issue for now, but please reopen if you think that is incorrect."]}, {"number": 21454, "title": "Turning off Teacher Forcing in decoders of Seq2Seq models ", "body": "Have I written custom code:N/A\r\nOS Platform and Distribution :macOS Sierra 10.12.6\r\nTensorFlow installed from : tensforflow.org\r\nTensorFlow version: TensorFlow Version: 1.3.0\r\nBazel version: NA\r\nCUDA/cuDNN version:NA\r\nGPU model and memory:NA\r\nExact command to reproduce:NA\r\nMobile device:NA\r\n\r\n\r\nHello , \r\n\r\nI'm posting this request here however I'm not sure if this is a new feature request or it is something already doable in tensorflow but I can not find find an example or any documentation about it  . \r\n\r\nGeting to the Point . I'm experimenting with  seq2seq models . I have followed all the examples available and all is good. Now my model uses Teacher forcing ( passing the true output to the decoder network during training ) and nI  would like to turn it off to see how the model performs without it . unfortunately I do not see any possible way to do it without making my own Helper . I tried to use the inferenceHelper during the Training instead of the TraningHelper. Since the inferenceHelper does not require the true output but the model gives a run time error after some epochs where inferenceHelper is returning predictions with shape other than the expected. my guess is that I inferenceHelper is not meant to be used during training ( as the name suggests) . \r\n\r\nis there a way to turn off the Teacher Forcing , if not are there any plans to incorporate this in Tensorflow ? \r\n\r\nmy original decoder network code : \r\n\r\n```\r\n`def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \r\n                     target_sequence_length, max_summary_length, \r\n                     output_layer, keep_prob):\r\n\"\"\"\r\nCreate a decoding layer for training\r\n:param encoder_state: Encoder State\r\n:param dec_cell: Decoder RNN Cell\r\n:param dec_embed_input: Decoder embedded input\r\n:param target_sequence_length: The lengths of each sequence in the target batch\r\n:param max_summary_length: The length of the longest sequence in the batch\r\n:param output_layer: Function to apply the output layer\r\n:param keep_prob: Dropout keep probability\r\n:return: BasicDecoderOutput containing training logits and sample_id\r\n\"\"\"\r\n\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\r\n                                                    sequence_length=target_sequence_length,\r\n                                                    time_major=False)\r\n\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, encoder_state, output_layer)\r\n\r\ntraining_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\r\n                                                            impute_finished=True,\r\n                                                            maximum_iterations=max_summary_length)[0]\r\nreturn training_decoder_output\r\n````\r\n\r\n\r\n\r\nthank you ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "is there anyone looking at this ? ", "'turning off' will means to change the 'graph'. So you may need to manually get the weight and calculate by yourself. I can't figure this out either. I want to calculate the validation error during training ", "Since tf.contrib has been depreciated, seq2seg has been moved under Tensorflow-Addons, you need to install tensorflow addons using \r\n`pip install tensorflow-addons`\r\nand import the addons like below.\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n```\r\nRefer to the `seq2seq` documentation [here](https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq) for the classes which would suite your requirement.\r\nIf you still face the issue, open new issue under https://github.com/tensorflow/addons.\r\n"]}, {"number": 21453, "title": "Cheery pick TFLite iOS build fix into 1.10 branch", "body": "- Fix flatbuffers to use 1.8 version.\r\n- Fix cc flags to not include --std=c++11.\r\n- Drop i386 build in fat binary.\r\n\r\nPiperOrigin-RevId: 205160898", "comments": []}, {"number": 21452, "title": "ConfigProto.experimental.num_dev_to_dev_copy_streams defaults to 0", "body": "by proto3 semantics.  Silently correct that value to 1, without logging\r\nan error.\r\n\r\nPiperOrigin-RevId: 205157429", "comments": ["(For the record, this is basically a cherrypick of an existing commit - bb52a6663a0141de53ddaf844f6c7087c0ddf7f7)"]}, {"number": 21451, "title": "Delete confusing comment related to missing output Variable", "body": "The method `KMeans().training_graph()` does not return anymore the\r\noutput Variable `cluster_centers_var`. Thus the related comment\r\nis outdated and should be deleted.\r\nSee related [commit](https://github.com/tensorflow/tensorflow/commit/3110185270e93e0b6a3e82be9199febed1239602).", "comments": []}, {"number": 21450, "title": "Adam docu: learning rate formula", "body": "Fixed formula for the learning rate in the documentation of class AdamOptimizer.\r\n\r\nBefore, it read\r\n\r\n    \\text{learning_rate} * \\sqrt{(1 - beta_2^t) / (1 - beta_1^t)}\r\n\r\n(i.e. the sqrt applied to both numerator and denominator). The new version\r\n\r\n    \\text{learning_rate} * \\sqrt{(1 - beta_2^t)} / (1 - beta_1^t)\r\n\r\n(sqrt applied to numerator only) is consistent with the Adam paper and how it\r\nis actually implemented (currently l. 185).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 21449, "title": "Traceback (most recent call last):", "body": "While running this code on CYGWIN \r\n\r\n`$ python -m scripts.label_image \\\r\n>   --graph=tf_files/retrained_graph.pb  \\\r\n>   --image=tf_files/flower_photos/daisy/3475870145_685a19116d.jpg`\r\n\r\nthis error showed up\r\n\r\n` Traceback (most recent call last):\r\n File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/home/Admin/tensorflow-for-poets-2/scripts/label_image.py\", line 24, in <module>\r\n    import numpy as np\r\nImportError: No module named numpy\r\n`", "comments": ["@pumpkinband it looks like you have not installed numpy. Please run the following in your terminal.\r\n\r\n```\r\npip install numpy==1.14.5\r\n```", "@av8ramit  actually i tried but every time this error keeps going\r\n`Command \"/usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-i                                                                                                                                  nstall-LkomBi/numpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f                                                                                                                                  .read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" i                                                                                                                                  nstall --record /tmp/pip-record-bAHGSs/install-record.txt --single-version-exter                                                                                                                                  nally-managed --compile\" failed with error code 1 in /tmp/pip-install-LkomBi/num                                                                                                                                  py/\r\n`", "I'd recommend trying in a virtual environment with the latest corresponding python version. If that doesn't work maybe update your setuptools version as well.\r\n\r\n```\r\npip install --upgrade setuptools==39.1.0\r\n```", "Tried both the things but 'numpy' isn't installing. ", "I recommend filing this issue on the [numpy](https://github.com/numpy/numpy/issues) repository as they will have a better idea of how to help you. After you get that working please comment or open a new issue if there is any issue with the TensorFlow portion.", "Thanks a lot brother. "]}, {"number": 21448, "title": "Fix incorrect doc in tf.nn.convolution ", "body": "The ranks of input and filter should be N+2 instead of N.\r\nSince the function computes N-D convolution, the input's rank should be N+2 with additional batch and input_channel dimensions, and the filter's rank should be N+2 with additional input_channel and output_channel dimensions.\r\nThe function actually requires the length of dilation_rate/stides to be rank(input)-2. Otherwise, it will raise ValueError.", "comments": ["@jbms could you take a look?", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21447, "title": "Camera is showing as half screen while running the application (Android)", "body": "Android - Oneplus 3", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution - Windows\r\nTensorFlow installed from - https://github.com/tensorflow/tensorflow.git\r\nTensorFlow version - 1.10.0rc\r\nBazel version - NA\r\nCUDA/cuDNN version - NA\r\nGPU model and memory -NA\r\nExact command to reproduce - NA\r\nMobile device - OnePlus3, Mi, Samsung J6, Samung J7", "Can you please elaborate and provide explicit details of the problem you're facing. Explain to us what you tried to do, what the issue was and some error logs etc. ", "@rohan100jain , There are no errors actually when i open the app application to detect camera is showing only 70 percent to the screen, like camera is not opening with fullscreen", "Please see the screenshot below \r\n\r\n![screenshot_20180814-150028](https://user-images.githubusercontent.com/25867821/44084022-38a6fd52-9fd3-11e8-8ff2-089f572f0064.jpg)\r\n", "Black screen is appearing in the bottom of the camera like in the image. \r\n\r\nI want to make it full screen is there any way to make it full screen", "@rohan100jain Is there any way to remove that black screen from the camera.", "@rohan100jain  Is there any work around solution to make camera with full screen", "Which app are you running? I'm not sure what you mean by \"the application\" here", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this since we didn't get any response from the user. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@rohan100jain ,he is referring to TFLite Demo app. In this app, he wants the camera to cover full screen and don't want the blank space at the bottom of the camera.", "@prasanna532 Did you find any solution?", "@true21 ya i had found the solution you need to increase the pixel then it is coming full screen but the problem it not finding the  objects quickly and it is hanging some times.\r\nbelow is the line for the full screen\r\nprivate static final Size DESIRED_PREVIEW_SIZE = new Size(1920, 720);"]}, {"number": 21446, "title": "the error of freeze model", "body": "- OS Platform:  Ubuntu 16.04\r\n- TensorFlow installed from:  anaconda\r\n- TensorFlow version: GPU-1.8\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: 9.0 / 7.1.2\r\n- GPU model and memory:16G\r\n\r\nThe model is mobilenetV2 + LSTM + attention.\r\nMobilenetV2 is restored from the model zoo in [https://github.com/tensorflow/models/tree/master/research/slim](https://github.com/tensorflow/models/tree/master/research/slim). It is not trained during the training of the whole model.\r\nLSTM and attention is trained.\r\n\r\nThe code about model is shown as following.\r\n```\r\n    def mobilenet_v2_rnn_attention(self, X, num_classes, dropout_keep_prob=0.8, is_train=False):\r\n        rnn_size = 4096\r\n        num_layers = 8\r\n        attention_size = 1024\r\n        arg_scope = training_scope()\r\n        with slim.arg_scope(arg_scope):\r\n            net_vis, end_points = mobilenet_base(X, num_classes=num_classes)\r\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\r\n            with tf.variable_scope('Logits_out'):\r\n                orig_shape = net_vis.get_shape().as_list()\r\n                net = tf.reshape(net_vis, [-1, orig_shape[1] * orig_shape[2], orig_shape[3]])\r\n\r\n                def lstm_cell():\r\n                    return tf.contrib.rnn.LSTMCell(rnn_size)\r\n\r\n                stack = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(0, num_layers)], state_is_tuple=True)\r\n                net, _ = tf.nn.dynamic_rnn(stack, net, dtype=tf.float32)\r\n                # (B, T, D) => (T, B, D)\r\n                net = tf.transpose(net, (1, 0, 2))\r\n                net = attention(net, attention_size, True)\r\n                net = slim.fully_connected(net, num_classes, activation_fn=None, scope='Logits_out1')\r\n        return net, net_vis\r\n```\r\n\r\n\r\n\r\n\r\nAt the first, I export the graph as following.\r\n```\r\n    X = tf.placeholder(tf.float32, [None, height, width, 3], name=\"inputs_placeholder\")\r\n    net, net_vis = build_model(X, num_classes, 1.0, False, arch_model)\r\n    net = tf.nn.sigmoid(net)\r\n    predict = tf.reshape(net, [-1, num_classes], name='predictions')\r\n\r\n    graph_def = graph.as_graph_def()\r\n\r\n    with gfile.GFile(FLAGS.output_file, 'wb') as f:\r\n      f.write(graph_def.SerializeToString())\r\n```\r\n\r\nThen, freeze the model using the api\r\n\r\n```\r\npython /root/anaconda3/pkgs/tensorflow-base-1.8.0-py36hc1a7637_0/lib/python3.6/site- \r\n    packages/tensorflow/python/tools/freeze_graph.py \\\r\n      --input_graph=${INFERENCE_GRAPH_PATH}/${TMP_GRAPH} \\\r\n      --input_checkpoint=${CHECKPOINT_PATH}/model.ckpt-3800 \\\r\n      --input_binary=true \\\r\n      --output_graph=${CHECKPOINT_PATH}/${EXPORT_INF_GRAPH} \\\r\n      --output_node_names=${OUTPUT_NODE_NAME}\r\n```\r\n\r\nThe error is shown as following.\r\n\r\n`[libprotobuf FATAL google/protobuf/wire_format.cc:830] CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nExact command to reproduce\nMobile device"]}, {"number": 21445, "title": "Fix incorrect doc in sparse_softmax_cross_entropy_with_logits", "body": "This fix fixes the incorrect doc in sparse_softmax_cross_entropy_with_logits.\r\nIn sparse_softmax_cross_entropy_with_logits, `labels` must be `int32` or `int64` (not the same as `logits` in existing docs), as is seen in:\r\nhttps://github.com/tensorflow/tensorflow/blob/46cf73f0214bc6208295e36650f1a8ffde4abdd7/tensorflow/core/ops/nn_ops.cc#L1092-L1099\r\n\r\nThis fix fixes #21435.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 21444, "title": "Estimator API -- Incorrect Super-Type Check for Hooks", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.4 LTS (Xenial Xerus)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: From source\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 7.1.4.18\r\n- **GPU model and memory**: Quadro K620 and Tesla K40c -- 2GB and 11.5GB respectively.\r\n- **Exact command to reproduce**:\r\n```\r\ndef model_fn(features, labels, mode):\r\n    \r\n    loss = tfsi_model(features)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        train_op, grads, saver = minimize(loss)\r\n        writer, merged = prepare_summary(tf.get_default_graph(), loss, grads)\r\n        chkpt_hook = tf.train.CheckpointSaverHook(\r\n            FLAGS.model_dir, \r\n            save_steps=FLAGS.saving_ckpt_freq, \r\n            saver=saver\r\n        )\r\n        summ_hook = tf.train.SummarySaverHook(\r\n            save_steps=FLAGS.saving_summ_freq,\r\n            output_dir=FLAGS.log_directory, \r\n            summary_writer=writer,\r\n            summary_op=merged\r\n        )\r\n        hooks = [chkpt_hook, summ_hook]\r\n        return tf.estimator.EstimatorSpec(mode, \r\n                                          loss=loss, \r\n                                          train_op=train_op, \r\n                                          training_chief_hooks=hooks)\r\n    else:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n```\r\n\r\nHello Tensorflow Devs,\r\nI'm sorry to report too many failures within Estimator API just in couple of days; but it seems it is really buggy, guys. When it checks out for the (super)type of session hooks, it looks for **tensorflow.python.training.session_run_hook.SessionRunHook**; but \"isinstance\" method fails to do so for it assumes that their type is **tensorflow.python.training.basic_session_run_hooks** though it is not. Would you mind replacing it with a proper validation method, please?\r\n\r\n```\r\n\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 3, '_save_checkpoints_steps': 15, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_num_worker_replicas': 1, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f8d98aaf780>, '_tf_random_seed': None, '_task_id': 0, '_device_fn': None, '_model_dir': './output', '_save_summary_steps': 5, '_session_config': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8d98aafcf8>, '_log_step_count_steps': 100, '_service': None, '_global_id_in_cluster': 0, '_is_chief': True, '_task_type': 'worker', '_master': ''}\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 66 with algorithm = nccl and num_packs = 1\r\nPreparing Tensor Summaries...\r\nAdding summary for training loss\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nType of hook:  <class 'tensorflow.python.training.basic_session_run_hooks.SummarySaverHook'>\r\nINFO:tensorflow:Done calling model_fn.\r\nPreparing Tensor Summaries...\r\nAdding summary for training loss\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nType of hook:  <class 'tensorflow.python.training.basic_session_run_hooks.SummarySaverHook'>\r\nINFO:tensorflow:Done calling model_fn.\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-10-cc8d515a6342> in <module>()\r\n     12 train_spec = tf.estimator.TrainSpec(gen_input_fn('train', FLAGS.num_epochs))\r\n     13 valid_spec = tf.estimator.EvalSpec(gen_input_fn('valid', 1))\r\n---> 14 tf.estimator.train_and_evaluate(classifier, train_spec, valid_spec)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    445         '(with task id 0).  Given task id {}'.format(config.task_id))\r\n    446 \r\n--> 447   return executor.run()\r\n    448 \r\n    449 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py in run(self)\r\n    529         config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    530       logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 531       return self.run_local()\r\n    532 \r\n    533     # Distributed case.\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py in run_local(self)\r\n    667           input_fn=self._train_spec.input_fn,\r\n    668           max_steps=self._train_spec.max_steps,\r\n--> 669           hooks=train_hooks)\r\n    670 \r\n    671       if not self._continuous_eval_listener.before_eval():\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    364 \r\n    365       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 366       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    367       logging.info('Loss for final step: %s.', loss)\r\n    368       return self\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1115   def _train_model(self, input_fn, hooks, saving_listeners):\r\n   1116     if self._distribution:\r\n-> 1117       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1118     else:\r\n   1119       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n   1248             training_hooks=training_hooks,\r\n   1249             training_chief_hooks=training_chief_hooks,\r\n-> 1250             scaffold=scaffold)\r\n   1251         return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n   1252                                                hooks, global_step_read_tensor,\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/model_fn.py in __new__(cls, mode, predictions, loss, train_op, eval_metric_ops, export_outputs, training_chief_hooks, training_hooks, scaffold, evaluation_hooks, prediction_hooks)\r\n    304         raise TypeError(\r\n    305             'All hooks must be SessionRunHook instances, given: {}'.format(\r\n--> 306                 hook))\r\n    307 \r\n    308     scaffold = scaffold or monitored_session.Scaffold()\r\n\r\nTypeError: All hooks must be SessionRunHook instances, given: PerDevice:{'/job:localhost/replica:0/task:0/device:GPU:1': <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x7f8ce4173ac8>, '/job:localhost/replica:0/task:0/device:GPU:0': <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x7f8d140390b8>}\r\n```", "comments": ["@cy89 Do you have any update?", "So weird, could you provide a minimal example? if I remember correctly, `tf.train.CheckpointSaverHoo` and `tf.train.SummarySaverHook` are subclass of `SessionRunHook` indeed.", "@facaiy It is absolutely weird... Code is simple:\r\n```\r\n\r\ndef model_fn(features, labels, mode):\r\n    \r\n    loss = tfsi_model(features)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        train_op, grads, saver = minimize(loss)\r\n        writer, merged = prepare_summary(tf.get_default_graph(), loss, grads)\r\n        chkpt_hook = tf.train.CheckpointSaverHook(\r\n            FLAGS.model_dir, \r\n            save_steps=FLAGS.saving_ckpt_freq, \r\n            saver=saver\r\n        )\r\n        summ_hook = tf.train.SummarySaverHook(\r\n            save_steps=FLAGS.saving_summ_freq,\r\n            output_dir=FLAGS.log_directory, \r\n            summary_writer=writer,\r\n            summary_op=merged\r\n        )\r\n        hooks = [chkpt_hook, summ_hook]\r\n        return tf.estimator.EstimatorSpec(mode, \r\n                                          loss=loss, \r\n                                          train_op=train_op, \r\n                                          training_chief_hooks=hooks)\r\n    else:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\nconfig = tf.estimator.RunConfig(\r\n    model_dir=\"./output\",\r\n    save_summary_steps=FLAGS.saving_summ_freq, \r\n    save_checkpoints_steps=FLAGS.saving_ckpt_freq,\r\n    keep_checkpoint_max=3,\r\n    train_distribute=tf.contrib.distribute.MirroredStrategy()\r\n)\r\nclassifier = tf.estimator.Estimator(\r\n    model_fn, \r\n    config=config\r\n)\r\ntrain_spec = tf.estimator.TrainSpec(gen_input_fn('train', FLAGS.num_epochs))\r\nvalid_spec = tf.estimator.EvalSpec(gen_input_fn('valid', 1))\r\ntf.estimator.train_and_evaluate(classifier, train_spec, valid_spec)\r\n```\r\n\r\nI think you don't need the rest of the code for it is irrelevant to the problem. Note that... I'm using Python3. Perhaps the behavior attributed to \"isinstance(...)\" had changed since Python2.x.", "same problem with tf 1.10.0. It seems that hooks under distributed training will become `PerDevice`, but desired is `tensorflow.python.training.basic_session_run_hooks.LoggingTensorHook`. Hope it can be fixed soon. \r\n```\r\nTypeError: All hooks must be SessionRunHook instances, given: PerDevice:{'/replica:0/task:0/device:GPU:0': <tensorflow.python.training.basic_session_run_hooks.LoggingTensorHook object at 0x7ff1371e5350>, '/replica:0/task:0/device:GPU:1': <tensorflow.python.training.basic_session_run_hooks.LoggingTensorHook object at 0x7ff290043850>}\r\n\r\n```", "Yes, indeed @klauscc .\r\n\r\nAnother problem is that this **batch_all_reduce** function somehow returns wrong batch size. I specify 5; but it returns 66 :S :) Well anyway. I think it should be a separate bug report.", "@klauscc Yes, it seems that Hooks are packed in PerDevice so that TypeError is raised. \r\n\r\n@iliTheFallen Thanks for your example. Unfortunately, the codes seems not self contained and I can't run it. It would be much useful for debug if the example could be verifiable.", "Same issue here.\r\n\r\n```python\r\nTypeError: All hooks must be SessionRunHook instances, given: PerDevice:{'/replica:0/task:0/device:GPU:0': <__main__.FoldHook object at 0x7fbfd79b72e8>, '/replica:0/task:0/device:GPU:1': <__main__.FoldHook object at 0x7fbe9c5753c8>}\r\n```", "@facaiy I wrote a small example that is runnable and that raises the issue mentioned above. Please find it here:\r\nhttps://gist.github.com/patzm/937a4f74a0555169feea5bd6bfe17397", "@patzm many thanks. It's really useful!\r\n\r\ncc @ispirmustafa @guptapriya Hi, any thoughts about this bug?", "FYI: Things should work without any issues if distribution strategy is not used. distribution strategy is under development. thank you for trying them and giving us feedback. ", "There is a PR which should be checked in soon: https://github.com/tensorflow/tensorflow/pull/19088", "The bug has been fixed: https://github.com/tensorflow/tensorflow/commit/007c234dfc3388f8bea96bcc9867d2f90d419098. Closing the issue.", "@yuefengz  Thank you!", "@yuefengz I am getting this error:\r\n\r\n```\r\nTypeError: All hooks must be `SessionRunHook` instances, given: <__main__.MyEvalHook object at 0x1335ba908>\r\n```\r\n\r\nAfter running\r\n\r\n```python\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=eval_input_fn,\r\n        steps=100,\r\n        start_delay_secs=0,\r\n        throttle_secs=5,\r\n        hooks=[MyEvalHook()]\r\n    )\r\n```\r\n\r\nWhere `MyEvalhook` is just:\r\n\r\n```python\r\nfrom tensorflow_core.python.training.session_run_hook import SessionRunArgs, SessionRunHook\r\n\r\nclass MyEvalHook(SessionRunHook):\r\n\r\n    def __init__(self):\r\n        pass\r\n\r\n    def before_run(self, run_context):\r\n        return SessionRunArgs(\r\n            dict(\r\n                global_step=tf.compat.v1.train.get_or_create_global_step()\r\n            )\r\n        )\r\n\r\n    def after_run(self, run_context, run_values):\r\n        pass\r\n```\r\n\r\nI am on Tensorflow 2.1.0\r\n\r\nIs this another issue or the same as #21444 ?", "> @yuefengz I am getting this error:\r\n> \r\n> ```\r\n> TypeError: All hooks must be `SessionRunHook` instances, given: <__main__.MyEvalHook object at 0x1335ba908>\r\n> ```\r\n> \r\n> After running\r\n> \r\n> ```python\r\n>     eval_spec = tf.estimator.EvalSpec(\r\n>         input_fn=eval_input_fn,\r\n>         steps=100,\r\n>         start_delay_secs=0,\r\n>         throttle_secs=5,\r\n>         hooks=[MyEvalHook()]\r\n>     )\r\n> ```\r\n> \r\n> Where `MyEvalhook` is just:\r\n> \r\n> ```python\r\n> from tensorflow_core.python.training.session_run_hook import SessionRunArgs, SessionRunHook\r\n> \r\n> class MyEvalHook(SessionRunHook):\r\n> \r\n>     def __init__(self):\r\n>         pass\r\n> \r\n>     def before_run(self, run_context):\r\n>         return SessionRunArgs(\r\n>             dict(\r\n>                 global_step=tf.compat.v1.train.get_or_create_global_step()\r\n>             )\r\n>         )\r\n> \r\n>     def after_run(self, run_context, run_values):\r\n>         pass\r\n> ```\r\n> \r\n> I am on Tensorflow 2.1.0\r\n> \r\n> Is this another issue or the same as #21444 ?\r\n\r\nHi\uff0c have you resolved it? I have encountered a similar problem in tensorflow==1.15."]}, {"number": 21443, "title": "Fix incorrect reference to version compatibility", "body": "This fix tries to address the issue raised in #21434 where\r\nthe link to version compatibility is not correct.\r\nThe link should be `version_compat.md`, not `version_semantics.md`.\r\n\r\nThis fix fixes #21434.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 21442, "title": "tensorflow.python.framework.errors_impl.InternalError: Could not allocate ndarray", "body": "\r\nHI,\r\nI am using running Places pre-trained VGG16 network in python  using CPU. I got this error? How to solve this memory issue after several computations? Thanks\r\nC:/Experiment/PicAlertData/public/4326658334.jpg\r\nTraceback (most recent call last):\r\n  File \"C:\\Python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Could not allocate ndarray\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Chiru\\places_trained_vgg16.py\", line 274, in <module>\r\n    model = VGG16_Places365(weights='places')\r\n  File \"C:\\Users\\Chiru\\places_trained_vgg16.py\", line 221, in VGG16_Places365\r\n    model.load_weights(weights_path)\r\n  File \"C:\\Python3.5\\lib\\site-packages\\keras\\engine\\network.py\", line 1161, in load_weights\r\n    f, self.layers, reshape=reshape)\r\n  File \"C:\\Python3.5\\lib\\site-packages\\keras\\engine\\saving.py\", line 928, in load_weights_from_hdf5_group\r\n    K.batch_set_value(weight_value_tuples)\r\n  File \"C:\\Python3.5\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2440, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)\r\n  File \"C:\\Python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Could not allocate ndarray", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thanks, I fixed the problem! I used model inside loop in my coding! I changed it and kept outside of loop for multiple images and now, it worked.", "Then can you close the issue?", "Thanks a lot,I have same error,after used model outsied loop,problem solved. "]}, {"number": 21441, "title": "building error on mac", "body": "ERROR: /private/var/tmp/_bazel_chenxu/eac512cc1715441e7965427cfb136e20/external/protobuf_archive/BUILD:665:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:172:13: error: _assigning to 'char *' from incompatible type 'const char *'_\r\n        if (PyString_AsStringAndSize(key, &name, &name_size) < 0) {\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:189:13: error: _assigning to 'char *' from incompatible type 'const char *'_\r\n        if (PyString_AsStringAndSize(key, &camelcase_name, &name_size) < 0) {\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMACOS high sierra\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nlastest\r\n- **Python version**:\r\n3.7\r\n- **Bazel version (if compiling from source)**:\r\n bazel version\r\nhomebrew build\r\nBuild label: 0.15.2-homebrew\r\n- **GCC/Compiler version (if compiling from source)**:\r\nmac default gcc( Apple LLVM version 9.1.0 (clang-902.0.39.2))\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Im facing this as well. Seem to be affecting python3.7. Found this issue on protobufs repo \r\nhttps://github.com/protocolbuffers/protobuf/issues/4086\r\nThis pr(https://github.com/protocolbuffers/protobuf/pull/4862) in protobuf is supposed to have fixed this issue. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This is still an issue, recently i had build tensorflow from source, on python3.6.5 and it worked without any errors, but after switching to python 3.7, i'm also getting the same error : \r\n\r\n```\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:170:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:192:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:209:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:231:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:252:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:273:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:294:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:312:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:330:7: error: assigning to 'char *' from incompatible type 'const char *'\r\n  if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:22: note: expanded from macro 'PyString_AsStringAndSize'\r\n       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n```\r\n\r\nThis is still an issue, i was building the version in the **master** branch.\r\n\r\nPS : I'm building the non-gpu support version of this, only the cpu version with support for SSE4.2, MAVX, MAVX2 configuration parameters in bazel build command", "not all of our dependencies are compatible with python 3.7\r\nThat is why we are not able to release python 3.7 binaries.\r\nOne such dependency is protobuf, which is why you are running into this problem.\r\n\r\nMarking this as a duplicate of https://github.com/tensorflow/tensorflow/issues/20517\r\n"]}, {"number": 21440, "title": "Feature request : Allow variable (None) input_shape for Tflite toco", "body": "Hi,\r\n\r\n- Have I written custom code: **NO**\r\n- OS Platform and Distribution: **Ubuntu 18.0.4 LTS**\r\n- TensorFlow installed from: **Github 1.9 release branch**\r\n- TensorFlow version: **1.9**\r\n- Bazel version: **0.16.0**\r\n- CUDA/cuDNN version: **9.1 / 7.0**\r\n- GPU model and memory: **GTX 1070 Ti, 8 Go**\r\n- Exact command to reproduce: **see below**\r\n\r\nI'm working on a FCN (Fully Convolutional Network https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) and wanted to make it work on mobile. The advantage of this type of network is that we can use any dimensions as input, the network still work. (for example, an image 512x512 or 512x384 or 612x358...)\r\n\r\nTfLite allow us to convert a freezed graph (.pb) to Tflite format (.tflite), but we need to put a fixed size for \"input_shape\".\r\n\r\n bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=frozen.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/tmp/test.tflite --inference_input_type=FLOAT --input_arrays=input_image --output_arrays=output_image --input_shapes=\"1,**384,512**,3\"\r\n\r\nAnd this what I would like:\r\n\r\n bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=frozen.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/tmp/test.tflite --inference_input_type=FLOAT --input_arrays=input_image --output_arrays=output_image --input_shapes=\"1,**None,None**,3\"\r\n\r\nI'm wondering if anyone is working on this feature, because I think it can be very useful for a lot of people.\r\n\r\nThank you for your time, and sorry if I duplicate any thread, I didn't find this feature request.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@Kayoku @aselle @gargn I encountered exactly the same issue when using `toco` (or python code directly) to convert an existing [frozen protobuf](https://github.com/openfoodfacts/openfoodfacts-ai/tree/master/table_detection/data)\r\n\r\n```\r\ntoco \\\r\n  --output_file ../data/mobile.tflite \\\r\n  --graph_def_file ../data/frozen_inference_graph.pb \\\r\n  --output_format TFLITE \\\r\n  --inference_type FLOAT \\\r\n  --inference_input_type FLOAT \\\r\n  --input_arrays image_tensor --input_shapes 1,-1,-1,3 \\\r\n  --output_arrays num_detections,detection_boxes,detection_scores,detection_classes\r\n```\r\n\r\n- Have I written custom code: **NO**\r\n- OS Platform and Distribution: **macOS 10.13.6**\r\n- TensorFlow installed from: **binary release**\r\n- TensorFlow version: **r1.10**\r\n- Bazel version: **0.16.0** \r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n- Exact command to reproduce: **see above**\r\n- Mobile device: **iOS phone simulator x86_64**\r\n\r\nerror:\r\n```\r\n2018-08-15 17:30:01.024915: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 320, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 316, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 121, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 309, in convert\r\n    allow_custom_ops=self.allow_custom_ops)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 225, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 107, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-08-15 17:30:13.970601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:13.970887: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\\n2018-08-15 17:30:13.971000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:13.971086: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:13.971163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:13.971236: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:13.971306: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:13.971393: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:13.971483: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond\\n2018-08-15 17:30:13.971599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:13.971675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:13.971748: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\\n2018-08-15 17:30:13.971897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:13.971978: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\\n2018-08-15 17:30:13.972025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:13.972095: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\\n2018-08-15 17:30:13.972168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\\n2018-08-15 17:30:13.972237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\\n2018-08-15 17:30:13.972279: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\\n2018-08-15 17:30:13.972375: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\\n2018-08-15 17:30:13.972454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\\n2018-08-15 17:30:13.972554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\\n2018-08-15 17:30:13.996164: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\\n2018-08-15 17:30:13.998047: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-15 17:30:13.998260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-15 17:30:13.998590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-15 17:30:13.998767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-15 17:30:13.998850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-15 17:30:13.999166: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:13.999256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:13.999301: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:13.999383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:13.999543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\\n2018-08-15 17:30:13.999764: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\\n2018-08-15 17:30:13.999921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\\n2018-08-15 17:30:14.000058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\\n2018-08-15 17:30:14.000144: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:14.000229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:14.000277: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:14.000345: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\\n2018-08-15 17:30:14.000388: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.000456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.000531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.000602: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.000645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.000736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.000814: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond\\n2018-08-15 17:30:14.000906: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.000977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.001055: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\\n2018-08-15 17:30:14.001125: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.001167: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.001230: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\\n2018-08-15 17:30:14.001273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.001337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.001379: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\\n2018-08-15 17:30:14.001445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.001485: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.001550: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\\n2018-08-15 17:30:14.001765: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\\n2018-08-15 17:30:14.001860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-15 17:30:14.001998: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-15 17:30:14.002151: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-15 17:30:14.002327: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-15 17:30:14.002511: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-15 17:30:14.002613: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-15 17:30:14.002750: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Size\\n2018-08-15 17:30:14.002823: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\\n2018-08-15 17:30:14.004045: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.004154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\\n2018-08-15 17:30:14.004212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.004283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\\n2018-08-15 17:30:14.004326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.004391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\\n2018-08-15 17:30:14.004435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\\n2018-08-15 17:30:14.004504: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\\n2018-08-15 17:30:14.004583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\\n2018-08-15 17:30:14.004649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\\n2018-08-15 17:30:14.004684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\\n2018-08-15 17:30:14.004742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\\n2018-08-15 17:30:14.004778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\\n2018-08-15 17:30:14.004868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\\n2018-08-15 17:30:14.004943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\\n2018-08-15 17:30:14.005008: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\\n2018-08-15 17:30:14.005075: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\\n2018-08-15 17:30:14.005141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\\n2018-08-15 17:30:14.005209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\\n2018-08-15 17:30:14.005273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\\n2018-08-15 17:30:14.112256: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1486 operators, 2523 arrays (0 quantized)\\n2018-08-15 17:30:14.296127: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1446 operators, 2441 arrays (0 quantized)\\n2018-08-15 17:30:14.543781: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1446 operators, 2441 arrays (0 quantized)\\n2018-08-15 17:30:14.718613: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge \\nAborted\\n'\r\nNone\r\n```", "@RockfordWei Your bug seems different than the one posted on this issue. There is a similar issue being tracked [here](https://github.com/tensorflow/tensorflow/issues/20878). Here is the recommendation:\r\n\r\n> The error message for the original issue (from \"resolve_tensorflow_switch.cc:95\") has been updated to give a little more context on the issue. Unfortunately, supporting arbitrary switch statements won't be supported in the near future. The best next step is to figure out how to prune that from the graph.\r\n\r\n@Kayoku It's currently not feasible to get TOCO to support this. A suggested work around is to put an arbitrary size into TOCO and use [ResizeInputTensor](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/interpreter.cc#L542) when calling the TFLite interpreter.", "@gargn do you have any sample code for this ?\r\n\r\nI have a image placedholder \r\n\r\n> self.input = tf.placeholder(tf.string, shape=(None,), name=\"input\")\r\n\r\n\r\nwhich I am preprocessing in a function and performing the resize to the required dimensions.\r\nI am getting issues in passing None to the input_shapes as you cant pass None or (None,)\r\n\r\n> TensorArrayGatherV3\\n2018-08-20 18:50:56.689410: F tensorflow/contrib/lite/toco/tooling_util.cc:822] Check failed: d >= 1 (0 vs. 1)\\nAborted (core dumped)\\n'\r\n> None\r\n\r\ncould you shed some light on this matter ?", "I skimmed over the paper and I think it should handle variable input size as you said. \r\n\r\nAssuming the network architecture can really handle arbitrary input size (I skimmed over the paper and I think it does), could you try this and let us know if it works: \r\n\r\n* When converting, use an arbitrary input size like (1, 512, 512, 3). \r\n* When using the interpreter, call `interpreter->ResizeInputTensor` to resize the input tensor before calling `interpreter->Invoke`. \r\n\r\nTheoretically it should do the trick. Let us know if it works for your case. ", "@miaout17 this looks like c++ code. I have very rusty cpp skills. Can we do something on Java or Python side ? ", "@thak123 Documentation on our Python API is available [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#interpreter). For resizing the tensor use [`resize_tensor_input`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/python/interpreter.py#L158).", "Nagging Assignees @miaout17, @gargn: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@gargn @thak123 @miaout17 \r\nI met the same problem in python. I already set the arbitrary size  [   1  704 1280    3] when converting. But when inference, I try to change the input size to [1, 1408, 2560, 3], the resize_tensor_input seems doesn't work for me.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninterpreter = tf.contrib.lite.Interpreter(model_path=\"saved_model.tflite\")\r\ninput_details = interpreter.get_input_details()\r\n# [{'quantization': (0.0, 0), 'index': 216, 'name': 'input_img', 'shape': array([   1,  704, 1280,    3], dtype=int32), 'dtype': <class 'numpy.float32'>}]\r\n\r\n### Resize the input tensor ###\r\ntensor_size = np.array((1,1408,2560,3), dtype=np.int32)\r\ninterpreter.resize_tensor_input(input_details[0]['index'],tensor_size)\r\nprint(input_details[0]['shape']) \r\n#  [   1  704 1280    3]\r\n```\r\nThe input tensor shape doesn't change.\r\n\r\nAnd then if I continue to try to \r\n`interpreter.allocate_tensors()`\r\n\r\nI would meet this error \r\n`RuntimeError: tensorflow/contrib/lite/kernels/concatenation.cc:75 t->dims->data[d] != t0->dims->data[d] (88 != 44)Node number 64 (CONCATENATION) failed to prepare.\r\n`\r\nthe tensorflow version is 1.12.0\r\n\r\nI don't know if I do this process wrong, or just the tflite doesn't support my model.\r\nThanks.\r\n", "Yeah LucyLu-LX. Same issue as yours\r\n`RuntimeError: tensorflow/contrib/lite/kernels/concatenation.cc:75 t->dims->data[d] != t0->dims->data[d] (88 != 44)Node number 64 (CONCATENATION) failed to prepare. `\r\n\r\nfor me too on tf1.13", "Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/concatenation.cc:74 t->dims->data[d] != t0->dims->data[d] (46 != 54)Node number 48 (CONCATENATION) failed to prepare.", "Has your problem been solved? I have a similar anomaly."]}, {"number": 21439, "title": "Reduce_sum error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bits\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: 9.0 / v7\r\n- **GPU model and memory**: Nvidia GTX TITANX 12GB, driver up to date (398.82)\r\n- **Bazel version**: N/A\r\n- **Mobile device**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nHi,\r\n\r\nSee test case below. On this specific setting it happens that tf.reduce_sum returns wrong values, as attested by the loss which should always return 0, but does not in this case (see logs at the end).\r\n\r\nThe bug does not happen when running on CPU, or when batchSize is strictly lesser than 33, or when \"models\" tensor has only 3 dimensions (even if we increase its size), or when \"models\" is smaller, or when the reduce_sum is applied on all axes instead of the last axis.\r\n\r\nThe reduce_sum is also correct for the first 32 elements of its first dimension, and only goes wrong for the 33rd (and next) element(s). \r\n\r\nIf we replace the \"diff = inputTensor - inputTensor\" tensor by a tf.zeros tensor, the bug does not appear.\r\n\r\nThe bug also happens with other reducing tensorflow methods (such as tf.reduce_min, tf.reduce_max and tf.reduce_mean).\r\n\r\nThanks in advance\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom random import random\r\n\r\nbatchSize = 33\r\n\r\nnp.random.seed(0)\r\nmodels = np.random.rand(batchSize, 2050, 2050, 2)\r\nmodels = np.asarray(models, dtype = \"float32\")\r\n\r\ndataPlaceholder = tf.placeholder(\"float32\", models.shape)\r\ninputTensor = dataPlaceholder\r\n\r\ndiff = inputTensor - inputTensor\r\n# diff = tf.zeros(inputTensor.shape)\r\n\r\nsum = tf.reduce_sum(diff, axis = 3)\r\n\r\nloss = tf.reduce_max(sum)\r\nloss = tf.Print(loss, [diff[batchSize-1,0,0], sum[batchSize-1,0,0]], message=\"diff[batchSize-1,0,0], reduce_sum[batchSize-1,0,0] : \")\r\n\r\nif __name__ == '__main__':\r\n\twith tf.Session() as sess:\r\n\t\tbatchLoss = sess.run(loss, feed_dict = {dataPlaceholder: models[:batchSize]})\r\n\t\tprint(\"batch loss : {}\".format(batchLoss))\r\n\r\n---------------------------------------------------\r\nbatch loss : 0.999999463558197\r\ndiff[batchSize-1,0,0], reduce_sum[batchSize-1,0,0] : [0 0][0.154408231]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "@rmlarsen Hi, could you please look into this issue ?", "@Lillypucien Looks like this was resolved in recent TF1.x. I tried your code with `TF1.15.3` and I cannot reproduce the issue. Please take a look the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/247c54a0df162eebec9513625add28d0/untitled.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21439\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21439\">No</a>\n"]}, {"number": 21438, "title": "Resnet50.py apply pooling at last layer regardless pooling parameter.", "body": "### Describe the problem\r\nResnet50.py apply pooling at last layer regardless pooling parameter.\r\nThe issue has been fixed in keras 2.2.0\r\n### Source code / logs\r\nCurrent:\r\n```python  \r\n  x = AveragePooling2D((7, 7), name='avg_pool')(x)\r\n  if include_top:\r\n    x = Flatten()(x)\r\n    x = Dense(classes, activation='softmax', name='fc1000')(x)\r\n  else:\r\n    if pooling == 'avg':\r\n      x = GlobalAveragePooling2D()(x)\r\n    elif pooling == 'max':\r\n      x = GlobalMaxPooling2D()(x)\r\n```\r\nFix:\r\n```python\r\n  if include_top:\r\n    x = AveragePooling2D((7, 7), name='avg_pool')(x)\r\n    x = Flatten()(x)\r\n    x = Dense(classes, activation='softmax', name='fc1000')(x)\r\n  else:\r\n    if pooling == 'avg':\r\n      x = GlobalAveragePooling2D()(x)\r\n    elif pooling == 'max':\r\n      x = GlobalMaxPooling2D()(x)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@weishi258 thanks for the issue! `keras_applications` has now been separated into its own module that both external Keras and tf.keras import, so this issue should be fixed if you install the nightly version"]}, {"number": 21437, "title": "Resnet50 applying last pooling layer regardless pooling parameter", "body": "### Describe the problem\r\nResnet50.py apply pooling at last layer regardless pooling parameter.\r\n\r\n### Source code / logs\r\nCurrent:\r\n  x = AveragePooling2D((7, 7), name='avg_pool')(x)\r\n  if include_top:\r\n    x = Flatten()(x)\r\n    x = Dense(classes, activation='softmax', name='fc1000')(x)\r\n  else:\r\n    if pooling == 'avg':\r\n      x = GlobalAveragePooling2D()(x)\r\n    elif pooling == 'max':\r\n      x = GlobalMaxPooling2D()(x)\r\nFix:\r\n  if include_top:\r\n    x = AveragePooling2D((7, 7), name='avg_pool')(x)\r\n    x = Flatten()(x)\r\n    x = Dense(classes, activation='softmax', name='fc1000')(x)\r\n  else:\r\n    if pooling == 'avg':\r\n      x = GlobalAveragePooling2D()(x)\r\n    elif pooling == 'max':\r\n      x = GlobalMaxPooling2D()(x)", "comments": []}, {"number": 21436, "title": "modify _TopKGrad so that all operations can run on GPU for better performance", "body": "This PR is intended to be a workaround to address #5719 by working on the Python client code level.\r\nRecap:\r\n1. Upcasting tensor to int64 type before sending to `array_ops.gather`, due to #5445;\r\n2. Replace `sparse_ops.sparse_to_dense` with `array_ops.scatter_nd`, due to #8321;", "comments": ["The modification has been done. ", "What should I do now? Is there any further modifications that need to be made? This is the first time I submit PR to tensorflow, guidance may be needed...", "I've updated the code, but the PR is still in blocked state. Is it still suspended? If there is anything I need to comply with or take further action in order to move on, please don't hesitate to tell me. @ebrevdo @drpngx ", "@tensorflow-jenkins  test this please.", "I just got upgraded to fathership ver 1.0.0 today and was busy looking after my wife and newborn when I missed a lot that had happened in this PR... Thanks for everyone who's moving this forward.\r\n\r\nNow during my break I noticed that the PR got stuck by a `Ubuntu contrib - Internal CI build`, which I don't quite understand... I looked into the details and found the `mangled stack trace`, which \r\n perplexed me further. I can't find any clue related to the `_TopKGrad` modification, but I got this relatively \"readable\" stack trace:\r\n\r\n    *** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace()\r\n\r\n\tgsignal\r\n\tabort\r\n\r\n\t__libc_malloc\r\n\topendir\r\n\ttensorflow::PosixFileSystem::GetChildren(std::string const&, std::vector<std::string, std::allocator<std::string> >*)\r\n\ttensorflow::internal::GetMatchingPaths(tensorflow::FileSystem*, tensorflow::Env*, std::string const&, std::vector<std::string, std::allocator<std::string> >*)\r\n\ttensorflow::PosixFileSystem::GetMatchingPaths(std::string const&, std::vector<std::string, std::allocator<std::string> >*)\r\n\ttensorflow::Env::GetMatchingPaths(std::string const&, std::vector<std::string, std::allocator<std::string> >*)\r\n\ttensorflow::RestoreV2::Compute(tensorflow::OpKernelContext*)\r\n\ttensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)\r\n\t\r\n    Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n\tstd::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n\r\n\tclone\r\n    *** End stack trace ***\r\n\r\nI'm not sure if this is a common issue lurking in the master code right now, or how this stack trace can be linked to the `_TopKGrad`, will get back to it and try to find out when I time permitted.", "Doesn't look related. Was it during running a topk test?\n\nOn Sat, Aug 11, 2018, 2:53 AM Jonathan Zhang <notifications@github.com>\nwrote:\n\n> I just got upgraded to fathership ver 1.0.0 today and was busy looking\n> after my wife and newborn when I missed a lot that had happened in this\n> PR... Thanks for everyone who's moving this forward.\n>\n> Now during my break I noticed that the PR got stuck by a Ubuntu contrib -\n> Internal CI build, which I don't quite understand... I looked into the\n> details and found the mangled stack trace, which\n> perplexed me further. I can't find any clue related to the _TopKGrad\n> modification, but I got this relatively \"readable\" stack trace:\n>\n> *** Begin stack trace ***\n> tensorflow::CurrentStackTrace()\n>\n> gsignal\n> abort\n>\n> __libc_malloc\n> opendir\n> tensorflow::PosixFileSystem::GetChildren(std::string const&, std::vector<std::string, std::allocator<std::string> >*)\n> tensorflow::internal::GetMatchingPaths(tensorflow::FileSystem*, tensorflow::Env*, std::string const&, std::vector<std::string, std::allocator<std::string> >*)\n> tensorflow::PosixFileSystem::GetMatchingPaths(std::string const&, std::vector<std::string, std::allocator<std::string> >*)\n> tensorflow::Env::GetMatchingPaths(std::string const&, std::vector<std::string, std::allocator<std::string> >*)\n> tensorflow::RestoreV2::Compute(tensorflow::OpKernelContext*)\n> tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)\n>\n> Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\n> std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n>\n> clone\n> *** End stack trace ***\n>\n> I'm not sure if this is a common issue lurking in the master code right\n> now, or how this stack trace can be linked to the _TopKGrad, will get\n> back to it and try to find out when I time permitted.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21436#issuecomment-412264559>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9mOAxbr8TGpqpeUtpkegLj1ftuRks5uPqmsgaJpZM4Vx1IE>\n> .\n>\n", "@ebrevdo  the test target points to `//tensorflow/contrib/data/python/kernel_tests/serialization:optimize_dataset_serialization_test`\r\n\r\nfull error log: [link](https://source.cloud.google.com/results/invocations/80e91e52-4e14-46ca-bef4-aac835bf4508/targets/%2F%2Ftensorflow%2Fcontrib%2Fdata%2Fpython%2Fkernel_tests%2Fserialization:optimize_dataset_serialization_test/log)\r\n\r\nBTW, I noticed that other PRs doesn't include the test target `optimize_dataset_serialization_test` in the `Ubuntu contrib` PR check automatically. Is this test mandatory to approve the PR?\r\n\r\n\r\n>Doesn't look related. Was it during running a topk test?", "That's unrelated to you, feel free to ignore\n\nOn Sat, Aug 11, 2018, 9:31 PM Jonathan Zhang <notifications@github.com>\nwrote:\n\n> the test target points to\n> //tensorflow/contrib/data/python/kernel_tests/serialization:optimize_dataset_serialization_test\n>\n> full error log: link\n> <https://source.cloud.google.com/results/invocations/80e91e52-4e14-46ca-bef4-aac835bf4508/targets/%2F%2Ftensorflow%2Fcontrib%2Fdata%2Fpython%2Fkernel_tests%2Fserialization:optimize_dataset_serialization_test/log>\n>\n> Doesn't look related. Was it during running a topk test?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21436#issuecomment-412317984>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9hgpoDzIrn_CqGIc2Aa-EV3uRgoks5uP6-OgaJpZM4Vx1IE>\n> .\n>\n", "Emmmm... I've got some time these days and I'd like to move on this PR. Since the failed test is not related to this particular PR, is it able to skip the test and carry out the other required procedure? @ebrevdo ", "@ebrevdo @drpngx Sorry to bother again. Any further guidance/instructions/recommendations on how to move this PR forward? Or Is it common for PRs for issues labeled as \"feature request\" to be put at low priority? Or is there better solutions under way?", "Not sure, @ebrevdo good to go?", "@ebrevdo I found an error report in _Ubuntu Sanity_ check indicating an unused import of `sparse_ops ` in the `nn_grad.py`. I made a new commit to resolve this issue, but I haven't squashed all these commits in order to preserve @drpngx 's commit, if it is advised to do it this way.\r\n\r\nThe former code review has been dismissed as stale automatically, could you please make another review and initiate the checks one more time? Sorry for all the inconvenience.", "@ebrevdo Thank you for taking the time to review this PR! It's very encouraging to see that it has passed all the checks except one - the _import/copybara_ check, which is also tagged as **Required** as I've noticed... Seems that this check report has been hanging there for more than 12 hours. I'm wondering if this is a normal state, but I don't find any entry to inspect the progress of it. So far, all that I can do is wait?", "Thanks @carusyte for checking, we were waiting for the CI tests to finish before we initiate the merge. Now it's `ready to pull` and should be pushed, as long as the internal tests pass. Stay tuned.", "@drpngx Thanks for the clarification of the process. Now I'm less confused. \r\nOne more question, I've been trying to track the progress of __import/copybara__ check (is it the same thing as the internal test you've mentioned?), by following the [Details](http://go/import-review/9781) link, but it directs me to an invalid url address, maybe it's an internal url or something. Anyway, is it a manual or automated process? How long would it take, usually speaking?", "I think you should be able to see the status `Import/copybara` as it transitions from one state to the other. It's a manual, multi-step process of reviewing the changes for security, running tests, and getting approvals. It's maybe one hour in the fastest case. In this instance, I forgot to check back and approve the first level. After that, it will need to pass tests and get another approval.", "Oops...the `import/copybara` reported `An error happened while migrating the change`, although this PR seemed to have been merged. I can't find any entry to inspect this error. Is it serious?", "I think it's probably fine. Please check that [the merged commit](https://github.com/tensorflow/tensorflow/commit/229652512f745b9ef110c02deb9dce98fd33b5be) has all your changes."]}, {"number": 21435, "title": "Docs error: sparse_softmax_cross_entropy_with_logits", "body": "There is an error in documentation of sparse_softmax_cross_entropy_with_logits:\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\r\n\r\nthere is:\r\n\"logits and labels must have the same dtype (either float16, float32, or float64).\"\r\n\r\nshould be:\r\n\"logits should have dtype float16, float32, or float64,\r\n labels should have dtype int32 or int64\"\r\n\r\n(in Args section of this docs it is written correctly)", "comments": ["@piteren I think you are correct. The labels in `SparseSoftmaxCrossEntropyWithLogits` can only be int32 or int64:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/46cf73f0214bc6208295e36650f1a8ffde4abdd7/tensorflow/core/ops/nn_ops.cc#L1092-L1099\r\n\r\nCreated a PR #21445 for the fix."]}, {"number": 21434, "title": "Broken link to the API stability page ", "body": "The link mentioned in the API stability warning is broken: https://www.tensorflow.org/install/install_go\r\nIt should be changed in this file as well: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_go.md", "comments": ["Added a PR #21443 to fix the incorrect link."]}, {"number": 21433, "title": "Shape mismatch after sampling", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary (pip install tensorflow-gpu)\r\n- **TensorFlow version (use command below)**: 1.9 ('v1.9.0-0-g25c197e023', '1.9.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9 / 7\r\n- **GPU model and memory**:  Nvidia Titan Xp (12 Gb)\r\n- **Exact command to reproduce**: see below\r\n\r\n\r\n### Describe the problem\r\nSince I upgraded from version 1.8 to 1.9, there seems to be an issue with the shape of tensors sampled from `tf.contrib.distributions.MultivariateNormalDiag.sample()` using the sample_shape argument: While `get_shape()` indicates that the output tensor is shaped as [sample_shape, dim] (where dim is the dimensionality of the distribution), using this tensor in `tf.multiply()` results in an output with transposed shape, whereas `tf.add()` throws an 'Incompatible shapes' error.\r\n\r\nThe error seems to indicate that a transpose operation is being used on the sampled tensor:\r\n```\r\nInvalidArgumentError (see above for traceback): Incompatible shapes: [100,5] vs. [5,100]\r\n\t [[Node: add_4 = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ones_12, MultivariateNormalDiag_4/sample/affine_linear_operator/forward/DistributionShape_1/undo_make_batch_of_event_sample_matrices/rotate_transpose/transpose)]]\r\n\t [[Node: add_4/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_14_add_4\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\nOn version 1.9, the code below outputs\r\n```\r\nsampled shape  (100, 5)\r\nnormal multiply  (100, 5)\r\nmultiply sampling output  (5, 100)\r\n``` \r\n If I downgrade to 1.8 again, I get the expected\r\n```\r\nsampled shape  (100, 5)\r\nnormal multiply  (100, 5)\r\nmultiply sampling output  (100, 5)\r\n```\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.ones([100, 5])\r\nb = tf.ones([100, 5])\r\nsam = tf.contrib.distributions.MultivariateNormalDiag(\r\n                loc=[0., 0., 0., 0., 0.],\r\n                scale_diag=[1., 1., 1., 1., 1.])\r\nc = sam.sample(sample_shape=[100])\r\n\r\nmul1 = tf.multiply(a, b)\r\nmul2 = tf.multiply(a, c)\r\nadd = a + c\r\n\r\n\r\nsess = tf.Session()\r\ninit = tf.initialize_all_variables()\r\nsess.run(init)\r\nval_c = c.eval(session=sess)\r\nval_1 = mul1.eval(session=sess)\r\nval_2 = mul2.eval(session=sess)\r\n# this throws an error:\r\n# val_3 = add.eval(session=sess)\r\n\r\nprint 'sampled shape ', val_c.shape\r\nprint 'normal multiply ', val_1.shape\r\nprint 'multiply sampling output ', val_2.shape\r\n```\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce\nMobile device", "@jvdillon - might I trouble you to triage?", "Hi!\r\n  Are you still seeing this issue? I ran this example on a newer version of tensorflow (1.10) and also on tensorflow_probability (for which distributions are migrating to) without issue (no error for the commented out line).", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Sorry for the late response, no, I don't see this in 1.10 either. Thanks."]}, {"number": 21432, "title": "BUG: Initial values are still random, even both the graph-level and the operation seed are set", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu Server & Window 10 \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 & v1.9.0-0-g25c197e023\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: V9.0.178\r\n- **GPU model and memory**: NVIDIA Tesla V100-SXM2-16GB & NVIDIA GeForce GTX 1080 Ti 11GB\r\n- **Exact command to reproduce**:\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\nseed = 1\r\ntf.set_random_seed(seed)\r\n\r\ninitializer1 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)\r\nConv1W = tf.identity(tf.Variable(initializer1.__call__(shape=[3,3,1,64])),name='Conv1')\r\n\r\ninitializer2 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)\r\nConv2W = tf.identity(tf.Variable(initializer2.__call__(shape=[3,3,64,64])),name='Conv2')\r\n\r\ninitializer3 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)\r\nfc1W = tf.identity(tf.Variable(initializer3.__call__(shape=[39,14,64,372])),name='fc1')\r\n\r\ninitializer4 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)\r\nfc2W = tf.identity(tf.Variable(initializer4.__call__(shape=[1,1,372,372])),name='fc2')\r\n\r\ninitializer5 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)\r\nfc3W = tf.identity(tf.Variable(initializer5.__call__(shape=[1,1,372,1])),name='fc3')\r\n\r\nprint(\"Session 1\")\r\nwith tf.Session() as sess1:\r\n    tf.global_variables_initializer().run()\r\n    Conv1W_eval1 = sess1.run(Conv1W)\r\n    Conv2W_eval1 = sess1.run(Conv2W)\r\n    fc1W_eval1 = sess1.run(fc1W)\r\n    fc2W_eval1 = sess1.run(fc2W)\r\n    fc3W_eval1 = sess1.run(fc3W)\r\n\r\nprint(\"Session 2\")\r\nwith tf.Session() as sess2:\r\n    tf.global_variables_initializer().run()\r\n    Conv1W_eval2 = sess2.run(Conv1W)\r\n    Conv2W_eval2 = sess2.run(Conv2W)\r\n    fc1W_eval2 = sess2.run(fc1W)\r\n    fc2W_eval2 = sess2.run(fc2W)\r\n    fc3W_eval2 = sess2.run(fc3W)\r\nprint('--------------------------------------------------')\r\nprint('Conv1W? %d' % all(Conv1W_eval1==Conv1W_eval2))\r\nprint('Conv2W? %d' % all(Conv2W_eval1==Conv2W_eval2))\r\nprint('fc1W? %d' % all(fc1W_eval1==fc1W_eval2))\r\nprint('fc2W? %d' % all(fc2W_eval1==fc2W_eval2))\r\nprint('fc3W? %d' % all(fc3W_eval1==fc3W_eval2))\r\n\r\n```\r\n\r\n### Describe the problem\r\n\r\nBy executing the above code (one or two times), we can see that the initial value of the tensors, fc1W & fc2W are NOT always the same in both session, even though the graph-level and the operation seed are set.\r\n\r\nThe randomness seems to be depended on the shape of the tensors. For example, the initial value of the tensors, Conv1W and Conv2W are always the same.\r\n\r\nPutting all tf.Variable into/under each session has the same effect.", "comments": ["**I guess the bug is at the QR Algorithm.** I guess tq.orthogonal_initializer and tf.qr share the same algorithm. So tf.qr also cause this bug.\r\n\r\nTo illustrate, I base on the code https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/ops/init_ops.py\r\nand I re-implemented/simplified my code as follow\r\n```\r\nimport tensorflow as tf\r\n\r\nseed = 1\r\ntf.set_random_seed(seed)\r\n\r\n#shape = [3,3,1,64]                # shape which QR algo always behaves ok \r\nshape = [39,14,64,372]          # troublesome shape\r\n# Flatten the input shape with the last dimension remaining\r\n# its original shape so it works for conv2d\r\nnum_rows = 1\r\nfor dim in shape[:-1]:\r\n    num_rows *= dim\r\nnum_cols = shape[-1]\r\nif num_rows < num_cols:\r\n    flat_shape = (num_cols, num_rows)\r\nelse:\r\n    flat_shape = (num_rows, num_cols)\r\n# Generate a random matrix & Compute the qr factorization\r\ninitializer = tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=seed, dtype=tf.float32)\r\ninitialValue = tf.Variable(initializer.__call__(shape=flat_shape))\r\nq,r = tf.qr(initialValue,full_matrices=False)\r\n# Make Q uniform\r\nd = tf.diag_part(r)\r\nq1 = q * tf.sign(d)\r\nif num_rows < num_cols:\r\n    q2 = tf.transpose(q1)\r\n    Conv1W = tf.reshape(q2, shape)\r\nelse:\r\n    Conv1W = tf.reshape(q1, shape)\r\n\r\nprint(\"Session 1\")\r\nwith tf.Session() as sess1:\r\n    tf.global_variables_initializer().run()    \r\n    initialValue_eval1 = sess1.run(initialValue)\r\n    q_eval1 = sess1.run(q)\r\n    r_eval1 = sess1.run(r)\r\n    d_eval1 = sess1.run(d)\r\n    Conv1W_eval1 = sess1.run(Conv1W)\r\nprint(\"Session 2\")\r\nwith tf.Session() as sess2:\r\n    tf.global_variables_initializer().run()\r\n    initialValue_eval2 = sess2.run(initialValue)\r\n    q_eval2 = sess2.run(q)\r\n    r_eval2 = sess2.run(r)\r\n    d_eval2 = sess2.run(d)\r\n    Conv1W_eval2 = sess2.run(Conv1W)\r\n    \r\nprint('--------------------------------------------------')\r\nprint('initialValue? %d' % all(initialValue_eval1==initialValue_eval2))\r\nprint('q? %d' % all(q_eval1==q_eval2))\r\nprint('r? %d' % all(r_eval1==r_eval2))\r\nprint('d? %d' % all(d_eval1==d_eval2))\r\nprint('Conv1W? %d' % all(Conv1W_eval1==Conv1W_eval2))\r\n```", "Opps, my fault, the randomness is due to the nondeterministic nature of GPU. Checked with the CPU computation result, they are the same.....  Think you can close this issue\r\n\r\nBut, by the way, is it possible that, in the tensorflow doc, to list out which tensorflow API/ops are\r\n1. non-deterministic/deterministic\r\n2. going to have an option for choosing non-deterministic/deterministic cuDNN algo\r\n(it seems you guys are working on that :) really appreciate your hard work :)\r\nhttps://github.com/tensorflow/tensorflow/issues/12871\r\nhttps://github.com/tensorflow/tensorflow/pull/10636 )", "@EdwardLin2014 We are checking to see if we can close this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF. If it is resolved then please feel free to move this issue to close status.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21432\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21432\">No</a>\n"]}, {"number": 21431, "title": "I get error not found symbol variable \"Fill\" in Zeros class", "body": "Hello guys i get error in Zeros class for not found symbol variable \"Fill\" in android studio and in Build panel showing this error and not running sample app from tensorflow to my android devices\r\n\r\nerror: cannot find symbol variable Fill where T is a type-variable: T extends Object declared in class Zeros", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Trying to build android source folder from tensorflow examples.\r\nUsing Linus 16.04 LTS\r\nTensorflow installed via pip.\r\n![erro](https://user-images.githubusercontent.com/40731977/43990691-951ac2a2-9d7c-11e8-9993-37497e9b7848.png)\r\n", "I think it is some bug with new version. Just run:\r\n`git checkout 'git rev-list -n 1 --before=\"2018-06-27 13:37\" master'`\r\nIt gets you back a few commits before issue and before Zero.java existing. I've tested it already and it compile without errors.", "![nowthus](https://user-images.githubusercontent.com/40731977/43998133-2e6ff852-9e0c-11e8-8b74-cd835039155c.png)\r\n\r\nHi wojciradom, \r\n\r\nThanks for the reply. I tried running the command, but I get the following errors. Any suggestions on how I could fix this.\r\n\r\nThanks,", "I think you forgot change nativeBuildSystem to none.\r\n\r\n3. Open the build.gradle file (you can go to 1:Project in the side panel and find it under the Gradle Scripts zippy under Android). Look for the nativeBuildSystem variable and set it to none if it isn't already:\r\n\r\n// set to 'bazel', 'cmake', 'makefile', 'none'\r\ndef nativeBuildSystem = 'none'", "I was running into this issue and can confirm that setting `def nativeBuildSystem = 'none'` and then running `./gradlew installDebug` (after `chmod +x gradlew`) worked with the latest `master` branch without rewinding back to any previous revision. No idea if that works in Android Studio or not, though.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I got the same issue\r\nJust upgarde build grade as the same version as your android studio version\r\nfor mine is:\r\ndependencies {\r\n        classpath 'com.android.tools.build:gradle:3.1.4' // android studio 3.1.4\r\n        classpath 'org.apache.httpcomponents:httpclient:4.5.4'\r\n    }\r\n\r\nthen you can run and see TF sample apk installed: TF Classify, TF Detect, TF Speech, TF Stylize", "I ran following commands:\r\ngit rev-list -n 1 --before=\"2018-06-27 13:37\" master\r\ngit checkout de93bd503a0f7ea8b326f17a95b18127716adc41\r\n\r\nand change \r\ndef nativeBuildSystem = 'none' //from 'bazel' in build.gradle\r\n\r\nI still have this error in Android studio (version 3.2)\r\n\"Execute task action 4/4 for :compileDebugJavaWithJavac\"", "ok. finally, made it work. Here is what I did.\r\n1) updated to Android Studio to the latest (version 3.2)\r\n2) git to master\r\ncommit cc5555d3d3daa64f462cc7f8d31fe915073429f4 (HEAD, origin/master, origin/HEAD, master)\r\n2) change this line in build.gradle:\r\n        classpath 'com.android.tools.build:gradle:3.1.4' //3.0.1'\r\n3) import to Android Studio (just installed whatever it recommended)", "@mirhosseinmousavi  Hi, does this issue still exist ?", "@koy14400 's answer helps me. Otherwise, after install the app, there are four app of Tensorflow and all of them are crash to open. Anybody know this issue?", "Closing due to staleness. Please use the latest version for TensorFlow and test again. Feel free to open a new issue if it still persists. Thanks!", "Still an issue.\r\n\r\nHave I written custom code > No\r\nOS Platform and Distribution > W10\r\nMobile device > GS7 edge\r\n(The rest is unknow to me)\r\n\r\nI  just wanted to try some exemples but I can't manage to compile.", "@Synless Please open a new issue [New Issue Template](https://github.com/tensorflow/tensorflow/issues/new/choose) and report your problem. \r\nWe ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will keep this issue closed in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 21430, "title": "Fix markdown indentation in install_raspbian.md", "body": "![](https://go-gyazo.appspot.com/69dd49d06d8f992b.png)\r\n\r\nhttps://www.tensorflow.org/install/install_raspbian\r\n", "comments": ["Thanks for the PR @mattn! Looks like the same change has already be applied to head. I will close this PR."]}, {"number": 21429, "title": "TocoConvertor: converting keras models to tflite doesn't support custom objects", "body": "Have I written custom code: No\r\nOS Platform and Distribution: Mac \r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.10-rc1\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: `tensorflow.contrib.lite.TocoConverter.from_keras_model_file(model_file, custom_objects=CUSTOM_OBJECTS)`\r\nMobile device: N/A\r\n\r\n-----\r\n\r\nThis is required to convert models that use custom layers or loss functions\r\n\r\nthis is the fix (will submit a PR soon):\r\n```\r\nFrom 2c2179765cc9006762cf75c6a1b587e06895b869 Mon Sep 17 00:00:00 2001\r\nFrom: Ophir Yoktan <ophir@ziprecruiter.com>\r\nDate: Wed, 1 Aug 2018 10:16:10 +0300\r\nSubject: [PATCH] add support for custom_objects when loading keras model for\r\n conversion\r\n\r\n---\r\n tensorflow/contrib/lite/python/lite.py | 5 +++--\r\n 1 file changed, 3 insertions(+), 2 deletions(-)\r\n\r\ndiff --git a/tensorflow/contrib/lite/python/lite.py b/tensorflow/contrib/lite/python/lite.py\r\nindex 2f9b9d469a2..c12617df061 100644\r\n--- a/tensorflow/contrib/lite/python/lite.py\r\n+++ b/tensorflow/contrib/lite/python/lite.py\r\n@@ -274,7 +274,8 @@ def from_keras_model_file(cls,\r\n                             model_file,\r\n                             input_arrays=None,\r\n                             input_shapes=None,\r\n-                            output_arrays=None):\r\n+                            output_arrays=None,\r\n+                            custom_objects=None):\r\n     \"\"\"Creates a TocoConverter class from a tf.keras model file.\r\n \r\n     Args:\r\n@@ -293,7 +294,7 @@ def from_keras_model_file(cls,\r\n     \"\"\"\r\n     _keras.backend.clear_session()\r\n     _keras.backend.set_learning_phase(False)\r\n-    keras_model = _keras.models.load_model(model_file)\r\n+    keras_model = _keras.models.load_model(model_file, custom_objects=custom_objects)\r\n     sess = _keras.backend.get_session()\r\n \r\n     # Get input and output tensors.\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@gargn , PTAL.", "Is there a PR for this? If so, can you add a link?", "I attached a patch. \r\nI will create a PR once I'm dome with the Contributor License Agreement", "Nagging Assignee @gargn: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "The issue is still open.\r\nwill appreciate if somebody will mere the patch from the original version (I have problems with submitting a  pull request with my user)", "Did you manage to convert a Keras model with custom layers to tflite? If so how?", "More or less. By applying the patch in this thread\nThere were other issues afterwards that are not keras related (unsupported\noperations)\n\nOn Fri, Feb 15, 2019, 10:40 gitman88 <notifications@github.com> wrote:\n\n> Did you manage to convert a Keras model with custom layers to tflite? If\n> so how?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21429#issuecomment-463952028>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE_ImPvrhpiwFdU77nlLmqwWtmij8urlks5vNnKVgaJpZM4Vxilc>\n> .\n>\n", "Okey! How did you apply the patch? I tried forking the repository and implement the functionality (like you did) and then install that version of tensorflow locally in an anaconda environment. Did not manage to do that however. Is there another approach or should it be doable?", "I applied it on the installed package (to avoid recompiling everything)\n\nOn Fri, Feb 15, 2019, 11:55 gitman88 <notifications@github.com> wrote:\n\n> Okey! How did you apply the patch? I tried forking the repository and\n> implement the functionality (like you did) and then install that version of\n> tensorflow locally in an anaconda environment. Did not manage to do that\n> however. Is there another approach or should it be doable?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21429#issuecomment-463974351>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE_ImP5k-ReupS0Fct3ZopzwwHLPVDF_ks5vNoP6gaJpZM4Vxilc>\n> .\n>\n", "Is this fixed? I also like to use custom objects. ", "This functionality has been added by 09deaeb.", "Somehow, this functionality is not in TF2. In TensorFlow 2.1.0 I get:\r\n\r\nTypeError: from_keras_model() got an unexpected keyword argument 'custom_objects'\r\nwhen using `tf.lite.TFLiteConverter.from_keras_model`\r\n\r\nor\r\n\r\nAttributeError: type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'\r\nwhen using `tf.lite.TFLiteConverter.from_keras_model_file`\r\n\r\nMy model was created using TensorFlow 2.1.0 (Keras 2.2.4-tf) so I cannot use the V1 converter.\r\n\r\nIf I understand correctly, it is currently not possible to use custom objects in .tflite models for TF2. When I use `tf.lite.TFLiteConverter.from_keras_model` without `custom_objects` it creates the .tflite file without errors/warnings, but in TensorFlow Lite this results in failure of `interpreter->AllocateTensors()`. This is probably due to an error in the converted model, because this does not happen when I don't use custom objects. `interpreter->AllocateTensors()` clearly fails because I didn't define `custom_objects`, which makes sense. Please copy this functionality to TF2.\r\n\r\n", "@Lotte1990  I have also this problem. Could you find a solution?\r\n`TypeError: load_model() got an unexpected keyword argument 'custom_object'`", "In 2.X you need to load your Keras model yourself with the `custom_objects` passed in and then call `from_keras_model`. Please reference [this](https://www.tensorflow.org/lite/convert/python_api#converting_a_keras_model_) example on how to convert a model using `from_keras_model`. Please reference [this](https://www.tensorflow.org/lite/convert/1x_compatibility) document on details on compatibility between 1.X and 2.X."]}, {"number": 21428, "title": "tf.clip_by_global_norm raise exception when use_norm = inf", "body": "Fix #21363", "comments": ["Do you think the `tf.where()` might cause a noticeable slowdown here?", "Yes, `tf.where` will result in performance loss, however we don't expect it would be noticeable. Would you mind applying the patch and watching its performance? Thanks very much.", "I don't have a benchmark set up for this, unfortunately.", "I think `use_norm = inf` is rare, is it better to replace `where` by assertion (say,  tf.verify_tensor_all_finite)? What do you think?  ", "That sounds like a good idea to me.", "Thanks for your review. I think all comments have been resolved."]}, {"number": 21427, "title": "builld tf 1.10.0, but version shows rc1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04 x64\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6.6 x64\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.2/7.1.5\r\n- **GPU model and memory**: GTX1080Ti GDDR5X 11GB  X 6\r\n- **Exact command to reproduce**: \r\n\r\npip generated as 1.10\r\ninstall complete and shows 1.10\r\n\r\n\r\nimport tensorflow as tf\r\nprint(tf.__verison__)\r\n\r\n1.10.0-rc1", "comments": ["https://github.com/tensorflow/tensorflow/releases\r\n1.10 is still prerelease\r\nYou used pip3 install tensorflow-gpu?  That's weird and definitely a bug if you did. ", "@gragundier  no i build from r1.10 source as I wrote above.....what ru sayin....", "yeah look at the link.  It's still pre-release.  When you choose r1.10, it just uses the latest pre-release version.  ", "@gragundier \r\nyou're really not helping\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/d2af5d50b6255c4bc76b9e9fc516a0aa027efe6d", "I'm just cranky because ci.tensorflow.org is down. \ud83d\ude1e ", "ci.tensorflow.org is not down. We have deprecated it. Any nightly builds you need can be found at the tf-nightly or tf-nightly-gpu.\r\n\r\nAdditionally I've just merged the [latest version change](https://github.com/tensorflow/tensorflow/commit/d2af5d50b6255c4bc76b9e9fc516a0aa027efe6d). Can you try again with the latest r1.10 branch?", "@alanpurple could you also share the output of `tf.__git_version__` ?\r\n@gragundier ci.tensorflow.org is deprecated, and is finally taken down for good. So the site being down is working as intended.", "@av8ramit Then, I've got a question.  Recently, the regular raspberry pi tf binary's tflite interpreter didn't work because of an armv7 optimization bug.  Turns out, I could pull the latest pi-zero binary compilation and use that instead.  I understand this isn't a problem with other hardware: \"Broken tf-nightly version? Roll it back to a stable release.\" But with Raspberry Pi's I can use either the Pi-Zero or Pi3 specific binaries.  How would I choose which of those binaries I pip install?", "@gunan \r\nb'v1.10.0-rc1-12-gbae6aeffc0'\r\n", "@gragundier I'm a bit confused as to how your question relates to this issue unless as I suspect it does not. Are you asking about nightly build binaries for Raspberry Pi now that ci.tensorflow.org is gone?", "@av8ramit it has absolutely nothing to do do with the topic and I don't see a DM feature.  Let me open a new issue and we can stop pestering alanpurple.  And yes, you're right on the money.\r\n::EDIT::\r\n#21469 \r\n::END EDIT::", "@alanpurple I suspect this is due to 1.10 official still not being out, and the tag not being generated in our repo.\r\nMay I ask, why do you need the package with 1.10 label before we release the official 1.10?", "@gunan \r\n\r\nThat's not what \"I need\"\r\nI just report this because your repo says \" 1.10 \" and actually is not\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/d2af5d50b6255c4bc76b9e9fc516a0aa027efe6d\r\nthis is just a report, not my problem", "Thanks for the report. The version string generation mechanism is actually more than just what is at the repo. Once all the git tags are created, and release is finalized, this will be pointing to 1.10.\r\n\r\nI will close this issue for now."]}, {"number": 21426, "title": "Fix issue in get_temp_export_dir under python 3", "body": "This fix tries to address the issue raised in #21403 where\r\ntensorflow.python.estimator.export.get_temp_export_dir\r\ndoes not work correctly under python 3 due to the combination\r\nof bytes vs non-bytes concatenation:\r\n```\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01)\r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow.python.estimator.export import export as export_helpers\r\n>>> export_dir = export_helpers.get_timestamped_export_dir(b\"/a/b/\")\r\n>>> export_dir\r\nb'/a/b/1533609814'\r\n>>> export_helpers.get_temp_export_dir(export_dir)\r\nb\"/a/b/temp-b'1533609814'\"\r\n>>>\r\n```\r\nThis fix address the issue by adding compat.as_bytes to 'temp-'\r\nbefore concatenation.\r\n\r\nThis fix fixes #21403.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can you add some test for the fix?", "Thanks @rohan100jain for the review. I updated the PR with unit test added. Without the change introduced in this PR the test will fail under python 3 (works fine in python 2). Please take a look and let me know if there are any issues.", "Thanks @superbobry. The PR has been updated.", "Nagging Assignee @drpngx: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry this fell through the cracks. @yongtang could you pull rebase and push again?", "Thanks @drpngx. It looks like `tensorflow/python/estimator` has been moved to another repo of https://github.com/tensorflow/estimator, so the files were deleted. Let me close this PR and create a PR on https://github.com/tensorflow/estimator repo instead."]}]