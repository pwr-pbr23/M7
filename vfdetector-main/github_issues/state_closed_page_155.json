[{"number": 50174, "title": "Add float16 support for SparseSegmentSumGrad", "body": "This is a follow-up to https://github.com/tensorflow/tensorflow/pull/49604 that adds fp16 support to the recently-added `SparseSegmentSumGrad` op.\r\n\r\ncc @nluehr ", "comments": []}, {"number": 50173, "title": "Error while using tf.io.encode_jpeg", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 2.5\r\n- Python version: 3.x\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior** : Using **`tf.io.encode_jpeg`** is resulting in error,\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-7f8b07d2c90d> in <module>()\r\n      1 import tensorflow as tf\r\n      2 \r\n----> 3 tf.io.encode_jpeg('Developing_1.jpeg')\r\n\r\n8 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n     97   ctx.ensure_initialized()\r\n---> 98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     99 \r\n    100 \r\n\r\nTypeError: Cannot convert 'Developing_1.jpeg' to EagerTensor of dtype uint8\r\n```\r\n\r\n**Describe the expected behavior** : There should be no error\r\n\r\n**Standalone code to reproduce the issue**  : \r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.io.encode_jpeg('Any_Image.jpeg')\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @rakeshmothukuru1, `tf.io.encode_jpeg` takes a tensor as input, not a string: https://www.tensorflow.org/api_docs/python/tf/io/encode_jpeg\n\n```\nimport tensorflow as tf\n\ntf.io.encode_jpeg(tf.constant([[[1, 0, 1]]], tf.uint8))\n```", "```python\r\nimage_path = '/content/Any_Image.jpeg'\r\nimage = tf.keras.preprocessing.image.load_img(image_path)\r\ninput_arr = tf.keras.preprocessing.image.img_to_array(image)\r\ntf.io.encode_jpeg(input_arr)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50173\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50173\">No</a>\n", "@ymodak,\r\nThank you for the solution."]}, {"number": 50171, "title": "EfficientNet fails to load with float16 policy on TF 2.5.0.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary.\r\n- TensorFlow version (use command below): v2.5.0-0-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.0 / 8.0.4\r\n- GPU model and memory: Nvidia Tesla T4, 15109MiB \r\n\r\n**Describe the current behavior**\r\nThe model cannot be created when using the mixed precision policy.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\npolicy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\r\ntf.keras.mixed_precision.set_global_policy(policy)\r\n\r\nmodel = tf.keras.applications.efficientnet.EfficientNetB0()\r\n```\r\n\r\nThe above will raise `ValueError` : `Tensor conversion requested dtype float16 for Tensor with dtype float32: <tf.Tensor 'normalization_1/Cast:0' shape=(None, 224, 224, 3) dtype=float32>`\r\n\r\nFull error below.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe model should work in mixed policy - I remember it used to work in TF 2.4.1, perhaps some changes were made?\r\n```python\r\n!pip uninstall tensorlfow\r\n!pip install tensorflow==2.4.1\r\npolicy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\r\ntf.keras.mixed_precision.set_global_policy(policy)\r\n\r\nmodel = tf.keras.applications.efficientnet.EfficientNetB0()  # OK\r\n```\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - yes, BUT: I do not exactly know which part causes the issue (I assume the problem is in the Normalization layer). I could try to fix the issue with some help provided.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\npolicy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\r\ntf.keras.mixed_precision.set_global_policy(policy)\r\n\r\nmodel = tf.keras.applications.efficientnet.EfficientNetB0()\r\n```\r\n\r\n**Other info / logs** \r\nFull error message:\r\n```python\r\n```python\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n   1244             r_op = getattr(y, \"__r%s__\" % op_name)\r\n-> 1245             out = r_op(x)\r\n   1246             if out is NotImplemented:\r\n\r\n17 frames\r\nValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: <tf.Tensor 'normalization_1/Cast:0' shape=(None, 224, 224, 3) dtype=float32>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    556                 \"%s type %s of argument '%s'.\" %\r\n    557                 (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\r\n--> 558                  inferred_from[input_arg.type_attr]))\r\n    559 \r\n    560         types = [values.dtype]\r\n\r\nTypeError: Input 'y' of 'Sub' Op has type float16 that does not match type float32 of argument 'x'.\r\n```\r\n\r\nBest regards,\r\nSebastian\r\n", "comments": ["Was able to reproduce the issue in [TF 2.5](https://colab.research.google.com/gist/saikumarchalla/9dc2f39888239c56699dfa950e281380/untitled.ipynb) where as in [TF 2.4 ](https://colab.research.google.com/gist/saikumarchalla/77c48aadcf55aac8741d57910edfeca3/untitled98.ipynb) it doesn't raise any error. Please find the attached gist.Thanks!", "@sebastian-sz This was resolved in recent `tf-nightly` by this https://github.com/tensorflow/tensorflow/commit/f048e532985895311891aa5521239303d28a9ce0. \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/288e047ee7ea64bf6d209471e079989d/untitled.ipynb) is a gist for a reference. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Ah, apologies I didn't find the linked issue before.\r\n\r\nI can confirm that the model works on `tf-nightly`. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50171\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50171\">No</a>\n"]}, {"number": 50169, "title": "Add weight to ```tf.keras.layers.experimental.preprocessing.TextVectorization``` ", "body": "**System information**\r\n- TensorFlow version (you are using): tensorflow2.5.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n```tf.keras.layers.experimental.preprocessing.TextVectorization``` is a very useful layer for nlp project , but it can't deal with a string which contains weight like this : \r\n```\r\nThis:0.1 is:0.2 a:0.3 tensorflow:0.25 issue:0.5 . \r\n```\r\nSo my feature request is that add a parameter like ```is_contain_weight``` to ```tf.keras.layers.experimental.preprocessing.TextVectorization``` . When the parameter is ```True``` , ```tf.keras.layers.experimental.preprocessing.TextVectorization``` will return both word index and word weight\r\n**Will this change the current api? How?**\r\nNo , since we can set the default value of ```is_contain_weight``` to ```False```\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who want input weight with text to ```tf.keras``` model\r\n\r\n", "comments": ["@DachuanZhao,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks! ", "> @DachuanZhao,\r\n> Can you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!\r\n\r\nWhen you train a ctr model , your model will have a feature which shows a user's interest . For example ,  the model have a feature named ```user_favourrite_tags``` and its value is ```btc:0.4 eth:0.6``` which means the user's interest is 0.6 at ```btc``` tag and 0.4 at ```eth``` tag . In deep learning model , you can get ```btc``` tag's embedding  and ```eth``` tag's embedding from ```tf.keras.layers.Embedding``` and  calculate ```0.6 * btc tag's embedding + 0.4 * eth tag's embedding``` as the user's ```user_favourrite_tags``` embedding .", "So what you expect to do with the token weights is multiply them with the embedding vectors for this token?\r\n\r\nDo the weights change for each input sample, or does one term (word) always have the same weight?", "> So what you expect to do with the token weights is multiply them with the embedding vectors for this token?\r\n> \r\n> Do the weights change for each input sample, or does one term (word) always have the same weight?\r\n\r\n The weights change for each user . The data is like this :\r\n|  Y   | item_id  | item_tags  | user_id  | user_tags  | some_other_feature_for_user_or_item  |\r\n|  ----  | ----  | ----  | ----  | ----  | ----  |\r\n| 1  | item1 | eth:0.3 btc:0.4 doge:0.3 | user1 | btc:0.4 eth:0.6 | other_value |\r\n| 0  | item2 | fil:0.6 char:0.4 | user1 | btc:0.4 eth:0.6 | other_value |\r\n| 1  | item2 | fil:0.6 char:0.4 | user2 | fil:0.3 char:0.5 eth:0.2 | other_value |\r\n\r\n\r\nThe ```item_tags``` is calculated by some algorithm (such as tf-idf) and the ```user_tags``` is calculated by the user's action ( such as  thumbs-up , share , comment , etc ) on the item .", "@DachuanZhao If there is any actionable PRs, please feel free to open them in [keras-team/keras](https://github.com/keras-team/keras/issues) repository. \r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus entirely on only keras. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50168, "title": "Conv2DTranspose output shapes vary with output_padding=0 and output_padding=None", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nIn the [Conv2DTranspose documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose), the following formula is used to calculate ```new_rows``` and ```new_cols``` if ```output_padding``` is specified:\r\n```\r\nnew_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\r\noutput_padding[0])\r\nnew_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +\r\noutput_padding[1])\r\n```\r\nFor the following settings: \r\n```input  shape=(128, 128, 3)```\r\n```filters=32, kernel_size=(4,4), strides=(2,2), padding='same', output_padding=(0,0)``` ,\r\n\r\nthe above mentioned formula gives ```new_rows=256``` and ```new_cols=256``` (which is actually the **expected shape**).\r\n\r\nI defined a function to calculate the shape of ```new_rows``` and ```new_cols``` using the above formula to verify the output shape:\r\n```\r\ndef compute_shape(rows=128, cols=128, strides=(2, 2), kernel_size=(4, 4),\r\n                      padding=(0, 0), output_padding=(0, 0)):\r\n    new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\r\n                    output_padding[0])\r\n    new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +\r\n                    output_padding[1])\r\n    return new_rows, new_cols\r\n\r\nnew_shape = compute_shape(padding=(1, 1))\r\nprint(new_shape)\r\n```\r\n\r\nNow if I specify ```output_padding=0``` for Conv2DTranspose layer for the same settings mentioned above , I get the output shape as ```(254, 254, 32)```.\r\n\r\nOutput:\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 128, 128, 3)]     0         \r\n_________________________________________________________________\r\nconv2d_transpose (Conv2DTran (None, 254, 254, 32)      1568      \r\n=================================================================\r\nTotal params: 1,568\r\nTrainable params: 1,568\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\n**Describe the expected behavior**\r\nIf I don't specify ```output_padding``` or use ```output_padding=None```, I get the expected output shape ```(256, 256, 32)```.\r\n\r\n\r\nWhy does the **difference arise in the output shape**  when ```output_padding=None``` and ```output_padding=0```? Doesn't ```output_padding=None``` and ```output_padding=0``` mean the same thing? Shouldn't both cases ideally give the **same output**?\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nfrom tensorflow.keras.layers import Input, Conv2DTranspose\r\nfrom tensorflow.keras import Model\r\n\r\n x = Input((128, 128, 3))\r\n y = Conv2DTranspose(filters=32, kernel_size=4, strides=2, padding='same', output_padding=0)(x)\r\n model = Model(x, y)\r\n model.summary()\r\n\r\n def compute_shape(rows=128, cols=128, strides=(2, 2), kernel_size=(4, 4),\r\n                      padding=(0, 0), output_padding=(0, 0)):\r\n     new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\r\n                    output_padding[0])\r\n     new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +\r\n                    output_padding[1])\r\n     return new_rows, new_cols\r\n\r\n new_shape = compute_shape(padding=(1, 1))\r\n print(new_shape)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue in TF 2.5, Please find the  gist [here](https://colab.research.google.com/gist/saikumarchalla/79f4572264221253366e1ef51414fac3/untitled.ipynb). Thanks!", "The output padding as `0` is different than setting as `None`.\r\nhttps://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/layers/convolutional.py#L1225", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50168\">No</a>\n"]}, {"number": 50167, "title": "tensorflow load saved_model throw error: google.protobuf.message.DecodeError: Error parsing message", "body": "Background: i set up a new MBP with MacOS Big Sur 11.4 then my TensorFlow model loading script is not working anymore (which works previously and still works on my another laptop), i tried to set up exactly same python virtual environment, and even use exactly same Dockerfile, still one is working and the other is not working...\r\n\r\nSo apparently this is OS setup issue, i just cannot figure out what is wrong, and already hit the wall for days....\r\n\r\n**Pls help to point out what potential stuffs i can check to fix this...**\r\n\r\n(and if possible pls don't close, as i already asked this question in StackOverflow yesterday: https://stackoverflow.com/q/67886470/7658313 and till now no answer yet)\r\n\r\nAnd the other confusion is: how come within the docker container also one laptop is working and another is not working, so it must be some physical host OS stuff can even affect the docker container...\r\n\r\nThe error trace stack:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 68, in parse_saved_model\r\n    saved_model.ParseFromString(file_content)\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"naive_tf1_model_loader.py\", line 55, in <module>\r\n    \"/tf1_models/object_detection/1\"\r\n  File \"naive_tf1_model_loader.py\", line 14, in __init__\r\n    self._sess, [tf.saved_model.SERVING], model_path\r\n  File \"/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 268, in load\r\n    loader = SavedModelLoader(export_dir)\r\n  File \"/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 284, in __init__\r\n    self._saved_model = parse_saved_model(export_dir)\r\n  File \"/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 71, in parse_saved_model\r\n    raise IOError(\"Cannot parse file %s: %s.\" % (path_to_pb, str(e)))\r\nOSError: Cannot parse file b'/path/saved_model.pb': Error parsing message.\r\n```\r\n\r\nThe model is downloaded from [tensorflow model zoo: this one][1].\r\n\r\n-----\r\n\r\nFYI: the model loading script:\r\n```\r\nimport time\r\nimport tensorflow as tf\r\n\r\n_INPUT_KEYS_TO_TENSOR       = \"input_keys_to_tensor\"\r\n_OUTPUT_KEYS_KEYS_TO_TENSOR = \"output_keys_to_tensor\"\r\n\r\nclass TF1Model:\r\n\r\n  def __init__(self, model_path):\r\n    self._sess = tf.compat.v1.Session()\r\n\r\n    if hasattr(tf.compat.v1.saved_model, \"load\"):\r\n      graph_meta_def = tf.compat.v1.saved_model.load(\r\n        self._sess, [tf.saved_model.SERVING], model_path\r\n      )\r\n    else:\r\n      graph_meta_def = tf.compat.v1.saved_model.loader.load(\r\n        self._sess, [tf.saved_model.SERVING], model_path\r\n      )\r\n\r\n    signature = graph_meta_def.signature_def\r\n\r\n    self._signature_tensor_mapping = {}\r\n    for signature_name in signature.keys():\r\n      indiv_sig_data = self._signature_tensor_mapping[signature_name] = {\r\n        _INPUT_KEYS_TO_TENSOR: {},\r\n        _OUTPUT_KEYS_KEYS_TO_TENSOR: {}\r\n      }\r\n\r\n      inputs = signature[signature_name].inputs\r\n      for k in inputs.keys():\r\n        tensor = self._sess.graph.get_tensor_by_name(inputs[k].name)\r\n        indiv_sig_data[_INPUT_KEYS_TO_TENSOR][k] = tensor\r\n\r\n      outputs = signature[signature_name].outputs\r\n      for k in outputs.keys():\r\n        tensor = self._sess.graph.get_tensor_by_name(outputs[k].name)\r\n        indiv_sig_data[_OUTPUT_KEYS_KEYS_TO_TENSOR][k] = tensor\r\n\r\n  def predict(self, payload):\r\n    start = time.time()\r\n    payload_sig = payload[\"signature_name\"]\r\n    res = self._sess.run(\r\n      self._signature_tensor_mapping[payload_sig][_OUTPUT_KEYS_KEYS_TO_TENSOR],\r\n      {\r\n        self._signature_tensor_mapping[payload_sig][_INPUT_KEYS_TO_TENSOR][\"inputs\"]: payload[\"inputs\"][\"inputs\"]\r\n      }\r\n    )\r\n    print(\"prediction took: {}s\".format(time.time() - start))\r\n    return res\r\n\r\nif __name__ == \"__main__\":\r\n\r\n  model = TF1Model(\"/tf1_models/object_detection/1\")\r\n\r\n  payload = {\r\n    \"signature_name\":\"serving_default\",\r\n    \"inputs\":{\r\n      \"inputs\":[\r\n        [[[0, 0, 0], [0, 0, 0]]]\r\n      ]\r\n    }\r\n  }\r\n\r\n  for _ in range(3):\r\n    model.predict(payload)\r\n```\r\n\r\n  [1]: http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz", "comments": ["sorry, false alarm closing..."]}, {"number": 50166, "title": "how to set the buffer_size in tf.data", "body": "hi,dear\r\nmy code runs more slowly then not distribute training ,\r\nwith big dataset,I use tf.data.Dataset.from_generator,\r\nis the buffer_size helpful for speeding up the training ?\r\nsimilar to the [issue](https://github.com/tensorflow/tensorflow/issues/33898),but a little different\r\n\r\ncould you pls help me ?\r\nthx\r\n", "comments": ["@ucasiggcas ,\r\n\r\nPlease take a look at this links for more information on buffer size in tf.data.It helps.[Link1](https://www.programmersought.com/article/60464190812/),[Link2](https://github.com/tensorflow/tensorflow/issues/14857#issuecomment-347261151),[Link3](https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle)\r\n\r\nThanks!", "Also, this is not a TensorFlow bug or feature request, it is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50166\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50166\">No</a>\n"]}, {"number": 50165, "title": "Problems to register a custom op in TF Lite", "body": "@tensorflow/micro\r\n\r\n  **System information**\r\n\r\n - OS Platform and Distribution: Linux Ubuntu 20.10\r\n - TensorFlow version: 2.4.1\r\n - Python version : 3.8 (installed via pip)\r\n - STM32CubeIDE-Lnx: 1.3.0\r\n - STM32 Nucleo-64 development board with STM32F401RE MCU\r\n\r\n**Describe the problem**\r\n\r\nI'd like to forecast RMSE values for seven days from a household power consumption dataset, using seven input test values corresponding to certain power consumption values, with a model which uses a custom op (Conv1D) and two builtin ops (Reshape and Fully Connected). I'm using a STM32 Nucleo-64 development board. The code is the following:\r\n\r\n          #include \"main.h\"\r\n          #include <string.h>\r\n         \r\n          #include \"tensorflow/lite/micro/kernels/micro_ops.h\"\r\n          #include \"tensorflow/lite/micro/micro_error_reporter.h\"\r\n          #include \"tensorflow/lite/micro/micro_interpreter.h\"\r\n          #include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\r\n          #include \"tensorflow/lite/version.h\"\r\n          #include \"tensorflow/lite/schema/schema_generated.h\"\r\n          #include \"power_consumption.h\"\r\n          #include \"tensorflow/lite/micro/micro_allocator.h\"\r\n          #include \"tensorflow/lite/micro/micro_op_resolver.h\"\r\n          #include \"tensorflow/lite/core/api/flatbuffer_conversions.h\"\r\n          #include \"flatbuffers/flatbuffers.h\"  // from @flatbuffers\r\n          #include \"tensorflow/lite/c/builtin_op_data.h\"\r\n          #include \"tensorflow/lite/c/common.h\"\r\n          #include \"tensorflow/lite/core/api/error_reporter.h\"\r\n          #include \"tensorflow/lite/kernels/internal/compatibility.h\"\r\n          #include \"tensorflow/lite/kernels/internal/reference/conv_1d.h\"\r\n          #include \"tensorflow/lite/schema/schema_generated.h\"\r\n          \r\n          CRC_HandleTypeDef hcrc;\r\n          TIM_HandleTypeDef htim11;\r\n          UART_HandleTypeDef huart2;\r\n          \r\n          // TFLite globals\r\n          namespace {\r\n          tflite::ErrorReporter* error_reporter = nullptr;\r\n          const tflite::Model* model = nullptr;\r\n          tflite::MicroInterpreter* interpreter = nullptr;\r\n          TfLiteTensor* model_input = nullptr;\r\n          TfLiteTensor* model_output = nullptr;\r\n          constexpr int kTensorArenaSize = 2 * 1024;\r\n          __attribute__((aligned(16)))uint8_t tensor_arena[kTensorArenaSize];\r\n          } // namespace\r\n    \r\n    \r\n          void SystemClock_Config(void);\r\n          static void MX_GPIO_Init(void);\r\n          static void MX_USART2_UART_Init(void);\r\n          static void MX_CRC_Init(void);\r\n          static void MX_TIM11_Init(void);\r\n\r\n          int main(void)\r\n          {\r\n\r\n            char buf[100];\r\n            int buf_len = 0;\r\n            TfLiteStatus tflite_status;\r\n            uint32_t num_elements;\r\n            uint32_t timestamp;\r\n            float y_val[7];\r\n            int i;\r\n\r\n            HAL_Init();\r\n            SystemClock_Config();\r\n            MX_GPIO_Init();\r\n            MX_USART2_UART_Init();\r\n            MX_CRC_Init();\r\n            MX_TIM11_Init();\r\n            HAL_TIM_Base_Start(&htim11);\r\n\r\n            static tflite::MicroErrorReporter micro_error_reporter;\r\n            error_reporter = &micro_error_reporter;\r\n\r\n            // Say something to test error reporter\r\n            error_reporter->Report(\"STM32 TensorFlow Lite test\");\r\n        \r\n           // Map the model into a usable data structure\r\n           model = tflite::GetModel(power_consumption);\r\n           if (model->version() != TFLITE_SCHEMA_VERSION)\r\n           {\r\n            error_reporter->Report(\"Model version does not match Schema\");\r\n            while(1);\r\n           }\r\n\r\n           // Pull in only needed operations (should match NN layers). \r\n           static tflite::MicroMutableOpResolver<3> micro_op_resolver;\r\n\r\n           // Add custom neural network layer operation\r\n           tflite_status = micro_op_resolver.AddCustom(\r\n           \"cd1\", tflite::ops::custom::Register_CONV_1D());\r\n\r\n           if (tflite_status != kTfLiteOk) {\r\n      \r\n            error_reporter->Report(\"Could not add Conv op\");\r\n            while(1);\r\n           }\r\n\r\n           tflite_status = micro_op_resolver.AddReshape();\r\n \r\n           if (tflite_status != kTfLiteOk) {\r\n             error_reporter->Report(\"Could not add RESHAPE op\");\r\n             while(1);\r\n           }\r\n\r\n          tflite_status = micro_op_resolver.AddFullyConnected();\r\n\r\n          if (tflite_status != kTfLiteOk) {\r\n          error_reporter->Report(\"Could not add FULLY CONNECTED op\");\r\n          while(1);\r\n          }\r\n\r\n          // Build an interpreter to run the model with.\r\n          static tflite::MicroInterpreter static_interpreter(\r\n          model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n          interpreter = &static_interpreter;\r\n\r\n          // Allocate memory from the tensor_arena for the model's tensors.\r\n          tflite_status = interpreter->AllocateTensors();\r\n          if (tflite_status != kTfLiteOk) {\r\n          error_reporter->Report(\"AllocateTensors() failed\");\r\n          while(1);\r\n          }\r\n\r\n         // Assign model input and output buffers (tensors) to pointers\r\n         model_input = interpreter->input(0);\r\n         model_output = interpreter->output(0);\r\n\r\n         // Get number of elements in input tensor\r\n         num_elements = model_input->bytes / sizeof(float);\r\n         buf_len = sprintf(buf, \"Number of input elements: %lu\\r\\n\", num_elements);\r\n         HAL_UART_Transmit(&huart2, (uint8_t *)buf, buf_len, 100);\r\n\r\n\r\n          /* Infinite loop */\r\n           while (1)\r\n           {\r\n            // Fill input buffer (use test value)\r\n            for (uint32_t i = 0; i < num_elements; i++)\r\n            {\r\n              model_input->data.f[i] = 150.0f;\r\n            }\r\n\r\n            // Get current timestamp\r\n            timestamp = htim11.Instance->CNT;\r\n\r\n            // Run inference\r\n            tflite_status = interpreter->Invoke();\r\n            if (tflite_status != kTfLiteOk)\r\n            {\r\n              error_reporter->Report(\"Invoke failed\");\r\n            }\r\n\r\n           // Read output RMSE (predicted y) of neural network\r\n           for(i=0; i<7; i++) {\r\n           y_val[i] = model_output->data.f[i];\r\n           }\r\n\r\n           // Print output of neural network along with inference time (microseconds)\r\n           for(i=0; i<7; i++) {\r\n           buf_len = sprintf(buf,\r\n                        \"Output: %f | Duration: %lu\\r\\n\",\r\n                        y_val[i],\r\n                        htim11.Instance->CNT - timestamp);\r\n           HAL_UART_Transmit(&huart2, (uint8_t *)buf, buf_len, 100);\r\n           }\r\n          // Wait before doing it again\r\n          HAL_Delay(500);\r\n\r\n       }\r\n      }\r\n\r\n\r\nI can compile it, but when I open PuTTY, I can read anything.\r\nFor the implementation of the custom op Conv_1D, I added a .cpp file and a .h file in tensorflow/lite/kernels.\r\n\r\nThe header file for the custom op is like this:\r\n\r\n            #ifndef TENSORFLOW_LITE_KERNELS_CONV1D_H_\r\n            #define TENSORFLOW_LITE_KERNELS_CONV1D_H_\r\n\r\n            #include \"tensorflow/lite/kernels/internal/types.h\"\r\n            #include \"tensorflow/lite/kernels/kernel_util.h\"\r\n\r\n            namespace tflite {\r\n            namespace ops {\r\n            namespace custom {\r\n\r\n            TfLiteRegistration* Register_CONV_1D();\r\n\r\n             }  // namespace custom\r\n             }  // namespace ops\r\n             }  // namespace tflite\r\n\r\n            #endif  // TENSORFLOW_LITE_KERNELS_CONV1D_H_\r\n\r\nThe .cpp file is like this (it's very simple and probably there are errors, for now I use certain weights because I don't know how to read the real ones from the trained model):\r\n\r\n                #include \"tensorflow/lite/kernels/conv_1d.h\"\r\n\r\n                #include <math.h>\r\n                #include <stddef.h>\r\n                #include <stdint.h>\r\n                \r\n                #include <vector>\r\n                \r\n                #include \"tensorflow/lite/c/common.h\"\r\n                \r\n                #include \"tensorflow/lite/kernels/internal/common.h\"\r\n                #include \"tensorflow/lite/kernels/internal/tensor.h\"\r\n                #include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\r\n                #include \"tensorflow/lite/kernels/kernel_util.h\"\r\n\r\n                namespace tflite {\r\n                namespace ops {\r\n                namespace custom {\r\n                namespace conv_1d {\r\n\r\n                const int dim = 5;\r\n                int dim_in;  \r\n                int dim_out;  \r\n                int dim_k = 3;    //kernel dimension\r\n                float copy[dim];\r\n\r\n                constexpr float kernel[3] = {1.2,2.0,4.2};\r\n                constexpr int dilation = 2;   //dilation\r\n\r\n                TfLiteStatus Conv1dPrepare(TfLiteContext* context, TfLiteNode* node) {\r\n  \r\n                TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);\r\n                TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\r\n\r\n                const TfLiteTensor* input = GetInput(context, node, 0);\r\n                TfLiteTensor* output = GetOutput(context, node, 0);\r\n\r\n                 int num_dims = NumDimensions(input);\r\n\r\n                 TfLiteIntArray* output_size = TfLiteIntArrayCreate(num_dims);\r\n                  for (int i=0; i<num_dims; ++i) {\r\n                  output_size->data[i] = input->dims->data[i];\r\n                  }\r\n\r\n                  return context->ResizeTensor(context, output, output_size);\r\n                 }\r\n\r\n                 TfLiteStatus Conv1dEval(TfLiteContext* context, TfLiteNode* node) {\r\n  \r\n                  const TfLiteTensor* input = GetInput(context, node,0);\r\n                  TfLiteTensor* output = GetOutput(context, node,0);\r\n\r\n                  float* input_data = input->data.f;\r\n                  float* output_data = output->data.f;\r\n  \r\n                  if (output->dims->data[0] > 1) \r\n                  dim_out = output->dims->data[0];\r\n    \r\n                  else dim_out = output->dims->data[1];\r\n    \r\n                  if (input->dims->data[0] > 1) \r\n                  dim_in = input->dims->data[0];\r\n    \r\n                  else dim_in = input->dims->data[1];\r\n  \r\n                  float copy0[4+dim_in];\r\n  \r\n                  for (int i=0; i<4; i++) {\r\n                  copy0[i] = 0;\r\n                  }\r\n  \r\n                  for (int i=0; i<dim_in; i++) {\r\n                  copy0[i+4] = input_data[i];\r\n                  }\r\n\r\n                   for (int i=0; i<dim_out; i++) {\r\n                   for (int m=0; m<dim; m++) {\r\n                   copy[m] = copy0[m+i];\r\n                   } \r\n                   for (int j=0; j<dim_k; j++) {\r\n                   output_data[i] = output_data[i] + copy[j*dilation]*kernel[j];\r\n                   }\r\n    \r\n                   }\r\n                   return kTfLiteOk;\r\n                   }\r\n\r\n\r\n               }  // namespace conv_1d\r\n\r\n              TfLiteRegistration* Register_CONV_1D() {\r\n              static TfLiteRegistration r = {nullptr, nullptr, conv_1d::Conv1dPrepare, conv_1d::Conv1dEval};\r\n              return &r;\r\n              }\r\n\r\n              }  // namespace custom\r\n              }  // namespace ops\r\n              }  // namespace tflite\r\n\r\nI think the problem is also not having register.cpp and register_ref.cpp files (I don't find them in tensorflow/lite/kernels), so I don't know if I can create these files myself or not (in this case I should be careful with the various header files).\r\n\r\nIn the BUILD file (I don't even have that) I'd write like this:\r\n\r\n            cc_library(\r\n            name = \"builtin_op_kernels\",\r\n            srcs = BUILTIN_KERNEL_SRCS + [\r\n               \"conv_1d.cc\",\r\n             ], \r\n             hdrs = [\r\n                \"dequantize.h\",\r\n                \"conv_1d.h\",\r\n             ],\r\n             copts = tflite_copts() + tf_opts_nortti_if_android() + EXTRA_EIGEN_COPTS,\r\n             visibility = [\"//visibility:private\"],\r\n             deps = BUILTIN_KERNEL_DEPS + [\r\n             \"@ruy//ruy/profiler:instrumentation\",\r\n             \"//tensorflow/lite/kernels/internal:cppmath\",\r\n             \"//tensorflow/lite:string\",\r\n             \"@farmhash_archive//:farmhash\",\r\n             ],\r\n             )\r\n\r\n\r\nI'm not sure if I'm missing some other steps to register the custom op.\r\nI used this tutorial as a reference: https://www.digikey.com/en/maker/projects/tinyml-getting-started-with-tensorflow-lite-for-microcontrollers/c0cdd850f5004b098d263400aa294023.", "comments": ["Could you upload this issue at the TF Lite Micro project instead?", "https://github.com/tensorflow/tflite-micro"]}, {"number": 50164, "title": "[ROCm] Adding rocm_py3x_pip.sh scripts for building TF+ROCm whls", "body": "/cc @angerson @perfinion ", "comments": ["Thanks for working on this!\r\n\r\nHowever, this directory is pretty tightly restricted to just the scripts that the TensorFlow team maintains. Would you mind putting these scripts in the SIG Build repository instead?", "> Thanks for working on this!\r\n> \r\n> However, this directory is pretty tightly restricted to just the scripts that the TensorFlow team maintains. Would you mind putting these scripts in the SIG Build repository instead?\r\n\r\nThe problem with sticking this in the SIG Build repo, is that these scripts become decoupled from the scripts they call (for eg `pip_new.sh` and others), from a version control point of view. These scripts do need tweaking from time to time, and those tweaks are typically tied to changes in the TF repo.\r\n\r\nFor e.g. imagine if we need to make some change to these scripts after we cut the TF 2.6 branch (as a consequence of some other change in the TF repo, which also occurs after we cut the TF 2.6 branch). At that point, we will need two versions of these scripts too (one that is tied to the TF 2.6 branch and does not have the changes and another in the master branch which has the changes).\r\n\r\nSo in short, it would be preferable to keep these scripts in the TF repo itself. How about here? - \r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/linux/rocm", "Good point. tools/ci_build/linux/rocm would be a better place for these than tensorflow/build.\r\n\r\n", "> Good point. tools/ci_build/linux/rocm would be a better place for these than tensorflow/build.\r\n\r\n@angerson done. please re-review"]}, {"number": 50163, "title": "Run pytest targets without compiling", "body": "This will add a specific `--targets` flag to specific user test targets.\r\n\r\nE.g. \r\n`./tensorflow/tools/ci_build/builds/run_pip_tests.sh --targets /tensorflow/python/eager:def_function_test`\r\n\r\n`./tensorflow/tools/ci_build/builds/run_pip_tests.sh --targets /tensorflow/python/eager:all`\r\n\r\nSee https://discuss.tensorflow.org/t/run-python-tests-without-compiling-tf\r\n\r\n/cc @angerson @perfinion @mihaimaruseac What kind of extra checks we need to add to promote this in Doc as a temp workaround for python PRs? \r\n", "comments": ["This LGTM, but waiting on Austin and Jason too.", "What we need to do to use python files in the checkout other then tests?\r\nGenerally with a PR you edit both the python source of the feature/bug and the related tests.", "@mihaimaruseac Thanks for the review but I am going to close this as in TF bazel BUILD files we don't have a clear separation between python an c++ targets."]}, {"number": 50162, "title": "Update raw_ops_test.py", "body": "", "comments": []}, {"number": 50161, "title": "Revert \"Update sparse_cross_op.cc\"", "body": "", "comments": []}, {"number": 50160, "title": "Disabling failing raw_ops_test", "body": "", "comments": []}, {"number": 50159, "title": "[tf.data] graduate ThreadingOptions from experimental to tf.data", "body": "**[UPDATED]** This PR graduates `tf.data.experimental.ThreadingOptions` to `tf.data.ThreadingOptions`.\r\n\r\n- [x] deprecate the old endpoint\r\n- [x] update docstring of the class\r\n- [x] Modify the serialization-deserialization implementation of `options.threading` and `options.experimental_threading` for backward-compatibility\r\n- [x] regenerate golden APIs\r\n- [x] add relevant test cases for the new ser-des implementation.\r\n- [x] update RELEASE.md\r\n\r\nTEST LOG\r\n```\r\nINFO: Elapsed time: 43.917s, Critical Path: 34.51s\r\nINFO: 351 processes: 130 internal, 221 processwrapper-sandbox.\r\nINFO: Build completed successfully, 351 total actions\r\n//tensorflow/python/data/kernel_tests:options_test                       PASSED in 3.3s\r\n\r\nINFO: Build completed successfully, 351 total actions\r\n```\r\n\r\ncc: @jsimsa", "comments": ["@jsimsa as per your comment, while converting the `experimental_threading` or `threading` options to proto we can choose based on non-default values. However, how do we decide which one to populate while retrieving from proto? We have to populate both to ensure backward compatibility, but that contradicts the comment when the users try to convert it to a proto again.", "> @jsimsa as per your comment, while converting the `experimental_threading` or `threading` options to proto we can choose based on non-default values. However, how do we decide which one to populate while retrieving from proto? We have to populate both to ensure backward compatibility, but that contradicts the comment when the users try to convert it to a proto again.\r\n\r\nYou raise a good point. The short answer is that I am fine with not providing backwards compatibility for getting the value of the threading options (which I expect to be an rare use case as the options are commonly set but user code but rarely read).  So my recommendation would be to only populate the `threading` object and when the `experimental_threading` attribute is access, we should log a warning to inform users that they should be using the `threading` attribute instead.", "@jsimsa I have taken a slightly different approach in which I set the `experimental_threading` and `threading` values to be the same while ser-des (a higher preference is given to `threading` while converting to proto). I have made the modifications to the tests and added a deprecation warning when users set the `experimental_threading` attribute. Let me know how it looks.", "@jsimsa added the logger which warns about the overridden options from `experimental_threading` and modified the test cases a bit. PTAL.", "@kvignesh1420 can you please resolve conflicts ?", "@rthadur the conflicts have been resolved."]}, {"number": 50158, "title": " NameError: name 'interpreter_wrapper' is not defined ", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nHi,\r\nI am executing tflite through mlperf on riscv linux, getting following error, how to fix it?\r\n\r\n>     ./run_local.sh tflite --profile ssd-mobilenet-tf\r\n> \r\n>     INFO:main:Namespace(accuracy=False, backend='tflite', cache=0, count=None, data_format=None, dataset='coco-300', dataset_list=None, dataset_path='/home/root/Ankita/data/coco', debug=False, find_peak_performance=False, inputs=['image_tensor:0'], max_batchsize=32, max_latency=None, mlperf_conf='../../mlperf.conf', model='/home/root/Ankita/model/resnet50_v1.tflite', model_name='ssd-mobilenet', output='/home/root/Ankita/inference/vision/classification_and_detection/output/tflite-cpu/resnet50', outputs=['num_detections:0', 'detection_boxes:0', 'detection_scores:0', 'detection_classes:0'], profile='ssd-mobilenet-tf', qps=None, samples_per_query=None, scenario='SingleStream', threads=4, time=None, user_conf='user.conf')\r\n>     INFO:coco:reduced image list, 4997 images not found\r\n>     INFO:coco:loaded 3 images, cache=0, took=9.0sec\r\n>     /home/root/Ankita/inference/vision/classification_and_detection/python/coco.py:115: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\r\n>     self.label_list = np.array(self.label_list)\r\n>     Traceback (most recent call last):\r\n>     File \"python/main.py\", line 558, in\r\n>     main()\r\n>     File \"python/main.py\", line 436, in main\r\n>     model = backend.load(args.model, inputs=args.inputs, outputs=args.outputs)\r\n>     File \"/home/root/Ankita/inference/vision/classification_and_detection/python/backend_tflite.py\", line 32, in load\r\n>     self.sess = interpreter_wrapper.Interpreter(model_path=model_path)\r\n>     NameError: name 'interpreter_wrapper' is not defined\r\n\r\nThanks", "comments": ["Instead of using the interpreter_wrapper, could you try using the `tf.lite.Interpreter` in your Python code?", "> Instead of using the interpreter_wrapper, could you try using the `tf.lite.Interpreter` in your Python code?\r\n\r\nHi,\r\n\r\n\r\n> Instead of using the interpreter_wrapper, could you try using the `tf.lite.Interpreter` in your Python code?\r\n\r\nYes I tried but tensorflow is not ported to riscv so I got tensorflow module not found error. Any other way to fix this?", "You may use `tflite_runtime` package to import `Interpreter`\r\nhttps://www.tensorflow.org/lite/guide/python#about_the_tensorflow_lite_runtime_package", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50158\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50158\">No</a>\n"]}, {"number": 50157, "title": "Update bot_config.yml", "body": "", "comments": []}, {"number": 50156, "title": "Update sparse_fill_empty_rows_op.cc", "body": "", "comments": []}, {"number": 50155, "title": "TypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.", "body": "I am implementing a 1d conv net on Google Colab but I am getting the following error when I run the attached part of code wich indicates my model.\r\n\r\n\r\nTypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.\r\n\r\nMy model is as follows:\r\n\r\n```\r\ninput = tf.keras.Input(shape=(100,1), name='input')\r\n\r\nx_0 = Conv1D(20,1,strides=1,activation='relu', name='C0')(input)\r\n\r\nx_1 = Conv1D(50,2,strides=1, activation='relu', name='C1')(x_0)\r\nx_1 = Dropout(0.3, name='DR1')(x_1)\r\nx_1 = GlobalMaxPooling1D(name='MP1')(x_1)\r\n\r\nx_2 = Conv1D(35,3,strides=1,activation='relu', name='C2')(x_0)\r\nx_2 = Dropout(0.3, name='DR2')(x_2)\r\nx_2 = GlobalMaxPooling1D(name='MP2')(x_2)\r\n\r\nx_3 = Conv1D(25,4,strides=1,activation='relu', name='C3')(x_0)\r\nx_3 = Dropout(0.3, name='DR3')(x_3)\r\nx_3 = GlobalMaxPooling1D(name='MP3')(x_3)\r\n\r\nx_4 = Conv1D(20,5,strides=1,activation='relu', name='C4')(x_0)\r\nx_4 = Dropout(0.3, name='DR4')(x_4)\r\nx_4 = GlobalMaxPooling1D(name='MP4')(x_4)\r\n\r\nconcat = Concatenate(axis=1, name='concat')([x_1, x_2, x_3, x_4])\r\nconcat = BatchNormalization(name='BN')(concat)\r\n\r\noutput = Dense(units=1, activation='linear',name='output')(concat)\r\n\r\nmodel = tf.keras.Model(inputs=input, outputs=output, name='VRP')\r\n```", "comments": ["@navid71 ,\r\n\r\nPlease, fill issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also please take a look at this issues with similar error.It helps.[Link1](https://github.com/tensorflow/tensorflow/issues/47311),[Link2](https://stackoverflow.com/questions/65366442/cannot-convert-a-symbolic-keras-input-output-to-a-numpy-array-typeerror-when-usi).\r\n\r\nThanks!", "I can reproduce the code without any erros.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/0e1ecf9695e68293258e519da9ea385b/50155.ipynb). Try to execute the code adding **tf.keras.layers** as mentioned in gist and check if you are facing issue again.Thanks", "@tilakrayal \r\n\r\nThanks a lot for your help. **tf.keras.layers** solved this issue.", "@navid71 ,\r\n\r\nGlad the issue is resolved for you, please move the issue to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50155\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50155\">No</a>\n"]}, {"number": 50153, "title": "[XLA] Bitcast lift", "body": "This PR includes 2 changes:\r\n1) when a rank-reducing bitcast is present inside a kInput fusion, we lift it to outside the fusion. This lower the indexing computation generated by XLA. LLVM isn't always able to optimize it.\r\n2) Mark kExp as not expensive. This cause its computation to be duplicated in the gradient of EfficientNet. This remove IO as the forward fusion now have one less output and the gradient have less inputs. This also allows a bigger mini-batch size. This is a one liner + test.\r\n\r\n\r\n\r\nNumber 1) speed up some kernels in EfficientNet. But number 2) on A100 cause one kernel to slow down as the simpler fusion is not as much optimized. Further optimization is possible. But to keep the changes changes, I combine them as the combination of both optimization doesn't cause any regression. Doing them in 2 separate PR cause one intermediate steps that will be faster for one kernel. **On V100, we do not have this problem.**\r\n\r\nHere is one not-optimized HLO dump of a simplified EfficientNet script that show the problem this optimize. Before this PR, it generates a fusion called: fusion.7. After this PR, that kernel is renamed fusion.6_bitcast.\r\n\r\n[module_0000.before_optimizations.txt](https://github.com/tensorflow/tensorflow/files/6617698/module_0000.before_optimizations.txt)\r\n\r\ntimed previous commit: 643e7db56ec5d9b0d7deecaf6a089374d140a286\r\ntimed current commit: dcc241f057047129e234a5cb3155b0d21e4aee59\r\n\r\nV100: old time: 660us, new time: 557us\r\nA100: old time: 622us, new tiem: 556us\r\n\r\n\r\n@cheshire", "comments": ["> Note those numbers come from our branch based on TF2.5 as replay computation give me this error with upstream TF:\r\n\r\nCould you clarify/start a separate email thread about it? I use `run_hlo_module` and it should work fine on unoptimized HLO. Should we merge it with `replay_computation` to avoid confusion?", "So who creates these bitcasts, the reduction normalization passes?", "> So who creates these bitcasts, the reduction normalization passes?\r\n\r\nExact.", "> > Note those numbers come from our branch based on TF2.5 as replay computation give me this error with upstream TF:\r\n> \r\n> Could you clarify/start a separate email thread about it? I use `run_hlo_module` and it should work fine on unoptimized HLO. Should we merge it with `replay_computation` to avoid confusion?\r\n\r\nI found that I had an XLA_FLAGS was setting the autotune to level 2. When I remove it, I'm able to get the speed up numbers on V100 and A100 with the commit in this PR. I updated the description with the timing for V100 and A100.\r\n\r\nI'll try to make a repro for the autotune problems and open an issue when I have that.", "We found that this PR cause speed regression for some shapes even if it speed up others. I'll investigate this this week. Just wait before merging this.", "These bitcasts are not actually necessary for correctness, unless we are doing tree reduction.\r\nWe saw other fusion issues from them.\r\n\r\nShould we move them behind the flag instead, same as the TreeReductionRewriter?", "The slow down for one specific shape is only on A100 and not on V100.\r\nThe file with the not optimized LLVM generated by XLA on V100 is a little bit smaller with the bitcast lift opt on V100. But on A100, it is 1.6x bigger. So there is something in XLA codegen that does something not expected.\r\n\r\nLet me a few days to investigate this. There is already tons of switch, so if possible, I would prefer not to have an extra one.", "Generally, I think we have found FileCheck-based tests to be more robust\nand more readable than EXPECT_ counterparts when matching IR.\n\nOn Thu, Jun 24, 2021 at 12:03 PM Fr\u00e9d\u00e9ric Bastien ***@***.***>\nwrote:\n\n> ***@***.**** commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/compiler/xla/service/gpu/fusion_bitcast_lift_test.cc\n> <https://github.com/tensorflow/tensorflow/pull/50153#discussion_r658313103>\n> :\n>\n> > +\n> +ENTRY main {\n> +  %param_0.59 = f16[2,14,14,672] parameter(0)\n> +  %param_1 = f32[] parameter(1)\n> +  ROOT %fusion.21_4d = (f32[672]{0}, f32[672]{0}) fusion(%param_0.59, %param_1), kind=kInput, calls=%fused_computation.21_4d\n> +}\n> +)\";\n> +  auto module = ParseAndReturnVerifiedModule(hlo_text).ValueOrDie();\n> +  EXPECT_TRUE(FusionBitcastLift().Run(module.get()).ValueOrDie());\n> +\n> +  auto* root = module->entry_computation()->root_instruction();\n> +  EXPECT_EQ(HloOpcode::kFusion, root->opcode());\n> +\n> +  // The fusion should have 2 inputs and the first one should be a\n> +  // bitcast with 2d output.\n> +  EXPECT_EQ(2, root->operands().size());\n>\n> I didn't know about the MatchOptimizedHlo fct. Only about the FileCheck\n> test in separate file. I can update the test to use MatchOptimizedHlo. But\n> it will probably be Monday.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/50153#discussion_r658313103>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH3TQBERSV3O4JGAYDTTUOTUDANCNFSM46KHCEEA>\n> .\n>\n", "I updated the test to use `RunFileCheck()`. I can't use `MatchOptimizedHlo()` as it apply the full optimization pipeline and this doesn't support MOF graph as input. So I apply only the optimization that I want to test as before.", "Just a heads-up that I had to fix a few things to get this PR merged:\r\n- Our ClangTidy tool complains about using .size() checks when .empty() is possible as well\r\n- There was no handling for Constants, which made one test fail that we only run internally (a tensorflow python test with XLA enabled). I added this.\r\n- Some BUILD file dependencies were wrong\r\n- Status return values were ignored, I added TF_RETURN_IF_ERROR macro calls around them.\r\n\r\nI hope that my changes did not introduce any problems.", "> Just a heads-up that I had to fix a few things to get this PR merged:\r\n> \r\n> * Our ClangTidy tool complains about using .size() checks when .empty() is possible as well\r\n> * There was no handling for Constants, which made one test fail that we only run internally (a tensorflow python test with XLA enabled). I added this.\r\n> * Some BUILD file dependencies were wrong\r\n> * Status return values were ignored, I added TF_RETURN_IF_ERROR macro calls around them.\r\n> \r\n> I hope that my changes did not introduce any problems.\r\n\r\nActually, merging is still not possible, the new fusion_bitcast_lift_test fails with --config=asan.\r\nCan you reproduce this problem, and do you possibly know what causes it? I haven't found it yet. If you find anything, please let me know and I can include the fix in my modified version of your PR.", "> Actually, merging is still not possible, the new fusion_bitcast_lift_test fails with --config=asan.\r\n> Can you reproduce this problem, and do you possibly know what causes it? I haven't found it yet. If you find anything, please let me know and I can include the fix in my modified version of your PR.\r\n\r\nI started the compilation. I'll investigate too and I'll check the code in case I can spot it.", "> > Actually, merging is still not possible, the new fusion_bitcast_lift_test fails with --config=asan.\r\n> > Can you reproduce this problem, and do you possibly know what causes it? I haven't found it yet. If you find anything, please let me know and I can include the fix in my modified version of your PR.\r\n> \r\n> I started the compilation. I'll investigate too and I'll check the code in case I can spot it.\r\n\r\nThe problem seems to be upstream. If I try to run the test `//tensorflow/compiler/xla/service/gpu:multi_output_fusion_test`, I also have a memory leak error *before* the compilation of the test itself. So it give this output:\r\n```//tensorflow/compiler/xla/service/gpu:multi_output_fusion_test  FAILED TO BUILD\r\n```\r\nand \r\n```\r\n//tensorflow/compiler/xla/service/gpu:fusion_bitcast_lift_test  FAILED TO BUILD\r\n```\r\n\r\nAs the error start with:\r\n```\r\n  bazel-out/k8-opt/bin/external/nasm/nasm -f ebf64    -DELF -DPIC -D__x86_64__    -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfig.h)/    -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfigint.ho;\r\n)/o   -I $(dirname external/libjpeg_turbo/simd/nasm/jsimdcfg.inc.h)/    -I $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/    -o $out    $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.o}.asm)                                                                                                                                                                                                           )/\r\ndone')                                                                                                                                                                                                                         \r\nExecution platform: @local_execution_config_platform//:platform\r\n```\r\nMy guess is that the problem isn't with my PR. I'll continue some testing.", "I can reproduce the error without my PR. I tried commit `160317c0b5aadecd8652af24d3e30eab82f432df` and the test `multi_output_fusion_test` and reproduce the error. What make you think the problem isn't already upstream?", "> I can reproduce the error without my PR. I tried commit `160317c0b5aadecd8652af24d3e30eab82f432df` and the test `multi_output_fusion_test` and reproduce the error. What make you think the problem isn't already upstream?\r\n\r\nI think you are seeing something different here than what I see. Here is the warning I see with your test:\r\n\r\n```\r\n[ RUN      ] FusionBitcastLiftTest.NoBroadcastTest\r\n=================================================================\r\n==10264==ERROR: AddressSanitizer: container-overflow on address 0x6030001498a0 at pc 0x7f103da9d506 bp 0x7ffc5c6badd0 sp 0x7ffc5c6badc8\r\nREAD of size 8 at 0x6030001498a0 thread T0\r\n```\r\n\r\n(a stack trace involving tensorflow/compiler/xla/service/gpu/fusion_bitcast_lift.cc:191:39)\r\n\r\n0x6030001498a0 is located 16 bytes inside of 32-byte region [0x603000149890,0x6030001498b0)\r\nallocated by thread T0 here:\r\n\r\n(a stack trace involving tensorflow/compiler/xla/service/gpu/fusion_bitcast_lift.cc:178:19)\r\n\r\nThe stack traces look a bit different internally than externally because our directory structure is different, therefore I didn't copy it as is.\r\nIn any case, I guess I will get your code merged with the test disabled and the pass not yet added to gpu_compiler. Then we have the fixes I already did, and can continue from there.", "I fixed all comments, including the kConstant fix.\r\nI quickly tested the only 1 user restriction to the general problem to my real world cases and it is too restrictive. I'll continue to think about this case.", "I thought of many ideas to fix the general issue.\r\nCurrently my prefered one is to build the new hypothetical fusion. Then do the replacement only if HLOVerifier pass. Is that acceptable with you? This would also prevent other potential issues, like having a MOF with multiple reduction, but only some of the reduction have the bitcast outside the Op. Then we end up with a mix of input dtype. I didn't test that case and I'm not sure how well it will be supported.", "> I thought of many ideas to fix the general issue.\r\n> Currently my prefered one is to build the new hypothetical fusion. Then do the replacement only if HLOVerifier pass. Is that acceptable with you? This would also prevent other potential issues, like having a MOF with multiple reduction, but only some of the reduction have the bitcast outside the Op. Then we end up with a mix of input dtype. I didn't test that case and I'm not sure how well it will be supported.\r\n\r\nOne argument against that approach would be that it is less predictable what the pass will do. At the same time, the pass would get pretty complex if you try to handle all cases, and running the verifier would at least make sure that the pass doesn't introduce any invalid HLO. So from that perspective, I think I am ok with this approach.\r\n\r\n@cheshire, what do you think?", "Also, I wonder whether you can take some inspiration from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/reshape_mover.cc. It does something similar to what you try to do here, just that it moves the reshapes in the other direction, and also does it before they are replaced with bitcasts.", "I looked at the reshape_mover optimization and didn't think of new way of doing that.\r\n\r\nI used ShapeVerifier, as HloVerifier works on module, not on HloComputation and ShapeVerifier cover what we want.\r\n\r\nSo I finished all the comments.", "I just pushed a fix to one CI error.", "The change got rolled back because a test was failing (the same one I had seen failing before which is a variant with XLA enabled of a tensorflow test, unfortunately not available in open source). It fails because due to the shape check, nothing is changed, but you still have the check enabled that the fusion is changed: \"We should have changed the fusion!\".\r\nI think I can fix this and roll it forward.\r\nSorry for not notifying you before, I was out of office when the change got rolled back."]}, {"number": 50152, "title": "Fix swapping of tensor_content when loading a SavedModel on s390x", "body": "Based on PR https://github.com/tensorflow/tensorflow/pull/45339, this PR addresses swapping of `tensor_content` data when importing a TF 2.x `SavedModel` into a TF 1.x-style `graph` on Big-Endian systems.\r\n\r\nTest case\u00a0`//tensorflow/python/keras/integration_test:saved_model_test`\u00a0fails on Big-Endian systems because it tries to load a TF 2.x `SavedModel` whose `tensor_content` data had been transformed to and stored as Little-Endian format.\r\n\r\n`load_graph()` function in `loader_impl.py` has been modified to fix this issue by swapping of `tensor_content` data from LE to BE after\u00a0`meta_graph_def`\u00a0proto is loaded on BE systems.\r\n\r\nSigned-off-by: Kun-Lu <kun.lu@ibm.com>", "comments": ["Hi @k-w-w , could you please take a look at this PR? Thank you!", "Update for this PR:\r\n\r\nTest case\u00a0`//tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model:keras.py.test` would also fail on Big-Endian systems. This test case saves a `Keras` model at first, then loads it from\u00a0`saved_model`\u00a0file. It uses\u00a0`python`\u00a0code to save the model, which swaps the tensor content for constant operators on Big-Endian systems. But when it comes to the model loading, the\u00a0`c++`\u00a0code doesn\u2019t swap the tensor data, so the dimension info of the tensors becomes incorrect.\r\n\r\nData swap code is added in function\u00a0`SavedModelV2Bundle::Load()`\u00a0in\u00a0`tensorflow/cc/saved_model/bundle_v2.cc`\u00a0after it loads the\u00a0`meta_graph_def`\u00a0from\u00a0`saved_model`\u00a0file. Function\u00a0`ByteSwapBuffer()`\u00a0is added to\u00a0`tensorflow/core/util/tensor_bundle/byte_swap.cc`\u00a0to do the actual data swap.\u00a0\r\n\r\nThis code change would not cause any regressions on existing test cases.", "Hi @ccrusius , could you please take a look at this PR? Thank you!", "Hi @gbaned @ccrusius , could you please have a look at the code changes? I am looking forward to hearing the comments from you. Thank you very much!", "@ccrusius Can you please review this PR ? Thanks!", "Hi @ccrusius, could you please take a look at this PR? Thank you very much!", "@kun-lu20  Can you please approve this PR by click on Approve and run button.  Thank you.", "Hi @gbaned , I didn\u2019t find the Approve and run button. It looks like I need a maintainer to approve running the workflow. Thanks!", "> Hi @gbaned , I didn\u2019t find the Approve and run button. It looks like I need a maintainer to approve running the workflow. Thanks!\r\n\r\nHi @kun-lu20  Sorry, I have clicked on Approve and run button. Thank you. ", "@gbaned No problem. Thank you very much!", "Hi @ccrusius @gbaned , could you please review this PR? I am looking forward to hearing the feedback from you. Thank you very much!", "Hi @ccrusius @gbaned , could you please take a look at this PR? Thank you!", "Re-assigning reviewer to @pcish - I remember already trying to get this in but running into some test issues.", "Hi @pcish , thank you very much for your detailed feedback. I've responded to your questions and made updates accordingly. Please take a look when you are available, thanks again!", "@pcish Thanks! Since the code changes are specific to big-endian machines, it's strange that some Windows tests are failing.", "Update: I'm fairly certain the current Windows failures are not related. I'll keep an eye on the build status and approve this as soon as the issue(s) with the Windows build are fixed.", "@pcish Thank you so much!", "Thank you @pcish !"]}, {"number": 50151, "title": "Update sparse_fill_empty_rows_op.cc", "body": "", "comments": []}, {"number": 50150, "title": "tensorflow.python.keras.applications.vgg16 model got low accuracy on the ILSVRC2012 validation dataset", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: NA\r\n-   **TensorFlow installed from (source or binary)**: pip install\r\n-   **TensorFlow version (use command below)**: 2.4\r\n-   **Python version**: 3.7\r\n-   **Bazel version (if compiling from source)**: NA\r\n-   **GCC/Compiler version (if compiling from source)**: NA\r\n-   **CUDA/cuDNN version**: CUDA 11.0/cuDNN 8.0\r\n-   **GPU model and memory**: GTX1060 6G\r\n-   **Exact command to reproduce**: NA\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nI imported the VGG-16 model from tensorflow.python.keras.applications.vgg16 and evaluated it on the ILSVRC2012 validation dataset. According to \"https://keras.io/api/applications/\", the top-1 and top-5 accuracy of VGG-16 model should be 71.3% and 90.1%, but I got only 65.7% and 86.8% on the ILSVRC2012 validation dataset.\r\n\r\nI downloaded the validation dataset and loaded it using \"image_dataset_from_directory()\" imported from tensorflow.keras.preprocessing. I preprocessed the images using \"preprocess_input()\" imported from tensorflow.python.keras.applications.vgg16. I have also dealed with the inconsistency between the original imagenet ILSVRC2012_ID and the class index which the pre-trained model uses.\r\n\r\nI wonder why I got low accuracy. I suspect that the image resize interpolation method affects data destribution. I suggest that the image preprocessing method should be explicitly listed since it might vary from model to model.\r\n\r\nI also found a bug. The function image_dataset_from_directory( ..., interpolation='nearest') will return a dataset of which the datat type is 'uint8' instead of 'float32', while other interpolation methods always return 'float32' dataset.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nsource code:\r\n```\r\nfrom __future__ import print_function as _print_function\r\n\r\nimport sys as _sys\r\n\r\nfrom tensorflow.python.keras.applications.vgg16 import VGG16\r\nfrom tensorflow.python.keras.applications.vgg16 import decode_predictions\r\nfrom tensorflow.python.keras.applications.vgg16 import preprocess_input\r\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\r\n\r\nimport numpy as np\r\n\r\ndef preprocess(img, label):\r\n    img = preprocess_input(img)\r\n    return img, label\r\n\r\nif __name__ == '__main__':\r\n    model = VGG16(include_top=True, weights='imagenet')\r\n    #model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy',metrics=['accuracy']) # 65.68%\r\n    model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy',metrics=['sparse_top_k_categorical_accuracy']) # 86.75%\r\n\r\n    ds = image_dataset_from_directory('C:\\ILSVRC2012\\ILSVRC2012_img_val',\r\n        labels=list(np.loadtxt('new_val_truth.txt', dtype='int')),\r\n        label_mode='int', color_mode='rgb', batch_size=32,\r\n        image_size=(224, 224), shuffle=False, interpolation='bilinear')\r\n    ds = ds.map(preprocess)\r\n\r\n    model.evaluate(ds, verbose=1)\r\n```\r\n\r\nlogs:\r\n```\r\n1563/1563 [==============================] - 684s 432ms/step - loss: 1.4227 - accuracy: 0.6568\r\n1563/1563 [==============================] - 682s 430ms/step - loss: 1.4227 - sparse_top_k_categorical_accuracy: 0.8675\r\n```", "comments": ["@nicolaswilde \r\nI reproduced the code shared but facing different [error](https://colab.research.google.com/gist/UsharaniPagadala/7ac3175bcf9309268c15594a67684624/untitled86.ipynb) because of `.txt` file.Could you please share the colab gist with all the dependencies to analyze more of it. Thanks", "@UsharaniPagadala \r\nThanks a lot! I have found the problem! According to \"https://stackoverflow.com/questions/58248297/keras-applications-vgg16-low-accuracy-on-imagenet\", the 224x224x3 images should be created by a center crop. For example, a 500 * 375 image should be firstly cropped to a 375 * 375 image, then resized to 224 * 224.\r\n\r\nI found that tensorflow.python.keras.preprocessing.image provides the function defined as \"smart_resize(x, size, interpolation='bilinear')\". Then I modified the regular resizing method `img = image_ops.resize_images_v2(img, image_size, method=interpolation)` in `tensorflow.python.keras.preprocessing.image_dataset.path_to_image(path, image_size, num_channels, interpolation):` by `img = image.smart_resize(img, image_size, interpolation=interpolation)`. When I used the \"smart_resize\" function to resize the images, I got 70.08%(interpolation='bicubic') top1-accuracy, which is close to 71.3% listed in the keras document. But I still have no idea how to improve the rest 1.2% accuracy.\r\n\r\nDuring the experiment I found another bug (Actually I have no idea whether it is a bug). When setting verbose = 1, the dynamic accuracy log is not consistent with the real accuracy. For example, when I run the following code:\r\n```\r\nloss, acc1, acc5 = model.evaluate(ds, verbose=1)\r\nprint(acc1, acc5)\r\n```\r\nI got:\r\n```\r\n1563/1563 [==============================] - 772s 488ms/step - loss: 1.2287 - sparse_categorical_accuracy: 0.7007 - sparse_top_k_categorical_accuracy: 0.8932\r\n0.6998400092124939 0.892520010471344\r\n```\r\nThe accuracies is a little bit different.", "@nicolaswilde \r\n\r\nThank you for your update, glad its working fine for you, kindly move this issue to closed status as it is resolved.\r\ncould you please post a new issue with that bug along with the reproducible code.\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50150\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50150\">No</a>\n"]}, {"number": 50149, "title": "Could compile C API library but I get \"undefined reference\" errors (Linking problem) using CMake", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **from source**\r\n- TensorFlow version: **master branch commit 700533808e6016dc458bb2eeecfca4babfc482ec**\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): **CMake version 3.20.3**\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**I could successfully compile the C API library, however, I get \"undefined reference\" errors when trying to link the library to the following simple CMake project. I would appreciate your help on this**\r\n\r\n```\r\n#include <stdio.h>\r\n#include \"tensorflow/lite/c/c_api.h\"\r\n\r\nint main() {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TfLiteVersion());\r\n  return 0;\r\n}\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ncmake ..\r\nmake\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n`\r\n[ 50%] Linking CXX executable bin/example\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(activations.cc.o): In function `tflite::ops::builtin::activations::SoftmaxFloat(TfLiteContext*, TfLiteTensor const*, TfLiteTensor*, TfLiteSoftmaxParams*, tflite::ops::builtin::activations::KernelType)':\r\nactivations.cc:(.text+0x6338): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, signed char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nbatch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e3): undefined reference to `ruy::Kernel8bitAvx(ruy::KernelParams8bit<8, 8> const&)'\r\nbatch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3a1): undefined reference to `ruy::Kernel8bitAvxSingleCol(ruy::KernelParams8bit<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `ruy::Kernel<(ruy::Path)64, signed char, signed char, int, signed char>::Run(ruy::PMat<signed char> const&, ruy::PMat<signed char> const&, ruy::MulParams<int, signed char> const&, int, int, int, int, ruy::Mat<signed char>*) const [clone .isra.664]':\r\nbatch_matmul.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaiaE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIiaEEiiiiPNS_3MatIaEE.isra.664[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e1): undefined reference to `ruy::Kernel8bitAvx512(ruy::KernelParams8bit<16, 16> const&)'\r\nbatch_matmul.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaiaE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIiaEEiiiiPNS_3MatIaEE.isra.664[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3c1): undefined reference to `ruy::Kernel8bitAvx512SingleCol(ruy::KernelParams8bit<16, 16> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)1, signed char, signed char, int, signed char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nbatch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3cb): undefined reference to `ruy::detail::MultiplyByQuantizedMultiplier(int, int, int)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, signed char, signed char, int, signed char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nbatch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e3): undefined reference to `ruy::Kernel8bitAvx2(ruy::KernelParams8bit<8, 8> const&)'\r\nbatch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3a1): undefined reference to `ruy::Kernel8bitAvx2SingleCol(ruy::KernelParams8bit<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `tflite::optimized_ops::BatchMatMul(tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::CpuBackendContext*)':\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0x5ce): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0x5dd): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0x794): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0x7f9): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xbab): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xc11): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xc19): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xc62): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xd48): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xd50): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xd99): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `tflite::optimized_ops::BatchMatMul(tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, float const*, int const*, int*, tflite::RuntimeShape const&, int*, float*, bool*, tflite::CpuBackendContext*)':\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x7e1): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x7ec): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x948): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x9c7): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x1120): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x1128): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x117b): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `void ruy::detail::CreateTrMulParamsAssumingColMajorDst<(ruy::Path)113, signed char, signed char, int, signed char>(ruy::Mat<signed char> const&, ruy::Mat<signed char> const&, ruy::Mat<signed char> const&, ruy::MulParams<int, signed char> const&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\nbatch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x110): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nbatch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d9): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nbatch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1e1): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nbatch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x232): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nbatch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x2ae): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nbatch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x4c3): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `tflite::optimized_ops::BatchMatMul(tflite::FullyConnectedParams const&, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char*, tflite::CpuBackendContext*)':\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x6b0): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x760): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x8ca): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x8d9): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0xb20): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0xb28): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0xb7f): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0xc20): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x100f): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x1185): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x1194): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x13b8): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x1452): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x14a3): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x14ab): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x14f7): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x158c): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nbatch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x197f): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `ruy::Kernel<(ruy::Path)64, signed char, signed char, int, unsigned char>::Run(ruy::PMat<signed char> const&, ruy::PMat<signed char> const&, ruy::MulParams<int, unsigned char> const&, int, int, int, int, ruy::Mat<unsigned char>*) const [clone .isra.1141]':\r\nconv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaihE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIihEEiiiiPNS_3MatIhEE.isra.1141[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e1): undefined reference to `ruy::Kernel8bitAvx512(ruy::KernelParams8bit<16, 16> const&)'\r\nconv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaihE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIihEEiiiiPNS_3MatIhEE.isra.1141[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3c1): undefined reference to `ruy::Kernel8bitAvx512SingleCol(ruy::KernelParams8bit<16, 16> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)1, unsigned char, unsigned char, int, unsigned char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nconv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EhhihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EhhihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3cb): undefined reference to `ruy::detail::MultiplyByQuantizedMultiplier(int, int, int)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `gemmlowp::WorkersPool::LegacyExecuteAndDestroyTasks(std::vector<gemmlowp::Task*, std::allocator<gemmlowp::Task*> > const&)':\r\nconv.cc:(.text._ZN8gemmlowp11WorkersPool28LegacyExecuteAndDestroyTasksERKSt6vectorIPNS_4TaskESaIS3_EE[_ZN8gemmlowp11WorkersPool28LegacyExecuteAndDestroyTasksERKSt6vectorIPNS_4TaskESaIS3_EE]+0x2a0): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `void ruy::Mul<(ruy::Path)113, signed char, signed char, int, int>(ruy::Matrix<signed char> const&, ruy::Matrix<signed char> const&, ruy::MulParams<int, int> const&, ruy::Context*, ruy::Matrix<int>*)':\r\nconv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0xbe): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\nconv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0xc9): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\nconv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x27e): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nconv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x30b): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\nconv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x349): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nconv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x351): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nconv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x39e): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nconv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x4ee): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<signed char, signed char, int, signed char, (tflite::cpu_backend_gemm::QuantizationFlavor)2>::Run(tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char const*, tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char const*, tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char*, tflite::cpu_backend_gemm::GemmParams<int, signed char, (tflite::cpu_backend_gemm::QuantizationFlavor)2> const&, tflite::CpuBackendContext*)':\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x1cc): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x1d7): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x426): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x4d8): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x5d1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x5d9): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x628): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x6b9): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x747): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `tflite::optimized_ops::Conv(tflite::ConvParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::RuntimeShape const&, float*, tflite::CpuBackendContext*)':\r\nconv.cc:(.text._ZN6tflite13optimized_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_PNS_17CpuBackendContextE[_ZN6tflite13optimized_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_PNS_17CpuBackendContextE]+0x2db): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `void ruy::detail::CreateTrMulParamsAssumingColMajorDst<(ruy::Path)113, unsigned char, unsigned char, int, unsigned char>(ruy::Mat<unsigned char> const&, ruy::Mat<unsigned char> const&, ruy::Mat<unsigned char> const&, ruy::MulParams<int, unsigned char> const&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\nconv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x107): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nconv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nconv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d9): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nconv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x22a): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nconv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x2a6): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nconv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x53b): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<unsigned char, unsigned char, int, unsigned char, (tflite::cpu_backend_gemm::QuantizationFlavor)1>::Run(tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char*, tflite::cpu_backend_gemm::GemmParams<int, unsigned char, (tflite::cpu_backend_gemm::QuantizationFlavor)1> const&, tflite::CpuBackendContext*)':\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x1c2): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x1cd): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x40f): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x4c1): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x5b9): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x5c1): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x610): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x6a1): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nconv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x72f): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, signed char, signed char, int, unsigned char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nconv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0xfa): undefined reference to `ruy::Kernel8bitAvx2(ruy::KernelParams8bit<8, 8> const&)'\r\nconv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x121): undefined reference to `ruy::Kernel8bitAvx2SingleCol(ruy::KernelParams8bit<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, unsigned char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nconv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0xfa): undefined reference to `ruy::Kernel8bitAvx(ruy::KernelParams8bit<8, 8> const&)'\r\nconv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x121): undefined reference to `ruy::Kernel8bitAvxSingleCol(ruy::KernelParams8bit<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `tflite::ops::builtin::fully_connected::EvalHybrid(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':\r\nfully_connected.cc:(.text+0x31a1): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `ruy::Kernel<(ruy::Path)64, signed char, signed char, int, short>::Run(ruy::PMat<signed char> const&, ruy::PMat<signed char> const&, ruy::MulParams<int, short> const&, int, int, int, int, ruy::Mat<short>*) const [clone .isra.958]':\r\nfully_connected.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaisE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIisEEiiiiPNS_3MatIsEE.isra.958[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e6): undefined reference to `ruy::Kernel8bitAvx512(ruy::KernelParams8bit<16, 16> const&)'\r\nfully_connected.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaisE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIisEEiiiiPNS_3MatIsEE.isra.958[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3c1): undefined reference to `ruy::Kernel8bitAvx512SingleCol(ruy::KernelParams8bit<16, 16> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)1, unsigned char, unsigned char, int, short> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nfully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EhhisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EhhisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3c3): undefined reference to `ruy::detail::MultiplyByQuantizedMultiplier(int, int, int)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `gemmlowp::WorkersPool::CreateWorkers(unsigned long)':\r\nfully_connected.cc:(.text._ZN8gemmlowp11WorkersPool13CreateWorkersEm[_ZN8gemmlowp11WorkersPool13CreateWorkersEm]+0x12d): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `tflite::optimized_ops::FullyConnectedSparseWeight1x4(TfLiteSparsity const&, tflite::FullyConnectedParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::CpuBackendContext*)':\r\nfully_connected.cc:(.text._ZN6tflite13optimized_ops29FullyConnectedSparseWeight1x4ERK14TfLiteSparsityRKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKfS9_SB_S9_SB_S9_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops29FullyConnectedSparseWeight1x4ERK14TfLiteSparsityRKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKfS9_SB_S9_SB_S9_PfPNS_17CpuBackendContextE]+0x8d7): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<signed char, signed char, int, signed char, (tflite::cpu_backend_gemm::QuantizationFlavor)1>::Run(tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char const*, tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char const*, tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char*, tflite::cpu_backend_gemm::GemmParams<int, signed char, (tflite::cpu_backend_gemm::QuantizationFlavor)1> const&, tflite::CpuBackendContext*)':\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x1cd): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x1d8): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x427): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x4d9): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x5d1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x5d9): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x628): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x6b9): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x747): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, signed char, signed char, int, short> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nfully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0xfb): undefined reference to `ruy::Kernel8bitAvx2(ruy::KernelParams8bit<8, 8> const&)'\r\nfully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x121): undefined reference to `ruy::Kernel8bitAvx2SingleCol(ruy::KernelParams8bit<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, short> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\nfully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0xfb): undefined reference to `ruy::Kernel8bitAvx(ruy::KernelParams8bit<8, 8> const&)'\r\nfully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x121): undefined reference to `ruy::Kernel8bitAvxSingleCol(ruy::KernelParams8bit<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `void ruy::detail::CreateTrMulParamsAssumingColMajorDst<(ruy::Path)113, unsigned char, unsigned char, int, short>(ruy::Mat<unsigned char> const&, ruy::Mat<unsigned char> const&, ruy::Mat<short> const&, ruy::MulParams<int, short> const&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\nfully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x10a): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nfully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nfully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d9): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nfully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x22a): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nfully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x2a6): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nfully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x4b3): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<unsigned char, unsigned char, int, short, (tflite::cpu_backend_gemm::QuantizationFlavor)1>::Run(tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<short> const&, short*, tflite::cpu_backend_gemm::GemmParams<int, short, (tflite::cpu_backend_gemm::QuantizationFlavor)1> const&, tflite::CpuBackendContext*)':\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x1d0): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x1db): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x41a): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x4cc): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x5c1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x5c9): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x618): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x6a9): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\nfully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x737): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(interpreter.cc.o): In function `tflite::Interpreter::Invoke()':\r\ninterpreter.cc:(.text+0x3c3): undefined reference to `ruy::ScopedSuppressDenormals::ScopedSuppressDenormals()'\r\ninterpreter.cc:(.text+0x48c): undefined reference to `ruy::ScopedSuppressDenormals::~ScopedSuppressDenormals()'\r\ninterpreter.cc:(.text+0x71f): undefined reference to `ruy::ScopedSuppressDenormals::~ScopedSuppressDenormals()'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(interpreter_builder.cc.o): In function `tflite::AcquireFlexDelegate()':\r\ninterpreter_builder.cc:(.text+0x292): undefined reference to `dlsym'\r\ninterpreter_builder.cc:(.text+0x2cd): undefined reference to `dlopen'\r\ninterpreter_builder.cc:(.text+0x2e1): undefined reference to `dlsym'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(rfft2d.cc.o): In function `tflite::ops::builtin::rfft2d::Rfft2dImpl(int, int, double**, int*, double*)':\r\nrfft2d.cc:(.text+0x7f0): undefined reference to `rdft2d'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)64, ruy::FixedKernelLayout<(ruy::Order)1, 1, 16>, float, float>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi16EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi16EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x260): undefined reference to `ruy::PackFloatColMajorForAvx512(float const*, float const*, int, int, int, float*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)32, ruy::FixedKernelLayout<(ruy::Order)1, 1, 8>, float, float>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi8EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi8EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x228): undefined reference to `ruy::PackFloatColMajorForAvx2(float const*, float const*, int, int, int, float*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::Kernel<(ruy::Path)64, float, float, float, float>::Run(ruy::PMat<float> const&, ruy::PMat<float> const&, ruy::MulParams<float, float> const&, int, int, int, int, ruy::Mat<float>*) const [clone .isra.87]':\r\ntranspose_conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EffffE3RunERKNS_4PMatIfEES6_RKNS_9MulParamsIffEEiiiiPNS_3MatIfEE.isra.87[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x14d): undefined reference to `ruy::KernelFloatAvx512(ruy::KernelParamsFloat<16, 16> const&)'\r\ntranspose_conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EffffE3RunERKNS_4PMatIfEES6_RKNS_9MulParamsIffEEiiiiPNS_3MatIfEE.isra.87[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x189): undefined reference to `ruy::KernelFloatAvx512SingleCol(ruy::KernelParamsFloat<16, 16> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, float, float, float, float> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\ntranspose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x136): undefined reference to `ruy::KernelFloatAvx2(ruy::KernelParamsFloat<8, 8> const&)'\r\ntranspose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x171): undefined reference to `ruy::KernelFloatAvx2SingleCol(ruy::KernelParamsFloat<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)64, ruy::FixedKernelLayout<(ruy::Order)0, 4, 16>, unsigned char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x113): undefined reference to `ruy::Pack8bitRowMajorForAvx512(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x225): undefined reference to `ruy::Pack8bitColMajorForAvx512(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2c4): undefined reference to `ruy::Pack8bitColMajorForAvx512(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)64, ruy::FixedKernelLayout<(ruy::Order)0, 4, 16>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x11f): undefined reference to `ruy::Pack8bitRowMajorForAvx512(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x232): undefined reference to `ruy::Pack8bitColMajorForAvx512(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2d0): undefined reference to `ruy::Pack8bitColMajorForAvx512(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)32, ruy::FixedKernelLayout<(ruy::Order)0, 4, 8>, unsigned char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x113): undefined reference to `ruy::Pack8bitRowMajorForAvx2(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x225): undefined reference to `ruy::Pack8bitColMajorForAvx2(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2c4): undefined reference to `ruy::Pack8bitColMajorForAvx2(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)32, ruy::FixedKernelLayout<(ruy::Order)0, 4, 8>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x11f): undefined reference to `ruy::Pack8bitRowMajorForAvx2(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x232): undefined reference to `ruy::Pack8bitColMajorForAvx2(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2d0): undefined reference to `ruy::Pack8bitColMajorForAvx2(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)0, 4, 8>, unsigned char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x113): undefined reference to `ruy::Pack8bitRowMajorForAvx(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x225): undefined reference to `ruy::Pack8bitColMajorForAvx(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2c4): undefined reference to `ruy::Pack8bitColMajorForAvx(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)0, 4, 8>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x11f): undefined reference to `ruy::Pack8bitRowMajorForAvx(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x232): undefined reference to `ruy::Pack8bitColMajorForAvx(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2d0): undefined reference to `ruy::Pack8bitColMajorForAvx(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, signed char, signed char, int, int> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\ntranspose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x22d): undefined reference to `ruy::Kernel8bitAvx2(ruy::KernelParams8bit<8, 8> const&)'\r\ntranspose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x281): undefined reference to `ruy::Kernel8bitAvx2SingleCol(ruy::KernelParams8bit<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, float, float, float, float> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\ntranspose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x136): undefined reference to `ruy::KernelFloatAvx(ruy::KernelParamsFloat<8, 8> const&)'\r\ntranspose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x171): undefined reference to `ruy::KernelFloatAvxSingleCol(ruy::KernelParamsFloat<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::Kernel<(ruy::Path)64, signed char, signed char, int, int>::Run(ruy::PMat<signed char> const&, ruy::PMat<signed char> const&, ruy::MulParams<int, int> const&, int, int, int, int, ruy::Mat<int>*) const [clone .isra.651]':\r\ntranspose_conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaiiE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIiiEEiiiiPNS_3MatIiEE.isra.651[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x22c): undefined reference to `ruy::Kernel8bitAvx512(ruy::KernelParams8bit<16, 16> const&)'\r\ntranspose_conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaiiE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIiiEEiiiiPNS_3MatIiEE.isra.651[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x271): undefined reference to `ruy::Kernel8bitAvx512SingleCol(ruy::KernelParams8bit<16, 16> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)1, 1, 8>, float, float>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':\r\ntranspose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi8EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi8EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x270): undefined reference to `ruy::PackFloatColMajorForAvx(float const*, float const*, int, int, int, float*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, int> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':\r\ntranspose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x22d): undefined reference to `ruy::Kernel8bitAvx(ruy::KernelParams8bit<8, 8> const&)'\r\ntranspose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x281): undefined reference to `ruy::Kernel8bitAvxSingleCol(ruy::KernelParams8bit<8, 8> const&)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::MulFrontEnd<(ruy::Path)113, float, float, float, float>(ruy::Mat<float> const&, ruy::Mat<float> const&, ruy::MulParams<float, float> const&, ruy::Ctx*, ruy::Mat<float>*)':\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x37): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x223): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2b0): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2e9): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2f1): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x33b): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x459): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void tflite::cpu_backend_gemm::Gemm<float, float, float, float, (tflite::cpu_backend_gemm::QuantizationFlavor)0>(tflite::cpu_backend_gemm::MatrixParams<float> const&, float const*, tflite::cpu_backend_gemm::MatrixParams<float> const&, float const*, tflite::cpu_backend_gemm::MatrixParams<float> const&, float*, tflite::cpu_backend_gemm::GemmParams<float, float, (tflite::cpu_backend_gemm::QuantizationFlavor)0> const&, tflite::CpuBackendContext*)':\r\ntranspose_conv.cc:(.text._ZN6tflite16cpu_backend_gemm4GemmIffffLNS0_18QuantizationFlavorE0EEEvRKNS0_12MatrixParamsIT_EEPKS4_RKNS3_IT0_EEPKSA_RKNS3_IT2_EEPSG_RKNS0_10GemmParamsIT1_SG_XT3_EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm4GemmIffffLNS0_18QuantizationFlavorE0EEEvRKNS0_12MatrixParamsIT_EEPKS4_RKNS3_IT0_EEPKSA_RKNS3_IT2_EEPSG_RKNS0_10GemmParamsIT1_SG_XT3_EEEPNS_17CpuBackendContextE]+0x257): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::MulFrontEnd<(ruy::Path)113, signed char, signed char, int, int>(ruy::Mat<signed char> const&, ruy::Mat<signed char> const&, ruy::MulParams<int, int> const&, ruy::Ctx*, ruy::Mat<int>*)':\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x37): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x220): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2a5): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2e1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2e9): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x333): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x453): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void tflite::ops::builtin::transpose_conv::EvalQuantizedPerChannel<(tflite::ops::builtin::transpose_conv::KernelType)1>(TfLiteContext*, TfLiteTransposeConvParams const*, tflite::ops::builtin::transpose_conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':\r\ntranspose_conv.cc:(.text._ZN6tflite3ops7builtin14transpose_conv23EvalQuantizedPerChannelILNS2_10KernelTypeE1EEEvP13TfLiteContextPK25TfLiteTransposeConvParamsPNS2_6OpDataEPK12TfLiteTensorSE_SE_SE_PSC_SF_SF_[_ZN6tflite3ops7builtin14transpose_conv23EvalQuantizedPerChannelILNS2_10KernelTypeE1EEEvP13TfLiteContextPK25TfLiteTransposeConvParamsPNS2_6OpDataEPK12TfLiteTensorSE_SE_SE_PSC_SF_SF_]+0x615): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::MulFrontEnd<(ruy::Path)113, unsigned char, unsigned char, int, int>(ruy::Mat<unsigned char> const&, ruy::Mat<unsigned char> const&, ruy::MulParams<int, int> const&, ruy::Ctx*, ruy::Mat<int>*)':\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x37): undefined reference to `ruy::Ctx::clear_performance_advisories()'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x20a): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x28f): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2c9): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2d1): undefined reference to `ruy::Ctx::GetMainAllocator()'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x31b): undefined reference to `ruy::Allocator::AllocateBytes(long)'\r\ntranspose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x41f): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void tflite::ops::builtin::transpose_conv::EvalQuantized<(tflite::ops::builtin::transpose_conv::KernelType)1>(TfLiteContext*, TfLiteTransposeConvParams const*, tflite::ops::builtin::transpose_conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':\r\ntranspose_conv.cc:(.text._ZN6tflite3ops7builtin14transpose_conv13EvalQuantizedILNS2_10KernelTypeE1EEEvP13TfLiteContextPK25TfLiteTransposeConvParamsPNS2_6OpDataEPK12TfLiteTensorSE_SE_SE_PSC_SF_SF_[_ZN6tflite3ops7builtin14transpose_conv13EvalQuantizedILNS2_10KernelTypeE1EEEvP13TfLiteContextPK25TfLiteTransposeConvParamsPNS2_6OpDataEPK12TfLiteTensorSE_SE_SE_PSC_SF_SF_]+0x60b): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::SubgraphInvoke(TfLiteContext*, TfLiteNode*)':\r\nxnnpack_delegate.cc:(.text+0x762): undefined reference to `xnn_setup_runtime'\r\nxnnpack_delegate.cc:(.text+0x7c5): undefined reference to `xnn_invoke_runtime'\r\nxnnpack_delegate.cc:(.text+0x85c): undefined reference to `xnn_setup_runtime'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitReshapeNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteReshapeParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&) [clone .isra.102]':\r\nxnnpack_delegate.cc:(.text+0xe75): undefined reference to `xnn_define_static_reshape'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitReluNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, float, float, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x1151): undefined reference to `xnn_define_clamp'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMaxPool2DNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLitePoolParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x142b): undefined reference to `xnn_define_max_pooling_2d'\r\nxnnpack_delegate.cc:(.text+0x157c): undefined reference to `xnn_define_clamp'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMeanNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteReducerParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x175e): undefined reference to `xnn_define_global_average_pooling_2d'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitResizeBilinearNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteResizeBilinearParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x1cb6): undefined reference to `xnn_define_static_resize_bilinear_2d'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitPadNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x21c8): undefined reference to `xnn_define_static_constant_pad'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMediaPipeMaxPoolingNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLitePoolParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x24c8): undefined reference to `xnn_define_argmax_pooling_2d'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitDepthwiseConv2DNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteDepthwiseConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x2b75): undefined reference to `xnn_define_depthwise_convolution_2d'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitFullyConnectedNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteFullyConnectedParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x3442): undefined reference to `xnn_define_fully_connected'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMediaPipeDeconvolutionNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteTransposeConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x391b): undefined reference to `xnn_define_deconvolution_2d'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitPreluNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x3b39): undefined reference to `xnn_define_prelu'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMediaPipeUnpoolingNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLitePoolParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x3fbe): undefined reference to `xnn_define_unpooling_2d'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitConv2DNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x4848): undefined reference to `xnn_define_convolution_2d'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitNode(xnn_subgraph*, TfLiteContext*, TfLiteRegistration*, TfLiteNode*, int, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':\r\nxnnpack_delegate.cc:(.text+0x4a20): undefined reference to `xnn_define_bankers_rounding'\r\nxnnpack_delegate.cc:(.text+0x4b40): undefined reference to `xnn_define_elu'\r\nxnnpack_delegate.cc:(.text+0x4c58): undefined reference to `xnn_define_ceiling'\r\nxnnpack_delegate.cc:(.text+0x4e15): undefined reference to `xnn_define_minimum2'\r\nxnnpack_delegate.cc:(.text+0x4f8d): undefined reference to `xnn_define_maximum2'\r\nxnnpack_delegate.cc:(.text+0x50a0): undefined reference to `xnn_define_sigmoid'\r\nxnnpack_delegate.cc:(.text+0x5203): undefined reference to `xnn_define_leaky_relu'\r\nxnnpack_delegate.cc:(.text+0x5406): undefined reference to `xnn_define_subtract'\r\nxnnpack_delegate.cc:(.text+0x5689): undefined reference to `xnn_define_average_pooling_2d'\r\nxnnpack_delegate.cc:(.text+0x586a): undefined reference to `xnn_define_add2'\r\nxnnpack_delegate.cc:(.text+0x5980): undefined reference to `xnn_define_floor'\r\nxnnpack_delegate.cc:(.text+0x5ab5): undefined reference to `xnn_define_depth_to_space'\r\nxnnpack_delegate.cc:(.text+0x5c18): undefined reference to `xnn_define_abs'\r\nxnnpack_delegate.cc:(.text+0x5d8d): undefined reference to `xnn_define_squared_difference'\r\nxnnpack_delegate.cc:(.text+0x5f5e): undefined reference to `xnn_define_multiply2'\r\nxnnpack_delegate.cc:(.text+0x6186): undefined reference to `xnn_define_divide'\r\nxnnpack_delegate.cc:(.text+0x6318): undefined reference to `xnn_define_square_root'\r\nxnnpack_delegate.cc:(.text+0x6428): undefined reference to `xnn_define_negate'\r\nxnnpack_delegate.cc:(.text+0x65a4): undefined reference to `xnn_define_square'\r\nxnnpack_delegate.cc:(.text+0x66cf): undefined reference to `xnn_define_hardswish'\r\nxnnpack_delegate.cc:(.text+0x67e8): undefined reference to `xnn_define_softmax'\r\nxnnpack_delegate.cc:(.text+0x6ddc): undefined reference to `xnn_define_clamp'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `TfLiteXNNPackDelegateCreate':\r\nxnnpack_delegate.cc:(.text+0x7000): undefined reference to `xnn_initialize'\r\nxnnpack_delegate.cc:(.text+0x709a): undefined reference to `pthreadpool_destroy'\r\nxnnpack_delegate.cc:(.text+0x7189): undefined reference to `pthreadpool_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::Create(TfLiteContext*, TfLiteDelegateParams const*, tflite::xnnpack::(anonymous namespace)::Delegate const*)':\r\nxnnpack_delegate.cc:(.text+0x8c23): undefined reference to `xnn_create_subgraph'\r\nxnnpack_delegate.cc:(.text+0x9493): undefined reference to `xnn_delete_subgraph'\r\nxnnpack_delegate.cc:(.text+0x978d): undefined reference to `xnn_define_quantized_tensor_value'\r\nxnnpack_delegate.cc:(.text+0x9974): undefined reference to `xnn_define_tensor_value'\r\nxnnpack_delegate.cc:(.text+0x9dad): undefined reference to `xnn_delete_subgraph'\r\nxnnpack_delegate.cc:(.text+0x9edf): undefined reference to `xnn_create_runtime_v2'\r\nxnnpack_delegate.cc:(.text+0x9f24): undefined reference to `xnn_delete_runtime'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(audio_spectrogram.cc.o): In function `tflite::ops::custom::audio_spectrogram::Init(TfLiteContext*, char const*, unsigned long)':\r\naudio_spectrogram.cc:(.text+0x1011): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\naudio_spectrogram.cc:(.text+0x13ab): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\naudio_spectrogram.cc:(.text+0x15dc): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv3d.cc.o): In function `tflite::ops::builtin::conv3d::EvalFloat(tflite::ops::builtin::conv3d::KernelType, TfLiteContext*, TfLiteNode*, TfLiteConv3DParams*, tflite::ops::builtin::conv3d::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':\r\nconv3d.cc:(.text+0x1405): undefined reference to `ruy::get_ctx(ruy::Context*)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(cpu_backend_context.cc.o): In function `tflite::CpuBackendContext::SetMaxNumThreads(int)':\r\ncpu_backend_context.cc:(.text+0x1f): undefined reference to `ruy::Context::set_max_num_threads(int)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(cpu_backend_context.cc.o): In function `tflite::CpuBackendContext::~CpuBackendContext()':\r\ncpu_backend_context.cc:(.text+0xe2): undefined reference to `pthread_join'\r\ncpu_backend_context.cc:(.text+0x158): undefined reference to `ruy::Context::~Context()'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(cpu_backend_context.cc.o): In function `tflite::CpuBackendContext::CpuBackendContext()':\r\ncpu_backend_context.cc:(.text+0x250): undefined reference to `ruy::Context::Context()'\r\ncpu_backend_context.cc:(.text+0x3cb): undefined reference to `pthread_join'\r\ncpu_backend_context.cc:(.text+0x446): undefined reference to `ruy::Context::~Context()'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(cpu_backend_context.cc.o): In function `tflite::CpuBackendContext::ClearCaches()':\r\ncpu_backend_context.cc:(.text._ZN6tflite17CpuBackendContext11ClearCachesEv[_ZN6tflite17CpuBackendContext11ClearCachesEv]+0x5): undefined reference to `ruy::Context::ClearPrepackedCache()'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(depthwise_conv.cc.o): In function `tflite::optimized_integer_ops::DepthwiseConvHybridPerChannel(tflite::DepthwiseParams const&, float const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, float const*, int*, tflite::CpuBackendContext*)':\r\ndepthwise_conv.cc:(.text._ZN6tflite21optimized_integer_ops29DepthwiseConvHybridPerChannelERKNS_15DepthwiseParamsEPKfRKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_PfS5_PiPNS_17CpuBackendContextE[_ZN6tflite21optimized_integer_ops29DepthwiseConvHybridPerChannelERKNS_15DepthwiseParamsEPKfRKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_PfS5_PiPNS_17CpuBackendContextE]+0x7f2): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(depthwise_conv.cc.o): In function `tflite::optimized_integer_ops::DepthwiseConvPerChannel(tflite::DepthwiseParams const&, int const*, int const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, signed char*, tflite::CpuBackendContext*)':\r\ndepthwise_conv.cc:(.text._ZN6tflite21optimized_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKiS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_PaPNS_17CpuBackendContextE[_ZN6tflite21optimized_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKiS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_PaPNS_17CpuBackendContextE]+0x7e2): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(depthwise_conv.cc.o): In function `void tflite::optimized_ops::DepthwiseConv<float, float>(tflite::DepthwiseParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::CpuBackendContext*)':\r\ndepthwise_conv.cc:(.text._ZN6tflite13optimized_ops13DepthwiseConvIffEEvRKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKT_S7_SA_S7_PKT0_S7_PS8_PNS_17CpuBackendContextE[_ZN6tflite13optimized_ops13DepthwiseConvIffEEvRKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKT_S7_SA_S7_PKT0_S7_PS8_PNS_17CpuBackendContextE]+0x984): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(depthwise_conv.cc.o): In function `void tflite::optimized_ops::DepthwiseConv<unsigned char, int>(tflite::DepthwiseParams const&, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, unsigned char*, tflite::CpuBackendContext*)':\r\ndepthwise_conv.cc:(.text._ZN6tflite13optimized_ops13DepthwiseConvIhiEEvRKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKT_S7_SA_S7_PKT0_S7_PS8_PNS_17CpuBackendContextE[_ZN6tflite13optimized_ops13DepthwiseConvIhiEEvRKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKT_S7_SA_S7_PKT0_S7_PS8_PNS_17CpuBackendContextE]+0x964): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(detection_postprocess.cc.o): In function `tflite::ops::custom::detection_postprocess::Init(TfLiteContext*, char const*, unsigned long)':\r\ndetection_postprocess.cc:(.text+0x313c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\ndetection_postprocess.cc:(.text+0x3191): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\ndetection_postprocess.cc:(.text+0x31f3): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\ndetection_postprocess.cc:(.text+0x3264): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\ndetection_postprocess.cc:(.text+0x3863): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(detection_postprocess.cc.o):detection_postprocess.cc:(.text._ZNK11flexbuffers9Reference7AsInt64Ev[_ZNK11flexbuffers9Reference7AsInt64Ev]+0xce): more undefined references to `flatbuffers::ClassicLocale::instance_' follow\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(eigen_support.cc.o): In function `tflite::eigen_support::GetThreadPoolDevice(TfLiteContext*)':\r\neigen_support.cc:(.text+0x1e84): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(lsh_projection.cc.o): In function `tflite::ops::builtin::lsh_projection::RunningSignBit(TfLiteTensor const*, TfLiteTensor const*, float)':\r\nlsh_projection.cc:(.text+0x2e1): undefined reference to `util::Fingerprint64(char const*, unsigned long)'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(mfcc.cc.o): In function `tflite::ops::custom::mfcc::Init(TfLiteContext*, char const*, unsigned long)':\r\nmfcc.cc:(.text+0x13a0): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nmfcc.cc:(.text+0x1414): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nmfcc.cc:(.text+0x148a): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nmfcc.cc:(.text+0x195b): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(mirror_pad.cc.o): In function `tflite::ops::builtin::mirror_pad::Eval(TfLiteContext*, TfLiteNode*)':\r\nmirror_pad.cc:(.text+0x1e24): undefined reference to `pthread_create'\r\nmirror_pad.cc:(.text+0x1fc2): undefined reference to `pthread_create'\r\nmirror_pad.cc:(.text+0x24b4): undefined reference to `pthread_create'\r\nmirror_pad.cc:(.text+0x2654): undefined reference to `pthread_create'\r\nmirror_pad.cc:(.text+0x29ac): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(numeric_verify.cc.o): In function `tflite::ops::custom::numeric_verify::Init(TfLiteContext*, char const*, unsigned long)':\r\nnumeric_verify.cc:(.text+0xc91): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nnumeric_verify.cc:(.text+0x10a7): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(reduce.cc.o): In function `tflite::optimized_ops::Mean(tflite::MeanParams const&, tflite::RuntimeShape const&, unsigned char const*, int, float, tflite::RuntimeShape const&, unsigned char*, int, float, tflite::CpuBackendContext*)':\r\nreduce.cc:(.text._ZN6tflite13optimized_ops4MeanERKNS_10MeanParamsERKNS_12RuntimeShapeEPKhifS6_PhifPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops4MeanERKNS_10MeanParamsERKNS_12RuntimeShapeEPKhifS6_PhifPNS_17CpuBackendContextE]+0xef6): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(reduce.cc.o): In function `tflite::optimized_integer_ops::Mean(tflite::MeanParams const&, tflite::RuntimeShape const&, signed char const*, int, float, tflite::RuntimeShape const&, signed char*, int, float, tflite::CpuBackendContext*)':\r\nreduce.cc:(.text._ZN6tflite21optimized_integer_ops4MeanERKNS_10MeanParamsERKNS_12RuntimeShapeEPKaifS6_PaifPNS_17CpuBackendContextE[_ZN6tflite21optimized_integer_ops4MeanERKNS_10MeanParamsERKNS_12RuntimeShapeEPKaifS6_PaifPNS_17CpuBackendContextE]+0xf26): undefined reference to `pthread_create'\r\n/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(spectrogram.cc.o): In function `tflite::internal::Spectrogram::ProcessCoreFFT()':\r\nspectrogram.cc:(.text+0xed): undefined reference to `rdft'\r\ncollect2: error: ld returned 1 exit status\r\nCMakeFiles/example.dir/build.make:96: recipe for target 'bin/example' failed\r\nmake[2]: *** [bin/example] Error 1\r\nCMakeFiles/Makefile2:82: recipe for target 'CMakeFiles/example.dir/all' failed\r\nmake[1]: *** [CMakeFiles/example.dir/all] Error 2\r\nMakefile:90: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n`\r\n", "comments": ["@terryheo Could you take a look?", "Could you share your CMakeLists.txt ?\r\n\r\nFYI, I saved your example code as hello_tfl.c and it linked well.\r\n```\r\ngcc ../hello_tfl.c -I../ libtensorflowlite_c.so\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50149\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50149\">No</a>\n", "@terryheo I could fix the issue by modifying the conan recipe I did to pack tflite into a conan package, I was not exporting correctly the C API so it could not be found. Thanks for the support.", "just consider https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal", "> @terryheo I could fix the issue by modifying the conan recipe I did to pack tflite into a conan package, I was not exporting correctly the C API so it could not be found. Thanks for the support.\r\n\r\nHow did you solve it? I met the same question when compiling an app on ARM."]}, {"number": 50148, "title": "How to qat model with BN", "body": " 1. System information\r\n\r\n- Linux Ubuntu 16.04):\r\n- TensorFlow-gpu2.2.0\r\n\r\n\r\nError:\r\n Layer batch_normalization:<class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'> is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.\r\n", "comments": ["@caijinana Could you Please try with latest TF version 2.5 and If possible, Please provide the simple standalone code to reproduce the issue.Thanks!", "@saikumarchalla \r\ni don't know how to deal with net with BN\r\nthis is my code \r\n```\r\nimport tempfile\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Dense, Input, Flatten, BatchNormalization\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Reshape, ZeroPadding2D\r\nfrom tensorflow.keras.datasets import mnist\r\nimport tensorflow_model_optimization as tfmot\r\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantize_config\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nimport argparse\r\nimport numpy as np\r\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\r\nX_train = (np.expand_dims(X_train,3))\r\nX_test = (np.expand_dims(X_test,3))\r\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\r\ntrain_datagen =ImageDataGenerator(\r\n    rescale=1./255,\r\n    featurewise_center=True,#mean=0\r\n    featurewise_std_normalization=True,#/std\r\n    rotation_range=20,\r\n    # preprocessing_function=my_random_crop\r\n)\r\n\r\ntest_datagen=ImageDataGenerator(\r\n    rescale=1./255,\r\n    featurewise_center=True,\r\n    featurewise_std_normalization=True,\r\n)\r\ntrain_datagen.fit(X_train)\r\ntest_datagen.fit(X_test)\r\n# parser = argparse.ArgumentParser()\r\n# parser.add_argument(\r\n#      '-trtm', '--trtmodel', type=str, default='../F_TorrentNet.trt')\r\n# parser.add_argument(\r\n#         '-m', '--model', type=str, default='F_TorrentNet.pth')\r\n# args = parser.parse_args()\r\nclass NoOpQuantizeConfig(quantize_config.QuantizeConfig):\r\n    \"\"\"QuantizeConfig which does not quantize any part of the layer.\"\"\"\r\n\r\n    def get_weights_and_quantizers(self, layer):\r\n        return []\r\n\r\n    def get_activations_and_quantizers(self, layer):\r\n        return []\r\n\r\n    def set_quantize_weights(self, layer, quantize_weights):\r\n        pass\r\n\r\n    def set_quantize_activations(self, layer, quantize_activations):\r\n        pass\r\n\r\n    def get_output_quantizers(self, layer):\r\n        return []\r\n\r\n    def get_config(self):\r\n        return {}\r\nquantize_config = NoOpQuantizeConfig()\r\ndef LeNet():\r\n    inputs = Input(shape=(28, 28,1))\r\n    # x = Reshape((28, 28, 1), input_shape=(28, 28))(inputs)\r\n    x = ZeroPadding2D(padding=(2, 2))(inputs)\r\n\r\n    h = Conv2D(6, 5, strides=(1, 1), activation='relu')(x)\r\n    # h_nor = BatchNormalization()(h)\r\n    h_nor = tfmot.quantization.keras.quantize_annotate_layer(\r\n      BatchNormalization(), quantize_config=quantize_config)(h)\r\n    h_max = MaxPooling2D(pool_size=(2, 2))(h_nor)\r\n\r\n    h = Conv2D(16, 5, strides=(1, 1), activation='relu')(h_max)\r\n    # h_nor = BatchNormalization()(h)\r\n    h_nor = tfmot.quantization.keras.quantize_annotate_layer(\r\n        BatchNormalization(), quantize_config=quantize_config)(h)\r\n    h_max = MaxPooling2D(pool_size=(2, 2))(h_nor)\r\n\r\n    h = Flatten()(h_max)\r\n\r\n    d = Dense(120, activation='relu')(h)\r\n    # d_nor = BatchNormalization()(d)\r\n    d_nor=tfmot.quantization.keras.quantize_annotate_layer(\r\n     BatchNormalization(), quantize_config=quantize_config)(d)\r\n    d = Dense(84, activation='relu')(d_nor)\r\n    # d_nor = BatchNormalization()(d)\r\n    d_nor = tfmot.quantization.keras.quantize_annotate_layer(\r\n        BatchNormalization(), quantize_config=quantize_config)(d)\r\n\r\n\r\n    y = Dense(10, activation='softmax')(d_nor)\r\n\r\n    model = Model(inputs=inputs, outputs = y)\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n                  loss='categorical_crossentropy', metrics=['accuracy'])\r\n    return model\r\ndef XMLP():\r\n    inputs = Input(shape=(28, 28,1))\r\n    h = Flatten()(inputs)\r\n\r\n    # h = Dense(512, activation='relu',\r\n    #           kernel_initializer=tf.keras.initializers.random_normal(mean=0, stddev=0.01))(h)\r\n    # h_nor = BatchNormalization()(h)\r\n    h=Dense(512,activation='linear')(h)\r\n    # h_nor=BatchNormalization()(h)\r\n    h_nor=tfmot.quantization.keras.quantize_annotate_layer(\r\n    BatchNormalization(), quantize_config=quantize_config)(h)\r\n    h =Dense(512,activation='relu')(h_nor)\r\n\r\n    # h = Dense(512, activation='relu',\r\n    #           kernel_initializer=tf.keras.initializers.random_normal(mean=0, stddev=0.01))(h_nor)\r\n    # h_nor = BatchNormalization()(h)\r\n    h = Dense(512, activation='linear')(h)\r\n    # h_nor = BatchNormalization()(h)\r\n    h_nor = tfmot.quantization.keras.quantize_annotate_layer(\r\n        BatchNormalization(), quantize_config=quantize_config)(h)\r\n    h = Dense(512, activation='relu')(h_nor)\r\n\r\n    # h = Dense(512, activation='relu',\r\n    #           kernel_initializer=tf.keras.initializers.random_normal(mean=0, stddev=0.01))(h_nor)\r\n    # h_nor = BatchNormalization()(h)\r\n    h = Dense(512, activation='linear')(h)\r\n    # h_nor = BatchNormalization()(h)\r\n    h_nor = tfmot.quantization.keras.quantize_annotate_layer(\r\n        BatchNormalization(), quantize_config=quantize_config)(h)\r\n    h = Dense(512, activation='relu')(h_nor)\r\n\r\n    # h = Dense(256, activation='relu',\r\n    #           kernel_initializer=tf.keras.initializers.random_normal(mean=0, stddev=0.01))(h_nor)\r\n    # h_nor = BatchNormalization()(h)\r\n    h = Dense(256, activation='linear')(h)\r\n    # h_nor = BatchNormalization()(h)\r\n    h_nor = tfmot.quantization.keras.quantize_annotate_layer(\r\n        BatchNormalization(), quantize_config=quantize_config)(h)\r\n    h = Dense(256, activation='relu')(h_nor)\r\n    y = Dense(10, activation='softmax')(h)\r\n\r\n    model = Model(inputs=inputs, outputs=y)\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n                  loss='categorical_crossentropy', metrics=['accuracy'])\r\n    return model\r\n\r\nmodel=LeNet()\r\nmodel.fit_generator(train_datagen.flow(X_train, y_train, batch_size=50, shuffle=True),\r\n                    steps_per_epoch=len(X_train) / 50, epochs=40)\r\ntf.saved_model.save(model, 'mlp')\r\nmodel.summary()\r\nquantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\r\nquantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\r\nquantize_scope = tfmot.quantization.keras.quantize_scope\r\nwith tf.keras.utils.custom_object_scope({'NoOpQuantizeConfig': NoOpQuantizeConfig}):\r\n    q_aware_model = tfmot.quantization.keras.quantize_model(model)\r\n    q_aware_model.summary()\r\n# `quantize_model` requires a recompile.\r\nprint(\"qat_____\")\r\nq_aware_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n                  loss='categorical_crossentropy', metrics=['accuracy'])\r\n# q_aware_model.fit(X_train, y_train, batch_size=50, epochs=40)\r\nq_aware_model.fit_generator(train_datagen.flow(X_train, y_train, batch_size=50, shuffle=True),\r\n                    steps_per_epoch=len(X_train) / 50, epochs=40)\r\n# q_aware_model.save('lenet-qat.h5')\r\ntf.saved_model.save(q_aware_model, 'mlp-qat')\r\nq_aware_model.summary()\r\n_, baseline_model_accuracy = model.evaluate(test_datagen.flow(X_test,y_test,batch_size=50,shuffle=False),\r\n verbose=1)\r\n\r\n_, q_aware_model_accuracy = q_aware_model.evaluate(test_datagen.flow(X_test,y_test,batch_size=50,shuffle=False),\r\n   verbose=1)\r\n\r\nprint('Baseline test accuracy:', baseline_model_accuracy)\r\nprint('Quant test accuracy:', q_aware_model_accuracy)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nquantized_tflite_model = converter.convert()\r\nopen(\"converted_Lenet-new.tflite\", \"wb\").write(quantized_tflite_model )\r\n\r\n```", "Hi @caijinana , The issue is not replicating TF 2.6 , providing[ Gist ](https://colab.research.google.com/gist/mohantym/978cc0b2f8765f297e3846a33f09f2bb/github_50148.ipynb)for reference.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50148\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50148\">No</a>\n"]}, {"number": 50146, "title": "Remove meaningless log in KernelDefBuilder", "body": "I see a wall of meaningless logs from KernelDefBuilder:\r\n```\r\n2021-06-08 15:48:21.962466: I tensorflow/core/framework/kernel_def_builder.cc:43] 0\r\n2021-06-08 15:48:21.962475: I tensorflow/core/framework/kernel_def_builder.cc:43] 1\r\n2021-06-08 15:48:21.962492: I tensorflow/core/framework/kernel_def_builder.cc:43] 0\r\n2021-06-08 15:48:21.962501: I tensorflow/core/framework/kernel_def_builder.cc:43] 1\r\n2021-06-08 15:48:21.962522: I tensorflow/core/framework/kernel_def_builder.cc:43] 0\r\n2021-06-08 15:48:21.962530: I tensorflow/core/framework/kernel_def_builder.cc:43] 1\r\n2021-06-08 15:48:21.962543: I tensorflow/core/framework/kernel_def_builder.cc:43] 0\r\n2021-06-08 15:48:21.962552: I tensorflow/core/framework/kernel_def_builder.cc:43] 1\r\n```\r\nand so on...\r\n\r\nNo point to print such logs.", "comments": []}, {"number": 50145, "title": "Building current master fails due to missing files", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git/master\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 4.1.0\r\n- GCC/Compiler version (if compiling from source): 11\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuilding TF from the current source on GH fails due to missing files (all from `llvm-openmp`):\r\n`tools.pm`\r\n`kmp.h`\r\n`kmp_platform.h`\r\n`kmp_os.h`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nMy build command was as follows:\r\n```bash\r\nbazel build  --config=mkl -config=nogcp --config=nonccl  -c opt --copt=-march=native --copt=-O3 -s //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The issue will move to closed status once the PR is merged.", "@eli-osherovich ,\r\n\r\nPlease free feel to move this issue to closed status.Thanks!", "@tilakrayal \r\nOriginal PR messed up after a rebase.\r\nRe-created it.\r\n", "@eli-osherovich, @tilakrayal, Intel is also seeing this build issue and investigating for solution. As a workaround before any fix, please try with adding this build option,  **--spawn_strategy=standalone** , to the build command and this should temporarily fix the build failure.", "Tensorflow has reverted a change that causes the build failure, there is no need to use the additional option mentioned above if you sync the project today which has this commit https://github.com/tensorflow/tensorflow/commit/763ae9bed64834d6a9a2e18d9eead0d8763df079 (the commit message is incorrect).", "@yimeisun123 \r\nThe above change just puts standalone into config. My fix is better - it keeps the usual build", "@eli-osherovich - tried your earlier PR#50143, and still has build failure. I see that you pushed a new PR#50179, will check. Thanks.", "@yimeisun123  it definitely works for me.\r\n\r\nBy the way, if you are working on TF inside Intel, have a look at this issue: #50176 . There is a weird  problem inside Intel's code with GCC 11. While trivial code changes can solve it, the problem is quite interesting. Probably worth solving without code changes. ", "@tilakrayal  can we move forward with this?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50145\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50145\">No</a>\n"]}, {"number": 50144, "title": "Re-initializing backends on demand", "body": "**Describe the current behavior**\r\n\r\nWhile using Jax, I have a use case where I wish to control the CPU devices being used based on `num_devices` input. But this couldn't be changed once the backend has been initialized.\r\n\r\n```python\r\n>>> import os\r\n>>> import jax\r\n>>> from jax.lib import xla_bridge\r\n>>> from jaxlib import xla_client\r\n>>> print(jax.devices(\"cpu\"))\r\n[CpuDevice(id=0)]\r\n>>> os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\r\n>>> print(xla_client.get_local_backend(\"cpu\").devices())\r\n[CpuDevice(id=0)]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI wish to re-initialize the backend and the new backend pick up the env variables, and return 8 CPU devices in the above example.\r\n", "comments": ["Hi, Looks like the issue is related to Jax, since [Jax](https://github.com/google/jax/issues) has a separate repository, you can raise the issue there and close the issue here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50144\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50144\">No</a>\n"]}, {"number": 50143, "title": "Build from source fails due to missing files (llvm-openmp issues)", "body": "Compiling TF git fails due to missing files. \r\nAdded these into relevant BUILD rules.\r\n\r\n\r\nFixes #50145 ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50143) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 50142, "title": " Input 0 of layer max_pooling2d is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: (None, None, None, None, 16)", "body": "my generator codde is:\r\nIMG_HEIGHT=256\r\nIMG_WIDTH=256\r\nMASK_HEIGHT=256\r\nMASK_WIDTH=256\r\nIMG_CHANNELS=3\r\n# generator fxn\r\n \r\ndef Generator(X_list, y_list, batch_size = 4):\r\n    c = 0\r\n \r\n    while(True):\r\n        X = np.empty((batch_size, IMG_HEIGHT, IMG_WIDTH,IMG_CHANNELS), dtype = 'float32')\r\n        y = np.empty((batch_size, MASK_HEIGHT, MASK_WIDTH,1), dtype = 'float32')\r\n        \r\n        for i in range(c,c+batch_size):\r\n            image = X_list[i]\r\n          \r\n            mask =  y_list[i]\r\n          \r\n           \r\n        X = X[:,:,:,np.newaxis] / 255   # normalization \r\n        y = y[:,:,:,np.newaxis] / 255   # normalization\r\n        \r\n        c += batch_size\r\n        if(c+batch_size >= len(X_list)):\r\n            c = 0\r\n        yield X, y\r\n\r\nand model code is:\r\n#Build the model\r\ninputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\r\ns = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\r\n\r\n#Contraction path\r\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\r\nc1 = tf.keras.layers.Dropout(0.1)(c1)\r\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\r\np1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\r\n\r\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\r\nc2 = tf.keras.layers.Dropout(0.1)(c2)\r\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\r\np2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\r\n \r\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\r\nc3 = tf.keras.layers.Dropout(0.2)(c3)\r\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\r\np3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\r\n \r\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\r\nc4 = tf.keras.layers.Dropout(0.2)(c4)\r\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\r\np4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\r\n \r\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\r\nc5 = tf.keras.layers.Dropout(0.3)(c5)\r\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\r\n\r\n#Expansive path \r\nu6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\r\nu6 = tf.keras.layers.concatenate([u6, c4])\r\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\r\nc6 = tf.keras.layers.Dropout(0.2)(c6)\r\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\r\n \r\nu7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\r\nu7 = tf.keras.layers.concatenate([u7, c3])\r\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\r\nc7 = tf.keras.layers.Dropout(0.2)(c7)\r\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\r\n \r\nu8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\r\nu8 = tf.keras.layers.concatenate([u8, c2])\r\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\r\nc8 = tf.keras.layers.Dropout(0.1)(c8)\r\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\r\n \r\nu9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\r\nu9 = tf.keras.layers.concatenate([u9, c1], axis=3)\r\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\r\nc9 = tf.keras.layers.Dropout(0.1)(c9)\r\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\r\n \r\noutputs = tf.keras.layers.Conv2D(1, (1, 1), activation='softmax')(c9)\r\n\r\nwhen i am trying to train this model using:\r\nresults1 = model.fit(train_gen, steps_per_epoch=steps_per_epoch, epochs = epochs,\r\n                             validation_data = val_gen, validation_steps = validation_steps,callbacks=[checkpoint1], verbose=2)\r\ni am getting this error:\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\r\n        y_pred = self(x, training=True)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:425 call\r\n        inputs, training=training, mask=mask)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph\r\n        outputs = node.layer(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\r\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility\r\n        str(tuple(shape)))\r\n\r\n    ValueError: Input 0 of layer max_pooling2d is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: (None, None, None, None, 16)\r\n\r\n\r\nhow to resolve this issue?", "comments": ["@vjamishi \r\nI  reproduced the code shared but facing a different [error](https://colab.research.google.com/gist/UsharaniPagadala/f2098fdafd1708e718de7e15fbb68ded/untitled84.ipynb) .Could you please share the colab gist with all the dependencies to analyze the issue reported here and also Please specify the tensorflow version used.Thanks", "@vjamishi\r\nI don't  understand your Generator function but I think you have to replace \r\n```\r\n X = X[:,:,:,np.newaxis] / 255   # normalization \r\n y = y[:,:,:,np.newaxis] / 255   # normalization\r\n```\r\nwith \r\n```\r\n X = X / 255   # normalization \r\n y = y/ 255   # normalization\r\n```\r\nExplanation: np.newaxis creates a new axis increasing the tensors n_dim from 4 to 5 which leads to an error as max_pooling2d expects a shape of 4 dims (batchsize, image_height, image_width, n_channels)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @vjamishi\r\n> I don't understand your Generator function but I think you have to replace\r\n> \r\n> ```\r\n>  X = X[:,:,:,np.newaxis] / 255   # normalization \r\n>  y = y[:,:,:,np.newaxis] / 255   # normalization\r\n> ```\r\n> \r\n> with\r\n> \r\n> ```\r\n>  X = X / 255   # normalization \r\n>  y = y/ 255   # normalization\r\n> ```\r\n> \r\n> Explanation: np.newaxis creates a new axis increasing the tensors n_dim from 4 to 5 which leads to an error as max_pooling2d expects a shape of 4 dims (batchsize, image_height, image_width, n_channels)\r\n\r\nThanks for the suggestion it worked for me"]}, {"number": 50141, "title": "[PluggableDevice] Add DEVICE_DEFAULT registration for int32 Const", "body": "Add DEVICE_DEFAULT registration for int32 Const so that pluggable devices don't have to register it themselves.", "comments": ["> Can we remove the registration for `DEVICE_GPU`?\r\n> \r\n> CC @penpornk\r\n\r\nThanks for the review! I removed the GPU registration.", "@sanjoy Do I need to do anything to satisfy the import/copybara check?", "@rthadur Do you know what's going on with the import/copybara failure?", "@sanjoy sorry I missed it "]}]