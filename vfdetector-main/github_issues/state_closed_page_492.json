[{"number": 39020, "title": "Micro kernel elementwise math ops issues (abs, sin, cos, log, sqrt, tanh)", "body": "@tensorflow/micro\r\n\r\n\r\nThe math ops in [elementwise.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/elementwise.cc) seem a little askew.\r\n\r\nFirstly AbsEval, which calls `std::abs`\r\n```\r\nTfLiteStatus AbsEval(TfLiteContext* context, TfLiteNode* node) {\r\n  return EvalNumeric(context, node, std::abs);\r\n}\r\n```\r\nThe math.h `int abs(int j)` is an integer function, what is probably intended is `fabs`, or better still `float fabsf(float x)` : return the absolute value of the floating-point number x.\r\n\r\nThe other float operations are actually referencing the double valued functions, e.g.\r\n```\r\nTfLiteStatus SinEval(TfLiteContext* context, TfLiteNode* node) {\r\n  return EvalNumeric(context, node, std::sin);\r\n}\r\n```\r\nwould be better referencing `std::sinf`, which is the float-valued counterpart. Same for `cosf`, `logf`, and `sqrtf`.\r\n\r\nUnfortunately changing e.g. `std::cos` to `std::cosf` will not compile on gcc, due to [an issue](https://stackoverflow.com/questions/55458487/stdexpf-and-stdlogf-not-recognized-by-gcc-7-2-0) in the `<cmath>` header. So to make it compile with gcc-based compilers the include should be changed to `#include <math.h>` and the functions referenced without `std::` prefix.\r\n\r\nI will make a PR that also includes the TANH op.", "comments": ["@pingdynasty This is a micro related issue,Please post this in [micro repository](https://github.com/tensorflow/tflite-micro/issues)  ..Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39020\">No</a>\n"]}, {"number": 39019, "title": "bug with _VALID_SCOPE_NAME_REGEX", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  mac 10.13.5\r\n- TensorFlow version (use command below):  1.11.0\r\n- Python version:  3.6.5\r\n\r\n**Describe the current behavior**\r\nThe `_VALID_SCOPE_NAME_REGEX` and `_VALID_OP_NAME_REGEX` are defined in line 1583 of `tensorflow/python/framework/ops.py`\r\n\r\n```\r\n_VALID_OP_NAME_REGEX = re.compile(\"^[A-Za-z0-9.][A-Za-z0-9_.\\\\-/]*$\")\r\n_VALID_SCOPE_NAME_REGEX = re.compile(\"^[A-Za-z0-9_.\\\\-/]*$\")\r\n```\r\nwhich should recognize the `\\` symbol.\r\n\r\nThe result are: \r\n```\r\n>>> _VALID_SCOPE_NAME_REGEX = re.compile(\"^[A-Za-z0-9_.\\\\-/]*$\")\r\n>>> _VALID_SCOPE_NAME_REGEX.match(\"n_CatCntc_campaign/c_campaign\")\r\n<_sre.SRE_Match object; span=(0, 29), match='n_CatCntc_campaign/c_campaign'>\r\n>>> _VALID_SCOPE_NAME_REGEX.match(\"n_CatCntc_campaign\\c_campaign\")\r\n>>> _VALID_SCOPE_NAME_REGEX.match(\"n_CatCntc_campaign\\\\c_campaign\")\r\n>>> _VALID_SCOPE_NAME_REGEX.match(\"n_CatCntc_campaign\\\\\\c_campaign\")\r\n>>> \r\n```\r\nThe above pattern can't recognize `\\`, but with below pattern, it works.\r\n\r\n```\r\n>>> _VALID_SCOPE_NAME_REGEX = re.compile(r\"^[A-Za-z0-9_.\\\\\\-/]*$\")\r\n>>> _VALID_SCOPE_NAME_REGEX.match(\"n_CatCntc_campaign\\c_campaign\")\r\n<_sre.SRE_Match object; span=(0, 29), match='n_CatCntc_campaign\\\\c_campaign'>\r\n>>> _VALID_SCOPE_NAME_REGEX.match(\"n_CatCntc_campaign\\\\c_campaign\")\r\n<_sre.SRE_Match object; span=(0, 29), match='n_CatCntc_campaign\\\\c_campaign'>\r\n>>> _VALID_SCOPE_NAME_REGEX.match(\"n_CatCntc_campaign/c_campaign\")\r\n<_sre.SRE_Match object; span=(0, 29), match='n_CatCntc_campaign/c_campaign'>\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n", "comments": ["The following will work as well (move `-` to end as otherwise it will be considered as range, and prefix with `r` to not escape). This avoid three backslash`\\\\\\` which might be less understandable:\r\n```\r\nVALID_OP_NAME_REGEX = re.compile(r\"^[A-Za-z0-9.][A-Za-z0-9_.\\\\/>-]*$\")\r\n_VALID_SCOPE_NAME_REGEX = re.compile(r\"^[A-Za-z0-9_.\\\\/>-]*$\")\r\n```\r\n\r\nAdded a PR #39029 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39019\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39019\">No</a>\n"]}, {"number": 39018, "title": "\"file not found\" Error in generating tfrecords", "body": "Hello,\r\nI am getting a error saying that the file or directory was not found while generating the tfrecords. I am running the code in Google Colaboratory.\r\n\r\n- **Here is the command I am using:**\r\n\r\n ```command output\r\n!cd '/content/kangaroo';python '/content/drive/My Drive/Colab Notebooks/n.py' --csv_input='/content/kangaroo/images/test_labels.csv' --image_dir='/content/kangaroo/test' --output_path='/content/kangaroo/test.record'\r\n```\r\n\r\n- **Here is the full traceback**\r\n```\r\nWARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/n.py:238: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nWARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/n.py:223: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\r\n\r\nW0429 10:10:48.066490 139827938125696 module_wrapper.py:139] From /content/drive/My Drive/Colab Notebooks/n.py:223: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\r\n\r\nWARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/n.py:182: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nW0429 10:10:48.075507 139827938125696 module_wrapper.py:139] From /content/drive/My Drive/Colab Notebooks/n.py:182: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nTraceback (most recent call last):\r\n  File \"/content/drive/My Drive/Colab Notebooks/n.py\", line 238, in <module>\r\n    tf.app.run()\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/content/drive/My Drive/Colab Notebooks/n.py\", line 228, in main\r\n    tf_example = create_tf_example(group, path)\r\n  File \"/content/drive/My Drive/Colab Notebooks/n.py\", line 183, in create_tf_example\r\n    encoded_jpg = fid.read()\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/lib/io/file_io.py\", line 122, in read\r\n    self._preread_check()\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/lib/io/file_io.py\", line 84, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /content/kangaroo/test/0004.jpg; No such file or directory\r\n```\r\n\r\n- **Here is the code I am using to generate tfrecords**\r\n```python\r\n\"\"\"\r\nUsage:\r\n  # From tensorflow/models/\r\n  # Create train data:\r\n  python generate_tfrecord.py --csv_input=data/train_labels.csv  --output_path=train.record\r\n  # Create test data:\r\n  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=test.record\r\n\"\"\"\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import absolute_import\r\n\r\nimport os\r\nimport io\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\nfrom PIL import Image\r\nfrom object_detection.utils import dataset_util\r\nfrom collections import namedtuple, OrderedDict\r\n\r\nflags = tf.app.flags\r\nflags.DEFINE_string('csv_input', '', 'Path to the CSV input')\r\nflags.DEFINE_string('output_path', '', 'Path to output TFRecord')\r\nflags.DEFINE_string('image_dir', '', 'Path to images')\r\nFLAGS = flags.FLAGS\r\n\r\n\r\n# TO-DO replace this with label map\r\ndef class_text_to_int(row_label):\r\n    if row_label == 'A':\r\n        return 1\r\n    elif row_label == 'B':\r\n        return 2\r\n    elif row_label == 'C':\r\n        return 3\r\n    elif row_label == 'D':\r\n        return 4\r\n    elif row_label == 'E':\r\n        return 5\r\n    elif row_label == 'F':\r\n        return 6\r\n    elif row_label == 'G':\r\n        return 7\r\n    elif row_label == 'H':\r\n        return 8\r\n    elif row_label == 'I':\r\n        return 9\r\n    else:\r\n        None\r\n\r\n\r\ndef split(df, group):\r\n    data = namedtuple('data', ['filename', 'object'])\r\n    gb = df.groupby(group)\r\n    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\r\n\r\n\r\ndef create_tf_example(group, path):\r\n    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\r\n        encoded_jpg = fid.read()\r\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\r\n    image = Image.open(encoded_jpg_io)\r\n    width, height = image.size\r\n\r\n    filename = group.filename.encode('utf8')\r\n    image_format = b'jpg'\r\n    xmins = []\r\n    xmaxs = []\r\n    ymins = []\r\n    ymaxs = []\r\n    classes_text = []\r\n    classes = []\r\n\r\n    for index, row in group.object.iterrows():\r\n        xmins.append(row['xmin'] / width)\r\n        xmaxs.append(row['xmax'] / width)\r\n        ymins.append(row['ymin'] / height)\r\n        ymaxs.append(row['ymax'] / height)\r\n        classes_text.append(row['class'].encode('utf8'))\r\n        classes.append(class_text_to_int(row['class']))\r\n\r\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\r\n        'image/height': dataset_util.int64_feature(height),\r\n        'image/width': dataset_util.int64_feature(width),\r\n        'image/filename': dataset_util.bytes_feature(filename),\r\n        'image/source_id': dataset_util.bytes_feature(filename),\r\n        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\r\n        'image/format': dataset_util.bytes_feature(image_format),\r\n        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\r\n        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\r\n        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\r\n        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\r\n        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\r\n        'image/object/class/label': dataset_util.int64_list_feature(classes),\r\n    }))\r\n    return tf_example\r\n\r\n\r\ndef main(_):\r\n    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\r\n    path = os.path.join(FLAGS.image_dir)\r\n    examples = pd.read_csv(FLAGS.csv_input)\r\n    grouped = split(examples, 'filename')\r\n    for group in grouped:\r\n        tf_example = create_tf_example(group, path)\r\n        writer.write(tf_example.SerializeToString())\r\n\r\n    writer.close()\r\n    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\r\n    print('Successfully created the TFRecords: {}'.format(output_path))\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n```\r\n- **My Directory stucture looks something like this**\r\n![Directory Structure](https://raw.githubusercontent.com/Monster-Gaming-Studios/hello/master/Capture.JPG)\r\n\r\n**P.S.:-** If you want some additional information like how I generated the csv files, ask me and I will be glad to provide", "comments": ["**Update:** Please do not ask me to check on stackoverflow on some where else. I tried looking everywhere.", "I solved the issue. It was my own mistake. In the parameter `--image_dir`,  I had given wrong path. I apologize for the inconvenience caused and I am closing the issue. "]}, {"number": 39017, "title": "Tf.reshape: allow different reshape order (e.g. column-wise)", "body": "**System information**\r\n- TensorFlow version (you are using): '2.1.0'/ latest\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, tf.reshape returns a new tf.Tensor(tensor, shape, name=None) that has the same values as tensor **in the same order**, except with a new shape given by shape ([see tf.reshape](https://www.tensorflow.org/api_docs/python/tf/reshape)).\r\n\r\nHowever, in some cases it would be beneficial to adjust the ordering; e.g. [np.reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) allows \u2018C\u2019, \u2018F\u2019, \u2018A\u2019 orders - I am in particular interested in column-wise order. \r\n\r\n**For example:** Given a tf.Tensor with multiple image patches (denoted by seq_len) of shape h, w, c: `(batch_size, seq_len, h, w, c)`. I would like to concat/ reshape these image patches horizontally along axis 3 that result in shape `(batch_size, h, w * seq_len, c)`, without rearranging the pixel values within the image patches. In the **fully convolutional setting**, where seq_len varies, I can not iterate over unknown axis dimension seq_len in order to achieve this goal by e.g. using tf.split/ tf.concat (as far as I understand). Please see example below for more details.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes - tf.reshape requires another optional parameter, e.g. 'order'.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone.\r\n\r\n**Any Other info.**\r\n\r\nThe following example shall demonstrate the desired behaviour of tf.reshape:\r\n\r\n```\r\n# simulate batch of image patches\r\nx = np.arange(3 * 2 * 8).reshape(3, 2, 4, 2, 1)\r\n\r\n# reshape/ concat image patches along axis 3\r\nx_new = tf.reshape(x, (3, 4, 4, 1))\r\n```\r\n\r\nVisualize first batch sample **(current behaviour)**\r\n\r\n```\r\nprint(x_new[0, :, :, 0])\r\n<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\r\narray([[ 0,  1,  2,  3],\r\n       [ 4,  5,  6,  7],\r\n       [ 8,  9, 10, 11],\r\n       [12, 13, 14, 15]])>\r\n```\r\n\r\nVisualize first batch sample **(desired behaviour)**\r\n\r\n```\r\n<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\r\narray([[ 0, 4, 8, 12],\r\n       [ 1, 5,  9, 13],\r\n       [ 2, 6, 10, 14],\r\n       [ 3, 7, 11, 15]])>\r\n```\r\n\r\nPlease correct me if I missed something. Thanks.", "comments": ["I am able to reproduce the issue with TF 2.1.0 and TF 2.2-rc3.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/b586281d4a99cef6453a813317838619/untitled839.ipynb)Thanks!", "It seems like you want tf.transpose+tf.reshape instead of purely tf.reshape. You can also use tf.einsum to have a saner syntax for writing tf.transpose.", "@alextp thanks, however, this does not solve the issue - e.g. consider three (seq_len=3) image patches:\r\n\r\n```\r\n# simulate batch of image patches\r\nx = np.arange(3 * 3 * 8).reshape(3, 3, 4, 2, 1)\r\n\r\n# reshape/ concat image patches along axis 3\r\nx_new = tf.reshape(x, (3, 4, 6, 1))\r\n```\r\nVisualize first batch sample (current behaviour):\r\n```\r\ntf.Tensor(\r\n[[ 0  1  2  3  4  5]\r\n [ 6  7  8  9 10 11]\r\n [12 13 14 15 16 17]\r\n [18 19 20 21 22 23]], shape=(4, 6), dtype=int64)\r\n```\r\n\r\nusing tf.transpose either changes the output dimension (case 1) or rearranges the pixel values within the image patch:\r\n\r\nCase 1:\r\n```\r\nx_new_t = tf.transpose(x_new)\r\nprint(x_new_t[0, :, :2, 0])\r\narray([[ 0,  6],\r\n       [ 1,  7],\r\n       [ 2,  8],\r\n       [ 3,  9],\r\n       [ 4, 10],\r\n       [ 5, 11]])\r\n\r\nprint(x_new_t.shape)\r\nTensorShape([1, 6, 4, 3])\r\n```\r\n\r\nCase 2:\r\n```\r\nx_new_t2 = tf.transpose(x_new, (0, 1, 2, 3))\r\nprint(x_new_t2.shape)\r\n(3, 4, 6, 1)\r\n\r\nprint(x_new_t2[0, :, :2, 0])\r\ntf.Tensor(\r\n[[ 0  1]\r\n [ 6  7]\r\n [12 13]\r\n [18 19]], shape=(4, 2), dtype=int64)\r\n```\r\n\r\nPlease reopen the issue. Thanks.", "Once you decide what columns you want to reorder you can reorder those\ncolumns with transpose and then reshape to get the shape you want.\n\nIf you give names to your original and intended axes the sequence of\nreshapes and transposes will become clear.\n\nFor example if you start with an image as n c h w i and you want to get n\ni*w c h you can do a reshape to coalesce i and w and then transpose them\ninto place. If you want to go instead to n i*h c w you can transpose to put\ni and h together, reshape, and then transpose again.\n\nOn Wed, Apr 29, 2020 at 10:14 AM Nikolai10 <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> thanks, however, this does not solve\n> the issue - e.g. consider three (seq_len=3) image patches:\n>\n> # simulate batch of image patches\n> x = np.arange(3 * 3 * 8).reshape(3, 3, 4, 2, 1)\n>\n> # reshape/ concat image patches along axis 3\n> x_new = tf.reshape(x, (3, 4, 6, 1))\n>\n> Visualize first batch sample (current behaviour):\n>\n> tf.Tensor(\n> [[ 0  1  2  3  4  5]\n>  [ 6  7  8  9 10 11]\n>  [12 13 14 15 16 17]\n>  [18 19 20 21 22 23]], shape=(4, 6), dtype=int64)\n>\n> using tf.transpose either changes the output dimension (case 1) or\n> rearranges the pixel values within the image patch:\n>\n> Case 1:\n>\n> x_new_t = tf.transpose(x_new)\n> print(x_new_t[0, :, :2, 0])\n> array([[ 0,  6],\n>        [ 1,  7],\n>        [ 2,  8],\n>        [ 3,  9],\n>        [ 4, 10],\n>        [ 5, 11]])\n>\n> print(x_new_t.shape)\n> TensorShape([1, 6, 4, 3])\n>\n> Case 2:\n>\n> x_new_t2 = tf.transpose(x_new, (0, 1, 2, 3))\n> print(x_new_t2.shape)\n> (3, 4, 6, 1)\n>\n> print(x_new_t2[0, :, :2, 0])\n> tf.Tensor(\n> [[ 0  1]\n>  [ 6  7]\n>  [12 13]\n>  [18 19]], shape=(4, 2), dtype=int64)\n>\n> Please reopen the issue. Thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39017#issuecomment-621346420>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJMRS2TEQOAIYOZC2TRPBN7VANCNFSM4MTRPUEQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Thank you!"]}, {"number": 39016, "title": "Increase consitency of tensorShape", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version : 2.2.0rc3 python API\r\n- Are you willing to contribute it : No (not enough experience of tensorflow architecture)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nUsing `where` I met an unexpecting behaviour on tensor shape testing, behaviour with tensorshape differs to tensor.\r\n\r\n`r = tf.random.uniform([2,3,4], 0, 4, dtype=tf.int32)`\r\n`print(tf.where(r==0, 0, 1).shape)` # (2,3,4) => ok\r\n`print(tf.where(r.shape==3, 0, 1).shape)` **() => expecting (3,)**\r\n\r\n\r\n**Will this change the current api? How?**\r\nSure, but consistency will increase. Tensorshape could be considered as a kind of tensor itself\r\n\r\n**Who will benefit with this feature?**\r\n\r\n1. community: code will be clearer to read, the concept is the same for tensorshape & tensor\r\n2. optimisation ?: tensorshape could be processed by tensor graph instead of python object evaluation\r\n\r\n", "comments": ["r.shape is not a tensor, it's a python object, so when you do r.shape==3 you get a python boolean back, which in turns gets turned into a scalar by tf.where.", "(tf.shape(r) is always a tensor, and then your code would work)", "Thank you.\r\nHere is the graph I got from the python script:\r\n\r\n```\r\n@tf.function\r\ndef test_where(a):\r\n  r = tf.zeros([2,2])\r\n  acc = tf.concat([r,a], axis=1)\r\n  return tf.where(tf.shape(acc)==3, 0, 1)\r\n\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/9859675/80835381-ecd5c300-8bf2-11ea-9ce2-945692e0c8a0.png)\r\n\r\nSo the link was lost between concat and where test, but that's ok for my need"]}, {"number": 39015, "title": "Tensorflow hang in TF1.10.", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos6\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.10\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):0.15.0\r\n- GCC/Compiler version (if compiling from source):gcc 5.3\r\n\r\nI find data race in rendevous in TF1.10. I try to recurrence in TF1.15, but I failed, I want to confirm data race is fixed in TF1.15.\r\nProblem is:\r\nthread A: Normally start recv in RpcRemoteRendezvous::RecvFromRemoteAsync. It has 4 steps\r\n**A1**. New a call.\r\n**A2**. Register this call to waitting set.\r\n**A3**. Add cancel function to this call('opts_).\r\n**A4**. Start Recv\r\nthread B: run abort in BaseRemoteRendezvous::StartAbort.\r\n\r\nNormal condition:\r\n1. A call obj is waitting in thread A, then thread B forcely call all cancel function in waitting set, then this Call is cancelled.\r\n2. Thread B forcely cancel all call objs in waitting set, then thread A try to register a call obj in queue, and failed then done and quit.\r\n\r\nBut I find a condition in TF1.10 that:\r\nThread A register a call obj successly,  then thread B try to cancel any call obj in waitting set, but A's call obj have no time to set cancel function, when B call this call obj 's cancel function, then thread A continue, thread A start to wait. this op don't finish forever, executor is hang.\r\nIn other words, CPU order is:\r\nA1, A2, B, A3, A4.\r\nDo you think this CPU order is a problem in TF1.15 ?\r\n\r\nI notice some extra codes in TF1.15, I don't know that codes can avoid this question.\r\n![image](https://user-images.githubusercontent.com/33950866/80574266-1c8a9c80-8a34-11ea-9cc8-daf00e41218d.png)\r\n", "comments": ["@zhaozheng09 \r\nIs there any particular reason for using the old version of tensor flow can you please upgrade to latest version and let us know if the problem exist.\r\nAlso in order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "1.10 is too old. Please try 1.15 or 2.1 (2.2 to come soon)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39015\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39015\">No</a>\n"]}, {"number": 39014, "title": "TFLiteConverter doesn't support int8 quantization of PReLU", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): from binary\r\n- TensorFlow version (or github SHA if from source): 2.1\r\n\r\n**Problem description**\r\nTFLiteConverter doesn't support int8 quantization of PReLU. Since the PReLU is implemented as two ReLU operations, it seems that the real blocker is the negation operation as shown in the network graph picture and TFLiteConverter runtime error. Could someone check if there would be a quick fix for this problem? Thanks a lot for your help.\r\nCheers\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nRuntimeError: Quantization not yet supported for op: NEG\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model, Input\r\nfrom tensorflow.keras.layers import Conv2D, PReLU, MaxPooling2D\r\n\r\n# define dummy network\r\ndef network():\r\n    input_shape = (600, 800, 3)\r\n    input_tensor = Input(input_shape, name='model_input')\r\n    x = Conv2D(10, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\")(input_tensor)\r\n    x = PReLU(alpha_initializer='ones',shared_axes=[1, 2])(x)\r\n    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(x)\r\n    \r\n    x = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\")(x)\r\n    x = PReLU(shared_axes=[1, 2], alpha_initializer='ones')(x)\r\n    \r\n    x = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\")(x)\r\n    x = PReLU(shared_axes=[1, 2], alpha_initializer='ones')(x)\r\n    \r\n    return Model(input_tensor, x)\r\n\r\nmodel = network()\r\ndummy_input = tf.random.uniform((1,600,800,3))\r\ndummy_output = model.predict(dummy_input)\r\n\r\ndef dataset_gen():\r\n    for i in range(100):\r\n        img_input = 2*np.random.rand(1,600,800,3)-1\r\n        yield [img_input.astype(np.float32)]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = dataset_gen\r\ntflite_quant_model = converter.convert()\r\n\r\nwith open('dummy_model_int8.tflite', 'wb') as file:\r\n    file.write(tflite_quant_model)\r\n```\r\n<img width=\"274\" alt=\"prelu3\" src=\"https://user-images.githubusercontent.com/9253234/80573217-7d47b400-89f7-11ea-9b21-de8da0ca0475.PNG\">\r\n\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["I'm also having the same problem. Do you know is there any workaround until the problem is resolved?", "I could reproduce the error with `tf-nightly`. I am posting full error trace here for our reference. [Here](https://colab.research.google.com/gist/jvishnuvardhan/a540fc1de3b749564ffe643b690d7c22/untitled150.ipynb) is a gist for our reference. Thanks!\r\n\r\n```\r\n--------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-8-fb190dc2a1ef> in <module>()\r\n     36 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n     37 converter.representative_dataset = dataset_gen\r\n---> 38 tflite_quant_model = converter.convert()\r\n     39 \r\n     40 with open('dummy_model_int8.tflite', 'wb') as file:\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, resize_input)\r\n     91     return self._calibrator.QuantizeModel(\r\n     92         np.dtype(input_type.as_numpy_dtype()).num,\r\n---> 93         np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n     94 \r\n     95   def calibrate_and_quantize_single(self,\r\n\r\nRuntimeError: Quantization not yet supported for op: PRELU\r\n```", "@Kevinpsk , can you please re-test using tf-nightly, the [recent commit](https://github.com/tensorflow/tensorflow/commit/fe40ddcc5e8dd3425d55b7d44fbdba3d83ea8108) may have resolved the problem.", "@Kevinpsk Agree with @akarmi . This looks like resolved in `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/b476c5f0c719cc494ca05f6058608020/untitled150.ipynb).\r\n\r\nCan you please verify once and close the issue? Thanks!", "Hi @akarmi and @jvishnuvardhan, thanks a lot for reminding me. Yes this bug has been fixed, and I am closing down this ticket now.\r\nCheers", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39014\">No</a>\n"]}, {"number": 39013, "title": "Loading a saved multi-input model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI have written a simple standalone code to reproduce the error\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nColab\r\n- TensorFlow version (use command below):\r\nv2.2.0-rc3-0-gaad398b5e9 2.2.0-rc3\r\n- Python version:\r\nPython 3.6.9\r\n- CUDA/cuDNN version:\r\nCUDA Version 10.1.243\r\n\r\n**Describe the current behavior**\r\n1. I save the whole model with a ModelCheckpoint callback\r\n2. I try to load the model from de checkpoint files\r\n3. ```TypeError: Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, 16]).```\r\n\r\n**Describe the expected behavior**\r\nThe model should be loaded and ready to continue training or to be used for evaluation\r\n**Standalone code to reproduce the issue**\r\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1CWRU1k2d0xtlww6hCavWHbKYb5aRrjw9)\r\n\r\n**Other info / logs** \r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in make_shape(v, arg_name)\r\n    210   try:\r\n--> 211     shape = tensor_shape.as_shape(v)\r\n    212   except TypeError as e:\r\n\r\n21 frames\r\nTypeError: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, 16])\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in make_shape(v, arg_name)\r\n    211     shape = tensor_shape.as_shape(v)\r\n    212   except TypeError as e:\r\n--> 213     raise TypeError(\"Error converting %s to a TensorShape: %s.\" % (arg_name, e))\r\n    214   except ValueError as e:\r\n    215     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\r\n\r\nTypeError: Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, 16]).\r\n```", "comments": ["Was able to reproduce the issue with TF v2.1, [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/87dee37fd31153716c292d1ab4df899a/39013.ipynb#scrollTo=n49ArHDEeON1) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/8d190e12a923604a08113f41a19c3338/39013-tf-nightly.ipynb). Please find the attached gist. Thanks!", "I seem to face the same issue, although it's not during the loading of a saved model.\r\nIt occurs during model building/compilation itself.", "Was able to reproduce the issue using TF version 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/2cd7779cafef3cc4a7d3aee0bd98c347/untitled79.ipynb).", "Hitting the same error", "@pabloi09 Looks like this may be due to providing multi-input to a keras Sequential model. Please note that Sequential models only works with single-input and single-output. For multi input/outputs, please use Functional model. \r\n\r\nThe warning also clearly mentions about multi-outputs provided to the model.\r\n\r\nI tried to remove one branch in your multi-input Sequential model. With those modification, i didn't see any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/6ff409f85a1aee9b8950f2e2d785c116/untitled79.ipynb). Thanks!", "Thanks to @jvishnuvardhan 's reply, I clear the error by remove Sequential model and just use Functional style code.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as this was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39013\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39013\">No</a>\n"]}, {"number": 39012, "title": "gher", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@hasa2020 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "Closing since no info was provided", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39012\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39012\">No</a>\n"]}, {"number": 39011, "title": "NFC - minor spelling tweaks in pbtxt", "body": "This PR addresses minor spelling tweaks in pbtxt files and one go file. These changes are in comments or messages.", "comments": []}, {"number": 39010, "title": "Revert \"Update version numbers for TensorFlow 2.2.0\"", "body": "Reverts tensorflow/tensorflow#38653", "comments": []}, {"number": 39009, "title": "Custom Constraint not working", "body": "I created a custom constraint using [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/constraints.py) which forces the sum of the elements to have sum(axis=1) to be one and be NonNeg at the same time, \r\n```\r\nclass SumConstraint(Constraint):\r\n    def __init__(self, axis):\r\n        super(SumConstraint, self).__init__()\r\n        self.axis = axis\r\n    def __call__(self, w):\r\n        w = w * K.cast(K.greater_equal(w, 0.), K.floatx())\r\n        s = K.sum(w, axis = self.axis)\r\n        s = s.numpy()\r\n        for i in range(6):\r\n            w[i]/=s[i]\r\n        return w\r\n    def get_config(self):\r\n        return {'axis': self.axis}\r\n```\r\nBut when I give, ```x = Dense(3, kernel_initializer = SumConstraint(axis = 1))```\r\n\r\nI get, **``` ValueError: Unknown constraint: SumConstraint ```**", "comments": ["Surprising to know that they work fine in TF1. ", "@prameth \r\n\r\nPlease, let is know Is this still an issue?. If the issue still persists please let us know Tensorflow version you are using,operating system.Also, provide colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram it\u2019s still an issue as I didn\u2019t closed it yet :)\r\n\r\nI\u2019ve shared everything I can. I used Colab, can\u2019t share it as its private. \r\nBut, when I use the built-in NonNeg() constraint as,\r\n\r\n` Dense(3, kernel_initializer = NonNeg())`, it works perfectly fine.\r\nYou can help me with a simple template how to implement custom constraints in TF2 Keras.", "> @prameth\r\n> \r\n> Please, let is know Is this still an issue?. If the issue still persists please let us know Tensorflow version you are using,operating system.Also, provide colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nThe issue\u2019s in TF2. ", "I tried your example with TF 2.2-rc4 version. The example executes successfully.\r\nPlease take a look at the [gist](https://colab.research.google.com/gist/ymodak/45c89a50cf3f7bc927c8e87aa061ca7c/untitled10.ipynb). Thanks!", "Thank you @ymodak, please edit the gist to `kernel_constraint`, instead of kernel_initialzer. \r\nI'm still facing an issue when I apply WeightNormalisation to the Dense Layer. The one without norm, was working previously too.\r\n<img width=\"1476\" alt=\"Screenshot 2020-05-03 at 4 24 45 PM\" src=\"https://user-images.githubusercontent.com/38836379/80912459-a4391980-8d5a-11ea-83aa-6df1c8cdf3a4.png\">\r\n\r\n\r\n* If I use `kernel_constraint = NonNeg()`, it works perfectly fine with WeightNorm. \r\n* Also, custom `kernel_initializer` doesn't work when WeightNorm is applied.", "I tested with tf 2.2 and made the change as you mentioned.\r\nThe code works successfully. See the [gist](https://colab.research.google.com/gist/ymodak/7f9ab9f46b94d38a47004e43bfccdd04/untitled10.ipynb)\r\nI notice you are importing tf addons module where as my example relies on tf core.\r\n", "I saw the code you shared, it doesn't noticeably resolve anything\r\nThe problem I had was with the WeightNorm applied with the CustomConstraints using the add-ons module.\r\nIt would be great if you fix that. \r\n<img width=\"1097\" alt=\"Screenshot 2020-05-14 at 3 24 30 PM\" src=\"https://user-images.githubusercontent.com/38836379/81920733-3c56be80-95f7-11ea-9c8e-8c8fbd117ca6.png\">\r\n", "@ymodak, I've fixed it.\r\nFor anyone, trying to figure this out, https://github.com/tensorflow/addons/issues/1839#issuecomment-629194916", "If I use custom constraint as `kernel_constraint` when I create a model:\r\n\r\n    model = keras.Sequential([keras.layers.Dense(y.shape[1], activation='softplus',\r\n                                                 kernel_constraint=SumConstraint(axis=0),\r\n                                                 # kernel_constraint=keras.constraints.MinMaxNorm(),\r\n                                                 bias_constraint=keras.constraints.MinMaxNorm(min_value=-100,\r\n                                                                                              max_value=100),\r\n                                                 input_shape=[x.shape[1]])])\r\n\r\nI always got this `ValueError: Unknown constraint: SumConstraint`.\r\n\r\nEven if I use the same class as @ymodak showed or use the example in document https://keras.io/api/layers/constraints/.\r\n\r\nI use TF 2.2.0"]}, {"number": 39008, "title": "tf-nightly-cpu couldn't trace any graph with subclass models.", "body": "**System information**\r\n- Have I written custom code: Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10.\r\n- TensorFlow installed from (source or binary): pip intall tf-nightly-cpu.\r\n- TensorFlow version (use command below):  2.2.0-dev20200426.\r\n- Python version: 3.7.7\r\n- Tensorboard version: 2.3.0a20200412 or 2.1.1\r\n\r\n**Describe the current behavior**\r\n\r\nI've created a `subclass model`, and trained it with tensorboard callback. Then I start tensorboard and select `Graphs dashboard`, but it shows error messages that `Graph visualization failed` as the picture shows\uff1a\r\n\r\n![image](https://user-images.githubusercontent.com/15494997/80565099-3d49f680-8a22-11ea-994d-ddc55ba4ee10.png)\r\n\r\nI've checked the issue [1961](https://github.com/tensorflow/tensorboard/issues/1961) here but didn't get any help.\r\n\r\nBy the way, `Sequential Model` could trace the graph.\r\n\r\n**Describe the expected behavior**\r\n\r\nThings works well when I use `tensorflow 2.1.0` with both `tensorboard 2.3.0a20200412` and `2.1.1`.\r\nSo I think `tf-nightly-cpu` should work as well.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\n\r\nclass ThreeLayerMLP(keras.Model):\r\n    def __init__(self, name=None):\r\n        super().__init__(name=name)\r\n        self.dense_1 = layers.Dense(64, activation='relu', name='dense_1')\r\n        self.dense_2 = layers.Dense(64, activation='relu', name='dense_2')\r\n        self.pred_layer = layers.Dense(10, name='predictions')\r\n\r\n    def call(self, inputs):\r\n        x = self.dense_1(inputs)\r\n        x = self.dense_2(x)\r\n        return self.pred_layer(x)\r\n\r\n\r\nmodel = ThreeLayerMLP(name='3_layer_mlp')\r\n\r\nx_train, y_train = (np.random.random(\r\n    (60000, 784)), np.random.randint(10, size=(60000, 1)))\r\nx_test, y_test = (np.random.random(\r\n    (10000, 784)), np.random.randint(10, size=(10000, 1)))\r\n\r\nmodel.compile(\r\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    optimizer=keras.optimizers.RMSprop())\r\n\r\ncallback = tf.keras.callbacks.TensorBoard(\r\n    'subclass_logs',\r\n    update_freq=2,\r\n)\r\nhistory = model.fit(x_train,\r\n                    y_train,\r\n                    batch_size=64,\r\n                    epochs=10,\r\n                    callbacks=[callback])\r\n\r\n```", "comments": ["@AlexanderJLiu This is very similar to the [issue](https://github.com/tensorflow/tensorboard/issues/1961). Please take a look at it and let me know if it helps.", "@gowthamkpr I've read this issue several times. And  I've tried some solutions recommended.\r\n\r\n![image](https://user-images.githubusercontent.com/15494997/80668383-590dd500-8ad4-11ea-8b96-a1ca9126291c.png)\r\n\r\nI tried add `run_eagerly` to `compile`, and it didn't work.\r\n\r\n![image](https://user-images.githubusercontent.com/15494997/80668516-abe78c80-8ad4-11ea-844a-6291f1abf4a7.png)\r\n\r\nI've also tried customized `get_cofing` method of subclass model  or layers, and it didn't work too.\r\n\r\nI think this issue is outdated, and couldn't get any useful infomation.\r\n\r\nThe most important things is `tf2.1` can trace subclass model's graph, why can't `tf-nightly-cpu` or the `tf2.2` coming soon?\r\n\r\nLooking forward to your reply. Thanks~", "@AlexanderJLiu Can you please share a colab notebook reproducing the issue with tensorflow 2.1 and tf-nightly. Thanks!", "@gowthamkpr \r\n\r\nThis [colab](https://colab.research.google.com/drive/1Uc9trzbx5ObYhUahLf2t6vWGKigsJT45) is related to tf-nightly.\r\nAnd this [colab](https://colab.research.google.com/drive/1C3FwT7h1j7qnK1QNoOSOpb355RpFX44F) is related to tf-2.1.\r\n\r\nThanks~", "It's seemed that problem still exists according to [colab here](https://colab.research.google.com/drive/12sUyrURdybadYmi-jjayB52LLhWTDcvv?usp=sharing) with tf-nightly-2.3.0-dev20200614.\r\n\r\nAny reply will be greatly appreciated. Thanks~", "@omalleyt12 Alexander here has a Keras subclass model and, of course, it returns `False` for `model._is_graph_network`. Can you remind us in what condition Keras constructs a graph? Does all subclass model have no graph at all?", "https://github.com/tensorflow/tensorflow/blob/89df3ddcd5451aefe52c7deb684f220b0520a6b1/tensorflow/python/keras/utils/layer_utils.py#L133-L135\r\n\r\nI believe this is an indicator that all subclassed models will return `is_graph_network` as False", "Hey @omalleyt12, any update on this?  As far as I can tell, there's no way to visualize the graph of a Keras model subclass in TF 2.2.  Is that correct?\r\n\r\ncc @w4nderlust ", "@reedwm, do you know what the current state of graph tracing in TensorFlow 2.2 with Keras model subclasses is?", "I'm not sure. /CC @fchollet @manivaradarajan can you take a look or triage?", "@manivaradarajan any thoughts on this?  Seems this is still broken in TF 2.3.", "Does this work in the nightlies?", "@tgaddair @tomerk  It seems to be working with  tf-nightly-2.4.0-dev20200811. Please see this [colab](https://colab.research.google.com/drive/1ZR3u993vmuLAjm6vMg5OHuw0jRap3Fop?usp=sharing)", "Excellent! I'll go ahead and close this issue then.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39008\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39008\">No</a>\n"]}, {"number": 39007, "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 20.1\r\n- Python version: Python 3.8 (64-bit)\r\n- Installed using virtualenv? pip? conda?: pip\r\n- GCC/Compiler version (if compiling from source): Jupyter\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Intel HD graphics 620 8239 MB\r\n- CPU Intel(R) Core(TM) i5-7300U CPU @ 2.60GHz 2.71 GHz\r\n\r\n\r\nPython 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\linj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@jlinxciv \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/38916#issuecomment-619572753), also, refer similar issues:\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n\r\nif possible please upgrade your tensorflow to later version and let us know if that helps you resolve the issue.\r\nThanks!", "Closing as duplicate", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39007\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39007\">No</a>\n", "Traceback (most recent call last):\r\n  File \"C:\\Users\\LEGION\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime. \r\n\r\nI have installed Anaconda inside my D drive and have made environment variable for C drive anaconda scripts\r\nI have installed tensorflow in a separate tf environment in anaconda from where this error is coming in my spyder tf env file\r\n", "> Traceback (most recent call last):\r\n> File \"C:\\Users\\LEGION\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in\r\n> from tensorflow.python._pywrap_tensorflow_internal import *\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> I have installed Anaconda inside my D drive and have made environment variable for C drive anaconda scripts\r\n> I have installed tensorflow in a separate tf environment in anaconda from where this error is coming in my spyder tf env file\r\n\r\n![same for tensor 2 3   it is not suitable for python3 8](https://user-images.githubusercontent.com/66783491/89713740-5f56fb80-d9b7-11ea-93fa-d08acc298f9b.JPG)\r\n", "this issue is same for every python 3.8 version.\r\n\r\nuse google collab          or  convert your python version into lower one.\r\n\r\n", "Python 3.8 works in TF 2.2 and TF 2.3. You need to have a 64 bit Python. If on Windows, you need to ensure you have the latest MSVC redistributable and that the Python binary is installed from official sources, not from Windows marketplace", "> this issue is same for every python 3.8 version.\r\n> \r\n> use google collab or convert your python version into lower one.\r\n\r\nI tried with Python 3.6 and 3.7 but no luck still facing same issue", "Hi, I am trying to install TensorFlow and getting below error:\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nI have tried with TF 2.3 as 2.2  and getting same error, Python 3.8.5.\r\n"]}, {"number": 39006, "title": "Thread hang for Stage/MapStage op when setting inter_op_parallelism_threads=1", "body": "**System information**\r\n\r\n - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n - TensorFlow version (use command below): tf-nightly\r\n - Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n\r\nThread will hang if setting `inter_op_parallelism_threads=1`.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nfrom six.moves import queue as Queue\r\nimport threading\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import data_flow_ops\r\nfrom tensorflow.python.platform import test\r\n\r\ndisable_eager_execution()\r\ntf.config.threading.set_inter_op_parallelism_threads(num_threads=1)  # pass if set 2\r\n\r\n\r\nclass ThreadHangTest(test.TestCase):\r\n    \"\"\"test Stage/MapStage\"\"\"\r\n\r\n    def testStage(self):\r\n        capacity = 3\r\n        with ops.device(test.gpu_device_name()):\r\n            x = array_ops.placeholder(dtypes.int32, name='x')\r\n            stager = data_flow_ops.StagingArea([dtypes.int32, ], capacity=capacity, shapes=[[]])\r\n\r\n        queue = Queue.Queue()\r\n        with self.session() as sess:\r\n            def thread_run():\r\n                for i in range(capacity + 1):\r\n                    sess.run(stager.put([x]), feed_dict={x: i})\r\n                    queue.put(0)\r\n\r\n            t = threading.Thread(target=thread_run)\r\n            t.daemon = True\r\n            t.start()\r\n\r\n            try:\r\n                for i in range(capacity + 1):\r\n                    queue.get(timeout=1)\r\n            except Queue.Empty:\r\n                pass\r\n\r\n            for i in range(capacity):\r\n                sess.run(stager.get())\r\n\r\n    def testMapStage(self):\r\n        capacity = 3\r\n        with ops.device(test.gpu_device_name()):\r\n            x = array_ops.placeholder(dtypes.int32, name='x')\r\n            pi = array_ops.placeholder(dtypes.int64, name='pi')\r\n            map_stager = data_flow_ops.MapStagingArea([dtypes.int32, ], capacity=capacity, shapes=[[]])\r\n\r\n        queue = Queue.Queue()\r\n        with self.session() as sess:\r\n            def thread_run():\r\n                for i in range(capacity + 1):\r\n                    sess.run(map_stager.put(pi, [x], [0]), feed_dict={x: i, pi: i})\r\n                    queue.put(0)\r\n\r\n            t = threading.Thread(target=thread_run)\r\n            t.daemon = True\r\n            t.start()\r\n\r\n            try:\r\n                for i in range(capacity + 1):\r\n                    queue.get(timeout=1)\r\n            except Queue.Empty:\r\n                pass\r\n\r\n            for i in range(capacity):\r\n                sess.run(map_stager.get())\r\n\r\n\r\nif __name__ == '__main__':\r\n    test.main()\r\n\r\n```", "comments": ["@GHGmc2 \r\n\r\nI have tried in colab with TF version 2.2.0-rc3 and i am seeing the below error.(\r\n`UnrecognizedFlagError: Unknown command line flag 'f'`).Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/b0aba773e091d8866d5e18b1739ac88b/untitled837.ipynb).Thanks!", "> @GHGmc2\r\n> \r\n> I have tried in colab with TF version 2.2.0-rc3 and i am seeing the below error.(\r\n> `UnrecognizedFlagError: Unknown command line flag 'f'`).Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/b0aba773e091d8866d5e18b1739ac88b/untitled837.ipynb).Thanks!\r\n\r\nThis may be the problem of colab, check [here](https://github.com/tensorflow/tensorflow/issues/17702#issuecomment-387335646), please try on your local environment.", "@GHGmc2 Are you encountering the same problem with Tensorflow 2.1, 2.2rc2, 2.2rc3 ?", "> @GHGmc2 Are you encountering the same problem with Tensorflow 2.1, 2.2rc2, 2.2rc3 ?\r\n\r\nYes.\r\nFor some reason, we need to set inter_op_parallelism_threads=1, and found that the thread is hanging, the code snippet is from [stage_op_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/stage_op_test.py#L168) and [map_stage_op_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/map_stage_op_test.py#L186), I just make it cleaner.", "I think this is working as intended. The issue is that the StagingArea.put() operation is blocked because you're pushing in one more than capacity. That occupies a thread. \r\n\r\nThat means that the StagingArea.get() operation, which is supposed to get an element and unblock the put() can't really proceed because it has no threads to run on."]}, {"number": 39005, "title": "Fixing typo in the comments", "body": "", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac @chanshah"]}, {"number": 39004, "title": "Huber Loss crashes training loop due to data type mismatch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): somewhat custom\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: laptop\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): \r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nHuber Loss crashes the script with the following error message:\r\n\r\n> TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.\r\n\r\nThat happens even though I cast everything to either `tf.float32` or `tf.float64` manually. It **does** work if I put this line \r\n```\r\ntf.keras.backend.set_floatx('float32')\r\n```\r\nOr if I remove the original line with `float64`. Seems to me like setting the global data type fails somewhere. And, I get the following warning that tensors are being re-casted automatically:\r\n>WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\n**Standalone code to reproduce the issue**\r\n```import os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nfrom sklearn.datasets import load_linnerud\r\nimport tensorflow as tf\r\ntf.keras.backend.set_floatx('float64')\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, Concatenate\r\n\r\nX, y = load_linnerud(return_X_y=True)\r\n\r\ndata = tf.data.Dataset.from_tensor_slices((X, y)).\\\r\n    map(lambda a, b: (tf.divide(a, tf.reduce_max(X, axis=0, keepdims=True)), b))\r\n\r\ntrain_data = data.take(16).shuffle(16).batch(4)\r\ntest_data = data.skip(16).shuffle(4).batch(4)\r\n\r\n\r\nclass FullyConnectedNetwork(Model):\r\n    def __init__(self):\r\n        super(FullyConnectedNetwork, self).__init__()\r\n        self.layer1 = Dense(9, input_shape=(3,))\r\n        self.layer2 = LSTM(8, return_sequences=True)\r\n        self.layer3 = Dense(27)\r\n        self.layer4 = Dropout(5e-1)\r\n        self.layer5 = Dense(27)\r\n        self.layer6 = Concatenate()\r\n        self.layer7 = Dense(3)\r\n\r\n    def __call__(self, x, *args, **kwargs):\r\n        x = tf.nn.tanh(self.layer1(x))\r\n        y = self.layer2(x)\r\n        x = tf.nn.selu(self.layer3(x))\r\n        x = self.layer4(x)\r\n        x = tf.nn.relu(self.layer5(x))\r\n        x = self.layer6([x, y])\r\n        x = self.layer7(x)\r\n        return x\r\n\r\n\r\nmodel = FullyConnectedNetwork()\r\n\r\nloss_object = tf.keras.losses.Huber()\r\n\r\ntrain_loss = tf.keras.metrics.Mean()\r\ntest_loss = tf.keras.metrics.Mean()\r\n\r\noptimizer = tf.keras.optimizers.Adamax()\r\n\r\n\r\n@tf.function\r\ndef train_step(inputs, targets):\r\n    with tf.GradientTape() as tape:\r\n        outputs = model(inputs)\r\n        loss = loss_object(outputs, targets)\r\n        train_loss(loss)\r\n\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n\r\n@tf.function\r\ndef test_step(inputs, targets):\r\n    outputs = model(inputs)\r\n    print(outputs.dtype, targets.dtype)\r\n    loss = loss_object(outputs, targets)\r\n    test_loss(loss)\r\n\r\n\r\ndef main():\r\n    train_loss.reset_states()\r\n    test_loss.reset_states()\r\n\r\n    for epoch in range(1, 10_000 + 1):\r\n        for x, y in train_data:\r\n            train_step(x, y)\r\n\r\n        for x, y in test_data:\r\n            test_step(x, y)\r\n\r\n        if epoch % 25 == 0:\r\n            print(f'Epoch: {epoch:>4} Train Loss: {train_loss.result().numpy():.2f} '\r\n                  f'Test Loss: {test_loss.result().numpy():.2f}')\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-30-b08781047662>\", line 86, in <module>\r\n    main()\r\n  File \"<ipython-input-30-b08781047662>\", line 75, in main\r\n    train_step(x, y)\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in converted code:\r\n    <ipython-input-20-f2c31267a363>:54 train_step  *\r\n        loss = loss_object(outputs, targets)\r\n    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py:126 __call__\r\n        losses = self.call(y_true, y_pred)\r\n    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py:221 call\r\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py:915 huber_loss\r\n        math_ops.multiply(delta, linear))\r\n    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py:180 wrapper\r\n        return target(*args, **kwargs)\r\n    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py:334 multiply\r\n        return gen_math_ops.mul(x, y, name)\r\n    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py:6125 mul\r\n        \"Mul\", x=x, y=y, name=name)\r\n    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:504 _apply_op_helper\r\n        inferred_from[input_arg.type_attr]))\r\n    TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.\r\n```", "comments": ["Was able to reproduce the issue with TF v2.1, [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/6baa93476d84d2fef692b159e39eaaaa/39004.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/869d37d6c2c3909353ce483f01fa5df0/39004-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Potentially related to #36790", "Added a PR #39123 for the fix of this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39004\">No</a>\n"]}, {"number": 39003, "title": "Cherry pick saved model checks", "body": "Prevents several issues when saved models can be maliciously crafted.", "comments": []}, {"number": 39002, "title": "rc-2.2 cherry-pick request: Add CompositeTensor support for Distributed iterators.", "body": "This change is required to fix a current regression(memory leak) with Keras CFit + tf.distribute. ", "comments": []}, {"number": 39001, "title": "'Sequential' object has no attribute '_get_save_spec'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): -\r\n- TensorFlow version (or github SHA if from source): 2.2\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nhttps://colab.research.google.com/drive/1IJWCGMZ9Wrf8C89oLJ_AWnGPH1ea4dma\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n**The output from the converter invocation**\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-17-83ed6c530300> in <module>()\r\n----> 1 converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py in model_input_signature(model, keep_original_batch_size)\r\n     75     TensorSpecs. This list does not contain the `training` argument.\r\n     76   \"\"\"\r\n---> 77   input_specs = model._get_save_spec(dynamic_batch=not keep_original_batch_size)  # pylint: disable=protected-access\r\n     78   if input_specs is None:\r\n     79     return None\r\n\r\nAttributeError: 'Sequential' object has no attribute '_get_save_spec'\r\n\r\n**Failure details**\r\nFails to convert\r\n\r\n**Any other info / logs**\r\nWhen running the exact same code on a Windows 10 machine with TF2.1, the error is different :\r\n\r\nD:\\dev\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py in __call__(self, inputs, **kwargs)\r\n    487             # Actually call the layer,\r\n    488             # collecting output(s), mask(s), and shape(s).\r\n--> 489             output = self.call(inputs, **kwargs)\r\n    490             output_mask = self.compute_mask(inputs, previous_mask)\r\n    491 \r\n\r\nTypeError: call() got an unexpected keyword argument 'training'", "comments": ["@davidbayon \r\nI ran the code shared by you but face a different error, please refer to [gist  here](https://colab.sandbox.google.com/gist/Saduf2019/2cf61fbb8cf36cd4b235f034b822f456/untitled162.ipynb)", "tf.keras not same keras\r\nmodel=tf.keras.models.load_model('model.h5', compile=False)", "Thanks a lot, that worked!\n\nOn Fri, May 1, 2020 at 12:28 PM LunaMK <notifications@github.com> wrote:\n\n> tf.keras not same keras\n> model=tf.keras.models.load_model('model.h5', compile=False)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39001#issuecomment-622218264>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJHKOX63TGFABXAOFMCRMNDRPIXTPANCNFSM4MTHIEXA>\n> .\n>\n\n\n-- \nDavid Bayon\n", "Issue was :\r\nmodel=tf.keras.models.load_model('model.h5', compile=False) had to be used instead of\r\nmodel=keras.models.load_model('model.h5', compile=False)\r\n"]}, {"number": 39000, "title": "Update version numbers for TensorFlow 2.2.0-rc4", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 2 -> 2\nPatch: 0 -> 0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.2.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/micro/examples/person_detection/README.md:275:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:276:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:282:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:295:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:331:2.2.0\ntensorflow/lite/micro/examples/magic_wand/train/README.md:106:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:177:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:178:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:184:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:197:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:235:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:167:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:168:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:174:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:187:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:223:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:180:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:181:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:187:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:200:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:236:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:26:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:28:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:78:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:127:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:161:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:188:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:191:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:222:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:90:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:107:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:139:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:172:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:199:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:202:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:233:2.2.0\ntensorflow/tensorflow.bzl:61:2.2.0\ntensorflow/tools/pip_package/setup.py:50:2.2.0\ntensorflow/tools/pip_package/setup.py:64:2.2.0\ntensorflow/tools/pip_package/setup.py:65:2.2.0\ntensorflow/tools/pip_package/setup.py:99:2.2.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.2.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/micro/examples/person_detection/README.md:275:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:276:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:282:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:295:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:331:2.2.0\ntensorflow/lite/micro/examples/magic_wand/train/README.md:106:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:177:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:178:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:184:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:197:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:235:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:167:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:168:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:174:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:187:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:223:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:180:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:181:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:187:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:200:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:236:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:26:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:28:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:78:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:127:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:161:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:188:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:191:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:222:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:90:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:107:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:139:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:172:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:199:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:202:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:233:2.2.0\ntensorflow/tensorflow.bzl:61:2.2.0\ntensorflow/tools/pip_package/setup.py:50:2.2.0\ntensorflow/tools/pip_package/setup.py:64:2.2.0\ntensorflow/tools/pip_package/setup.py:65:2.2.0\ntensorflow/tools/pip_package/setup.py:99:2.2.0\n```", "comments": ["Are we doing an rc4? ", "> Are we doing an rc4?\r\n\r\nYes @gunan we are doing an rc4.", "OK, I will leave merging to you guys, to ensure I am not breaking the workflow."]}, {"number": 38999, "title": "Does not compile on windows not even close why bother", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sr99622 \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the [Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!", "**ENVIRONMENT**\r\n\r\nWindows 10\r\nbazel 2.0.0\r\nMSVC 2019\r\nPython 3.6.8\r\nTensorflow r2.2\r\n\r\n**BUILD COMMAND**\r\n\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\n*ERROR MESSAGE**\r\n\r\nC:/users/sr996/tensorflow/tensorflow/core/lib/monitoring/BUILD:176:1: C++ compilation of rule '//tensorflow/core/lib/monitoring:percentile_sampler' failed (Exit 2)\r\ntensorflow/core/lib/monitoring/percentile_sampler.cc(32): error C2679: binary '=': no operator found which takes a right-hand operand of type 'initializer list' (or there is no acceptable conversion)\r\n.\\tensorflow/core/lib/monitoring/percentile_sampler.h(71): note: could be 'tensorflow::monitoring::PercentileSamplerCell::Sample &tensorflow::monitoring::PercentileSamplerCell::Sample::operator =(tensorflow::monitoring::PercentileSamplerCell::Sample &&)'\r\n.\\tensorflow/core/lib/monitoring/percentile_sampler.h(71): note: or       'tensorflow::monitoring::PercentileSamplerCell::Sample &tensorflow::monitoring::PercentileSamplerCell::Sample::operator =(const tensorflow::monitoring::PercentileSamplerCell::Sample &)'\r\n.\\tensorflow/core/lib/monitoring/percentile_sampler.h(68): note: or       'bool tensorflow::monitoring::PercentileSamplerCell::Sample::operator =(void *) const'\r\ntensorflow/core/lib/monitoring/percentile_sampler.cc(32): note: while trying to match the argument list '(tensorflow::monitoring::PercentileSamplerCell::Sample, initializer list)'\r\ntensorflow/core/lib/monitoring/percentile_sampler.cc(76): error C2440: 'initializing': cannot convert from 'initializer list' to 'tensorflow::monitoring::PercentilePoint'\r\ntensorflow/core/lib/monitoring/percentile_sampler.cc(76): note: No constructor could take the source type, or constructor overload resolution was ambiguous", "This error can be removed by\r\n\r\n  struct Sample sample;\r\n  sample.nstime = nstime;\r\n  sample.sample = sample;\r\n  //samples_[next_position_] = {nstime, sample};\r\n  sample_[next_position_] = sample;\r\n\r\nHowever this is just a single problem\r\n\r\n**NEXT ERROR**\r\nERROR: C:/users/sr996/tensorflow/tensorflow/core/BUILD:2307:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 2)\r\n.\\tensorflow/core/util/bcast.h(131): error C2988: unrecognizable template declaration/definition\r\n.\\tensorflow/core/util/bcast.h(131): error C2059: syntax error: '('\r\ntensorflow/core/util/bcast.cc(21): error C2653: 'BCast': is not a class or namespace name\r\ntensorflow/core/util/bcast.cc(21): error C2146: syntax error: missing ';' before identifier 'FromShape'\r\ntensorflow/core/util/bcast.cc(21): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/util/bcast.cc(21): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/util/bcast.cc(30): error C2653: 'BCast': is not a class or namespace name\r\n", "And then there's this, and yes I set the BAZEL_VC environment variable\r\n\r\nERROR: C:/users/sr996/_bazel_sr996/g37aiu74/external/llvm-project/llvm/BUILD:3980:1: C++ compilation of rule '@llvm-project//llvm:text_api' failed (Exit 2)\r\nexternal/llvm-project/llvm/include\\llvm/Support/Compiler.h(88): fatal error C1189: #error:  LLVM requires at least MSVC 2017.", "Our CI has been able to build TF just fine both at master and 2.2 branches, so I am not sure what issues you are running into.\r\n\r\nThere is still a lot of information missing.\r\nDid you run `./configure`?\r\nI see that you added `--config=cuda` but you do not include which CUDA version you are trying to build with.\r\nWhich commit on r2.2 branch are you synced to?\r\nPlease include as much information as possible if you need help.", "It would appear that the root cause of the problem lies with how bazel generates the BUILD file.  My theory is that bazel generates the BUILD file on the initial run, then uses that for parameter initialization, ignoring the environment variable settings after that.  This situation would arise if you have multiple MSVC installations and inadvertently used the wrong one on the first run.  Subsequent changes to the BAZEL_VC variable seem to be ignored.  The solution seems to be to delete entirely the _bazel_%USERNAME% folder in your home directory, thus forcing bazel to re-build it from scratch and incorporate the current environment variable.", "Thank you so much for your help, it is greatly appreciated", "Yes, if anything changes in your environment, you definitely need to clear your bazel cache. You can do that manually, or you can run `bazel clean --expunge`\r\n\r\nWere you able to successfully build?", "Yes I was able to compile successfully, thank you :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38999\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38999\">No</a>\n"]}, {"number": 38998, "title": "saved keras model with custom layers supports ragged inputs cannot be loaded back", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF2.2 rc3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI created a custom keras Layer that accepts ragged inputs:\r\n\r\n```\r\nclass EmbeddingMean(tf.keras.layers.Layer):\r\n  def __init__(self, **kwargs):\r\n    super(EmbeddingMean, self).__init__()\r\n    self._supports_ragged_inputs = True\r\n  def call(self, inputs, **kwargs):\r\n    return tf.reduce_mean(inputs, axis=1)\r\n```\r\nIf i create a model like this:\r\n\r\n```\r\nfeature = Input(shape=(None,), ragged=True, name='input_1', dtype=tf.int32)\r\nembedded = Embedding(10, 3)(feature)\r\nembedded_mean = EmbeddingMean()(embedded)\r\nm = Model(feature, Dense(1)(embedded_mean))\r\n```\r\nAnd save the model as a keras model:\r\n`m.save('/tmp/test')`\r\n\r\nI cannot reload that model using \r\n`model_reloaded = tf.keras.models.load_model('/tmp/test')`\r\n> ValueError: Layer embedding_mean_1 does not support RaggedTensors as input. \r\n\r\nIf i save the model as a saved model it works correctly but i loose all the keras features.\r\n\r\n**Describe the expected behavior**\r\nWe should be able to load back the model as expected. \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/gist/tanguycdls/c9b1026bbea22cca4018139f447f249c/untitled7.ipynb\r\n\r\n**Additional Informations**\r\nThe issue seems to come from the fact that `self._supports_ragged_inputs` is not correctly saved: when we load the model the layer has the attribute set to False instead of True. ", "comments": ["Was able to reproduce the issue with TF v2.1, [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/883c9f51d1f7b6b8c119f115e914eb99/38998-2-2.ipynb#scrollTo=wjK4Jauu050A) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/a7d8e38707ef1af10cea71a0faa7736d/38998-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Hey @amahendrakar-\r\n\r\nWe're going to be removing the supports_ragged_inputs attr. This change has been submitted internally and should show up in the nightlies as soon as it is sync'd.", "This is fixed in latest tf-nightly [commit](862a62d6b4009ab29e666e2e5c6b498c1a8b68a5), `Layer._supports_ragged_inputs` property is removed. Thanks!", "Thanks @ymodak i checked and it works ! \r\nthank you for the prompt answer !\r\n ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38998\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38998\">No</a>\n"]}, {"number": 38997, "title": "checkpoint conversion to tf2.0 question", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nubuntu19.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\ntf2.1.0\r\n- Python version:\r\npython 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\nGCC 7.3.0\r\n- CUDA/cuDNN version:\r\n10.1/7.6.5\r\n- GPU model and memory:\r\nRTX2060S 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nHi, \r\nThis is just a general question\r\nI have a model which is written in tf1.x and the model is like this: \r\nhttps://github.com/CR-Ko/MegaDepth_Tensorflow/blob/master/hourglass_mega_tf_resize_bilinear_tflayer_prepost.py\r\nIt has checkpoint as well in tf1.x style. However, I have to rewrite it in TF2.1. For the rewrited 2.1 model, can I still use the TF1.X checkpoint? Or do I need use converter for the checkpoint?\r\nhttps://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/tools/checkpoint_converter.py\r\nAlso, about naming layers in rewrited 2.1 model, how should I do it to make sure correspondance between pretrain TF1.X checkpoint and rewrited TF2.1 model? \r\n", "comments": ["There's a mention at the end of tf.train.Checkpoint.restore's docstring: https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restore\r\n\r\nCheckpoints that came from Saver are matched using names, so you need to make sure Variable.name matches when loading one of those. It can be tricky to do, so the recommendation is to do it once and then save with Checkpoint.save so you get object-based matching in the future.\r\n\r\nFor more see the guide: https://www.tensorflow.org/guide/checkpoint"]}, {"number": 38996, "title": "[r2.2Cherrypick] Add CompositeTensor support to distributed iterators.", "body": null, "comments": ["I have tested the code on Multi Workers Mirrored Strategy. And it solved memory leak issue. I run it on AI Platform with Runtime 2.1", "closing this, the changes are now in this [PR](https://github.com/tensorflow/tensorflow/pull/39002)"]}, {"number": 38995, "title": "Object detection - Training of MobileNet-SSD", "body": "**System information**\r\n- OS Platform and Distribution (Windows 10 / OSX / Rapsbian):\r\n- TensorFlow installed from (source or binary): pip install tensorflow==1.5.0\r\n- TensorFlow version: 1.5.0\r\n- Python version: 3.6.1\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n\r\nHi everyone,\r\nI wanted your help to solve a problem with my project at the University. I want to recognize magpie with a Raspberry Pi to studied there behavior.\r\nI'm doing this tutorial at this part : https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial/\r\n\r\nI succeed every last command like : \r\n\r\n`python generate_tfrecord.py --csv_input=data/train_labels.csv --output_path=data/train.record`\r\n`python generate_tfrecord.py --csv_input=data/test_labels.csv --output_path=data/test.record`\r\n\r\nAnd I've my `train.record` et `test.record` well created in my data folder.\r\n\r\nBut after that, when I run the training, I've that error : \r\n\r\n```\r\nC:\\Users\\ALIENWARE\\Desktop\\workspace\\models\\research\\object_detection>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_custom.config\r\nWARNING:tensorflow:From C:\\Users\\ALIENWARE\\Desktop\\workspace\\models\\research\\object_detection\\trainer.py:210: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.create_global_step\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.\r\nWARNING:tensorflow:From C:\\Python\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\n2020-04-28 21:18:30.194654: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX\r\nINFO:tensorflow:Restoring parameters from training/model.ckpt-0\r\n2020-04-28 21:18:34.687701: W C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Out of range: Read fewer bytes than requested\r\n \r\n\u2026\r\n \r\n2020-04-28 21:18:34.692416: W C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Out of range: Read fewer bytes than requested\r\n2020-04-28 21:18:34.696681: W C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Out of range: Read fewer bytes than requested\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 164, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Python\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 160, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"C:\\Users\\ALIENWARE\\Desktop\\workspace\\models\\research\\object_detection\\trainer.py\", line 332, in train\r\n    saver=saver)\r\n  File \"C:\\Python\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py\", line 746, in train\r\n    master, start_standard_services=False, config=session_config) as sess:\r\n  File \"C:\\Python\\lib\\contextlib.py\", line 84, in __enter__\r\n    raise RuntimeError(\"generator didn't yield\") from None\r\nRuntimeError: generator didn't yield\r\n\r\n```\r\n\r\nI tried everything, impossible to have any correction of this. I tried this on my Macbook, on the Raspberry, on Windows and still nothing.\r\nI tried that with Python 3.5.3, 3.6.0, 3.6.1, with TensorFlow 1.4.0, 1.5.0, 1.10.0, 1.11.0, 1.12.0.\r\n\r\nIt is the final step of my project to pass my year, if someone can help me that would be amazing.\r\n\r\nAnthony T.\r\n", "comments": ["@AnthonyBarbier \r\nCan you please let us know why are you using an old version of tensor flow, and if possible use later version of tensor flow and let us know if you still face any issue.", "In fact I'm French, and that was better tutorial I found because there is nothing about TensorFlow in French. But now I founded a new tutorial and it's work.\r\nThanks for your reply"]}, {"number": 38994, "title": "Can't load saved keras model.h5", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2 & 10.1\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen loading the model with keras.models.load_model the activation functions are not recognized. This happens only when activation functions are not builtin strings\r\n\r\n**Describe the expected behavior**\r\nTo load the model with success\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nlayer = tf.keras.layers.Dense(1, activation=tf.keras.layers.PReLU(alpha_initializer='random_uniform', alpha_regularizer=None, alpha_constraint=None, shared_axes=None),\\\r\n                            name='layerX', kernel_initializer=tf.keras.initializers.he_normal())\r\nmodel = tf.keras.Sequential(layer)\r\nmodel.compile(\"adam\", \"binary_crossentropy\", [\"accuracy\"])\r\nmodel.fit([[1]], [1])\r\nmodel.save(\"keras_model.h5\", save_format='h5' \")\r\nmodel = tf.keras.models.load_model(\"keras_model.h5\")\r\n```\r\n**EDIT**: After reading the docs I saved the model with save_format h5, first time I tried without any format (the default is tf for tensorflow 2.0+)\r\nInstead I found a workaround: I can load the model if I save it with (default) save_format='tf'\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```Traceback (most recent call last):\r\n  File \"C:\\Users\\Teo\\OneDrive\\Licenta\\main.py\", line 138, in <module>\r\n    main()\r\n  File \"C:\\Users\\Teo\\OneDrive\\Licenta\\main.py\", line 135, in main\r\n    model = keras.models.load_model(os.path.join(dataset_path, \"keras_model.h5\"))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\", line 168, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\serialization.py\", line 106, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\", line 303, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 377, in from_config\r\n    custom_objects=custom_objects)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\serialization.py\", line 106, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\", line 305, in deserialize_keras_object\r\n    return cls.from_config(cls_config)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 519, in from_config\r\n    return cls(**config)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\", line 1082, in __init__\r\n    self.activation = activations.get(activation)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\activations.py\", line 450, in get\r\n    identifier, printable_module_name='activation')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\", line 292, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\", line 250, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown activation: PReLU\r\n```", "comments": ["@teodor440 \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "> \r\n> \r\n> @teodor440\r\n> \r\n> Request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nhttps://colab.research.google.com/drive/1qZ8RuI-oNoTsD2uokHZztqeCb20RX3wR\r\nI also updated the code in the issue", "I have tried in colab with TF 2.1.0 , 2.2-rc3 and was able to reproduce the issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d18890e6d91c6fe62dcff2934500927a/untitled840.ipynb).Thanks!", "@teodor440 I was able to reproduce the issue with `save_format='h5'`. However, when I used `save_format = 'tf'` everything worked as expected. `model.predict` before saving and after loading are exactly same. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/e753ca365a61c21d77bdd70844156f68/untitled840.ipynb). Thanks!", "I am not familiar with the implementation of tensorflow, but seems like there is a problem with how tf sees keras models\r\nI can recall the same value error when trying to convert the model to an estimator with  keras.estimator.model_to_estimator", "Ok so the problem was that I specified the activations in the layers as something different than a string. For some reason or another this confuses the operations made on keras models by tf", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38994\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38994\">No</a>\n", "@teodor440 You can use \"tf\" format as it is working well. However, this is still a bug with \"h5\" format. I will reopen the issue. Thanks!", "You can see now at the collab link that the issue with saving h5 model doesn't persist anymore if using separate layers for dense and activation. So basically this is a workaround for the problem", "Closing this issue since the associated PR has been merged and the issue is [fixed](https://colab.research.google.com/gist/ymodak/6217fe6a0850d369c7c8da9107821808/38994.ipynb). Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38994\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38994\">No</a>\n"]}, {"number": 38993, "title": "Reshape causing segmentation fault in TFLite Python interpreter", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 xenial\r\n- Kernel: x86_64 Linux 4.15.0-88-generic\r\n- Shell: zsh 5.1.1\r\n- CPU: Intel Core i7-7800X CPU @ 4GHz\r\n- GPU: GeForce GTX 1080 Ti, GeForce GTX 1080 Ti\r\n- TensorFlow installed from (source or binary): binary, via conda\r\n- TensorFlow version (or github SHA if from source): 1.15.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nBelow is a reproducable script that creates a very simple graph that reshapes `X` that segfaults on my and my collegues devices.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef _interpreter_predict(input_data: np.ndarray, interpreter: tf.lite.Interpreter):\r\n    \"\"\"Run inference on the interpreter.\"\"\"\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    input_tensor_index = input_details[0][\"index\"]\r\n    input_dtype = input_details[0][\"dtype\"]\r\n    output_tensor_index = output_details[0][\"index\"]\r\n    input_data = input_data.astype(input_dtype)\r\n    interpreter.set_tensor(input_tensor_index, input_data)\r\n    interpreter.invoke()\r\n    return interpreter.get_tensor(output_tensor_index)\r\n\r\n\r\n# Create a basic graph that reshapes the input (in lieu of an op such as expand dims)\r\ninput_shape = [1, 4096, 1]\r\nwith tf.Graph().as_default() as graph:\r\n    X = tf.placeholder(shape=input_shape, dtype=tf.float32, name=\"input\")\r\n    y = tf.reshape(X, [1, 1, 4096, 1], name=\"output\")\r\n    with tf.Session(graph=graph) as sess:\r\n        converter = tf.lite.TFLiteConverter.from_session(sess, [X], [y])\r\n        tflite_model = converter.convert()\r\n        with open(\"mvp.tflite\", \"wb\") as stream:\r\n            stream.write(tflite_model)\r\n\r\n# Load models.\r\nfloat32_model = tf.lite.Interpreter(model_path=\"mvp.tflite\")\r\n# Create data.\r\ninput_data = np.random.uniform(size=input_shape)\r\nactivations = _interpreter_predict(input_data, float32_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\nN/A - _it may be more appropraite to say this appear to be an issue with the Interpreter_\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\nI don't freeze the graph def, so haven't uploaded, but I'll include the tflite file `mvp.tflite` here (I've zipped it up because Github doesn't allow `.tflite` extensions to be attached)\r\n\r\n[mvp.tflite.zip](https://github.com/tensorflow/tensorflow/files/4548044/mvp.tflite.zip)\r\n\r\n**Failure details**\r\n\r\nSummary:\r\n* As mentioned, the conversion appears to be successful, but there are issues with the generated model.\r\n* The tflite flatbuffer is successfully loaded.\r\n* When the script gets to this line, (in `def _interpreter_predict`):\r\n\r\n```python\r\n    interpreter.set_tensor(input_tensor_index, input_data)\r\n```\r\n* We get a segfault.\r\n\r\n**Any other info / logs**\r\n\r\nI can also run the attached script with gdb, to inspect the stack with the command:\r\n```bash\r\n$ PYTHONMALLOC=malloc gdb -ex r --args python\r\n```\r\n\r\nThere is a lot of output **(please scroll to the end to see me inspect the stack)**:\r\n```\r\n(dev) \u279c  study_id=9156 PYTHONMALLOC=malloc gdb -ex r --args python mvp_segfault.py\r\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\r\nand \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-linux-gnu\".\r\nType \"show configuration\" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n<http://www.gnu.org/software/gdb/documentation/>.\r\nFor help, type \"help\".\r\nType \"apropos word\" to search for commands related to \"word\"...\r\nReading symbols from python...done.\r\nStarting program: /home/leonfedden/miniconda3/envs/dev/bin/python mvp_segfault.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7fff9a755700 (LWP 12246)]\r\nWARNING:tensorflow:From mvp_segfault.py:20: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From mvp_segfault.py:22: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2020-04-28 20:30:27.108799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n[New Thread 0x7fff98b7a700 (LWP 12249)]\r\n2020-04-28 20:30:27.141296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:17:00.0\r\n2020-04-28 20:30:27.141774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:65:00.0\r\n2020-04-28 20:30:27.155220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-28 20:30:27.167354: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-28 20:30:27.175155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-04-28 20:30:27.180762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-04-28 20:30:27.190838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-04-28 20:30:27.199906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-04-28 20:30:27.211992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-28 20:30:27.217731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2020-04-28 20:30:27.218362: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n[New Thread 0x7fff835b0700 (LWP 12250)]\r\n[New Thread 0x7fff82daf700 (LWP 12251)]\r\n[New Thread 0x7fff825ae700 (LWP 12252)]\r\n[New Thread 0x7fff81dad700 (LWP 12253)]\r\n[New Thread 0x7fff815ac700 (LWP 12254)]\r\n[New Thread 0x7fff80dab700 (LWP 12255)]\r\n[New Thread 0x7fff2bfff700 (LWP 12256)]\r\n[New Thread 0x7fff2b7fe700 (LWP 12257)]\r\n[New Thread 0x7fff2affd700 (LWP 12258)]\r\n[New Thread 0x7fff2a7fc700 (LWP 12259)]\r\n[New Thread 0x7fff29ffb700 (LWP 12260)]\r\n[New Thread 0x7fff297fa700 (LWP 12261)]\r\n[New Thread 0x7fff28ff9700 (LWP 12262)]\r\n[New Thread 0x7fff13fff700 (LWP 12263)]\r\n[New Thread 0x7fff137fe700 (LWP 12264)]\r\n2020-04-28 20:30:27.246123: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3500000000 Hz\r\n[Thread 0x7fff13fff700 (LWP 12263) exited]\r\n[New Thread 0x7fff13fff700 (LWP 12265)]\r\n[New Thread 0x7fff12ffd700 (LWP 12266)]\r\n[New Thread 0x7fff127fc700 (LWP 12267)]\r\n[New Thread 0x7fff11ffb700 (LWP 12268)]\r\n[New Thread 0x7fff117fa700 (LWP 12269)]\r\n[New Thread 0x7fff10ff9700 (LWP 12270)]\r\n[New Thread 0x7ffeeffff700 (LWP 12271)]\r\n[New Thread 0x7ffeef7fe700 (LWP 12272)]\r\n[New Thread 0x7ffeeeffd700 (LWP 12273)]\r\n[New Thread 0x7ffeee7fc700 (LWP 12274)]\r\n[New Thread 0x7ffeedffb700 (LWP 12275)]\r\n[New Thread 0x7ffeed7fa700 (LWP 12276)]\r\n2020-04-28 20:30:27.248566: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55555e3af020 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-28 20:30:27.248605: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n[New Thread 0x7ffeecff9700 (LWP 12277)]\r\n[New Thread 0x7ffecffff700 (LWP 12278)]\r\n[New Thread 0x7ffecf7fe700 (LWP 12279)]\r\n[New Thread 0x7ffeceffd700 (LWP 12280)]\r\n[New Thread 0x7ffece7fc700 (LWP 12281)]\r\n[New Thread 0x7ffecdffb700 (LWP 12282)]\r\n[New Thread 0x7ffecd7fa700 (LWP 12283)]\r\n2020-04-28 20:30:27.453872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:17:00.0\r\n2020-04-28 20:30:27.454308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:65:00.0\r\n2020-04-28 20:30:27.454346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-28 20:30:27.454355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-28 20:30:27.454365: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-04-28 20:30:27.454374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-04-28 20:30:27.454383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-04-28 20:30:27.454392: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-04-28 20:30:27.454400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-28 20:30:27.455918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2020-04-28 20:30:27.455949: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-28 20:30:27.457102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-28 20:30:27.457113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1\r\n2020-04-28 20:30:27.457119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y\r\n2020-04-28 20:30:27.457124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N\r\n2020-04-28 20:30:27.458736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10479 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n[New Thread 0x7ffeccff9700 (LWP 12284)]\r\n[New Thread 0x7ffec5fff700 (LWP 12285)]\r\n2020-04-28 20:30:27.460287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10322 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n[New Thread 0x7ffec57fe700 (LWP 12286)]\r\n[New Thread 0x7ffec4ffd700 (LWP 12287)]\r\n[New Thread 0x7ffeb1fff700 (LWP 12288)]\r\n[New Thread 0x7ffeb17fe700 (LWP 12289)]\r\n[New Thread 0x7ffeb0ffd700 (LWP 12290)]\r\n[Thread 0x7ffeb17fe700 (LWP 12289) exited]\r\n2020-04-28 20:30:27.462897: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55555f27b300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-28 20:30:27.462912: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-04-28 20:30:27.462918: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n[Thread 0x7ffeb0ffd700 (LWP 12290) exited]\r\n[New Thread 0x7ffeb0ffd700 (LWP 12291)]\r\n[New Thread 0x7ffeb17fe700 (LWP 12292)]\r\n[New Thread 0x7ffe99fff700 (LWP 12293)]\r\n[New Thread 0x7ffe997fe700 (LWP 12294)]\r\n[New Thread 0x7ffe98ffd700 (LWP 12295)]\r\n[New Thread 0x7ffe73fff700 (LWP 12296)]\r\n[New Thread 0x7ffe737fe700 (LWP 12297)]\r\n[New Thread 0x7ffe72ffd700 (LWP 12298)]\r\n[New Thread 0x7ffe727fc700 (LWP 12299)]\r\n[New Thread 0x7ffe71ffb700 (LWP 12300)]\r\n[New Thread 0x7ffe717fa700 (LWP 12301)]\r\n[New Thread 0x7ffe70ff9700 (LWP 12302)]\r\n[New Thread 0x7ffe4ffff700 (LWP 12303)]\r\n2020-04-28 20:30:27.468970: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n[New Thread 0x7ffe4f7fe700 (LWP 12304)]\r\n[New Thread 0x7ffe4effd700 (LWP 12305)]\r\n2020-04-28 20:30:27.469634: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n[New Thread 0x7ffe4e7fc700 (LWP 12306)]\r\n[New Thread 0x7ffe4dffb700 (LWP 12307)]\r\n[Thread 0x7ffe4f7fe700 (LWP 12304) exited]\r\n[Thread 0x7ffe4effd700 (LWP 12305) exited]\r\n[New Thread 0x7ffe4effd700 (LWP 12308)]\r\n[New Thread 0x7ffe4f7fe700 (LWP 12309)]\r\n2020-04-28 20:30:27.474639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:17:00.0\r\n2020-04-28 20:30:27.475820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:65:00.0\r\n2020-04-28 20:30:27.475881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-28 20:30:27.475907: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-28 20:30:27.475931: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-04-28 20:30:27.475954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-04-28 20:30:27.475977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-04-28 20:30:27.476000: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-04-28 20:30:27.476023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-28 20:30:27.480329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2020-04-28 20:30:27.480424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-28 20:30:27.480442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1\r\n2020-04-28 20:30:27.480457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y\r\n2020-04-28 20:30:27.480469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N\r\n2020-04-28 20:30:27.485406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10479 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-04-28 20:30:27.486411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10322 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n[New Thread 0x7ffe4d7fa700 (LWP 12310)]\r\n[New Thread 0x7ffe4cff9700 (LWP 12311)]\r\n[New Thread 0x7ffe2ffff700 (LWP 12312)]\r\n[New Thread 0x7ffe2f7fe700 (LWP 12313)]\r\n[New Thread 0x7ffe2effd700 (LWP 12314)]\r\n[New Thread 0x7ffe2e7fc700 (LWP 12315)]\r\n[New Thread 0x7ffe2dffb700 (LWP 12316)]\r\n[New Thread 0x7ffe2d7fa700 (LWP 12317)]\r\n[New Thread 0x7ffe2cff9700 (LWP 12318)]\r\n[New Thread 0x7ffe0ffff700 (LWP 12319)]\r\n[New Thread 0x7ffe0f7fe700 (LWP 12320)]\r\n[New Thread 0x7ffe0effd700 (LWP 12321)]\r\n[New Thread 0x7ffe0e7fc700 (LWP 12322)]\r\n[New Thread 0x7ffe0dffb700 (LWP 12323)]\r\n[New Thread 0x7ffe0d7fa700 (LWP 12324)]\r\n[Thread 0x7ffe4e7fc700 (LWP 12306) exited]\r\n[Thread 0x7ffe4dffb700 (LWP 12307) exited]\r\n[Thread 0x7ffe0d7fa700 (LWP 12324) exited]\r\n[Thread 0x7ffe0dffb700 (LWP 12323) exited]\r\n[Thread 0x7ffe2f7fe700 (LWP 12313) exited]\r\n[Thread 0x7ffe0e7fc700 (LWP 12322) exited]\r\n[Thread 0x7ffe2ffff700 (LWP 12312) exited]\r\n[Thread 0x7ffe2cff9700 (LWP 12318) exited]\r\n[Thread 0x7ffe2effd700 (LWP 12314) exited]\r\n[Thread 0x7ffe0effd700 (LWP 12321) exited]\r\n[Thread 0x7ffe2e7fc700 (LWP 12315) exited]\r\n[Thread 0x7ffe0f7fe700 (LWP 12320) exited]\r\n[Thread 0x7ffe0ffff700 (LWP 12319) exited]\r\n[Thread 0x7ffe2d7fa700 (LWP 12317) exited]\r\n[Thread 0x7ffe2dffb700 (LWP 12316) exited]\r\n[Thread 0x7ffe4effd700 (LWP 12308) exited]\r\n[Thread 0x7ffe4d7fa700 (LWP 12310) exited]\r\n[Thread 0x7ffe4cff9700 (LWP 12311) exited]\r\n2020-04-28 20:30:27.502006: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\r\n[New Thread 0x7ffe4cff9700 (LWP 12325)]\r\n[New Thread 0x7ffe4d7fa700 (LWP 12326)]\r\n2020-04-28 20:30:27.502577: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n[New Thread 0x7ffe4effd700 (LWP 12327)]\r\n[New Thread 0x7ffe0dffb700 (LWP 12328)]\r\n[Thread 0x7ffe4d7fa700 (LWP 12326) exited]\r\n[Thread 0x7ffe4cff9700 (LWP 12325) exited]\r\n[New Thread 0x7ffe4d7fa700 (LWP 12329)]\r\n2020-04-28 20:30:27.507610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:17:00.0\r\n2020-04-28 20:30:27.508716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:65:00.0\r\n2020-04-28 20:30:27.508769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-28 20:30:27.508794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-28 20:30:27.508817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-04-28 20:30:27.508838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-04-28 20:30:27.508860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-04-28 20:30:27.508881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-04-28 20:30:27.508903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-28 20:30:27.515729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2020-04-28 20:30:27.515825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-28 20:30:27.515844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1\r\n2020-04-28 20:30:27.515858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y\r\n2020-04-28 20:30:27.515871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N\r\n2020-04-28 20:30:27.519266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10479 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2020-04-28 20:30:27.520407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10322 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n[New Thread 0x7ffe4cff9700 (LWP 12330)]\r\n[New Thread 0x7ffe4e7fc700 (LWP 12331)]\r\n[New Thread 0x7ffe4dffb700 (LWP 12332)]\r\n[New Thread 0x7ffe2ffff700 (LWP 12333)]\r\n[New Thread 0x7ffe2f7fe700 (LWP 12334)]\r\n[New Thread 0x7ffe2effd700 (LWP 12335)]\r\n[New Thread 0x7ffe2e7fc700 (LWP 12336)]\r\n[New Thread 0x7ffe2dffb700 (LWP 12337)]\r\n[New Thread 0x7ffe2d7fa700 (LWP 12338)]\r\n[New Thread 0x7ffe2cff9700 (LWP 12339)]\r\n[New Thread 0x7ffe0ffff700 (LWP 12340)]\r\n[New Thread 0x7ffe0f7fe700 (LWP 12341)]\r\n[New Thread 0x7ffe0effd700 (LWP 12342)]\r\n[New Thread 0x7ffe0e7fc700 (LWP 12343)]\r\n[New Thread 0x7ffe0d7fa700 (LWP 12344)]\r\n[Thread 0x7ffe0dffb700 (LWP 12328) exited]\r\n[Thread 0x7ffe4effd700 (LWP 12327) exited]\r\n[Thread 0x7ffe0d7fa700 (LWP 12344) exited]\r\n[Thread 0x7ffe0e7fc700 (LWP 12343) exited]\r\n[Thread 0x7ffe2f7fe700 (LWP 12334) exited]\r\n[Thread 0x7ffe0effd700 (LWP 12342) exited]\r\n[Thread 0x7ffe2ffff700 (LWP 12333) exited]\r\n[Thread 0x7ffe0f7fe700 (LWP 12341) exited]\r\n[Thread 0x7ffe4dffb700 (LWP 12332) exited]\r\n[Thread 0x7ffe2cff9700 (LWP 12339) exited]\r\n[Thread 0x7ffe2effd700 (LWP 12335) exited]\r\n[Thread 0x7ffe0ffff700 (LWP 12340) exited]\r\n[Thread 0x7ffe2e7fc700 (LWP 12336) exited]\r\n[Thread 0x7ffe2dffb700 (LWP 12337) exited]\r\n[Thread 0x7ffe2d7fa700 (LWP 12338) exited]\r\n[Thread 0x7ffe4d7fa700 (LWP 12329) exited]\r\n[Thread 0x7ffe4cff9700 (LWP 12330) exited]\r\n[Thread 0x7ffe4e7fc700 (LWP 12331) exited]\r\n[Thread 0x7fff28ff9700 (LWP 12262) exited]\r\n[Thread 0x7ffeb1fff700 (LWP 12288) exited]\r\n[Thread 0x7ffeb0ffd700 (LWP 12291) exited]\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n__memmove_avx_unaligned () at ../sysdeps/x86_64/multiarch/memcpy-avx-unaligned.S:238\r\n238     ../sysdeps/x86_64/multiarch/memcpy-avx-unaligned.S: No such file or directory.\r\n```\r\n\r\nNote it finishes with:\r\n```\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n__memmove_avx_unaligned () at ../sysdeps/x86_64/multiarch/memcpy-avx-unaligned.S:238\r\n238     ../sysdeps/x86_64/multiarch/memcpy-avx-unaligned.S: No such file or directory.\r\n```\r\n\r\nI then (in gdb) call backtrace/bt which gives me this:\r\n```\r\n(gdb) bt\r\n#0  __memmove_avx_unaligned () at ../sysdeps/x86_64/multiarch/memcpy-avx-unaligned.S:238\r\n#1  0x00007fff480e3f4d in tflite::interpreter_wrapper::InterpreterWrapper::SetTensor(int, _object*) () from /home/leonfedden/miniconda3/envs/dev/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#2  0x00007fff480e1dde in _wrap_InterpreterWrapper_SetTensor () from /home/leonfedden/miniconda3/envs/dev/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so\r\n#3  0x00005555556bda30 in _PyMethodDef_RawFastCallKeywords () at /tmp/build/80754af9/python_1585235023510/work/Objects/call.c:698\r\n#4  0x00005555556bdbd1 in _PyCFunction_FastCallKeywords (func=0x55555f21aa30, args=<optimised out>, nargs=<optimised out>, kwnames=<optimised out>) at /tmp/build/80754af9/python_1585235023510/work/Objects/call.c:734\r\n#5  0x000055555572457b in call_function (kwnames=0x0, oparg=3, pp_stack=<synthetic pointer>) at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:4568\r\n#6  _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:3093\r\n#7  0x00005555556bd02b in function_code_fastcall (globals=<optimised out>, nargs=3, args=<optimised out>, co=<optimised out>) at /tmp/build/80754af9/python_1585235023510/work/Objects/call.c:283\r\n#8  _PyFunction_FastCallKeywords () at /tmp/build/80754af9/python_1585235023510/work/Objects/call.c:408\r\n#9  0x00005555557241e9 in call_function (kwnames=0x0, oparg=<optimised out>, pp_stack=<synthetic pointer>) at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:4616\r\n#10 _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:3093\r\n#11 0x00005555556bd02b in function_code_fastcall (globals=<optimised out>, nargs=3, args=<optimised out>, co=<optimised out>) at /tmp/build/80754af9/python_1585235023510/work/Objects/call.c:283\r\n#12 _PyFunction_FastCallKeywords () at /tmp/build/80754af9/python_1585235023510/work/Objects/call.c:408\r\n#13 0x000055555571fd40 in call_function (kwnames=0x0, oparg=<optimised out>, pp_stack=<synthetic pointer>) at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:4616\r\n#14 _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:3110\r\n#15 0x00005555556bd02b in function_code_fastcall (globals=<optimised out>, nargs=2, args=<optimised out>, co=<optimised out>) at /tmp/build/80754af9/python_1585235023510/work/Objects/call.c:283\r\n#16 _PyFunction_FastCallKeywords () at /tmp/build/80754af9/python_1585235023510/work/Objects/call.c:408\r\n#17 0x000055555571fac6 in call_function (kwnames=0x0, oparg=<optimised out>, pp_stack=<synthetic pointer>) at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:4616\r\n#18 _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:3124\r\n#19 0x0000555555669389 in _PyEval_EvalCodeWithName () at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:3930\r\n#20 0x000055555566a2b4 in PyEval_EvalCodeEx () at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:3959\r\n#21 0x000055555566a2dc in PyEval_EvalCode (co=<optimised out>, globals=<optimised out>, locals=<optimised out>) at /tmp/build/80754af9/python_1585235023510/work/Python/ceval.c:524\r\n#22 0x0000555555780664 in run_mod () at /tmp/build/80754af9/python_1585235023510/work/Python/pythonrun.c:1035\r\n#23 0x000055555578aa91 in PyRun_FileExFlags () at /tmp/build/80754af9/python_1585235023510/work/Python/pythonrun.c:988\r\n#24 0x000055555578ac83 in PyRun_SimpleFileExFlags () at /tmp/build/80754af9/python_1585235023510/work/Python/pythonrun.c:429\r\n#25 0x000055555578bdb5 in pymain_run_file (p_cf=0x7fffffffdea0, filename=0x5555558c3430 L\"mvp_segfault.py\", fp=0x555555a8b6a0) at /tmp/build/80754af9/python_1585235023510/work/Modules/main.c:462\r\n#26 pymain_run_filename (cf=0x7fffffffdea0, pymain=0x7fffffffdfb0) at /tmp/build/80754af9/python_1585235023510/work/Modules/main.c:1641\r\n#27 pymain_run_python (pymain=0x7fffffffdfb0) at /tmp/build/80754af9/python_1585235023510/work/Modules/main.c:2902\r\n#28 pymain_main () at /tmp/build/80754af9/python_1585235023510/work/Modules/main.c:3442\r\n#29 0x000055555578bedc in _Py_UnixMain () at /tmp/build/80754af9/python_1585235023510/work/Modules/main.c:3477\r\n#30 0x00007ffff7810830 in __libc_start_main (main=0x55555564a1f0 <main>, argc=2, argv=0x7fffffffe108, init=<optimised out>, fini=<optimised out>, rtld_fini=<optimised out>, stack_end=0x7fffffffe0f8) at ../csu/libc-start.c:291\r\n#31 0x000055555572f3e0 in _start () at ../sysdeps/x86_64/elf/start.S:103\r\n```\r\n\r\n", "comments": ["Hey there, looks like there was a bug in my script. In TF2.2 there is now a helpful error that complains when `interpreter.allocate_tensors()` is not called before calling `interpreter.set_tensor()`, whereas in TF1.15.0 it was a little more of a silent fail. \r\n\r\nThanks for adding the error message in the future version of TF"]}, {"number": 38992, "title": "TFLMicro mul kernel input shapes broadcasting", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2 (master)\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\ntensorflow/lite/kernels/mul.cc kernel does not appear to be working with inputs that need broadcasting. An example pair of input shapes is (1, 1, 13) and (13,). Here is a test case that fails:\r\n\r\n```\r\nTF_LITE_MICRO_TEST(FloatBroadcast2) {\r\n  float output_data[6];\r\n  tflite::testing::TestMulFloat(\r\n      {2, 1, 6},                      // input1 shape\r\n      {-2.0, 0.2, 0.7, 0.8, 1.1, 2.0},      // input1 data\r\n      {1, 6},                               // input2 shape\r\n      {0.1,0.1,0.1,0.1,0.1,0.1},                                // input2 data\r\n      {2, 1, 6},                      // output shape\r\n      {-0.2, 0.02, 0.07, 0.08, 0.11, 0.2},  // expected output data\r\n      output_data, kTfLiteActNone);\r\n}\r\n```\r\n\r\n```\r\n> make -f tensorflow/lite/micro/tools/make/Makefile test_kernel_mul_test\r\ntensorflow/lite/micro/tools/make/Makefile:280: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:280: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/testing/test_linux_binary.sh tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/kernel_mul_test '~~~ALL TESTS PASSED~~~'\r\ntensorflow/lite/micro/testing/test_linux_binary.sh: line 46: 22443 Abort trap: 6           $1 > ${MICRO_LOG_FILENAME} 2>&1\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:340: test_kernel_mul_test] Error 134\r\n```\r\n**Describe the expected behavior**\r\nThe kernel is supposed to broadcast inputs if necessary. \r\n\r\n**Other info / logs** \r\nIt appears that the problem is in `ProcessBroadcastShapes()`. If in `EvalFloat()` in mul.cc, I force broadcast using `TF_LITE_MUL(BroadcastMul4DSlow)`, I get correct results.  \r\n", "comments": ["Hi @Abhipray ! \r\nWe are checking to see whether you still need help in this issue . Have you checked in latest TF versions yet(TF 2.6)  yet . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38992\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38992\">No</a>\n"]}, {"number": 38991, "title": "Speech Command Recognition example crashes with GPUDelegate", "body": "Tensorflow lite issue:\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes:\r\n\r\n\r\nIn SpeechActivity when the interpreter is created:\r\n(https://github.com/tensorflow/examples/blob/master/lite/examples/speech_commands/android/app/src/main/java/org/tensorflow/lite/examples/speech/SpeechActivity.java line: 182)\r\n\r\n(deprecated)\r\ntfLite = new Interpreter(loadModelFile(getAssets(), actualModelFilename));\r\n\r\nI change to:\r\n      Interpreter.Options options = new Interpreter.Options();\r\n      options.addDelegate(new GpuDelegate());\r\n      options.setNumThreads(1);\r\n      tfLite = new Interpreter(loadModelFile(getAssets(), actualModelFilename), options);\r\n\r\n**Standalone code to reproduce the issue**\r\nI have an instrumented test that reproduces the bug, I will attach it\r\n\r\n**Crashes with GpuDelegate, does not crash with NnapiDelegate**\r\n\r\n(Change to .java for use, also include the proper dependencies in gradle, and create the proper directories for gradle to find and build out test: androidtest->java-> etc)\r\n[SpeechTest.txt](https://github.com/tensorflow/tensorflow/files/4547752/SpeechTest.txt)\r\n", "comments": ["can you attach the tflite model?", "There are two ways of obtaining the model as described on the readme page. Through the download_model.gradle, or manually downloading it via the link.\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android\r\n\r\nBut just for clarity I will zip the one specifically in my project\r\n\r\n[conv_actions_frozen.zip](https://github.com/tensorflow/tensorflow/files/4561286/conv_actions_frozen.zip)\r\n\r\n", "bump", "Thank you for the attached model.  `AudioSpectrogram` & `Mfcc` is not an op that exists in the GPU delegate.  What's puzzling to me is that the tensor dimensions not matching.  Currently, most of the input/output tensors of each op are scalar without tensor dimensions.  For TFLite GPU, tensor shapes need to be well-defined.\r\n", "Hi @impjdi,\r\nThanks for the reply. I have some questions regarding this information. \r\n1) How do you know which operations are supported by GPU delegate, is there a list?\r\n2) How do you obtain information regarding each op in the graph. aka, its shape/dimensions?", "1. `//tensorflow/lite/delegates/gpu/common/model_builder.cc` has the information you're looking for.\r\n\r\n2. We don't obtain it from the graph but from the tensor.  If you look at the definition of `struct TfLiteTensor`, that information is embedded there.  If that information is missing, it's considered a scalar (which is incorrect).  I am not familiar with the CPU execution and how it determines the shapes; GPU requires them to be set in advance.", "Thanks for the info, I appreciate it.\r\n\r\nWhen creating an interpreter with the Gpudelegate option with a model the gpu delegate doesnt support, would it be possible to throw an exception, instead of crashing the process and generating a tombstone... then that exception can be caught, and an interpreter can be created on the cpu, or NnapiDelegate.\r\n  \r\nSomething like \"OperationNotSupportedException\" ", "Absolutely.  It's not supposed to crash, and if it does, it's a bug on our end.", "@srjoglekar246 \r\n\r\nCan you assign this to the Java API owner to take a look so that it doesn't crash, but rather throws an exception?", "@amitDaMan The Interpreter shouldn't crash if the delegate does not support one or more ops. It just delegates as many ops as it can, and runs the rest on CPU. Can you provide details of what error you see? Or where it is originating?\r\n\r\nThrowing an exception for unsupported ops is a little tricky, since many models (like the SSD) do contain some ops that should be run on CPU - while the rest are handled by GPU (which is expected).", "Hi @srjoglekar246,\r\n\r\nTo reiterate above, the issue is regarding the Speech Commands example app:\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/speech_commands/android/README.md\r\n\r\nThe model in question takes in two inputs:\r\nfloat[][] floatInputBuffer = new float[RECORDING_LENGTH][1]; //recording length is 16000\r\nint[] sampleRateList = new int[] {SAMPLE_RATE}; //Sample Rate is 16000\r\n\r\nand according to @impjdi: \r\nincludes AudioSpectrogram & Mfcc which are unsupported by the Gpudelegate\r\n\r\nIn the app SpeechActivity:182:\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/speech_commands/android/app/src/main/java/org/tensorflow/lite/examples/speech/SpeechActivity.java\r\n\r\n`tfLite = new Interpreter(loadModelFile(getAssets(), actualModelFilename));\r\n`\r\nI replace that line with:\r\n\r\n```\r\nInterpreter.Options opts = new Interpreter.Options();\r\nopts.addDelegate(new GpuDelegate());\r\nopts.setNumThreads(1);\r\ntfLite = new Interpreter(loadModelFile(getAssets(), actualModelFilename), opts);\r\n```\r\nAnd it crashes.\r\n\r\nNote that it will work for NNAPI:\r\n```\r\nInterpreter.Options opts = new Interpreter.Options();\r\nopts.addDelegate(new NnApiDelegate());\r\nopts.setNumThreads(1);\r\ntfLite = new Interpreter(loadModelFile(getAssets(), actualModelFilename), opts);\r\n```\r\n\r\nAs well as specifying no delegate:\r\n```\r\nInterpreter.Options opts = new Interpreter.Options();\r\nopts.setNumThreads(1);\r\ntfLite = new Interpreter(loadModelFile(getAssets(), actualModelFilename), opts);\r\n```\r\nAs for the crash:\r\nwhen it gets to:\r\n`tfLite = new Interpreter(loadModelFile(getAssets(), actualModelFilename), opts);`\r\n\r\nlogcat will begin to output:\r\n--------- beginning of crash\r\n09-25 12:10:16.722 21335 21335 F libc    : Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x30 in tid 21335 (examples.speech), pid 21335 (examples.speech)\r\n09-25 12:10:16.984 21410 21410 I crash_dump64: obtaining output fd from tombstoned, type: kDebuggerdTombstone\r\n09-25 12:10:16.986  1300  1300 I /system/bin/tombstoned: received crash request for pid 21335\r\n09-25 12:10:16.988 21410 21410 I crash_dump64: performing dump of process 21335 (target tid = 21335)\r\n09-25 12:10:17.018 21410 21410 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n09-25 12:10:17.018 21410 21410 F DEBUG   : Build fingerprint: 'qti/kona/kona:10/QKQ1.190918.001/eng.lnxbui.20190924.230323:userdebug/test-keys'\r\n09-25 12:10:17.018 21410 21410 F DEBUG   : Revision: '0'\r\n09-25 12:10:17.018 21410 21410 F DEBUG   : ABI: 'arm64'\r\n09-25 12:10:17.020 21410 21410 F DEBUG   : Timestamp: 2019-09-25 12:10:17+0000\r\n09-25 12:10:17.020 21410 21410 F DEBUG   : pid: 21335, tid: 21335, name: examples.speech  >>> org.tensorflow.lite.examples.speech <<<\r\n09-25 12:10:17.020 21410 21410 F DEBUG   : uid: 10206\r\n09-25 12:10:17.020 21410 21410 F DEBUG   : signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x30\r\n09-25 12:10:17.020 21410 21410 F DEBUG   : Cause: null pointer dereference\r\n09-25 12:10:17.020 21410 21410 F DEBUG   :     x0  00000077d889b950  x1  00000078c7e62a0c  x2  0000000000000000  x3  0000007fd42cb9d0\r\n09-25 12:10:17.020 21410 21410 F DEBUG   :     x4  0000000000000003  x5  0000000000000000  x6  0000000000000000  x7  0000000000000000\r\n09-25 12:10:17.020 21410 21410 F DEBUG   :     x8  0000007fd42cb938  x9  0000000000000030  x10 0000007831a23d68  x11 00000000ffffffff\r\n09-25 12:10:17.021 21410 21410 F DEBUG   :     x12 0000000000000000  x13 fffffffffc000000  x14 0000000000000010  x15 0000000400000000\r\n09-25 12:10:17.021 21410 21410 F DEBUG   :     x16 00000078c3b138f0  x17 00000078c3b05070  x18 0000000000000000  x19 0000007fd42cb938\r\n09-25 12:10:17.021 21410 21410 F DEBUG   :     x20 0000000000000003  x21 0000000000000000  x22 0000007fd42cb698  x23 00000078c7e62a00\r\n09-25 12:10:17.021 21410 21410 F DEBUG   :     x24 00000078c7fc2020  x25 00000077d889b950  x26 0000000000000003  x27 00000078c7fc2020\r\n09-25 12:10:17.021 21410 21410 F DEBUG   :     x28 0000000000000070  x29 0000007fd42cb4a0\r\n09-25 12:10:17.021 21410 21410 F DEBUG   :     sp  0000007fd42cb3c0  lr  00000077d750ca64  pc  00000077d750e804\r\n09-25 12:10:17.150 21410 21410 F DEBUG   :\r\n09-25 12:10:17.150 21410 21410 F DEBUG   : backtrace:\r\n09-25 12:10:17.150 21410 21410 F DEBUG   :       #00 pc 00000000000e8804  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_gpu_jni.so\r\n09-25 12:10:17.150 21410 21410 F DEBUG   :       #01 pc 00000000000e6a60  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_gpu_jni.so\r\n09-25 12:10:17.150 21410 21410 F DEBUG   :       #02 pc 00000000000d8a88  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_gpu_jni.so\r\n09-25 12:10:17.150 21410 21410 F DEBUG   :       #03 pc 00000000000d960c  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_gpu_jni.so\r\n09-25 12:10:17.150 21410 21410 F DEBUG   :       #04 pc 000000000001d8cc  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_gpu_jni.so\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #05 pc 000000000001cfb0  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_gpu_jni.so\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #06 pc 0000000000147bf0  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_jni.so\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #07 pc 0000000000147854  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_jni.so\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #08 pc 0000000000147598  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_jni.so\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #09 pc 000000000001ce94  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_gpu_jni.so\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #10 pc 00000000001497c0  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_jni.so\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #11 pc 000000000014cc40  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_jni.so\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #12 pc 000000000000f6ec  /data/app/org.tensorflow.lite.examples.speech-w2DgG080sdMY8xFF-l2Xbg==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_applyDelegate+40)\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #13 pc 000000000013f350  /apex/com.android.runtime/lib64/libart.so (art_quick_generic_jni_trampoline+144) (BuildId: 41b9f73e55dd3e009e7d2d5c956d40fe)\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #14 pc 000000000013f65c  /apex/com.android.runtime/lib64/libart.so (art_quick_instrumentation_entry+252) (BuildId: 41b9f73e55dd3e009e7d2d5c956d40fe)\r\n09-25 12:10:17.151 21410 21410 F DEBUG   :       #15 pc 00000000007f700c  [stack]\r\n09-25 12:10:17.555   893 21417 W ActivityTaskManager:   Force finishing activity org.tensorflow.lite.examples.speech/.SpeechActivity\r\n09-25 12:10:17.558  1300  1300 E /system/bin/tombstoned: Tombstone written to: /data/tombstones/tombstone_26\r\n0", "Also, if it's not suppose to crash, and suppose to split the graph and continue to work as intended, that is fine.\r\nSome debugging messages might be nice, for example, if I could set a property:\r\nadb shell setprop debug.gpu on\r\n\r\nAnd then in the logcat it would tell me which ops are not being supported on the gpu\r\n\r\nand when the property is set to off it won't display anything. ", "@lintian06 Tian could you take a look? The model in the README works on GPU with the benchmark tool, but the app is crashing.", "bump", "@lintian06  any sort of progress/input?", "and another ping, just for good measure", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38991\">No</a>\n"]}]