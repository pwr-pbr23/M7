[{"number": 49067, "title": "Prevent heap OOB error in `MaxPoolGrad`", "body": "PiperOrigin-RevId: 372424854\nChange-Id: Idac0f23867ad8b0601cafbaaa52d5e64269e63a7", "comments": []}, {"number": 49066, "title": "Fix SEGV in CTC ops", "body": "PiperOrigin-RevId: 372430279\nChange-Id: I7ec2ad9d6f4d0980c33de45d27c6b17df5c6e26f", "comments": []}, {"number": 49065, "title": "Validate arguments of `FractionalMaxPoolGrad`", "body": "PiperOrigin-RevId: 372274982\nChange-Id: If46b0c442efa4eaef635ce6a476717060420122c", "comments": []}, {"number": 49064, "title": "Validate inputs of `FractionalAvgPoolGrad`.", "body": "PiperOrigin-RevId: 372420640\nChange-Id: Icc583928e6cdc3062e12498e4d2337a8fe3da016", "comments": []}, {"number": 49063, "title": "Fix assertion failure in pooling_ops_3d", "body": "PiperOrigin-RevId: 372364504\nChange-Id: Iecde4fe26b47a8fa935d6e2611b5585ed5777781", "comments": []}, {"number": 49062, "title": "Fix OOB read issue with `tf.raw_ops.CTCLoss`.", "body": "PiperOrigin-RevId: 372242187\nChange-Id: I347228ed8c04e1d2eb9d2479ae52f51d1b512c6e", "comments": []}, {"number": 49061, "title": "Fix nullptr deref in `tf.raw_ops.CTCLoss`.", "body": "PiperOrigin-RevId: 372266334\nChange-Id: Ic52c3e9f13a38f54482d670907eda1688450862b", "comments": []}, {"number": 49060, "title": "[CherryPick 2.5] Add missing validation to pooling_ops_3d and prevent heap OOB", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49060) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49060) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49060) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 49059, "title": "Add missing validation to pooling_ops_3d", "body": "PiperOrigin-RevId: 372218727\nChange-Id: I6b9ed4266aa7286c02f1f230d7bea922c1be547e", "comments": []}, {"number": 49058, "title": "Don't do any work if output tensor is null (prevent div by 0)", "body": "PiperOrigin-RevId: 372208700\nChange-Id: Iea6b6293e887ade8538facfdb50fb931e17f511e", "comments": []}, {"number": 49057, "title": "Add missing validations to reverse_sequence_op", "body": "PiperOrigin-RevId: 372178683\nChange-Id: Iac97ebab5b342f1262c77a7d9bcb4267b305ce5b", "comments": []}, {"number": 49056, "title": "Add several missing validations in SDCA", "body": "PiperOrigin-RevId: 372172877\nChange-Id: Id366da962432e18dcbfac847d11e98488bebb70a", "comments": []}, {"number": 49055, "title": "Fix memory corruption issue with `tf.raw_ops.DrawBoundingBoxesV2`.", "body": "PiperOrigin-RevId: 372033910\nChange-Id: I8a9f4efc1c8ddaacbc26ec1fbe4bfdd6791c226d", "comments": []}, {"number": 49054, "title": "Fix heap buffer overflow", "body": "PiperOrigin-RevId: 372132844\nChange-Id: Idef9895efaf145f2b1c23d31983601ec980cd5e4", "comments": []}, {"number": 49053, "title": "Fix out of bound read in requantization_range_op.cc", "body": "PiperOrigin-RevId: 372129031\nChange-Id: Ie684ab98a3840c5186ead3eafffc0e0ed0e8030d", "comments": []}, {"number": 49052, "title": "Fix heap-buffer-overflow issue with `tf.raw_ops.SparseDenseCwiseMul`.", "body": "PiperOrigin-RevId: 372054410\nChange-Id: Ifcce0491e2e3816838c87e73be30a1e61b65174d", "comments": []}, {"number": 49051, "title": "Fix breakage in parameterized_truncated_normal_op.cc", "body": "PiperOrigin-RevId: 372041718\nChange-Id: Iff79e77a2bb27032423eefcb84211627b27dfe81", "comments": []}, {"number": 49050, "title": "Add missing validations in dillation ops.", "body": "PiperOrigin-RevId: 372037158\nChange-Id: I4ee304c84a02550c030288a6534000b934fc1599", "comments": []}, {"number": 49049, "title": "Made error message in keras.backend.bias_add match the check", "body": "closes #49046 \r\n", "comments": ["/cc @nikitamaia Assigned/inprogress"]}, {"number": 49048, "title": "File-Based Dataset Sharding not working with experimental dataset load()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: 2x RTX A6000\r\n\r\n**Describe the current behavior**\r\nI am experimenting with the new api for loading and saving a tf.data dataset, and everything works fine until I go to distribute on multiple GPUs. When doing so, I get warnings such as the following indicating that file-based sharding failed and that data-based sharding will be used:\r\n\r\n```\r\n2021-05-10 11:42:25.281639: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard\r\n. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"LoadDataset/_1\"                                                                                                                               \r\nop: \"LoadDataset\"                                                                                                                                                                                                                       \r\ninput: \"Const/_0\"         \r\nattr {                    \r\n  key: \"Treader_func_args\"\r\n  value {             \r\n    list {            \r\n    }                 \r\n  }                   \r\n}                     \r\nattr {                \r\n  key: \"compression\"  \r\n  value {             \r\n    s: \"GZIP\"         \r\n  }                   \r\n}                     \r\nattr {                \r\n  key: \"output_shapes\"\r\n  value {             \r\n    list {            \r\n      shape {         \r\n        dim {         \r\n          size: 678400\r\n        }         \r\n      }           \r\n      shape {     \r\n        dim {     \r\n          size: 16\r\n        }         \r\n      }           \r\n      shape {     \r\n        dim {     \r\n          size: 64\r\n        }         \r\n      }           \r\n      shape {     \r\n        dim {     \r\n          size: -1\r\n        }\r\n        dim {\r\n          size: 5\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {                                                                                                                                                                                                                                  \r\n  key: \"output_types\"                                                                                                                                                                                                                   \r\n  value {                                                                                                                                                                                                                               \r\n    list {                                                                                                                                                                                                                              \r\n      type: DT_FLOAT                                                                                                                                                                                                                    \r\n      type: DT_FLOAT                                                                                                                                                                                                                    \r\n      type: DT_FLOAT\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"reader_func\"\r\n  value {\r\n    func {\r\n      name: \"__inference_load_lambda_53\"\r\n    }\r\n  }\r\n}\r\n. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_poli\r\ncy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\r\n```\r\n\r\nThe thing is, file-based sharding should work. When I saved the dataset using tf.data.experimental.save(), I specified splitting into 20 shards, 10 for each of my 2 GPUs, but somehow the distribute strategy isn't aware of these file shards.\r\n\r\n**Describe the expected behavior**\r\nTensorflow would detect a file-sharded dataset and use file sharding rather than data sharding.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nHow I'm saving the data:\r\n\r\n```\r\ndef shard_func(w, x, y, z):\r\n    return tf.random.uniform([1], minval=0, maxval=20, dtype=tf.int64)\r\n\r\ntf.data.experimental.save(tensor_ds, 'mypath', compression='GZIP', shard_func=shard_func)\r\n```\r\nHow I'm loading it:\r\n\r\n```\r\ntensor_ds = tf.data.experimental.load('mypath', (\r\n                                                    tf.TensorSpec(shape=[678400], dtype=tf.float32), \r\n                                                    tf.TensorSpec(shape=[16], dtype=tf.float32), \r\n                                                    tf.TensorSpec(shape=[64], dtype=tf.float32), \r\n                                                    tf.TensorSpec(shape=[None,5], dtype=tf.float32)),\r\n                                                    compression=\"GZIP\"\r\n                                                    ).shuffle(1024).prefetch(128)\r\ntensor_ds = tensor_ds.map(shape, num_parallel_calls=32)\r\ntensor_ds = tensor_ds.map(transform, num_parallel_calls=32)\r\ntensor_ds = tensor_ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=32, drop_remainder=True))\r\nfinal_ds = tensor_ds.map(fill_boxes, num_parallel_calls=tf.data.AUTOTUNE).prefetch(2)\r\n```\r\n\r\nFinally, I'm using tf.distribute.MirroredStrategy() as my strategy.\r\n\r\nIs it possible my additional map() steps in the data pipeline are causing the information about sharding to get lost? I.e. things would work if the model directly consumed the data from load() function? ", "comments": ["Hi @atyshka, the warning you're seeing refers to the `AutoShardPolicy` used when doing distributed training. This is different from saving a dataset in shards. Autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset. The available policies are FILE, DATA, and AUTO. If, for example, you had three workers and six input files, and you set the policy to FILE, those six input files will divided up amongst the workers.\r\n\r\nHowever, if you're using `MirroredStrategy`, there is only one worker, so there is no autosharding and the autosharding policy registers as no-op. Parts of the `MirroredStrategy` code is shared with `MultiWorkerMirroredStrategy` (which does require autosharding), so you see this warning regardless. I think ideally this warning should not show up at all in the `MirroredStrategy` case, since it just seems to add confusion. You don't need to worry about autosharding unless you're in the multi-worker scenario.\r\n\r\nIf you're curious, you can read more about autosharding in the sharding section of the [Distributed Input guide.](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Ok thanks! You\u2019re right that it would be nice if this was hidden for mirrored strategy, but other than that this seems resolved ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49048\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49048\">No</a>\n"]}, {"number": 49047, "title": "DSP(Hexagon) delegate run tensorflow-lite C++ example label_image more slowly than CPU", "body": "I compile tensorflow-lite and the example label_image in tensorflow-lite source code success,\r\nI did run with delegates of DSP(Hexagon) and CPU by ADB with running comands:\r\nCPU: ./label_image -m tflite_model_int8.tflite -i grace_hopper.bmp -l labels.txt 1\r\nDSP: ./label_image -m tflite_model_int8.tflite -i grace_hopper.bmp -l labels.txt -j 1\r\nBoth seems runs success, but average time or DSP: 60.00 ms, average time or CPU : 40.00 ms\r\nIs it right? DSP is more slow than CPU for 50% ?\r\nIf Not, what should I do to make DSP(Hexagon) runs at good performance?\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9.0\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BBK S5d (SOC: Qualcomm Snapdragon 660)\r\nTensorFlow installed from (source or binary): build by myself\r\nTensorFlow version (use command below): tensorflow-2.4.1\r\nPython version: Python 3.8.5\r\nBazel version (if compiling from source): 3.1.0\r\nGCC/Compiler version (if compiling from source): g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nSOC: Qualcomm Snapdragon 660\r\n\r\nCould anyone help? Or give a advice?\r\nThank you so much~\r\n", "comments": ["@karimnosseir could you take a look?", "Hi,\r\nCan you share the model you're using ? Also, the log should have how many ops are delegated, can you share these details ?\r\n\r\nUsually for best performance the whole model should be delegated, partially delegated models will not have best performance and will have overhead from switching between DSP and CPU\r\nThanks", "@karimnosseir Hello, I have only part log as bellow. I tested it several days ago, and I have no device now. I tested several basic models.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nloaded libcdsprpc.so\r\nINFO: TfLiteHexagonDelegate delegate: 69 nodes delegated out of 71 nodes with 1 partitions.", "Thanks @limdlh for the details.\r\nCan you point to any of the basic models so i can try reproducing the issue on my side.\r\n\r\nThanks", "Hello @karimnosseir, please refor to: https://github.com/limdlh/share", "Hello @karimnosseir, do you want some other files or information?", "Hi @limdlh sorry for the delay.\r\n\r\nI benchmarked the shared model on SD 660 and DSP is faster as expected.\r\nCPU 4 threads ~38 ms\r\nHexagon ~15 ms\r\n\r\nAlso, tried on Samsung S9 the label_image and hexagon is also faster as expected.\r\n\r\nYou may want to retry.\r\n\r\nThanks", "Hello @karimnosseir, thank you for your reply.\r\nYour result is a little more than 2 time faster for Hexagon than CPU, does Hexagon could only run this speed?\r\nI think Hexagon should be faster than CPU for several times or even dozens"]}, {"number": 49046, "title": "Unclear error message in keras.backend.bias_add", "body": "https://github.com/tensorflow/tensorflow/blob/40caef44549a199eaac327b673fa862194b66fc4/tensorflow/python/keras/backend.py#L6004-L6007\r\n\r\nIs line 6007 supposed to be `(len(bias_shape), ndim(x) - 1))`?", "comments": ["Thanks.. Can you submit a small PR?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49046\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49046\">No</a>\n"]}, {"number": 49045, "title": "Converted model ERROR: Unsupported data type 14 in tensor", "body": "### 1. System information\r\nConverted on:\r\n- Widows 10\r\n- TensorFlow 2.5.0\r\n\r\nRun on:\r\n\r\n\r\n- iOS 11.1\r\n- POD installation:\r\n- TensorFlowLite 2.4.0:\r\n\r\n### 2. Code\r\nregular tensroflowlite ios app example\r\n\r\n### 3. Failure after conversion\r\n\r\nERROR:\r\nUnsupported data type 14 in tensor\r\n\r\n------------------------------------\r\nDetails:\r\nI converted tensorflow model to tensorflow lite and tried to use it with iOS app (firstly on simulator) and unfortunately it makes an error during the interpreter build\r\n\r\nModel path:\r\nhttps://drive.google.com/drive/folders/1rdbLyp9BJWhz9oyR0-PAdsLPjcAzno6Z\r\nYou can ask for access if you need it\r\n\r\n\r\n@mariaHit FYI\r\n", "comments": ["This is an intended behavior. We recommend using the same or higher TensorFlow version for runtime than the TensorFlow version for conversion.", "@abattery thank you for fast response !!! But TF-Lite version 2.5.0-rc2 is not available through PODs...\r\nOnly official versions... The latest one is 2.4.0\r\n\r\nEven on the git: https://github.com/tensorflow/tensorflow/tree/v2.5.0-rc3/tensorflow/lite/ios\r\nWe can see that the ios (POD) release version on v2.5.0-rc3 branch is 2.4.0", "@abattery BTW I can update the code to the right version if you accept external code-change", "@yyoon could you take a look at this? When will the POD for TensorFlow 2.5 be released?", "We'll be releasing 2.5.0 version of CocoaPods artifacts when the TensorFlow 2.5.0 (not RCs) is released.\r\nWe don't release separate RC versions for TFLite, but you could try using the nightly versions instead in the meantime.\r\n(e.g., specify `~> 0.0.1-nightly` as the version constraint)", "@yyoon  Thank you for clarification. I will try to use nightly.\r\n\r\nWBR\r\nMax", "@MaxxTr , you can close this issue if you don't have any further questions. Thanks!", "@sachinprasadhs let me check that everything works fine once I'm taking the nightly version and I'll definitely close this ticket", "@sachinprasadhs there is a new error:\r\n\r\nTensorFlow Lite Error: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nTensorFlow Lite Error: Node number 183 (FlexTensorListReserve) failed to prepare.\r\n\r\nFailed to invoke the interpreter with error: Must call allocateTensors().\r\n\r\nCan you please explain the meaning and the solution for this issue ?\r\n\r\nAdditional clarification:\r\nI found some possible solutions on the web.\r\nSo I tried the following:\r\n\r\nAdd new dependency: pod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'\r\nAdd new flag: \"-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps\" to the build linker flags\r\n\r\nAfter that I got the following error: \r\nFailed to invoke the interpreter with error: Invalid tensor index 1, max index is 0.\r\nThe same models works fine when I use it directly though Python API\r\n\r\nThank you\r\n", "The second error (`Invalid tensor index 1, max index is 0`) is likely caused by some tensor accessing code in your Swift code side.\r\nCan you double check if you are passing the tensor index value correctly in your code?", "@yyoon You are totally right. My bad - I assumed the same tensor shape is it was for mobile net.\r\nJust one additional question if you maybe ca assist me:\r\n\r\nthe run time of my converted net is HUGE ~45sec.\r\nThat's really surprising. Can you please redirect me to some guide about optimization of the converted nets ?\r\n\r\nThank you & best regards\r\nMax\r\n", "There are some official docs on model optimization, which you can find here:\r\nhttps://www.tensorflow.org/lite/performance/model_optimization\r\n\r\nThat being said, a single inference taking 45 sec is indeed huge. A few additional points which might be helpful to you.\r\n* Make sure that you were using a real device (rather than a simulator), and make sure you were using release mode.\r\n* Try using the [iOS benchmark app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/ios) and see what the numbers look like there.\r\n* If you still see the 45s inference time, you might need to consider using a simpler model architecture to begin with.\r\n\r\ncc/ @abattery @ethkim for any additional guidance on additional model optimization / removing select tf ops dependency.", "The model is using the variant type. Currently, variant opeators can be supported only through the Select TF option.\r\n", "@yyoon  & @abattery  Thank you for your answers I'm closing this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49045\">No</a>\n"]}, {"number": 49044, "title": "How to pack libtensorflow_cc.so along with wheel file", "body": "I am building TF 1.15 from source and I am able to build the wheel file using this command:\r\n**bazel build -c opt //tensorflow/tools/pip_package:build_pip_package**\r\n**./bazel-bin/tensorflow/tools/pip_package/build_pip_package ../**\r\n And i am able to install and use above wheel file without any issue in another system.\r\n\r\nNext i am trying to build libtensorflow_cc.so and i am using below command:\r\n**bazel build -c opt //tensorflow:libtensorflow_cc.so**\r\nI can see the .so file built and present in the path: **tensorflow/bazel-bin/tensorflow/libtensorflow_cc.so**\r\nIs there any way I can package the **libtensorflow_cc.so** file along with wheel file?\r\nWhat would be command/changes required to do it?\r\nHow to get single wheel file that I can build in one system and use the wheel file in another system so that i am able to run both python interface as well as C++ interface?", "comments": ["@ratan \r\nCould you please provide the following details to analyse the issue reported.\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\nThanks", "@UsharaniPagadala \r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04**\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\nTensorFlow installed from (source or binary): **Source**\r\nTensorFlow version: **1.15**\r\nPython version: **3.7.10**\r\nInstalled using virtualenv? pip? conda?: **conda**\r\nBazel version (if compiling from source): **0.26.1**\r\nGCC/Compiler version (if compiling from source): **9.3.0**\r\nCUDA/cuDNN version: **building for CPU**\r\nGPU model and memory:**N/A**", "@ratan \r\nThank you for filling the template asked for,Could you please have a look at [this](https://forums.developer.nvidia.com/t/building-tensorflow-1-13-on-jetson-xavier/75966) , and let us know  if it [helps](https://www.tensorflow.org/install/source#tested_build_configurations).Thanks!", "@UsharaniPagadala Thanks for the links.\r\nAs i had mentioned in the first question that i was able to build **libtensorflow_cc.so** file. After reading the comment by r7vme in the [link](https://forums.developer.nvidia.com/t/building-tensorflow-1-13-on-jetson-xavier/75966/3) you provided, i am able to include **libtensorflow_cc.so** in wheel file too.\r\nI can see the .so file located in my conda env after installing the wheel file:\r\n/home/ratan/anaconda3/envs/tf-1.15-env/lib/python3.7/site-packages/tensorflow_core/\r\n\r\nBut to build any C++ tests, it is not able to find protobuf, eigen etc. I need to clone and build them, install them to make it working. How to include them in wheel file so that i can get .so file as well as all the required packages for C++?", "@ratan \r\nAs there is no active support for 1.x can you please upgrade to 2.x and let us know.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi,\n\nPlease close this issue thanks.\nWe found a way to create libtensorflow_cc.so in both TF 1.15 and TF 2.x.\n\n\nOn Thu, Oct 28, 2021 at 4:57 PM google-ml-butler[bot] <\n***@***.***> wrote:\n\n> This issue has been automatically marked as stale because it has no recent\n> activity. It will be closed if no further activity occurs. Thank you.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/49044#issuecomment-953755124>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAAYQEAFHUFCAY3CXOJY4T3UJEXQHANCNFSM44Q6JYSA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "Thank you for your update, moving this to closed status as its resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49044\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49044\">No</a>\n"]}, {"number": 49042, "title": "Half pixel centers", "body": "This PR adds half-pixel-center support for nearest neighbour and bilinear interpolation for XLA backends. This would enable the use of tf.keras.layers.UpSampling2D() over an XLA backend.", "comments": ["@smit-hinsu  Can you please review this PR ? Thanks!", "@lisphacker  Can you please resolve conflicts? Thanks!", "@lisphacker Any update on this PR? Please. Thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49042) for more info**.\n\n<!-- need_author_consent -->", "@gbaned Sorry for delay, I've rebased the diff.", "Hmm, just noticed the number of files showing as changed? Maybe I need to rebase this again?", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49042) for more info**.\n\n<!-- need_author_consent -->", "@mikedelorimier  Can you please review this PR ? Thanks!\r\n", "@mikedelorimier Can you please review this PR ? Thanks!", "@mikedelorimier Can you please review this PR ? Thanks!", "@mikedelorimier Can you please review this PR ? Thanks!", "@lisphacker Can you please resolve conflicts? Thanks!", "Hello, can this PR be re-opened?"]}, {"number": 49041, "title": "Error: No such file or directory", "body": "Can anyone help me fix this error:\r\n```\r\n...\r\n  File \"/home/notooth/anaconda3/lib/python3.8/site-packages/tensorflow_text/__init__.py\", line 21, in <module>\r\n    from tensorflow_text.python import metrics\r\n  File \"/home/notooth/anaconda3/lib/python3.8/site-packages/tensorflow_text/python/metrics/__init__.py\", line 20, in <module>\r\n    from tensorflow_text.python.metrics.text_similarity_metric_ops import *\r\n  File \"/home/notooth/anaconda3/lib/python3.8/site-packages/tensorflow_text/python/metrics/text_similarity_metric_ops.py\", line 28, in <module>\r\n    gen_text_similarity_metric_ops = load_library.load_op_library(resource_loader.get_path_to_datafile('_text_similarity_metric_ops.so'))\r\n  File \"/home/notooth/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/notooth/anaconda3/lib/python3.8/site-packages/tensorflow_text/python/metrics/_text_similarity_metric_ops.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb\r\nbash: --gin_param=input_filename='input.txt': No such file or directory\r\n```", "comments": ["@notooth1 Can you please share more details on code,Tensorflow version, OS and Python version, reproducible code would be better. ", "Generally we don't support directly Anacoda env as it is supported by third party:\r\n https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\r\nhttps://github.com/AnacondaRecipes/tensorflow_recipes\r\n", "@notooth1 You can find the similar issues which may solve your issue, [issue1](https://github.com/tensorflow/text/issues/385#issuecomment-692309756),[issue2](https://github.com/tensorflow/tensorflow/issues/20320#issuecomment-414036732).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49041\">No</a>\n"]}, {"number": 49040, "title": "Update rules_apple", "body": "The currently used version of rules_apple is not compatible with the upcoming versions of Bazel and will cause breakages.", "comments": ["@vladmos Can you please fix build failures ? Thanks!", "Updated sha256.", "@vladmos can you please sanity build failures ?", "@vladmos  Any update on this PR? Please. Thanks!"]}, {"number": 49039, "title": "[CherryPick 2.5] TF Security", "body": null, "comments": []}, {"number": 49038, "title": "[CherryPick:2.5] TF Security", "body": null, "comments": []}, {"number": 49037, "title": "Validate (and ensure validation sticks) inputs for `MatrixTriangularSolve`.", "body": "PiperOrigin-RevId: 370282444\nChange-Id: Iaed61a0b0727cc42c830658b72eb69f785f48dc5", "comments": []}]