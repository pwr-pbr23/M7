[{"number": 29115, "title": "Support RaggedTensors in sequence feature columns ", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0-dev20190527\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently `tf.feature_column.sequence_*` columns support only sparse tensor inputs.\r\nIt would be great if they will support ragged tensors too. At least thru type checking and `to_sparse` method (but maybe there will be a better performing way).\r\n\r\n**Will this change the current api? How?**\r\nNo, API won't changes.\r\n\r\n**Who will benefit with this feature?**\r\nDevelopers who work with NLP models where ragged tensors are more natural than Sparse ones.\r\n\r\n**Any Other info.**\r\nSee `Code to reproduce` and especially comment `This is my wish` from https://github.com/tensorflow/tensorflow/issues/29113", "comments": ["@shkarupa-alex We are checking to see if you still need help on this issue. Could you please check with latest stable version of TF which is `TF2.7` ? please check [link](https://www.tensorflow.org/guide/ragged_tensor) .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29114, "title": "Gradient clipping by norm has different semantics in tf.keras.optimizers against keras.optimizers.", "body": "Fixes #29108 .", "comments": ["@chenchc Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks! ", "@chenchc Gentle ping to check the reviewer comments. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 29113, "title": "All implicitly derived inputs to subclassed Models must be tf.Tensors (found SparseTensor)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.12.1-2821-gc5b8e15064', '2.0.0-dev20190527')\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the current behavior**\r\nException raises when trying to feed Sparse/Ragged tensors to keras model.fit().\r\nBut that is an ordinary pipeline for Estimators.\r\n\r\nBesides, method mentioned in error (`self._add_inputs`) does not exist in TF source code.\r\n\r\n**Describe the expected behavior**\r\nSparse and Ragged tensors should be acceptable input for keras models.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nFEATURES_DIM = 5\r\n_randoms = np.random.random((10, 32, FEATURES_DIM))\r\n\r\n# # This is my wish: ragged input to sequence feature column\r\n# features = tf.RaggedTensor.from_tensor(_randoms, ragged_rank=1)\r\n# labels = tf.reduce_sum(tf.expand_dims(features, axis=-2), axis=-1)\r\n\r\n# This is what should work right now\r\n_indexes = tf.where(tf.not_equal(_randoms, 0.0))\r\n_values = tf.gather_nd(_randoms, _indexes)\r\nfeatures = tf.SparseTensor(_indexes, _values, _randoms.shape)\r\nlabels = tf.sparse.reduce_sum(features, axis=-1, keepdims=True)    \r\n\r\ndataset = tf.data.Dataset \\\r\n    .from_tensor_slices(({'features': features}, labels)) \\\r\n    .batch(32)\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.features = tf.keras.experimental.SequenceFeatures([\r\n            tf.feature_column.sequence_numeric_column('features', shape=(FEATURES_DIM,))\r\n        ])\r\n        self.dense_1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(32, activation='relu'))\r\n        self.dense_2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))\r\n\r\n    def call(self, inputs):\r\n        outputs = self.features(inputs)\r\n        outputs = self.dense_1(outputs)\r\n        outputs = self.dense_2(outputs)\r\n        \r\n        return outputs\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        shape = tf.TensorShape(input_shape).as_list()\r\n        shape[-1] = 1\r\n        \r\n        return tf.TensorShape(shape)\r\n\r\n    \r\nmodel = MyModel()\r\nmodel.compile(optimizer='Adam', loss='mse')\r\nmodel.fit_generator(dataset, epochs=5)\r\n```\r\n\r\n**Other info / logs**\r\n```python\r\nEpoch 1/5\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-37-1cf666c5a52e> in <module>()\r\n     40 model = MyModel()\r\n     41 model.compile(optimizer='Adam', loss='mse')\r\n---> 42 model.fit_generator(dataset, epochs=5)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1175         shuffle=shuffle,\r\n   1176         initial_epoch=initial_epoch,\r\n-> 1177         steps_name='steps_per_epoch')\r\n   1178 \r\n   1179   def evaluate_generator(self,\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_generator.pyc in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    262 \r\n    263       is_deferred = not model._is_compiled\r\n--> 264       batch_outs = batch_function(*batch_data)\r\n    265       if not isinstance(batch_outs, list):\r\n    266         batch_outs = [batch_outs]\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.pyc in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n    895     x, y, sample_weights = self._standardize_user_data(\r\n    896         x, y, sample_weight=sample_weight, class_weight=class_weight,\r\n--> 897         extract_tensors_from_dataset=True)\r\n    898 \r\n    899     # If `self._distribution_strategy` is True, then we are in a replica context\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.pyc in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2341               'tf.Tensors (found %s). To add non-tf.Tensor inputs, please call '\r\n   2342               'self._add_inputs(tf.keras.Input/SparseInput/RaggedInput (etc)) '\r\n-> 2343               'in your subclassed Model object.' % (input_tensor,))\r\n   2344 \r\n   2345       # Build the model using the retrieved inputs (value or symbolic).\r\n\r\nValueError: All implicitly derived inputs to subclassed Models must be tf.Tensors (found SparseTensor(indices=tf.Tensor(\r\n[[ 0  0  0]\r\n [ 0  0  1]\r\n [ 0  0  2]\r\n ...\r\n [ 9 31  2]\r\n [ 9 31  3]\r\n [ 9 31  4]], shape=(1600, 3), dtype=int64), values=tf.Tensor([0.76254113 0.44757111 0.9459519  ... 0.64881651 0.2802026  0.79660244], shape=(1600,), dtype=float64), dense_shape=tf.Tensor([32 32  5], shape=(3,), dtype=int64))). To add non-tf.Tensor inputs, please call self._add_inputs(tf.keras.Input/SparseInput/RaggedInput (etc)) in your subclassed Model object.\r\n```", "comments": ["@shkarupa-alex It looks code snippet is incomplete. Can you please provide complete code to reproduce the reported issue. Thanks!", "> @shkarupa-alex It looks code snippet is incomplete. Can you please provide complete code to reproduce the reported issue. Thanks!\r\n\r\nYou're right, mistake found.\r\nCode in first message updated.", "@shkarupa-alex Thanks for updating. Now, I am able to reproduce the issue. Thanks!", "Hi @shkarupa-alex,\r\n\r\nThis error is occurring because we can't yet infer the shape of CompositeTensor inputs for subclassed Models. To get around it, please create a tf.keras.Input(..., sparse=True) in your __init__, and call self._add_inputs([my_input]). This will allow us to properly shape-check the input object.", "@markomernick\r\nshkarupa-alex already said \"Besides, method mentioned in error (self._add_inputs) does not exist in TF source code\".", "> @markomernick\r\n> shkarupa-alex already said \"Besides, method mentioned in error (self._add_inputs) does not exist in TF source code\".\r\n\r\nForget about this method.\r\nUsing tf.keras.Input(..., sparse=True) as input placeholder do the trick.", "I have a similar problem where one of the features is a tf.sparse.SparseTensor. As the it is said above, self._add_inputs does not exist. In the snippet\r\n\r\n```python\r\nclass M(tf.keras.Model):\r\n  def __init__(self):\r\n    super(M, self).__init__()\r\n    ...\r\n    self._features = tf.keras.layers.DenseFeatures([...])\r\n\r\n  def call(self, inputs):\r\n    # inputs is a dict of sparse tensors.\r\n    layer = self._features(inputs)\r\n    ...\r\n    return prediction\r\n```\r\n\r\nWhat is the best way to make the code work considering that self._add_inputs does not exist?", "Thanks for the issue. This code now works in the latest nightly (note that SequenceFeatures returns a list of features):\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nFEATURES_DIM = 5\r\n_randoms = np.random.random((10, 32, FEATURES_DIM))\r\n\r\n# # This is my wish: ragged input to sequence feature column\r\n# features = tf.RaggedTensor.from_tensor(_randoms, ragged_rank=1)\r\n# labels = tf.reduce_sum(tf.expand_dims(features, axis=-2), axis=-1)\r\n\r\n# This is what should work right now\r\n_indexes = tf.where(tf.not_equal(_randoms, 0.0))\r\n_values = tf.gather_nd(_randoms, _indexes)\r\nfeatures = tf.SparseTensor(_indexes, _values, _randoms.shape)\r\nlabels = tf.sparse.reduce_sum(features, axis=-1, keepdims=True)    \r\n\r\ndataset = tf.data.Dataset \\\r\n    .from_tensor_slices(({'features': features}, labels)) \\\r\n    .batch(32)\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.features = tf.keras.experimental.SequenceFeatures([\r\n            tf.feature_column.sequence_numeric_column('features', shape=(FEATURES_DIM,))\r\n        ])\r\n        self.dense_1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(32, activation='relu'))\r\n        self.dense_2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))\r\n\r\n    def call(self, inputs):\r\n        outputs = self.features(inputs)\r\n        outputs = self.dense_1(outputs[0])\r\n        outputs = self.dense_2(outputs)\r\n        \r\n        return outputs\r\n\r\n    \r\nmodel = MyModel()\r\nmodel.compile(optimizer='Adam', loss='mse')\r\nmodel.fit_generator(dataset, epochs=5)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29113\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29113\">No</a>\n"]}, {"number": 29112, "title": "tf.function runtime error when modifying file", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-2828-ga9a1a64d25 2.0.0-dev20190528\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: CUDA version 10.0.130\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\nI'm running some experiments using tensorflow 2.0 nightly. I decorated the train step with `@tf.function`. In the meantime I'm modifying the file involved in the training.\r\nAfter some time I get a random error in a function invocation (inside the function decorated with `tf.function`. If I do not modify any file the error does not happen.\r\nI guess it's something related to the implementation of `@tf.function`, probably `tf.function` creates some temp file and updates them. The file update however is not managed well since modifying files the training should not be affected.\r\n\r\n**Code to reproduce the issue**\r\nThis is an issue difficult to reproduce.\r\n", "comments": ["@mdanatg could this be autograph?", "It's difficult to tell, but unlikely.\r\n\r\n@EmanueleGhelfi could you include the error message (redacted is fine, the type and parts of the error message would be helpful). Also, could you try to see if the error reproduces with `@tf.function(autograph=False)`?", "@EmanueleGhelfi, Is the still persists? \r\nDid you check the @mdanatg's comment. Thanks", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity. Please re-open if anything new comes up.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29112\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29112\">No</a>\n"]}, {"number": 29111, "title": "tensorflow_addons can not be imported", "body": "Could you plz help to solve this issue?\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution ( Linux Ubuntu 16.04):\r\n\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.0.0-dev20190528\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: GTX 2080 ti\r\n\r\n**Describe the current behavior**\r\n\r\nI updated Tensorflow from 2.0 a to 2.0.0-dev20190528 and came up with the error  while importing tensorflow_addons, some object could not be opened by the TF framework:\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow_addons as tfa\r\n\r\n**Other info / logs**\r\n\r\n  File \"/mnt/HDD1/kaiyi/Split-UNet/module.py\", line 1, in <module>\r\n    import tensorflow_addons as tfa\r\n  File \"/home/bmiter/anaconda3/envs/SplitU/lib/python3.6/site-packages/tensorflow_addons/__init__.py\", line 75, in <module>\r\n    from tensorflow_addons import image\r\n  File \"/home/bmiter/anaconda3/envs/SplitU/lib/python3.6/site-packages/tensorflow_addons/image/__init__.py\", line 20, in <module>\r\n    from tensorflow_addons.image.distort_image_ops import adjust_hsv_in_yiq\r\n  File \"/home/bmiter/anaconda3/envs/SplitU/lib/python3.6/site-packages/tensorflow_addons/image/distort_image_ops.py\", line 24, in <module>\r\n    get_path_to_datafile(\"custom_ops/image/_distort_image_ops.so\"))\r\n  File \"/home/bmiter/anaconda3/envs/SplitU/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so: cannot open shared object file: No such file or directory\r\n\r\nProcess finished with exit code 1", "comments": ["I believe tensorflow-gpu 2.0 nightly is incompatible with the pypi addons distribution, and the addons nightly in pypi is currently broken. So what you need to do is build the addons repo from source by cloning it locally.", "Did you install the [tensorflow-addon module](https://github.com/tensorflow/addons#installation) using?\r\n```pip install tensorflow-addons```", "https://github.com/tensorflow/addons/issues/193\r\n\r\nhttps://github.com/tensorflow/addons/issues/241", "> Did you install the [tensorflow-addon module](https://github.com/tensorflow/addons#installation) using?\r\n> `pip install tensorflow-addons`\r\n\r\nYep, it could work before I update Tensorflow version of my virtual environment.", "> [tensorflow/addons#193](https://github.com/tensorflow/addons/issues/193)\r\n> \r\n> [tensorflow/addons#241](https://github.com/tensorflow/addons/issues/241)\r\n\r\nThanks weboyt, for some reasons I have to use the latest TF2.0 to avoid another Error, LMAO. Are you suggesting me in option2 to clone the latest version of tensorflow_addons to and import it directly from local dir? Actually, I m using tfa 0.3.1, just wondering if there is a newer version than that.", "> I believe tensorflow-gpu 2.0 nightly is incompatible with the pypi addons distribution, and the addons nightly in pypi is currently broken. So what you need to do is build the addons repo from source by cloning it locally.\r\n\r\nHi weboyt, I tried to update tensorflow_addons from source with the following [command](https://github.com/tensorflow/addons):\r\npip install ./artifacts/tensorflow_addons-0.3.0.dev0-cp27-cp27mu-linux_x86_64.whl\r\nFollowed by the Error below:\r\ntensorflow_addons-0.3.0.dev0-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform.\r\n\r\nthe tensorflow version I am using is '2.0.0-dev20190528'\r\n\r\nCould you plz help to explain that?", "Outside of my experience unfortunately. I got it working on a 18.04 laptop but it has bugs, I don't think the nightly/addons is in a release ready state quite yet. Youll have the best luck reverting from 2.0 if you need addons and dont know how to build from source, just use the standard pip install.\r\n\r\nstartup time is roughly 2000x slower on my nvidia10 gpu compared to the old version.\r\ntransformer impl has bugs compared to the one built in official/model.", "@seanpmorgan Can you please take a look? Thanks!", "@wesboyt is correct. The PyPi package is built against tf2-alpha and it will break against tf2 nightly. We're aiming to have a tensorflow-addons nightly built in the next week or so... alternatively you can build form source. We also expect a tf2-rc at some point and we'll make sure to package against that ASAP.\r\n\r\nMy guess from the pip error you're getting is  that either your python env is not py27 (as that particular whl was built for) or you're not on a linux system. \r\n\r\nRegarding the speed up... GPU kernels have yet to be implemented but they are on the near term roadmap https://github.com/tensorflow/addons/issues/118. The ability to use dynamic kernels was just added. If there are any bugs in the transformer as it exists though please post an issue and we'll get it taken care of.", "Closing this issue. Feel free to reopen if still have any further problems. Thanks!", "> @wesboyt is correct. The PyPi package is built against tf2-alpha and it will break against tf2 nightly. We're aiming to have a tensorflow-addons nightly built in the next week or so... alternatively you can build form source. We also expect a tf2-rc at some point and we'll make sure to package against that ASAP.\r\n> \r\n> My guess from the pip error you're getting is that either your python env is not py27 (as that particular whl was built for) or you're not on a linux system.\r\n> \r\n> Regarding the speed up... GPU kernels have yet to be implemented but they are on the near term roadmap [tensorflow/addons#118](https://github.com/tensorflow/addons/issues/118). The ability to use dynamic kernels was just added. If there are any bugs in the transformer as it exists though please post an issue and we'll get it taken care of.\r\n\r\n@seanpmorgan thanks for your explanation. Unfortunately, I had to use the latest version of tensorflow to avoid another bug lol. Will keep trying the latest tensorflow-addons version.", "@weiminson Sounds good... be on the lookout for a tf-addons 0.4 release soon following the TF2 release candidate in the coming days. Once we build against that a lot of these mismatch issues should be fixed.", "yea that was my conundrum as well, using the transformer model example required the nightly build because model saving is fried in alpha. Because addons breaks nightly you cannot use nightly either. So realistically you either build from source entirely, or you revert to stable release. So any user without the know how to build from source by themselves is stuck on stable.", "> transformer\r\n\r\n@wesboyt Your explanation makes sense. Cheers.", "Just FYI this was released earlier today. Let us know if you have any issues:\r\nhttps://github.com/tensorflow/addons/releases/tag/v0.4.0", "> Just FYI this was released earlier today. Let us know if you have any issues:\r\n> https://github.com/tensorflow/addons/releases/tag/v0.4.0\r\n\r\nThanks dude, will try."]}, {"number": 29110, "title": "How to test a saved model trained with tf.train.batch?", "body": "By calling graph.get_tensor_by_name(\"output:0\")\uff0cI found that it returns a tensor shaped [batch_size,num]\uff0cHow to test a saved model trained with tf.train.batch?", "comments": ["@guoguoguilai Please have look on this [link](https://www.tensorflow.org/guide/saved_model#loading_a_savedmodel_in_python) to load saved model. Thanks!  ", "It looks, This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think it is a bug. Let us know. Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29109, "title": "Added 8-bit Quantization Support for FloorDiv.", "body": "", "comments": []}, {"number": 29108, "title": "Gradient clipping by norm has different semantics in tf.keras.optimizers against keras.optimizers", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0 + cuDNN 7.4\r\n- GPU model and memory: NVIDIA Tesla V100 32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nThe gradient clipping mechanism is implemented in the abstract class, tf.keras.optimizers.Optimizer (keras.optimizers.Optimizer) so that every optimizer inherited from the class supports it.\r\nWe find different semantics of the implementations between tf.keras and keras.io.\r\n\r\nIn tf.keras, the gradient norm is calculated and clipped **per gradient tensor**.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bb8cf258bc87f68612a5e032d5b4def0d3c52566/tensorflow/python/keras/optimizers.py#L98-L99\r\n\r\nBut in keras.io, the gradient norm is calculated globally **across all gradient tensors**.\r\n\r\n```python\r\n        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\r\n            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\r\n            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\r\n```\r\nCode link: https://github.com/keras-team/keras/blob/eab1b5bcdf105746ede02d2eb8a5cb3ca359b1b5/keras/optimizers.py#L96-L98\r\n\r\nIMO, tf.keras should adopt the latter method since\r\n(1) The paper introducing gradient clipping (https://arxiv.org/abs/1211.5063) suggests the latter one, and\r\n(2) tf.keras should be compliant with keras.io.\r\n\r\n", "comments": ["I already provided comment in the PR. Closing this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29108\">No</a>\n", "Hi @tanzhenyu , thanks for your reply. \r\nBut where can I find the comment you left?", "@chenchc in the PR, go to files changes tab (or the regular page) you should be able to see it", "@tanzhenyu I cannot see the comments in both tabs. Maybe they are invisible to my account?\r\n\r\n<img width=\"1015\" alt=\"Screen Shot 2019-06-26 at 12 37 12\" src=\"https://user-images.githubusercontent.com/6285919/60151408-33a69f80-980f-11e9-8cf6-360f5933b1f0.png\">\r\n", "@chenchc My major comment is that I don't see where in this paper describing it should use global clip instead of local clip."]}, {"number": 29107, "title": "[LITE] l2norm refactor, remove double calculation of index", "body": "", "comments": ["Also, while this seems like a trivial change, every PR takes additional time to guide through the landing process, so please focus on important changes rather than minor cleanups or code tweaks."]}, {"number": 29106, "title": "Check for closed session", "body": "Fixes #28959.", "comments": []}, {"number": 29105, "title": "There is a incorrect link in CONTRIBUTING.md", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md\r\n\r\n## Description of issue (what needs changing):\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\nNo.\r\n```\r\n1.  Using tools and libraries installed directly on your system.\r\n\r\n    Refer to the\r\n    [CPU-only developer Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel)\r\n    and\r\n    [GPU developer Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu)\r\n    for the required packages. Alternatively, use the said\r\n```\r\nThe link of 'CPU-only developer Dockerfile' and 'GPU developer Dockerfile' are 404.\r\n\r\n### Submit a pull request?\r\nI'd like to fix it.\r\n", "comments": ["@jvishnuvardhan I had already opened a PR to resolve the issue :sweat_smile: "]}, {"number": 29104, "title": "Remove unused attributes", "body": "", "comments": []}, {"number": 29103, "title": "tfjs model converted from Keras model cannot be loaded", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\ndevice:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):1.9\r\n- Python version:3.7.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n***Step 1***\r\nTrain a keras model, and test it OK. saved as `model.h5`\r\nmodel as below:\r\n```\r\nmodel = Sequential()\r\n\r\nmodel.add(SeparableConv2D(32,\r\n                          kernel_size=(3, 3),\r\n                          padding='same',\r\n                          activation='relu',\r\n                          input_shape=input_shape))\r\n\r\nmodel.add(SeparableConv2D(64,\r\n                          kernel_size=(3, 3),\r\n                          padding='same',\r\n                          activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(SeparableConv2D(256,\r\n                          kernel_size=(3, 3),\r\n                          padding='same',\r\n                          activation='relu' ))\r\nmodel.add(AveragePooling2D(pool_size=(14, 14)))\r\nmodel.add(Flatten())\r\n```\r\n***Step 2***\r\nuse `tensorflowjs_converter` to convert keras model to JS model.\r\n```\r\n>$ tensorflowjs_converter --input_format=keras ./my_model.h5 ./assets\r\n```\r\nand generate 2 files `group1-shard1of1` and `model.json`\r\n\r\n***Step 3***\r\nuse converted model in JS project.\r\n```\r\nthis.model = await tf.loadLayersModel(model_path);\r\n```\r\nand get errors like below:\r\n```\r\ncore.js:15724 ERROR Error: Uncaught (in promise): Error: Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.\r\nError: Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.\r\n    at new t (tf-layers.esm.js:17)\r\n    at t [as constructor] (tf-layers.esm.js:17)\r\n    at new t (tf-layers.esm.js:17)\r\n    at e.fromConfig (tf-core.esm.js:17)\r\n    at deserializeKerasObject (tf-layers.esm.js:17)\r\n    at deserialize (tf-layers.esm.js:17)\r\n    at t.fromConfig (tf-layers.esm.js:17)\r\n    at deserializeKerasObject (tf-layers.esm.js:17)\r\n    at deserialize (tf-layers.esm.js:17)\r\n    at tf-layers.esm.js:17\r\n    at resolvePromise (zone.js:831)\r\n    at zone.js:741\r\n    at rejected (tslib.es6.js:69)\r\n    at ZoneDelegate.push../node_modules/zone.js/dist/zone.js.ZoneDelegate.invoke (zone.js:391)\r\n    at Object.onInvoke (core.js:17299)\r\n    at ZoneDelegate.push../node_modules/zone.js/dist/zone.js.ZoneDelegate.invoke (zone.js:390)\r\n    at Zone.push../node_modules/zone.js/dist/zone.js.Zone.run (zone.js:150)\r\n    at zone.js:889\r\n    at ZoneDelegate.push../node_modules/zone.js/dist/zone.js.ZoneDelegate.invokeTask (zone.js:423)\r\n    at Object.onInvokeTask (core.js:17290)\r\n```\r\n\r\n## Note: if I replace the `SeparableConv2D` by `Conv2D`, everything will be OK.", "comments": ["It looks from the error that SeparableConv2D uses depthwiseInitializer, depthwiseRegularizer etc therefore need to use these instead. For better help and faster resolution, you can post this issue on [tensorflow/tfjs](https://github.com/tensorflow/tfjs/issues/new) repo. Please let me know in case you have any clarification. Thanks!", "Closing the issue. Please let us know in case of any clarification. Thanks!", "**SOLUTION**    There is a **BUG** when using`tensorflowjs_converter` to convert a network containing `SeparableConv2D ` to `jsmodel`.\r\nYou need to remove the fields `kernelInitializer ` `kernelRegularizer ` and `kernelConstraint ` in `model.json`.  ", "I used `tensorflowjs==0.8.0` , latest version not test yet.", "@changkaizhao how do you remove those fields from model.json?  Just remove those pairs from the JSON completely?", "error disappears for me when I export as a tfjs_graph_model instead.\r\n`tensorflowjs_converter --input_format tfjs_layers_model --output_format tfjs_graph_model your_exported_tfjs_model/model.json graph_model`\r\n", "How to solve this? I facing the same issue ", "The issue is not solved for me when using the tfjs_graph model. \r\nBut you can solve it by replacing all \"kernel_initializer\" with \"depthwise_initializer\", \"kernel_regularizer\" with \"depthwise_intializer\" and \"kernel_contraint\" with \"depthwise_constraint\" in  model.json file.\r\n\r\n"]}, {"number": 29102, "title": "Tf 2.0 with cuda 10.1 build from source compiles to 1.13 package", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution:Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: TF2.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.25.3\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1060 3GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build tensorflow 2.0 from source with cuda 10.1 . I followed a method used by #28936  but the package I build is always 1.13 even with --config=v2 flag\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nconda create -n tf2build pip python=3.7\r\nsource activate tf2build\r\npip install --upgrade --force-reinstall pip setuptools\r\npip install wheel numpy scipy keras\r\ngit clone https://github.com/tensorflow/tensorflow.git --single-branch --branch master\r\ncd tensorflow\r\n\r\n./configure\r\n Use default values except:\r\n CUDA support: Y\r\n\r\nexport TMP=/tmp\r\nbazel build -c opt --config=opt --config=v2  \\\r\n    --copt=-march=native --cxxopt=-march=native \\\r\n    //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\ncreating the package had the following log:\r\n\r\nThere are no errors during the compilation process, only warnings\r\n\r\nTue May 28 16:30:14 EDT 2019 : === Preparing sources in dir: /tmp/tmp.C6skAbFOnh\r\n~/tfbuild/tf2build/tensorflow ~/tfbuild/tf2build/tensorflow\r\n~/tfbuild/tf2build/tensorflow\r\nTue May 28 16:30:32 EDT 2019 : === Building wheel\r\nwarning: no files found matching '*.pyd' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dylib' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.csv' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/Eigen'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/google'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/unsupported'\r\nTue May 28 16:30:54 EDT 2019 : === Output wheel file is in: /tmp/tensorflow_pkg\r\n\r\n\r\n\r\n", "comments": ["From the instructions above, it does appear the final build is 2.0, regardless of what the version says. In several places in master the version number is still listed as 1.13.1 (for example: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L50)\r\n\r\nIf you want to update the version numbers prior to the build, you can run this script prior to running configure\r\n`./tensorflow/tools/ci_build/update_version.py --nightly --version 2.0.0`", "The wheel file it generates is tf_nightly-1.13.1-cp37-cp37m-linux_x86_64.whl\r\nAfter I install the package, tf.__version__ still shows 1.13 ", "@lifeishard Can you please follow the TF installation steps mentioned in this [link](https://www.tensorflow.org/install/source)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29101, "title": "Random seed not set in graph context of `Dataset#map`", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jupyter notebook on https://colab.research.google.com\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Stock on https:///colab.research.google.com\r\n- TensorFlow version (use command below): b'v1.13.1-2-g09e3b09e69' 1.13.1\r\n- Python version: 2/3\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA \r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe random seed set via `tf.set_random_seed(seed)` is not set in the context in which the functions passed to `tf.data.Dataset#map` are invoked. Even for the single thread case. \r\n**Describe the expected behavior**\r\nThe random seed set via `tf.set_random_seed(seed)` should be set in the context in which the functions passed to `tf.data.Dataset#map` are invoked, at least for the single thread case. \r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef seed_assert(elt):\r\n  seed = tf.get_default_graph().seed\r\n  print(\"Seed is {}\".format(seed))\r\n  assert seed is not None, \"Random seed is not set. Random graph operations added during mapping will not be reproducible.\"\r\n  return elt\r\n\r\nseed = 37\r\n      \r\ntf.set_random_seed(seed)  \r\n\r\nds = tf.data.Dataset.from_generator(lambda : (yield (0)), (tf.int64))\r\n\r\nseed_assert(None)\r\n\r\nds.map(seed_assert)\r\n```\r\nCan run here:\r\n[Seed in Dataset#Map.ipynb](https://colab.research.google.com/drive/1SIGhYzOKc6Sg147DiB-NFz_lXKym_mto#scrollTo=ENwL9Bo60BYw) \r\n\r\n**Other info / logs**\r\nI originally saw this issue locally but was able to reproduce it on the Jupyter notebook provided by Google. Here is the log of the errors I see when running the above code.\r\n\r\n```python\r\nSeed is 37\r\nSeed is None\r\n\r\n---------------------------------------------------------------------------\r\n\r\nAssertionError                            Traceback (most recent call last)\r\n\r\n<ipython-input-7-38991a9ee77e> in <module>()\r\n     15 seed_assert(None)\r\n     16 \r\n---> 17 ds.map(seed_assert)\r\n\r\n8 frames\r\n\r\n<ipython-input-7-38991a9ee77e> in seed_assert(elt)\r\n      4   seed = tf.get_default_graph().seed\r\n      5   print(\"Seed is {}\".format(seed))\r\n----> 6   assert seed is not None, \"Random seed is not set. Random graph operations added during mapping will not be reproducible.\"\r\n      7   return elt\r\n      8 \r\n\r\nAssertionError: Random seed is not set. Random graph operations added during mapping will not be reproducible.\r\n```", "comments": ["@eha11 I tried to reproduce the issue on my system and also on google colab but code executed as expected. Can you try once and let us know if that still an issue. Thanks! ", "@gadagashwini as I wrote the example originally it wouldn't fail, just print out that the seed was None. I've simplified and updated the example to make it assert when the seed is `None`. I can reliably reproduce this in the linked Google Collab above and locally.", "A related but slightly different issue was posted in https://github.com/tensorflow/tensorflow/issues/13932. That particular issue is more about race conditions due to parallelism.\r\n\r\nThe issue in this MR, is also reported in another MR's comment: https://github.com/tensorflow/tensorflow/issues/23789#issuecomment-443009310\r\nThat MR is closed and the solution stated as:\r\n> the fix here will depend on switching to the new Python function implementation\r\n@shivaniag reports that:\r\n>  The new python function implementation is in and this issue has been fixed with that\r\nand has subsequently closed the issue, without linking to the actual solution.\r\n\r\nFrom gathering a few snippets, I have created the following test that shows the issue:\r\n\r\n```py\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test(threads):\r\n  tf.reset_default_graph()\r\n  np.random.seed(42)\r\n  tf.set_random_seed(42)\r\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\r\n\r\n  def mapfn(p):\r\n    return  tf.image.random_hue(p, 0.04)\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices(images)\r\n  dataset = dataset.map(mapfn, num_parallel_calls=threads)\r\n  dataset = dataset.batch(32)\r\n  x = dataset.make_one_shot_iterator().get_next()\r\n\r\n  with tf.Session() as sess:\r\n    return sess.run(x)\r\n\r\nassert np.allclose(test(1), test(1)), \"num_parallel_calls=1 undeterministic\"\r\nassert np.allclose(test(15), test(15)),  \"num_parallel_calls=15 undeterministic\"\r\n```\r\n\r\n## num_parallel_calls == 1 fails\r\n\r\nAbove fails with \r\n> `AssertionError: num_parallel_calls=1 undeterministic`\r\n\r\nWe can solve this first test with 1 parallel call in two ways:\r\n```diff\r\n   def mapfn(p):\r\n-    return  tf.image.random_hue(p, 0.04)\r\n+    return  tf.image.random_hue(p, 0.04, seed=42)\r\n```\r\nOr\r\n```diff\r\n   def mapfn(p):\r\n+    tf.set_random_seed(42)\r\n     return  tf.image.random_hue(p, 0.04)\r\n```\r\n\r\n@mrry, Derek, It's unclear to me why `map` does not respect the graph's default seed?\r\n\r\n## num_parallel_calls > 1 fails\r\n\r\nAnd of course, after fixing the first test as above, the second one will fail with 15 parallel calls:\r\n> `AssertionError: num_parallel_calls=15 undeterministic`\r\n\r\nBut that case is just a race condition and not necessarily a bug. This is what https://github.com/tensorflow/tensorflow/issues/13932 is all about.\r\n\r\n", "> @gadagashwini as I wrote the example originally it wouldn't fail, just print out that the seed was None. I've simplified and updated the example to make it assert when the seed is `None`. I can reliably reproduce this in the linked Google Collab above and locally.\r\n\r\nYou are correct. I am able to reproduce the reported issue with Tensorflow 1.13. Thanks!", "@eha11 could you please see if the issue is present in 1.14? RC0 for 1.14 was released three days ago.\r\n\r\nI was not able to reproduce the issue locally running the bleeding edge of TensorFlow which hopefully means this issue has been indeed fixed.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "> Hi There,\r\n> \r\n> We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help.\r\n> \r\n> This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.\r\n\r\nNice robot. This post has nothing to do with TF1 specifically.", "Yes this was fixed at least by 1.15 so it can be closed. Hi @TimZaman !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29101\">No</a>\n"]}, {"number": 29100, "title": "Keras Vanilla Installation | RuntimeError: Python version >= 3.5 required", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n[+] No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n[+] Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n[+] NA\r\n- TensorFlow installed from (source or binary):\r\n[+] Binary\r\n- TensorFlow version (use command below):\r\n[+] Tensorflow v1.13.1-0-g6612da8951\r\n- Python version:\r\n[+] Python 3.6.0\r\n- Bazel version (if compiling from source):\r\n[+] NA\r\n- GCC/Compiler version (if compiling from source):\r\n[+] GCC 7.4.0\r\n- CUDA/cuDNN version:\r\n[+] CUDA v9.1.85\r\n- GPU model and memory:\r\n[+] GTX 1050 Mobile 4Gb\r\n\r\n**Describe the current behavior**\r\n\r\nInstallation of Keras from source fails by doing the following.\r\n\r\nRunning the commands as per the Keras installation instructions in a virtual environment.\r\n\r\nsudo python setup.py install\r\n...\r\nFile \"/tmp/easy_install-vOk903/scipy-1.3.0/setup.py\", line 31, in\r\nauthor_email='francois.chollet@gmail.com',\r\nRuntimeError: Python version >= 3.5 required.\r\n\r\n**Describe the expected behavior**\r\n\r\nSuccessful Keras installation.\r\n\r\n**Code to reproduce the issue**\r\n\r\n[+] conda create -n kerasenv python=3.5.0\r\n[+] conda activate kerasenv\r\n[+] pip install tensorflow\r\n[+] git clone https://github.com/keras-team/keras.git\r\n[+] cd keras\r\n[+] sudo python setup.py install\r\n\r\n**Other info / logs**\r\n[+] [errorlog2.txt](https://github.com/tensorflow/tensorflow/files/3230162/errorlog2.txt)\r\n\r\n[+] Keras Issue: https://github.com/keras-team/keras/issues/12837\r\n", "comments": ["Full stack trace: \r\n[trace.txt](https://github.com/tensorflow/tensorflow/files/3230219/trace.txt)\r\n", "Because `scipy==1.3.0` does not support python 2 anymore. You can install `scipy==1.2.1` before install `Keras`.", "Thanks for the reply. That does not fix the problem. Why does it revert back to python 2 when the environment is set to use python 3.6.0?", "My guess is the step `sudo python setup.py install` is using the system python, because the sudo command would not inherit the current users environment. \r\n\r\nWhat does `sudo python --version` return?  2.7 probably\r\n\r\nDo you need the sudo command?", "> My guess is the step `sudo python setup.py install` is using the system python, because the sudo command would not inherit the current users environment.\r\n> \r\n> What does `sudo python --version` return? 2.7 probably\r\n> \r\n> Do you need the sudo command?\r\n\r\nYup, `sudo python --version` returns `Python 2.7.15rc1` ... So there is the problem.\r\n\r\n`sudo` is needed I believe to allow the `setup.py` file to have the correct permissions to install all the necessary dependencies. \r\n\r\nRunning `python setup.py install` returns `error: [Errno 13] Permission denied`", "Can you do `which python`, and then run `sudo <path to python> setup.py install` ?  ", "Thanks @wdirons that fixes the issue. Installation completes without any errors. \r\n\r\nIs there any way to fix this issue permanently? ie not have to explicitly specify to use the environment python version? Or is that just how the chips will fall if attempting to install keras from source in a virtualenv? ", "I ran myself:\r\n\r\n[+] conda create -n kerasenv python=3.5.0\r\n[+] conda activate kerasenv\r\n[+] pip install tensorflow\r\n[+] git clone https://github.com/keras-team/keras.git\r\n[+] cd keras\r\n[+] python setup.py install\r\n\r\nand didn't have any problems at all.\r\n\r\n```\r\nUsing /home/irons/anaconda2/envs/kerasenv/lib/python3.5/site-packages\r\nFinished processing dependencies for Keras==2.2.4\r\n```\r\n\r\nAs conda should be creating a virtual environment in a directory that user has access to, pip or python setup.py install should have access to write to that directory.\r\n", "@justinvzyl : Can we close the issue since it looks like it is resolved. Thanks!\r\n@wdirons : Thanks for the support.", "@achandraa Yup will close, thanks for the support @wdirons "]}, {"number": 29099, "title": "Drastically different behavior between TF1 and TF2", "body": "I've noticed drastically different behavior of the following code between _2.0.0-alpha0_ and _1.13.1_.\r\n```python\r\nimport numpy as np\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = mnist.load_data()\r\nx_train = x_train_raw.reshape(60000, 784)\r\nx_test = x_test_raw.reshape(10000, 784)\r\ny_train = to_categorical(y_train_raw)\r\ny_test = to_categorical(y_test_raw)\r\n\r\nbasic_net = Sequential([\r\n    Dense(36, activation='relu', input_shape=(784,)),\r\n    Dense(10, activation='softmax')\r\n])\r\n\r\nW_bio = np.load('W_bio.npy')\r\n\r\nbasic_net.layers[0].set_weights([W_bio.transpose(), basic_net.layers[0].get_weights()[1]])\r\nbasic_net.layers[0].trainable = False\r\nbasic_net.compile(optimizer=Adam(lr=0.0001), metrics=['accuracy'], loss='categorical_crossentropy')\r\nbasic_net.fit(x=x_train, y=y_train, epochs=15)\r\n```\r\n\r\nIn _2.0.0-alpha0_ the accuracy of the network consistently reaches >80% in the first few epochs. In _1.13.1_ the accuracy consistently reaches only <20% after all 15 epochs. What is going on?\r\n\r\nHere is the file with weights: [W_bio.npy](https://drive.google.com/file/d/12O0XE98jl_yYmiBbZMWyLMaeZiHH-FXH/view?usp=sharing)", "comments": ["The difference in behavior has something to do with using ReLU. If instead the activation function used for the hidden layer is sigmoid then the behavior becomes the same in TF1 and TF2 (<20% accuracy).", "@danaugrs I hope you are using ReLU for the hidden layers now. Can you please let us know if we have to keep this issue open ", "@muddham I think you seriously misunderstood my last comment. I was just helping whoever is going to debug this difference in behavior between _2.0.0-alpha0_ and _1.13.1_ by pointing out that it probably lies in the ReLU implementation. The difference in behavior is only apparent when using ReLU. All the other activation functions I've tried lead to the same behavior in both versions.", "The accuracy difference doesn't seem to be as severe now, but it is definitely still present (~65% vs. ~85%) However switching to distribution strategy (which changes the execution path) restores the accuracy in 2.0. We're standardizing on the latter path, so this should be fixed before the 2.0 release. In the mean time something like:\r\n\r\n```\r\nwith tf.distribute.OneDeviceStrategy(\"GPU:0\").scope():\r\n  basic_net = Sequential([\r\n    Dense(36, activation='relu', input_shape=(784,)),\r\n    Dense(10, activation='softmax')\r\n  ])\r\n  \r\n  basic_net.compile(optimizer=Adam(lr=0.0001), metrics=['accuracy'], loss=\"categorical_crossentropy\")\r\n  basic_net.fit(x=x_train, y=y_train, epochs=15)\r\n```\r\nwill do the right thing. Thanks for the report, and for the fantastic repro.\r\n\r\ncc @tomerk @qlzh727 @omalleyt12 @karmel ", "@danaugrs I think the issue was resolved in `tf-nightly` and `tf-nightly-2.0-preview`. I ran your code in both the versions and the accuracy after 15 Epochs is `78.57%` and `78.29` with `TF2.0` and `tf-nightly (TF.1.15)`. \r\n\r\nHere is [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/d57e90aed84f74b7c422bccb23abf48b/tfnightly20preview_29099_keras_perfomance_relu.ipynb) with `TF2.0` and [here is the gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/415672186e6fcadd6093198dc33be64e/tfnightly_29099_keras_perfomance_relu.ipynb) with `tf-nightly`.\r\n\r\nI am closing the issue as it was resolved recent nightly. Please feel free to open it if the issue persists again. Thanks!\r\n\r\n ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29099\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29099\">No</a>\n"]}, {"number": 29098, "title": "Refactor {Generator, Interleave, PaddedBatch} DatasetOps", "body": "This PR refactors `GeneratorDatasetOp`, `InterleaveDatasetOp`, and `PaddedBatchDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa Thanks for your review! The comments are addressed by this commit (https://github.com/tensorflow/tensorflow/pull/29098/commits/dc7ae9b8431a373e13d1689ea990b6a2df1d7761). Could you please have another look when you get a chance?", "@jsimsa The ubuntu CC checks failed caused by the undefined reference to some static members (these tests run successfully on my mac).  According to the bottom of [this reference](https://en.cppreference.com/w/cpp/language/static), the re-declaration seems to be necessary for C++11. Do you have any suggestion?", "> @jsimsa The ubuntu CC checks failed caused by the undefined reference to some static members (these tests run successfully on my mac). According to the bottom of [this reference](https://en.cppreference.com/w/cpp/language/static), the re-declaration seems to be necessary for C++11. Do you have any suggestion?\r\n\r\nIt [seems](https://stackoverflow.com/questions/2605520/c-where-to-initialize-static-const) that need to initialize static member `char *` constants in the .cc file.\r\n\r\nStarting from C++17 we will have another [option](https://stackoverflow.com/a/1563906/1497311).", "> > @jsimsa The ubuntu CC checks failed caused by the undefined reference to some static members (these tests run successfully on my mac). According to the bottom of [this reference](https://en.cppreference.com/w/cpp/language/static), the re-declaration seems to be necessary for C++11. Do you have any suggestion?\r\n> \r\n> It [seems](https://stackoverflow.com/questions/2605520/c-where-to-initialize-static-const) that need to initialize static member `char *` constants in the .cc file.\r\n> \r\n> Starting from C++17 we will have another [option](https://stackoverflow.com/a/1563906/1497311).\r\n\r\n@jsimsa Thanks for your help. It looks like that a constexpr static data member can be initialized right inside the class definition. Here is the quote from [this page](https://en.cppreference.com/w/cpp/language/static): \r\n\r\n> If a static data member of LiteralType is declared constexpr, it must be initialized with an initializer in which every expression is a constant expression, right inside the class definition. \r\n\r\n> If a const non-inline (since C++17) static data member or a constexpr static data member (since C++11) is odr-used, a definition at namespace scope is still required, but it cannot have an initializer. This definition is deprecated for constexpr data members (since C++17).\r\n\r\nIt seems to be a good practice to redeclare the static member as discussed [here](https://stackoverflow.com/questions/23439848/whether-redeclare-a-const-static-variable-outside-a-class-or-not). Following [some examples](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/remote_fused_graph_execute_utils.cc#L146-L171) in TF repo, do you think it will be good to add the redeclaration in the .cc files? ", "Yes, redeclaring the static member in .cc sounds good to me.", "@jsimsa The redeclarations are added in this commit (https://github.com/tensorflow/tensorflow/pull/29098/commits/4f465bb1e34197309ed0fbf4e0e66c0e8060c3e7). Could you please have another look?", "@jsimsa The internal checks failed. Could you help check and paste the log here?", "The test failures might have been spurious and the internal tests are current being re-ran.", "@jsimsa This PR is rebased to resolve the conflicts ([line 285-289](https://github.com/tensorflow/tensorflow/pull/29098/commits/832420653f34d7ec2ad38f628df25bacd256da79#diff-dc0ccd73f8e55e6d48eec8752cc8c169R285) and [294-295](https://github.com/tensorflow/tensorflow/pull/29098/commits/832420653f34d7ec2ad38f628df25bacd256da79#diff-dc0ccd73f8e55e6d48eec8752cc8c169R294)) with this commit (https://github.com/tensorflow/tensorflow/commit/ffdec8e49f14541afaf90bb4a40b89eafd6602ec). Could you have a look at the changes?", "@jsimsa The internal checks failed again. Could you help check if these failures are related?", "```\r\nthird_party/tensorflow/core/kernels/data/interleave_dataset_op_test.cc:15:10: error: module //third_party/tensorflow/core/kernels/data:interleave_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/kernels/data/name_utils.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:interleave_dataset_op_test\r\n#include \"third_party/tensorflow/core/kernels/data/name_utils.h\"\r\n```", "@jsimsa Thanks for posting the log here. The issue is resolved by removing the redundant import. Please have a look at this commit (https://github.com/tensorflow/tensorflow/pull/29098/commits/97747b9a0243f372ebe8b3cc794a076f1d667d65) when you get a chance. \r\n\r\nI will have a trip next week and may not be able to check my emails timely. Will reply your comments when I am online.", "@jsimsa The code in this PR has been merged here (https://github.com/tensorflow/tensorflow/commit/262634cca6097dfaaddc6ff902893376ac882999), but this PR is not closed. Please feel free to close it if needed."]}, {"number": 29097, "title": "[ROCm] Adding ROCm support for debug ops", "body": "This PR adds ROCm support for debug ops\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.  \r\n\r\n--------------------\r\n\r\n@tatianashp , @whchung\r\n", "comments": []}, {"number": 29096, "title": "Cannot install tensorflow via PIP ", "body": "so following the tutorial: https://www.tensorflow.org/install/pip?lang=python3 didnt really work..\r\ni even deleted all my previous python versions and got a singe python 3.6.0 64 bit running (all my other where 32 bit)\r\n\r\n`python --version` => 3.6.0\r\n`pip --version` =>19.1.1\r\n`virtualenv --version `=>16.6.0\r\n\r\n`pip install --upgrade tensorflow`\r\n=>\r\n> Collecting tensorflow\r\n>   ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\n> ERROR: No matching distribution found for tensorflow", "comments": ["Maybe consider following these instructions for the installation at [stackoverflow](https://stackoverflow.com/a/42596864).", "i have also tried to install via link found on the tensorflow install page, this gave the error wheel not supported.", "@extreme4all Did you get chance to try the link suggested by @lufol , Please let us know your OS", "> \r\n> \r\n> @extreme4all Did you get chance to try the link suggested by @lufol , Please let us know your OS\r\n\r\nfor some reason the link provided there (python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl) works but the windows one on tensorflow forum doesn't \r\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl\r\n\r\nEDIT i run windows 10 home\r\nintel core I7-4700MQ 2.4ghz\r\n16GB ram\r\n64 bit", "i have tried some extra things:\r\n\r\nall of these give the error not a suppported wheel on this platform:\r\n\r\nWindows\r\n--\r\nPython 3.5 CPU-only | https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp35-cp35m-win_amd64.whl\r\nPython 3.5 GPU\u00a0support | https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.13.1-cp35-cp35m-win_amd64.whl\r\nPython 3.6 CPU-only | https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl\r\nPython 3.6 GPU\u00a0support | https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.13.1-cp36-cp36m-win_amd64.whl\r\n\r\nthis does actualy work but gives error when running code demo.py [from](https://github.com/run-youngjoo/SC-FEGAN) :\r\n\r\nmacOS (CPU-only)\r\n--\r\nPython 2.7 | https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.13.1-py2-none-any.whl\r\nPython > 3.4 | https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.13.1-py3-none-any.whl\r\n\r\nerror:\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nEDIT\r\ni downloaded python 3.6.8 from https://www.python.org/downloads/windows/", "I also met this problem\u3002\r\nI have already solved it\u3002\r\nhere is my solution\uff1a\r\n    install anaconda-64bit\uff08This error will occur in other versions.\uff09\r\n    pip install virtualenv\r\n    activate virtualenv\r\n    pip install tensorflow", "@extreme4all did @forrestneo 's solution solved your problem?", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "pip install tensorflow\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n\r\nfor me, I got an error like this. can you all help?", "@cheadevit Can you please create a new issue with more details on the commands you used? Thanks!", "I solved this issue by:\r\n\r\npip install virtualenv\r\nactivate virtualenv \r\nsource python \r\nsource activate\r\npip install tensorflow==\"x.x.x\" (needed version)"]}, {"number": 29095, "title": "[ROCm] Adding a wrapper macro to declare dynamic shared memory", "body": "@chsigg \r\n\r\nThis is a follow-up PR for your request in PR #28451 \r\n\r\nIt creates a macro (with CUDA/ROCm specific implementations) for declaring dynamic shared memory on GPUs. \r\n\r\n---------------------\r\n\r\n@tatianashp @whchung ", "comments": ["I took a look at the logs for the failing CI runs. The errors in the CI runs do not seem to be related to the changes in this PR"]}, {"number": 29094, "title": "[ROCm] Added ROCm support for inplace_ops", "body": "This PR adds ROCm support for `inplace_ops`.  \r\n\r\n#### Background info\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n```\r\n//tensorflow/python/kernel_tests:inplace_ops_test                        PASSED in 10.7s\r\n```\r\n\r\n---- \r\n@tatianashp , @whchung", "comments": []}, {"number": 29093, "title": "[ROCm][XLA]Provide support for creation of AMDGPU kernels", "body": "Provide utility that can create both AMDGPU/NVPTX kernels ", "comments": ["@thomasjoerg, Thanks for the comments. I have incorporated both the suggestions. "]}, {"number": 29092, "title": "There have no tensorflow/contrib/tpu/ops module in b211c7a commit", "body": "Sorry if I'm something miss. It's my first attempt to make an issue.\r\n\r\ncommit b211c7a\r\n\r\nIn the :\r\ntensorflow/contrib/cmake/python_modules.txt:\r\n....\r\ntensorflow/contrib/tpu\r\ntensorflow/contrib/tpu/ops   <<< line 435\r\ntensorflow/contrib/tpu/profiler\r\ntensorflow/contrib/tpu/python\r\n...\r\n\r\nBut this module is not in the directory:\r\ntensorflow/contrib/tpu\r\n\r\n", "comments": ["@trialzuki Could you provide more details about the issue and context?", "> @trialzuki Could you provide more details about the issue and context?\r\n\r\nI tried to bild project from commit **b211c7a** (last release at 28 may 2019) with \r\n- Windows 7\r\n- Visual Studio 2019\r\n- cmake\r\n.. and got build error: \r\ntensorflow/contrib/cmake/tf_python.cmake:217 module tensorflow/contrib/tpu/ops does not exist. \r\n\r\nthen \r\nchecked tensorflow/contrib/cmake/tf_python.cmake .. \r\nchecked tensorflow/contrib/cmake/python_modules.txt ..\r\n\r\nand found that in tensorflow/contrib/cmake/python_modules.txt:435 \r\nnotes module tensorflow/contrib/tpu/ops, but there no module \"ops\" in tensorflow/contrib/tpu, only in\r\ntensorflow/contrib/tpu/python\r\n", "The required directories exist in [r.13 branch]. Hope this helps. (https://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/contrib/tpu)", "In commit:\r\n```\r\n-=-=- \r\ncommit a83ccd34e729bb642232e63925e5e95e8cf65c1d\r\nAuthor: Jonathan Hseu <jhseu@google.com>\r\nDate:   Wed Feb 13 16:58:28 2019 -0800\r\n\r\n    Move TPU ops to TF core.\r\n\r\n    PiperOrigin-RevId: 233859132\r\n-=-=-\r\n```\r\n\r\nPath tensorflow/contrib/tpu/ops was removed. \r\nI think Jonathan Hseu forgot remove line 217 from tensorflow/contrib/cmake/tf_python.cmake file.\r\n\r\nCan I fix it?", "Thanks @trialzuki . I just sent out a change to delete that line.", "https://github.com/tensorflow/tensorflow/commit/a45d4bd80c3d93466a8daf641fe699c6d00096f2\r\n\r\nIf that doesn't work for you, feel free to reopen or send a pull request.", "> [a45d4bd](https://github.com/tensorflow/tensorflow/commit/a45d4bd80c3d93466a8daf641fe699c6d00096f2)\r\n> \r\n> If that doesn't work for you, feel free to reopen or send a pull request.\r\n\r\nSorry, i still have the problem \"CMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/tpu/ops\"\r\n\r\nand I download the tensorflow-r1.14 from github\r\n\r\nDo you have any suggestion about the error? Many thanks for any information.", "Hi. \r\nAs I can remember I'm already make the same fix it in my local copy. )\r\n\r\n\r\n"]}, {"number": 29091, "title": "[ROCm] Adding ROCm support for data format ops", "body": "This PR adds ROCm support for the data format ops\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n---------------\r\n\r\n@tatianashp , @whchung", "comments": []}, {"number": 29090, "title": "Add contribution covenant badge", "body": "See https://github.com/tensorflow/community/issues/28", "comments": ["/cc @ewilderj"]}, {"number": 29089, "title": "TFLite model almost the same size as frozen graph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nFreeze [xception65_coco_voc_trainaug](http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz) from this [model zoo](https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md) using:\r\n```\r\npython tensorflow/models/deeplab/export_model.py ...\r\n```\r\n\r\nResulting `.pb` is 157mb.\r\n\r\nConvert `.pb` to `.tflite` using `tflite_convert`. Resulting `.tflite` is 156mb. Just 1mb smaller than the `.pb`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe TFLite model should perhaps be considerably smaller. If not, why not?\r\n\r\n[The frozen graph I'm using](https://drive.google.com/file/d/14usyseWBL36SGNdYPm17D07OQ1P_5Hjx/view?usp=sharing)", "comments": ["This is expected behavior because the model contains the same number of weights and ops. If you are interested in reducing the model size then please reference our [FAQ](https://www.tensorflow.org/lite/guide/faq#how_do_i_reduce_the_size_of_my_converted_tensorflow_lite_model). We also have the [model optimization toolkit](https://www.tensorflow.org/model_optimization) which was recently launched."]}, {"number": 29088, "title": "TF 2.0 Beta: custom metric example from documentation is wrong", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI implemented the custom metric shown in this page (CatgoricalTruePositives) https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation\r\n\r\nI think it is full of bugs, it has some comments saying (#TODO: fix this). Any way, here what's particularly wrong about it. The accuracy of the NN approaches 99%, yet, this metric says:\r\n\r\nbinary_true_positives: 8459.0000\r\n\r\n(as shown on the website too). If number of samples in MNIST is 50,000, then at least 45k of them should be true positives.  It is unstable. I messed with it once and I got 49k true positive (which makes total sense). Then I reran it and it returned to 8k.\r\n\r\n**Describe the expected behavior**\r\nThe results are shown on the website. For a low loss of 0.03, the true positives should be close to 50k. However they're shown to be 8k only.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Tried to execute the code and getting loss as 0.066 and categorical_true_positive : ~7863.", "Hi. Yes, I got the same thing. Now, if you add one more metric (accuracy) to your fit method, you will notice that the 0.06 loss you got actually correspond to very high accuracy (above 90%), which means that true positives should be well above 40,000. Not 7863. Please notice that we have 50k examples. The metric is way off base. \r\nI suspect that we need to add axis=1 argument to tf.argmax method. (Not sure)", "Hi. I have some great news for you. I solved it.\r\nHere what's wrong with your docs:\r\n\r\nIn the update_state method, you should have this:\r\n\r\n>       y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\r\n\r\nCurrently it is:\r\n\r\n>         y_pred = tf.argmax(y_pred)\r\n\r\nWith my fix, the categorical true positives will be close to 50k which makes total sense\r\n50000/50000 [==============================] - 1s 29us/sample - loss: 0.0905 - categorical_true_positives: 48647.0000 - accuracy: 0.9729\r\n\r\nBecause the accuracy is close to 100%.\r\n\r\nExplanation: your argmax should act along axis 1 not axis 0 which is the batch axis. It was returning nonsensical stuff like 49 and 64 which are indices of batch datapoints. Second thing, you need to reshape the result to match the shape of y_pred which is what I did.\r\n", "@kernelizd Thank you for looking into this. Do you want to send a PR updating the doc?", "@pavithrasv \r\nSure. Will do.", "\r\n@pavithrasv \r\nHi, I'm not an expert in github. I've just learned the basics. I made the pull request #674 \r\nI hope I did it correctly. Let me know.", "@kernelizd Thank you for creating a PR, the link you have provided is pointing to a different PR, can you share the entire url may be?", "@pavithrasv \r\n\r\nI'm sorry. Here is the url you asked for:\r\n\r\nhttps://github.com/tensorflow/docs/pull/674", "Looks like the diff is more than just this line that has changed. You may need to update your PR.", "Hi @pavithrasv  \r\nIt is exactly one line. No more.\r\nBut for some reason, Github says everything got changed. If you investigate it yourself, you see that there is no change at all. ", "Seems like indentation mismatch.", "I guessed as much. Different Jupyter version?", "Fixed in PR"]}, {"number": 29087, "title": "[ROCm] Adding ROCm support for the stateless random ops", "body": "This PR adds ROCm support for the stateless random ops\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-----------------------------------\r\n\r\n@tatianashp , @whchung", "comments": []}, {"number": 29086, "title": "Error trying tensorflow litem operations are not supported by GPU delegate", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Yes, aarch64, android 8.1\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.2\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm trying to run a c++ demo of tflite+opengles on my aarch64 android 8.1 board.\r\nI have tried to build  benchmark_model tool with bazel, it ran the [deeplabv3_257_mv_gpu.tflite ](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite)model successfully on my device.\r\n\r\nNow I want to integrate a simple demo of tflite (similar to benchmark) to my codes, which is built by cmake. I spent some time extract all the static libs of tensorflowlite from bazle-out folder and linked them in my cmake. I built the code successfully with ndk standalone toolchain r17c. \r\n\r\nBut when I ran this new demo on my device it shows me errors like:\r\n\r\n`INFO: Created TensorFlow Lite delegate for GPU.\r\nApply delegate for GPU\r\nNext operations are not supported by GPU delegate:\r\nAVERAGE_POOL_2D: Expected 1 input tensor(s), but node has 0 runtime input(s).\r\nCONV_2D: Expected 1 input tensor(s), but node has 0 runtime input(s).\r\nCONV_2D: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\nCONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\nDEPTHWISE_CONV_2D: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\nDEPTHWISE_CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\nRESIZE_BILINEAR: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\nFirst 1 operations will run on the GPU, and the remaining 69 on the CPU.\r\nTfLiteGpuDelegate Prepare: ReadValue: value is a constant tensor: 183\r\nNode number 70 (TfLiteGpuDelegate) failed to prepare.\r\n\r\nFailed to apply GPU delegate.\r\nDelegate setting done\r\nNode number 70 (TfLiteGpuDelegate) failed to prepare.\r\n\r\nFailed to allocate tensors!\r\n`\r\nThe model here is still deeplabv3_257_mv_gpu.tflite, which have been proved working on my device. I also have tried to build my new demo code in bazel, which perform correctly.\r\n\r\n\r\n**Describe the expected behavior**\r\nBuild Tflite + opengl delegate successfully with Cmake and perform correctly on my aarch64 board. \r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nuse your official benchmark_model.cc\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@lwu025 \r\n\r\nThe shader code is very sensitive to what a runtime tensor is and what a constant weight / bias is.  You might have changed some constant tensors as runtime tensors so that you can easily inspect tensor content, but that's not compatible with the GPU backend at this point.  From:\r\n\r\n> AVERAGE_POOL_2D: Expected 1 input tensor(s), but node has 0 runtime input(s).\r\n> CONV_2D: Expected 1 input tensor(s), but node has 0 runtime input(s).\r\n> CONV_2D: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n> CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\n> DEPTHWISE_CONV_2D: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n> DEPTHWISE_CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\n> RESIZE_BILINEAR: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n\r\nshows that there's bunch of wrongly connected things, e.g. what's an `AVERAGE_POOL_2D` or `CONV_2D` useful if there's no runtime tensor?\r\n", "Thanks. I rebuild the tensorflow lite with the following modifications:\r\nAdd the following code to tensorflow/lite/BUILD\r\n`cc_binary(\r\n\tname = \"libtensorflow-lite.so\",\r\n\tlinkopts=[\r\n\t\t\"-shared\",\r\n\t\t\"-Wl,-soname=libtensorflow-lite.so\",\r\n\t\t],\r\n\t\tlinkshared = 1,\r\n\t\tcopts = tflite_copts(),\r\n\t\tdeps = [\r\n\t\t\":framework\",\r\n\t\t\"//tensorflow/lite/kernels:builtin_ops\",\r\n\t] + select({\r\n        \"//tensorflow:android_arm64\": [\"//tensorflow/lite/delegates/gpu:gl_delegate\"],\r\n        \"//conditions:default\": [],\r\n    }),\r\n)`\r\n\r\nand build command:\r\n`sudo bazel build //tensorflow/lite:libtensorflow-lite.so --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++14\" --experimental_better_python_version_mixing\r\n`\r\n\r\nI followed the sample code in delegate/gpu/readme.md and my demo works good!\r\n\r\nI have one more question here:\r\nIs it possible to build the tensorflow lite lib for aarch64 linux (NO android!)?\r\nIf yes, how to build it in bazel?\r\n\r\n\r\n", "@lwu025 \r\n\r\nHm, good question.  I haven't built anything else but x86, android_arm64, and ios_arm64.  So, I don't know the answer to that question.\r\n\r\nre: GPU running on those platforms, we haven't tried, and thus we don't know either.  One comment I can add though is that we were running the GPU delegate on x86 desktop with mesa for unit tests.  So maybe something equivalent exists for aarch64, but I'm not sure =/\r\n\r\nGiven threads like https://github.com/tensorflow/tensorflow/issues/22629 I think it should be possible.  You should be able to build things with https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/BUILD and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/BUILD should mesa-equivalent thing run for your aarch64.", "@lwu025 tflite gpu only ios arm64 and x86_64?not support armv7 armv7s?\r\nI need armv7 armv7a as my ios project \r\n![image](https://user-images.githubusercontent.com/17869361/60001400-f9b38d00-9698-11e9-9f7d-9c1072e582be.png)\r\n", "@weinixuehao I never try armv7 before. You can try it simply replay the --cpu option when you build tflite.", "> @lwu025\r\n> \r\n> The shader code is very sensitive to what a runtime tensor is and what a constant weight / bias is. You might have changed some constant tensors as runtime tensors so that you can easily inspect tensor content, but that's not compatible with the GPU backend at this point. From:\r\n> \r\n> > AVERAGE_POOL_2D: Expected 1 input tensor(s), but node has 0 runtime input(s).\r\n> > CONV_2D: Expected 1 input tensor(s), but node has 0 runtime input(s).\r\n> > CONV_2D: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n> > CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\n> > DEPTHWISE_CONV_2D: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n> > DEPTHWISE_CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\n> > RESIZE_BILINEAR: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n> \r\n> shows that there's bunch of wrongly connected things, e.g. what's an `AVERAGE_POOL_2D` or `CONV_2D` useful if there's no runtime tensor?\r\n\r\n@impjdi : Can you explain this better by dumbing it down a bit, I have the same error, the **tflite model** works great on my pc with the _python interpreter_, but on android gives the following error:\r\n\r\n```\r\njava.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: Next operations are not supported by GPU delegate:\r\n2020-02-25 15:15:57.160 22863-22863/com.valuepitch.intruderdetector W/System.err: CONV_2D: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n2020-02-25 15:15:57.161 22863-22863/com.valuepitch.intruderdetector W/System.err: CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).\r\n2020-02-25 15:15:57.161 22863-22863/com.valuepitch.intruderdetector W/System.err: L2_NORMALIZATION: Operation is not supported.\r\n2020-02-25 15:15:57.161 22863-22863/com.valuepitch.intruderdetector W/System.err: MEAN: Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n2020-02-25 15:15:57.161 22863-22863/com.valuepitch.intruderdetector W/System.err: SUM: Operation is not supported.\r\n2020-02-25 15:15:57.162 22863-22863/com.valuepitch.intruderdetector W/System.err: TOPK_V2: Operation is not supported.\r\n2020-02-25 15:15:57.162 22863-22863/com.valuepitch.intruderdetector W/System.err: First 0 operations will run on the GPU, and the remaining 186 on th\r\n2020-02-25 15:15:57.163 22863-22863/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)\r\n2020-02-25 15:15:57.163 22863-22863/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:145)\r\n2020-02-25 15:15:57.164 22863-22863/com.valuepitch.intruderdetector W/System.err:     at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:311)\r\n2020-02-25 15:15:57.164 22863-22863/com.valuepitch.intruderdetector W/System.err:     at com.valuepitch.intruderdetector.RunModel.justnewTest(RunModel.java:723)\r\n2020-02-25 15:15:57.164 22863-22863/com.valuepitch.intruderdetector W/System.err:     at com.valuepitch.intruderdetector.RunModel.access$100(RunModel.java:43)\r\n```\r\n\r\n", "> Thanks. I rebuild the tensorflow lite with the following modifications:\r\n> Add the following code to tensorflow/lite/BUILD\r\n> `cc_binary( name = \"libtensorflow-lite.so\", linkopts=[ \"-shared\", \"-Wl,-soname=libtensorflow-lite.so\", ], linkshared = 1, copts = tflite_copts(), deps = [ \":framework\", \"//tensorflow/lite/kernels:builtin_ops\", ] + select({ \"//tensorflow:android_arm64\": [\"//tensorflow/lite/delegates/gpu:gl_delegate\"], \"//conditions:default\": [], }), )`\r\n> \r\n> and build command:\r\n> `sudo bazel build //tensorflow/lite:libtensorflow-lite.so --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++14\" --experimental_better_python_version_mixing `\r\n> \r\n> I followed the sample code in delegate/gpu/readme.md and my demo works good!\r\n> \r\n> I have one more question here:\r\n> Is it possible to build the tensorflow lite lib for aarch64 linux (NO android!)?\r\n> If yes, how to build it in bazel?\r\n\r\n@lwu025 : Does this remove the ` CONV_2D: Expected 1 input tensor(s), but node has 2 runtime input(s).` type errors? Or did you correct your network architecture as mentioned by @impjdi  _that there's bunch of wrongly connected things, e.g. what's an AVERAGE_POOL_2D or CONV_2D useful if there's no runtime tensor?_ ? to make your demo working", "@sirius0503 \r\n\r\nYeah, as you noted, the properties of the input tensors, the number of the input tensors, and the order of the input tensors is very important for the GPU delegate.  So for example, if you are trying to feed in some constant tensor just to see whether things produce the expected number, you're not gonna have a good time, because you'll get something like:\r\n\r\n> Expected 1 input tensor(s), but node has 0 runtime input(s).\r\n\r\nYou should properly have a placeholder and fill it in, rather than feeding a constant tensor.\r\n\r\nAlso, it's the same with the weights and biases.  They are supposed to be holding the content after training, so they should be read only constant tensors.  If you want to check whether things work, and make them runtime tensors, you will start seeing messages like:\r\n\r\n> Expected 1 input tensor(s), but node has 2 runtime input(s).\r\n\r\n`MEAN` doesn't compute the mean of multiple tensors, but just inside a single tensor.  It's added for instance normalization.  We probably haven't studied `MEAN` too well and didn't anticipate that use case (maybe it's a valid thing in TFLite).\r\n\r\nI don't think we ever implemented `L2_NORMALIZATION`, `SUM`, and `TOPK_V2`.\r\n\r\n\r\n\r\n\r\n", "@impjdi : I have used keras `input layer` for feeding inputs, and not a `tf.constant`, also the `CONV_2D` comes from my `pretrained model`, so I think as you mentioned, I have made my weights and biases as runtime tensors ( also what are runtime tensors) and how did my weights and biases become runtime tensors, can you explain. ( I don't have a clue). Also , `tf.nn.top_k, tf.nn.l2_normalization ` are supported for **tflite**, when you say there isn't an implementation are you saying a **gpu implementation**. Also, thanks for the great explanation! Really helped! :)\r\n\r\n> Yeah, as you noted, the properties of the input tensors, the number of the input tensors, and the order of the input tensors is very important for the GPU delegate.\r\n\r\nCan you also explain the above part : specifically how does _number of inputs, and order of the input tensors_ impact the GPU delegate?", "@sirius0503 \r\n\r\n> how did my weights and biases become runtime tensors, can you explain.\r\n\r\nUh, no idea.  I am not really familiar with training & converting; I usually only work with finished TFLite files.  I wonder whether you set up the TF graph wrong, so that it propagated all the way down to TFLite.  Or it could be that there is a bug in TOCO (which I doubt, as everyone must be screaming by now).\r\n\r\n> are you saying a gpu implementation\r\n\r\nThat is correct.  GPU shader implementation :)\r\n\r\n> number of inputs\r\n\r\nAs a hypothetical example, our `ADD` implementation currently only takes in 1 or 2 input tensors.  I think in TFLite, you can do more, but with TFLite GPU, if you do more, it will probably say it can't handle it.\r\n\r\n> order of the input tensors\r\n\r\nAs an example, the input tensors of `CONV_2D` must be 0: input tensor, 1: weight tensor, and 2: optional bias tensor.  You can't change thh order to, e.g., 0: bias tensor, 1: input tensor, 2: weight tensor, and hope that GPU can figure out the stuff by its own by looking at the tensor dimensions.", "@impjdi : \r\n\r\n>  how did my weights and biases become runtime tensors, can you explain.\r\n\r\nCan somebody else explain why this is happening ? I can't say whether I set the TF graph wrong. If someone can give me an example of such a wrong graph, I can better understand where I'm making mistakes.\r\n\r\nAlso, I used the command line `tflite_convert` and not **TOCO** or the `python api` can it make a difference?", "@sirius0503 \r\n\r\nYou froze the graph, right?", "@impjdi I am facing this error for simple subtraction but I am unable to understand your comment of \r\n\r\n> The shader code is very sensitive to what a runtime tensor is and what a constant weight / bias is. You might have changed some constant tensors as runtime tensors so that you can easily inspect tensor content, but that's not compatible with the GPU backend at this point. \r\n\r\nCan you explain in more details what it means and how to avoid such errors? What is a runtime tensor and what is a constant tensor? ", "A runtime tensor is a tensor that is computed with at least another.  The very first runtime tensor is possibly the input tensor of the model.\r\n\r\nA constant tensor is a tensor that is fixed.  These are, but not limited to, trained weights and biases for conv/fully connected ops or constants for add/mul or parameters for reshape etc.\r\n\r\nIf you think about the formula y = a * x + b, x and y are runtime tensors (variables) and a and b are constant tensors (constants).\r\n\r\n", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29086\">No</a>\n"]}]