[{"number": 16625, "title": "[For Test; DO NOT MERGE] Add grpcio as a pip dependency of tensorflow", "body": "", "comments": ["@inc0 this is related to an upcoming feature that requires the Python gRPC library. You'll hear about more about it.\r\n\r\nClosing this PR. The same change has been submitted as an internal change and will be sync'ed to external soon."]}, {"number": 16624, "title": "Crash caused by depthwise_conv on iOS", "body": "Hi\r\n\r\nI integrated the same tflite mode to android and iOS app, on android it always works fine, but on iOS the result is incorrect and sometimes app crashed, seems like there's an issue at depthwise_conv. \r\nThe crash log is as follows:\r\n\r\n```\r\nHardware Model:      iPhone7,1\r\nProcess:             Demo [287]\r\nPath:                /private/var/containers/Bundle/Application/D412E130-5C9E-45B1-A84D-FD93FFAB9025/Demo.app/Demo\r\nIdentifier:          Sensteer.Demo2\r\nVersion:             1.0.0 (1.0)\r\nCode Type:           ARM-64 (Native)\r\nRole:                Non UI\r\nParent Process:      launchd [1]\r\nCoalition:           Sensteer.Demo2 [457]\r\n\r\n\r\nDate/Time:           2018-01-31 19:08:36.5843 +0800\r\nLaunch Time:         2018-01-31 19:06:58.3105 +0800\r\nOS Version:          iPhone OS 11.1.2 (15B202)\r\nBaseband Version:    6.17.00\r\nReport Version:      104\r\n\r\nException Type:  EXC_BAD_ACCESS (SIGSEGV)\r\nException Subtype: KERN_INVALID_ADDRESS at 0x0000000102ea41dc\r\nVM Region Info: 0x102ea41dc is not in any region.  Bytes after previous region: 16861  Bytes before following region: 15908\r\n      REGION TYPE                      START - END             [ VSIZE] PRT/MAX SHRMOD  REGION DETAIL\r\n      mapped file            0000000102e64000-0000000102ea0000 [  240K] r--/r-- SM=ALI  \u0001\r\n--->  GAP OF 0x8000 BYTES\r\n      MALLOC_LARGE           0000000102ea8000-0000000102ec8000 [  128K] rw-/rwx SM=PRV  \r\n\r\nTermination Signal: Segmentation fault: 11\r\nTermination Reason: Namespace SIGNAL, Code 0xb\r\nTerminating Process: exc handler [0]\r\nTriggered by Thread:  0\r\n\r\nFiltered syslog:\r\nNone found\r\n\r\nThread 0 name:  Dispatch queue: com.apple.main-thread\r\nThread 0 Crashed:\r\n0   libsystem_platform.dylib      \t0x0000000185d21b60 _platform_memmove + 176\r\n1   Demo                          \t0x0000000100fcd238 tflite::optimized_ops::DepthwiseConv+ 463416 (float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, int, int, int, int, int, float, float, float*, tflite::Dims<4> const&) + 820\r\n2   Demo                          \t0x0000000100fd435c void tflite::ops::builtin::depthwise_conv::EvalFloat<(tflite::ops::builtin::depthwise_conv::KernelType)2>+ 492380 (TfLiteContext*, TfLiteNode*, TfLiteDepthwiseConvParams*, tflite::ops::builtin::depthwise_conv::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) + 744\r\n3   Demo                          \t0x0000000100fcbc60 TfLiteStatus tflite::ops::builtin::depthwise_conv::Eval<(tflite::ops::builtin::depthwise_conv::KernelType)2>+ 457824 (TfLiteContext*, TfLiteNode*) + 96\r\n4   Demo                          \t0x0000000100f827d4 tflite::Interpreter::Invoke+ 157652 () + 316\r\n```\r\n\r\n\r\n**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.2\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): cloned on 2018.1.24\r\nPython version: 2.7.10\r\nBazel version (if compiling from source): 0.7.0-homebrew", "comments": ["I tried again with the latest code on 2018.2.23, sometimes panic still exists, sometimes not panic but the result is incorrect. \r\n\r\n<img width=\"1430\" alt=\"2018-02-26 6 19 56\" src=\"https://user-images.githubusercontent.com/33706846/36705329-929b10d6-1b9f-11e8-9321-3a600e049b03.png\">", "Sorry this isn't resolved, @llyyun, could you please provide a reproducible test case so we can see the model architecture and exact shapes?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16623, "title": "toco crashed while change model to lite", "body": "I use toco to change model to lite format, it crashed like the following statck (PS: toco was compiled in my own machine with the tensorflow code. The git commit is 9fb9ac66ce5236ff045630cad6793f0531bf9d2c, with date Sun Jan 21 07:43:08 2018 +0800 )\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x000000000046bdf8 in void toco::(anonymous namespace)::EvaluateBinaryOperatorOnConstantInputs<(toco::ArrayDataType)2, (toco::ArrayDataType)2>(toco::Model*, toco::Operator const*) ()\r\n(gdb) bt\r\n#0  0x000000000046bdf8 in void toco::(anonymous namespace)::EvaluateBinaryOperatorOnConstantInputs<(toco::ArrayDataType)2, (toco::ArrayDataType)2>(toco::Model*, toco::Operator const*) ()\r\n#1  0x000000000046e3e9 in toco::ResolveConstantBinaryOperator::Run(toco::Model*, unsigned long) ()\r\n#2  0x0000000000489dd3 in toco::(anonymous namespace)::GraphTransformationsPass(int, toco::Model*, toco::GraphTransformationsSet const&) ()\r\n#3  0x000000000048aae0 in toco::RunGraphTransformations(toco::Model*, std::string const&, toco::GraphTransformationsSet const&) ()\r\n#4  0x000000000042b608 in toco::Transform(toco::TocoFlags const&, toco::Model*) ()\r\n#5  0x000000000041c5e8 in main ()\r\n\r\n\r\nIt can reproduce by unzip my attached file\r\n[test.zip](https://github.com/tensorflow/tensorflow/files/1681710/test.zip)\r\n, and run the command \r\ntoco --input_file=/home/jeyawn/ML/test.pb --output_file=/home/jeyawn/ML/test.lite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=FLOAT --input_shape=1,368,368,3 --input_array=image --input_data_type=FLOAT --output_arrays=Openpose/MConv_Stage6_L1_5_pointwise/BatchNorm/FusedBatchNorm,Openpose/MConv_Stage6_L2_5_pointwise/BatchNorm/FusedBatchNorm\r\n\r\n\r\nFollowing is my env:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux jeyawn-virtual-machine 3.13.0-128-generic #177-Ubuntu SMP Tue Aug 8 11:40:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"14.04.5 LTS, Trusty Tahr\"\r\nVERSION_ID=\"14.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux jeyawn-virtual-machine 3.13.0-128-generic #177-Ubuntu SMP Tue Aug 8 11:40:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.8.2)\r\nprotobuf (2.5.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nnvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n----------------------------------------------------------------------------------------------------\r\n\r\n\r\n\r\n", "comments": ["/CC @andrehentz, can you take a look?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 98 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 114 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 129 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I ran your test.zip file through toco and it worked for me. Please reopen if it still doesn't work when you run through a recent version of TF Lite."]}, {"number": 16622, "title": "update tensorflow to 1.5 ", "body": "I update tensorflow to 1.5,and reinstall cuda to 9.1,now  I run my program get the error:\r\nTraceback (most recent call last):\r\n  File \"/home/chris/tensorflowDemo/7_1_Word2Vec.py\", line 24, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/chris/.local/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "comments": ["That's an issue with your CUDA install: cuBLAS is not in your library path. If you are on Linux, you should set up your `LD_LIBRARY_PATH` properly, e.g. like this (depending on where you installed CUDA 9):\r\n```\r\nexport CUDA_HOME=/usr/local/cuda/\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$CUDA_HOME/lib64:$CUDA_HOME/extras/CUPTI/lib64\"\r\n```", "**I have set the variables**\r\nchris@chris:~$ echo $CUDA_HOME\r\n/usr/local/cuda-9.1\r\nchris@chris:~$ echo $LD_LIBRARY_PATH\r\n/home/chris/torch/install/lib:/home/chris/torch/install/lib:/usr/local/cuda-9.1/lib64:/usr/local/cuda-9.1/extras/CUPTI/lib64:/home/chris/torch/install/lib:\r\n\r\nand\uff0cI found that there is no such lib in the cuda/lib64.\r\ndoes tf1.5 not support cuda 9.1?\r\n", "In that case there is something wrong with your CUDA installation. CUDA should come with `libcublas.so` in the `lib64` subdirectory. My CUDA 9.1 installation has it, so should yours. Try to reinstall CUDA?", "@CNugteren I meet the same problem, and my CUDA come with libcublas.so in the lib64 subdirector.At this env,caffe can run well.I install tenworflow with cmd \"pip install tensorflow_gpu\"\r\n![2018-02-01 18-34-58](https://user-images.githubusercontent.com/8893270/35674136-ac0253e8-077e-11e8-89a5-103bb00dd2cf.png)\r\n", "\r\nI uninstalled the cuda9.1,and install cuda9.0. Now it works.", "Where did you install cuda 9.0 from? I only see there the 9.1 version (for ubuntu 16)", "@nadavb \r\nhttps://developer.nvidia.com/cuda-90-download-archive", "Closing, but please reopen if this is not resolved."]}, {"number": 16621, "title": "Using tf.train.SyncReplicasOptimizer with multiple optimizers", "body": "I am trying to run the DeepLab Resnet (https://github.com/DrSleep/tensorflow-deeplab-resnet) in a distributed setup. I opted Synchronous Data parallel training approach similar to the one demonstrated in the Inception distributed training example.(https://github.com/tensorflow/models/tree/master/research/inception).\r\n\r\nIn the Inception example, a single RMS optimizer is used to reduce the loss. The tf.train.SyncReplicasOptimizer function wraps the optimizer and becomes responsible for synchronization, aggregation and application of gradients to various workers. Also, it takes care of updating the global_step variable. In my case, the DeepLab Resnet makes use of three optimizers each handling specific portions of the network. Following snippet explains the case:\r\n\r\n    `#Three optimizers declared with different learning rates\r\n     opt_conv = tf.train.MomentumOptimizer(learning_rate, args.momentum)\r\n     opt_fc_w = tf.train.MomentumOptimizer(learning_rate * 10.0, args.momentum)\r\n     opt_fc_b = tf.train.MomentumOptimizer(learning_rate * 20.0, args.momentum)`\r\n\r\n    #Scope for every optimizer \r\n    grads = tf.gradients(reduced_loss, conv_trainable + fc_w_trainable + fc_b_trainable)\r\n    grads_conv = grads[:len(conv_trainable)]\r\n    grads_fc_w = grads[len(conv_trainable) : (len(conv_trainable) + len(fc_w_trainable))]\r\n    grads_fc_b = grads[(len(conv_trainable) + len(fc_w_trainable)):]\r\n\r\n    #Gradients applied to various portions of the network\r\n    train_op_conv = opt_conv.apply_gradients(zip(grads_conv, conv_trainable))\r\n    train_op_fc_w = opt_fc_w.apply_gradients(zip(grads_fc_w, fc_w_trainable))\r\n    train_op_fc_b = opt_fc_b.apply_gradients(zip(grads_fc_b, fc_b_trainable))\r\n \r\n    `#tf.group to combine all three operations\r\n    train_op = tf.group(train_op_conv, train_op_fc_w, train_op_fc_b)`\r\n   \r\nI don't have any clue about using the tf.train.SyncReplicasOptimizer for multiple optimizers to achieve synchronous data parallel training. Also, I don't have an idea about updating the global_step variable and using the chief_queue_runner for this case. Please help me on this.\r\n\r\n\r\n \r\n\r\n\r\n", "comments": ["When you say\r\n> I don't have any clue about using the tf.train.RMSPropOptimizer\r\n\r\nDo you mean tf.train.SyncReplicasOptimizer? I'll assume you do.\r\n\r\nIn any case, I'm not sure how to handle this. I think if you use three SyncReplicasOptimizer, each wrapping an optimizer, everything will work, but I'm not sure. /CC @ispirmustafa @panyx0718, any ideas?\r\n\r\n", "I don't think `tf.train.SyncReplicasOptimizer` designed to handle multiple optimizers. You can workaround it by creating your own optimizer. something like:\r\n\r\n```\r\nclass MyOptimizer(tf.train.Optimizer):\r\n  def __init__(...)\r\n     self.opt_conv = ..\r\n     self.opt_fc_w = ...\r\n    ...\r\n  def apply_gradient(...):\r\n     extract relevant gradients for each optimizer\r\n     self.opt_conv.apply_gradient(...)\r\n     self.opt_fc_w.apply_gradient(...)\r\n    ...\r\n\r\noptimizer = SyncReplicasOptimizer(MyOptimizer(), ...)\r\n\r\n```\r\n   \r\n     "]}, {"number": 16620, "title": "How to use model.summary() when using placeholder instead of Input(keras)", "body": "Dear all, \r\nI follow  post in \"https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\"\r\nThe little modified code I use is:\r\n----------------------------------------------------------------------------------------------------------------------------------\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.backend import categorical_crossentropy\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.python.keras.models import Model\r\n\r\n\r\nsess = tf.Session()\r\nimg = tf.placeholder(tf.float32, shape=(None, 784))\r\nx = Dense(128, activation='relu')(img)  # fully-connected layer with 128 units and ReLU activation\r\nx = Dense(128, activation='relu')(x)\r\npreds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation\r\n\r\nlabels = tf.placeholder(tf.float32, shape=(None, 10))\r\nloss = tf.reduce_mean(categorical_crossentropy(labels, preds))\r\nmnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\r\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\n\r\ninit_op = tf.global_variables_initializer()\r\nsess.run(init_op)\r\nwith sess.as_default():\r\n    for i in range(100):\r\n        batch = mnist_data.train.next_batch(50)\r\n        train_step.run(feed_dict={img: batch[0],\r\n                                  labels: batch[1]})\r\n\r\n----------------------------------------------------------------------------------------------------------------------------------\r\nIt work fine until I use model.summary :  \r\n\r\nmodel = Model(inputs=img, outputs=preds)\r\n\r\nThe error message show\" Input tensors to a Model must come from `tf.layers.Input`\"\r\nI can use tf.layers.Input to solve this problem.\r\nBut I really want to use tf.placeholder so I can feed data as I like.\r\nCan anyone help me?  Thanks!!", "comments": ["I solve my problem!!\r\nJust use inputs = Input(tensor=img)\r\neverything will be fine : )", "I have one more question, the code I use is:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.backend import categorical_crossentropy\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import  Input\r\nmnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\r\nimg_size_flat = 28*28\r\nimg = tf.placeholder(tf.float32, shape=(None, 784))\r\ninputs = Input(tensor=img)   # tf_input\r\n#inputs = Input(shape=(img_size_flat,))  #keras_input\r\nx = Dense(128, activation='relu')(inputs)  # fully-connected layer with 128 units and ReLU activation\r\nx = Dense(128, activation='relu')(x)\r\npreds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation\r\nmodel = Model(inputs=inputs, outputs=preds)\r\n#%%\r\nmodel.compile(optimizer='rmsprop',\r\n               loss='categorical_crossentropy',\r\n               metrics=['accuracy'])\r\nmodel.fit(x=mnist_data.train.images,\r\n           y=mnist_data.train.labels,\r\n           epochs=1, batch_size=128)\r\n\r\nThe error message shows \r\nValueError: ('Error when checking model input: expected no data, but got:', array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       ..., \r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32))\r\nBut when I commnet # tf_input and uncomment # keras_input\r\nIt work fine. How can I use #tf_input for this?\r\n", "Input layer used without tensor arg ```inputs = Input(shape=(784,))``` creates a placeholder tensor internally. When you train a model using this input, you are always required to give a value for the placeholder tensor. In your code snippet, ```x=mnist_data.train.images``` is the value you give for that tensor. So, if you are looking to feed in input values you can use this approach and you will not need to use `tensor` argument.\r\n\r\nThe [tensor](https://keras.io/layers/core/#input) argument is used when there is an existing input tensor. When this is used, no placeholder will be created and no input value is expected. As you are passing input value ```x=mnist_data.train.images``` when using the tensor arg, you are seeing the error message `Error when checking model input: expected no data`. Example usage of tensor argument:\r\n```\r\nimg = tf.convert_to_tensor(mnist_data.train.images, dtype=tf.float32)\r\ninputs = Input(tensor=img)\r\nx = Dense(128, activation='relu')(inputs)\r\nx = Dense(128, activation='relu')(x)\r\npreds = Dense(10, activation='softmax')(x)\r\nmodel = Model(inputs=inputs, outputs=preds)\r\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\r\nmodel.fit(y=mnist_data.train.labels,epochs=1, batch_size=55000)\r\n```\r\n\r\nFrom what I understand it looks like the first method should be good for your use case. Hope this helps!", "Thanks for your kindly reply.\r\nIt can train properly.\r\nBut even I change epochs from 1 to 100.\r\nThe acc still around 0.1...\r\nI also want to feed batch by batch instead of whole 55000 data.\r\nDo you have other suggestion? Thanks!", "You can use fit_generator with Python data generators to train data in batches. Search for fit_generator [here](https://keras.io/models/sequential/). You can also refer this Keras [issue](https://github.com/keras-team/keras/issues/107).\r\n\r\n```\r\ninputs = Input(shape=(784,))\r\nx = Dense(128, activation='relu', name='Boing')(inputs)\r\nx = Dense(128, activation='relu', name='Achar')(x)\r\npreds = Dense(10, activation='softmax', name='Paniyaram')(x)\r\nmodel = Model(inputs=inputs, outputs=preds)\r\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\r\nmodel.fit_generator(generator=training_generator, steps_per_epoch=steps_per_epoch, epochs=1000)\r\n```", "Thanks for your reply!!\r\nHere is my code:\r\n\r\n```\r\nimg_size_flat = 28*28\r\nbatch_size = 64\r\n\r\ndef gen(batch_size=32):\r\n    while True:\r\n        batch_data, batch_label = mnist_data.train.next_batch(batch_size)\r\n        yield batch_data, batch_label   \r\n\r\n\r\ninputs = Input(shape=(img_size_flat,))\r\nx = Dense(128, activation='relu')(inputs)  # fully-connected layer with 128 units and ReLU activation\r\nx = Dense(128, activation='relu')(x)\r\npreds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation\r\nmodel = Model(inputs=inputs, outputs=preds)\r\nmodel.compile(optimizer='rmsprop',\r\n               loss='categorical_crossentropy',\r\n               metrics=['accuracy'])\r\nmodel.fit_generator(gen(batch_size), steps_per_epoch=len(mnist_data.train.labels)//batch_size, epochs=2)\r\n\r\n```\r\n", "I have one more question in the following link:\r\nhttps://github.com/tensorflow/tensorflow/issues/16721\r\nI appreciate your help again!\r\n"]}, {"number": 16619, "title": "add new feature for tfdbg that ignoring specific layers", "body": "Using tfdbg, `has_inf_or_nan` filter is very useful. But when I need `inf` at some calculations in the network explicitly, this filter catches `inf` every time. (e.g. https://github.com/keras-team/keras/issues/9161 )\r\n\r\nSo I want to suggest new feature, `ignore option`, to `add_tensor_filter`. (I think it's around [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/wrappers/local_cli_wrapper.py#L193). right?)\r\n\r\nThis option can take layer's names and layers which is selected to this option are ignored explicitly when tfdbg check `inf_or_nan` in the network.\r\n\r\nThis is also useful other than in my case above. If my understanding is not correct, let me know.\r\n\r\nThanks.", "comments": ["@mercy0387 thanks for filing the feature request. I agree that adding arguments to tfdbg filters like `has_inf_or_nan` will be useful. I will work on the new feature.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Update: We have a CL under review for fulfilling this feature request."]}, {"number": 16618, "title": "Fixed typo", "body": "Fixed some typos", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I signed it!"]}, {"number": 16617, "title": "tf.tensordot on GPU", "body": "Hi, I came across a problem when using eager execution with GPU.\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution **:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: Python 2.7.12\r\n- **CUDA/cuDNN version**: cuda-9.0\r\n- **GPU model and memory**:  GeForce GTX 1080 Ti, 11Gb RAM\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    a = tf.ones([1, 2])\r\n    b = tf.ones([2, 1])\r\n    c = tf.tensordot(a, b, axes=1)`\r\n```\r\n\r\nI get the error\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'ListDiff' OpKernel for GPU devices compatible with node ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](dummy_input, dummy_input)  \r\n```\r\nIf I remove `device_policy=tfe.DEVICE_PLACEMENT_SILENT`, there is still an error\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Tensors on conflicting devices: cannot compute Cast as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(), or transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Cast] name: Tensordot/Cast/\r\n```\r\n", "comments": ["/CC @asimshankar", "Thanks for the report. This issue isn't specific to eager execution - the underlying problem is that `tf.tensordot()` utilizes operations on `tf.int32` tensors, and many operations do not have GPU kernels that support int32. For example, without eager execution, you'll see a similar error:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n  a = tf.ones([1, 2])\r\n  b = tf.ones([2, 1])\r\n  c = tf.tensordot(a, b, axes=1)\r\n\r\nwith tf.Session() as sess:\r\n  print(sess.run(c))\r\n```\r\nleads to:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Tensordot/Gather_3': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n         [[Node: Tensordot/Gather_3 = Gather[Tindices=DT_INT32, Tparams=DT_INT32, validate_indices=true, _device=\"/device:GPU:0\"](Tensordot/Shape_1, Tensordot/add_3)]]\r\n```\r\n\r\nThis probably has something to do with the use of `gather` and other operations with int32 axes here: https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/ops/math_ops.py#L2751\r\n\r\n@rmlarsen : Could you take a look and comment on the use of `tf.tensordot` on GPUs?", "@asimshankar Thanks. How can I switch between CPU and GPU when I use eager execution? Since some operations like `tf.tensordot` do not have GPU kernels and I need to accelerate other related operations. It seems that\r\n```python\r\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\r\n```\r\ndoesn't work.", "@xinmei9322, `tf.device()` blocks can be used to force operations on a particular device, even CPU. Also, `tf.tensordot` is not a primitive TensorFlow operation (in that it is a Python function that composes other existing TensorFlow operations). Note that operations inside a `tf.device()` block are telling TensorFlow \"these operations *must* execute on the provided device, or fail otherwise\". Operations placed outside a `tf.device()` block (or with `tf.device(None)`) tell TensorFlow \"feel free to pick a device that will work\".\r\n\r\nSo the simplest thing for your case would be to just move the `tf.tensordot()` call outside the `with tf.device()` block:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\n# Force a and b to be on GPU\r\nwith tf.device(\"/gpu:0\"):\r\n  a = tf.ones([1, 2])\r\n  b = tf.ones([2, 1])\r\n# Run tensordor outside the tf.device() block\r\nc = tf.tensordot(a, b, axes=1)\r\n```\r\n\r\nHope that helps.", "Are those int-operations missing for XLA as well? I'm getting:\r\n`No registered 'ListDiff' OpKernel for XLA_CPU_JIT devices compatible with node Tensordot/ListDiff`\r\nwhen running tfcompile on a graph that has tensordot calls.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@patrikohlsson - Yeah, it seems `ListDiff` does not have an XLA kernel defined yet. I'd suggest opening a separate issue (though, that will likely be marked as \"Contributions Welcome\", CC @hawkinsp )\r\n\r\n@xinmei9322 : Closing this since I believe the solution mentioned in https://github.com/tensorflow/tensorflow/issues/16617#issuecomment-362106679 should work.\r\n\r\nFeel free to reopen if I'm mistaking (haven't heard back in a while, hence closing the issue due to inactivity)"]}, {"number": 16616, "title": "[WIP] More generic configuration", "body": "https://github.com/tensorflow/tensorflow/issues/13851 \r\n@drpngx", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to inactivity."]}, {"number": 16615, "title": "Refactoring by extracting duplicate code into methods", "body": "I extracted duplicate code into methods to improve maintainability.", "comments": ["Sorry, I am not qualified to review this PR. Adding @suharshs.\r\n\r\n> No description provided.\r\n\r\nThis is not helpful. It's a 600 line change, please explain why you did it and what it does.\r\n", "Closing this pull request now."]}, {"number": 16614, "title": "AttributeError: module 'tensorflow' has no attribute 'keras'", "body": "(tensorflow) admins-Mac-Pro:get_started admin$ python premade_estimator.py\r\nTraceback (most recent call last):\r\n  File \"premade_estimator.py\", line 88, in <module>\r\n    tf.app.run(main)\r\n  File \"/Users/admin/Work/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"premade_estimator.py\", line 34, in main\r\n    (train_x, train_y), (test_x, test_y) = iris_data.load_data()\r\n  File \"/Users/admin/Work/tensorflow/models/samples/core/get_started/iris_data.py\", line 19, in load_data\r\n    train_path, test_path = maybe_download()\r\n  File \"/Users/admin/Work/tensorflow/models/samples/core/get_started/iris_data.py\", line 12, in maybe_download\r\n    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\r\nAttributeError: module 'tensorflow' has no attribute 'keras'", "comments": ["What is tensorflow version?", "tensorflow version 0.12.0", "The keras was added in tensorflow 1.4\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.5/RELEASE.md#release-141\r\nI think you should update tensorflow", "Thank you @rimidalv,  I upgrade it and it's working fine", "thank you", "thanks", "train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\r\n**AttributeError: 'module' object has no attribute 'keras'**\r\n\r\nOS: Mac 10.13.3\r\nTensorFlow Version: 1.5.0\r\n\r\n", "`**AttributeError: 'module' object has no attribute 'keras'**`\r\n\r\nPython 2.7.12\r\nTensorFlow 1.2.0\r\n Keras 2.2.4", "Hi Everyone, I am getting the AttributeError: module 'keras_applications' has no attribute 'set_keras_submodules'.  I am running the code on Ububtu 16.04 \r\nTensroflow:1.12\r\nI would appreciate your help with this. Thanks. \r\n```\r\n[Traceback (most recent call last):\r\n  File \"multiprocess_detect_actions.py\", line 11, in <module>\r\n    import object_detection.object_detector as obj\r\n  File \"/home/prashantb/Documents/prashant_workspaace/ACAM_Demo/object_detection/object_detector.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 88, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras import applications\r\n  File \"/home/prashantb/anaconda3/envs/demo/lib/python3.6/site-packages/tensorflow/python/keras/applications/__init__.py\", line 37, in <module>\r\n    keras_applications.set_keras_submodules)[0]:\r\nAttributeError: module 'keras_applications' has no attribute 'set_keras_submodules'\r\n]\r\n```\r\n\r\n", "still not resolved, my Tensorflow version 2.0.0a0", "@pakashu \r\n\r\nPlease check Keras-Applications version on your environment(\"pip list | grep Keras\"). Try with remove existing Keras-Applications and install freshly and try one more time.\r\n\r\nUse https://pypi.org/project/Keras-Applications/ to install Keras-Applications.", "I'm sorry to bump this closed ticket, but I'm having the same issue as Pakashu.  \r\n\r\n```AttributeError: module 'tensorflow' has no attribute 'keras'```\r\n\r\nOn tensorflow-gpu 2.0.0a0, even after a forced uninstall/reinstall of Keras-Applications.", "hey,\r\nI have the same issue when I working in google cloud.\r\nin my computer it works fine.\r\nwhat should I do?\r\nThanks!", "![image](https://user-images.githubusercontent.com/44237873/66004539-1240f980-e477-11e9-990e-f9d467ad51de.png)\r\nWhat should I do?", "I'm having the same issue here\r\n", "Similar ticket here: https://github.com/tensorflow/tensorflow/issues/28213\r\n\r\nI had the same problem because my folder was named tensorflow. Check if there isn't any other file named tensorflow for you.", "In my case in JupyterNotebook after fixing and reinstall all dependencies I still got this error while importing Keras.\r\nuntil restarting notebook: Kernel -> Restart", "> still not resolved, my Tensorflow version 2.0.0a0\r\n\r\ntry reinstall tensorflow, sunch as `pip install tensorflow-gpu==2.2.0rc2` solved my problem", "I have the same problem \r\ntrain_path = tf.keras.utils.get_file(\r\nAttributeError: module 'tensorflow' has no attribute 'keras'\r\n\r\nIt took me years to install tensorflow with bazel, and the keras is there installed. In tensorflow... the file is there:\r\n/usr/lib64/python3.6/site-packages/tensorflow/keras/preprocessing/image/__init__.py \r\nwhich it says it doesn't exist..\r\n\r\nI installed aside Keras, aside... Now keras is in \r\n ls /usr/lib/python3.6/site-packages/keras\r\nactivations.py  callbacks.py    engine           layers      models.py      preprocessing    utils\r\napplications    constraints.py  initializers.py  losses.py   objectives.py  __pycache__      wrappers\r\nbackend.py      datasets        __init__.py      metrics.py  optimizers     regularizers.py\r\n\r\nSo all the files are there installed. the same is in /usr/lib64/python3.6/site-packages/tensorflow/keras/....\r\n\r\nBut I continue with the problem \r\n\r\ntrain_path = tf.keras.utils.get_file(\r\nAttributeError: module 'tensorflow' has no attribute 'keras'\r\n\r\nAnd when I explicitly import keras, \r\nthe error is:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/keras/__init__.py\", line 19, in <module>\r\n    from . import datasets\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/keras/datasets/__init__.py\", line 14, in <module>\r\n    from . import imdb\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/keras/datasets/imdb/__init__.py\", line 11, in <module>\r\n    from tensorflow.python.keras.datasets.imdb import get_word_index\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py\", line 25, in <module>\r\n    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/keras/preprocessing/__init__.py\", line 26, in <module>\r\n    from tensorflow.python.keras.preprocessing import image\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/keras/preprocessing/image.py\", line 23, in <module>\r\n    from keras_preprocessing import image\r\n  File \"/usr/lib/python3.6/site-packages/keras_preprocessing/image.py\", line 18, in <module>\r\n    backend = get_keras_submodule('backend')\r\n  File \"/usr/lib/python3.6/site-packages/keras_preprocessing/__init__.py\", line 24, in get_keras_submodule\r\n    raise ImportError('You need to first `import keras` '\r\nImportError: You need to first `import keras` in order to use `keras_preprocessing`. For instance, you can do:\r\n\r\n```\r\nimport keras\r\nfrom keras_preprocessing import image\r\n```\r\n\r\nOr, preferably, this equivalent formulation:\r\n\r\n```\r\nfrom keras import preprocessing\r\n```\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/carlos/.PyCharmCE2019.3/config/scratches/classification.py\", line 7, in <module>\r\n    import keras\r\n  File \"/usr/lib/python3.6/site-packages/keras/__init__.py\", line 6, in <module>\r\n    'Keras requires TensorFlow 2.2 or higher. '\r\nImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`\r\n\r\nMy tensorflow installation when I do  \r\n%pip3 list | grep tensorflow is:\r\n\r\ntensorflow                             2.2.0\r\ntensorflow-estimator                   2.2.0\r\n\r\nAnd when I do:\r\n%pip3 list | grep keras\r\n\r\nkeras-tuner                            1.0.2rc1.dev0\r\n\r\n\r\nPlease, any help... How do I tell pycharm that everythin is there...!!!!\r\n\r\n"]}, {"number": 16613, "title": "tensorflow upgrade made the spyder-ide editor auto-complite fail in python 3.6.4", "body": "the enviroment is in the python 3.6.4, with the tensorflow upgrade to the new version 1.5.0. The function in spyder editor auto-complitation was failed. After I uninstall the package future and futures, the function was work again. \r\nI also made a test in python 3.5.4, and there was no problem.", "comments": ["Can you clarify what you mean by \r\n> The function in spyder editor auto-complitation was failed.\r\n\r\nNote I don't use spyder. This looks like a Spyder issue, not a TensorFlow issue.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity. Reopen if this is still an issue."]}, {"number": 16612, "title": "Simplify loader_impl.py logic around main Op Tensor.", "body": "", "comments": ["@drpngx would you mind taking a look or triaging?"]}, {"number": 16611, "title": "Bug: tf.train.monitoredtrainingsession non-chief worker does not start", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: GTX-1080 11GB (chief) and M2000 4GB (slave)\r\n- **Exact command to reproduce**:\r\nCUDA_VISIBLE_DEVICES='' python3 test.py --job-name ps --task-index 0\r\nCUDA_VISIBLE_DEVICES=0 python3 test.py --job-name worker --task-index 0\r\nCUDA_VISIBLE_DEVICES='' python3 test.py --job-name ps --task-index 1\r\nCUDA_VISIBLE_DEVICES=0 python3 test.py --job-name worker --task-index 1\r\n\r\n### Describe the problem\r\nI am trying to run the [distributed training](https://www.tensorflow.org/deploy/distributed) and use [tf.train.MonitoredTrainingSession](https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession) with 2 PCs. PC1 has one GTX-1080 11GB and is set as chief, while PC2 has one M2000 4GB and is set as non-chief. They are connected back-to-back without switch/router. The chief worker was running okay but the non-chief worker was stuck at `tf.train.MonitoredTrainingSession` and did not proceed to execute the code within the session.\r\n\r\n### Source code / logs\r\n```\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\ndef parse_command():\r\n    parser = argparse.ArgumentParser(description='Monitor Training Session Test.')\r\n    parser.add_argument('--job-name', dest='job_name', default=\"worker\", nargs='?', help='job name [worker|ps]')\r\n    parser.add_argument('--task-index', dest='task_index', type=int, default=0, help='task index')\r\n    return parser.parse_args()\r\n\r\nif __name__ == '__main__':\r\n    print(\"Test started...\")\r\n\r\n    cluster = {\r\n        \"ps\" : [\r\n             \"192.168.0.2:2221\",\r\n             \"192.168.0.1:2221\"\r\n             ],\r\n        \"worker\" : [\r\n             \"192.168.0.2:2222\",\r\n             \"192.168.0.1:2222\"\r\n             ]}\r\n\r\n    options = parse_command()\r\n    cluster_spec = tf.train.ClusterSpec(cluster)\r\n    server = tf.train.Server(server_or_cluster_def=cluster_spec,\r\n                             job_name=options.job_name,\r\n                             task_index=options.task_index)\r\n\r\n    if options.job_name == \"ps\":\r\n        server.join()\r\n        sys.exit(0)\r\n\r\n    is_chief = (options.task_index == 0)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    config.log_device_placement = True\r\n    step_size = 5\r\n\r\n    print(\"Running \" + options.job_name + \":\" + str(options.task_index))\r\n\r\n    with tf.device(tf.train.replica_device_setter(\r\n        worker_device = \"/job:worker/task:%d\" % options.task_index,\r\n        cluster = cluster_spec)) :\r\n\r\n        global_step = tf.train.get_or_create_global_step()\r\n        learning_rate = tf.train.exponential_decay(0.1, global_step, step_size, 0.94, staircase=True)\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                               is_chief=is_chief) as session:\r\n\r\n            print(\"MonitoredTrainingSession started\")\r\n\r\n            for i in range(10):\r\n                for j in range(step_size):\r\n                    lr, gstep = session.run([learning_rate, global_step])\r\n                    print(\"learning rate=\" + str(lr) + \", global step=\" + str(gstep))\r\n```\r\n\r\n**PC1 (GTX-1080) Logs**\r\n\r\n```\r\n...\r\nRunning worker:0\r\n2018-01-31 10:29:44.888497: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 9a6571b5ba45d49d with config:\r\nMonitoredTrainingSession started\r\nlearning rate=0.1, global step=0\r\nlearning rate=0.1, global step=0\r\nlearning rate=0.1, global step=0\r\n...\r\n```\r\n\r\n**PC2 (M2000) Logs**\r\n```\r\n...\r\n2018-01-31 10:29:37.753239: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\r\nRunning worker:1\r\n(...wait for 1800 secs)\r\nMonitoredTrainingSession \"Session was not ready after waiting 1800 secs\"\r\n```\r\n\r\nI was using the [tf.train.SyncReplicasOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer) example to implement between-graph and synchronous training but found that the non-chief worker has never printed `MonitoredTrainingSession started`. Then I slowly remove all the unnecessary code (which becomes the code provided above) and found that `tr.train.MonitoredTrainingSession` does not seem to work for the bare minimum configuration. Please can you kindly have a look? \r\n\r\nMany thanks!", "comments": ["Thank you for creating a short example! I am able to reproduce the issue.\r\n\r\n/CC @ispirmustafa, know what the problem is?", "I'm experiencing the same problem.", "I had a similar issue but found that removing...\r\n\r\n    device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % args.task_index],\r\n\r\n... from my session configuration fixed it in my case.  `task_index` refers to the worker 0/1/2/3 as in the example given at https://www.tensorflow.org/deploy/distributed, and 0 is treated as the chief.", "Hi @chesschi,\r\nDo you still see this issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to staleness. Please use the latest version for TensorFlow and build again. Feel free to open a new issue if it still persists. Thanks!", "> Closing due to staleness. Please use the latest version for TensorFlow and build again. Feel free to open a new issue if it still persists. Thanks!\r\n\r\nI used tf1.4.0 and raise the error \"Session was not ready after 1800 seconds\", which version of tensorflow worked? thanks."]}, {"number": 16610, "title": "Fixed typo", "body": "changed variable \"sensorOrienation\" to \"sensorOrientation\". This was annoying me", "comments": []}, {"number": 16609, "title": "Tensorflow on AMD - specific chips", "body": "Hi\r\n\r\nI live abroad so the choice in hardware is more limited here. I only have the below option for graphics card, but from reading i find it hard to figure out if tensorflow would work w this or not. Some threads say that it works with \"newer ones from AMD\", others say \"it requires a lot of setup\". Can anyone say if tensorflow can run of the gpu of the below \"out of the box\"?\r\n\r\nAMD Radeon Pro WX 4150 w/4GB GDDR5", "comments": ["/CC @benoitsteiner can you comment on AMD and OpenCL support?", "See https://github.com/tensorflow/tensorflow/issues/14736#issuecomment-349733227", "Bhack, I have seen that thread, that's one of the ones I referred to, it's totally unclear. One guy says not ready, next guy says it works for newer ones. So I askee for mine specifically", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "according to this link https://www.notebookcheck.net/AMD-Radeon-Pro-WX-4150-GPU.215312.0.html\r\nAMD Radeon Pro WX 4150's architecture is Polaris which is supported by hiptensorflow.  for related info and installation see:\r\nhttps://github.com/ROCmSoftwarePlatform/hiptensorflow/blob/hip/README.ROCm.md\r\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "From what I'm reading, the solution would have been the abovementioned hiptensorflow, where HIP is a tool to convert code from CUDA (nvidia specific) to ROCm (AMD specific). Sadly it looks like that repo has been pulled from github, all links (even from current Google searches) give 404. Would be great to hear an official update on the status.", "@jharbott ROCm not only AMD...", "https://github.com/ROCmSoftwarePlatform/tensorflow", "Check also https://github.com/ROCmSoftwarePlatform/tensorflow/issues/35#comment-379410874", "I think that we can close this for https://github.com/ROCmSoftwarePlatform/tensorflow/issues/37#issuecomment-379568900. /cc @gstoner", "Nagging Assignee @tatianashp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing the issue since the question has been answered."]}, {"number": 16608, "title": "Can't find Stochastic Tensors class", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **Ubuntu 16.04**:\r\n- **source**:\r\n- **TensorFlow version 1.5**:\r\n- **3.6**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nApparently, `stochastic_tensor` is not present in `tf.contrib.bayesflow` which doesn't reflect here in the [api_guides](https://www.tensorflow.org/api_guides/python/contrib.bayesflow.stochastic_tensor). I have not mentioned my complete system information as I think it might not be needed, please let me know if it is required to further investigate the issue. Any help/suggestions would be highly appreciated.\r\n\r\n### Source code / logs\r\n```\r\nIn [7]: import tensorflow as tf\r\n\r\nIn [8]: st=tf.contrib.bayesflow.stochastic_tensor\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-0979a5c2af2d> in <module>()\r\n----> 1 st=tf.contrib.bayesflow.stochastic_tensor\r\n\r\nAttributeError: module 'tensorflow.contrib.bayesflow' has no attribute 'stochastic_tensor'\r\n\r\nIn [9]: tf.__version__\r\nOut[9]: '1.5.0-rc1'\r\n```", "comments": ["There seems to be something called `stochastic_variables`, but I don't see anything called `stochastic_tensor`.", "The api guide [LINK](https://www.tensorflow.org/api_guides/python/contrib.bayesflow.stochastic_tensor) shows it should have been there for version 1.5. Also, looking at the documentation source [here](https://github.com/tensorflow/tensorflow/blob/6e054dbd4b741d5b8fa8af93fdd7c9b74ae67ce0/tensorflow/docs_src/api_guides/python/contrib.bayesflow.stochastic_tensor.md) it looks like this wrapper was present in this [file](https://github.com/tensorflow/tensorflow/blob/6e054dbd4b741d5b8fa8af93fdd7c9b74ae67ce0/tensorflow/contrib/bayesflow/python/ops/stochastic_tensor.py).", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We're deprecating StochasticTensor (ST).  As we move our code to github/tensorflow/probability, we'll ensure that no functionality is lost; you will however have to port your existing use of StochasticTensor.  To that end, our new design pattern is to either use a new version of Edward (which will be checked into github/tensorflow/probability) or to write your probabilistic model as a callable and pass that as argument to functions.\r\n\r\nFor example, previously you could use a VIMCO loss with ST. The same functionality exists as part of:\r\nhttps://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/vi/csiszar_divergence.py#L885", "Nagging Assignee @jvdillon: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 79 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 94 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jvdillon: It has been 109 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "`StochasticTensor` is deprecated."]}, {"number": 16607, "title": "MKL: Pooling and AddN bug fixes", "body": "", "comments": ["@tensorflow-jenkins test this please."]}, {"number": 16606, "title": "tf.contrib.data.rejection_resample not balancing class/freq on random crops", "body": "randomly sampling / cropping data seems to break rejection_resample ==> meaning it won't do any re balancing of the class probability -- see these two simple feeders as example:\r\n\r\nThe first randomly samples data with tf.random_uniform and breaks rejection_resample -- the second with static random data having  the same distribution is correctly resampled (output has p(class=0)=0)\r\n\r\nThis creates issues when trying to real-time sample/crop/data-augment and at the same time rebalance classes with on the same pipeline\r\n\r\n```python\r\ndef get_data_breaks(self, batch_size, iihook, this_set='train'):\r\n    def sample(data, label): # sample detection window inside chunk\r\n        xx = tf.cast(tf.random_uniform([1])*self.class_num, tf.int32)[0]\r\n        with tf.control_dependencies([xx]): tf.Print(xx , [xx], 'xx>>')\r\n        return xx, xx\r\n\r\n    initial_dist=[1.0/self.class_num for cc in range(self.class_num)]\r\n    classes = np.random.choice(self.class_num,20000,p=initial_dist)\r\n\r\n    data_ph = tf.placeholder(classes.dtype, classes.shape)\r\n    labels_ph = tf.placeholder(classes.dtype, classes.shape)\r\n    dataset = tf.data.Dataset.from_tensor_slices((data_ph, labels_ph))\r\n\r\n    dataset = dataset.map(sample, num_parallel_calls=1)\r\n\r\n    target_dist=[1.0/self.class_num for cc in range(self.class_num)]\r\n    target_dist[1]+=target_dist[0] ; target_dist[0]=0\r\n    print('target-dist>>', target_dist)\r\n    initial_dist = None\r\n\r\n    dataset = dataset.apply(tf.contrib.data.rejection_resample(\r\n                class_func=lambda c, _: c,\r\n                target_dist=target_dist,\r\n                initial_dist=initial_dist,\r\n                seed=42)).map(lambda a,b: b)\r\n\r\n    dataset = dataset.repeat(None)\r\n    iterator = dataset.make_initializable_iterator()\r\n    iihook.iterator_initializer_func = lambda sess: sess.run(iterator.initializer,\r\n                    feed_dict={data_ph: classes, labels_ph: classes})\r\n\r\n    return iterator.get_next()\r\n\r\n\r\ndef get_data_works(self, batch_size, iihook, this_set='train'):\r\n\r\n    initial_dist=[1.0/self.class_num for cc in range(self.class_num)]\r\n    classes = np.random.choice(self.class_num,20000,p=initial_dist)\r\n\r\n    data_ph = tf.placeholder(classes.dtype, classes.shape)\r\n    labels_ph = tf.placeholder(classes.dtype, classes.shape)\r\n    dataset = tf.data.Dataset.from_tensor_slices((data_ph, labels_ph))\r\n\r\n    target_dist=[1.0/self.class_num for cc in range(self.class_num)]\r\n    target_dist[1]+=target_dist[0] ; target_dist[0]=0\r\n    print('target-dist>>', target_dist)\r\n    initial_dist = None\r\n\r\n    dataset = dataset.apply(tf.contrib.data.rejection_resample(\r\n                class_func=lambda c, _: c,\r\n                target_dist=target_dist,\r\n                initial_dist=initial_dist,\r\n                seed=42)).map(lambda a,b: b)\r\n\r\n    dataset = dataset.repeat(None)\r\n    iterator = dataset.make_initializable_iterator()\r\n    iihook.iterator_initializer_func = lambda sess: sess.run(iterator.initializer,\r\n                    feed_dict={data_ph: classes, labels_ph: classes})\r\n\r\n    return iterator.get_next()\r\n", "comments": ["@joel-shor ptal?", "I must say my use-case is a bit different ~ maybe that's why we didn't run into this issue before.\r\n\r\nBasically my sampler can change class_it (this is just a skeleton of my main sampler // minimal code that breaks) --  think of sampling random part of an image and depending where you land you hit a different object so class_id will be different ... that clearly will change stats for the following rejection_resampler ~ but somehow the resampler bails and gives me uniform (input?) frequencies when I use the TF sampler (call the map(sample)) - but works when using fixed random stream\r\n\r\nI can post a fully runnable script if you wish / if it helps\r\n\r\nps// hello png :)", "here, I made it nice and self-contained -- just run this --\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef fix_sample_and_rebalance(which='works'):\r\n    class_num=8\r\n    def get_data_breaks(batch_size=1):\r\n        def sample(data, label): # sample detection window inside chunk\r\n            xx = tf.cast(tf.random_uniform([1])*class_num, tf.int32)[0]\r\n            # with tf.control_dependencies([xx]): tf.Print(xx , [xx], 'xx>>')\r\n            return xx, xx\r\n\r\n        initial_dist=[1.0/class_num for cc in range(class_num)]\r\n        classes = np.random.choice(class_num,20000,p=initial_dist)\r\n\r\n        data_ph = tf.placeholder(classes.dtype, classes.shape)\r\n        labels_ph = tf.placeholder(classes.dtype, classes.shape)\r\n        dataset = tf.data.Dataset.from_tensor_slices((data_ph, labels_ph))\r\n\r\n        dataset = dataset.map(sample, num_parallel_calls=1)\r\n\r\n        target_dist=[1.0/class_num for cc in range(class_num)]\r\n        target_dist[1]+=target_dist[0] ; target_dist[0]=0\r\n        print('target-dist>>', target_dist)\r\n        initial_dist = None\r\n\r\n        dataset = dataset.apply(tf.contrib.data.rejection_resample(\r\n                    class_func=lambda c, _: c,\r\n                    target_dist=target_dist,\r\n                    initial_dist=initial_dist,\r\n                    seed=42)).map(lambda a,b: b)\r\n\r\n        dataset = dataset.repeat(None)\r\n        iterator = dataset.make_initializable_iterator()\r\n        iterator_initializer_func = lambda sess: sess.run(iterator.initializer,\r\n                        feed_dict={data_ph: classes, labels_ph: classes})\r\n\r\n        return iterator.get_next(), iterator_initializer_func\r\n\r\n    def get_data_works(batch_size=1):\r\n        initial_dist=[1.0/class_num for cc in range(class_num)]\r\n        classes = np.random.choice(class_num,20000,p=initial_dist)\r\n\r\n        data_ph = tf.placeholder(classes.dtype, classes.shape)\r\n        labels_ph = tf.placeholder(classes.dtype, classes.shape)\r\n        dataset = tf.data.Dataset.from_tensor_slices((data_ph, labels_ph))\r\n\r\n        target_dist=[1.0/class_num for cc in range(class_num)]\r\n        target_dist[1]+=target_dist[0] ; target_dist[0]=0\r\n        print('target-dist>>', target_dist)\r\n        initial_dist = None\r\n\r\n        dataset = dataset.apply(tf.contrib.data.rejection_resample(\r\n                    class_func=lambda c, _: c,\r\n                    target_dist=target_dist,\r\n                    initial_dist=initial_dist,\r\n                    seed=42)).map(lambda a,b: b)\r\n\r\n        dataset = dataset.repeat(None)\r\n        iterator = dataset.make_initializable_iterator()\r\n        iterator_initializer_func = lambda sess: sess.run(iterator.initializer,\r\n                        feed_dict={data_ph: classes, labels_ph: classes})\r\n\r\n        return iterator.get_next(), iterator_initializer_func\r\n\r\n\r\n    with tf.Session() as sess:\r\n        if which == 'works':\r\n            get_next, iterator_initializer_func = get_data_works()\r\n        else:\r\n            get_next, iterator_initializer_func = get_data_breaks()\r\n        iterator_initializer_func(sess)\r\n        returned = []\r\n        for kk in range(0,10000):\r\n            try:\r\n                sample=sess.run(get_next)\r\n                # print(sample[0].shape, sample[1].shape, sample[1])\r\n                returned.append(sample[0])\r\n                print(np.bincount(np.array(returned)))\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n\r\n# output is nice and (un)balanced 0 on first class 2x on second and then all unif\r\n# [   0 2485 1284 1267 1271 1228 1250 1215]\r\nfix_sample_and_rebalance()\r\n\r\n# output shows po(x)==pi(x) -- rejection_resample didn't do nada~niente~nulla!\r\n# [1193 1247 1267 1341 1264 1260 1191 1237]\r\nfix_sample_and_rebalance('breaks')\r\n", "FWIW, I'm still debugging but simplified your example:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass_num=8\r\ndata_np = np.random.choice(class_num,20000,p=[1.0/class_num]*class_num)\r\n\r\ndef sample(_):\r\n  xx = tf.cast(tf.random_uniform([1])*class_num, tf.int32)[0]\r\n  return xx\r\n\r\ndef fix_sample_and_rebalance(which='works'): \r\n  data_tensors = tf.constant(data_np, dtype=tf.int32)\r\n  dataset = tf.data.Dataset.from_tensor_slices(data_tensors)\r\n\r\n  target_dist = [1.0/class_num] * class_num\r\n  target_dist[1]+=target_dist[0] ; target_dist[0]=0\r\n  print('target-dist>>', target_dist)\r\n\r\n  if which == 'breaks':\r\n    dataset = dataset.map(sample)\r\n\r\n  dataset = dataset.apply(tf.contrib.data.rejection_resample(\r\n      class_func=lambda c: c,\r\n      target_dist=target_dist))\r\n  dataset = dataset.map(lambda a, _: a)\r\n\r\n  return dataset.make_one_shot_iterator().get_next()\r\n\r\ndef run_thing(which):\r\n  tf.reset_default_graph()\r\n  with tf.Session() as sess:\r\n    get_next = fix_sample_and_rebalance(which)\r\n    returned = []\r\n    for kk in range(0,100):\r\n      try:\r\n          sample=sess.run(get_next)\r\n          returned.append(sample)\r\n      except tf.errors.OutOfRangeError:\r\n          break\r\n    print(np.bincount(np.array(returned)))\r\n\r\n\r\nrun_thing('works')  # this works\r\nrun_thing('breaks')  # this doesn't\r\n```", "Nagging Assignee @joel-shor: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @joel-shor: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16605, "title": "Description in docs of one-hot vector for mnist deep example confusing and/or wrong", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.4\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**:n/a\r\n- **GCC/Compiler version (if compiling from source)**:n/a\r\n- **CUDA/cuDNN version**:n/a\r\n- **GPU model and memory**:n/a\r\n- **Exact command to reproduce**:n/a\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDocumentation found at https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros describes one of the two parameters passed to the training process as a 2d tensor of \"one-hot\" 10-dimensional vectors specifying the classes of the samples in the other 2d tensor parameter. But clearly the placeholders define 2d and 1d tensors, not 2d and 2d. There is no \"one-hot\" representation used at all as far as I can tell by using print() statements - if the class if a sample is class 3, then the corresponding entry is simple 3, not the one-hot representation of it. If this class is subsequently converted into a one-hot representation, it does not happen in the mnist_deep.py source file. I'm too much of a beginner to say what the documentation should say, but it seems at best confusing, and at worst completely wrong.\r\n\r\n### Source code / logs\r\nn/a\r\n", "comments": ["This example was part of the latest release until a few days ago. It's probably a very low priority to fix it now.", "The tutorial has been deleted, so it doesn't really matter. But the lines\r\n\r\n```python\r\nx = tf.placeholder(tf.float32, shape=[None, 784])\r\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\r\n```\r\nindicate both arguments are 2d tensors. Where did you place the `print()` statements? ", "I got tensorflow on 25 Jan. On that date, the mnist_deep.py program specified shape=[None] for the y_ parameter. Clearly something changed that was not obvious to someone innocently reading the documentation online.\r\n\r\nIt would be very helpful if the new online documentation for Iris examples had clear date/version information on the python code they refer to in case someone changes the underlying code again.", "Ah, I see the problem. The tutorial you linked to has the line\r\n> You can copy and paste each code snippet from this tutorial into a Python environment to follow along, or you can download the fully implemented deep net from [mnist_deep.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py).\r\n\r\nThe tutorial had the correct shape, but mnist_deep.py has a 1D shape for some reason, and other parts of the code were modified to accommodate that. Thank you for finding this discrepancy! Since has been deleted, there isn't anything to fix anymore.\r\n\r\n/CC @jugglerix, can you comment on the version information suggestion? If someone modifies the python file for a tutorial, it might become out of sync with the documentation on tensorflow.org, which is only updated when a new TensorFlow version comes out."]}, {"number": 16604, "title": "Branch 183881907", "body": "", "comments": ["fyi I think @case540 is also doing a push at #16595. But this one includes the clang format change with possible conflicts?", "@yifeif Yes, this includes the formatting ones."]}, {"number": 16603, "title": "1.3.0-py3 flagged by security issue CVE-2017-5754", "body": "Any chance a rebuild of the 1.3.0-py3 docker image is easy enough to pick up security patches?  It would save a team a lot of work.  Thank you.", "comments": ["/CC @gunan ", "CC @martinwicke \r\n`apt-get update && apt-get upgrade` should be sufficient on the images.\r\nBut if the host system is patched, would the guest system on running docker be vulnerable?", "And looking at this, looks like that is exactly what you need to do:\r\nhttps://security.stackexchange.com/questions/176657/how-to-mitigate-meltdown-in-docker-images", "This page mentions the \"Userspace Mitigations\" separate from the host kernel:\r\nhttps://wiki.ubuntu.com/SecurityTeam/KnowledgeBase/SpectreAndMeltdown\r\n\r\nI can't believe I didn't think of `apt-get update && apt-get upgrade` in the downstream image to address the issue, simple.  Sorry for the noise.", "https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#run\r\n\r\n> You should avoid RUN apt-get upgrade or dist-upgrade, as many of the \u201cessential\u201d packages from the parent images can\u2019t upgrade inside an unprivileged container. If a package contained in the parent image is out-of-date, you should contact its maintainers.\r\n\r\nOk, sorry for the Just-In-Time learning, but I do think I was right in opening this ticket.  Up to you if you wish to support an older version of tensorflow or not, but it would save the team I'm working with from maintaining the 1.3 base image themselves until they are ready to upgrade.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Sorry for the late reply,\r\nI am confused, the page I shared mentions:\r\n```\r\nThe patch against Meltdown is kernel only. Docker containers run within the kernel of the host system. This means the resistance against Meltdown depends on the host kernel only.\r\n```\r\n\r\nSo from what I can understand, you upgrade the host machine and you are safe, no matter what the docker container has?\r\n", "The security scan that halts our build flagged [CVE-2017-5754](https://people.canonical.com/~ubuntu-security/cve/2017/CVE-2017-5754.html).  That page mentions meltdown.\r\n\r\nThat page links to the[ ubuntu wiki](https://wiki.ubuntu.com/SecurityTeam/KnowledgeBase/SpectreAndMeltdown).  The wiki page again asserts Meltdown is associated with CVE-2017-5754. That page also has a list of \"Userspace Mitigations - Mitigations have been released for the following non-kernel packages\".  Exploring those, firefox ([USN-3516-1](https://usn.ubuntu.com/usn/usn-3516-1/)) is has a reference to CVE-2017-5754. ", "CC @martinwicke \r\n\r\nLooking at this\r\nhttps://wiki.ubuntu.com/SecurityTeam/KnowledgeBase/SpectreAndMeltdown#Userspace_Mitigations\r\nI do not see any package that we knowingly include in our cpu dockerfiles.\r\n\r\nMoreover, even if we add \"apt-get upgrade\" at the end of our dockerfiles, it only runs upgrade at build time.\r\nSo for 1.3, 1.4, 1.5 those would be when the releases were last built. What we upload to docker are static docker images, which you simply download and start working on. So whatever we do, the image will only be updated at the time of the build/release we are doing, we cannot make the built images pickup security updates automatically.\r\n\r\nAlso looking at your comment about dockerfile best practices in https://github.com/tensorflow/tensorflow/issues/16603#issuecomment-362939637 , I think we can use the same logic to argue that ubuntu images are the ones where apt-get upgrade should be run, rather than ours. \r\n\r\nI am not sure how to proceed here. @martinwicke On our side what is needed would be to pull every single image we have, run apt-get upgrade repush them.\r\nI am not sure if it is something we would like to do since the main place the vulnerabilities should be mitigated is the host kernel.", "I agree -- we're not adding any packages to the container that need updating. If the upstream images (nvidia-docker, and ultimately, the base ubuntu image) is updated (possibly to include a apt-get upgrade) we should rebuild. \r\n\r\nHas that happened?\r\n\r\nOtherwise, there's not much that can be done here. I'll close, but we could of course be wrong about any of this, please reopen if we are."]}, {"number": 16602, "title": "Remove query_layer in LuongMonotonicAttention", "body": "In the constructor for LuongMonotonicAttention, a query layer was being created but it was ultimately never used.\r\n\r\nFixes #16287.", "comments": []}, {"number": 16601, "title": "Moving code using new to absl::make_unique. This should make cleaning\u2026", "body": "\u2026 this up when std::make_unique is available easier with automatic tooling easier, and remove references to new.", "comments": ["Can't check at the moment, but did my changes cause the above CI build failures?", "There seems to be some new build failures, so I think yes. You may need to download the full log to see the errors though.", "Ok I fixed a build file error (trailing comma). Now I am just getting license check errors in the sanity checker:\r\n```\r\n=== Summary of sanity check results ====\r\n1. do_pylint PYTHON2: Python 2 pylint\r\n  PASS\r\n2. do_pylint PYTHON3: Python 3 pylint\r\n  PASS\r\n3. do_check_futures_test: Check that python files have certain __future__ imports\r\n  PASS\r\n4. do_buildifier: buildifier check\r\n  PASS\r\n5. do_bazel_nobuild: bazel nobuild\r\n  PASS\r\n6. do_pip_package_licenses_check: pip: license check for external dependencies\r\n  FAIL\r\n7. do_lib_package_licenses_check: C library: license check for external dependencies\r\n  FAIL\r\n8. do_java_package_licenses_check: Java Native Library: license check for external dependencies\r\n  FAIL\r\n9. do_pip_smoke_test: Pip Smoke Test: Checking py_test dependencies exist in pip package\r\n  PASS\r\n10. do_check_load_py_test: Check load py_test: Check that BUILD files with py_test target properly load py_test\r\n  PASS\r\n11. do_code_link_check: Code Link Check: Check there are no broken links\r\n  PASS\r\n12. do_cmake_python_sanity: Test entries in /tensorflow/contrib/cmake/python_{modules|protos|protos_cc}.txt for validity and consistency\r\n  PASS\r\n13. do_check_file_name_test: Check file names for cases\r\n  PASS\r\n```\r\nIt looks like there is a white list and blacklist containing absl here:\r\nhttps://github.com/tensorflow/tensorflow/blob/1752d9c8fac5f6cf85a41e77d92e2743adbfc446/tensorflow/tools/ci_build/ci_sanity.sh#L312\r\nbut maybe not specifically absl memory?\r\n\r\nShould I change it to link the entire thing, or maybe we should change the blacklist white list mechanism?\r\n\r\nNote: This is the specific error in the log\r\n```\r\nFAIL: mismatch in packaged licenses and external dependencies\r\nPlease remove the licenses for the following external dependencies:\r\n@bazel_tools//src\r\n```\r\n", "That was a breakage we fixed on master.\r\nRerunning tests to see if everything passes now.", "Now according to ubuntu cc:\r\nhttps://source.cloud.google.com/results/invocations/7df7e734-4077-4a68-a497-410680df8b6b/targets\r\n```\r\narrow_drop_down  Broken target (1)\r\n//tensorflow/core:lib_gtl_inlined_vector_test\r\n```\r\n\r\n\r\nYet this passes locally for me:\r\n```\r\nTarget //tensorflow/core:lib_gtl_inlined_vector_test up-to-date:\r\n  bazel-bin/tensorflow/core/lib_gtl_inlined_vector_test\r\nINFO: Elapsed time: 3.290s, Critical Path: 2.70s\r\nINFO: Build completed successfully, 8 total actions\r\n//tensorflow/core:lib_gtl_inlined_vector_test                            PASSED in 0.0s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\n```\r\nIs the CC Build influenced by something other than my changes?\r\n", "It is curious.\r\nCould you rebase your change on top of current master? Thats all I can think of.", "Rebased and pushed. The test still passes locally even after rebase on master.\r\n\r\nI also tried ```git submodule update --recursive```\r\n\r\nLastly, my git log is\r\n```\r\n53d50b6 fixed build errors using buildifier\r\n90bbfd8 Moving code using new to absl::make_unique. This should make cleaning this up when std::make_unique is available easier with automatic tooling easier, and remove references to new.\r\n1752d9c Merge pull request #16715 from jhseu/branch_184352399\r\n```\r\n\r\nThe most recent 2 are my commits that I rebased, and then the 3rd is the master I rebased on.\r\n", "I just pulled and ran:\r\n\r\n```\r\nbazel test //tensorflow/contrib/bayesflow:hmc_test --test_output=errors\r\n```\r\nI am getting an error, but I checked and its also broken on master for me...", "OK rebased once again, I can't test at the moment, but if the build isn't breaking maybe it will work?", "OK, the test that previously broke seems to work after rebase.", "Should be good now that we rebased?", "Mind checking the test failures? ", "Will rebuild and push soon, but going but in the error logs reported by gpu cc, I see things like this showing up:\r\n\r\n```\r\n\r\n: /tmpfs/src/github/tensorflow/tensorflow/core/kernels/BUILD:2503:1: Couldn't build file tensorflow/core/kernels/_objs/svd_op_gpu/tensorflow/core/kernels/svd_op_gpu.cu.o: error while parsing .d ERRORfile: /tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/core/kernels/_objs/svd_op_gpu/tensorflow/core/kernels/svd_op_gpu.cu.d (No such file or directory)\r\nIn file included from /usr/local/cuda-9.0/bin/..//include/common_functions.h:50:0,\r\n                 from /usr/local/cuda-9.0/bin/..//include/cuda_runtime.h:115,\r\n                 from <command-line>:0:\r\n/usr/local/cuda-9.0/bin/..//include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\nexternal/com_google_absl/absl/base/config.h:196:5: note: in expansion of macro '__CUDACC_VER__'\r\n     __CUDACC_VER__ >= 7000\r\n     ^\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/core/kernels/BUILD:2503:1: Couldn't build file tensorflow/core/kernels/_objs/svd_op_gpu/tensorflow/core/kernels/svd_op_gpu.cu.pic.o: error while parsing .d file: /tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/core/kernels/_objs/svd_op_gpu/tensorflow/core/kernels/svd_op_gpu.cu.pic.d (No such file or directory)\r\nIn file included from /usr/local/cuda-9.0/bin/..//include/common_functions.h:50:0,\r\n                 from /usr/local/cuda-9.0/bin/..//include/cuda_runtime.h:115,\r\n                 from <command-line>:0:\r\n/usr/local/cuda-9.0/bin/..//include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n```\r\nI am going to rebase and push soon, I'll try to run the above tests.. is there any particular trick to making sure I am running them with the same configuration as the CI setup?", "So after the rebase, on my local machine:\r\n```bazel test //tensorflow/c:c_api_function_test```\r\nworks, however, I am assuming the gpu cc CI is setup to run linking to cuda or something like this. That being said, I am not sure my code is causing the above problem?\r\n", "Just pushed the rebase, and I'm trying to test via docker gpu with:\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh GPU bazel test //tensorflow/c:c_api_function_test\r\n```", "@gunan -- we apparently already depend on ABSL C++, which I think we decided is fine. But we need to be careful about any such symbol not making it into any API.", "I will rerun the tests here, it's possible that this does not work for nvcc?", "@ibebrett there are a number of build errors in the GPU builds. Not sure what the cause is, but could you take a look?", "I will try to look later tonight.", "Looks like the problem may be caused by `CUDA_CC_VER` used in `absl/base/config.h`\r\nThis is already fixed in the head of absl.\r\n@ibebrett Your build will be fixed if you also update this block to use a newer commit:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L110\r\n\r\nCould you make that change in this PR too?", "Thanks! Sorry, got busy with the day job. I'll make this fix tonight. Thanks for figuring this out!", "Ok I think i updated the build file correctly?", "Thank you very much for patiently working through the issues.\r\nI just triggered the tests, let's see if this solves the problems.", "Ok, looks like this is now only breaking the cmake build\r\n\r\nI think we can probably follow the directions for abseil here:\r\nhttps://github.com/abseil/abseil-cpp/tree/master/CMake\r\n\r\nand add a dependency to tensorflow/contrib/cmake/external/ ? (The other cmake dependencies live here it seems).\r\n\r\nAccording to https://bazel.build/, bazel runs on windows.. Is it necessary to maintain an entirely separate build system? Seems like it will mean extra maintenance work.\r\n\r\nIf the above steps seem reasonable I can take a look tonight after work and try to build out the cmake dep for absiel.", "Our bazel build on windows still has some problems we need to fix. Until then, we have to maintain the cmake build.\r\n\r\nFor adding the cmake dependency, your steps look good to me.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Ping!\r\nAny updates?", "Closing due to lack of activity, but feel free to reopen when the issue is addressed. Thanks!"]}, {"number": 16600, "title": "Moving code using new in the linear algebra kernels to absl::make_unique. This should make cleaning\u2026", "body": "Moving code using new in the linear algebra kernels to absl::make_unique. This should make cleaning this up when std::make_unique is available with automatic tooling easier, and remove references to new.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16599, "title": "Branch 183846994", "body": "", "comments": []}, {"number": 16598, "title": "Merge in final changes from the r1.5 branch", "body": null, "comments": []}, {"number": 16597, "title": "Branch 183846994", "body": "", "comments": []}, {"number": 16596, "title": "Merge pull request #1 from tensorflow/master", "body": "merge upstream changes", "comments": []}]