[{"number": 28048, "title": "[Intel MKL] IsMklEnabled() check in CI now works with TF 2.0", "body": "", "comments": []}, {"number": 28047, "title": "Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@local_config_cc//'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: version r1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): VisualStudio 2017\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nTrying to build tensorflow.\r\nHave installed bazel and MySys2 and python\r\nConfigured for no GPU (see below for print outs)\r\nIssuing the following command results in error:\r\n`bazel build --config=monolithic //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**CONFIGURE OUTPUT**\r\n```\r\n(tf3.6) D:\\repos\\tensorflow>python ./configure.py\r\nExtracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\jesaremi\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf3.6\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\jesaremi\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf3.6\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\jesaremi\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf3.6\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n\r\n```\r\n\r\n**BUILD OUTPUT**\r\n```\r\n(tf3.6) D:\\repos\\tensorflow>bazel build --config=monolithic //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: no such package '@local_config_cc//': Traceback (most recent call last):\r\n        File \"C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 52\r\n                configure_windows_toolchain(repository_ctx)\r\n        File \"C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 406, in configure_windows_toolchain\r\n                setup_vc_env_vars(repository_ctx, vc_path)\r\n        File \"C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 204, in setup_vc_env_vars\r\n                execute(repository_ctx, [\"./get_env.bat\"], e...)\r\n        File \"C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/lib_cc_configure.bzl\", line 163, in execute\r\n                auto_configure_fail((\"non-zero exit code: %d, comman...)))\r\n        File \"C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/lib_cc_configure.bzl\", line 109, in auto_configure_fail\r\n                fail((\"\\n%sAuto-Configuration Error:%...)))\r\n\r\nAuto-Configuration Error: non-zero exit code: 255, command [\"./get_env.bat\"], stderr: (\";D:\\Program Files\\CMake\\bin;C:\\Program Files\\Java\\jdk1.8.0_201\\bin;D:\\Program Files\\apache-maven-3.3.9\\bin;D:\\Program Files\\protoc\\bin;D:\\Program Files\\scala-2.12.8\\bin;C:\\msys64\\usr\\bin;C:\\Program Files (x86)\\Notepad++;D:\\Program Files\\bazel;\"==\"\" was unexpected at this time.\r\n)\r\n```", "comments": ["@jeffsaremi Can you do a \r\n```\r\ncat bazel-tensorflow/external/local_config_cc/get_env.bat\r\n```\r\nand tell me what's the content of this file?", "@meteorcloudy :\r\n\r\n```\r\n(tf3.6) D:\\repos\\tensorflow>cat bazel-tensorflow/external/local_config_cc/get_env.bat\r\n@echo off\r\ncall \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\\\VC\\Auxiliary\\Build\\VCVARSALL.BAT\" amd64 > NUL\r\necho PATH=%PATH%,INCLUDE=%INCLUDE%,LIB=%LIB%,WINDOWSSDKDIR=%WINDOWSSDKDIR%\r\n\r\n```", "@jeffsaremi The file looks correct to me, can you run it in cmd.exe and see what's the output?", "@meteorcloudy \r\nIs this what you asked for?\r\n```\r\n(tf3.6) D:\\repos\\tensorflow>\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\\\VC\\Auxiliary\\Build\\VCVARSALL.BAT\" amd64\r\n**********************************************************************\r\n** Visual Studio 2017 Developer Command Prompt v15.9.11\r\n** Copyright (c) 2017 Microsoft Corporation\r\n**********************************************************************\r\n[vcvarsall.bat] Environment initialized for: 'x64'\r\n\r\n```", "I think there is something wrong with the parameters I am choosing. For instance --config. I don't know why I chose monolithic but the documentation didn't really advise anything on this.", "Almost, can you try to run `bazel-tensorflow\\external\\local_config_cc\\get_env.bat` and see what happens? Because the following error looks like comes from running this script.\r\n```\r\n;D:\\Program Files\\CMake\\bin;C:\\Program Files\\Java\\jdk1.8.0_201\\bin;D:\\Program Files\\apache-maven-3.3.9\\bin;D:\\Program Files\\protoc\\bin;D:\\Program Files\\scala-2.12.8\\bin;C:\\msys64\\usr\\bin;C:\\Program Files (x86)\\Notepad++;D:\\Program Files\\bazel;\"==\"\" was unexpected at this time.\r\n```", "I ran that and got the same error complaining about the ==.\r\nI then did this:\r\n`set VSCMD_DEBUG=3`\r\nto get debug prints from the `vcvarsall.bat `in `VisualStudio`\r\nand upon examining the print outs I realized that I had a `\"` (double quote) in my path, caused by having an entry which itself had a semicolon in it.\r\nAfter fixing the path, I ran the build and is running with no problems.\r\nWe can close this case now."]}, {"number": 28046, "title": "[LITE] ADDN 8bit quantization support ", "body": "", "comments": ["@suharshs  Could you please spend some time to review this PR. TIA\r\n\r\n", "Hi, really sorry for a delay. We are taking a look now. Thanks!", "@jianlijianli Thanks a lot for the review. I have updated the comments based on your comments. Could you please have a look. TIA", "Can one of the admins verify this patch?", "@siju-samuel sorry for the delay , can you please resolve conflicts ?", "@suharshs Could you please approve again. i rebased to the latest code. Thanks for the review.", "@siju-samuel Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 28045, "title": "Graph_transformations/propagate_fixed_sizes.cc Unhandled operator type CTCBeamSearchDecoder", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.13.1\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**: 0.24.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 10.0/7.5\r\n- **Exact command to reproduce**:\r\n\r\n```bash\r\nbazel-bin/tensorflow/lite/toco/toco --input_file=model.pb --output_file=model.tflite \\\r\n--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n--input_arrays=input_audio,input_length,input_labels/values,input_labels/indices \\\r\n--output_arrays=labels_1,labels_2,labels_3,labels_4,labels_5,labels_6,labels_7,labels_8,labels_9,labels_10,weight_1,weight_2,weight_3,weight_4,weight_5,weight_6,weight_7,weight_8,weight_9,weight_10,neglogprob \\\r\n--target_ops=TFLITE_BUILTINS,SELECT_TF_OPS \\\r\n--allow_custom_ops \\\r\n--input_shapes=1,49,82:1:49:49,2\r\n```\r\n\r\n(Also tried)\r\n```bash\r\ntflite_convert \\\r\n  --graph_def_file=model.pb --output_file=model.tflite \\ \r\n  --input_arrays=input_audio,input_length,input_labels/values,input_labels/indices \\\r\n  --output_arrays=labels_1,labels_2,labels_3,labels_4,labels_5,labels_6,labels_7,labels_8,labels_9,labels_10,weight_1,weight_2,weight_3,weight_4,weight_5,weight_6,weight_7,weight_8,weight_9,weight_10,neglogprob \\\r\n  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS \\\r\n  --enable_select_tf_ops --allow_custom_ops \\\r\n  --input_shapes=1,49,82:1:49:49,2\r\n```\r\n\r\n### Describe the problem\r\nMy frozen model contains ctc_beam_search_decoder operator.\r\n```bash\r\ninputs = tf.placeholder(tf.float32, [1, None, \r\n                            FLAGS.freq_dim*FLAGS.stack_frames],\r\n                            name='input_audio')\r\n    lens = tf.placeholder(tf.int32, [1], name='input_length')\r\n    logits = model.GRU_Model(inputs, lens, \r\n                             FLAGS.freq_dim*FLAGS.stack_frames,\r\n                             FLAGS.hidden_dim, FLAGS.voc_size + 1, \r\n                             FLAGS.num_layers)\r\n\r\n    labels = tf.sparse_placeholder(tf.int32, name='input_labels')\r\n    neglogprob = tf.nn.ctc_loss(labels, logits, lens, time_major=False)\r\n    tf.identity(neglogprob, name='neglogprob')\r\n\r\n    decoded, _ = tf.nn.ctc_beam_search_decoder(tf.transpose(logits, (1, 0, 2)), \r\n                                               lens, FLAGS.beam, FLAGS.N)\r\n    for k in range(FLAGS.N):\r\n        tf.cast(decoded[k].values, dtype=tf.int32, \r\n                name='labels_N'.replace('N', str(k+1)))\r\n        tf.reciprocal(tf.nn.ctc_loss(tf.cast(decoded[k], dtype=tf.int32), \r\n                      logits, lens, time_major=False),\r\n                      name='weight_N'.replace('N', str(k+1)))\r\n\r\n    saver = tf.train.Saver(tf.global_variables())\r\n    with tf.Session() as sess:\r\n        print('Loading model from %s' % FLAGS.checkpoint)\r\n        sess.run(tf.global_variables_initializer())\r\n        saver.restore(sess, FLAGS.checkpoint)\r\n        print('Node names:\\n')\r\n        for node in tf.get_default_graph().as_graph_def().node:\r\n            print(node.name)\r\n        output_nodes = ['neglogprob']\r\n        for k in range(FLAGS.N):\r\n            output_nodes += ['weight_%d' % (k+1), 'labels_%d' % (k+1)]\r\n        frozen_graph_def = graph_util.convert_variables_to_constants(sess,\r\n                           sess.graph_def, output_nodes)\r\n        tf.train.write_graph(frozen_graph_def, \r\n                             os.path.dirname(FLAGS.output_file),\r\n                             os.path.basename(FLAGS.output_file),\r\n                             as_text=False)\r\n        print('Saved frozen graph to %s' % FLAGS.output_file)\r\n```\r\nWhen I try to convert .pb model to .tflite I get:\r\n```bash\r\n2019-04-22 14:53:34.726887: I tensorflow/stream_executor/platform/default/dso_loader.cc:43] Successfully opened dynamic library libcudart.so.10.0\r\n2019-04-22 14:53:34.771557: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\r\n2019-04-22 14:53:34.781295: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-04-22 14:53:34.781432: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\r\n2019-04-22 14:53:34.781475: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-04-22 14:53:34.781653: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3\r\n2019-04-22 14:53:34.781720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.781758: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.781807: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.781859: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.781908: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.781929: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.781973: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.781998: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782024: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond\r\n2019-04-22 14:53:34.782085: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3\r\n2019-04-22 14:53:34.782108: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782128: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-04-22 14:53:34.782146: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782175: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782487: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782535: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782624: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782644: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782804: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782831: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782894: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.782913: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.783092: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.783119: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.783166: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.783191: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.783238: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.783276: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\r\n2019-04-22 14:53:34.783299: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\n2019-04-22 14:53:34.783319: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-04-22 14:53:34.783359: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit\r\n2019-04-22 14:53:34.783379: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3\r\n2019-04-22 14:53:34.783421: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3\r\n2019-04-22 14:53:34.783536: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.783590: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.783610: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.783649: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.783668: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.783705: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.783723: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.783761: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.783780: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.783818: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.783838: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.783875: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.783894: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.783933: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.783951: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.783990: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.784009: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.784045: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.784064: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.784102: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss\r\n2019-04-22 14:53:34.784120: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-04-22 14:53:34.785713: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 209 operators, 353 arrays (0 quantized)\r\n2019-04-22 14:53:34.788612: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 206 operators, 345 arrays (0 quantized)\r\n2019-04-22 14:53:34.791340: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 206 operators, 345 arrays (0 quantized)\r\n2019-04-22 14:53:34.792679: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:2425] Unhandled operator type CTCBeamSearchDecoder\r\nAborted (core dumped)\r\n```\r\n\r\nHow could I avoid this problem? I dind't find any issue contains such problem.\r\n", "comments": ["Thanks for filing the issue, I have a fix: https://github.com/tensorflow/tensorflow/commit/a90747cc3e8b292d4de3248652d93166524446f0\r\n\r\ncan you try that?", "It works.\r\nNow I get error \"TensorFlow Lite currently doesn't support control flow ops: Merge, Switch\", but I saw another issue about it.\r\nThank you for resolving my problem!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28045\">No</a>\n"]}, {"number": 28044, "title": "tf_nightly_gpu_2.0_preview-2.0.0.dev20190420  import error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip install \r\n- TensorFlow version (use command below):tf_nightly_gpu_2.0_preview2.0.0.dev20190420\r\n- Python version:3.6.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nimport tensorflow as tf\r\n**Describe the expected behavior**\r\nAble to import\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"D:/Python/Project/PythonProject/TF20/code/NFFM/code/NFFM-2.0.py\", line 21, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Z&J\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 42, in <module>\r\n    from tensorflow._api.v2 import compat\r\n  File \"C:\\Users\\Z&J\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py\", line 21, in <module>\r\n    from tensorflow._api.v2.compat import v1\r\n  File \"C:\\Users\\Z&J\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py\", line 643, in <module>\r\n    'tensorflow_estimator.python.estimator.api._v1.estimator'))\r\n  File \"C:\\Users\\Z&J\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\tools\\component_api_helper.py\", line 56, in package_hook\r\n    child_pkg = importlib.import_module(child_package_str)\r\n  File \"C:\\Users\\Z&J\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Z&J\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\api\\_v1\\estimator\\__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator.python.estimator.api._v1.estimator import experimental\r\n  File \"C:\\Users\\Z&J\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\api\\_v1\\estimator\\experimental\\__init__.py\", line 29, in <module>\r\n    _sys.modules[__name__], \"estimator.experimental\")\r\nTypeError: __init__() missing 1 required positional argument: 'deprecated_to_canonical'\r\n", "comments": ["I think this is a duplicate of https://github.com/tensorflow/tensorflow/issues/27955, it should be fixed in newer nightly builds.", "\r\n\r\n\r\n\r\n> I think this is a duplicate of #27955, it should be fixed in newer nightly builds.\r\n\r\nok\uff0cthanks.", "@AiIsBetter This issue is not present in Tensorflow version: tf-nightly-gpu-2.0-preview-2.0.0.dev20190423. You might want to give it a try instead. Thanks!", "> @AiIsBetter This issue is not present in Tensorflow version: tf-nightly-gpu-2.0-preview-2.0.0.dev20190423. You might want to give it a try instead. Thanks!\r\n\r\nOkay, I'll try. Thank you. ", "@AiIsBetter Were you able to resolve this issue ?", " Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28043, "title": "Weird behavior using tf.Variable in a tf.data.experimental.map_and_batch callback function", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): 1.13.1 from conda\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef foo(x):\r\n    array = tf.Variable(lambda: tf.zeros(20, dtype=tf.float32),\r\n            trainable=False, use_resource=True)\r\n    ind = tf.reshape(x, [-1, 1])\r\n    val = tf.reshape(tf.cast(x, tf.float32), [-1])\r\n\r\n    array_assign = array.assign(tf.zeros_like(array))\r\n    with tf.control_dependencies([array_assign]):\r\n        array = array.scatter_nd_update(ind, tf.cast(val, dtype=tf.float32))\r\n\r\n    return array\r\n\r\ndataset = tf.data.Dataset.range(20)\r\n# dataset = dataset.map(foo)\r\n# dataset = dataset.batch(5)\r\ndataset = dataset.apply(\r\n        tf.data.experimental.map_and_batch(foo, 5, num_parallel_batches=1))\r\n\r\niterator = dataset.make_initializable_iterator()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(iterator.initializer)\r\n    next_element = iterator.get_next()\r\n    fetches = sess.run(next_element)\r\n    print(fetches)\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nIf using normal `map` and `batch`, one get\r\n\r\n```\r\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIf using `map_and_batch`, one get changing results like:\r\n\r\n```\r\n[[0. 1. 2. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 1. 2. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 1. 2. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r\n```\r\n\r\nThere should be always only one and non-duplicate non-zeros per-line.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nNot sure if this is related to #27507 . On a [Colab with tf-nightly](https://colab.research.google.com/drive/1DzTvOJRXYUk0CSjvGa11ewmjTP7VJ1kF), #27507 seems fixed but this issue still reproduces.", "comments": ["@prclibo I got the following result \r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r\n\r\nCould you try to run it in Google colab or upgrade your TF1.13.1 and run it. Please let me know how it progresses. Thanks!", "@jvishnuvardhan No, on the above colab (https://colab.research.google.com/drive/1DzTvOJRXYUk0CSjvGa11ewmjTP7VJ1kF) the issue still reproduces (though seems less messy), with updated tf-nightly version:\r\n\r\n> `Successfully installed google-pasta-0.1.5 tb-nightly-1.14.0a20190423 tf-estimator-nightly-1.14.0.dev2019042301 tf-nightly-1.14.1.dev20190423 wrapt-1.11.1`\r\n\r\nWhich version are you using?", "@prclibo I had used TF1.13.1 as you mentioned TF1.13.1 in your issue template.\r\nI just tried TF-nightly and the output is as follows\r\nW0424 01:48:58.248569 139709344798592 deprecation.py:323] From <ipython-input-3-5ac9480b764d>:19: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\r\n[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 2. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r\n\r\nThe result is not what is expected but the map_and_batch is going to be deprecated (check the warning above). The suggested op is map followed by batch which is working well in tf-nightly.  What is the point in using deprecated op when you have better working ops that replaced the deprecated op. Please let me know what you think. Thanks!", "@jvishnuvardhan Thanks for the tests. However I still reproduce it with separate `map` and `batch` in 1.14.1-dev20190423 (`!pip install tf-nightly`). See the last cell in https://colab.research.google.com/drive/1DzTvOJRXYUk0CSjvGa11ewmjTP7VJ1kF\r\n\r\nMaybe you can share a successful Colab?", "@prclibo I have changed one line in your code\r\n`dataset = dataset.map(foo, num_parallel_calls=1)`\r\nThis is the output I get\r\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [0. 0. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r\n\r\nHere is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/2f3d68380a525f793bf63b7f13543c9e/test_tensorflow_27507.ipynb). Thanks!\r\n", "@jvishnuvardhan thanks, but setting `num_parallel_calls=1` does not really solve the issue, right?\r\nI would expect that when `num_parallel_calls > 1` each line in its output has only one element scattered and the element is different from each other lines. The order of line can be permuted or not depending on the threading implementation. But the issue is not solved for `num_parallel_calls > 1` right now.", "The behavior you see if expected because your `foo` function is using shared state -- the `array` variable will be shared by all invocations of the `foo` function because that's how resource variables in TensorFlow work. Furthermore, `map_and_batch` with `num_parallel_batches` with perform concurrent invocations of `foo`. That means that you will have multiple instances of `foo` all accessing (reading / writing) `array` at the same time, which explains the behavior you are seeing.\r\n\r\nIf you would like to achieve the \"expected\" behavior, use a constant instead of variable. As far as I can tell there is no good reason for you to need to be using a variable.", "@jsimsa Thanks. The reason of using a variable is that I failed to use a constant tensor in a `Dataset.map` callback operated by `scatter_nd_update`. See the second cell in https://colab.research.google.com/drive/1DzTvOJRXYUk0CSjvGa11ewmjTP7VJ1kF#scrollTo=JhIQnyNFYIR6\r\n\r\nDoing scattering onto a constant `tf.zeros(20)` causes `AttributeError: 'Tensor' object has no attribute '_lazy_read'`. Is there any workaround to solve this?", "You can achieve the desired the behavior using the following simpler program:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef foo(x):\r\n  array = tf.one_hot(x, 20) * tf.cast(x, dtype=tf.float32)\r\n  return array\r\n\r\ndataset = tf.data.Dataset.range(20)\r\ndataset = dataset.map(foo)\r\ndataset = dataset.batch(5)\r\n\r\niterator = dataset.make_initializable_iterator()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(iterator.initializer)\r\n    next_element = iterator.get_next()\r\n    fetches = sess.run(next_element)\r\n    print(fetches)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28043\">No</a>\n"]}, {"number": 28042, "title": "Added Support for SoftPlus operator in tflite.", "body": "This is part of issue #27822.", "comments": ["Do you have a specific model which uses this operator?\r\n\r\nWe're trying to raise the bar for adding new operators to TFLite, preferring a smaller set of core operators where possible, as it makes it more difficult to maintain parity with our delegate/accelerator pipeline.", "@jdduke , thanks for the comments, this implementation is inline with the issue #27822. But i understand your point. So  should we use graph transform to convert softplus operator or leave it for the custom ops ?\r\nAs i feel graph transform might again not be so efficient as it will create again two nodes(when value is within the Range). Let me know your take on this.\r\n\r\nRegards\r\nAmit", "I'm going to defer to @miaout17 for further review. I agree that the graph transform won't be quite as important. Can you tell if the SoftPlus operator in that graph is used sparingly? Or all throughout?", "@jdduke , thanks for the response, I think the model uses softplus sparingly, but softplus seems to be picking up momentum.\r\n@miaout17 , could you please review and provide the feedback.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 could you please resolve the conflicts? ", "@gbaned , thanks for pointing this out, i have resolved the conflicts.\r\n\r\nRegards\r\nAmit", "While the code itself looks good, the following questions are unanswered yet:\r\n\r\n* What models use SoftPlus op? \r\n* Should we do graph transformation or define a new op? (tradeoff between efficiency versus fewer builtin ops). \r\n\r\n@jdduke are you comfortable adding this as a builtin op? If yes, I can follow up the code review process", "@miaout17 thanks for the review i have updated all the comments as per your suggestion, kindly check.\r\n\r\n\r\nRegards\r\nAmit", "Hi @amitsrivastava78, we're working on some guidelines for new operators, could you give us a few days to get back to you?\r\n\r\nAs noted previously, it would be good to know if other models are using this operator, and whether a different activation would suffice.", "@jdduke , thanks for your response, sure i will wait for the conclusion from your side, also i will check which all models are using this operator and update you.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 could you please resolve the conflicts? Thanks!", "@gbaned , thanks for pointing this out, i have resolved the conflicts.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 could you please resolve the conflicts? Thanks!", "@gbaned , i have rebased the code and resolved the merge conflicts.\r\n\r\n@jdduke can you please have a look at the PR and let me know your feedback\r\n\r\nRegards\r\nAmit", "Until we find another model which requires this op, I'd rather we rely on using the select TF ops, or we add this as a custom op that users can optionally link into their app.", "See also https://github.com/tensorflow/tensorflow/commit/03195f13456354deea8b81c9e583621b1337b952#diff-2b45693b554369bde8c98e9a76b80036", "@amitsrivastava78 Could you please address the jdduke's comments. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28041, "title": "I have the error FailedPreconditionError: Attempting to use uninitialized value dense_2/bias \t [[node dense_2/bias/read (defined at C:/Users/TRAN THI DIEM/Documents/diem/research/ReinforcementLearning/code/CodeforlearningReinforcement/CNNandRL/Object-recognition-CIFAR-10-master/code_22_4_2019_done_firstversion_50loop_100images_verify_optimize_error_loss_ok_backup_version1.py:76) ]]", "body": "Here is my code. Please let me know what is my problem\r\n\r\nimport tensorflow as tf\r\nfrom keras.datasets import cifar10\r\nfrom keras.utils import np_utils\r\nimport numpy as np\r\n\r\nimport keras.backend as K\r\nK.set_learning_phase(0)\r\nimport random\r\nstate_size = [32,32,3]\r\ny = tf.placeholder(tf.float32, [None,10])\r\ninputs_ = tf.placeholder(tf.float32, [None, *state_size])\r\nlearning_rate = 0.001\r\nsess = tf.Session()\r\n#sess.run(tf.initialize_all_variables())\r\nsess.run(tf.global_variables_initializer())\r\ninit_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())\r\nsess.run(init_op)\r\ndef neural_network_model ():\r\n    #inputs_ = tf.placeholder(tf.float32, [None, *state_size])\r\n\r\n    \r\n    conv1 = tf.layers.conv2d(inputs=inputs_,\r\n                              filters=32,\r\n                              kernel_size=[3,3],\r\n                              strides=[1,1],\r\n                              padding=\"same\",\r\n                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\r\n                              )\r\n\r\n    conv1_batchnorm = tf.layers.batch_normalization(conv1,\r\n                                                     training=True,\r\n                                                     epsilon=1e-5\r\n                                                     )\r\n\r\n    conv1_out = tf.nn.elu(conv1_batchnorm)\r\n## --> [30, 30, 32]\r\n\r\n    conv2 = tf.layers.conv2d(inputs=conv1_out,\r\n                              filters=32,\r\n                              kernel_size=[3, 3],\r\n                              strides=[1,1],\r\n                              padding=\"same\",\r\n                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\r\n                              )\r\n\r\n    conv2_batchnorm = tf.layers.batch_normalization(conv2,\r\n                                                     training=True,\r\n                                                     epsilon=1e-5\r\n                                                     )\r\n\r\n    conv2_out = tf.nn.elu(conv2_batchnorm)\r\n## --> [28, 28, 32]\r\n\r\n    conv3 = tf.layers.conv2d(inputs=conv2_out,\r\n                              filters=64,\r\n                              kernel_size=[3,3],\r\n                              strides=[1,1],\r\n                              padding=\"VALID\",\r\n                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\r\n                              )\r\n\r\n    conv3_batchnorm = tf.layers.batch_normalization(conv3,\r\n                                                     training=True,\r\n                                                     epsilon=1e-5\r\n                                                     )\r\n\r\n    conv3_out = tf.nn.elu(conv3_batchnorm)\r\n## --> [26, 26, 64]\r\n\r\n    flatten = tf.layers.flatten(conv3_out)\r\n## --> [43,264]\r\n\r\n    fc = tf.layers.dense(inputs=flatten,\r\n                          units=512,\r\n                          activation=tf.nn.elu,\r\n                          kernel_initializer=tf.contrib.layers.xavier_initializer()\r\n                          )\r\n\r\n    output = tf.layers.dense(inputs=fc,\r\n                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\r\n                              units=10,\r\n                              activation=None)\r\n    return output\r\n\r\ndef training_network_option1 ():\r\n    sess = tf.Session()\r\n#sess.run(tf.initialize_all_variables())\r\n    sess.run(tf.global_variables_initializer())\r\n#    y = tf.placeholder(tf.float32, [None,10])\r\n    prediction = neural_network_model()\r\n    # OLD VERSION:\r\n    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\r\n    # NEW:\r\n    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\r\n    optimizer=tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\r\n    return cost, optimizer\r\n    \r\ndef training_network_option2 ():\r\n#    y = tf.placeholder(tf.float32, [None,10])\r\n    sess = tf.Session()\r\n#sess.run(tf.initialize_all_variables())\r\n    sess.run(tf.global_variables_initializer())\r\n    prediction = neural_network_model()\r\n    # OLD VERSION:\r\n    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\r\n    # NEW:\r\n    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\r\n    optimizer=tf.train.MomentumOptimizer(learning_rate).minimize(cost)    \r\n    return cost, optimizer\r\n    \r\ndef training_network_option3 ():\r\n#    y = tf.placeholder(tf.float32, [None,10])\r\n    sess = tf.Session()\r\n#sess.run(tf.initialize_all_variables())\r\n    sess.run(tf.global_variables_initializer())\r\n    prediction = neural_network_model()\r\n    # OLD VERSION:\r\n    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\r\n    # NEW:\r\n    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\r\n    optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost) \r\n    return cost, optimizer\r\n\r\n    \r\n    \r\n\r\n\r\n\r\n\r\ndef possible_action():\r\n    batch =0\r\n    stochastic =1\r\n    mini_batch = 2\r\n\r\n    actions =np.array([batch, stochastic, mini_batch])\r\n    return actions\r\n\r\n\r\n\r\ndef normalize(x):\r\n    \"\"\"\r\n        argument\r\n            - x: input image data in numpy array [32, 32, 3]\r\n        return\r\n            - normalized x\r\n    \"\"\"\r\n    min_val = np.min(x)\r\n    max_val = np.max(x)\r\n    x = (x-min_val) / (max_val-min_val)\r\n    return x\r\n\r\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\r\n\r\n\r\nX_train = X_train.astype('float32')\r\nX_test = X_test.astype('float32')\r\nX_train = normalize(X_train)\r\nX_test = normalize(X_test)\r\ny_train = np_utils.to_categorical(y_train)\r\ny_test = np_utils.to_categorical(y_test)\r\nnum_classes = y_test.shape[1]\r\n\r\n#DQNetwork\r\n\r\naction_size = 3\r\n\r\nbatch_size = 10\r\n\r\nstate_batch = 100//batch_size\r\n# Exploration parameters for epsilon greedy strategy\r\nexplore_start = 1.0            # exploration probability at start\r\nexplore_stop = 0.01            # minimum exploration probability \r\ndecay_rate = 0.1            # exponential decay rate for exploration prob\r\n\r\n# Q learning hyperparameters\r\ngamma = 0.95  \r\n\r\nactions = np.array(possible_action())\r\ncurrent_action =actions [0]\r\n\r\n\r\n\r\n#dqn1 = DQNetwork(state_size, action_size)\r\n#dqn2 = DQNetwork(state_size, action_size)\r\n#dqn_total = DQNetwork(state_size, action_size)\r\n\r\n#current_state =X_train[0:64]\r\n#Y_label =y_train[0:64]\r\n\r\n\r\n\r\n\r\n\r\n\r\nloss_actions = []\r\nloss_actions_test = []\r\naction_test = []\r\nloss_total_test = []\r\n#loss_next_state = 100*random ()\r\n\r\n\r\nloss_before = 2*(random.random())\r\nQtable =np.zeros((state_batch,action_size))\r\nnp.random.seed(100)\r\nexp_exp_tradeoff = 0.5\r\n\r\n#print (\"random of explore\", exp_exp_tradeoff)\r\nstep=0\r\n\r\nfor eps in range(50):\r\n   i = 0\r\n   print (\"\\n\")\r\n   while i<state_batch-1:\r\n   #while i<20:\r\n        current_state = X_train[i*batch_size:(i+1)*batch_size]\r\n    \r\n        Y_label = y_train [i*batch_size:(i+1)*batch_size]\r\n        \r\n       # dqn = DQNetwork(state_size, action_size, learning_rate)\r\n        #sess.run(dqn.optimizer,feed_dict={dqn.inputs_:current_state,dqn.label:Y_label\r\n                                                       # }) \r\n        #sess.run(dqn.input_action, feed_dict = {dqn.input_action: current_action})\r\n       # dqn = DQNetwork(state_size, action_size, learning_rate,current_action)\r\n        if current_action == 0:\r\n           cost,optimizer = training_network_option1 ()\r\n        elif current_action ==1:\r\n           cost,optimizer = training_network_option2 ()\r\n        elif current_action ==2:   \r\n           cost,optimizer = training_network_option2 ()\r\n        \r\n   \r\n       \r\n        _, loss_current = sess.run([optimizer, cost], feed_dict={inputs_: current_state, y: Y_label})\r\n        ", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n"]}, {"number": 28040, "title": "tflite segmentation model throws error on android", "body": "**System information**\r\n- What is the top-level directory of the model you are using: models->research ->Deeplab\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):1.13\r\n- Bazel version (if compiling from source): bazel-0.24.0-installer-darwin-x86_64\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: No GPUs used\r\n- Exact command to reproduce:\r\ntflite_convert \\\r\n  --output_file=test.lite \\\r\n  --graph_def_file=frozen_inference_graph.pb \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=SemanticPredictions \\\r\n  --input_shapes=1,600,450,3 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --inference_type=FLOAT \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=128 \r\n\r\n\r\n**Describe the current behavior**\r\nI am using tflite for semantic segmentation. I have a model trained to segment objects from background, this model is trained on deeplab.\r\n\r\nI have converted this model(frozen inference graph) into tflite format using the below code:\r\n```\r\ntflite_convert \\\r\n  --output_file=test.lite \\\r\n  --graph_def_file=frozen_inference_graph.pb \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=SemanticPredictions \\\r\n  --input_shapes=1,600,450,3 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --inference_type=FLOAT \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=128 \r\n```\r\nThe model loads on android, but when I try to run inference it gives me this error:\r\n\r\n\r\n```\r\n    Caused by: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: third_party/tensorflow/lite/kernels/unpack.cc:54 NumDimensions(input)\r\n\r\n        1 was not true.Node number 4 (UNPACK) failed to prepare.\r\n\r\n```\r\n\r\n**Code to reproduce the issue**\r\n\r\ntflite_convert \\\r\n  --output_file=test.lite \\\r\n  --graph_def_file=frozen_inference_graph.pb \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=SemanticPredictions \\\r\n  --input_shapes=1,600,450,3 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --inference_type=FLOAT \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=128 \r\n\r\n**How do I resove this error?**", "comments": ["I got to the next step by applying:\r\n\r\n```\r\ntflite_convert \\\r\n  --output_file=test2.lite \\\r\n  --graph_def_file=frozen_inference_graph_3mbvoc.pb \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=SemanticPredictions \\\r\n  --input_shapes=1,450,600,3 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --inference_type=FLOAT \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=128\r\n```\r\n\r\nThe problem was in input shape mismatch. Even if this removes the error the graph generated by using above code does not produce any output. \r\n\r\n![Screenshot from 2019-04-23 14-42-20](https://user-images.githubusercontent.com/17012391/56633675-bfb81180-667c-11e9-9cde-908655827e40.png)\r\n\r\nThe graph on left is [my tflight-graph](https://drive.google.com/file/d/1uWcOCAHBcYwwf0yQSIJLlrctFNSr91oq/view). and the graph on right is googles tflight-graph.\r\nHow do I convert my tflight-graph to match that of [googles graph](https://drive.google.com/file/d/1wJ9RMFuZqs_TOW27ZPK6feToqmnNbepw/view)", "Give the input_arrays as sub_7 and output_arrays as ResizeBilinear_2. You will get the desired output.", "Sry.. It must be sub_2.. Just now checked your graph properly\r\n", "ya just realized that, it generates the graph now, im checking its results on android", "I converted it here ([link](https://drive.google.com/file/d/1A-FYCHGCcafyk66ALm_7_EVt7Zd31697/view?usp=sharing))however when I run it on android (I am using [this](https://github.com/dailystudio/ml/tree/master/deeplab) guys code to do that). I get an error :\r\n\r\n`Caused by: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 810000 bytes and a ByteBuffer with 792588 bytes.`", "This is because of the variation in the input and output sizes from the default GPU model. The value fixed by dailystudio is based on 224 as input and output size. You will have to find out what will be your corresponding values.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28040\">No</a>\n"]}, {"number": 28039, "title": "Failed to create capture session; configuration failed", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): None\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: im using Raspberry Pi for android things implementation\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): \r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n i was going to implement the android repository(i edited the AndroidManifest.xml a little so only the DetectorActivity is going to launch) in android things when it failed to open the camera  \r\n\r\n**Describe the expected behavior**\r\nthis was supposed to show a live camera preview\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\n`ImageUtils:Native library not found,native RGB -> YUV conversion may be unavailable.\r\nCameraConnectionFragment: Opening camera preview: 640x4480\r\nCameraDevice-JV-0: Stream configuration failed due to: endConfigure:372: Camera 0: Unsupported set of inputs/outputs provided\r\nCameraCaptureSession: Session 0: Failed to create capture session; configuration failed\r\n`\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 28038, "title": "how to use tf.contrib.framework.fuse_op to fuse graph", "body": "Hello,\r\nI want to use tf.contrib.framework.fuse_op to fuse graph to fuse my graph for faster inference,but when i call the api \r\n`new_graphdef = tf.contrib.framework.fuse_op(\r\n                                           graph_def,\r\n                                           input_nodes=[\"ac_input\"],\r\n                                           output_nodes=[\"Inference/input_tansfer/mul\"],\r\n                                           output_dtypes=[tf.float32],\r\n                                           output_quantized=False,\r\n                                           op_name=\"trans_fused\",\r\n                                           op_type=\"add_mul\"\r\n                                       )`\r\nI got an error\r\n`File \"build_fusedop.py\", line 1006, in main\r\n    op_type=\"add_mul\" \r\n  File \"/home/yx.wang/anaconda3/envs/py27tfgpu/lib/python2.7/site-packages/tensorflow/contrib/framework/python/framework/graph_util.py\", line 115, in fuse_op\r\n    new_node.attr[\"_output_types\"].list.type[:] = output_dtypes\r\nTypeError: tf.float32 has type DType, but expected one of: int, long`\r\n\r\nI cannot find the detail deacription of the parameters, the source code as followings:\r\n`graph_def: A graph_pb2.GraphDef proto.\r\ninput_nodes: input nodes to the subgraph to be fused.\r\noutput_nodes: output nodes to the subgraph to be fused.\r\noutput_dtypes: A list of output datatypes for the custom op\r\noutput_quantized: A boolean flag that indicates if output is quantized\r\nop_name: fused op name.\r\nop_type: fused op type.` \r\n\r\nwhat is the correct input of the api?", "comments": ["Sorry, I found  the test example and changed my code to \r\n`input_list = [\"ac_input\"]\r\n                output_list = [\"Inference/Reshape\"]\r\n                default_type = types_pb2.DT_FLOAT\r\n                new_graph = fuse_op(\r\n                                    graph_def=graph_def,\r\n                                    input_nodes=input_list,\r\n                                    output_nodes=output_list,\r\n                                    output_dtypes=[default_type for _ in output_list],\r\n                                    output_quantized=False,\r\n                                    op_name=\"trans_fused\",\r\n                                    op_type=\"CustomTfTransOp\")`\r\nand del the 123 line of https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/framework/python/framework/graph_util.py `assert len(name_to_node[n].input) == 1` , but the way, why do the assert ? \r\n\r\nwith above changes, I can return and save the new_graph, But when load the saved graph I got an error \r\n`tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'CustomTfTransOp' in binary running on ac2690. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n`\r\nwhat does the fused operator do?  If I want to fuse the add and mul operation together, Should I add  an op do this? what is the mean of \"fuse_op\"?", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "@jvishnuvardhan ok, But I think the function of this api is to merge existing calculations to a new op, why the new op doesnot exit? if I should add a new op with the new operation I don't need to use this  api", "@wangyunxiaa If you have a solution, you could raise a PR and merge your contributions to TF repository. Please let me know if you want to contribute. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 28037, "title": "tensorflow2.0\u7248\u672c\u7684legacy_seq2seq\u5728\u54ea\uff1f", "body": "tensorflow2.0\u7248\u672c\u7684legacy_seq2seq\u5728\u54ea\uff1f\r\n", "comments": []}, {"number": 28036, "title": "AUTHORS references non-existent file", "body": "**Describe the documentation issue**\r\n\r\nThe [AUTHORS](https://github.com/tensorflow/tensorflow/blob/master/AUTHORS) file references a CONTRIBUTORS files, but no `CONTRIBUTORS` or `CONTRIBUTORS.md` file exists:\r\n\r\n```\r\n# This file is distinct from the CONTRIBUTORS files.\r\n# See the latter for an explanation.\r\n```\r\n\r\nIf CONTRIBUTORS is some where else, this should be made explicit in AUTHORS.\r\n\r\n**We welcome contributions by users. Can you submit a PR?**\r\n\r\nThis is a question for maintainers and a PR is not yet appropriate.", "comments": ["Hi, may I know what exactly do we need to add in CONTRIBUTORS.md and can I work on this?", "@pocc The [AUTHORS](https://github.com/tensorflow/tensorflow/blob/master/AUTHORS) file is only consists of authors only.  Contributors list is provided in the release notes. For `TF2.4` version, [here](https://github.com/tensorflow/tensorflow/releases/tag/v2.4.0) is the list of contributors. Thanks!\r\n\r\nI am closing this issue as this was already resolved. Thanks!", "The documentation on the AUTHORS file is still wrong. It mentions a CONTRIBUTORS **file**, however, it means to reference a CONTRIBUTORS list that is provided on new releases.\r\n\r\nAUTHORS file should be updated to reflect this information."]}, {"number": 28035, "title": "Can I use the same record file for tf_record_input_reader (Object Detection)?", "body": "I have one image dataset with some classes and needed annotations.\r\nSo I made a record file (`train.record`) for training an object detection model.\r\nI set it in `train_input_reader`\r\n\r\nBut what should I set for `eval_input_reader`? Can I use the same record file? \r\n\r\n\"Eval\" is used for verifying the quality of training as I understand\r\n\r\n    train_input_reader: {\r\n      tf_record_input_reader {\r\n        input_path: \"object_detection/train.record\"\r\n      }\r\n      label_map_path: \"object_detection/training/labelmap.pbtxt\"\r\n    }\r\n    \r\n    ...\r\n    \r\n    eval_input_reader: {\r\n      tf_record_input_reader {\r\n        input_path: \"???\"\r\n      }\r\n      label_map_path: \"object_detection/training/labelmap.pbtxt\"\r\n      shuffle: false\r\n      num_readers: 1\r\n    }", "comments": ["@anonym24 This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 28034, "title": "Missing test case added for Conv2D", "body": "", "comments": []}, {"number": 28033, "title": "Object detection API, training part: NewRandomAccessFile failed to Create/Open: data/object-detection.pbtxt", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No I followed Sentdex tutorial about object detection API\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip install, GPU version of tensorflow\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA toolkit: 10.0.130 and cuDNN: 7.3.1\r\n- GPU model and memory: NVIDIA GTX970M, 3GB\r\n\r\n\r\nWhen I try this command in the anaconda prompt:\r\n(from ...Tensorflow\\models\\research\\object_detection)\r\n\r\npython legacy/train.py \r\n--train_dir=training/ \r\n--pipeline_config_path=training/faster_rcnn_inception_v2_pets.config \r\n--logtostderr\r\n\r\nIt should start training to detect the objects I want but I get this error: \r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open: data/object-detection.pbtxt : Le fichier sp\\udce9cifi\\udce9 est introuvable.\r\n; No such file or directory\r\n\r\nAnd I can't figure why I get this error, the object-detection.pbtxt is in the right place, I have also tried to put entire path ( C:/folder1/folder2/.../data/object-detection.pbtxt ) in the config file. I tried to use normal / double slash or backslash in my command line and inside the config file.\r\n\r\nIs it because of object-detection.pbtxt that is considered as a .txt file when I created it with the notepad ?\r\nHoping to find a solution quick.", "comments": ["You should post this issue to [tensorflow/models](https://github.com/tensorflow/models).\r\n", "hey I am getting similar error\r\ncan you tell me where I am going wrong"]}, {"number": 28032, "title": "5526 illegal hardware instruction (core dumped)  python3", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:1.13.1\r\n- Python version:3.7.3\r\n- Installed using virtualenv? pip? conda?:pip3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:GTX950M/4G\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Darkells Could you please provide the remaining details requested on the issue template? Otherwise it is difficult to properly assess and provide a solution. Thanks", "@Carmezim Thank you for your reply, I have fixed this problem.Because my cpu G4400 isn't support AVX", "Great that the issue got resolved. Let me close it."]}, {"number": 28031, "title": "java api runs much slower than Python API", "body": "i predict image with python api \r\nonly need 15ms,\r\nbut with the same model,java api need 500+ms", "comments": ["the java api need 400+ms\r\ncode: \r\nList<Tensor<?>> tesnsors=s.runner().feed(\"input\", image).fetch(\"dense_decoded\").run();\r\n\r\nbut the python api only need about 40ms \uff0c\r\n\r\ni think java api should more faster than the python api,\r\n\r\nwhat can i do to let java api more faster?", "i test it on the same computer,the same model,the same image\r\n\r\nthe tensorflow version is 1.12.0.\r\n\r\n  <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>tensorflow</artifactId>\r\n            <version>1.12.0</version>\r\n        </dependency>", "@mengpengfei This looks duplicate of [#27999.](https://github.com/tensorflow/tensorflow/issues/27999) Can we close this. Thanks!", "yes of course,   my goal is to solve this problem,   but nobody solve it", "@mengpengfei Since the issue is a duplicate. We shall close this issue and can track resolution on this [#27999](https://github.com/tensorflow/tensorflow/issues/27999). Thanks!", "ok", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28031\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28031\">No</a>\n"]}, {"number": 28030, "title": "i use tensoflow model to predict with java api,why it is more slowly than python or c++ api?", "body": "i use tensoflow model to predict with java api,why it is more slowly than python or c++ api?", "comments": ["@mengpengfei This looks duplicate of [#27999](https://github.com/tensorflow/tensorflow/issues/27999). Can we close this. Thanks! ", "yes\uff0c no problems", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28030\">No</a>\n"]}, {"number": 28029, "title": "sample_weight ignored", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\"Ubuntu 16.04.5 LTS\"\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nDocker image from tensorflow/tensorflow:latest-py3-jupyter\r\n- TensorFlow version (use command below):\r\nb'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version:\r\n3.5.2 (default, Nov 12 2018, 13:43:14) \r\n[GCC 5.4.0 20160609]\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nsample_weight seems to be ignored by fit(), test_on_batch(), etc.\r\n\r\n**Describe the expected behavior**\r\nsample_weight should affect the loss.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.losses import mse\r\n\r\n# a dummy model that just returns the inputs\r\ni=Input(shape=(1,))\r\nm=Model(inputs=[i],outputs=[i])\r\nm.compile(optimizer=Adam(),loss=[mse])\r\n\r\n# the unweighted loss should be 1\r\nxs=np.zeros((1,1))\r\nys=np.ones((1,1))\r\n\r\n# expecting loss to be weighted by sample_weight (so be 0.1 for the second test)\r\nprint(m.test_on_batch(xs,ys))\r\nprint(m.test_on_batch(xs,ys,np.array([.1])))\r\n```", "comments": ["In TF 2.0.0-alpha0 the behavior is as expected - that led me to believe the behavior in 1.13.1 is a bug.", "Looks like behavior changed in https://github.com/tensorflow/tensorflow/commit/bacd851cf8281b3a87d782bbaf1888df571337a6#diff-4f0d455edc07c640cbb22c8e39a61dd5.\r\nPresuming I cannot update to a nightly, what is the workaround meanwhile?", "@abirkmanis I can reproduce the issue. Output from TF2.0 is 1.0, and 0.1 whereas output from TF1.13.1 is 1.0 and 1.0. Thanks!", "Hi @abirkmanis , looks like this was fixed as you say\r\n\r\nIIRC, previous behavior was to divide the sample_weights by the mean of the sample_weights, which is why it looks like it is being ignored in this case. My suggestion is if the mean of your sample_weights is not 1, then to either rescale the sample_weights or rescale your loss_fn in order to get the desired behavior", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28029\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28029\">No</a>\n"]}, {"number": 28028, "title": "[TF2.0] Custom training loop for keras example should provide information about tf.keras.backend.set_learning_phase()", "body": "**System information**\r\n- TensorFlow version: tensorflow-gpu 2.0.0-alpha0\r\n- Doc Link: https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#using_the_gradienttape_a_first_end-to-end_example\r\n\r\n**Describe the documentation issue**\r\nThat samples did not use tf.keras.backend.set_learning_phase() for manual training loop. So if the model has BatchNormalization() or other layers which depend on training state, it will be not trained correctly. There is no description about tf.keras.backend.set_learning_phase(), even in the page of BatchNormalization(). (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization)\r\n", "comments": ["Are there any examples on a custom training loop with BatchNormalization?\r\nI've managed to set training_phase, although the model doesn't train the same way as via keras default training loop.", "1. Keras has different side operations. It depends on whether train, test or predict phase is executed.\r\n- https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L497\r\n- https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L524\r\n- https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L542\r\n2. In general a layer has unconditional and conditioned on input updates and losses. Updates can be that of running mean average in batch normalization.\r\nhttps://github.com/keras-team/keras/blob/master/keras/layers/normalization.py#L197\r\nLayer losses may include regularization.\r\nhttps://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py#L246\r\n3. Upon first call to K.learning_phase() a placeholder for the phase state is created with default value False. To train the model feed dict should contain {K.learning_phase(): True}.\r\nhttps://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L123\r\nhttps://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3345\r\nhttps://github.com/keras-team/keras/blob/master/keras/layers/normalization.py#L206", "I simply called set training_phase before creating the model, and train the model with same method in TF 2.0 tutorial. Without calling it, the model does not trained well.", "I found that other document describes \"training=True\" flag for manual training loop:\r\n\r\nhttps://www.tensorflow.org/alpha/guide/migration_guide#customize_the_training_step\r\n\r\nBut this does not:\r\n\r\nhttps://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#using_the_gradienttape_a_first_end-to-end_example\r\n\r\nAlso, second document does not use tf.function for its training step, so extremely slow than first sample. Additionally, GradientTape version manual trainig loop requires more memory than train_on_batch version. I can't figure out whether this issue is intended behaviour or bug. (Maybe related to #28110)", "With respect to the initial issue, I don't thing a discussion of the learning phase is appropriate, because the example doesn't include any layers that depend on it. Writing a custom loop that uses batch norm is something of an advanced use case both because of the change in behavior between training and inference and the fact that its updates are not gradient updates. All of that complexity would distract from the goal of a simple first example.\r\n\r\nWith respect to the actual use of the learning phase, I would recommend directly using the `training` argument of the model rather than `set_learning_phase`. Passing what you want directly into a function (a model is a function, after all) is more idiomatic TF 2.0 than setting a global persistent state. (Or `keras.backend.learning_phase_scope` can split the difference) I do, however, agree that the learning phase is not always a straightforward concept and it can be easy to get tripped up on layers whose behavior depend on it. @dynamicwebpaige @fchollet Thoughts on adding a learning phase example to the documentation queue?\r\n\r\nLastly, with respect to performance what you describe is expected. Eager performance is expected to be slower than graph in most cases, and higher memory consumption is also expected. In graph mode the runtime is free to free memory during backprop, but in eager it must simultaneously hold all of the activations and all of the gradients in memory. As for acceleration on with tf.function: there are definitely guides on that, but it has some quirks (function tracing and only once execution, trace caching, etc.) that again makes me think it probably shouldn't be in the very first run loop example.", "I agree that it would be nice to have an example with batch normalization.\r\n\r\nI also stumbled about this when I converted my code to TF 2: In graph mode you had to run the collected update ops from your model. In eager mode all updates were done in place and the call to updates() just did nothing. But once I wrapped my training step in a tf.function I ended up getting an error and it was really not clear to me if I had to run the updates() inside a tf.function or not. I think it might be helpful to clarify that.", "> Lastly, with respect to performance what you describe is expected. Eager performance is expected to be slower than graph in most cases, and higher memory consumption is also expected. In graph mode the runtime is free to free memory during backprop, but in eager it must simultaneously hold all of the activations and all of the gradients in memory. As for acceleration on with tf.function: there are definitely guides on that, but it has some quirks (function tracing and only once execution, trace caching, etc.) that again makes me think it probably shouldn't be in the very first run loop example.\r\n\r\n@robieta Thanks for clarification. I think that this also should be clearly described in documents for new users. Current training sample (Eager without tf.function) is extremely slow as you described, so it makes user misunderstand that TF 2.0 is slow (like me).\r\n\r\nI know that TF 2.0 is in the early stage, but these issue makes me that hard to investigate whether an issue is intended behaviour or bug without documentations.", "With regard to the tf.keras.BatchNormalization layer, the documentation specifies that the second layer input is a python boolean indicating whether the model is used for training or not.\r\nHow does this relate to the training parameter of Model? If one left the layer flag unspecified, would the Model parameter be used instead? \r\nIn the examples, only the layer input is applied to BatchNormalization. \r\nWhat if one wants to alternate a train and test step, is it sufficient to edit the model variable or should it be rebuilt? \r\n\r\nI think that the documentation is lacking this kind of information, which is quite crucial for successfully using recent models.", "Additionally, when I using Model.train_on_batch(), there is no way to pass Training=True flag, so, does TF 2.0 automatically set True when calling train_on_batch() or should I set set_learning_phase() manually?\r\n\r\nCurrent documentation is too useless for that situation. ", "I think this is a hidden thing in tf.keras now.\r\nWith keras model.fit() api, all these are hidden. @robieta documentation is necessary.\r\nHowever, if you want to directly use keras layers and models, there is a training field inside keras model. For evaluation, you shall set it as false. https://github.com/tensorflow/models/blob/master/official/bert/model_training_utils.py#L249\r\n\r\nCorrect me if I am wrong....\r\nThx", "The migration guide actually have it for TF2.0: https://www.tensorflow.org/beta/guide/migration_guide. But it wasn't mentioned in keyword. So it's sort of hidden here.", "> **System information**\r\n> \r\n> * TensorFlow version: tensorflow-gpu 2.0.0-alpha0\r\n> * Doc Link: https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#using_the_gradienttape_a_first_end-to-end_example\r\n> \r\n> **Describe the documentation issue**\r\n> That samples did not use tf.keras.backend.set_learning_phase() for manual training loop. So if the model has BatchNormalization() or other layers which depend on training state, it will be not trained correctly. There is no description about tf.keras.backend.set_learning_phase(), even in the page of BatchNormalization(). (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization)\r\n\r\nAdditionally, using `training=False` or `training=True` doesn't set learning phase in keras. So when I print `keras.backend.learning_phase()`, it will also say 0 (test phase) even if I use the training argument. This is confusing.", "> Additionally, using `training=False` or `training=True` doesn't set learning phase in keras. So when I print `keras.backend.learning_phase()`, it will also say 0 (test phase) even if I use the training argument. This is confusing.\r\n\r\nIt doesn't because ```training``` variable directly affects which computing branch is being returned at graph construction time inside a python interpreter.\r\nhttps://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3363\r\n", "> Are there any examples on a custom training loop with BatchNormalization?\r\n> I've managed to set training_phase, although the model doesn't train the same way as via keras\r\n> default training loop.\r\n\r\nYou can find an example of the `training` variable usage for batch norm layers using the functional API and custom training loops in the Tensorflow resnet50 implementation. \r\n\r\n[Here](https://github.com/tensorflow/models/blob/e38d570ef03c16c26d037b23fc2f35ecd4f87032/official/resnet/ctl/ctl_imagenet_main.py#L184) is the model call with the `training=True` in the custom training loop.\r\n\r\nNote that the model definition is using the functional API and does **not** specify the batch norm layers with the `training` parameter. [see here](https://github.com/tensorflow/models/blob/e38d570ef03c16c26d037b23fc2f35ecd4f87032/official/resnet/keras/resnet_model.py#L202)\r\n\r\nWhen the custom training loop calls the model with the `training` parameter, the [`call` method in tensorflow/python/keras/engine/network.py](https://github.com/tensorflow/tensorflow/blob/7c2a7d4684b55f6f869bd814d67f2d2d4629fd55/tensorflow/python/keras/engine/network.py#L675) calls `_run_internal_graph` which resets the `training` parameter for each layer that has `'training'` in its argspec. [see here](https://github.com/tensorflow/tensorflow/blob/7c2a7d4684b55f6f869bd814d67f2d2d4629fd55/tensorflow/python/keras/engine/network.py#L828-L830)", "What about models from keras.applications?\r\nI would to like to use keras.Model from keras.applications in my custom training loop.\r\n\r\nIt is not clear, because these models create like tf.keras.Model(inputs, outputs) and there is not any information about call function parameters.", "@rwl93 , I cannot find any special handling of batch norm for custom training loops in the code you linked\r\n@robieta , you mention that custom training loops with batch norm are an advanced topic\r\n\r\nSo which is it ? When I try, my model is not training to the same accuracy as with model.fit(). But then I might be hitting another issue.\r\n\r\nIs there something special to do when using tf.keras.layers.BatchNormalization with a custom training loop ?", "I put together a quick example of batch norm and CTL [gist](https://colab.sandbox.google.com/gist/robieta/044ec2031399e1dae5e42fc7c8bd48d3/batch_norm_in_ctl.ipynb). The main thing is to pass the training arg when calling the model. (I was lazy and just used model.evaluate to assess the CTL) Interestingly that also seems to apply the updates to the moving statistics. For whatever reason I thought those had to be handled separately (handling them correctly was a major stumbling block of 1.x CTL's), but it seems that either the layer or automatic control dependencies are picking them up automagically. So I guess no so advanced a use case as I initially thought.", "I also noticed the problem months ago. I remember none of our examples pass\ntraining arg. However, we should pass it explicitly in CTL and all official\nmodels have that now.\n\nOn Thu, Oct 3, 2019, 11:17 PM Taylor Robie <notifications@github.com> wrote:\n\n> I put together a quick example of batch norm and CTL gist\n> <https://colab.sandbox.google.com/gist/robieta/044ec2031399e1dae5e42fc7c8bd48d3/batch_norm_in_ctl.ipynb>.\n> The main thing is to pass the training arg when calling the model. (I was\n> lazy and just used model.evaluate to assess the CTL) Interestingly that\n> also seems to apply the updates to the moving statistics. For whatever\n> reason I thought those had to be handled separately (handling them\n> correctly was a major stumbling block of 1.x CTL's), but it seems that\n> either the layer or automatic control dependencies are picking them up\n> automagically. So I guess no so advanced a use case as I initially thought.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28028?email_source=notifications&email_token=ABFFXZPC3L7LOUBIK7VVPI3QM3NXVA5CNFSM4HHM2SS2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAKR4WQ#issuecomment-538254938>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABFFXZJXQNEHSWV4HWQ6O6LQM3NXVANCNFSM4HHM2SSQ>\n> .\n>\n", "Thank you for putting this example together. It is really nice. You say the main thing is to call the model with model(data, training=True) during training. After reading your code, it seems to me that's the *only* thing you are doing differently. Correct ?", "That is correct.", "Recommend to look at this method\r\nhttps://github.com/tensorflow/tensorflow/blob/99dda8ef941eac413e196b7bd6a7ba58b553b4ca/tensorflow/python/keras/engine/base_layer.py#L627\r\n\r\nit calls before every specific layer calls\r\n\r\nfirstly training flag is searched in kwargs, and if not training= backend.learning_phase\r\n", "> I put together a quick example of batch norm and CTL [gist](https://colab.sandbox.google.com/gist/robieta/044ec2031399e1dae5e42fc7c8bd48d3/batch_norm_in_ctl.ipynb). The main thing is to pass the training arg when calling the model. (I was lazy and just used model.evaluate to assess the CTL) Interestingly that also seems to apply the updates to the moving statistics. For whatever reason I thought those had to be handled separately (handling them correctly was a major stumbling block of 1.x CTL's), but it seems that either the layer or automatic control dependencies are picking them up automagically. So I guess no so advanced a use case as I initially thought.\r\n\r\n@robieta  Thanks for sample code. It clarifies how to train BatchNormalization layer by using GradientTape. But how about train_on_batch() and predict_on_batch() API? it doesn't have training parameter. In my understand, keras API (train, predict, evaluate and so on) handles training status internally (so it will be True while I running train_xx method) and I should not call set_learning_phase() manually. Is this correct?", "That is correct: the `Model.*_on_batch` methods set the learning phase for you.", "With regards to the original issue, I made [this commit](https://github.com/tensorflow/docs/commit/7ad5e9598e66efeef8b3ed9fa53fa5ed023ddbed) that gives users a heads up about the training=True/False for custom training loops across the tutorials on tensorflow.org where possible (some examples have single layers that don't have or need a training=True/False and trying to pass the training param would throw an error). "]}, {"number": 28027, "title": "`import tf.compat.v2.summary` broken since 88ca0db75e", "body": "The following import should work:\r\n\r\n```python\r\nimport tensorflow.compat.v2.summary as b\r\nb.scalar\r\n```\r\n\r\nThis worked fine in d4342f77b6, but broke in 88ca0db75e (first bad):\r\n\r\n```\r\n$ cat ./repro.py\r\nimport tensorflow as tf\r\nprint(tf.__git_version__)\r\nimport tensorflow.compat.v2.summary as b\r\nprint(b)\r\nprint(b.scalar)\r\n```\r\n\r\n```\r\n$ virtualenv -q -p python2.7 ./ve-old/\r\n$ . ./ve-old/bin/activate\r\n(ve-old) $ pip install ~/tmp/tf-builds/tf-d4342f77b6/* >/dev/null 2>/dev/null\r\n(ve-old) $ python ./repro.py\r\nv1.12.1-143-gd4342f77b6\r\n<module 'tensorflow.compat.v2.summary' from '/tmp/tmp.kn2U5CUdHI/ve-old/local/lib/python2.7/site-packages/tensorboard/summary/_tf/summary/__init__.pyc'>\r\n<function scalar at 0x7f682249aa28>\r\n(ve-old) $ deactivate\r\n```\r\n\r\n```\r\n$ virtualenv -q -p python2.7 ./ve-new/\r\n$ . ./ve-new/bin/activate\r\n(ve-new) $ pip install ~/tmp/tf-builds/tf-88ca0db75e/* >/dev/null 2>/dev/null\r\n(ve-new) $ python ./repro.py\r\nv1.12.1-144-g88ca0db75e\r\n<module 'tensorflow.compat.v2.summary' from '/tmp/tmp.kn2U5CUdHI/ve-new/local/lib/python2.7/site-packages/tensorflow/_api/v1/compat/v2/summary/__init__.pyc'>\r\nTraceback (most recent call last):\r\n  File \"./repro.py\", line 5, in <module>\r\n    print(b.scalar)\r\nAttributeError: 'module' object has no attribute 'scalar'\r\n(ve-new) $ deactivate\r\n```\r\n\r\nNote that the imported module path has changed from [TensorBoard\u2019s\r\ncomponent package entry point][tb] to the stub provided by TensorFlow:\r\n\r\n```python\r\n# This file is MACHINE GENERATED! Do not edit.\r\n# Generated by: tensorflow/python/tools/api/generator/create_python_api.py script.\r\n\"\"\"Operations for writing summary data, for use in analysis and visualization.\r\n\r\nSee the [Summaries and\r\nTensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) guide.\r\n\r\n\"\"\"\r\n\r\nfrom __future__ import print_function as _print_function\r\n\r\nfrom tensorflow._api.v1.compat.v2.summary import experimental\r\nfrom tensorflow.python.ops.summary_ops_v2 import SummaryWriter\r\nfrom tensorflow.python.ops.summary_ops_v2 import _flush_fn as flush\r\nfrom tensorflow.python.ops.summary_ops_v2 import create_file_writer_v2 as create_file_writer\r\nfrom tensorflow.python.ops.summary_ops_v2 import create_noop_writer\r\nfrom tensorflow.python.ops.summary_ops_v2 import record_if\r\nfrom tensorflow.python.ops.summary_ops_v2 import trace_export\r\nfrom tensorflow.python.ops.summary_ops_v2 import trace_off\r\nfrom tensorflow.python.ops.summary_ops_v2 import trace_on\r\nfrom tensorflow.python.ops.summary_ops_v2 import write\r\n\r\ndel _print_function\r\n```\r\n\r\n(Note the absence of any imports of `scalar`, etc.)\r\n\r\n[tb]: https://github.com/tensorflow/tensorboard/blob/90a386d1da4d9f477cb60b679f07aa98045d8c7e/tensorboard/summary/_tf/summary/__init__.py\r\n\r\nThis is blocking TensorBoard\u2019s CI.\r\n", "comments": ["cc @annarev @nfelt ", "Reopening due to rollback in f1f5865307d1b162b937dc2e788eaef21713e22b.\r\n\r\nFor what it\u2019s worth, confirmed fixed with `v1.12.1-152-g93b440f793`.\r\n", "Can you try again? The rollback has been rolled forward", "Yep, fixed on latest nightly. Thanks!"]}, {"number": 28026, "title": "Logger suggests non-existent module attribute 'tf.keras.layers.CuDNNLSTM' when using 'tf.keras.layers.LSTM'", "body": "**System information**\r\n- Windows 10 Education\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0\r\n- TensorFlow version: tensorflow-gpu==2.0.0-alpha0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.3.1\r\n- GPU model and memory: NVIDIA GeForce GTX 1080 / 16GB\r\n\r\n**Describe the current behavior**\r\nLogger throws warning, that the use of 'tf.keras.layers.LSTM' is not optimized for performance on GPU. Suggests 'tf.keras.layers.CuDNNLSTM' instead, but module attribute 'CuDNNLSTM' doesn't exist.\r\n\r\n[Screenshot](https://imgur.com/gdvAPAr)\r\n\r\n**Describe the expected behavior**\r\nUsage of 'tf.keras.layers.CuDNNLSTM' doesn't throw an AttributeError, when being a suggested solution.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n# Initialising the RNN\r\nregressor = tf.keras.models.Sequential()\r\n\r\n# Adding the first LSTM layer and some Dropout regularisation\r\nregressor.add(tf.keras.layers.LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))   # using not-perfomance-optimized LSTM, produces warning\r\nregressor.add(tf.keras.layers.Dropout(0.2))\r\n\r\n# Adding a second LSTM layer and some Dropout regularisation\r\nregressor.add(tf.keras.layers.CuDNNLSTM(units = 50, return_sequences = True))   # using log-suggested LSTM, throws AttributeError\r\nregressor.add(tf.keras.layers.Dropout(0.2))\r\n```\r\n[lstm_stocks.zip](https://github.com/tensorflow/tensorflow/files/3101494/lstm_stocks.zip)", "comments": ["@jwiebels Please find instructions in this [link](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM#used_in_the_tutorials) which help you to use tf.keras.layers.LSTM with CuDNN implemetation. Let me know how it progresses. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28025, "title": "Transformer model for language understanding link broken", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: tf 2.0\r\n- Doc Link: https://www.tensorflow.org/alpha/tutorials/text/transformer\r\n\r\n\r\n**Describe the documentation issue**\r\nWhen trying to find the code on github, through the button, the path is broken as well as the Run in google colab button.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": []}, {"number": 28024, "title": "Using tensorflow-gpu == 1.7.0 with CUDA 8", "body": "I am facing ```ImportError``` when I try calling ```tensorflow-gpu == 1.7.0``` on a system with ```CUDA 8``` and ```CuDNN 5.1```. I am not authorized to update the CUDA version of the system, so am looking for a workaround.\r\n\r\n> ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "comments": ["You can try to compile the tensorflow gpu of the specified CUDA version.But not sure if this will succeed.", "```tensorflow-gpu == 1.7.0``` is a requirement that needs to be met in the project. ```CUDA``` and ```CuDNN``` versions of the system are not to be altered with. Is there a workaround to this problem?", "@Akella17 You can still compile TensorFlow from source with the codebase on v.1.7.0 targeting the CUDA/CuDNN versions specified. ", "Okay. Thanks :)"]}, {"number": 28023, "title": "fix(TimeDistributed): error with custom layer output_shape tuples", "body": "**Problem**\r\nWhen using custom layers with the `TimeDistributed` wrapper in the v2 preview, an error is thrown if the output_shape is not returned as a `TensorShape` instance. This goes contrary to the v1 docs, and also how custom layers work without the time distributed wrapper. \r\n\r\n**Reproduction**\r\nhttps://colab.research.google.com/gist/justindujardin/d22de372c3eb6adaf7b6d85c8f6c5dfb/tf_keras_layer_output_shape_bug.ipynb\r\n\r\n**Solution**\r\nCoerce tuple/list output shape types into TensorShape without erroring.", "comments": ["While writing the test I noticed the same error was in the `BiDirectional` class wrapper so I added a similar fix and test for it."]}, {"number": 28022, "title": "Cannot run tensorflow GPU 2.0 with cuda 9.2", "body": "## System information\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): I used virtualenv and _pip install --upgrade tensorflow-gpu==2.0.0-alpha0_\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: installed in virtualenv using\r\n- Name of the Virtualenv: `tensorflow` \r\n- CUDA/cuDNN version: cuda 9.2\r\n---\r\n## Problem\r\nHello, I've installed tensorflow 2.0 with cuda 9.2 and when I run: `>> import tensorflow`, it prompts: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/_api/v2/audio/__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/kexin/virtualenv/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```\r\nThank you!", "comments": ["Tensorflow 2.0 Alpha 2.0 requires CUDN 10.0.So you need to use CUDN10.0.", "+ Latest TensorFlow supports cuda 8-10. cudnn 6-7.\r\n+ Each TensorFlow binary has to work with the version of cuda and cudnn it was built with. If they don't match, you have to change either the TensorFlow binary or the Nvidia softwares.\r\n+ Official tensorflow binaries (the one downloaded by pip or conda) are built with cuda 9.0, cudnn 7 since TF 1.5, and cuda 10.0, cudnn 7 since TF 1.13. These are written in the [release notes](https://github.com/tensorflow/tensorflow/releases). You have to use the matching version of cuda if using the official binaries.\r\n+ If you don't like to install a different version of Nvidia software, you can:\r\n(1) Use a different version of official TensorFlow binary\r\n(2) Use non-official binaries built by others, if they happen to match your existing Nvidia software. Some can be found at https://github.com/mind/wheels/releases, https://github.com/hadim/docker-tensorflow-builder#builds, \r\nhttps://github.com/inoryy/tensorflow-optimized-wheels\r\n(3) Build the binaries by yourself from source with your version of Nvidia software.", "@ChristophePRAT Please try downgrading your tensorflow version and check and the [link](https://www.tensorflow.org/install/gpu) says tensorflow-gpu==2.0.0-alpha0 supports CUDA10.0 . Please follow the steps and let us know your progress.", "@ChristophePRAT In short, \r\n\r\n(1) Official pip binaries ofl TF1.13.1 and TF2.0 are built with CUDA10.0. So you need to upgrade your CUDA if you want to install TF1.13.1 or TF2.0. \r\n\r\n(2) Official binary of TF1.12 was built using CUDA9.0. So you need to downgrade from CUDA9.2 to CUDA9.0 if you want to install TF1.12.\r\n\r\n(3) If you don't want to touch CUDA9.2, then you can use one of the link (https://github.com/inoryy/tensorflow-optimized-wheels) provided by @ppwwyyxx to install TF1.10\r\n\r\n(4) If you are able to install TF from source, then you could use CUDA 9.2 with TF2.0. But, it could take more time and efforts.\r\nPlease let us know how it progresses. Thanks! ", "Thank you very much everybody! Just installed Cuda 10.0 with tensorflow 2.0 and it works."]}, {"number": 28021, "title": "Fix build error", "body": "```cuda_version``` was assigned in one line, reassigned in second and compared in the third which gives an error because of the reassignment step. So, moving the reassignment step in last resolves the problem.", "comments": ["@chsigg yeah, that would work too, but I was thinking about minimal changes to the code.\r\nHow about this in ```else```\r\n`cuda_lib_version = (\"64_%s%s\" if is_windows else \"%s.%s\") % (cuda_version[0], cuda_version[1])`"]}, {"number": 28020, "title": "tf.control_dependencies does execute nodes in the right order", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.10.0-rc1-19-g656e7a2b34,  1.10.0\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0/7\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nwith tf.control_dependiencies does not execute the dependant nodes before the nodes in the with block\r\n\r\n**Describe the expected behavior**\r\nthe nodes in the with block are executed after the dependant nodes\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.Variable(initial_value = 0.0, name = \"x\", dtype = tf.float32)\r\ny = tf.Variable(initial_value = 0.0, name = \"y\", dtype = tf.float32)\r\nzero = tf.constant(value = 0.0, dtype = np.float32, name = \"zero\")\r\nop1 = tf.assign(x, zero)\r\nwith tf.control_dependencies([op1]):\r\n    op2 = tf.assign(y, x)\r\nwith tf.control_dependencies([op2]):\r\n    tf.assign_add(y, 1, name = \"assign_add\")\r\n\r\nsess = tf.Session()\r\nsess.run(\"assign_add\")\r\nprint(\"y\", sess.run(\"y:0\"))\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["You should include the output of your code to show what the problem is. Please note that not everyone gets the same output running the same code.\r\n\r\nYour issue sounds similar to existing ones such as #16277 and #14498, which can be solved by using ResourceVariable.", "Please have a look on @ppwwyyxx  suggestion and let us know. Thanks!", "Hi, thanks for the links to similar issues. My problem now seems to be more a question regarding the specification: In the code snippet below, can we say anything of the order of execution for the nodes node_a, node_b and node_c?\r\n\r\nwith tf.control_dependencies([node_a, node_b]):\r\n    node_d = 10*node_c\r\nsess.run(node_d)\r\n\r\nI assumed that node_a and node_b were executed before node_c but when testing I saw that this is not always the case (even though it is the case in some situations) \r\n\r\nFrom #16277 it sounds like the behaviour for tf.Variable is complicated and depends on the type of operations in node_a, node_b and node_c, while the behaviour for tfe.Variable should be more predictable.\r\n\r\nIn the responses to a similar issue I also found: \r\n\"The only guarantee tensorflow makes about order of execution is that all dependencies (either data or control) of an op are executed before that op gets executed.\"\r\nhttps://github.com/tensorflow/tensorflow/issues/15077#issuecomment-349111997# \r\n\r\nAm I correct in saying that you should not use tf.control_dependencies() to ensure the order of reading/writing variables?", "> Am I correct in saying that you should not use tf.control_dependencies() to ensure the order of reading/writing variables?\r\n\r\nFor `tf.Variable` I think this is true. Execution order on ResourecVariable or `tfe.Variable` is better defined, however.", "@jonassod : Were you able to get the clarification ?", "Well, I understand that this is not a bug in Tensorflow, so in that sens it\u2019s clear. The intended behavior for tf.control_dependencies() is however not clear to me. The question I am having is as follows:\n\nIn the code snippet below, can we say anything of the order of execution for the nodes node_a, node_b and node_c?\n\nwith tf.control_dependencies([node_a, node_b]):\nnode_d = 10*node_c\nsess.run(node_d)\n\n\nSent from my iPhone\n\n> On 26 Apr 2019, at 11:31, achandraa <notifications@github.com> wrote:\n> \n> @jonassod : Were you able to get the clarification ?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 28019, "title": "can't do serving with model generated by TF2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0rc0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI trained model with tf.keras's functional api with tensorflow 2.0 and save model with tf.keras.Model.save() and convert the h5 model with tf.compat.v1.saved_model.simple_save(). When I try to serve the model with the command\r\n\r\n```Bash\r\nsaved_model_cli run --dir ./serving_model --tag_set serve --signature_def serving_default --input_exp 'input_image = np.random.normal(size=(1,28,28,1))'\r\n```\r\n\r\n I get error message\r\n\r\n>RunTimeError: The Session graph is empty. Add operations to the graph before calling run().\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect that an output tensor representing the class of the input handwriting digit is returned from the serving.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nPlease refer to my project at https://github.com/breadbread1984/EagerExecutionDemo. after train with train_mnist.py, executing start_serving.sh will reproduce the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@breadbread1984 This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "With eager execution enabled I'd use v2 SavedModel APIs rather than compat.v1. Could you take a look at the [2.x SavedModel guide](https://www.tensorflow.org/alpha/guide/saved_model) and see if that works for you?", "Thx, I will try it."]}]