[{"number": 53454, "title": "ERROR: gen_sparse_csr_matrix_ops not exists in tensorflow.python.ops.linalg.sparse", "body": "\r\nhttps://github.com/tensorflow/tensorflow/blob/1e3cc64881fc65226f98b7fc1c8ac6f5debac7df/tensorflow/python/ops/linalg/sparse/sparse_csr_matrix_ops.py#L32\r\n\r\nfrom tensorflow.python.ops.linalg.sparse import gen_sparse_csr_matrix_ops always returns error", "comments": ["BUILD needed"]}, {"number": 53453, "title": "Description error of 'non_max_suppression_with_scores'", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n1. In line 3755\uff0cI think it we should modify `tf.image.non_max_suppression_padded` to be `tf.image.non_max_suppression_with_scores`;\r\n2. In line 3760\uff0cI think it we should modify `tf.image.non_max_suppression_padded` to be `tf.image.non_max_suppression_with_scores`;\r\n", "comments": ["@sushreebarsa should i submit a PR \uff1f or someone will fix docs bug together?", "@raoshenglong Sorry for the late response! \r\nPlease let me know if I should raise a PR on your behalf to fix this issue ?Thanks!", "@sushreebarsa OK\uff0cyou can raise a PR to fix it and then close this issue.", "@raoshenglong This issue will be closed once the [PR](https://github.com/tensorflow/tensorflow/pull/53496) is merged.Thanks!", "@raoshenglong Could you please confirm if we can close this issue as the [PR](https://github.com/tensorflow/tensorflow/pull/53496) is merged?Thanks!", "OK, I will close it. @sushreebarsa "]}, {"number": 53452, "title": "Fixed Typo in  \"iterator_ops.py\"", "body": "Changed \"warpped\" to \"wrapped\" in iterator_ops.py", "comments": ["Usually we only accept these changes to master. Usually we don't accept changes that only fix one single typo.\r\n\r\nThe documentation on the old branches won't get updated with the patch releases, it is supposed that patch releases only fix security issues, not add new features / new documentation."]}, {"number": 53451, "title": "Tensor shape mismatches the associate numpy array shape", "body": "The following code demonstrates a simple TF graph with transpose, pad and another transpose that gives the output data with shape that mismatches tensor shape.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\n\r\nif __name__ == '__main__':\r\n    tf.disable_eager_execution()\r\n    tf.reset_default_graph()\r\n\r\n    input_ = tf.placeholder(shape=(5, 3, 2, 4), dtype=tf.float32)\r\n\r\n    permutation1 = tf.constant(value=(0, 2, 3, 1))\r\n    transpose1 = tf.transpose(a=input_, perm=permutation1)\r\n\r\n    pad_shape = tf.constant(value=((0, 0), (0, 0), (0, 0), (0, 0)))\r\n    # pad_shape = tf.constant(value=((0, 0), (10, 0), (0, 0), (0, 0)))  # THIS VERSION WORKS!!!\r\n\r\n    pad = tf.pad(tensor=transpose1, paddings=pad_shape)\r\n\r\n    identity1 = tf.identity(input=pad)\r\n\r\n    permutation2 = tf.constant(value=(0, 3, 1, 2))\r\n    out_tensor = tf.transpose(a=identity1, perm=permutation2)\r\n\r\n    tf.identity(out_tensor)\r\n\r\n    input_data = np.random.randint(low=-127, high=127, size=np.prod(input_.shape)).reshape(input_.shape).astype(\r\n        np.float32)\r\n\r\n    with tf.Session() as session:\r\n         tf_out_arrays = session.run(fetches=[out_tensor], feed_dict={input_: input_data})\r\n\r\n    output_data = tf_out_arrays[0]\r\n\r\n    assert input_.shape == input_data.shape, 'Input data shape'\r\n    assert output_data.shape == out_tensor.shape, f'Output data shape mismatch {output_data.shape} != {out_tensor.shape}'\r\n```\r\n\r\nGives the assertion error:\r\n```\r\nAssertionError: Output data shape mismatch (5, 4, 3, 2) != (5, 3, 2, 4)\r\n```\r\n\r\nThe transposition was not applied by the TF and tensor shape now mismatches the data shape.\r\n\r\nInterestingly if the padding values are not zeros, that is if the line\r\n```\r\n  pad_shape = tf.constant(value=((0, 0), (0, 0), (0, 0), (0, 0)))\r\n```\r\nis changed to \r\n```\r\npad_shape = tf.constant(value=((0, 0), (10, 0), (0, 0), (0, 0)))\r\n```\r\nthe code works.\r\n\r\nThe TF version is:\r\n```\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n2021-12-16 11:45:20.516535: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-12-16 11:45:20.516563: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nv2.4.0-49-g85c8b2a817f 2.4.1\r\n```\r\nbut I have also tested it against 2.4.4, 2.5.2, 2.6.2  and 2.7.0 - the mismatch reproduces. \r\n\r\nThe code was tested in Ubuntu 20.04 and ArchLinux.\r\n\r\nNo GPU was used.\r\n", "comments": ["@Saduf2019 ,\r\nI was able to reproduce the issue in tf v2.7, v2.5 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f6426f728b851140c5a515b19921be4a/53451.ipynb).", "@Evgeny-IT \r\nWe see that you have not filled the issue template and used 1.x, the error reported is because the two shapes are not same.\r\nAs this is not bug or feat request hence please create this issue in discussion forum and move this to closed status.", "@Saduf2019\r\n\r\n> We see that you have not filled the issue template\r\n\r\nNo, I filled them all. What fields of issue template do you think are missing?\r\n\r\n> used 1.x,\r\n\r\nNo, I used 2.x, see the reply of @tilakrayal - he was able to reproduce the issue for 2.x package.\r\n\r\n> As this is not bug\r\n\r\nNo, the error is here and is reproduced. Why this is a not bug? The expected behavior is not met.", "@Evgeny-IT \r\nAs you can see you have used disable eager that means you are using tf 1.x, as explained this is not a bug we cannot fix coding issues in this repo, kindly open this in discussion forum.", "@Saduf2019 \r\n\r\n> explained this is not a bug\r\n\r\nCan you please give a more detailed explanation why this is not a bug? I see statements that this is not a bug and not explanation why. tf 1.x bugs are not considered bugs? ", "@Evgeny-IT \r\nThere is no support for tf 1.x anymore hence we cannot look into its issues. you are using disable and expecting it to work on 2.x this is incorrect coding and the errors reported is due to shape mismatch which is not  a bug we cannot fix such coding errors in this repo.", "@Evgeny-IT Session.run is deprecated. We see that you're using Session which does not work with  eager execution\r\n (TF V2). To migrate code that uses sessions to TF2, rewrite the code without it. Please see the [migration guide](https://www.tensorflow.org/guide/migrate#1_replace_v1sessionrun_calls) on replacing Session.run calls.\r\n\r\nTF v1.x is currently  not actively supported so please try to upgrade to TF v2.4 or later versions.\r\nFor any further queries regarding TF v1.x specific issues please post it in [TF forum](https://discuss.tensorflow.org/) where there is a larger community to get you the right help.\r\nThanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53451\">No</a>\n"]}, {"number": 53450, "title": "Support for external delegate in label_image.py python example", "body": "The PR adds 2 command line arguments to label_image.py python example to load and use an external delegate. The behavior is similar to one provided by external delegate Registrar (class `ExternalDelegateProvider` in  [tensorflow/lite/tools/delegates/external_delegate_provider.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/delegates/external_delegate_provider.cc))", "comments": ["> @robert-kalmar Did you run this with any specific external delegate? How was the performance? Can you add some details to the PR description?\r\n\r\nYes, I used ArmNN Delegate and the VX Delegate (https://github.com/VeriSilicon/tflite-vx-delegate). What performance you mean? It depends on the external delegate capabilities. The performance is comparable with the benchmark_model application using the same delegate.  ", "> What performance you mean? It depends on the external delegate capabilities. \r\n\r\nCould you mention the speedups you got with the delegates, on an example device? I understand it depends on the model/device/delegate, but some indicative numbers will be good for the record on this PR. Thanks!"]}, {"number": 53448, "title": "Fix typo in `trt_engine_op_shape_test.py`", "body": "This PR fixes a small typo introduced in #53327 that prevented the test from being run.\r\n\r\n@DEKHTIARJonathan ", "comments": ["Are you sure the change is correct?\r\nI got this error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/framework/test_util.py\", line 1645, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 1101, in _Test\r\n    self.RunTest(run_params)\r\n  File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 1018, in RunTest\r\n    infer_saved_model_dir = self._GetInferGraph(run_params, saved_model_dir)\r\n  File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/trt_engine_op_shape_test.py\", line 61, in _GetInferGraph\r\n    func, _ = get_func_from_saved_model(trt_saved_model_dir)\r\n  File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/trt_engine_op_shape_test.py\", line 56, in get_func_from_saved_model\r\n    saved_model_dir, tags=[tag_constants.SERVING])\r\nTypeError: 'module' object is not callable", "@Kh4L Can you please check @bixia1's comments and keep us posted ? Thanks!", "@Kh4L Any update on this PR? Please. Thanks!", "> Are you sure the change is correct? I got this error:\r\n> \r\n> Traceback (most recent call last): File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/framework/test_util.py\", line 1645, in decorated return f(self, *args, **kwargs) File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 1101, in _Test self.RunTest(run_params) File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 1018, in RunTest infer_saved_model_dir = self._GetInferGraph(run_params, saved_model_dir) File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/trt_engine_op_shape_test.py\", line 61, in _GetInferGraph func, _ = get_func_from_saved_model(trt_saved_model_dir) File \"/build/work/f597b7130c23d9a7ed113a86d697d5f754ae/google3/runfiles/google3/third_party/tensorflow/python/compiler/tensorrt/test/trt_engine_op_shape_test.py\", line 56, in get_func_from_saved_model saved_model_dir, tags=[tag_constants.SERVING]) TypeError: 'module' object is not callable\r\n\r\nthis PR doesn't seem to make the right change, as I pointed out before.", "@Kh4L Can you please check @bixia1's comments and keep us posted ? Thanks!", "@bixia1 I think this change is not relevant here, it was an internal fix, closing now"]}, {"number": 53447, "title": "[TFLite] Support building external delegate with CMake", "body": "TensorFlow Lite supports external delegate but not supported in CMake build, do we have plan to support it?", "comments": ["Sorry, looks like https://github.com/tensorflow/tensorflow/pull/52962 is addressing this request.", "@Honry! Thank for the feature request. This issue will be closed once above [PR](https://github.com/tensorflow/tensorflow/pull/52962) is merged.  Thanks!", "Hi @Honry ! Can this issue be closed now ? The above PR is merged now.\r\n", "Yeah sure, thank you for your great work!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 53445, "title": "Can not run the first test", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\ndef execute2_lds(plan, start, capacity):\r\n# letter count\r\nvalue = 0\r\npoints = []\r\npoints.append(start)\r\n# count the letter\r\nfor i in range(len(plan)):\r\nexvalue = 0\r\nif plan[i][0] > capacity:\r\nexxvalue = abs(plan[i][2][0] - plan[i][1][0])\r\nexyvalue = abs(plan[i][2][1] - plan[i][1][1])\r\n# determine capacity\r\nexunits = (exxvalue +exyvalue) * (plan[i][0] % capacity + 1) * 2\r\nexvalue += exunits\r\nvalue += exvalue\r\nnplan = plan[i][1:]\r\nfor j in nplan:\r\n# put all coodinates into a list\r\npoints.append(j)\r\nfor z in range(len(points) - 1):\r\nxunits = abs(points[z + 1][0] - points[z][0])\r\nyunits = abs(points[z + 1][1] - points[z][1])\r\n# add together\r\nvalue += (xunits + yunits)\r\nreturn value\r\n\r\n1 + 4 + 2 + 3\r\n... print(execute2_lds([(3, (1, 0), (3, 2)), (7, (2, 1), (0, 0))], (0, 0), 9))\r\n10\r\n\r\n1 + 4 + 2 + 3 + 3 + 3 + 3 + 3\r\n... print(execute2_lds([(3, (1, 0), (3, 2)), (7, (2, 1), (0, 0))], (0, 0), 3))\r\n22\r\n\r\n5 + 5 + 5 + 4 + 1 + 1\r\n... print(execute2_lds([(8, (0, 0), (1, 4)), (2, (1, 4), (1, 0)), (4, (0, 0), (1, 0))], (0, 0), 4))\r\n21", "comments": ["@Pik1229 \r\nCould you please run this in colab and share a gist as the code shared is not indented.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53445\">No</a>\n"]}, {"number": 53444, "title": "for i", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["def execute2_lds(plan, start, capacity):\r\n    # letter count\r\n    value = 0\r\n    points = []\r\n    points.append(start)\r\n    # count the letter\r\n    for i in range(len(plan)):\r\n        exvalue = 0\r\n        if plan[i][0] > capacity:\r\n            exxvalue = abs(plan[i][2][0] - plan[i][1][0])\r\n            exyvalue = abs(plan[i][2][1] - plan[i][1][1])\r\n            # determine capacity\r\n            exunits = (exxvalue +exyvalue) * (plan[i][0] % capacity + 1) * 2\r\n            exvalue += exunits\r\n        value += exvalue\r\n        nplan = plan[i][1:]\r\n        for j in nplan:\r\n            # put all coodinates into a list\r\n            points.append(j)\r\n    for z in range(len(points) - 1):\r\n        xunits = abs(points[z + 1][0] - points[z][0])\r\n        yunits = abs(points[z + 1][1] - points[z][1])\r\n        # add together\r\n        value += (xunits + yunits)\r\n    return value\r\n\r\n>>> # 1 + 4 + 2 + 3\r\n... print(execute2_lds([(3, (1, 0), (3, 2)), (7, (2, 1), (0, 0))], (0, 0), 9))\r\n10\r\n>>> # 1 + 4 + 2 + 3 + 3 + 3 + 3 + 3\r\n... print(execute2_lds([(3, (1, 0), (3, 2)), (7, (2, 1), (0, 0))], (0, 0), 3))\r\n22\r\n>>> # 5 + 5 + 5 + 4 + 1 + 1\r\n... print(execute2_lds([(8, (0, 0), (1, 4)), (2, (1, 4), (1, 0)), (4, (0, 0), (1, 0))], (0, 0), 4))\r\n21", "@Pik1229 ! Closing this issue as it is not related to Tensorflow. Thanks!"]}, {"number": 53443, "title": "Support partial parameter warm-start from pretrained checkpoints in tensorflow v2 (non-estimator mode)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tensorflow 2.x\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWarm-starting from an existing checkpoint is an important feature for all kinds of model training that's well supported under tf.estimator framework. The latter unfortunately seems relegated to second class status in tensorflow 2.x, since it's not eager (at least not natively?). \r\n\r\nThe closest way to achieve warm-start in tf2.x seems to be via tf.train.Checkpoint, a more directly approach than tf.estimator.warm_start_utils, that is, if implemented well. So far however I see that it supports loading all the parameters in a checkpoint altogether, but not loading only some of the parameters. \r\n\r\n**Will this change the current api? How?**\r\nPossibly. Maybe provide a kwarg in Checkpoint initializer called parameter_list.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to reuse a pretrained model but not all of its parameters.\r\n\r\n**Any Other info.**\r\nThere is a lot of emphasis on eagerness, efficiency, in tensorflow v2, but in my opinion not sufficient focus on flexibility and directness of usage so far. Warm-starting is one example.\r\n\r\nUpdate: my colleague found an unofficial implementation of flexible warm-start from a checkpoint in the TF2 version of BERT [here](https://github.com/kpe/bert-for-tf2/blob/master/bert/loader.py#L236). I would recommend adding some official examples along similar lines.", "comments": ["Found a solution to load specific weights of a pretrained model. The solution is described in the following comment:\r\nhttps://github.com/tensorflow/tensorflow/issues/37793#issuecomment-1003677482\r\nMaybe it will help you in some way.", "@yunjiangster Is this resolved or still an issue for you. Also, please check the \"comment\" as mentioned by @ArtyomZemlyak.  \r\n\r\nPlease let us know If there is any actionable item like PR to update or create a tutorial or guide. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Hi, can this issue please be reopened. The solution @ArtyomZemlyak posted only works for TF1 (if you look at his source, it was posted in Posted on 2017-12-13, years before TF2 was released). Can we have a solution for TF2?"]}, {"number": 53442, "title": "Update tensorflow-io-gcs-filesystem to 0.23.1", "body": "This PR updates tensorflow-io-gcs-filesystem to 0.23.1,\r\nsee https://github.com/tensorflow/io/issues/1591\r\n\r\nThis PR fixes https://github.com/tensorflow/io/issues/1591\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 53441, "title": "Tensorflow-metal error when running .fit()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.9.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: Apple Metal 32gb (unified)\r\n\r\nThe error occurs when I attempt to train the model using the GPU. The error does not happen when I specify `with tf.device('/cpu:0'):`, so CPU training appears to be fine, and returns similar results when cross-checked with the results obtained on Google Colab. This appears to be an issue specifically due to tensorflow-metal. My tensorflow-metal version is `0.3.0`. \r\n\r\n**Standalone code to reproduce the issue**\r\nThe example on [3D image classification from CT scans](https://keras.io/examples/vision/3D_image_classification/)\r\nMore specifically, the section that starts model training:\r\n\r\n<pre><code># Compile model.\r\ninitial_learning_rate = 0.0001\r\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\r\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\r\n)\r\nmodel.compile(\r\n    loss=\"binary_crossentropy\",\r\n    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\r\n    metrics=[\"acc\"],\r\n)\r\n\r\n# Define callbacks.\r\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\r\n    \"3d_image_classification.h5\", save_best_only=True\r\n)\r\nearly_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\r\n\r\n# Train the model, doing validation at the end of each epoch\r\nepochs = 100\r\n\r\n\r\nmodel.fit(\r\n    train_dataset,\r\n    validation_data=validation_dataset,\r\n    epochs=epochs,\r\n    shuffle=True,\r\n    verbose=2,\r\n    callbacks=[checkpoint_cb, early_stopping_cb],\r\n)</code></pre>\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n[error.txt](https://github.com/tensorflow/tensorflow/files/7723232/error.txt)\r\n\r\n", "comments": ["@MichaelXCChen ,\r\nI was able to run the code without any issues on TF v2.7. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/7d2e8ac14efb016f0552e904c1a749f7/3d_image_classification.ipynb).Could you please create a virtual environment and test your code again. It helps. Thanks!\r\n\r\n\r\n", "@tilakrayal \r\nJust wondering, did you use a conda environment using `conda create` or a python env `python3 -m venv` (I don't think there is a way to install `tensorflow-deps` in pip, right?)? I first installed Miniforge from [here](https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh), created a conda env, then installed:\r\n`conda install -c apple tensorflow-deps==2.7.0`\r\n`python3 -m pip install tensorflow-macos`\r\n`python3 -m pip install tensorflow-metal`\r\n`conda install jupyter pandas numpy matplotlib scikit-learn`\r\n`conda install -c conda-forge nibabel`\r\n\r\nI have tried to create a fresh conda environment using the above process and the same error occurs. The thing is, the code works fine on Google Colab, but not locally on my M1 Max Macbook. The code also works fine when I force it to run on M1 CPU locally, but the error occurs when I try to use the GPU.", "@MichaelXCChen ,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.As mentioned I was able to run the code without any issues on TF v2.7.Could you please create a virtual environment and test your code again. It helps. Thanks!", "@tilakrayal \r\nThank you for the response. What version of tensorflow-metal are you running the code on? Is it 0.3.0? ", "@MichaelXCChen ,\r\nSince its not a bug from TF end and you also mentioned that the issue is with M1, Can you try posting your question in this tf-metal [developer forum of Apple](https://developer.apple.com/forums/tags/tensorflow-metal)? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53441\">No</a>\n"]}, {"number": 53440, "title": "[bazel] Remove rules_cc dependency", "body": "This public repository is seemingly dead as far as using it as a\r\nreplacement cc toolchain. Because of this, and tensorflow's use of some\r\nfiles under @bazel_tools, this can lead to incompatibilities https://github.com/bazelbuild/bazel/pull/13449#issuecomment-994882040\r\n\r\nSince this repo isn't active, and doesn't provide anything over the\r\ndefault bazel infra that tf needs, we can just drop it.", "comments": ["Interested to see if CI agrees with this", "The `cc_shared_library` is some feature from rules_cc TF's migrating to. Can you upgrade rules_cc to a version that contains [this fix](https://github.com/bazelbuild/rules_cc/pull/105)?", "I've submitted https://github.com/tensorflow/runtime/pull/86 although the PR template said the repo is not accepting contributions!", "We can also add rules_cc here to override the version: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace3.bzl#L7", "The other repo has a `fail()` if it already exists to make sure it can apply a patch ", "@keith Can you please resolve conflicts? Thanks!", "Hi Keith, sorry for the confusion about tensorflow/runtime not accepting contributions. I approved your change and will get it merged. Thanks.", "Thanks!", "Fixed by the runtime update and https://github.com/tensorflow/tensorflow//commit/861059969df2f4992ce76e7c1ca962ed86719a71", "Thank you so much! Now I can enable TensorFlow in Bazel's downstream!"]}, {"number": 53439, "title": "Allign type of TOSA.Const so that both attribute and Result have the \u2026", "body": "Allign type of TOSA.Const so that both attribute and Result have the same type\r\n\r\nbazel test tensorflow/compiler/mlir/tosa/... --test_timeout 240: Passes\r\n", "comments": ["Adding @rsuderman for review.\r\n\r\nApologies recreated the Patch Request to solve merging issue. ", "@rsuderman I believe I have addressed some of the issues on this patch and left some comments. "]}, {"number": 53438, "title": "RNN predict() raised error if shape is not the same as training set for the first time", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 21H2 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): 2.7.0 source\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.9.7\r\n- Bazel version (if compiling from source): 4.2.2\r\n- GCC/Compiler version (if compiling from source): msvc v1916\r\n- CUDA/cuDNN version: 11.5/ 8.2\r\n- GPU model and memory: GTX1060 6GB\r\n- \r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1Baaf74EzBjmuCUyY8MwFOGHAJwK2BUSP?usp=sharing\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\nsample, sample_label = x_train[0], y_train[0]\r\n\r\nmodel = Sequential()\r\n\r\n# Layers\r\nmodel.add(LSTM(128, input_shape=(x_train.shape[1:]), return_sequences=True))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(LSTM(128))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(32, activation='relu'))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(10, activation='softmax'))\r\n\r\n# Optimizer\r\nopt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)\r\n\r\n#Compile\r\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n\r\n#Fit\r\nmodel.fit(x_train, y_train, epochs=1, validation_data=(x_test, y_test))\r\n```\r\n\r\nand the run\r\n\r\n```python\r\nmodel.predict(x_test[0:1, :10, :])\r\n```\r\n\r\nraised error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp/ipykernel_5600/1128729238.py in <module>\r\n      1 ans = []\r\n      2 for i in range(1, 28):\r\n----> 3     ans.append(model.predict(x_test[0:1, :i, :])[0])\r\n\r\n~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in autograph_handler(*args, **kwargs)\r\n   1127           except Exception as e:  # pylint:disable=broad-except\r\n   1128             if hasattr(e, \"ag_error_metadata\"):\r\n-> 1129               raise e.ag_error_metadata.to_exception(e)\r\n   1130             else:\r\n   1131               raise\r\n\r\nValueError: in user code:\r\n\r\n    File \"C:\\Users\\Henry\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\r\n        return step_function(self, iterator)\r\n    File \"C:\\Users\\Henry\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"C:\\Users\\Henry\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\r\n        outputs = model.predict_step(data)\r\n    File \"C:\\Users\\Henry\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\r\n        return self(x, training=False)\r\n    File \"C:\\Users\\Henry\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n        raise e.with_traceback(filtered_tb) from None\r\n    File \"C:\\Users\\Henry\\miniconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\r\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\r\n\r\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 28, 28), found shape=(None, 1, 28)\r\n```\r\n\r\nBut then if you run this first\r\n\r\n```python\r\nmodel.predict(x_test[0:1])\r\n```\r\n\r\nand then run the same code gives expected result\r\n\r\n```python\r\nmodel.predict(x_test[0:1, :10, :])\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n```python\r\nmodel.predict(x_test[0:1, :10, :])\r\n```\r\nshould run fine the first time without running `model.predict(x_test[0:1])` first.\r\n\r\n**Briefly describe your candidate solution(if contributing):**\r\n\r\nSomething (a shape checking) is probably wrong when building the predict function for the model? thats why after the first predict() ran without error, the code will work fine.", "comments": ["Hi @henrysky ! It seems you have raised this in Keras repo too. Could you please this close issue here as it will be tracked down there in Keras repo.\r\n\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53438\">No</a>\n"]}, {"number": 53437, "title": "Integrating cuBLASLt", "body": "Adds support for the cuBLASLt library for GEMM operations on GPUs.\r\n\r\nThe library can be activated by setting the environment variable `TF_USE_CUBLASLT=1`.", "comments": ["@philipphack Can you please resolve conflicts? Thanks!", "@gbaned Done.", "Wow this is great, thanks!", "Thanks again for the review @cheshire. Can you PTAL?", "> The functionality of cuBLASLt can be evaluated by setting the flags and\nrunning general GEMM based tests.\n\nDo we have that flag on by default in some of the tests?\n\nOn Tue, Jan 25, 2022 at 4:07 PM Philipp Hack ***@***.***>\nwrote:\n\n> @philipphack <https://github.com/philipphack> requested your review on:\n> #53437 <https://github.com/tensorflow/tensorflow/pull/53437> Integrating\n> cuBLASLt.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/53437#event-5953568391>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH5CMUKBDOEYOGFPUV3UX43LTANCNFSM5KEMHLOQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because your review was requested.Message ID:\n> ***@***.***>\n>\n", "@philipphack Can you please resolve conflicts? Thanks!", "Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/53437\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "In any case thanks a lot , this work is really appreciated! Sorry for all the nitpicks, I know it's our fault that we don't have the tooling to enforce the invariants we need externally (e.g. no new TF deps)", "Seems a conflict has to be resolved before it can be merged, try to rebase & update.", "Now it seems like GemmRewriteTest is failing?", "also buildifier has found some errors:\r\n\r\n```\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\ntensorflow/compiler/xla/service/gpu/BUILD:\r\n938d937\r\n<         \"//tensorflow/stream_executor:matmul_util\",\r\n939a939\r\n>         \"//tensorflow/stream_executor:matmul_util\",\r\n958d957\r\n<         \"@com_google_absl//absl/types:optional\",\r\n964d962\r\n<         \"//tensorflow/core/protobuf:autotuning_proto_cc\",\r\n966a965\r\n>         \"//tensorflow/core/protobuf:autotuning_proto_cc\",\r\n971d969\r\n<         \"//tensorflow/stream_executor/gpu:redzone_allocator\",\r\n972a971,972\r\n>         \"//tensorflow/stream_executor/gpu:redzone_allocator\",\r\n>         \"@com_google_absl//absl/types:optional\",\r\nexit status 1\r\ntensorflow/stream_executor/BUILD:\r\n9,10c9,10\r\n< load(\"//tensorflow/core/platform/default:cuda_build_defs.bzl\", \"if_cuda_is_configured\",)\r\n< load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\", \"if_cuda_or_rocm\",)\r\n---\r\n> load(\"//tensorflow/core/platform/default:cuda_build_defs.bzl\", \"if_cuda_is_configured\")\r\n> load(\"//tensorflow:tensorflow.bzl\", \"if_cuda_or_rocm\", \"tf_cc_test\")\r\nexit status 1\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```", "@philipphack Can you please address Ubuntu Sanity errors? Thanks!", "@philipphack Can you please resolve conflicts? Thanks!", "I am currently working to resolve internal conflicts", "@gbaned should the status be set to Merged?", "> @gbaned should the status be set to Merged?\r\n\r\nHey @philipphack, thanks for making this change. I made the appropriate changes to get it all building in with our system. It looks like this change is now merged into TF master (https://github.com/tensorflow/tensorflow/commit/e1ba3cdfaac0010d6069d85b2e66cee659089ff7). I'd say this can be closed.", "Hmm, @philipphack, I know this was intended to be disabled by default, though that seems to not be the case. We've seen some out-of-memory tests failing and are rolling-back this change. I suspect that the DoGemm vs DoGemmLt changes in RunGemm might not be respecting the configuration flag. I'm looking into it currently, but would also like your input. Thanks!", "@SandSnip3r can you share additional information on the failing tests? I don't see how DoGemmLt can be reached unless the flag has been set. Having said that, the PR modifies the [matrix solve op test](https://github.com/tensorflow/tensorflow/blob/cc25dbb4634365bc7c15a37cecdc06e374da3e4d/tensorflow/compiler/tests/matrix_solve_op_test.py) to run with cuBLASLt enabled to verify its functionality.\r\n", "@philipphack Can you please resolve conflicts? Thanks!", "> @SandSnip3r can you share additional information on the failing tests? I don't see how DoGemmLt can be reached unless the flag has been set. Having said that, the PR modifies the [matrix solve op test](https://github.com/tensorflow/tensorflow/blob/cc25dbb4634365bc7c15a37cecdc06e374da3e4d/tensorflow/compiler/tests/matrix_solve_op_test.py) to run with cuBLASLt enabled to verify its functionality.\r\n\r\n@philipphack Yesterday I verified a big spike in memory usage (~2.25x, 120MB->270MB) from this change in one of our tests. I am asking around to see what is required to share this test, or one similar. The biggest concern the the moment is that there is any memory usage change at all with this PR, since it should be disabled by default. I am investigating this issue, but would also appreciate your help in doing so.", "@SandSnip3r profiling of several tests, including [//tensorflow/compiler/tests:qr_op_test](https://github.com/tensorflow/tensorflow/pull/53437/files#diff-5ecd56f4cf5650d1fd77bd15f915aa87705bbed4b04c41a2fd5fac170c9f31e6), indicates identical device memory usage of TF master and cc25dbb when the XLA flag is not set.", "@SandSnip3r can you run your test after setting the environment variables `CUBLASLT_LOG_LEVEL=2` and `CUBLASLT_LOG_FILE=LOGFILE.%i`? This should generate logs of all cuBLASLt calls.\r\n\r\nFor the tests in tensorflow/compiler/tests, I see the same output for github/master and cc25dbb."]}, {"number": 53436, "title": "Use actual number of classes for output layer", "body": "The problem is that in the yoga example's training data there are 5 classes but when using any other amount of classes as training data the training fails. Set the output layer to the actual number of classes instead.\r\n\r\nWithout this fix, a ValueError is raised (e.g. `ValueError: Shapes (None, 7) and (None, 5) are incompatible`).", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/53436\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>"]}, {"number": 53435, "title": "Memory leak in saving and loading a keras model containing CategoricalEncoding and Lookup layers", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.9.2009 (Core)\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: not applicable\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: 2.6.0 (v2.6.0-rc2-32-g919f693420e), 2.7.0 (v2.7.0-rc1-69-gc256c071bb2)\r\n-   **Python version**: 3.7.11, 3.8.12, 3.9.6\r\n-   **Bazel version (if compiling from source)**: not applicable\r\n-   **GCC/Compiler version (if compiling from source)**: not applicable\r\n-   **CUDA/cuDNN version**: no GPU available\r\n-   **GPU model and memory**: no GPU available\r\n-   **Exact command to reproduce**: See code below\r\n\r\n### Describe the problem\r\n\r\nTo solve a binary classification problem, I have a keras model that processes categorical input (as as well as numeric input).  \r\nI need to save (`model.save`) and load (`tf.keras.models.load_model`) the model multiple times (performig training of the model inbetween).  \r\n\r\nI expect that the model consumes constant disk space and constant RAM everytime I load the model since the architecture does not change (only the parameter values change).\r\nThis does not happen when the model contains an `IntegerLookup` layer followed by a `CategoryEncoding` layer. \r\n\r\nThe issue can be reproduced without training the model at all.  \r\nHere is a minimal code example that creates a model and saves it to disk:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    input_layer = tf.keras.Input(shape=(1,), dtype=\"int32\") \r\n    index = tf.keras.layers.IntegerLookup(max_values=2)\r\n    index.adapt(np.array(range(2)))\r\n    encoder = tf.keras.layers.CategoryEncoding(max_tokens=index.vocab_size())\r\n    encoded_layer = encoder(index(input_layer))\r\n    output_layer = tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)(encoded_layer)\r\n    model = tf.keras.Model(input_layer, output_layer)\r\n    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\r\n    model.save(\"model\")\r\n    \r\nEvery time I execute\r\n\r\n    model = tf.keras.models.load_model(\"model\")\r\n    model.save(\"model\")\r\n\r\nthe space that the model consumes on disk increases by approx. 8 kB.\r\nThe even worse: When I load the model, the RAM useage increases by approx. 9 MB in each iteration.\r\nSo after 100 iterations, the model needs approx. 1 MB on disk and 950 MB RAM (which *is* problematic).\r\n\r\nThis also happens if I start a new python process in each iteration.\r\n\r\nIn my application, the memory consumption grows even faster because the model has several input layers and also several inner layers.\r\nThis makes the model unusable after some iterations because I cannot load it anymore.\r\n\r\nAdditionaly, of course, the load and save cycles are getting slower with each repetition.\r\n\r\nSo far, I could reproduce this issue on tensorflow versions 2.6 and 2.7 running on python 3.7, 3.8 or 3.9. The behavior is identical.", "comments": ["@AdrianFuchsData \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "Alright, let's discuss the issue here: https://github.com/keras-team/keras/issues/15807", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53435\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53435\">No</a>\n"]}, {"number": 53432, "title": "Tf Keras Dense layer output nan with kernel_initializer='random_normal' before fitting", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nBefore fitting, I used model.predict with my train set, It was ensure that train set was free of nan values. Next, I printed output of each layer.\r\n The output is:\r\n```\r\n0 [[ 1.000000e-03 -1.310000e-01 -9.504200e-02 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n [ 0.000000e+00 -1.000000e-02  1.556190e+00 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n [ 1.000000e+00  5.763000e+00  3.027034e+00 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n ...\r\n [ 8.930000e-01 -5.760000e-01  4.266911e+00 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n [ 1.000000e+00  5.978000e+00  4.650479e+00 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n [ 8.500000e-01  8.910000e-01  7.704330e-01 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]]\r\n1 [[ 1.000000e-03 -1.310000e-01 -9.504200e-02 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n [ 0.000000e+00 -1.000000e-02  1.556190e+00 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n [ 1.000000e+00  5.763000e+00  3.027034e+00 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n ...\r\n [ 8.930000e-01 -5.760000e-01  4.266911e+00 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n [ 1.000000e+00  5.978000e+00  4.650479e+00 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]\r\n [ 8.500000e-01  8.910000e-01  7.704330e-01 ...  0.000000e+00\r\n   0.000000e+00  1.000000e+00]]\r\n2 [[nan nan nan nan]\r\n [nan nan nan nan]\r\n [nan nan nan nan]\r\n ...\r\n [nan nan nan nan]\r\n [nan nan nan nan]\r\n [nan nan nan nan]]\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\ninputs = Input(shape=data.shape)\r\nx = Flatten()(inputs)\r\nx = Dense(data.classnum, kernel_initializer='random_normal')(x)\r\nx = tf.keras.layers.Softmax()(x)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=x)\r\n\r\nfor idx in range(len(model.layers)):\r\n        # print(model.layers[idx].name,model.layers[idx].get_weights())\r\n        intermediate_layer_model = keras.Model(inputs=model.input,\r\n                                    outputs=model.layers[idx].output)\r\n        intermediate_output = intermediate_layer_model.predict(data.train_dataset)\r\n        print(idx,intermediate_output)\r\n```\r\n\r\n", "comments": ["I tried with other kernel_initializer strategies such as Ones() and Zeros() but bug is still here.", "@mainguyenanhvu \r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThanks!", "I posted [here](https://github.com/keras-team/keras/issues/15798).\r\n\r\nThanks.", "@mainguyenanhvu \r\nPlease move this to closed status as it is tracked in keras repo.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53432\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53432\">No</a>\n"]}, {"number": 53431, "title": "Could not load library cudnn_cnn_infer64_8.dll. Error code 126", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version: 2.7.0\r\n- Python version: 3.9.8\r\n- Installed using virtualenv? pip? conda?:  pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):  vs2022\r\n- CUDA/cuDNN version:  cuda11.5/cudnn8.3\r\n- GPU model and memory:  RTX3060TI 8G\r\n\r\n\r\n\r\n**Describe the problem**\r\nI try to run the model but get the such error:\r\n![image](https://user-images.githubusercontent.com/35481439/146149453-0cb4ba2a-bc6c-42d7-8003-684f3fb9d98c.png)\r\n\r\nbut i am sure i have installed the cuda and cudnn successfully. Here is the result of testing cuda.\r\n**\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5\\extras\\demo_suite>deviceQuery.exe\r\ndeviceQuery.exe Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"NVIDIA GeForce RTX 3060 Ti\"\r\n  CUDA Driver Version / Runtime Version          11.5 / 11.5\r\n  CUDA Capability Major/Minor version number:    8.6\r\n  Total amount of global memory:                 8192 MBytes (8589410304 bytes)\r\n  (38) Multiprocessors, (128) CUDA Cores/MP:     4864 CUDA Cores\r\n  GPU Max Clock rate:                            1665 MHz (1.66 GHz)\r\n  Memory Clock rate:                             7001 Mhz\r\n  Memory Bus Width:                              256-bit\r\n  L2 Cache Size:                                 3145728 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               zu bytes\r\n  Total amount of shared memory per block:       zu bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  1536\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          zu bytes\r\n  Texture alignment:                             zu bytes\r\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device supports Compute Preemption:            Yes\r\n  Supports Cooperative Kernel Launch:            Yes\r\n  Supports MultiDevice Co-op Kernel Launch:      No\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.5, CUDA Runtime Version = 11.5, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3060 Ti\r\nResult = PASS\r\n**\r\n\r\nAnd when i search for the version of tensorflow with cuda and cudnn, i found there is not any information for tensorflow-gpu-2.7.0.\r\n![image](https://user-images.githubusercontent.com/35481439/146150973-c7644279-a912-4884-9b68-2e42c93b7524.png)\r\n\r\nAny suggestions? thanks!\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["@Victoria-1 Could you please have a look at the[ build from source](https://www.tensorflow.org/install/source_windows#gpu) and let us know if it helps?Thanks!", "> @Victoria-1 Could you please have a look at the[ build from source](https://www.tensorflow.org/install/source_windows#gpu) and let us know if it helps?Thanks!\r\n\r\nThanks for your replying. I thought I mixed the environment. For I tried to install tensorflow and pytorch the same time, but the newest version of pytorch just supports cuda11.3.  That means I have both cuda11.5 in system and 11.3  in virtual environment. \r\nI uninstalled the all the packages and created a conda environment, reinstalled cuda11.0.2 toolkit and cudnn8.0.5 which support both tensorflow-gpu-2.4.0 and pytorch1.7.1 by conda. And it works now. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53431\">No</a>\n"]}, {"number": 53430, "title": "Segmentation fault and no log information available, add some tips about segmentation fault", "body": "**System information**\r\n- TensorFlow version (you are using): tensorflow 1.12.0, python 3.6.13\r\n- Are you willing to contribute it (Yes/No): Yes. \r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI have trouble retraining my colleague's code from a year ago. I notice that when I try to load my model in the former code base on *libtensorflow-cpu-linux-x86_64-1.12.0* it reports a `Segmentation fault`, without any further information. Upon inspecting my code, I notice that it stops when I load the model, function `TF_SessionRun`. I'm not sure what to do to fix it. A moment later, my colleague realized the problem was the parameters in the placeholder name were incorrect. A moment later, my colleague realized the problem was the parameters in the placeholder name were incorrect. We fixed it after changing the placeholder name.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n**Who will benefit with this feature?**\r\nThose who are not familiar with tensorflow v1's place holder.\r\n**Any Other info.**\r\nNo.", "comments": ["@beiluo97 ,\r\nCan you please share a reproducible code that supports your statement so that the issue can be easily understood? Thanks!", "Also we see that you are using tf version 1.12, 1.x is not actively supported, please update to v2.7 and let us know if you are using same issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53429, "title": "Bump psutil from 5.6.2 to 5.6.6 in /tensorflow/tools/ci_build/release", "body": "Bumps [psutil](https://github.com/giampaolo/psutil) from 5.6.2 to 5.6.6.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/giampaolo/psutil/blob/master/HISTORY.rst\">psutil's changelog</a>.</em></p>\n<blockquote>\n<h1>5.6.6</h1>\n<p>2019-11-25</p>\n<p><strong>Bug fixes</strong></p>\n<ul>\n<li>1179_: [Linux] Process cmdline() now takes into account misbehaving processes\nrenaming the command line and using inappropriate chars to separate args.</li>\n<li>1616_: use of Py_DECREF instead of Py_CLEAR will result in double free and\nsegfault\n(<code>CVE-2019-18874 &lt;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-18874&gt;</code>__).\n(patch by Riccardo Schirone)</li>\n<li>1619_: [OpenBSD] compilation fails due to C syntax error.  (patch by Nathan\nHoughton)</li>\n</ul>\n<h1>5.6.5</h1>\n<p>2019-11-06</p>\n<p><strong>Bug fixes</strong></p>\n<ul>\n<li>1615_: remove pyproject.toml as it was causing installation issues.</li>\n</ul>\n<h1>5.6.4</h1>\n<p>2019-11-04</p>\n<p><strong>Enhancements</strong></p>\n<ul>\n<li>1527_: [Linux] added Process.cpu_times().iowait counter, which is the time\nspent waiting for blocking I/O to complete.</li>\n<li>1565_: add PEP 517/8 build backend and requirements specification for better\npip integration.  (patch by Bern\u00e1t G\u00e1bor)</li>\n</ul>\n<p><strong>Bug fixes</strong></p>\n<ul>\n<li>875_: [Windows] Process' cmdline(), environ() or cwd() may occasionally fail\nwith ERROR_PARTIAL_COPY which now gets translated to AccessDenied.</li>\n<li>1126_: [Linux] cpu_affinity() segfaults on CentOS 5 / manylinux.\ncpu_affinity() support for CentOS 5 was removed.</li>\n<li>1528_: [AIX] compilation error on AIX 7.2 due to 32 vs 64 bit differences.\n(patch by Arnon Yaari)</li>\n<li>1535_: 'type' and 'family' fields returned by net_connections() are not\nalways turned into enums.</li>\n<li>1536_: [NetBSD] process cmdline() erroneously raise ZombieProcess error if\ncmdline has non encodable chars.</li>\n<li>1546_: usage percent may be rounded to 0 on Python 2.</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/c6cd256da95ffe9599792759b1c2586ba24fa047\"><code>c6cd256</code></a> pre release</li>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/b2414b83d3d728ec34ea0e35bfb21517ee231401\"><code>b2414b8</code></a> revert <a href=\"https://github-redirect.dependabot.com/giampaolo/psutil/issues/1595\">#1595</a></li>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/c63369e999b458ecbd559bdde895c344b4db2841\"><code>c63369e</code></a> updat HISTORY</li>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/edb20f664f28653dcdd24f0bf0191984738dca6e\"><code>edb20f6</code></a> linux, cmdline(), fix for <a href=\"https://github-redirect.dependabot.com/giampaolo/psutil/issues/1179\">#1179</a>, comment 552984549: sometimes string ends wit...</li>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/d739cbb1a5b207212d467b219dfc25b017911530\"><code>d739cbb</code></a> use PROCESS_QUERY_LIMITED_INFORMATION</li>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/f7e898b0987f97352c7551bdd9b29b594e1236f6\"><code>f7e898b</code></a> <a href=\"https://github-redirect.dependabot.com/giampaolo/psutil/issues/1595\">#1595</a>: use psutil_pid_is_running() instead of GetExitCodeProcess</li>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/72c84cb4edb5c0968a83c1f45ad5cc51235e0af3\"><code>72c84cb</code></a> #fix <a href=\"https://github-redirect.dependabot.com/giampaolo/psutil/issues/1595\">#1595</a> / windows: kill() may not raise AccessDenied</li>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/1f8d432db12a907544ac533b66a5a61ba25321fb\"><code>1f8d432</code></a> Merge branch 'master' of github.com:giampaolo/psutil</li>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/e6faebcd7adaa327d1ce57385cbebe7724d02350\"><code>e6faebc</code></a> release gil around users()/BSD (<a href=\"https://github-redirect.dependabot.com/giampaolo/psutil/issues/1425\">#1425</a>)</li>\n<li><a href=\"https://github.com/giampaolo/psutil/commit/5cb1b0b526765720253fdb2e8eff0bf380bbe0a8\"><code>5cb1b0b</code></a> Merge branch 'master' of github.com:giampaolo/psutil</li>\n<li>Additional commits viewable in <a href=\"https://github.com/giampaolo/psutil/compare/release-5.6.2...release-5.6.6\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=psutil&package-manager=pip&previous-version=5.6.2&new-version=5.6.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/tensorflow/tensorflow/network/alerts).\n\n</details>", "comments": ["Looks like psutil is no longer a dependency, so this is no longer needed."]}, {"number": 53428, "title": "Addressed case where quantized tfl.fully_connected output shape mismatches", "body": "Need to ensure that the output shape and fully_connected shape are not\r\nrequired to have the same case. Updated so when the RescaleOp is created\r\nit is allowed to infer its own return shape.", "comments": []}, {"number": 53426, "title": "M1 Pro After installing TF based on Official apple guide gives ERROR for even importing TF", "body": "SYSTEM:\r\n- MacBook Pro 14 (M1 Apple Silicon)\r\n- MacOS 12.0.1\r\n\r\nDONE:\r\n- https://developer.apple.com/metal/tensorflow-plugin/\r\n- I have followed this for ARM M1 apple silicon for my 14\"\r\n- (I had anaconda installed before, that may causing the error but I DO NOT want to delete my anaconda all together)\r\n\r\nCODE\r\n```\r\nimport tensorflow as tf\r\n```\r\n\r\nERROR\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~/miniforge3/lib/python3.9/site-packages/numpy/core/__init__.py in <module>\r\n     21 try:\r\n---> 22     from . import multiarray\r\n     23 except ImportError as exc:\r\n\r\n~/miniforge3/lib/python3.9/site-packages/numpy/core/multiarray.py in <module>\r\n     11 \r\n---> 12 from . import overrides\r\n     13 from . import _multiarray_umath\r\n\r\n~/miniforge3/lib/python3.9/site-packages/numpy/core/overrides.py in <module>\r\n      6 \r\n----> 7 from numpy.core._multiarray_umath import (\r\n      8     add_docstring, implement_array_function, _get_implementing_args)\r\n\r\nImportError: dlopen(/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): Library not loaded: @rpath/libcblas.3.dylib\r\n  Referenced from: /Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so\r\n  Reason: tried: '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/bin/../lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/bin/../lib/libcblas.3.dylib' (no such file), '/usr/local/lib/libcblas.3.dylib' (no such file), '/usr/lib/libcblas.3.dylib' (no such file)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n/var/folders/yp/mq9ddgh54gjg2rp7mw_t015c0000gn/T/ipykernel_95218/3793406994.py in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/__init__.py in <module>\r\n     39 \r\n     40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n---> 41 from tensorflow.python.eager import context\r\n     42 \r\n     43 # pylint: enable=wildcard-import\r\n\r\n~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/context.py in <module>\r\n     28 \r\n     29 from absl import logging\r\n---> 30 import numpy as np\r\n     31 import six\r\n     32 \r\n\r\n~/miniforge3/lib/python3.9/site-packages/numpy/__init__.py in <module>\r\n    138     from . import _distributor_init\r\n    139 \r\n--> 140     from . import core\r\n    141     from .core import *\r\n    142     from . import compat\r\n\r\n~/miniforge3/lib/python3.9/site-packages/numpy/core/__init__.py in <module>\r\n     46 \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\r\n     47         __version__, exc)\r\n---> 48     raise ImportError(msg)\r\n     49 finally:\r\n     50     for envkey in env_added:\r\n\r\nImportError: \r\n\r\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\r\n\r\nImporting the numpy C-extensions failed. This error can happen for\r\nmany reasons, often due to issues with your setup or how NumPy was\r\ninstalled.\r\n\r\nWe have compiled some common reasons and troubleshooting tips at:\r\n\r\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\r\n\r\nPlease note and check the following:\r\n\r\n  * The Python version is: Python3.9 from \"/Users/ps/miniforge3/bin/python\"\r\n  * The NumPy version is: \"1.19.5\"\r\n\r\nand make sure that they are the versions you expect.\r\nPlease carefully study the documentation linked above for further help.\r\n\r\nOriginal error was: dlopen(/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): Library not loaded: @rpath/libcblas.3.dylib\r\n  Referenced from: /Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so\r\n  Reason: tried: '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/bin/../lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/bin/../lib/libcblas.3.dylib' (no such file), '/usr/local/lib/libcblas.3.dylib' (no such file), '/usr/lib/libcblas.3.dylib' (no such file)\r\n```", "comments": ["@stromal Could you please let us know the TF version you are using ?Thanks!", "Problem resolved by \r\n- not installing TF to the base environment \r\n- uninstall numpy\r\n- reinstall numpy", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53426\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53426\">No</a>\n", "@sushreebarsa I have the same issue. I tried installing both TF 2.7.0 and 2.6.0. Neither worked. The resolution suggested by @stromal did not work for me. After following the suggestion, I get the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/dhruva/general_code/gpu_env/trial_gpu.py\", line 9, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/dhruva/general_code/gpu_env/env/lib/python3.9/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/Users/dhruva/general_code/gpu_env/env/lib/python3.9/site-packages/tensorflow/python/__init__.py\", line 28, in <module>\r\n    import ctypes\r\n  File \"/Users/dhruva/general_code/gpu_env/env/lib/python3.9/ctypes/__init__.py\", line 8, in <module>\r\n    from _ctypes import Union, Structure, Array\r\nImportError: dlopen(/Users/dhruva/general_code/gpu_env/env/lib/python3.9/lib-dynload/_ctypes.cpython-39-darwin.so, 0x0002): Library not loaded: @rpath/libffi.7.dylib\r\n  Referenced from: /Users/dhruva/general_code/gpu_env/env/lib/python3.9/lib-dynload/_ctypes.cpython-39-darwin.so\r\n  Reason: tried: '/Users/dhruva/general_code/gpu_env/env/lib/libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/Users/dhruva/general_code/gpu_env/env/lib/libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/Users/dhruva/general_code/gpu_env/env/lib/python3.9/lib-dynload/../../libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/Users/dhruva/general_code/gpu_env/env/lib/libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/Users/dhruva/general_code/gpu_env/env/lib/libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/Users/dhruva/general_code/gpu_env/env/lib/python3.9/lib-dynload/../../libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/Users/dhruva/general_code/gpu_env/env/lib/libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/Users/dhruva/general_code/gpu_env/env/bin/../lib/libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/Users/dhruva/general_code/gpu_env/env/lib/libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/Users/dhruva/general_code/gpu_env/env/bin/../lib/libffi.7.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/usr/local/lib/libffi.7.dylib' (no such file), '/usr/lib/libffi.7.dylib' (no such file)\r\n\r\n\r\n", "@DhruvaKartik & @sushreebarsa \r\n\r\nTF 2.7.\r\n\r\nProbably the problem is the following:\r\n- **### Anaconda cannot run on M1, Miniforge is used to replace it.** - https://makeoptim.com/en/deep-learning/tensorflow-metal\r\n- Do not install it in to the base directory \r\n- only installs libraries to it that are compatible with it\r\n\r\nOther Helpful links:\r\n\r\n- https://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html\r\n- https://discuss.tensorflow.org/t/accelerating-tensorflow-using-apple-m1-max/5282/16\r\n- https://makeoptim.com/en/deep-learning/tensorflow-metal\r\n- https://github.com/pytorch/pytorch/issues/47702#issuecomment-965625139\r\n- https://betterprogramming.pub/installing-tensorflow-on-apple-m1-with-new-metal-plugin-6d3cb9cb00ca\r\n- https://www.youtube.com/watch?v=_CO-ND1FTOU\r\n- \u00a0https://stackoverflow.com/questions/67352841/tensorflow-is-not-using-my-m1-macbook-gpu-during-training\r\n- GUIDE - https://makeoptim.com/en/deep-learning/tensorflow-metal\r\n- ERROR SOS - tf apple silicon install\r\n\r\n", "@stromal Thanks for the additional resources. I have been using Anaconda with Rosetta. That might be creating the issue. \r\n\r\nRemoving Anaconda worked! Thanks!"]}, {"number": 53425, "title": "Hide JPEG symbols on Darwin (to align with Linux)", "body": "//tensorflow:tensorflow_framework specifies a version script so that the internal version of libjpeg isn't leaked, however this is not reflected in the darwin settings.  Align them.", "comments": []}, {"number": 53423, "title": "Hexagon delegate fails on a known good model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.7\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running inference with TFlite and Hexagon delegate,\r\ninitialization fails with the following message:\r\n\r\n09-19 08:24:18.566  4958  4958 I tflite  : Created TensorFlow Lite XNNPACK delegate for CPU.\r\n09-19 08:24:18.568  4958  4958 E tflite  : Calling prepare multiple times\r\n09-19 08:24:18.568  4958  4958 E tflite  : Node number 90 (TfLiteHexagonDelegate) failed to prepare.\r\n09-19 08:24:18.775  4958  4958 E tflite  : Restored original execution plan after delegate application failure.\r\n09-19 08:24:18.782  4958  4958 E tflite  : Error in applying the default TensorFlow Lite delegate indexed at 0, and all previously applied delegates are reverted.\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen using the same model with the same code compiled against TensorFlow 2.3, it works fine.\r\nSee attached the model in question.\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/7713641/model.zip)", "comments": ["Hi @sachinprasadhs ! Could you please look at this issue?", "Hi, are there any updates on this issue?", "That looks like related to the new effort to enable Xnnpack by default.\r\n\r\n@multiverse-tf Can you check it please.\r\n\r\nEDIT: Verified on device that disabling  Xnnpack resolves the issue.", "> That looks like related to the new effort to enable Xnnpack by default.\r\n> \r\n> @multiverse-tf Can you check it please.\r\n> \r\n> EDIT: Verified on device that disabling Xnnpack resolves the issue.\r\n\r\nI could also verify that disabling XNNPACK resolved the issue. There are two ways to disable the XNNPACK-by-default feature, and you could choose either of them to achieve:\r\n1. Build the Tensorflow Lite library to disable this feature. If using Bazel, you could add \"--define=tflite_with_xnnpack=false\" flag.\r\n2. If you could change your code, use [BuiltinOpResolverWithoutDefaultDelegates](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.h#L39) instead of BuiltinOpResolver when creating the TFLite interpreter.\r\n\r\nMeanwhile, we will investigate the root cause and push a fix asap. Sorry for the inconvenience.\r\n", "@yakovdan A [fix](https://github.com/tensorflow/tensorflow/commit/c8bafb818a10928b366ad9328ce6091596f0d4f2) has been pushed. Could you try your code again?", "Fix was merged. Closing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53423\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53423\">No</a>\n"]}, {"number": 53422, "title": "Allign type of TOSA.Const so that both attribute and Result have the \u2026", "body": "Allign type of TOSA.Const so that both attribute and Result have the same type\r\n\r\nbazel test tensorflow/compiler/mlir/tosa/... --test_timeout 240: Passes\r\n\r\nChange-Id: I7e83117e8c2a32416160a9f67b95d54caf4f4791\r\nSigned-off-by: Aaron DeBattista <aaron.debattista@arm.com>", "comments": ["Adding @rsuderman for review", "@aardeb  Can you please resolve conflicts? Thanks!"]}, {"number": 53421, "title": "Add a Legalization Enable Option to the TFLite to TOSA pass in order \u2026", "body": "Add a Legalization Enable Option to the TFLite to TOSA pass in order to add rewrites dynamically\r\n\r\nChange-Id: I24c29e1e3488a6ebf2a0433a749bd487918e6e3b\r\nSigned-off-by: Aaron DeBattista <aaron.debattista@arm.com>", "comments": ["Adding @rsuderman for review", "Can you expand the description on the general motivation and context for this feature?", "@joker-eph in reply to the rational behind this change: In our compilation flow we consume a mlir module with tflite operations. We then legalize the tflite model to tosa before starting compilation. In our compilation flow any ops we do not want to execute on a particular piece of IP are left as tflite operations. The final compiled stream is a tflite model of interleaved tflite ops and tflite custom ops (containing npu command streams) which is then executed by the uTFLite runtime. Hence in order to achieve this functionality we need the ability to dynamically choose which ops not to lower to TOSA and to leave as tflite. This commit allows this functionality. If the legalization_disable is not defined in the pass add call all ops are lowered to TOSA. If an Op is defined in legalization_disable, that particular op is not lowered. ", "@rsuderman & @joker-eph could you please help in getting this submitted ? There is one failing check, but I am not able to get visibility (or at least not sure how) of what is failing. ", "What I see right now isn't a visibility issue but a missing dependency:\r\n```\r\nthird_party/tensorflow/compiler/mlir/tosa/transforms/legalize_tfl.cc:28:10: error: module //third_party/tensorflow/compiler/mlir/tosa:tfl_passes does not depend on a module exporting 'mlir/Dialect/Traits.h'\r\n```\r\n\r\nYou need to add a dependency to \"@llvm-project//mlir:Dialect\" here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/BUILD#L184\r\n"]}, {"number": 53420, "title": "Update tensorflow-io-gcs-filesystem version and add python3.10 support", "body": "With the release of 0.23.0 tensorflow-io-gcs-filesystem, it is possible\r\nto update and remove the limitation of python3.10.\r\n\r\n/cc @mihaimaruseac \r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 53418, "title": "Use the same variable", "body": null, "comments": []}]