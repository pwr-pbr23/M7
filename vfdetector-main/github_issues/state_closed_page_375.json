[{"number": 42760, "title": "PReLU fails when alpha_initializer='Identity'", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):TF2-GPU\r\n- Python version:3.6\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I tried to define a PReLU layer and set alpha_initializer='Identity', the program crashed.\r\n**Describe the expected behavior**\r\nwhen I use MNIST it works. I expect that CIFAR-10 and CIFAR-100 should got the same performance as MNIST.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nbatch_size = 23\r\nepochs = 55\r\nnum_classes = 10\r\nimport os\r\nsave_dir = 'model'\r\nmodel_name = 'test98_trained_model.h5'\r\nimport tensorflow.keras as keras\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\nimg_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.PReLU(alpha_initializer='Identity'))\r\n\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(num_classes))\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\nprint('Test accuracy:', score[1])\r\nmodel_path = os.path.join(save_dir, model_name)\r\nmodel.save(model_path)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nValueError: Identity matrix initializer can only be used for 2D matrices.\r\n\r\n```\r\n", "comments": ["\r\nI have tried in colab with TF version 2.3, nightly versions(`2.4.0-dev20200831`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3f47833a5404490d6bc42684e977e787/untitled.ipynb#scrollTo=q-ty_DthCzDF). Thanks!", "It seems like in that example the shape of `alpha` is `[32, 32, 3]` (i.e. 3D).  As stated in the [documentation for `tf.keras.initializers.Identity`](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Identity), \r\n\r\n> Only usable for generating 2D matrices.\r\n\r\nWhat is the bug being reported?", "I think this error is intended. As @EPronovost noted above, the alpha weight is created with shape [32, 32, 3] and the identity weight initializer is only applicable for 2D weights. Please change to use other initializer or constant initializer with the value you want.\r\n\r\nClosing this bug since its working as intended.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42760\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42760\">No</a>\n"]}, {"number": 42759, "title": "libtensorflowlite XNNPACK support beyond f32", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information** march=armhf mfpu=neon\r\n- TensorFlow version (you are using): v2.3.0\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn both `master` and `v2.3.0` where xnnpack config is available, it seems that only f32 ops are supported\r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/BUILD#L35\r\ncc_library(\r\n    name = \"xnnpack_delegate\",\r\n    ...\r\n    deps = [\r\n        ...\r\n        \"@XNNPACK//:xnnpack_f32\",\r\n    ],\r\n```\r\n\r\nCan this be expanded to support quantized models as well? For example, xnnpack already supports `QS8MobileNetV1` \r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @multiverse-tf , can you take a look or triage?", "Hi @honglh ! I think the range has been expanded for [XNNPACK builds](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/BUILD#L118) now in 2.8 version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42758, "title": "Conv2DTranspose failed when kernel_initializer='Identity' with CIFAR-100", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):tensorflow2-GPU\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I tried to define a Conv2DTranspose layer and set kernel_initializer='Identity', the program crashed.\r\n**Describe the expected behavior**\r\nwhen I use MNIST it works. I expect that CIFAR-10 and CIFAR-100 should got the same performance as MNIST.\r\n\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nbatch_size = 28\r\nepochs = 77\r\nnum_classes = 100\r\nimport os\r\nsave_dir = 'model'\r\nmodel_name = 'test60_trained_model.h5'\r\nimport tensorflow.keras as keras\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\nimg_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Conv2DTranspose(filters=11,kernel_size=(19, 19), strides=(19, 19), padding='valid',activation='elu',kernel_initializer='Identity'))\r\n\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(num_classes))\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint('Test accuracy:', score[1])\r\nmodel_path = os.path.join(save_dir, model_name)\r\nmodel.save(model_path)\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nValueError: Identity matrix initializer can only be used for 2D matrices.\r\n```\r\n\r\n\r\n", "comments": ["Hi @bugreporter450 ,\r\nI have come up with a solution and found that it works. I am sharing the changes I have made.\r\n\r\n#Your Program\r\nIt is better to give the input_shape argument in the Conv2DTranspose layer when it is used as the first layer of the Model. I found it in the Keras Conv2DTranspose Layer documentation and hence have added it in the code below.\r\n```\r\nbatch_size = 28\r\nepochs = 77\r\nnum_classes = 100\r\nimport os\r\nsave_dir = 'model'\r\nmodel_name = 'test60_trained_model.h5'\r\nimport tensorflow.keras as keras\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\nimg_rows, img_cols, img_channels = x_train.shape[1], x_train.shape[2], x_train.shape[3]\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Conv2DTranspose(input_shape = ( img_rows, img_cols, img_channels ), filters = 11 ,kernel_size= 19 , strides= 19 , padding='valid',activation='relu', kernel_initializer = 'Identity'))\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(num_classes))\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint('Test accuracy:', score[1])\r\n\r\n```\r\n\r\n1) In tensorflow module's init_ops_v2.py Program:\r\n```\r\n class Identity(Initializer):\r\n  \"\"\"Initializer that generates the identity matrix.\r\n\r\n  Initializers allow you to pre-specify an initialization strategy, encoded in\r\n  the Initializer object, without knowing the shape and dtype of the variable\r\n  being initialized.\r\n\r\n  Only usable for generating 2D matrices.\r\n\r\n  Examples:\r\n\r\n  >>> def make_variable(k, initializer):\r\n  ...   return tf.Variable(initializer(shape=[k, k], dtype=tf.float32))\r\n  >>> make_variable(2, tf.initializers.Identity())\r\n  <tf.Variable ... shape=(2, 2) dtype=float32, numpy=\r\n  array([[1., 0.],\r\n         [0., 1.]], dtype=float32)>\r\n  >>> make_variable(3, tf.initializers.Identity(gain=0.5))\r\n  <tf.Variable ... shape=(3, 3) dtype=float32, numpy=\r\n  array([[0.5, 0. , 0. ],\r\n         [0. , 0.5, 0. ],\r\n         [0. , 0. , 0.5]], dtype=float32)>\r\n\r\n  Args:\r\n    gain: Multiplicative factor to apply to the identity matrix.\r\n  \"\"\"\r\n\r\n  def __init__(self, gain=1.0):\r\n    self.gain = gain\r\n\r\n  def __call__(self, shape, dtype = dtypes.float32):\r\n    \"\"\"Returns a tensor object initialized as specified by the initializer.\r\n\r\n    Args:\r\n      shape: Shape of the tensor.\r\n      dtype: Optional dtype of the tensor. Only floating point types are\r\n       supported.\r\n\r\n    Raises:\r\n      ValueError: If the dtype is not floating point\r\n      ValueError: If the requested shape does not have exactly two axes.\r\n    \"\"\"\r\n    partition_info = None  # Keeps logic so can be readded later if necessary\r\n    dtype = _assert_float_dtype(dtype)\r\n    full_shape = shape if partition_info is None else partition_info.full_shape\r\n\r\n    if len(full_shape) < 2:\r\n      raise ValueError(\r\n          \"Identity matrix initializer can only be used for 2D matrices.\")\r\n          \r\n    initializer = linalg_ops_impl.eye(full_shape[0],full_shape[1], batch_shape = [full_shape[3],full_shape[2]], dtype = dtypes.float32)\r\n    if partition_info is not None:\r\n      initializer = array_ops.slice(initializer, partition_info.var_offset,\r\n                                    shape)\r\n    return self.gain * initializer\r\n\r\n  def get_config(self):\r\n    return {\"gain\": self.gain}\r\n```\r\n2) The eye() function in linalg_ops_impl.py:\r\n\r\n```\r\n def eye(num_rows,\r\n        num_columns=None,\r\n        batch_shape=None,\r\n        dtype=dtypes.float32,\r\n        name=None):\r\n  \"\"\"Construct an identity matrix, or a batch of matrices.\r\n\r\n  See `linalg_ops.eye`.\r\n  \"\"\"\r\n  with ops.name_scope(\r\n      name, default_name='eye', values=[num_rows, num_columns, batch_shape]):\r\n    is_square = num_columns is None\r\n    batch_shape = [] if batch_shape is None else batch_shape\r\n    num_columns = num_rows if num_columns is None else num_columns\r\n     \r\n    # We cannot statically infer what the diagonal size should be:\r\n    if (isinstance(num_rows, ops.Tensor) or\r\n        isinstance(num_columns, ops.Tensor)):\r\n      diag_size = math_ops.minimum(num_rows, num_columns)\r\n    else:\r\n      # We can statically infer the diagonal size, and whether it is square.\r\n      if not isinstance(num_rows, compat.integral_types) or not isinstance(\r\n          num_columns, compat.integral_types):\r\n        raise TypeError(\r\n            'num_rows and num_columns must be positive integer values.')\r\n      is_square = num_rows == num_columns\r\n      diag_size = np.minimum(num_rows, num_columns)\r\n\r\n    # We can not statically infer the shape of the tensor.\r\n    if isinstance(batch_shape, ops.Tensor) or isinstance(diag_size, ops.Tensor):\r\n      batch_shape = ops.convert_to_tensor(\r\n          batch_shape, name='shape', dtype=dtypes.int32)\r\n      diag_shape = array_ops.concat((batch_shape, [diag_size]), axis=0)\r\n      \r\n      if not is_square:\r\n        shape = array_ops.concat((batch_shape, [num_rows, num_columns]), axis=0)\r\n    # We can statically infer everything.\r\n    else:\r\n      \r\n      diag_shape = batch_shape + [diag_size]\r\n\r\n      if not is_square:\r\n        shape = batch_shape + [num_rows, num_columns]\r\n        \r\n    diag_ones = array_ops.ones(diag_shape, dtype=dtype)\r\n    if is_square:\r\n      return np.array(array_ops.matrix_diag(diag_ones)).reshape(diag_size,diag_size,batch_shape[1],batch_shape[0])\r\n    else:\r\n      zero_matrix = array_ops.zeros(shape, dtype=dtype)\r\n      return np.array(array_ops.matrix_set_diag(zero_matrix, diag_ones)).reshape(num_rows,num_columns,batch_shape[1],batch_shape[0])\r\n\r\n# pylint: enable=invalid-name,redefined-builtin\r\n```\r\nKindly let me know if it works for you. Hope it helps!  ", "@bugreporter450 \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42758\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42758\">No</a>\n"]}, {"number": 42756, "title": "STM32F407G Build Error - AddBuiltin() mismatch in arguments", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Ubuntu 18.04.5 LTS\r\n- TensorFlow installed from : source\r\n- Tensorflow version : 2.3.0\r\n- Target platform : STM32F407G - Discovery Board\r\n- Tool chain: STM32 Cube IDE Tool chain\r\n\r\n**Problem**\r\n\r\nMismatch in number of arguments between the function definition and the calling function in `AddBuiltin()` function.\r\n\r\nFunction Definition: `tensorflow/lite/micro/micro_mutable_op_resolver.h` (Line 473)\r\n\r\n**Code Snippet:  (Number of arguments - 3)**\r\n`TfLiteStatus AddBuiltin(tflite::BuiltinOperator op, const TfLiteRegistration& registration,                           MicroOpResolver::BuiltinParseFunction parser)`\r\n\r\nCalling Function: main.cpp (generated using STM Toolchain)\r\n\r\nCode Snippet: (Number of arguments - 2)\r\n`tflite_status = micro_op_resolver.AddBuiltin( tflite::BuiltinOperator_FULLY_CONNECTED, tflite::ops::micro::Register_FULLY_CONNECTED());`\r\n\r\n**Sequence of Steps Followed:**\r\n\r\nWe built all the examples for various compilers using this command.  \r\n\r\n`$make -f tensorflow/lite/micro/tools/make/Makefile TAGS=\"portable_optimized\" generate_non_kernel_projects`\r\n\r\nhello_world project is created using STM32 Cube IDE.\r\n\r\nThe tensorflow and third party folders generated inside hello_world project workspace are copied. \r\n\r\n`main.cpp` is created for hello_world\r\n\r\nThe project is built in the STM32 Cube IDE. During the build process, we have the following error.\r\n`\r\n'g++' -std=c++11 -DTF_LITE_STATIC_MEMORY -DNDEBUG -O3 -DTF_LITE_DISABLE_X86_NEON -I. -I./third_party/gemmlowp -I./third_party/flatbuffers/include -I./third_party/ruy  -c tensorflow/lite/micro/examples/hello_world/main_functions.cc -o tensorflow/lite/micro/examples/hello_world/main_functions.o\r\ntensorflow/lite/micro/examples/hello_world/main_functions.cc: In function \u2018void setup()\u2019:\r\ntensorflow/lite/micro/examples/hello_world/main_functions.cc:81:53: error: no matching function for call to \u2018tflite::MicroMutableOpResolver<1>::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration*)\u2019\r\n   \ttflite::ops::micro::Register_FULLY_CONNECTED());\r\n                                                 \t^\r\nIn file included from ./tensorflow/lite/micro/all_ops_resolver.h:16:0,\r\n             \tfrom tensorflow/lite/micro/examples/hello_world/main_functions.cc:18:\r\n./tensorflow/lite/micro/micro_mutable_op_resolver.h:473:16: note: candidate: TfLiteStatus tflite::MicroMutableOpResolver<tOpCount>::AddBuiltin(tflite::BuiltinOperator, const TfLiteRegistration&, tflite::MicroOpResolver::BuiltinParseFunction) [with unsigned int tOpCount = 1; TfLiteStatus = TfLiteStatus; TfLiteRegistration = TfLiteRegistration; tflite::MicroOpResolver::BuiltinParseFunction = TfLiteStatus (*)(const tflite::Operator*, tflite::BuiltinOperator, tflite::ErrorReporter*, tflite::BuiltinDataAllocator*, void**)]\r\n   TfLiteStatus AddBuiltin(tflite::BuiltinOperator op,\r\n            \t^~~~~~~~~~\r\n./tensorflow/lite/micro/micro_mutable_op_resolver.h:473:16: note:   candidate expects 3 arguments, 2 provided\r\nMakefile:36: recipe for target 'tensorflow/lite/micro/examples/hello_world/main_functions.o' failed\r\n`\r\n \r\n\r\nPlease suggest the code changes\r\n\r\nThe above steps are executed as suggested in [youtube](https://www.youtube.com/watch?v=gDFWCxrJruQ) \r\n\r\n\r\n", "comments": ["Hi all, \r\nI solved it by modifying the following code portion of main.cpp:\r\n\r\nstatic tflite::MicroMutableOpResolver<1> micro_op_resolver;\r\n// Add dense neural network layer operation\r\n  tflite_status = micro_op_resolver.AddBuiltin(\r\n  tflite::BuiltinOperator_FULLY_CONNECTED,\r\n  tflite::ops::micro::Register_FULLY_CONNECTED());\r\n\r\nin this way:\r\nstatic tflite::MicroMutableOpResolver<1> micro_op_resolver;\r\ntflite_status = micro_op_resolver.AddFullyConnected();\r\n\r\n\r\nThe fact is that AddFullyConnected() function (tensorflow/lite/micro/micro_mutable_op_resolver.h) is defined as follows:\r\n\r\nTfLiteStatus AddFullyConnected() {\r\n    return AddBuiltin(BuiltinOperator_FULLY_CONNECTED,\r\n                               *tflite::ops::micro::Register_FULLY_CONNECTED(),\r\n                                ParseFullyConnected);\r\n  }\r\n\r\nwhich avoids the mismatch above.\r\n\r\nHope this can be helpful.", "@Pradeepiit Could you please let us know if this issue resolved or not? please check https://github.com/tensorflow/tensorflow/issues/42756#issuecomment-770443361", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42756\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42756\">No</a>\n"]}, {"number": 42755, "title": "Unable to find the tf.raw_ops.mfcc python code in tensorflow", "body": "Hello,\r\n\r\nI would like to see the procedure of calculating the mfcc and filterbank values in tensorflow package using tf.raw_ops.mfcc, but I could not find it.\r\nThe closest code that I found was:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/signal/mfcc_ops.py \r\n\r\nIs there anywhere I can find the exact  tf.raw_ops.mfcc function code?\r\n\r\n", "comments": ["I haven't received any answer yet! Is there any Github repo for tf.raw_ops package?", "@marjanemd,\r\nSorry for the delayed response. [This is the path of the source code for tf.raw_ops.Mfcc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_Mfcc.pbtxt). Hope this helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42754, "title": "Make effective scale in quantize kernel consistent b/w Lite and Micro.", "body": "There is no reason for the Micro implementation to do a division in single precision.\r\n\r\nFixes #42648", "comments": []}, {"number": 42753, "title": "for testing", "body": "for testing...", "comments": []}, {"number": 42752, "title": "Rama/legal notice", "body": "LEGAL-NOTICE addition.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42752) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 42751, "title": "Failed to load the native TensorFlow runtime.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 Home 1909**\r\n- TensorFlow installed from (source or binary): **Tensorflow.org**\r\n- TensorFlow version: **1.13.2 gpu**\r\n- Python version: **3.75 (64-bit)**\r\n- Installed using virtualenv? pip? conda?: **pip**\r\n- CUDA/cuDNN version: **10.0**\r\n- GPU model and memory: **ZOTAC GeForce GT 1030 2GB GDDR5**\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nSo I was trying to run \"CorentinJ's Real Time Voice Cloning project\" using a guide for windows 10 (https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/) and I followed the guide exactly as it said to.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\scripts\\real-time-voice-cloning-master> .\\pip install tensorflow-gpu==1.13.2`\r\n\r\n**I restarted powershell after installation finished**\r\n\r\n`C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\scripts\\real-time-voice-cloning-master> .\\python.exe demo_toolbox.py`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"demo_toolbox.py\", line 2, in <module>\r\n    from toolbox import Toolbox\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\scripts\\real-time-voice-cloning-master\\toolbox\\__init__.py\", line 3, in <module>\r\n    from synthesizer.inference import Synthesizer\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\scripts\\real-time-voice-cloning-master\\synthesizer\\inference.py\", line 1, in <module>\r\n    from synthesizer.tacotron2 import Tacotron2\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\scripts\\real-time-voice-cloning-master\\synthesizer\\tacotron2.py\", line 3, in <module>\r\n    from synthesizer.models import create_model\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\scripts\\real-time-voice-cloning-master\\synthesizer\\models\\__init__.py\", line 1, in <module>\r\n    from .tacotron import Tacotron\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\scripts\\real-time-voice-cloning-master\\synthesizer\\models\\tacotron.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Dolphin EMU\\appdata\\local\\programs\\python\\python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "Please assign me this issue", "@Toastmanv3 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42751\">No</a>\n"]}, {"number": 42750, "title": "Error on doing regression with weighted time steps using sample_weight_mode='temporal'", "body": "I am running into problems while doing regression when the output of the network contains multiple values which can be considered as output for a series of time-points. The loss of each output timestep needs to be weighted differently for each sample.\r\n\r\nI have built a simple model with a dummy dataset. Here is the [Colab Notebook](https://colab.research.google.com/drive/1WbGHndnon8VbXcYg5Kmarva3l9tui1RB?usp=sharing)\r\n\r\nI get an error in the calculation of the weighted losses. My understanding is that the sample weights should have the shape of (batch_size x time_steps)\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndummy_input = tf.random.normal((1024, 1000))\r\ndummy_output = tf.random.normal((1024, 10))\r\ndummy_output_weights = tf.random.uniform(minval=0, maxval=1, shape=(1024, 10))\r\n\r\nprint(dummy_input.shape)\r\nprint(dummy_output.shape)\r\nprint(dummy_output_weights.shape)\r\n\r\ndummy_model = tf.keras.Sequential()\r\ndummy_model.add(tf.keras.Input(shape=(1000,)))\r\ndummy_model.add(tf.keras.layers.Dense(32, activation='relu'))\r\ndummy_model.add(tf.keras.layers.Dense(10, activation='linear'))\r\n\r\ndummy_model.compile(optimizer='sgd', loss='mse', sample_weight_mode='temporal')\r\ndummy_model.summary()\r\n\r\ndummy_model.fit(x=dummy_input, y=dummy_output, batch_size=32, epochs=10, sample_weight=dummy_output_weights)\r\n```\r\n\r\nError message\r\n```\r\nEpoch 1/10\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-3d254f045172> in <module>()\r\n----> 1 dummy_model.fit(x=dummy_input, y=dummy_output, batch_size=32, epochs=10, sample_weight=dummy_output_weights)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1096                 batch_size=batch_size):\r\n   1097               callbacks.on_train_batch_begin(step)\r\n-> 1098               tmp_logs = train_function(iterator)\r\n   1099               if data_handler.should_sync:\r\n   1100                 context.async_wait()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    821       # This is the first call of __call__, so we have to initialize.\r\n    822       initializers = []\r\n--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    824     finally:\r\n    825       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    695     self._concrete_stateful_fn = (\r\n    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 697             *args, **kwds))\r\n    698 \r\n    699     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3073             arg_names=arg_names,\r\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3075             capture_by_value=self._capture_by_value),\r\n   3076         self._function_attributes,\r\n   3077         function_spec=self.function_spec,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\r\n        y, y_pred, sample_weight, regularization_losses=self.losses)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\r\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:151 __call__\r\n        losses, sample_weight, reduction=self._get_reduction())\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:112 compute_weighted_loss\r\n        losses, sample_weight)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/util.py:143 scale_losses_by_sample_weight\r\n        losses, None, sample_weight)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/util.py:95 squeeze_or_expand_dimensions\r\n        sample_weight = array_ops.squeeze(sample_weight, [-1])\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507 new_func\r\n        return func(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:4259 squeeze\r\n        return gen_array_ops.squeeze(input, axis, name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:10044 squeeze\r\n        \"Squeeze\", input=input, squeeze_dims=axis, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\r\n        compute_device)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\r\n        op_def=op_def)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1975 __init__\r\n        control_input_ops, op_def)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for '{{node mean_squared_error/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:2)' with input shapes: [32,10].\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b2650ff46e9f3125e6aeffe68d788852/42750.ipynb). Thanks!", "I ran the code and the issue exists in tf-nightly [2.5.0-dev20210114] , please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e089d3fb996ff199cd733453833b9796/untitled499.ipynb)", "I'm also hitting the same error, I've had to workaround by splitting my output of length N into N scalar outputs which is quite inconvenient...", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210526, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/34f88c4fe27bece6507576eb79df9e9d/42750.ipynb). Thanks!", "In Tensorflow 2.7, you can see the specific error with the shape you're providing, by modifying shape the error was gone, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/89c68341991e9f7a0347b8e762ec2933/42750.ipynb) for the same and add the dimensions according to your requirement, Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42750\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42750\">No</a>\n"]}, {"number": 42749, "title": "Extend test _loss_scaling", "body": "Test if Optimizers with Slots could be used with SavedModel to train again with reproducible results.", "comments": ["/cc @reedwm @k-w-w ", "I also still see this TODO in master on the `load` https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/load.py#L116\r\n\r\nI don't know if it is still (partially) valid.\r\n", "I confirmed loading a model with `load_model` does not restore slot variables. This explains the failure both in this PR and your sample code in https://github.com/tensorflow/addons/pull/2126#issuecomment-683266384. Since slot variables are not restored and Adam (unlike SGD without momentum) has slot variables, saving and loading a model is not equivalent to simply keeping the same model.\r\n\r\nI'm closing this PR since currently it's impossible to test this sort of thing, as slot variables are not restored. Let me know if you'd like to modify this PR and I can reopen.", "Yes I suppose It was expected but we need to put this in the documentation cause It Is not a so edge case and this source code comment It Is on a not exposed API.\n\nAlso I think we need raise a warning somwhere? On load? On fit?", "@k-w-w can we mention that slot variables are not restored in the documentation of `tf.saved_model.load` and `tf.keras.models.load_model`? And perhaps the corresponding save functions as well.\r\n\r\nI personally don't think we should warn since this won't negatively effect most users, and it would effectively warn every time a model is loaded. ", "Actually it would make sense IMO if there is a warning when calling fit() on a model loaded from a SavedModel with a restored optimizer with slot variables . This would not warn for most SavedModels users, only those who would be affected by the fact slot variables are not restored.", "Need we to add a setter to the model to detect this case?", "Just a Draft for an entry point: https://github.com/tensorflow/tensorflow/pull/42846\r\nLet me know there how we could go ahead..", "Are there any workarounds for this? Can one manually save the optimizer state (some combination of `get_weights`/`get_slots`)?"]}, {"number": 42748, "title": "CMSIS-NN Conv kernel writes outside of allocated memory when num_channels > 256", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): b36436b087bd8e8701ef51718179037cccdfc26e\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ST IoT Discovery Kit\r\n\r\n**Describe the problem**\r\n\r\nThe CMSIS-NN convolutional and depthwise convolutional layers write in uninitialized memory in `CalculateOpData` when `num_channels` is higher than 256 ([kMaxChannels](https://github.com/tensorflow/tensorflow/blob/5d6860d3749109d62964d4e15e267f6b4465258c/tensorflow/lite/micro/kernels/cmsis-nn/conv.cc#L60)). There are no boundary checks for this function, and thus no error is thrown when this happens.  Non CMSIS-NN kernels have no problem. \r\n\r\nI can get around this by switching the structure to use dynamic memory:\r\n\r\n```cpp\r\nvoid* per_channel_output_multiplier;\r\ncontext->AllocatePersistentBuffer(context, sizeof(int32_t) * num_channels, &per_channel_output_multiplier);\r\ndata->per_channel_output_multiplier = (int32_t*)per_channel_output_multiplier;\r\n```\r\n\r\nBut this is a temporary object, so I assume I should use the scratch buffer instead. However, I have no idea how to clear the object again from the scratch buffer.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nThe problem shows in the following tflite model. In one of the final layers it has 1280 channels. [ei-plants-vs-lamps-transfer-learning-tensorflow-lite-int8-quantized-model.lite.zip](https://github.com/tensorflow/tensorflow/files/5143770/ei-plants-vs-lamps-transfer-learning-tensorflow-lite-int8-quantized-model.lite.zip)\r\n\r\n/cc @kwagyeman This could be the same issue as you're facing with CMSIS-NN. It shows on our image models.", "comments": ["Awesome find Jan. If this is fixed then CMSIS-NN will work for images finally.", "@kwagyeman Seems like it. I've written a patch and now have CMSIS-NN working with MobileNetV2, will open up a PR after the weekend.", "@janjongboom Are there any other parts in the code where they allocate static buffers like this? It's kinda down right absurd in production code by ARM employees...", "Conv.cc and depthwiseconv.cc both have the issue. You need to fix both.", "Hello @kwagyeman @janjongboom! \r\n\r\nI represent Arm. Some background; kMaxChannels is there for historical reasons - that's how the reference kernels used to be implemented as well. Some months back, the AllocatePersistentBuffer API was introduced in favor of the static buffers. Hence the ref kernels was updated to use that API. Our intention is to 'up-merge' changes, such as this one, to our CMSIS-NN kernels on a regular basis. However, we had to revert that change since we discovered an issue with the AllocatePersistentBuffer API, see https://github.com/tensorflow/tensorflow/pull/41121 for details. We're worked with the TFLu team to resolve this issue and it looks like this was fixed this(!) friday by https://github.com/tensorflow/tensorflow/commit/59d177d9acabe8e70bc33e554a364d2620bc6999#diff-7aab15c6eb5282359c2b928998bab743.\r\n\r\nIn the meanwhile, the quick fix is to bump kMaxChannels to the size it needs to be on your local fork.\r\n\r\nI hope this gives some clarity to the issue.\r\n\r\n@janjongboom thanks for the PR (https://github.com/tensorflow/tensorflow/pull/42770)! Let continue the technical discussion there.\r\n\r\nCheers!\r\nFredrik", "@freddan80 Thanks for the update. I ran into the same issue on our 2.3.0 fork, and replaced the scratch buffer with a persistent one to get around that.\r\n\r\n> In the meanwhile, the quick fix is to bump kMaxChannels to the size it needs to be on your local fork.\r\n\r\nThis adds 10K of RAM *per* convolutional layer, which won't fit on our targets.", "@janjongboom thanks for the updated PR!\r\n\r\n> This adds 10K of RAM per convolutional layer, which won't fit on our targets.\r\n\r\nMake sense. Then your PR is the preferred solution.\r\n\r\nHowever, allocating the memory using `AllocatePersistentBuffer` API will only move the memory claim from the BSS to the memory arena (which may well be located in BSS as well). It actually won't save memory, it's just putting it in a different part of the SRAM.\r\n\r\nCheers!\r\n\r\n\r\n", "The tensor arena is dynamically allocated and freed so memory can be used for other things until when it\u2019s needed for TF lite. Having it be dynamically allocated is a must.\r\n\r\nI don\u2019t wish to be mean here, but, I have been consistently disappointed with ARMs lack of effort in support for these faster kernels... I would like to ask that future code released has more effort put into making it useful for a wider audience. Hard coded static buffers in code with max sizes without range checks is not production code.\r\n\r\nAnyway, glad to see this getting fixed so we can enable it on the OpenMV Cam finally.", "@kwagyeman I believe there is some misunderstanding here. The TFLu feature to handle interleaved persistent memory request and arena scratch buffer requests, is only possible since the 28th of Aug 2020 (see 59d177d9acabe). We've been working with Google towards getting this TFLu feature so that it improves the usage of CMSIS-NN for wider audience like you.\r\n\r\nSorry that you feel that it wasn't given attention. Hope PR #42770 helps the case now and please reach out by a Github ticket, such as this one, for unsupported features or bugs, either in the TFLu or the CMSIS-NN repo. That way the issue is seen and known by a wider audience and thereby gain traction and leverage support from the entire community.\r\n\r\nCheers!", "> However, allocating the memory using AllocatePersistentBuffer API will only move the memory claim from the BSS to the memory arena (which may well be located in BSS as well). It actually won't save memory, it's just putting it in a different part of the SRAM.\r\n\r\nYep, but I only have one very large convolutional layer and a lot of smaller ones, so dynamically allocating it saves me a lot of memory. But great to see PRs being landed to mainline these types of issues. \r\n\r\n@freddan80 Q though (we can take this offline if you prefer): are there currently CI/CD pipelines set up for the CMSIS-NN kernels? E.g. we found this issue & https://github.com/tensorflow/tensorflow/issues/41816 through our own CI (and a bunch of debugging :-)), and we have currently automated tests for a wide variety of development boards that we run on real hardware for releases, both with the micro allocator and custom allocators, and with and without CMSIS-NN. But we don't follow mainline TensorFlow so we only find these issues once we upgrade. Perhaps it would be beneficial to share that with Arm?", "> @freddan80 Q though (we can take this offline if you prefer): are there currently CI/CD pipelines set up for the CMSIS-NN kernels? E.g. we found this issue & #41816 through our own CI (and a bunch of debugging :-)), and we have currently automated tests for a wide variety of development boards that we run on real hardware for releases, both with the micro allocator and custom allocators, and with and without CMSIS-NN. But we don't follow mainline TensorFlow so we only find these issues once we upgrade. Perhaps it would be beneficial to share that with Arm?\r\n\r\nWe have our internal CI, that tests TFLu-CMSIS-NN usecases on FPGAs and HW models with Cortex-M4 up to Cortex-M55 reference systems. We usually find issues related to CMSIS-NN quite soon. Unfortunately the networks we use did not have a reshape before FC as you encountered in #41816, or it would have been caught. Usually we create a kernel unit test case - in `fully_connected_test.cc` in this case - when we find issues such as the one in #41816, and then contribute that to avoid the same issue in the future. For the issue reported in this ticket, we\u2019ve been waiting for 59d177d to make a permanent fix (thanks again for you PR!). We had actually bumped `kMaxChannels` to 2048 in our internal tests (not being aware of the missing range check), but didn\u2019t want to deliver that since it increases the memory footprint unnecessarily for smaller network usecases.\r\n\r\nThat\u2019s some words about our internal CI. In parallel, we're working with the TFLu team to enable CMSIS-NN regression directly in the Tensorflow Github project, using Renode as target emulator (see `test_stm32f4_binary.sh`) to prevent breaking PR\u2019s being merged. It\u2019s work ongoing and may take some time since there's an issue running this using Docker. However, I believe CMSIS-NN is tested using at least one of the targets in the internal TFLu tests, so there is some level of protection.\r\n\r\nIt\u2019s awesome to hear that you have CMSIS-NN tests in you CI tests as well! I\u2019d love to hear more about that. Perhaps some of your work can be upstreamed to the Tensorflow Github repo?\r\n\r\nCheers!\r\nFredrik", "I can provide some additional context from the TF Micro side:\r\n\r\nAs you probably already know, the conv and depthwise_conv kernels use per-channel scale and zero-point quantization parameters. From the scale and zero points values, we calculate a multiplier and shift during the Prepare stage, which involves some computationally expensive double precision math as well as exponent calculations. Applying the multiplier and shift is a relatively cheap operation which occurs during the Eval stage. The storage required for these multipliers and shifts correlates to the number of channels along the quantized dimension (see the [TFLite quantization spec](https://www.tensorflow.org/lite/performance/quantization_spec#int8_quantized_operator_specifications) for more info).\r\n\r\nSince we pre-calcualate shifts and multipliers during Prepare, we need to store the resulting values in persistent buffers, which means the memory cannot be re-used between kernel invocations (and the buffers must be allocated via context->AllocatePersistentBuffer). This is the price we pay for avoiding the large runtime overhead of calculating the multiplier and shift on-the-fly. Before we had a way to allocate persistent buffers in TF Micro, we implemented the conv and depthwise_conv kernels with a fixed-length buffer with 256 values. This issue seen here results from that previous implementation, and the fact not all of our kernels  have been updated to use dynamic allocation.\r\n\r\nOn a side note, it seems that there is increasing evidence that allowing models to use per-tensor quantization instead of per-channel for conv and depthwise_conv may be better for some TF Micro usecases. This is something we are aware of and discussing internally."]}, {"number": 42747, "title": "fix the wrong link", "body": "", "comments": ["Thanks!"]}, {"number": 42746, "title": "small fix for the autoclustering_xla.ipynb", "body": "", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/42746\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>"]}, {"number": 42745, "title": "fix linux build errors by moving static constexpr members outside the class", "body": "fixes linux debug build errors like in #42427", "comments": ["> Moving these outside of the class is not a good way to solve this issue (because it pollutes the `data::experimental` namespace). Instead, you should simply add a definition of these to the .cc file: (here is an example of how the MapDataset class does this https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/map_dataset_op.cc#L31-L39).\r\n\r\nChanging the fix as you suggested, I noticed that this changes already made it into master via deeeffefe317a0728b70262b9177712cadd77df1, so we can close this PR."]}, {"number": 42744, "title": "TFLite: TF2.3 error building tensorflow-lite-select-tf-ops", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):  3.1.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0 \r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n- NDK: android-ndk-r20\r\n\r\n\r\n\r\n**Describe the problem**\r\nGetting this error if building tensorflow-lite with tensorflow-lite-select-tf-ops:\r\n```bash\r\nERROR: /home/${User}/projects/downloads/tensorflow/tensorflow/lite/delegates/flex/BUILD:77:1: C++ compilation of rule '//tensorflow/lite/delegates/flex:delegate_only_runtime' failed (Exit 1)\r\ntensorflow/lite/delegates/flex/delegate.cc:152:37: error: 'TF_AcquireFlexDelegate' has C-linkage specified, but returns user-defined type 'tflite::TfLiteDelegateUniquePtr' (aka 'unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate *)>') which is incompatible with C [-Werror,-Wreturn-type-c-linkage]\r\n    tflite::TfLiteDelegateUniquePtr TF_AcquireFlexDelegate() {\r\n                                    ^\r\n1 error generated.\r\nTarget //tmp:tensorflow-lite-select-tf-ops failed to build\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```bash\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout -b master 6e9d916229b5aefbdcfd33cbc4b34c9f48b5e6e1\r\nnano .tf_configure.bazelrc\r\n```\r\n\r\nBazel config .tf_configure.bazelrc:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --action_env ANDROID_NDK_HOME=\"CHANGE_TO_YOUR_ANDROID_NDK_HOME\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"21\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"28.0.0\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"23\"\r\nbuild --action_env ANDROID_SDK_HOME=\"CHANGE_TO_YOUR_ANDROID_SDK_HOME\"\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\nAnd compile with \r\n```bash\r\nbazel build --cxxopt='--std=c++14' -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a --config=monolithic \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  //tensorflow/lite/java:tensorflow-lite \\\r\n  //tensorflow/lite/java:tensorflow-lite-gpu \\\r\n  //tensorflow/lite/delegates/flex:delegate \\\r\n  //tensorflow/lite/experimental/kernels:ctc_beam_search_decoder_op \\\r\n  //tmp:tensorflow-lite-select-tf-ops\r\n```\r\nor\r\n```bash\r\nEXPORT_DIR=/home/${User}/output/tflite\r\n./tensorflow/lite/tools/build_aar.sh \\\r\n  --input_models=$EXPORT_DIR/detect.tflite,$EXPORT_DIR/Decode.tflite,$EXPORT_DIR/Encoder.tflite \\\r\n  --target_archs=arm64-v8a,armeabi-v7a \\\r\n  --tflite_custom_ops_deps=//tensorflow/lite/experimental/kernels:ctc_beam_search_decoder_op\r\n```\r\n\r\n**Fast fix**\r\nJust comment the lines 152-154 in `tensorflow/lite/delegates/flex/delegate.cc` (https://github.com/tensorflow/tensorflow/commit/536f6be302d945513fedff57e65d89f6e0e026db#diff-5b63f45ccbd2b9d4aa2edaf64a92e14b), build and it worked (tested on Pixel 2 XL with Android 11)\r\n```c++\r\n  //  tflite::TfLiteDelegateUniquePtr TF_AcquireFlexDelegate() {\r\n//  return tflite::FlexDelegate::Create();\r\n//}\r\n```", "comments": ["I can confirm that i have the same error when trying to build benchmark with flex", "Acked. Let me handle this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42744\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42744\">No</a>\n", "Thanks!"]}, {"number": 42743, "title": "fix DEBUG_BUILD option on linux not getting passed into third_party/aws-checksums", "body": "- fixes #37498\r\n- probably obsoletes #39239 (already merged) which doesn't work (in at least one case)", "comments": ["Ubuntu CPU CI fails on `tensorflow/core/data/service/credentials_factory_test.cc:47:29: error: 'make_unique' is not a member of 'absl'\r\n   auto test_factory = absl::make_unique<TestCredentialsFactory>();`. I don't see a link to this PR's code changes + see the very same error on other PRs. Maybe some other fixes need to be merged into this branch?"]}, {"number": 42742, "title": "how to compile 32-bit tensorflow lib and DLL", "body": "I try to compile tensorflow with bazel, but I don't know how to compile 32-bit tensorflow lib and DLL in windows , I only git the 64-bit .So I want to know how to compile DLL and lib of Win32 with bazel or CMake in windows. I use the tensorflow verion of 2.3.\r\n", "comments": ["@1095560081,\r\nPlease try building TensorFlow from source using [this guide](https://www.tensorflow.org/install/source_windows) and let us know if it works. \r\n\r\nAlso, please take a look at similar issues [#20362](https://github.com/tensorflow/tensorflow/issues/20362) and [#32315](https://github.com/tensorflow/tensorflow/issues/32315) for more information. Thanks!", "You will need a bazel compiled for 32 bits. You will need to ensure all compile time dependencies are 32 bits. Then the install instructions linked above should work.\r\n\r\nGetting a list of dependencies and how to get them to 32 bits is out of scope for the issues in this repository. Suggest Stack Overflow. Closing this issue as it is not related to TF code issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42742\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42742\">No</a>\n"]}, {"number": 42741, "title": "ModelCheckpoint behavior doesn't change when changing save_weights_only from True to False for custom tf.keras.Model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Python version: 3.6.9\r\n- Tensorflow version: 2.3.0 ('v2.3.0-0-gb36436b087')\r\n\r\n**Describe the current behavior**\r\nI am migrating from TF1 to TF2 and I am trying to understand best practices when it comes to model saving/checkpointing during training.\r\n\r\nI would like to checkpoint the model training so as to be able to resume training later.\r\n\r\nQuoting from the documentation (https://www.tensorflow.org/guide/keras/save_and_serialize#saving_loading_only_the_models_weights_values)\r\n```\r\nYou can choose to only save & load a model's weights. This can be useful if:\r\n* You only need the model for inference: in this case you won't need to restart training, so you don't need the compilation information or optimizer state.\r\n* You are doing transfer learning: in this case you will be training a new model reusing the state of a prior model, so you don't need the compilation information of the prior model.\r\n```\r\n\r\nSo it seems to me given my use-case of resuming training that I would want to checkpoint the full model and not just the weights correct ? More specifically I would like the following to be saved at each epoch.\r\n\r\n```\r\n* The model's architecture/config\r\n* The model's weight values (which were learned during training)\r\n* The model's compilation information (if compile()) was called\r\n* The optimizer and its state, if any (this enables you to restart training where you left)\r\n```\r\n\r\nThe following code snippet shown in the documentation (https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_usage) only saves the weights:\r\n```\r\ncheckpoint_path = \"training_1/cp.ckpt\"\r\ncheckpoint_dir = os.path.dirname(checkpoint_path)\r\n\r\n# Create a callback that saves the model's weights\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n                                                 save_weights_only=True,\r\n                                                 verbose=1)\r\n\r\n# Train the model with the new callback\r\nmodel.fit(train_images, \r\n          train_labels,  \r\n          epochs=10,\r\n          validation_data=(test_images,test_labels),\r\n          callbacks=[cp_callback])  # Pass callback to training\r\n```\r\n\r\nSo I made a single change to the above code changing `save_weights_only` from False to True\r\n```\r\ncheckpoint_path = \"training_1/cp.ckpt\"\r\ncheckpoint_dir = os.path.dirname(checkpoint_path)\r\n\r\n# Create a callback that saves the model's weights\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n                                                 save_weights_only=False,   # <- this is the change I made \r\n                                                 verbose=1)\r\n\r\n# Train the model with the new callback\r\nmodel.fit(train_images, \r\n          train_labels,  \r\n          epochs=10,\r\n          validation_data=(test_images,test_labels),\r\n          callbacks=[cp_callback])  # Pass callback to training\r\n```\r\nand now I see a cp.ckpt file instead of the `.data` and `.index` file - all good so far.\r\n\r\nHowever the example in the documentation is using a `tf.keras.Sequential` model - and I would like to subclass `tf.keras.Model` instead. So I replicate the example's model using a very simple tf.keras.Model\r\n\r\n```\r\nclass SimpleModel(tf.keras.Model):\r\n  def __init__(self, **kwargs):\r\n    super().__init__(**kwargs)\r\n    self.layer_1 = keras.layers.Dense(512, activation='relu', input_shape=(784,))\r\n    self.layer_2 = keras.layers.Dropout(0.2)\r\n    self.layer_3 = keras.layers.Dense(10)\r\n  \r\n  def call(self, inputs):\r\n    x = self.layer_1(inputs)\r\n    x = self.layer_2(x)\r\n    return self.layer_3(x)\r\n```\r\nand now I no longer see cp.ckpt files being created by the callback but I see `.data` and `.index` files (indicating just the weights are being saved)\r\n\r\nIt would be great if someone can explain how to go about checkpointing the full model in a SavedModel (.tf) format for the purpose of resuming training specifically for models implemented by subclassing tf.keras.Model.", "comments": ["After some digging into the code it seems - the `set_model` call in `ModelCheckpoint` reverts back the `save_weights_only` to True \r\n\r\n```\r\n  def set_model(self, model):\r\n    self.model = model\r\n    # Use name matching rather than `isinstance` to avoid circular dependencies.\r\n    if (not self.save_weights_only and\r\n        not model._is_graph_network and  # pylint: disable=protected-access\r\n        model.__class__.__name__ != 'Sequential'):\r\n      self.save_weights_only = True\r\n```\r\ngiven `self._is_graph_network` is False - is there a reason for this behavior ? I see two other issues that seem to be duplicates (https://github.com/tensorflow/tensorflow/issues/39679  and https://github.com/tensorflow/tensorflow/issues/37620) of my issue that also are still open with no response ...  \r\n\r\n\r\nAlso if the switch has to be made back to `self.save_weights_only=True` (which I still don't understand why) - then it is best to show a warning to the user and for the logs to reflect this change - otherwise this goes unseen ", "Just to test things out - I created my own checkpointing callback - which just removes this behavior from `set_model` \r\n\r\n```\r\nclass CustomCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n    def set_model(self, model):\r\n        self.model = model\r\n```\r\nand now the `save_weights_only` doesnt get overriden and checkpointing works fine ... \r\n\r\nI still dont understand why `save_weights_only` has to be False when `model. _is_graph_network` is False, given that clearly if `model._is_graph_network` is False, this doesn't mean a model cant call model.save ...  ", "here is the commit that created this piece of logic in set_model\r\n```\r\n    # Use name matching rather than `isinstance` to avoid circular dependencies.\r\n    if (not self.save_weights_only and\r\n        not model._is_graph_network and  # pylint: disable=protected-access\r\n        model.__class__.__name__ != 'Sequential'):\r\n      self.save_weights_only = True\r\n```\r\nhttps://github.com/tensorflow/tensorflow/commit/ec5e355a1df575b85f9d0114e0522b64e1b9c6ff by @fchollet \r\n\r\nreading the commit message - it seems the intended behavior was to make the checkpointer default `save_weights_only` to False when using a subclassed model but not to override `save_weights_only`.  \r\n", "@marwan116 \r\nThe code is not complete for us to replicate the issue, can you please share a colab gist with the issue reported, have you tried on tf-nightly if you are facing the issue.", "@Saduf2019  - please find the colab gist to reporduce this issue (I made sure I am using tf-nightly)\r\nhttps://colab.research.google.com/gist/marwan116/ab3ac395d0dfe111eb0ec0853bb8e3f8/untitled1.ipynb\r\n", "@marwan116 In theory that `if` it is request only for the `h5` `model.save` format.\r\nCheck https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/save.py#L118-L132", "But also in the `SaveModel` format it is still not totally safe to recover the training: https://github.com/tensorflow/tensorflow/pull/42846", "/cc @rchao ", "@bhack  - sorry to respond back late - sure if you are saying this check should apply when using an h5 format, that's understandable - but currently the if statement used in `set_model` doesn't take the saving format into account. It seems to me the fix here is to add something like `if (save_format == 'h5')` check to the statement to prevent it from reverting`self.save_weights_only` to True - as for when the save format is `SavedModel` throwing the warning about slot variables as you mention in https://github.com/tensorflow/tensorflow/pull/42749 when loaded should suffice in my opinion", "@marwan116 Have you solved the problem yet? I also use 'save_best_only=True' to save the model, but when I use 'tf.keras.models.load_model' to load the model file, It happens the error: \"ValueError: No model found in config file.\"", "@chengjianhong  - my current workaround is to implement my own ModelCheckpoint - that overrides `set_model`  ... \r\n\r\n```\r\nclass CustomCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n    def set_model(self, model):\r\n        self.model = model\r\n```", "Wow, such an easy fix! Thanks @marwan116 \r\n", "@marwan116 Looks like this was resolved in recent `tf-nightly`. I couldn't reproduce the issue. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/52ab320e11923cc3badd101cb6ead32b/untitled1.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42741\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42741\">No</a>\n", "I was having issues with no .pb file being generated from my modelcheckpoint, but creating the custom callback fixed that."]}, {"number": 42740, "title": "AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TENSOR_LIKE_TYPES'", "body": "I installed the latest version of keras and tensorflow on anaconda.\r\nThe problem looks like the same as the following and it is still awaiting a response\r\nhttps://github.com/tensorflow/tensorflow/issues/38589\r\n\r\n```\r\n\r\nfrom keras.datasets import mnist\r\nfrom keras.utils import np_utils\r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import Input, Dense, Dropout, Activation, Flatten\r\nfrom keras.layers.advanced_activations import LeakyReLU\r\nfrom keras.optimizers import Adam, RMSprop\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport random\r\nfrom tqdm import notebook\r\n\r\n# Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\r\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n\r\n# Preprocessing\r\n\r\nX_train = X_train.reshape(60000, 784)\r\nX_test = X_test.reshape(10000, 784)\r\nX_train = X_train.astype('float32')/255\r\nX_test = X_test.astype('float32')/255\r\n\r\n\r\n# Set the dimensions of the noise\r\nz_dim = 100\r\n\r\n# Optimizer\r\nadam = Adam(lr=0.0002, beta_1=0.5)\r\n\r\ng = Sequential()\r\ng.add(Dense(256, input_dim=z_dim, activation=LeakyReLU(alpha=0.2)))\r\ng.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\r\ng.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\r\ng.add(Dense(784, activation='sigmoid'))  # Values between 0 and 1\r\ng.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\r\n\r\nd = Sequential()\r\nd.add(Dense(1024, input_dim=784, activation=LeakyReLU(alpha=0.2)))\r\nd.add(Dropout(0.3))\r\nd.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\r\nd.add(Dropout(0.3))\r\nd.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\r\nd.add(Dropout(0.3))\r\nd.add(Dense(1, activation='sigmoid'))  # Values between 0 and 1\r\nd.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\r\n\r\nd.trainable = False\r\ninputs = Input(shape=(z_dim, ))\r\nhidden = g(inputs)\r\noutput = d(hidden)\r\ngan = Model(inputs, output)\r\ngan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\r\n\r\n```\r\n\r\n\r\n\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-fc09a4f0e640> in <module>\r\n      3 \r\n      4 g = Sequential()\r\n----> 5 g.add(Dense(256, input_dim=z_dim, activation=LeakyReLU(alpha=0.2)))\r\n      6 g.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\r\n      7 g.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\sequential.py in add(self, layer)\r\n    164                     # and create the node connecting the current layer\r\n    165                     # to the input layer we just created.\r\n--> 166                     layer(x)\r\n    167                     set_inputs = True\r\n    168             else:\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in symbolic_fn_wrapper(*args, **kwargs)\r\n     73         if _SYMBOLIC_SCOPE.value:\r\n     74             with get_graph().as_default():\r\n---> 75                 return func(*args, **kwargs)\r\n     76         else:\r\n     77             return func(*args, **kwargs)\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py in __call__(self, inputs, **kwargs)\r\n    444                 # Raise exceptions in case the input is not compatible\r\n    445                 # with the input_spec specified in the layer constructor.\r\n--> 446                 self.assert_input_compatibility(inputs)\r\n    447 \r\n    448                 # Collect input shapes to build layer.\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py in assert_input_compatibility(self, inputs)\r\n    308         for x in inputs:\r\n    309             try:\r\n--> 310                 K.is_keras_tensor(x)\r\n    311             except ValueError:\r\n    312                 raise ValueError('Layer ' + self.name + ' was called with '\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in is_keras_tensor(x)\r\n    693     ```\r\n    694     \"\"\"\r\n--> 695     if not is_tensor(x):\r\n    696         raise ValueError('Unexpectedly found an instance of type `' +\r\n    697                          str(type(x)) + '`. '\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in is_tensor(x)\r\n    701 \r\n    702 def is_tensor(x):\r\n--> 703     return isinstance(x, tf_ops._TENSOR_LIKE_TYPES) or tf_ops.is_dense_tensor_like(x)\r\n    704 \r\n    705 \r\n\r\nAttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TENSOR_LIKE_TYPES'", "comments": ["@ClearLens \r\n\r\nThere seems to be some weird compatibility issues between keras and tensorflow.keras. So please try with tensorflow.keras instead.Also, please let us know Tensorflow version you are using?\r\n\r\n```\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.python.keras.utils import np_utils\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras import model\r\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten\r\nfrom tensorflow.keras.optimizers import Adam, RMSprop\r\n```\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42740\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42740\">No</a>\n", "I got same error. After I tried with tensorflow it worked.\r\nTensorflow version : '2.3.1' \r\nTensorflow.keras version : '2.4.0'\r\nThanks.", "> @ClearLens\r\n> \r\n> There seems to be some weird compatibility issues between keras and tensorflow.keras. So please try with tensorflow.keras instead.Also, please let us know Tensorflow version you are using?\r\n> \r\n> ```\r\n> from tensorflow.keras.datasets import mnist\r\n> from tensorflow.python.keras.utils import np_utils\r\n> from tensorflow.keras import Sequential\r\n> from tensorflow.keras import model\r\n> from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten\r\n> from tensorflow.keras.optimizers import Adam, RMSprop\r\n> ```\r\n> \r\n> Thanks!\r\n\r\nReplace 'keras' by 'tensorflow.keras' worked for me today.", " it is giving following error \r\ntensorflow.python.keras.engine.base_layer_v1"]}, {"number": 42739, "title": "Extended the http response codes to be handled", "body": "This PR extends the switch cases of response codes to be handled by the `CurlHttpRequest` class. The codes were observed while working on https://github.com/tensorflow/io/pull/1099 and the error handling seemed to be missing in the `CurlHttpRequest` class.", "comments": ["cc: @michaelbanfield \r\n"]}, {"number": 42738, "title": "Successful NUMA node read from SysFS had negative value (-1)", "body": "Hello everyone,\r\nI am running a tensorflow code on GPU and getting the following warnings and errors. \r\n\r\n```\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-08-28 16:02:27.033368: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-28 16:02:27.033380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-08-28 16:02:27.033390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-08-28 16:02:27.033400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-08-28 16:02:27.033411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-28 16:02:27.033421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-28 16:02:27.033431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-28 16:02:27.033482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-28 16:02:27.033732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-28 16:02:27.033932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-08-28 16:02:27.033951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-28 16:02:27.033957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-08-28 16:02:27.033961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n\r\nINFO:tensorflow:Running local_init_op.\r\nI0828 16:08:52.357333 139711269271360 session_manager.py:505] Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nI0828 16:08:52.375836 139711269271360 session_manager.py:508] Done running local_init_op.\r\n2020-08-28 16:08:52.660709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-08-28 16:08:53.080643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-28 16:08:53.190000: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-08-28 16:08:53.218688: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\n\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node resnet18/conv2d/Conv2D}}]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node resnet18/conv2d/Conv2D}}]]\r\n\t [[add_5/_453]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\n\r\n1.   successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2.  Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n3. Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\n\r\n\r\nCan anyone please help me in solving these issues? Thanks\r\n\r\n-mz\r\n\r\n\r\n", "comments": ["@engrmz,\r\nIn order to expedite the trouble-shooting process, please provide the complete code, dataset you are using and the below details\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory \r\n\r\nThanks!", "@engrmz,\r\nAlso, try setting a hard limit on the total GPU memory as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if it works.\r\n\r\nSimilar issue [#36025](https://github.com/tensorflow/tensorflow/issues/36025#issuecomment-628145158) for reference. Thanks!", "Hi @amahendrakar ,\r\nThanks for your prompt response. \r\nHere are the details;\r\n\r\n-  I am running the code, CvxNet from [here](https://github.com/tensorflow/graphics/tree/master/tensorflow_graphics/projects/cvxnet)\r\n- Ubuntu 18.04\r\n- Tensorflow=2.2.0\r\n- Python = 3.7.6\r\n- cudatoolkit               10.1.243               \r\n- cudnn                     7.6.5 ,               cuda=10.1_0  \r\n- tensorboard               2.2.1                \r\n- tensorboard-plugin-wit    1.6.0                        \r\n- tensorflow                2.2.0             \r\n- tensorflow-base           2.2.0             \r\n- tensorflow-estimator      2.2.0                \r\n- tensorflow-gpu            2.2.0                   \r\n- tensorflow-graphics-gpu   1.0.0                   \r\n\r\n- NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0\r\n- GPU GeForce GTX 960M (4046MiB)\r\n\r\n\r\n\r\nBy considering your recommendations I used;\r\n```\r\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor device in gpu_devices:\r\n    tf.config.experimental.set_memory_growth(device, True)\r\n```\r\nIt solves the error, but the warning **NUMA node read from SysFS had negative value (-1)** is still here. Secondly, the output of the code is incorrect (dummy output).\r\n\r\nMoreover, with approx. same settings, the code is working properly on _desktop_ computer  (Quadro k2200 - 4034MiB). ", "Could it be due to a problem with the Ubuntu? I am thinking to re-install it. ", "@engrmz,\r\nCould you please update TensorFlow to v2.3 and check if you are still facing the same issue. Thanks!", "> ```\r\n> gpu_devices = tf.config.experimental.list_physical_devices('GPU')\r\n> for device in gpu_devices:\r\n>     tf.config.experimental.set_memory_growth(device, True)\r\n> ```\r\nThank you. Had the exact same problem. This worked.\r\n\r\n", "> Could you please update TensorFlow to v2.3 and check if you are still facing the same issue. Thanks!\r\n\r\n@engrmz,\r\nAny updates regarding this issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Same issue using r2.4 sources on arch. ", "@skaldek,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "same problem", "here too", "The same problem occurred in tensorflow r2.5.0 with ubuntu20.04", "The same problem occurred in TF 2.2.0 with RHEL8.3", "To get rid of this non-fatal warning use this command:\r\n`for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done`\r\n\r\nFor details go to [this question](https://stackoverflow.com/questions/44232898/memoryerror-in-tensorflow-and-successful-numa-node-read-from-sysfs-had-negativ) in stackoverflow. ", "This worked for me. Thanks @nsssayom.\r\n", "@nsssayom This also worked for me. Thanks! ", "@nsssayom this worked for me too, thanks", "same problem. Ubuntu 21.10. \r\nHey TF guys: Wake up. CUDA/GPUs work just fine with Julia on same machine.\r\n "]}, {"number": 42737, "title": "TPU - Probable memory leak in TF 2.3 on data augmentation using tf.keras.layers.experimental.preprocessing.*", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Install from pip\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Cloud TPU v2-8\r\n\r\n**Describe the current behavior**\r\n\r\nThe recent Random augmentations added in TF 2.3.0 generate a memory leak on TPU leading to a crash and GRPC Socket 14 error message.\r\nTPU memory usage grows up to 315GB, then a crash occurs.\r\nThe problem does not appear when augmentations in the code below are disabled, or tf.image augmentations are applied.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nCreate a standard data loading pipeline, and insert a map call to a Sequential model containing tf.keras.layers.experimental.preprocessing transformations.\r\n\r\n```\r\naugmentations = tf.keras.Sequential([\r\n    tf.keras.layers.experimental.preprocessing.RandomFlip(mode='horizontal',dtype=tf.float32),\r\n    tf.keras.layers.experimental.preprocessing.RandomContrast(0.1,dtype=tf.float32),\r\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.25/4,dtype=tf.float32), #0.25=pi/2\r\n    tf.keras.layers.experimental.preprocessing.RandomTranslation(width_factor=0.1,height_factor=0.1,dtype=tf.float32),\r\n    tf.keras.layers.experimental.preprocessing.RandomZoom(0.1,dtype=tf.float32)\r\n])\r\n\r\ndef data_augment(image, label):\r\n      image = augmentations(image, training=True)\r\n    return image, label   \r\n\r\ndef get_training_dataset():\r\n    dataset = load_dataset(TRAINING_FILENAMES)\r\n    dataset = dataset.shuffle(8192)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(BATCH_SIZE)\r\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO) # Memory Leak here\r\n    dataset = dataset.prefetch(AUTO)\r\n    return dataset\r\n\r\n```\r\nThe addition of dtype=tf.float32 in the augmentation code does not change anything.\r\nThe use of a single augmentation (instead of 5 in the example above) does not change the problem.\r\n\r\nAlso, if I replace the use of the augmentation layer by a simple lambda function such as the following, the problem disappears:\r\n\r\n```\r\ndef data_augment_ok(image, label):\r\n    max_angle = 30*math.pi/180\r\n    rotation = tf.random.uniform(shape=[], minval=-max_angle, maxval=max_angle, dtype=tf.float32)\r\n    image = tf.image.random_flip_left_right(image)\r\n    image = tf.image.random_contrast(image, 0.7, 1)\r\n    image = tfa.image.rotate(image,rotation)\r\n    return image, label   \r\n\r\ndef get_training_dataset():\r\n    dataset = load_dataset(TRAINING_FILENAMES)\r\n    dataset = dataset.map(data_augment_ok, num_parallel_calls=AUTO) # Notice the call earlier in the pipeline\r\n  \r\n    dataset = dataset.shuffle(8192)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(BATCH_SIZE)\r\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\r\n    return dataset\r\n```\r\n\r\nThanks!\r\n\r\n", "comments": ["@bduclaux \r\nPlease provide with complete indented stand alone code to replicate the issue faced or if possible share a colab gist with the error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42736, "title": "Tensorflow flooding my Spyder Console", "body": "Hello,\r\n\r\nI've been searching for a way to suppress tensorflow information like this:\r\n...\r\n`\r\n2020-08-28 14:53:56.959449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n`\r\n...\r\n\r\nThis is very annoying to me. \r\nI tried various recommendations:\r\n`\r\ntf.autograph.set_verbosity(x) # x in [0,1,2,3,4,5 ...]\r\n`\r\n\r\n`\r\nos.environ[\"AUTOGRAPH_VERBOSITY\"]=\"y\" # y in [0,1,2,3]\r\n`\r\n\r\n`\r\nimport logging\r\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\r\nlogging.getLogger(\"tensorflow\").addHandler(logging.NullHandler(logging.INFO)) \r\n`\r\nand basically every permutation of those.\r\nThese annoying information seem to appear randomly.\r\n\r\nMy setting is:\r\n- Spyder with python 3.7\r\n- tensorflow-gpu==2.1.0\r\n\r\nIs there anything I am doing wrong?\r\nSorry in advance, if this is not a proper issue. \r\nBut I'm kinda tilted.\r\n\r\nGreetings,\r\nIlkay", "comments": ["@IlkayW \r\n\r\nPlease, see similar stack overflow [issue ](https://stackoverflow.com/a/42121886)to supress information or warning logs.Thanks!", "Sadly none of these code snippets is working.", "@IlkayW \r\n\r\nMake sure that you restart the runtime after changing the log level and also set the log level before importing tensorflow. Please, check the [gist](https://colab.research.google.com/gist/amahendrakar/eed2d7127748df36b1f8a4eeaad7e7b4/42736.ipynb#scrollTo=4CPZThCZApop) for reference.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42735, "title": "Android - Drawback of Fixing Error (Regular TensorFlow ops) is increasing the app size to 195MB! ", "body": "### Object detection Android TF LITE EXAMPLE\r\nI am running the example of [object detection Android version](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android), which need about 14MB storage on the physical phone.\r\n\r\n\r\n### My trained Object detection Android TF LITE\r\nHowever, when I used my object detection trained model, (which is [converted to TF LITE using this link](https://github.com/tensorflow/tensorflow/issues/42114#issuecomment-671593386)), I got the following exception when the app started: \r\n\r\n\r\n```\r\norg.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 5086\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 223 (FlexSize) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:347)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:196)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)\r\n        at android.os.Handler.handleCallback(Handler.java:938)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:223)\r\n        at android.os.HandlerThread.run(HandlerThread.java:67)\r\n```\r\n\r\n* My tf-Lite model size: 5MB\r\n* The app size on the mobile is about 15 MB\r\n\r\n\r\n### To Solve This Error:\r\n- I include the following dependency on the **build.gradle (Modle app)**:\r\n\r\n```\r\n    // This dependency adds the necessary TF op support.\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n```\r\n\r\n<img width=\"1019\" alt=\"Screen Shot 2020-08-28 at 3 23 41 PM\" src=\"https://user-images.githubusercontent.com/68266028/91560386-764b9680-e942-11ea-827e-28e2680d866c.png\">\r\n\r\n\r\nThat solves the problem and my object detection app is working very well. \r\n\r\n### However, The app is now 195MB size (storage) on the physical mobile! Does this (-tf-ops) dependency has this large size! If so how can I fix the previous exception without including this dependency in the android app?\r\n\r\n\r\n<img width=\"305\" alt=\"Screen Shot 2020-08-28 at 3 21 26 PM\" src=\"https://user-images.githubusercontent.com/68266028/91560234-266ccf80-e942-11ea-8b08-9ecb6aa2bb5d.png\">\r\n\r\n\r\n\r\n\r\n", "comments": ["Which model are you using from the TF2 detection zoo? I am working on a script to convert SSD models to a more TFLite-friendly format, which should make the usage of TF-Select ops unnecessary. ", "@srjoglekar246 \r\nI follow [this tutorial ](https://github.com/abdelrahman-gaber/tf2-object-detection-api-tutorial)to train object detection on a custom dataset. \r\n\r\n* In the tutorial, the following model is used as a backbone model.\r\n\r\n`wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz`\r\n\r\n* Then convert it to TF LITE using [this script](https://github.com/tensorflow/tensorflow/issues/42114#issuecomment-671593386)\r\n\r\n**By the way, the size of the converted model (tf lite) is only 5 MB**", "@srjoglekar246 \r\nThis is TF-Select ops converting [script](https://github.com/tensorflow/tensorflow/issues/42114#issuecomment-671593386) I used to convert the model to the lite.\r\n\r\n", "How about using the selective build? https://www.tensorflow.org/lite/guide/reduce_binary_size", "@hahmad2008 That unfortunately isn't the best way to convert SSD models to TFLite, since it includes parts of the graph not intended for on-device inference. I have an upcoming script to do this for TF2 models, please refer to this: https://github.com/tensorflow/models/issues/9033", "Thanks @srjoglekar246 \r\nBut where can I see that script? ", "In internal review, hopefully will land soon :-)", "Then, I have to follow the thread.", "@srjoglekar246 I used the same [script](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md). with the latest version of tf-nightly-2.4.\r\n\r\n```\r\n# From the tensorflow/models/research/ directory\r\npython object_detection/export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path path/to/ssd_model/pipeline.config \\\r\n    --trained_checkpoint_dir path/to/ssd_model/checkpoint \\\r\n    --output_directory path/to/exported_model_directory\r\n```\r\n\r\nI got a _544 byte_ lite model, but when I tried to test the lite model, I got the error:\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\nprint(input_shape)\r\n```\r\n\r\n- Error for this line `interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)`\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates, num_threads)\r\n    206               custom_op_registerers_by_func))\r\n    207       if not self._interpreter:\r\n--> 208         raise ValueError('Failed to open {}'.format(model_path))\r\n    209     elif model_content and not model_path:\r\n    210       custom_op_registerers_by_name = [\r\n\r\nValueError: Did not get operators, tensors, or buffers in subgraph 0.\r\n```\r\n\r\nAny idea? how to solve this?", "@hahmad2008 Please use the latest nightly for tflite_convert. Otherwise you won't get the latest convertor changes. Looks like you got it converting now, how big is the model?\r\n\r\n`export_tflite_graph_tf2.py` has some parameters that you can use to fine-tune how many results it provides, and the accuracy.\r\n\r\nSee `ssd_use_regular_nms` & `ssd_max_detections` [here](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py#L98). provide a higher value to ssd>max_detections and/or set ssd_use_regular_nms to True.\r\n\r\n*But* note that these changes will increase latency.", "@srjoglekar246 Thank you so much. it works now. the app is now 15 MB size \ud83d\udc4d ", "@srjoglekar246 but why the performance of the model is not natural, I mean the percentage of scores + when there is no object is trained on is visible it then detect the view as on of the trained objects with a high score :( -_-", "Did you try [these params](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py#L98)? `--ssd_use_regular_nms=true` while exporting will make your model more accurate, so will increasing `ssd_max_detections`."]}, {"number": 42734, "title": "TPU - Memory leak in TF 2.3 on data augmentation using tf.keras.layers.experimental.preprocessing/", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@bduclaux \r\nPlease update the issue template with the issue details.", "if this is duplicate of #42737, please close this as it will be tracked.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42734\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42734\">No</a>\n"]}, {"number": 42733, "title": "Problem with creating a tf.data.Dataset iterator from numpy array", "body": "I am trying to create a tensorflow data iterator from a numpy array, placed on a certain device. I expected it to be of type tf.data.Dataset..., but what I got were something else. I wonder how to create an iterator based on this target_data. Or is there a better way to convert numpy to tf.data? \r\n```\r\ntarget_data = tf.data.Dataset.from_tensor_slices(target_batched)\r\n# <class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\r\ntarget_data= target_data.apply(tf.data.experimental.prefetch_to_device(\"/cpu:0\", 12))\r\n# <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\r\n```\r\n(here, target_data.make_initializable_iterator() doesn't work) \r\n", "comments": ["Hi @BoyuanJackChen your `target_data` is a `tf.data.Dataset`. If you look [here in the source code](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/data/ops/dataset_ops.py#L4286) at the `PrefetchDataset` class, and then look up the inheritance hierarchy, the root is class `DatasetV2` which has the tf export symbol `@tf_export(\"data.Dataset\", v1=[])`\r\n\r\nAs for `make_initializable_iterator()` this method is no longer available in TF2. You import it from the compat module, but I don't this it will work with eager execution. If you want to iterate over your data, you can do that easily with your `tf.data.Dataset`. The examples [here in the docs](https://www.tensorflow.org/api_docs/python/tf/data/Iterator) on how to iterate over your dataset might be of use. Happy to provide more help if needed, if you share the version of TF you are using as well as the specific error you saw when calling `make_initializable_iterator()` \r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42732, "title": "fix typo in documentation", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42732) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42732) for more info**.\n\n<!-- ok -->"]}, {"number": 42731, "title": "Build issue on libtensorflowlite.a for cross compile", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nI used docker image downloaded few days ago from docker hub -->  tensorflow/tensorflow:devel \r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\ncross compiler version : (result of $ arm-cortex_a9-linux-gnueabi-gcc --version)\r\narm-cortex_a9-linux-gnueabi-gcc (crosstool-NG linaro-1.13.1-4.8-2013.11 - nexell) 4.7.4 20131111 (prerelease)\r\nCopyright (C) 2012 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\n\r\nI want to build a libtensorflowlite.a for my board, but it is not compatible with existing general cross compilers.\r\nI followed the guide from the site (https://www.tensorflow.org/lite/guide/build_arm64?hl=ko), and I got libraries for 32-bit and 64-bit, but those has built with the toolchain described in the webpage (https://github.com/tensorflow/tensorflow/tree/master/third_party/toolchains/embedded/arm-linux).\r\n\r\nI use arm-cortex_a9-linux-gnueabi- compilers for my board(nxp4330), and there are errors when I try to build the library using script file.\r\n\r\nI'm not sure what is the problem, and I attached logs from the execution. Thanks in advance.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. I copied my toolchain into docker\r\n2. I copied build_aarch64_lib.sh as build_armv7l_lib.sh (temporary file) and I modified toolchain path. (but it didn't work well)\r\n3. I execute the script and I specified the tools with commands as follow:\r\nroot@c41d54d7a748:/tensorflow_src# ./tensorflow/lite/tools/make/build_armv7l_lib.sh CC=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc CXX=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ AR=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ar LD=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ld\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nroot@c41d54d7a748:/tensorflow_src# ./tensorflow/lite/tools/make/build_armv7l_lib.sh CC=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc CXX=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ AR=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ar LD=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ld\r\n+ set -e\r\n+++ dirname ./tensorflow/lite/tools/make/build_armv7l_lib.sh\r\n++ cd ./tensorflow/lite/tools/make\r\n++ pwd\r\n+ SCRIPT_DIR=/tensorflow_src/tensorflow/lite/tools/make\r\n+ TENSORFLOW_DIR=/tensorflow_src/tensorflow/lite/tools/make/../../../..\r\n++ free -m\r\n++ awk '/^Mem/ {print $2}'\r\n+ FREE_MEM=128532\r\n+ [[ FREE_MEM -gt 2000 ]]\r\n+ NO_JOB=4\r\n+ TOOL_CHAIN_PREFIX=/root/4.7.4/arm=cortex_a9-linux-gnueabi/\r\n+ CC=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc\r\n+ CXX=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++\r\n+ make -j 4 TARGET=r9 ARCH=armv7-a -C /tensorflow_src/tensorflow/lite/tools/make/../../../.. -f tensorflow/lite/tools/make/Makefile CC=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc CXX=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ AR=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ar LD=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ld\r\nmake: Entering directory '/tensorflow_src'\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/allocation.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/allocation.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/arena_planner.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/arena_planner.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/c/c_api.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/c/c_api.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/c/c_api_experimental.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/c/c_api_experimental.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc -O3 -DNDEBUG -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/c/common.c -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/c/common.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/api/error_reporter.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/api/error_reporter.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/api/flatbuffer_conversions.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/api/flatbuffer_conversions.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/api/op_resolver.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/api/op_resolver.o\r\ntensorflow/lite/c/c_api.cc:40:7: note: the mangling of \u2018va_list\u2019 has changed in GCC 4.4\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/api/tensor_utils.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/api/tensor_utils.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/subgraph.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/subgraph.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/experimental/resource/resource_variable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/experimental/resource/resource_variable.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/experimental/resource/static_hashtable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/experimental/resource/static_hashtable.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/external_cpu_backend_context.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/external_cpu_backend_context.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/graph_info.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/graph_info.o\r\ntensorflow/lite/core/subgraph.cc:1058:6: note: the mangling of \u2018va_list\u2019 has changed in GCC 4.4\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/interpreter.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/interpreter.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/interpreter_builder.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/interpreter_builder.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/activations.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/activations.o\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/dispatch_gemm_shape.h:23:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/gemmlowp.h:19,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_context.h:21,\r\nfrom tensorflow/lite/kernels/activations.cc:25:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h: In member function \u2018void gemmlowp::BlockingCounter::Wait()\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h:203:9: error: \u2018sleep_for\u2019 is not a member of \u2018std::this_thread\u2019\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/add.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/add.o\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:33:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\nfrom tensorflow/lite/kernels/activations.cc:29:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h: At global scope:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:76:36: error: invalid pure specifier (only \u2018= 0\u2019 is allowed) before \u2018;\u2019 token\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:11: error: \u2018kMaxMulParamsAlignment\u2019 is not a type\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: ISO C++ forbids declaration of \u2018alignas\u2019 with no type [-fpermissive]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: expected \u2018;\u2019 at end of member declaration\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\nfrom tensorflow/lite/kernels/activations.cc:29:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function \u2018void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: \u2018is_trivially_copyable\u2019 is not a member of \u2018std\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before \u2018>\u2019 token\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: \u2018::value\u2019 has not been declared\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/dispatch_gemm_shape.h:23:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/gemmlowp.h:19,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_context.h:21,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:44,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,\r\nfrom tensorflow/lite/kernels/add.cc:15:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h: In member function \u2018void gemmlowp::BlockingCounter::Wait()\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h:203:9: error: \u2018sleep_for\u2019 is not a member of \u2018std::this_thread\u2019\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:33:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,\r\nfrom tensorflow/lite/kernels/add.cc:15:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h: At global scope:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:76:36: error: invalid pure specifier (only \u2018= 0\u2019 is allowed) before \u2018;\u2019 token\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:11: error: \u2018kMaxMulParamsAlignment\u2019 is not a type\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: ISO C++ forbids declaration of \u2018alignas\u2019 with no type [-fpermissive]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: expected \u2018;\u2019 at end of member declaration\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,\r\nfrom tensorflow/lite/kernels/add.cc:15:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function \u2018void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: \u2018is_trivially_copyable\u2019 is not a member of \u2018std\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before \u2018>\u2019 token\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: \u2018::value\u2019 has not been declared\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/add_n.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/add_n.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/arg_min_max.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/arg_min_max.o\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/assign_variable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/assign_variable.o\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/dispatch_gemm_shape.h:23:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/gemmlowp.h:19,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_context.h:21,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:44,\r\nfrom tensorflow/lite/kernels/arg_min_max.cc:23:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h: In member function \u2018void gemmlowp::BlockingCounter::Wait()\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h:203:9: error: \u2018sleep_for\u2019 is not a member of \u2018std::this_thread\u2019\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:33:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\nfrom tensorflow/lite/kernels/arg_min_max.cc:23:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h: At global scope:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:76:36: error: invalid pure specifier (only \u2018= 0\u2019 is allowed) before \u2018;\u2019 token\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:11: error: \u2018kMaxMulParamsAlignment\u2019 is not a type\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: ISO C++ forbids declaration of \u2018alignas\u2019 with no type [-fpermissive]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: expected \u2018;\u2019 at end of member declaration\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\nfrom tensorflow/lite/kernels/arg_min_max.cc:23:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function \u2018void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: \u2018is_trivially_copyable\u2019 is not a member of \u2018std\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before \u2018>\u2019 token\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: \u2018::value\u2019 has not been declared\r\n/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/audio_spectrogram.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/audio_spectrogram.o\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:402:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:465:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:526:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:1544:79: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:6092:73: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\ntensorflow/lite/tools/make/Makefile:311: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/activations.o' failed\r\nmake: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/activations.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/dispatch_gemm_shape.h:23:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/gemmlowp.h:19,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_context.h:21,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:44,\r\nfrom tensorflow/lite/kernels/audio_spectrogram.cc:24:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h: In member function \u2018void gemmlowp::BlockingCounter::Wait()\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h:203:9: error: \u2018sleep_for\u2019 is not a member of \u2018std::this_thread\u2019\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:33:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\nfrom tensorflow/lite/kernels/audio_spectrogram.cc:24:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h: At global scope:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:76:36: error: invalid pure specifier (only \u2018= 0\u2019 is allowed) before \u2018;\u2019 token\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:11: error: \u2018kMaxMulParamsAlignment\u2019 is not a type\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: ISO C++ forbids declaration of \u2018alignas\u2019 with no type [-fpermissive]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: expected \u2018;\u2019 at end of member declaration\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\nfrom /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\nfrom ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\nfrom ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\nfrom tensorflow/lite/kernels/audio_spectrogram.cc:24:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function \u2018void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: \u2018is_trivially_copyable\u2019 is not a member of \u2018std\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before \u2018>\u2019 token\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: \u2018::value\u2019 has not been declared\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:402:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:465:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:526:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:1544:79: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:6092:73: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\ntensorflow/lite/tools/make/Makefile:311: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/add.o' failed\r\nmake: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/add.o] Error 1\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:402:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:465:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:526:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:1544:79: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:6092:73: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\ntensorflow/lite/tools/make/Makefile:311: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/arg_min_max.o' failed\r\nmake: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/arg_min_max.o] Error 1\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:402:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:465:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:526:45: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:1544:79: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of \u2018void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from \u2018static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from \u2018static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from \u2018void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from \u2018void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from \u2018static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]\u2019\r\n./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from \u2018void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]\u2019\r\n./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:6092:73: required from here\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named \u2018LhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named \u2018RhsLayout\u2019 in \u2018using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>\u2019\r\ntensorflow/lite/tools/make/Makefile:311: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/audio_spectrogram.o' failed\r\nmake: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/audio_spectrogram.o] Error 1\r\nmake: Leaving directory '/tensorflow_src'", "comments": ["I used lower version glibc than guide document, and I guess that occurs the problem. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42731\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42731\">No</a>\n", "@emzei did you sovle this problem ?  I found newest version of respberry pi toolchian si 4.9.3. `std::is_trivially_copyable` is unimplemented", "I used Qt 5.13.2 it worked for me.\n\nTry to use it.\n\nOn Sat, Oct 10, 2020, 03:13 Eye <notifications@github.com> wrote:\n\n> @emzei <https://github.com/emzei> did you sovle this problem ? I found\n> newest version of respberry pi toolchian si 4.9.3.\n> std::is_trivially_copyable is unimplemented\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42731#issuecomment-706468857>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AHD3NAD62YV6YXAMZT4IWLTSJ67MHANCNFSM4QN6XQPQ>\n> .\n>\n", "Did anyone solve this? It is broken in the latest tensorflow/tensorflow:devel (dc2e612d3ea3). How does Qt solve this?\r\n ", "My solution was to comment out the offending static_assert in tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h.\r\nApparently the cross development toolchain that is loaded does not support std::is_trivially_copyable.\r\n", "I have the same issue  in the latest tensorflow/tensorflow:devel, trying to do cross-compile for pi Zero.\r\n\r\nComenting the static assert, does not work for me :(\r\n\r\nAny solutions??", "I had to rebuild my Linux VM before I could go back and retry this; sorry it took me two days to respond.  Here's the version of the cross-compiler I'm using:\r\n\r\narm-linux-gnueabihf-g++ (crosstool-NG crosstool-ng-1.22.0-88-g8460611) 4.9.3\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\n\r\nThere is only one instance of std::is_trivially_copyable that needs to be commented out in create_trmul_params.h.  Is your cross-compiler tool chain the same version as I'm using?  You'll have to ignore all the clang::fallthrough warnings that the compiler generates.\r\n\r\nI'm still fighting with tensorflow-lite and getting a Python wheel built for the Pi-Zero.  It's an arm6 architecture and the arm7 wheels that are available won't install, obviously.  Have you successfully installed the tensorflow lite python support on a Pi-Zero and verified it works?", "When you are using the latest version of a cross compiler for a very popular board (rpi3B) and tensorflow lite will not compile without a local code modification, then this issue is wrongly closed!!!!!"]}, {"number": 42730, "title": "Error building tensorflow.dll (Windows 10)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version: 2.3.0\r\n- Bazel version (if compiling from source): 3.1.0\r\n\r\n**Describe the problem**\r\nError building tensorflow.dll\r\n```\r\nERROR: C:/users/lotte/downloads/tensorflow-2.3.0/tensorflow/BUILD:663:1: Linking of rule '//tensorflow:tensorflow_framework.dll' failed (Exit 1127): link.exe failed: error executing command\r\n  cd C:/users/lotte/_bazel_lotte/6a53jthk/execroot/org_tensorflow\r\n  SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/lotte/AppData/Local/Programs/Python/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/lotte/AppData/Local/Programs/Python/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\lotte\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n    SET TMP=C:\\Users\\lotte\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/HostX64/x64/link.exe @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_framework.dll-2.params\r\nExecution platform: @local_execution_config_platform//:platform\r\nexternal\\mkl_windows\\lib\\libiomp5md.lib : fatal error LNK1127: library is corrupt\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nINFO: Elapsed time: 9542.848s, Critical Path: 619.14s\r\nINFO: 8184 processes: 8184 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\npython configure.py\r\n    empty\r\n    empty\r\n    n\r\n    n\r\n    empty\r\n    y\r\n    n\r\n\r\nbazel build -c opt --config=mkl //tensorflow/tools/lib_package:libtensorflow\r\n```\r\n\r\n**Edit: Possibly related to #42703 (Error building tensorflow_cc.dll)**", "comments": ["@Lotte1990 \r\nThis issue reported is duplicate of #42703 and it is tracked there, please move this to closed status as its tracked already.", "This is not a duplicate issue. This issue concerns tensorflow.dll (C API) and #42703 concerns tensorflow_cc.dll (C++ API). Please keep both issues open until resolved. Thanks!", "Definitely the same issue as #42703 it is the exact same library (mkl) failing to link.\r\nPossibly mkl build is not configured to run on windows. Closing as duplicate.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42730\">No</a>\n"]}]