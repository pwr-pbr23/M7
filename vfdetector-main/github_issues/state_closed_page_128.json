[{"number": 51113, "title": "Update keras.py", "body": "Changed some verbiage & formatting around", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51113) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "It looks like your PR relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. Thankyou.\r\n@fchollet, @qlzh727"]}, {"number": 51112, "title": "Add missing validation to `maxpooling_op.cc`", "body": "PiperOrigin-RevId: 387932441\r\nChange-Id: I43a0b24e6a12cc965611144ba035accd384594b9", "comments": []}, {"number": 51111, "title": "Prevent crash/heap OOB due to integer conversion to unsigned in NMS k\u2026", "body": "\u2026ernels\r\n\r\nPiperOrigin-RevId: 387938262\r\nChange-Id: Id361a715307e7179977cf5c64391c199a966f2ad", "comments": []}, {"number": 51110, "title": "Reorganize and add more validation to MKL requantization", "body": "PiperOrigin-RevId: 387901341\r\nChange-Id: I2515b9034c64e113db0bcec8337d30643ab0a0f1", "comments": []}, {"number": 51109, "title": "Fix nullptr deref and heap OOB access in binary cwise ops.", "body": "PiperOrigin-RevId: 387936777\r\nChange-Id: I608b8074cec36a982cca622b7144cb2c43e6e19f", "comments": []}, {"number": 51108, "title": "Add one missing valdiation to `matrix_set_diag_op.cc`", "body": "PiperOrigin-RevId: 387923408\r\nChange-Id: If6a97b9098c13879400f56c22f91555cdf0ce5d7", "comments": []}, {"number": 51107, "title": "Add missing validation to `matrix_diag_op.cc`", "body": "PiperOrigin-RevId: 387923533\r\nChange-Id: Idfffeb328d5f9c6748d992d28a56d6e9e45103a0", "comments": []}, {"number": 51106, "title": "Prevent heap oob access in `resource_variable_ops.cc`", "body": "PiperOrigin-RevId: 387936433\r\nChange-Id: I9e71ddaa8dbd51ec6afbf163a6b3b591f193b4f6", "comments": []}, {"number": 51105, "title": "Prevent division by 0 in `resource_variable_ops.cc`", "body": "PiperOrigin-RevId: 387939939\r\nChange-Id: Ib04902d63756633999959a70613f2eaa30c2c151", "comments": []}, {"number": 51104, "title": "Prevent use after free.", "body": "A very old version of the code used `result` as a simple pointer to a resource. Two years later, the pointer got changed to a `unique_ptr` but author forgot to remove the call to `Unref`. Three years after that, we finally uncover the UAF.\r\n\r\nPiperOrigin-RevId: 387924872\r\nChange-Id: I70fb6f199164de49fac20c168132a07b84903f9b", "comments": []}, {"number": 51103, "title": "Mm 4", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51103) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 51102, "title": "Ensure validation sticks in `save_restore_v2_ops.cc`", "body": "PiperOrigin-RevId: 387924206\r\nChange-Id: I6156842eb3230076b5812c0815f3e66bd5241454", "comments": []}, {"number": 51101, "title": "Prevent heap OOB in sparse reduction ops.", "body": "PiperOrigin-RevId: 387934524\r\nChange-Id: I894aa30f1e454f09b471d565b4a325da49322c1a", "comments": []}, {"number": 51100, "title": "Add remaining missing validation to `BoostedTreesCalculateBestFeatureSplit`", "body": "(replaces #51059 as that is partial)", "comments": []}, {"number": 51099, "title": "2.6.0rc1 regression: isinstance(layer, Layer) no longer works. Affects: `keras.layers.Wrapper`", "body": "Hello,\r\n  One of the CI tests in the [R interface to keras](https://github.com/rstudio/keras) surfaced this regression in 2.6.0rc1: \r\n  `tf.keras.layers.Wrapper` seemingly doesn't work the way it used to.\r\n  \r\nSpecifically, this assertion is being raised where it wasn't before:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8406c010e1793f458a9fd5d73404c1a93689bcfc/tensorflow/python/keras/layers/wrappers.py#L50\r\n\r\n\r\nTracking this down, it seems this is a minimal example in the change in behavior:\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.keras.engine.base_layer import Layer\r\nlayer = tf.keras.layers.Dense(1)\r\n\r\nisinstance(layer, Layer) # True with 2.5.0, False with 2.6.0rc1\r\n```\r\n    \r\n", "comments": ["@t-kalinowski Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n  Thank you!", "Moved issue to https://github.com/keras-team/keras/issues/15056 as requested.", "@t-kalinowski Thank you for your response ,closing this issue here as you have moved the issue to  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) .You will get the right help there,Thanks!\r\n"]}, {"number": 51098, "title": "AttributeError: '_UserObject' object has no attribute 'add_slot' ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below):v2.6.0-rc1-12-gb61c987109e 2.6.0-rc1\r\n- Python version:3.8.5 \r\n- Bazel version (if compiling from source):3.7.2\r\n- GCC/Compiler version (if compiling from source):7.5.0 \r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ngit checkout r2.6\r\nbuild the source (CPU only), install, generate mnist model, then try to convert it, has issue.\r\n\r\n1. install\r\nsudo apt install python3-dev python3-pip\r\npip install -U --user pip numpy wheel\r\npip install -U --user keras_preprocessing --no-deps\r\n\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\npip install /tmp/tensorflow_pkg/tensorflow-2.6.0rc1-cp38-cp38-linux_x86_64.whl\r\n\r\npip install tensorflow-model-optimization\r\n\r\n2. generate model: refer to below code\r\n3. convert:\r\nbazel-bin/tensorflow/lite/python/tflite_convert --saved_model_dir mnist_qat_notdocker/float_model/ --output_file oo\r\n\r\nerror message:\r\n...\r\n  File \"/home/tensorflow2/tensorflow/bazel-bin/tensorflow/lite/python/tflite_convert.runfiles/org_tensorflow/tensorflow/python/saved_model/load.py\", line 448, in _load_nodes\r\n    slot_variable = optimizer_object.add_slot(\r\nAttributeError: '_UserObject' object has no attribute 'add_slot'\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nimport tempfile\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n# Load MNIST dataset\r\nmnist = keras.datasets.mnist\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n# Normalize the input image so that each pixel value is between 0 to 1.\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\n# Define the model architecture.\r\nmodel = keras.Sequential([\r\n  keras.layers.InputLayer(input_shape=(28, 28)),\r\n  keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\r\n  keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n  keras.layers.Flatten(),\r\n  keras.layers.Dense(10)\r\n])\r\n\r\n# Train the digit classification model\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(\r\n  train_images,\r\n  train_labels,\r\n  epochs=1,\r\n  validation_split=0.1,\r\n)\r\n\r\nmodel.save('mnist_qat_notdocker/float_model')\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@tensorbuffer ,\r\nI was able to execute the given code without any error.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/19559c9ef0d9b598e6d0177e1af90bca/untitled51098.ipynb).Also please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/50659) with similar error.It helps.Thanks!", "@tilakrayal Thanks for the quick response. The problem happens when you convert, your link does not have the convert step.\r\nYes I have searched and read that issue you referred, in that case I think a custom OP is used, different than mine.\r\nI also tried to do a new conda environment and do everything there (checkout source, build pip package, install, run python to save model and convert). I had the same issue with branch 2.6, however 2.5 branch works. So I do believe there's some problem.", "Using the Python API seems to convert without problems. I think it might be a problem with **`tflite_convert`** built with Bazel.\r\nhttps://colab.research.google.com/drive/1uuAl1BP8v1R1xhcOyBIDyh9nkO8AW6xD?usp=sharing\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('mnist_qat_notdocker/float_model')\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nwith open('mnist_qat_notdocker/float_model/model_float32.tflite', 'wb') as w:\r\n    w.write(tflite_model)\r\n```", "Thanks. Ok I will try that. I remember that with python convert, I can set attributes like inference type, while in tflite_convert command line there's no such option. Wondering if they are the same.", "I remember that command line tools were deprecated a long time ago. They are probably rarely maintained.", "As mentioned by @PINTO0309 Python API is recommended for TF Lite converter,  command line tool support basic model conversion.\r\nhttps://www.tensorflow.org/lite/convert\r\nFeel free to reopen if running into issues using Python API for conversion. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51098\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51098\">No</a>\n", "I have same problem, in TF2.6. But, I fix it just need append\r\nfrom tensorflow.keras import Model\r\n\r\nhope it's useful for you."]}, {"number": 51097, "title": "Parameter description generating weirdly in `tf.keras.metrics.AUC`", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe description for `num_labels` is generating weirdly:\r\n![Screen Shot 2021-08-02 at 9 30 05 AM](https://user-images.githubusercontent.com/29757116/127869653-5cc9728d-9c29-4de2-9c85-f8452fb1eaa1.png)\r\n", "comments": ["This is fixed in tf-nightly version docs. See https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC?version=nightly#args"]}, {"number": 51096, "title": "Can't save custom layers to .h5 file !", "body": "<em>Can't save custom layers to .h5 file !</em>\r\n\r\n**Can't save !**\r\nI use tensorflow 2.4.1, this code works very well, but it can't save to .h5 file. I have tried Sequential() method, but it not working. I want to ask which approch should I adopt it. I have add get_config params, and which not working in my network that is very puzzle me. I don't know what cause this problem, can somebody tell me, thank you very much.\r\nPlease note that `tf.keras` code was moved entirely to\r\n```\r\nclass MultiSpectralAttentionLayer(layers.Layer):\r\ndef __init__(self, reduction=16, freq_sel_method='top16'):\r\n    super(MultiSpectralAttentionLayer, self).__init__()\r\n    self.reduction = 16\r\n    self.freq_sel_method = freq_sel_method\r\n\r\ndef build(self, input_shape):\r\n    _, h, w, c = input_shape\r\n    self.channel = int(c)\r\n    self.reduction = self.reduction\r\n    self.fc1 = layers.Dense(self.channel / self.reduction, use_bias=True, activation='relu')\r\n    self.fc2 = layers.Dense(self.channel, use_bias=True, activation='sigmoid')\r\n    self.reshapeTensor = tf.keras.layers.Reshape((1, 1, c))\r\n    self.mapper_x, self.mapper_y = self.get_freq_indices(self.freq_sel_method)\r\n    self.dynamic_weight = tf.Variable(self.get_dct_filter(h, w, self.mapper_x, self.mapper_y, channel=c))\r\n    self.mapper_x = [temp_x * (h // 7) for temp_x in self.mapper_x]\r\n    self.mapper_y = [temp_y * (2 // 7) for temp_y in self.mapper_y]\r\n\r\n    super().build(input_shape)\r\n\r\ndef get_freq_indices(self, methods):\r\n    assert methods in ['top1', 'top2', 'top4', 'top8', 'top16', 'top32',\r\n                       'bot1', 'bot2', 'bot4', 'bot8', 'bot16', 'bot32',\r\n                       'low1', 'low2', 'low4', 'low8', 'low16', 'low32']\r\n    num_freq = int(methods[3:])\r\n\r\n    if 'top' in methods:\r\n        all_top_indices_x = [0, 0, 6, 0, 0, 1, 1, 4, 5, 1, 3, 0, 0, 0, 3, 2, 4, 6, 3, 5, 5, 2, 6, 5, 5, 3, 3, 4, 2,\r\n                             2,\r\n                             6, 1]\r\n        all_top_indices_y = [0, 1, 0, 5, 2, 0, 2, 0, 0, 6, 0, 4, 6, 3, 5, 2, 6, 3, 3, 3, 5, 1, 1, 2, 4, 2, 1, 1, 3,\r\n                             0,\r\n                             5, 3]\r\n        mapper_x = all_top_indices_x[:num_freq]\r\n        mapper_y = all_top_indices_y[:num_freq]\r\n\r\n    elif 'low' in methods:\r\n        all_low_indices_x = [0, 0, 1, 1, 0, 2, 2, 1, 2, 0, 3, 4, 0, 1, 3, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 1,\r\n                             2,\r\n                             3, 4]\r\n\r\n        all_low_indices_y = [0, 1, 0, 1, 2, 0, 1, 2, 2, 3, 0, 0, 4, 3, 1, 5, 4, 3, 2, 1, 0, 6, 5, 4, 3, 2, 1, 0, 6,\r\n                             5,\r\n                             4, 3]\r\n\r\n        mapper_x = all_low_indices_x[:num_freq]\r\n        mapper_y = all_low_indices_y[:num_freq]\r\n\r\n    elif 'bot' in methods:\r\n        all_bot_indices_x = [6, 1, 3, 3, 2, 4, 1, 2, 4, 4, 5, 1, 4, 6, 2, 5, 6, 1, 6, 2, 2, 4, 3, 3, 5, 5, 6, 2, 5,\r\n                             5,\r\n                             3, 6]\r\n        all_bot_indices_y = [6, 4, 4, 6, 6, 3, 1, 4, 4, 5, 6, 5, 2, 2, 5, 1, 4, 3, 5, 0, 3, 1, 1, 2, 4, 2, 1, 1, 5,\r\n                             3,\r\n                             3, 3]\r\n        mapper_x = all_bot_indices_x[:num_freq]\r\n        mapper_y = all_bot_indices_y[:num_freq]\r\n    else:\r\n        raise NotImplementedError\r\n    return mapper_x, mapper_y\r\n\r\ndef get_dct_filter(self, tile_size_x, tile_size_y, mapper_x, mapper_y, channel):\r\n    dct_numpy_filter = np.zeros(shape=(tile_size_y, tile_size_x, channel), dtype=np.float32)\r\n    c_part = channel // len(mapper_x)\r\n\r\n    for i, (u_x, u_y) in enumerate(zip(mapper_x, mapper_y)):\r\n        for t_x in range(tile_size_x):\r\n            for t_y in range(tile_size_y):\r\n                dct_numpy_filter[t_y, t_x, i * c_part: (i + 1) * c_part] = self.build_filter(t_x, u_x, tile_size_x) * \\\r\n                                                                           self.build_filter(t_y, u_y, tile_size_y)\r\n    return dct_numpy_filter\r\n\r\n\r\ndef build_filter(self, pos, frequency, POS):\r\n    result = np.cos(np.pi * frequency * (pos + 0.5) / POS) / np.sqrt(POS)\r\n    if frequency == 0:\r\n        return result\r\n    else:\r\n        return result * np.sqrt(2)\r\n\r\n\r\ndef get_config(self):\r\n    base_config = super(MultiSpectralAttentionLayer, self).get_config()\r\n    config = {\r\n        \"reduction\": self.reduction,\r\n        \"freq_sel_method\": self.freq_sel_method,\r\n    }\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n\r\n\r\n@tf.function\r\ndef call(self, inputs, training=None):\r\n    x = inputs * self.dynamic_weight\r\n    result = tf.math.reduce_sum(x, axis=[1, 2], keepdims=True)\r\n    x = self.fc1(result)\r\n    x = self.fc2(x)\r\n    x = self.reshapeTensor(x)\r\n    return inputs * x\r\n```\r\n\r\n**I use the following code to inference images, but it not working.**\r\n\r\n```\r\n    def load_trained_model():\r\n    _custom_objects = {\r\n    \"SeBlock\" : SeBlock,\r\n    \"MultiSpectralAttentionLayer\": MultiSpectralAttentionLayer,\r\n}\r\n    model_name = r\"F:\\log\\ablation\\model.01-3.4231-1.h5\"  \r\n    function_model = load_model(model_name, custom_objects=_custom_objects)\r\n    print('model load success')\r\n    return function_model\r\n```\r\n\r\nThe error message is \" init() got an unexpected keyword argument 'name' \". I have assigned names to this custom layers each times.\r\n\r\nThe code is re-implement FcaNet -->**https://github.com/cfzd/FcaNet/blob/aa5fb63505575bb4e4e094613565379c3f6ada33/model/layer.py#L29**.\r\n", "comments": ["@jiayugedede Could you please refer to the [link](https://www.tensorflow.org/guide/keras/save_and_serialize) having documentation on save and load keras models and these similar issues[ link1](https://stackoverflow.com/questions/64337087/typeerror-init-got-an-unexpected-keyword-argument-name-when-loading-a-m), [link2](https://stackoverflow.com/questions/62280161/saving-keras-models-with-custom-layers) mentioned  in stackoverflow and let us know if it helps ? Thank you!", "> @jiayugedede Could you please refer to the [link](https://www.tensorflow.org/guide/keras/save_and_serialize) having documentation on save and load keras models and these similar issues[ link1](https://stackoverflow.com/questions/64337087/typeerror-init-got-an-unexpected-keyword-argument-name-when-loading-a-m), [link2](https://stackoverflow.com/questions/62280161/saving-keras-models-with-custom-layers) mentioned in stackoverflow and let us know if it helps ? Thank you!\r\n\r\nOk, will try it.", "@jiayugedede Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "> @jiayugedede Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\n> To know more see;\r\n> https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n> Thank you!\r\n\r\nThank you, your suggestion sloved my problem, thank you very much.", "@jiayugedede Can you please let us know if we can close this issue as it has been resolved? Thanks!", "> @sushreebarsa Yes, you can close this issue, your suggestion is working to me. Thank you very much.", "> @jiayugedede Can you please let us know if we can close this issue as it has been resolved? Thanks!\r\n\r\nYes, you can close this issue, your suggestion is working to me. Thank you very much.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51096\">No</a>\n"]}, {"number": 51095, "title": "Rollforward \"PR #50519: GPU Symmetric Fake Quantization\"", "body": "This PR re-applies #50519 which has been rolled back e4f883065966d41ce6e04061100c3c5d8e0eec7a due to internal CI failures.\r\n\r\nI am resubmitting following https://github.com/tensorflow/tensorflow/pull/50768#issuecomment-886893162 to make sure this doesn't get lost. @Xhark do you have any updates on the internal test failures?", "comments": ["@pkanwar23 Can you please review this PR ? Thanks!", "@Xhark @gbaned Any news on this?", "@Xhark Can you please review this PR ? Thanks!", "@Xhark Can you please review this PR ? Thanks!", "@lgeiger Can you please resolve conflicts? Thanks!", "These changes have been rolled forward in 61290927a200e6165983862c52f917df72cfe9ec, so we can close this PR."]}, {"number": 51094, "title": "Fix bug in GpuRadixSort for SparseSegmentReduceGrad", "body": "This works around an [issue](https://github.com/NVIDIA/cub/issues/353) in CUB `DeviceRadixSort` where it fails when `begin_bit = end_bit = 0`.\r\n\r\nThis should fix the known bug in `SparseSegmentReduceGrad`, and thus re-enables it (please confirm with internal models before merging).\r\n(The PR also optimizes `SparseSegmentReduceGrad` for this case).\r\n\r\nAlso adds relevant tests.\r\n\r\ncc @nluehr @sanjoy @reedwm ", "comments": []}, {"number": 51093, "title": "Conv2DTranspose in Hexagon delegate crash", "body": "**System information**\r\n\r\n* Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOS, Ubuntu...\r\n* TensorFlow installed from (source or binary):\r\nsource\r\n* Tensorflow version (commit SHA if source):\r\nTag v2.5.0\r\n* Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\nAndroid\r\n\r\n**Describe the problem**\r\n\r\nWhen I run my tflite quantized model on my mobile device's DSP, I get a crash:\r\n\r\n`ERROR: Bias/channel scales number mismatch for bias tensor: model/conv2d_3/BiasAdd;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d_3/Conv2D`\r\n`Segmentation fault`\r\n\r\nThe crash occurs in Hexagon delegate, in the recently added code related to Transposed Convolution with bias.\r\nThe crash does NOT occur if I set use_bias=False to Conv2DTranspose\r\n\r\nThe error log comes from the ProcessPerChannelQuantizedBias() method in conv_2d_helpers.cc file.\r\n\r\nSee attached a small sample code that creates a model reproducing the crash.\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/6916360/model.zip)\r\n\r\n`inputs = keras.layers.Input(shape=(128, 128, 1))`\r\n`x = keras.layers.Conv2D(96, kernel_size=2, strides=2, padding='valid')(inputs)`\r\n`x = keras.layers.Conv2D(96, kernel_size=2, strides=2, padding='valid')(x)`\r\n`x = keras.layers.Conv2D(96, kernel_size=2, strides=2, padding='valid')(x)`\r\n`x = keras.layers.Conv2D(96, kernel_size=2, strides=2, padding='valid')(x)`\r\n`outputs = keras.layers.Conv2DTranspose(16, kernel_size=2, strides=2, use_bias=True)(x)`\r\n`model = keras.Model(inputs=inputs, outputs=outputs)`\r\n\r\nThe crash can be easily reproduced with the benchmark tool:\r\n\r\n`> benchmark_model --graph=model.tflite --input_layer=normalized_input_image_tensor --input_layer_shape=1,128,128,1 --warmup_runs=100 --num_runs=1000 --run_delay=0 --num_threads=4 --use_hexagon=true --enable_op_profiling=false`\r\n\r\n`STARTING!`\r\n`Log parameter values verbosely: [0]`\r\n`Min num runs: [1000]`\r\n`Inter-run delay (seconds): [0]`\r\n`Num threads: [4]`\r\n`Min warmup runs: [100]`\r\n`Graph: [model.tflite]`\r\n`Input layers: [normalized_input_image_tensor]`\r\n`Input shapes: [1,128,128,1]`\r\n`Enable op profiling: [0]`\r\n`#threads used for CPU inference: [4]`\r\n`Use Hexagon: [1]`\r\n`Loaded model model.tflite`\r\n`INFO: Initialized TensorFlow Lite runtime.`\r\n`INFO: TfLiteHexagonDelegate delegate: 5 nodes delegated out of 5 nodes with 1 partitions.`\r\n\r\n`ERROR: Bias/channel scales number mismatch for bias tensor: \r\nmodel/conv2d_3/BiasAdd;model/conv2d/BiasAdd/ReadVariableOp/resource;model/conv2d_3/Conv2D`\r\n`Segmentation fault`\r\n", "comments": ["@srjoglekar246 could you take a look at this?", "Hi, any workaround to propose in the meanwhile ? Thanks.", "@twkx  Can you share how you quantized the model?", "(Maybe not the actual data you used to quantize, but just the code snippet)", "> @twkx Can you share how you quantized the model?\r\n\r\nThe full code is in the zip file attached in issue description and runs in standalone.", "> (Maybe not the actual data you used to quantize, but just the code snippet)\r\n\r\nSee the model.py file in the model.zip file attached previously.\r\n\r\nThanks for the help", "This is an issue on our side, I will ping back here once I have a fix.", "My latest [commit](https://github.com/tensorflow/tensorflow/commit/eed7a20e5e247d70951990fea764e8147148d407) fixes this. @twkx can you try running the benchmark from a nightly version that includes this fix?", "@srjoglekar246 I confirm you that your commit fixes the crash in my test model and in my true model.\r\n\r\nThanks a lot for the effort!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51093\">No</a>\n"]}, {"number": 51091, "title": "How to collect code coverage of C++ code when running Python tests?", "body": "I am trying to figure out the way to get the code coverage of C++ code for tests written in Python. I know that `coverage` is the package to measure code coverage of Python code for tests that run in Python.\r\nAlso, it appears to me that `bazel` is the build system for C++ projects, and `bazel coverage` can collect the C++ code coverage for C++ tests.\r\n\r\nIf there is anything wrong with my previous findings, please feel free to point out!\r\n\r\nI am currently looking for the way to collect the code coverage of C++ when running Python code. Is there any potential ways? Thanks!", "comments": ["I have tried some commands like this after compiling tensorflow from source:\r\n```\r\nexport flags=\"--config=opt --collect_code_coverage=true\"\r\nbazel test ${flags} tensorflow/python/kernel_tests:softmax_op_test\r\n```\r\nThe tests under `tensorflow/python/kernel_tests` are indeed python tests.\r\nHowever, an issue arise:\r\n```\r\nFAIL: //tensorflow/python/kernel_tests:softmax_op_test (see SOME_PATH_HERE/288208e8edcff60b9143f8b7da1d43aa/execroot/org_tensorflow/\r\nbazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/softmax_op_test/test.log)\r\nERROR: tensorflow/python/kernel_tests/BUILD:2467:1: output 'tensorflow/python/kernel_tests/softmax_op_test/coverage.dat' was not created\r\nERROR: tensorflow/python/kernel_tests/BUILD:2467:1: not all outputs were created or valid\r\n```\r\n\r\nAm I running the correct command for coverage collection? Thanks!\r\n", "@sturmianseq \r\nCan you please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/18839) and let us know. [[link](https://github.com/tensorflow/tensorflow/issues/28105)]", "Thanks for your reminding!\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: Tensorflow 1.14.0 (branch/tag1.14.0, from git)\r\n- Python version: 3.7\r\n- Installed using virtualenv? conda\r\n- Bazel version (if compiling from source): Bazel 0.24.1 (as indicated in https://www.tensorflow.org/install/source?hl=uk for tf1.14.0)\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: N/A (only CPU version, without CUDA support)\r\n- GPU model and memory: N/A (only CPU version, without CUDA support)", "I have also tried commands from https://github.com/tensorflow/tensorflow/issues/46477 (`bazel coverage -s --instrument_test_targets --coverage_report_generator=@bazel_tools//tools/test:coverage_report_generator --coverage_support=@bazel_tools//tools/test:coverage_support --collect_code_coverage --jobs 5 //tensorflow`), but it simply didn't work.", "> Thanks for your reminding!\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS\r\n> * TensorFlow installed from (source or binary): source\r\n> * TensorFlow version: Tensorflow 1.14.0 (branch/tag1.14.0, from git)\r\n> * Python version: 3.7\r\n> * Installed using virtualenv? conda\r\n> * Bazel version (if compiling from source): Bazel 0.24.1 (as indicated in https://www.tensorflow.org/install/source?hl=uk for tf1.14.0)\r\n> * GCC/Compiler version (if compiling from source): 4.8.5\r\n> * CUDA/cuDNN version: N/A (only CPU version, without CUDA support)\r\n> * GPU model and memory: N/A (only CPU version, without CUDA support)\r\n\r\nCan you confirm if you have upgraded tf version as there is no active support for 1.x now.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51091\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51091\">No</a>\n", "I'm sorry. I wonder if there is any resolution on this particular issue?"]}, {"number": 51090, "title": "Single scalar ground truth for a batch?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.5.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen calling `model.fit(X)` where `X` is a generator giving the input and ground truth of batches, the ground truth has to be a vector (otherwise it emits error). However, in some case, I want to train a model with a single scalar ground truth for a batch. I would like to aggregate the prediction of the batch into a single scalar and compare it with the single scalar ground truth.  \r\n\r\n**Will this change the current api? How?**\r\nIt would make the API more permissible. `fit()` would allow the ground truth for a batch to be a single scalar.\r\n\r\n**Who will benefit with this feature?**\r\nThose want to train a model with a single scalar ground truth for a batch. \r\n\r\n**Any Other info.**\r\nI don't see why this is not allowed. The ground truth is only used in loss calculation, and it should be fine to be a single scalar as long as the user loss function can handle that.\r\nCurrently I simply zero-pad `y_true` into a vector and only use `y_true[0]` in loss calculation.", "comments": ["@riaqn,\r\nCan you please explain the Use Case where you think this feature would be useful? Thanks! ", "@rmothukuru Thank you for taking care of this. I'm training a model that inputs image and outputs some rating of that image. To augment my datasets, instead of labeling each image with a rating, I label a video excerpt with a single rating.  In a single batch, images will be frames from a single video excerpt, and the ground truth will be a single scalar representing the rating of that video excerpt. In `loss` calculation, I would do some sophisticated calculation (not just `reduce_mean`) to aggregate the `y_pred` of each frame into a single scalar,  and compare that with `y_true` which is supposed to be a single scalar.", "@riaqn Can you please share a simple standalone code to reproduce the issue or add more details about your use case? Are you going to have a batch of data with single label? Are not shuffling the data? I am not sure how it is going to perform.\r\n\r\nIf you add little more details and a small reproducible code, it would help us understand the feature better.\r\n\r\nIf there is any actionable PRs, please feel free to open them in [keras-team/keras](https://github.com/keras-team/keras/issues) repository. \r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus entirely on only keras. Thanks! ", "@jvishnuvardhan Thank you for the reply. Unfortunately I have switched to pytorch now. Please feel free to close the issue. Thank you."]}, {"number": 51089, "title": "16x8 quantization mode & TFLite Micro", "body": "### 1. System information\r\n\r\n- OS Platform Ubuntu 20.04\r\n- TensorFlow installation : pip package\r\n- TensorFlow library 2.5.0\r\n\r\nHi, \r\nI'm using 16x8 experimental quantization on my model.\r\nI convert my model after I follow instructions on the tflite 16x8 info page, i also set:\r\nconverter_fp.inference_input_type = tf.int16\r\nconverter_fp.inference_output_type = tf.int16\r\n\r\nWhen I invoke the converted model from Jupyter Notebook everything works fine.\r\n\r\nWhen I transfer the converted model to TFLite Micro then I get the following error:\r\n../src/tensorflow/lite/micro/kernels/fully_connected.cc Hybrid models are not supported on TFLite Micro.\r\ninput->type is kTfLiteInt16\r\nfilter->type is kTfLiteInt8\r\n\r\nIs there a way to make this work? And if not, when is this functionality expected to be available on TFLite Micro?\r\n\r\nThanks and Best Regards,\r\nAlex\r\n", "comments": ["@kozlova666 could you post this feature request at the TFLite micro project(https://github.com/tensorflow/tflite-micro) instead?"]}, {"number": 51088, "title": "RTX30 series: unsupported compute compability for CUDA 9.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- TensorFlow installed from: source \r\n- TensorFlow version: 1.10\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: building, not installed yet\r\n- Bazel version: 0.15\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- CUDA/cuDNN version: 9.0/7.1.2\r\n- GPU model and memory: RTX3060*4, 64GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to build TensorFlow 1.10 from source, with compute compatibility of 3060 assigned. The building process failed at : \r\n```\r\nERROR: /home/yulab/packages_software/tensorflow-r1.10/tensorflow/core/kernels/BUILD:3699:1:error while parsing .d file: /home/yulab/.cache/bazel/_bazel_yulab/0ab03e3c981f656d0173f64ae90fcc89/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/core/kernals/_objs/batch_space_ops_gpu/tensorflow/core/kernels/spacetobatch_functor_gpu.cu.pic.d (No such file or directory)\r\nnvcc fatal: unsupported GPU architecture 'compute_86'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nI manually assigned compute compatibility of 3060 in compile options,  but seems CUDA 9.0 does not support it. And I can't update TF to 2.x as  my application specified TF 1.10. \r\nCan this error be fixed by changing parameters? Or I have to give up GPU support?\r\n", "comments": ["@Neutrino0532 We are not actively supporting TF 1.x , as it is officially considered as end of life ,we recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions ? Could you please have a look at the [link 1](https://www.tensorflow.org/install/gpu?hl=eng), [link 2](https://www.tensorflow.org/install/source) , [link 3](https://www.tensorflow.org/guide/migrate)  for your reference and let us know if it helps ? Thank you!  ", "> @Neutrino0532 We are not actively supporting TF 1.x , as it is officially considered as end of life ,we recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions ? Could you please have a look at the [link 1](https://www.tensorflow.org/install/gpu?hl=fr), [link 2](https://www.tensorflow.org/install/source) , [link 3](https://www.tensorflow.org/guide/migrate) for your reference and let us know if it helps ? Thank you!\r\n\r\nI referenced the link 1&2 before compile. The application that uses TensorFlow is developed by another team, so I can't change the code as I do not know it well. So these links didn't help much.  But still, thank you for your quick response!", "@Neutrino0532 Could you please have a look on this[ link](https://www.tensorflow.org/install/source#gpu),and let us know if it helps? Thanks!", "@sushreebarsa I also referenced this link before compile,  to decide the versions of my building tools. I think my building tools are in the correct version, as I have listed in the system information. \r\nI'd like to discuss if it's possible to assign a lower compute compatibility so that compile will continue, despite the compute compatibility is not supported by my graphics card. And how will the new build behave under 3060 device. Because official builds without assigning compute compatibility will freeze for a long time before graphic cards start to work. Do you have any idea? Thanks!\r\n\r\n", "@Neutrino0532 TF 1.10 is obsolete and out of support window. Have you considered installing tf 1.10 pip packages instead? Also is it possible to try latest TF 1.15 version if not TF 2.X? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51088\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51088\">No</a>\n"]}, {"number": 51087, "title": "Updated README.md", "body": "Corrected Grammar", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51087) for more info**.\n\n<!-- need_sender_cla -->", "@KukretiShubham  Can you please sign CLA. Thanks!", "> @KukretiShubham Can you please sign CLA. Thanks!\r\n\r\nHey, @gbaned I did that already. \r\nHere I signed : https://cla.developers.google.com/clas\r\nCorrect me if I am wrong.\r\nThank you", "@KukretiShubham \r\n\r\n>Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.", "@googlebot  I signed it!", "I'm sorry. It seems that the label has already been OK'd in response to my comment.", "@mihaimaruseac I will look for more issues. I am beginner to tensorflow. Is there any community forum or channel where I can learn or get more insights on how i can contribute to TensorFlow. \r\nThanks ", "https://discuss.tensorflow.org/ is the best forum.\r\n\r\nIf you want to fix typos, please fix typos in an entire directory, instead of just 1-2"]}, {"number": 51086, "title": "Does TFLite Support conv + LSTM ?", "body": "https://github.com/tensorflow/tensorflow/issues/49381 - \r\n\r\nI have raised the above issue for getting data formats,\r\n\r\nHere, it is mentioned that N, S, D is only supported format in tflite .\r\n\r\nNeed, confirmation whether conv + lstm is supported or not in TFLite ?\r\n\r\nUsing TFLite version 2.4.1.", "comments": ["@MeghnaNatraj @renjie-liu could you take a look at this question?", "Yes, it's supported.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I would try to create a script, How to know which are supported? Its not listed in tflite operator support .", "TFLite is able to support models that are created using the high level tf.keras.*  APIs and the low level tf.* APIs. This means that layers such as Conv and LSTM are supported. Please see [this documentation](tensorflow.org/lite/convert) for more information.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51086\">No</a>\n"]}, {"number": 51085, "title": "add missing dependencies for ChloPasses", "body": "I tried to build the IREE project acording to the docs at https://google.github.io/iree/building-from-source/getting-started/.\r\n\r\nDuring compilation I got the following error:\r\n```\r\nScanning dependencies of target obj.ChloPasses\r\n[ 74%] Building CXX object third_party/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.ChloPasses.dir/chlo_legalize_to_hlo.cc.o\r\n[ 74%] Building CXX object third_party/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.ChloPasses.dir/chlo_legalize_to_hlo_pass.cc.o\r\nIn file included from /home/stella/iree_self_build/iree/third_party/mlir-hlo/lib/Dialect/mhlo/transforms/chlo_legalize_to_hlo_pass.cc:18:\r\n/home/stella/iree_self_build/iree/third_party/mlir-hlo/include/mlir-hlo/Dialect/mhlo/transforms/PassDetail.h:32:10: fatal error: mlir-hlo/Dialect/mhlo/transforms/mhlo_passes.h.inc: No such file or directory\r\n   32 | #include \"mlir-hlo/Dialect/mhlo/transforms/mhlo_passes.h.inc\"\r\n      |          ^~~~~~~~~~~~\r\ncompilation terminated.\r\nmake[2]:  [third_party/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.ChloPasses.dir/build.make:76: third_party/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.ChloPasses.dir/chlo_legalize_to_hlo_pass.cc.o] Error 1\r\nmake[1]:  [CMakeFiles/Makefile2:51799: third_party/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.ChloPasses.dir/all] Error 2\r\nmake: *** [Makefile:163: all] Error 2\r\n```\r\n\r\nI added the lines:\r\n```\r\nMLIRMhloPassIncGen\r\nMLIRLmhloPassIncGen\r\nMLIRDiscRalPassIncGen\r\n```\r\nto get rid of all the errors. After I fixed the error above I got new errors that why the second and third line needed to be included as well.\r\n\r\n", "comments": []}, {"number": 51082, "title": "NotImplementedError: Cannot convert a symbolic Tensor (digit_capsule_layer_2/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported", "body": "tensorflow - 2.5.0\r\nnumpy - 1.21.1\r\npython - 3.8 \r\nCUDA - 11.2\r\nCuDNN - 8.1\r\n\r\nMany other issues related to it suggested downgrading the NumPy version to 1.19.5, after downgrading the NumPy version, unfortunately, it still doesn't work. \r\n", "comments": ["@yashkarbhari ,\r\n\r\n In order to expedite the trouble-shooting process, could you please provide a complete code to reproduce the issue.Thanks!", "Also Could you please try to test your code again in new virtual environment and let us know if you are facing same issue. Thanks!", "> Also Could you please try to test your code again in new virtual environment and let us know if you are facing same issue. Thanks!\r\n\r\nI'm currently working in a virtual environment, and it shows the same issue", "> @yashkarbhari ,\r\n> \r\n> In order to expedite the trouble-shooting process, could you please provide a complete code to reproduce the issue.Thanks!\r\n\r\n```\r\nclass DigitCapsuleLayer(Layer):\r\n    # creating a layer class in keras\r\n    def __init__(self, **kwargs):\r\n        super(DigitCapsuleLayer, self).__init__(**kwargs)\r\n        \r\n    \r\n    def build(self, input_shape): \r\n        # initialize weight matrix for each capsule in lower layer\r\n        self.W = self.add_weight(shape = [2, 6*6*6*32, 16, 8], name = 'weights')\r\n        self.built = True\r\n    \r\n    def call(self, inputs):\r\n        inputs = tf.expand_dims(inputs, 1)\r\n        inputs = tf.tile(inputs, [1, 2, 1, 1])\r\n        # matrix multiplication b/w previous layer output and weight matrix\r\n        inputs = tf.map_fn(lambda x: tf.keras.backend.batch_dot(x, self.W, [2, 3]), elems=inputs)\r\n        b = tf.zeros(shape = [tf.shape(inputs)[0], 2, 6*6*6*32])\r\n        \r\n        # routing algorithm with updating coupling coefficient c, using scalar product b/w input capsule and output capsule\r\n        for i in range(3-1):\r\n            c = tf.nn.softmax(b, dim=1)\r\n            s = tf.keras.backend.batch_dot(c, inputs, [2, 2])\r\n            v = squash(s)\r\n            b = b + tf.keras.backend.batch_dot(v, inputs, [2,3])\r\n            \r\n        return v \r\n    def compute_output_shape(self, input_shape):\r\n        return tuple([None, 10, 16])\r\n\r\ndef output_layer(inputs):\r\n    return tf.sqrt(tf.sum(tf.square(inputs), -1) + tf.epsilon())\r\n\r\ndigit_caps = DigitCapsuleLayer()(squashed_output)\r\noutputs = Lambda(output_layer)(digit_caps)\r\n\r\n```\r\n", "@yashkarbhari ,\r\nOn running the given code snippet, I am facing an error stating **NameError: name 'Layer' is not defined** Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/0cbd77fd94b232b99b09b89504da1884/untitled51082.ipynb).\r\n \r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.Thanks!\r\n\r\n\r\n\r\n", "> @yashkarbhari ,\r\n> On running the given code snippet, I am facing an error stating **NameError: name 'Layer' is not defined** Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/0cbd77fd94b232b99b09b89504da1884/untitled51082.ipynb).\r\n> \r\n> In order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.Thanks!\r\n\r\nAs my work is a research project, I'm not sure if I can show the complete code to reproduce the issue. Also for the **NameError**\r\nLayer is equal to tensorflow.keras.layers.Layer I had imported it earlier as Layer.", "@yashkarbhari ,\r\n\r\nWithout the reproducible code, it would be difficult for us to debug the issue. If you cannot simplify the code,please consider posting this issue in Stackoverflow or [tensorflow discussion forum](https://discuss.tensorflow.org/) where there is a large community to help and support each other. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51082\">No</a>\n"]}, {"number": 51081, "title": "dnn implementation Error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code:\r\n- System: Windows 10\r\n- TensorFlow installed from (source or binary): Described below\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.2 / cuDNN 8.1 \r\n- GPU model and memory: NVIDIA GeForce RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nI am getting an error..\r\nUnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]\r\nafter I installed tensorflow-text and no matter what I do i cant get rid of it. \r\nHere is exactly what happened...\r\n\r\n\r\n\r\n## FIRST INSTALLATION\r\n\r\nIn Anaconda Navigator under Environment install\r\n\t-keras = 2.4.3\r\n\t-keras-gpu= 2.4.3\r\n\r\ntried running program (code below) got the following error. \r\nNotImplementedError: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n\r\nIn Anaconda Navigator under Environment downgrade Numpy 1.2 with...\r\n\t-numpy = 1.19.2\r\n\t-numpy-base = 1.19.2\r\n\r\nran program and it WORKED\r\n\r\nIn Anaconda Prompt i did\r\n    conda install pandas\r\n    conda install matplotlib\r\nran PROGRAM in jupyter notebook; still works\r\n\r\nIn Anaconda Prompt i did\r\n    pip install tensorflow_hub and got the following error when trying to import in temp file. \r\n    ImportError: cannot import name 'parameter_server_strategy_v2' from 'tensorflow.python.distribute' \r\n    (C:\\Users\\shapi\\Anaconda3\\envs\\temp1\\lib\\site-packages\\tensorflow\\python\\distribute\\__init__.py)\r\n    pip uninstall tensorflow_hub\r\n    conda install tensorflow-hub\r\nran PROGRAM in jupyter notebook; still works\r\n\r\nIn Anaconda Prompt i did\r\n     python -c \"import tensorflow as tf;print(tf.__version__)\"\r\n     2.5.0\r\n     pip install --user tensorflow_text==2.5.0\r\n\r\nTRIED TO RUN PROGRAM AND GOT ERROR\r\nUnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]\r\n\r\n## Shutdown computer and Tried to make a new environment.\r\n\r\nin Navigator create conda environment python == 3.8\r\nIn Anaconda Navigator under Environment install\r\n\t-keras = 2.4.3\r\n\t-keras-gpu= 2.4.3\r\nIn Anaconda Navigator under Environment downgrade\r\n\t-numpy = 1.19.2\r\n\t-numpy-base = 1.19.2\r\nIn Anaconda Prompt\r\nconda install tensorflow-hub\r\npython -c \"import tensorflow as tf;print(tf.__version__)\"\r\n2.5.0\r\nTRIED TO RUN PROGRAM AND GOT ERROR\r\nUnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]\r\n\r\nThe problem is at this point the PROGRAM worked fine in the first installation. I have checked my path variables and they didn't change. I tried adding allow_growth = TRUE and that didn't help. I am completely stuck I have no idea what happened and why even a regular installation wont work anymore. \r\n\r\n\r\n## PROGRAM \r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nimport time\r\nfrom tensorflow import keras\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\r\n\r\ndef build_model1():\r\n    macro_data = tf.keras.Input(shape=(None, 3)) \r\n    \r\n    whole_seq_output, final_memory_state, final_carry_state = layers.LSTM(16,\r\n                                                                          dropout=.95,\r\n                                                                          input_shape=(None,3),\r\n                                                                          return_sequences=True, \r\n                                                                          return_state=True)(macro_data)   \r\n    \r\n    SDF_Network = tf.keras.Model(\r\n                            inputs=[macro_data],\r\n                            outputs=[whole_seq_output],\r\n                            name=\"SDF_Network\"\r\n                            )         \r\n    \r\n    return SDF_Network\r\n\r\ntemp = np.array([[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]]])\r\nSDF_Network = build_model1()\r\nSDF_Network([temp])\r\n```\r\n\r\n## Full Error\r\n\r\n```\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-4-da1b52ecaec0> in <module>\r\n      1 import numpy as np\r\n      2 SDF_Network = build_model1()\r\n----> 3 SDF_Network([temp])\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n   1028         with autocast_variable.enable_auto_cast_variables(\r\n   1029             self._compute_dtype_object):\r\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\r\n   1031 \r\n   1032         if self._activity_regularizer:\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py in call(self, inputs, training, mask)\r\n    418         a list of tensors if there are more than one outputs.\r\n    419     \"\"\"\r\n--> 420     return self._run_internal_graph(\r\n    421         inputs, training=training, mask=mask)\r\n    422 \r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py in _run_internal_graph(self, inputs, training, mask)\r\n    554 \r\n    555         args, kwargs = node.map_arguments(tensor_dict)\r\n--> 556         outputs = node.layer(*args, **kwargs)\r\n    557 \r\n    558         # Update tensor_dict.\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n    666 \r\n    667     if initial_state is None and constants is None:\r\n--> 668       return super(RNN, self).__call__(inputs, **kwargs)\r\n    669 \r\n    670     # If any of `initial_state` or `constants` are specified and are Keras\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n   1028         with autocast_variable.enable_auto_cast_variables(\r\n   1029             self._compute_dtype_object):\r\n-> 1030           outputs = call_fn(inputs, *args, **kwargs)\r\n   1031 \r\n   1032         if self._activity_regularizer:\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py in call(self, inputs, mask, training, initial_state)\r\n   1257           # GPU implementation when GPU is available.\r\n   1258           if can_use_gpu:\r\n-> 1259             last_output, outputs, new_h, new_c, runtime = gpu_lstm(\r\n   1260                 **gpu_lstm_kwargs)\r\n   1261           else:\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py in gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths)\r\n   1509       # Reverse axis 0 since the input is already convert to time major.\r\n   1510       inputs = array_ops.reverse(inputs, axis=[0])\r\n-> 1511     outputs, h, c, _ = gen_cudnn_rnn_ops.CudnnRNN(\r\n   1512         input=inputs, input_h=init_h, input_c=init_c, params=params,\r\n   1513         is_training=True, rnn_mode='lstm')\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\tf_export.py in wrapper(*args, **kwargs)\r\n    402           'Please pass these args as kwargs instead.'\r\n    403           .format(f=f.__name__, kwargs=f_argspec.args))\r\n--> 404     return f(**kwargs)\r\n    405 \r\n    406   return tf_decorator.make_decorator(f, wrapper, decorator_argspec=f_argspec)\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\r\n     96       pass\r\n     97     try:\r\n---> 98       return cudnn_rnn_eager_fallback(\r\n     99           input, input_h, input_c, params, rnn_mode=rnn_mode,\r\n    100           input_mode=input_mode, direction=direction, dropout=dropout,\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py in cudnn_rnn_eager_fallback(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)\r\n    176   \"direction\", direction, \"dropout\", dropout, \"seed\", seed, \"seed2\", seed2,\r\n    177   \"is_training\", is_training)\r\n--> 178   _result = _execute.execute(b\"CudnnRNN\", 4, inputs=_inputs_flat,\r\n    179                              attrs=_attrs, ctx=ctx, name=name)\r\n    180   if _execute.must_record_gradient():\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n\r\nUnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]\r\n```", "comments": ["SOLUTION\r\n\r\nfor some reason re-installing cudnn worked. \r\n\r\nconda install -c conda-forge cudnn", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51081\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51081\">No</a>\n"]}]