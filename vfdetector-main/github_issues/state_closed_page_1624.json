[{"number": 4190, "title": "ERROR: undeclared inclusion(s) in rule '//tensorflow/core/kernels:multinomial_op_gpu'", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nnone\n### Environment info\n\nOperating System:\n\nUbuntu 16.04 64bit with all updates applied\n\nInstalled version of CUDA and cuDNN: \n\nI've downloaded these from Nvidia and installed them per \"official\" instructions:\n- cuda_8.0.27_linux.run\n- cuda_8.0.27.1_linux.run\n- cudnn-8.0-linux-x64-v5.1.tgz\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n# ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Sep  3 21:37 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep  3 21:37 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep  3 21:37 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Sep  3 21:37 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Sep  3 21:37 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Sep  3 21:37 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Sep  3 21:37 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Sep  3 21:37 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Sep  3 21:37 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\n```\n# git rev-parse HEAD\n3c177a54cf48efdab49d716ee27936ead31c3388\n```\n1. The output of `bazel version`\n\n```\n# bazel version     \nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n```\n\nIt's bazel 0.3.1 actually. No idea why the output from version is so useless. I've installed it from the official deb repo.\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\ngit clone https://github.com/tensorflow/tensorflow.git\ncd tensorflow\n./configure\n\n# Hit ENTER on every question except:\n# Do you wish to build TensorFlow with GPU support? (answer: y)\n# Please specify a list of comma-separated Cuda compute capabilities you want to build with. (answer: 6.1)\n\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nIt's actually a bit more complicated, as the first attempt fails to even begin to compile. So I delete the TF repo, clone again, and configure/build again. It begins to compile, keeps chugging along for a few minutes, and then I get the error shown in the title.\n\nMore details here (you'll find there a fully automated procedure to completely reproduce the environment and build process):\n\nhttps://github.com/tensorflow/tensorflow/issues/4105#issuecomment-244573108\n### What other attempted solutions have you tried?\n\nTried to google around, hoping it's a known issue. Could not find anything related.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\nThis is the end of the build messages:\n\nhttps://gist.github.com/FlorinAndrei/b378e45709185ef40808e7537a3df4e8\n", "comments": ["I got the same issue here.\nERROR: /home/unbuilt/Works/tensorflow/tensorflow/core/kernels/BUILD:1550:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:depth_space_ops_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/depthtospace_op_gpu.cu.cc':\n  '/usr/local/cuda-8.0/include/cuda_runtime.h'\n  '/usr/local/cuda-8.0/include/host_config.h'\n  '/usr/local/cuda-8.0/include/builtin_types.h'\n  '/usr/local/cuda-8.0/include/device_types.h'\n...\n", "When you run `./configure`, did you explicitly specify your CUDA version? If not, then you are most likely hitting #3985.\n", "I did not specify the CUDA version explicitly. I'll try again with explicit settings and will let you know how it goes.\n", "I've explicitly specified CUDA version 8.0 and now compilation fails in a different way:\n\nhttps://github.com/tensorflow/tensorflow/issues/4214\n", "@FlorinAndrei Thanks. In that case, I'll de-dup this with #3985.\n\n@4214 appears to be unrelated.\n"]}, {"number": 4189, "title": "Fix a typo", "body": "", "comments": ["@DenisGorbachev, thanks for your PR! By analyzing the annotation information on this pull request, we identified @danmane, @keveman and @nsthorat to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "`I signed it!`\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks!\n"]}, {"number": 4188, "title": "Documentation updates for r0.10(non-rc) release.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke, @keveman and @vrv to be potential reviewers\n", "Not sure about the change I made for TF_Buffer. \n", "https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/g3doc/api_docs/cc/StructTF_Buffer.md  you'd have to cherrypick the change that removed the generated docs too :/\n", "Will do right away.\nHowever, I see that TF_Buffer is still here:\nhttps://github.com/tensorflow/tensorflow/blob/f9ae4fe749c5d7dbf1851cffc94730086e588c35/tensorflow/c/c_api.h#L146\n\nIs removing it (and what I am doing in this cl the right approach?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "I don't have the context for what you are doing exactly\n", "He's just rebuilding the docs -- TF_Buffer doesn't show up any more.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "LGTM.\n", "I confirmed with Manjunath that c_api file is still not compatible with doxygen. That is the reason for TF_Buffer to be missing. removing it from docs for now is the correct course of action.\n"]}, {"number": 4187, "title": "Library not loaded: @rpath/libcudart.7.5.dylib", "body": "+@trevorwelch\n\nFrom the discussion in #4105 and #4145, this is the tracking bug for the following error:\n\n```\nERROR: /Users/production204/Github/tensorflow/tensorflow/cc/BUILD:179:1: Executing genrule //tensorflow/cc:training_ops_genrule failed: bash failed: error executing command \n  (cd /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/cuda/bin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/usr/local/bin:usr/local/sbin:/usr/local/mysql/bin:/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\n    TMPDIR=/var/folders/h3/pn9k79xn6qd9jgksqbkpn3l80000gn/T/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.cc 0'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 5.\ndyld: Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/execroot/tensorflow/bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc\n  Reason: image not found\n/bin/bash: line 1: 74845 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/cc/ops/training_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/training_ops.cc 0\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 3111.405s, Critical Path: 3097.65s\n\nproduction204@Trevors-MacBook-Pro tensorflow $ \n```\n", "comments": ["@davidzchen \n\nyour error exactly like mine, i have fix this problem.\nMy compute:\n2012 early iMac\nCPU:i5\nDisplay Card: GT640M\n\nXcode 7.3\nCUDA 7.5.27\nCUDNN 4\n\nhere my solution:\n[https://github.com/JimmyKon/tensorflow_build_issue_fix/tree/master](https://github.com/JimmyKon/tensorflow_build_issue_fix/tree/master)\n\nI found the genrule-setup.sh file will execute before error.\n\n```\n...execroot/tensorflow/external/bazel_tools/tools/genrule/genrule-setup.sh\n```\n\nok, print file timestamp first.\n\n```\nstat genrule-setup.sh\n```\n\noutput like this:\n\n```\n16777217 56288053 -rwxr-xr-x 1 ****** wheel 0 242 \"Sep  4 23:26:23 2016\" \"Sep  2 22:34:23 2026\" \"Sep  4 22:34:24 2016\" \"Sep  4 22:34:21 2016\" 4096 8 0 genrule-setup.sh\n```\n\n\"Sep  2 22:34:23 2026\"? yes, record this timestamp.\n\nopen this file, add the environment configuration to the end of the file\n\n```\nexport DYLD_LIBRARY_PATH=/usr/local/cuda/lib\n```\n\nand then, recover genrule-setup.sh timestamp\n\n```\ntouch -t YYYYMMDDhhmm.SS genrule-setup.sh\n```\n\nYYYYMMDDhhmm.SS is your recorded timestamp,my situation is 202609022234.23\n\ncompile again, done.\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n", "Thanks for filing this bug, David!  I'm triaging issues and it seems you might be the most appropriate owner; if not just un-assign yourself and I'll re-triage it.\n", "thanks for your help on this one. So, I'm still getting this same error on branch r0.10, although on Master I'm getting #4105 \n\nFollowing @JimmyKon JimmyKon's instructions on branch r0.10, including adding the `DYLD_LIBRARY_PATH` to genrule-setup.sh ...\n\n```\n$ stat genrule-setup.sh\n16777220 36599236 -rwxr-xr-x 1 production204 staff 0 242 \"Sep 12 19:37:15 2016\" \"Aug 19 16:39:34 2016\" \"Aug 29 16:10:10 2016\" \"Aug 19 16:39:34 2016\" 4096 8 0 genrule-setup.sh\n\n$ touch -t 201609121937.15 genrule-setup.sh\n```\n\nupon compile I get:\n\n```\nERROR: /Users/production204/Github/tensorflow/tensorflow/cc/BUILD:116:1: Executing genrule //tensorflow/cc:parsing_ops_genrule failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 5.\ndyld: Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/execroot/tensorflow/bazel-out/host/bin/tensorflow/cc/ops/parsing_ops_gen_cc\n  Reason: image not found\n/bin/bash: line 1: 73929 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/cc/ops/parsing_ops_gen_cc bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/parsing_ops.h bazel-out/local_darwin-opt/genfiles/tensorflow/cc/ops/parsing_ops.cc 0\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 2525.303s, Critical Path: 1420.09s\n```\n\nWhich, I've confirmed the library is where it needs to be, I've even tried locating the library directly in `export DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-7.5/lib` and following the rest of the above instructions again to no avail \n", "@trevorwelch \n\nyou still got error after follow my instructions?\ndid you miss some step like blew:\n\n```\n$ sudo mv include/cudnn.h /Developer/NVIDIA/CUDA-7.5/include/\n$ sudo mv lib/libcudnn* /Developer/NVIDIA/CUDA-7.5/lib\n$ sudo ln -s /Developer/NVIDIA/CUDA-7.5/lib/libcudnn* /usr/local/cuda/lib/\n```\n\nBTW, if you get error:\n\n```\nSegmentation fault: 11\n```\n\nplease, try this command:\n\n```\nln -sf /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib\n```\n", "@JimmyKon  Thanks for engaging with this. yes (truncated results indicated by ...) :\n\n```\n$ cd Developer/NVIDIA/CUDA-7.5/include/\n\n$ ls -l\n...\n-r--r--r--@  1 production204  staff     99657 Jun  9 10:49 cudnn.h\n...\n```\n\n```\nCUDA-7.5$ cd lib\n\nlib$ ls -l\n\n...\n-rwxr-xr-x@ 1 production204  wheel   60108616 Feb  8  2016 libcudnn.4.dylib\n-rwxr-xr-x@ 1 production204  staff   58975112 Jun 10 03:30 libcudnn.5.dylib\nlrwxr-xr-x@ 1 production204  staff         16 Jun 10 03:31 libcudnn.dylib -> libcudnn.5.dylib\n-rw-r--r--@ 1 production204  staff   56392320 Jun 10 03:30 libcudnn_static.a\n...\n```\n\nSlightly different error after re-running with pip instead of tutorial, same missing lib though\n\n```\nERROR: /Users/production204/Github/tensorflow/tensorflow/contrib/layers/BUILD:30:1: Executing genrule //tensorflow/contrib/layers:bucketization_op_pygenrule failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 5.\ndyld: Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: /private/var/tmp/_bazel_production204/ed2bbf43bcd665c40f1e3ebaa04f68f6/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/layers/gen_bucketization_op_py_wrappers_cc\n  Reason: image not found\n/bin/bash: line 1:  5368 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/contrib/layers/gen_bucketization_op_py_wrappers_cc 0 > bazel-out/local_darwin-opt/genfiles/tensorflow/contrib/layers/ops/gen_bucketization_op.py\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1748.825s, Critical Path: 1673.19s\n\n```\n", " @trevorwelch  you are welcome :)\n\nyou can use --verbose_failures to see the error details, please let me know.\n", "@JimmyKon Thanks for looking into this and for providing the workaround!\n\nIt would be good to figure out why the Bazel build is not providing the correct libcudart library since it should have been linked from [`@local_config_cuda//cuda:cudart`](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda/BUILD.tpl#L76).\n\n@JimmyKon @trevorwelch When you ran your `configure` script, did you explicitly provide a CUDA version?\n", "@davidzchen \n\n## When you ran your configure script, did you explicitly provide a CUDA version?\n\ni did.\n\ni resolved this problem, through add lib path into genrule-setup.sh file.\n", "@davidzchen \n\nMaybe my solution quite trick, but it is work. :)\n", "@davidzchen \n\n> When you ran your `configure` script, did you explicitly provide a CUDA version?\n\nYes, I've tried both ways (system default, or explicitly pointing). \n\nI imagine that in future versions of bazel I won't have this issue, but bazel seems to just not work in general on my particular system configuration as it relates to cuda. For example, on tensorflow/magenta, all of my bazel tests fail, but magenta runs fine on the GPU.\n\n```\n$ bazel run //magenta/models/lookback_rnn:lookback_rnn_generate -- \\\n> --run_dir=/tmp/lookback_rnn/logdir/run1 \\\n> --hparams=\"{'batch_size':64,'rnn_layer_sizes':[64,64]}\" \\\n> --output_dir=/tmp/lookback_rnn/generated \\\n> --num_outputs=10 \\\n> --num_steps=128 \\\n> --primer_melody=\"[60]\"\nINFO: Found 1 target...\nTarget //magenta/models/lookback_rnn:lookback_rnn_generate up-to-date:\n  bazel-bin/magenta/models/lookback_rnn/lookback_rnn_generate\nINFO: Elapsed time: 0.460s, Critical Path: 0.00s\n\nINFO: Running command line: bazel-bin/magenta/models/lookback_rnn/lookback_rnn_generate '--run_dir=/tmp/lookback_rnn/logdir/run1' '--hparams={'\\''batch_size'\\'':64,'\\''rnn_layer_sizes'\\'':[64,64]}' '--output_dir=/tmp/lookback_rnn/generated' '--num_outputs=10' '--num_steps=128' '--primer_melody=[60]'\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\nINFO:tensorflow:hparams = {'rnn_layer_sizes': [64, 64], 'temperature': 1.0, 'decay_rate': 0.95, 'dropout_keep_prob': 1.0, 'batch_size': 1, 'decay_steps': 1000, 'clip_norm': 5, 'initial_learning_rate': 0.01, 'skip_first_n_losses': 0}\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x1274a7a50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x127126b50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] OS X does not support NUMA - returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GT 650M\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9\npciBusID 0000:01:00.0\nTotal memory: 1023.69MiB\nFree memory: 151.57MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)\nINFO:tensorflow:Checkpoint used: /tmp/lookback_rnn/logdir/run1/train/model.ckpt-111\nINFO:tensorflow:Wrote 10 MIDI files to /tmp/lookback_rnn/generated\n\n```\n\nAnyway, in the meantime I have TF running on the GPU, and I don't need to build from source for any particular project at the moment. \n", "Getting the same error while installing version `0.10.0` from source. Not getting the error with version `0.10.0rc0` though.\n", "@JimmyKon fix worked perfectly in my case (Master branch).\n\nMy specs:\n- Clang: 7.3 build 703\n- macOS: 10.11.6-x86_64\n- CUDA V7.5.26\n- CUDNN 5.1\n- GPU: GT750M\n", "@JimmyKon 's fixes worked for me. had to modify the genrule script and symlink like in #3263. for: r0.10 on OSX 10.10.5 with Cuda 7.5, CUDNN 5, and a GT 650M.\n", "@JimmyKon's fix worked for me.\n\nConfig\nOSX 10.12\nCUDA 8\nXCode 7.3\nr0.11\n", "I couldn't find a \"genrule-setup.sh\" anywhere on my Mac.  \n\nI've been going in circles--trying brew/pip/conda, 2.7/3.5, trying to get TensorFlow w/NVIDIA GPU support working.  It seems that Mac OS X is simply poorly supported by TensorFlow and/or NVIDIA.  \n", "@BKJackson can you send the versions of OSX, CUDA, Xcode, tf etc.\n\ngenrule-setup.sh is located in a tmp directory. There is a symbolic link called bazel-tensorflow in your tensorflow folder which points to the tmp directory.\n\nso try this tensorflow-dir/bazel-tensorflow/external/bazel_tools/tools/genrule/genrule-setup.sh\n", "I tried @JimmyKon 's solution, worked for me. Thanks. \n\nOS X 10.11, \nCUDA 7.5.27 CUDNN 5.1, \nXcode 7.3.1 \ntensorflow-0.11.0rc0-py2-none-any.whl \nanaconda/python2.7\ngt 750m\ndidn't specify cuda/cudnn versions in ./configure\n", "OSX: 10.11.6 (El Capitan)\nGPU: NVIDIA GeForce GT 750M 2048 MB (0.5 in the NVIDIA/CUDA number system)\nXcode: 7.3.1\nCUDA 7.5.27 CUDNN 5.1\nPython default environment: 3.5.0rc4 | Anaconda custom (x86_64)\nGCC: 4.2.1 (Apple build 5577) on darwin\nTensorFlow: Installed in Conda Python 2.7 environment (\"py27\"): 0.11.0rc0 installed with pip (& protobuf 3.0.0 also installed with pip). \n\nWhen try \"import tensorflow as tf\" in my \"py27\" Anaconda environment, I get the error: \n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Applications/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Applications/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/Applications/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/Applications/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: dlopen(/Applications/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): no suitable image found.  Did find:\n    /Applications/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x03\n```\n\nIt looks like it's complaining about swig (?), which I installed with brew.  I'm wondering if it's a brew/anaconda problem, because I don't see swig in the \"conda list\".  Can Anaconda's python environment see and use libraries downloaded with brew?  \n", "I dont think Anaconda's python environment sees brew python's packages. Usually the search path is setup in bash profile where one takes precedence over the other\n", "As a data point, configuring TF 0.11.0 source from git (282823b877f173e6a33bbc9d4b9ad7dd8413ada6) for GPU usage specifying '8.0' in the configure script, and using Xcode Command Line Tools 7.3.1 results in the same error\n\n`Executing genrule //tensorflow/contrib/layers:bucketization_op_pygenrule failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 6.\ndyld: Library not loaded: @rpath/libcudart.8.0.dylib\n`\nI in fact do have a libcudart.8.0.dylib in the default cuda install location.\n", "@JimmyKon 's solution worked here as well:\r\n\r\nOS X 10.12,\r\nCUDA 8.0 CUDNN 5.2,\r\nXcode Command line tools\r\ntensorflow 0.12.0rc1\r\npython3.5\r\ngt 650m\r\ndidn't specify cuda/cudnn versions in ./configure\r\n\r\nThanks Jimmy!", "I am curious as our mac test machines have been working fine so far.\r\n\r\nIs it possible the `LD_LIBRARY_PATH` environment variable is not set to point to cuda, when you are running into the issue?\r\n", "@gunan 's comment helped me! \r\n\r\nI've been reading about this issue and possible fixes for it for a bit now. \r\n\r\nPeople have success with \r\n\r\n    sudo ln -s /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib\r\n\r\nand with setting \r\n\r\n    export DYLD_LIBRARY_PATH=\"/Developer/NVIDIA/CUDA-8.0/lib:/usr/local/cuda/lib\"\r\n\r\n----\r\n\r\nHowever. In my case, these two did not help solve the issue fully. I **also** had to do:\r\n\r\n    export LD_LIBRARY_PATH=/usr/local/cuda/lib\r\n\r\nNow, it works! ", "OK, then I will close this issue.\r\nPlease reopen if you still run into it.", "Hi, I'm running into the same issue, MBP late 2013, CUDA 8.0 CuDNN 5.1, OXS 10.12.1 Py 2.7, but I can't find the genrule folder. In fact, I can't even find tensorflow/external. Any idea why this might be? ", "The error means you have TensorFlow built for CUDA 7.5 (I'm guessing you have 0.11 or earlier)\r\n", "I just hit this error in r1.0 alpha.\r\n\r\ndyld: Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /private/var/tmp/_bazel_Keiji/1dc08e629317533c84e553c6b75a1110/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/layers/gen_bucketization_op_py_wrappers_cc\r\n  Reason: image not found\r\n/bin/bash: line 1: 83868 Abort trap: 6           bazel-out/host/bin/tensorflow/contrib/layers/gen_bucketization_op_py_wrappers_cc 0 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/contrib/layers/ops/gen_bucketization_op.py\r\n\r\nThe @JimmyKon instructions worked here as well.\r\n\r\nmacOS 10.12.2\r\nAnaconda Python 3.5\r\nCUDA 8.0 \r\n\r\n", "the first solution worked for me thanks"]}, {"number": 4186, "title": "Push from internal.", "body": "", "comments": ["@tensorflow-jenkins test this please.\n"]}, {"number": 4185, "title": "Use valid GPU for allocating CUDA host memory", "body": "This PR fixes (again!) issue #1888.\n\nBefore this commit, when allocating CUDA host memory, device 0 would\nalways be used to allocate the memory (because the memory is allocated\non the host with DMA enabled in the same way for all devices; it does\nnot matter which stream executor allocates the host memory). This\nworks, but if device 0 is not a valid device in this session (e.g. it\nis turned off using GPUOptions.visible_device_list), doing so would\nallocate a context on GPU 0, even when it was not being used. This\nchange fixes this, so that a visible device is found by looking at\nwhich GPU allocators exist, and then using that device to allocate host\nmemory.\n\n@vrv \n", "comments": ["This is pretty much what I was thinking, so thanks!  I wonder what happened to the CLA bot though, we need that to go through...\n", "@googlebot where are you\n", "I guess if this is being done as part of your work from Baidu, we'd need Baidu to sign the corp CLA.  Perhaps I should just implement this fix myself instead, unless you want to push that through ;)\n", "Probably best  to just have you re-implement it, then, especially given how small it is; this is not being done for Baidu (hence non corp CLA) and I do not think I can get a corp CLA there for this work at this point, but given how small this change is, may as well play it safe.\n", "ah, the commit had a baidu.com email address, so I wasn't sure.  Okay, sorry about that, legal stuff makes me sad too.  I'll send in a fix soon.\n"]}, {"number": 4184, "title": "Push from internal.", "body": "", "comments": ["@tensorflow-jenkins test this please.\n", "aborting mac tests, as there are other failures at the moment.\n"]}, {"number": 4183, "title": "Make wide_n_deep_tutorial.py work on Python 2 and 3", "body": "Use the six.moves module as suggested by @mrry to make this work with Python 2 and 3\n", "comments": ["@jpangburn, thanks for your PR! By analyzing the annotation information on this pull request, we identified @caisq and @tensorflower-gardener to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for sending this in!\n", "@tensorflow-jenkins test this please.\n", "@googlebot can you validate the CLA?\n", "@jpangburn It looks like our bot for checking CLAs (contributor license agreements) has failed to respond to this PR. However, I've checked manually and it seems you've signed the CLA (thanks!) so I'm going to merge this now.\n", "Yes, I did sign the CLA just today, not sure if that\u2019s why it didn\u2019t find it automatically.  Thanks!\n\n> On Sep 2, 2016, at 12:24 PM, Derek Murray notifications@github.com wrote:\n> \n> @jpangburn https://github.com/jpangburn It looks like our bot for checking CLAs (contributor license agreements) has failed to respond to this PR. However, I've checked manually and it seems you've signed the CLA (thanks!) so I'm going to merge this now.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub https://github.com/tensorflow/tensorflow/pull/4183#issuecomment-244500916, or mute the thread https://github.com/notifications/unsubscribe-auth/AEJ8pu4kgUAC9x_k8-KD9ZcJiTEpPNEVks5qmKIZgaJpZM4J0CzM.\n"]}, {"number": 4182, "title": "Improve performance of tokenization for seq2seq/translation ", "body": "by reducing compilation time of regular expression\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@mithrandir Please sign our CLA neither late, nor early, but precisely when you mean to.\n", "Hi Daniel,\n\nActually I'm employee of Microsoft and I'm now checking if I'm ok with\nsigning that. It would be great to know if there was similar case before.\n\nThanks,\nJiin\n\nOn Wed, Sep 7, 2016 at 5:08 AM, Daniel W Mane notifications@github.com\nwrote:\n\n> @mithrandir https://github.com/mithrandir Please sign our CLA neither\n> late, nor early, but precisely when you mean to.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4182#issuecomment-245073545,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAN58eOWDADRSnrQ4F1xCbje68j_sJo6ks5qnchDgaJpZM4J0AYq\n> .\n", "I believe that Microsoft is already covered under a Google CLA. You may need to switch the commit author email in the git log to your microsoft.com email address.\n", "After reviewing internal policy, I decided to use my personal email instead\nof @microsoft.com address because I did this not for my work. I'll sign\nthis when I come back to home.\n\nThanks,\nJiin\n\nOn Fri, Sep 9, 2016 at 3:43 AM, Daniel W Mane notifications@github.com\nwrote:\n\n> I believe that Microsoft is already covered under a Google CLA. You may\n> need to switch the commit author email in the git log to your\n> microsoft.com email address.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4182#issuecomment-245697228,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAN58UXIYvGVoMXuhKAACzbYvF3VV267ks5qoFdSgaJpZM4J0AYq\n> .\n", "Ok, I signed it with tisphie@gmail.com\n\nThanks,\nJiin\n\nOn Fri, Sep 9, 2016 at 11:15 AM, Park Ji-In tisphie@gmail.com wrote:\n\n> After reviewing internal policy, I decided to use my personal email\n> instead of @microsoft.com address because I did this not for my work.\n> I'll sign this when I come back to home.\n> \n> Thanks,\n> Jiin\n> \n> On Fri, Sep 9, 2016 at 3:43 AM, Daniel W Mane notifications@github.com\n> wrote:\n> \n> > I believe that Microsoft is already covered under a Google CLA. You may\n> > need to switch the commit author email in the git log to your\n> > microsoft.com email address.\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/4182#issuecomment-245697228,\n> > or mute the thread\n> > https://github.com/notifications/unsubscribe-auth/AAN58UXIYvGVoMXuhKAACzbYvF3VV267ks5qoFdSgaJpZM4J0AYq\n> > .\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@lukaszkaiser PTAL\n", "Looks good to me, when tests pass let's get this in. Thanks!\n", "Sorry -- not meant to close. Adding vrv to merge.\n", "Can one of the admins verify this patch?\n", "Please merge.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4181, "title": "Document events file format", "body": "Hello,\nIn the documentation for Tensorboard, I can't find a description for the format of the events file beyond that it contains Event protobufs. Is it recordio? Are there any tools for writing event files for 3rd-party Tensorflow clients that aren't using the Python API? \n", "comments": ["The code that manages writing out the events file is tf.train.SummaryWriter (Python), which is defined here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/summary_io.py\n\nIt uses the tensorflow::EventsWriter (C++) class to actually write each file:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/events_writer.cc\n\nThe underlying file format is indeed recordio.  I don't believe there are any special tools for writing Tensorboard events files; you're just supposed to use the tf.train.SummaryWriter Python class.\n\n@danmane probably has an opinion on using Tensorboard with non-Python clients.\n", "It's close to recordio, but it is actually \"tfrecord\" It has a few differences from recordio so a standard recordio reader won't work.\n\nIf you want to write events without using the python api, you should use the tensorflow::EventsWriter class, like tatatodd suggested. \n\nPlease feel free to submit a pull request improving the documentation :)\n", "This issue has been migrated to https://github.com/tensorflow/tensorboard/issues/72."]}, {"number": 4180, "title": "Push from internal.", "body": "", "comments": ["@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 4179, "title": "Fix bazel warnings by using their official names in the other repo WORKSPACE", "body": "Updated the commits to points where they have this new name.  highwayhash update also\nmight include windows support, which is a bonus.\n\nAs far as I can tell, protobuf and gemmlowp don't have these name declarations yet.  gemmlowp doesn't even have a WORKSPACE file\n", "comments": ["@vrv, thanks for your PR! By analyzing the annotation information on this pull request, we identified @kirilg, @ysuematsu and @martinwicke to be potential reviewers\n"]}, {"number": 4178, "title": "What do readers do after their input is exhausted?", "body": "I would like to train for multiple epochs on a dataset, using TensorFlow readers and queues. How do reader signal the end of the dataset? I couldn't find any [documentation](https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#readers) on this.\n", "comments": ["There's some useful documentation under the \"How-Tos\" section of the tensorflow.org site.  The one on \"Reading Data\" is most relevant to your question; in particular look here:\nhttps://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#aside-how-clean-shut-down-when-limiting-epochs-works\n\nFYI we use github issues primarily for bugs and installation issues.  In the future please ask these types of questions on StackOverflow and tag it with the `tensorflow` tag.  Thanks!\n"]}, {"number": 4177, "title": "TensorFlow: remove no longer needed third_party/boringssl files.", "body": "", "comments": ["@vrv, thanks for your PR! By analyzing the annotation information on this pull request, we identified @rinugun to be a potential reviewer\n", "(I couldn't find any reference to the build files anymore, so I'm assuming they can be deleted)\n", "Yep, I think this completes the work started in 4af1fb42d1c84d8493561089d710b2f4a059a279. LGTM.\n", "LGTM\n"]}, {"number": 4176, "title": "Why reuse default to True for loop_function() in seq2seq.rnn_decoder ?", "body": "My question concerns the rnn_decoder() in seq2seq.\n\nI am batchnormalizing activation maps in the loop_function(). To this end, I call the tf.contrib.layers.batch_norm() function. Like many other optionalities, it calls variables using tf.get_variable().\n\nIdeally, you'd set reuse=None during the first call of loop_function(). That way, tf.get_variable() functions can construct variables. During second and subsequent calls of loop_function() you set reuse=True.\n\nHowever, in rnn_decoder() reuse is set to True also during the first call. This doesn't allow the tf.get_variable() functions to construct variables. \n\nMy question is: why is this so? Am I doing something wrong? Or what is the logic here?\n\nMore specifically, my loop_function is:\n\n``` python\n    def loop_function(output, i):\n      ##some code##\n      some_variable = tf.contrib.layers.batch_norm(some_variable,is_training= self.is_training)\n      ##some code##\n      return some_variable\n```\n\nI made a work-around, by defining my own rnn_decoder. I copy-paste the Tensorflow implementation and set reuse=None during the first call\n\n``` python\ndef rnn_decoder_rob(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  REUSE=None\n  with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n    state = initial_state\n    outputs = []\n    prev = None\n    for i, inp in enumerate(decoder_inputs):\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(\"loop_function\", reuse=REUSE):\n          inp = loop_function(prev, i)\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n        REUSE = True\n      output, state = cell(inp, state)\n      outputs.append(output)\n      if loop_function is not None:\n        prev = output\n  return outputs, state\n```\n", "comments": ["@ebrevdo can you comment on this?  Thanks!\n", "@tatatodd is that still current?", "Closing due to inactivity. Feel free to re-open if you would like us to look again."]}, {"number": 4175, "title": "Contrib metric `metric_ops.streaming_auc` type error within function", "body": "### Environment info\n\nVersion 0.10.0rc0\nUbuntu\n\nI'm using a `RandomForest` Estimator and am feeding it `tf.contrib.metrics.streaming_auc` and it fails.\n### What other attempted solutions have you tried?\n\nModifying source to convert both inputs to float using `tf.to_float`. It worked, but still an issue.\n### Logs or other output that would be helpful\n\n```\nTraceback (most recent call last):\n  File \"main.py\", line 284, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"main.py\", line 219, in main\n    eval_steps=1\n  File \"/home/user/Documents/tensorflow/cross_validation/evaluate.py\", line 45, in evaluate\n    metrics=metrics\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 356, in evaluate\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 630, in _evaluate_model\n    eval_dict = self._get_eval_ops(features, targets, metrics)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/random_forest.py\", line 205, in _get_eval_ops\n    result[name] = metric(predictions, labels)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 795, in streaming_auc\n    fp_update_op) = _tp_fn_tn_fp(predictions, labels, thresholds, ignore_mask)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 667, in _tp_fn_tn_fp\n    (thresh_tiled)),\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 845, in greater\n    result = _op_def_lib.apply_op(\"Greater\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 468, in apply_op\n    inferred_from[input_arg.type_attr]))\nTypeError: Input 'y' of 'Greater' Op has type float32 that does not match type int64 of argument 'x'.\n```\n", "comments": ["The RandomForest Estimator currently only supports passing one of ['sigmoid_entropy', 'softmax_entropy', 'accuracy', 'r2', 'predictions'] in the accuracy_metric argument of the constructor to use for evaluation. If passing streaming_auc in the metrics argument of evaluate() worked with casted inputs, that's great, but I would be wary that it's giving you the right results.  We pass class probabilities and one-hot labels to the metric, which not all metrics are expecting. For instance, streaming_accuracy is expecting predictions, so we wrap it in a function that performs an argmax. \n\nWe have an action item to improve the compatibility of the underlying random forest implementation with the tf.learn wrapper, so thanks for pointing this out so it can be included.\n", "Oops let me clarify something; I modified the source of the Estimator so that when get_eval_ops is called it passes the Predictions and Label (using argmax for both, label post one-hot) instead of the probabilities so other metrics could be used. Just remembered I did that earlier and forgot before I posted this. The AUC I've been receiving has been accurate after modification.\n\nShouldn't the AUC metric not fail if I'm passing in solely the predictions and labels as tf.int32 with the default threshold value?\n", "Ah ok.  AUC itself shouldn't fail with the same type of inputs, is that what you're seeing?  Can you replicate without RandomForest estimator?   \n", "Yeah it shouldn't and yes it fails for me when using any Estimator when I specify `{'auc': streaming_auc}` as a desired metric. LinearEstimator traceback:\n\n```\nTraceback (most recent call last):\n  File \"main.py\", line 308, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"main.py\", line 239, in main\n    eval_steps=1\n  File \"/usr/local/lib/tensorflow/cross_validation/evaluate.py\", line 45, in evaluate\n    metrics=metrics\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 356, in evaluate\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 630, in _evaluate_model\n    eval_dict = self._get_eval_ops(features, targets, metrics)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\", line 204, in _get_eval_ops\n    metrics)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 454, in _get_eval_ops\n    return self._target_column.get_eval_ops(features, logits, targets, metrics)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/target_column.py\", line 288, in get_eval_ops\n    self.get_weight_tensor(features)))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/target_column.py\", line 371, in _run_metrics\n    result[name] = metric(predictions, targets)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 802, in streaming_auc\n    fp_update_op) = _tp_fn_tn_fp(predictions, labels, thresholds, ignore_mask)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 673, in _tp_fn_tn_fp\n    thresh_tiled),\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 845, in greater\n    result = _op_def_lib.apply_op(\"Greater\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 468, in apply_op\n    inferred_from[input_arg.type_attr]))\nTypeError: Input 'y' of 'Greater' Op has type float32 that does not match type int64 of argument 'x'.\n```\n", "I got the same error when using wide and deep model:\n\n```\nm = tf.contrib.learn.DNNLinearCombinedClassifier(\n    model_dir=FLAGS.model_dir,\n    linear_feature_columns=wide,\n    dnn_feature_columns=deep,\n    dnn_hidden_units=[100, 50],\n    config=run_config\n)\nvalidation_metrics = {\n    \"streaming_auc\": tf.contrib.metrics.streaming_auc,\n    \"precision\": tf.contrib.metrics.streaming_precision\n}\n\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n    input_fn=lambda: input_fn(test_file_list),\n    every_n_steps=FLAGS.validation_steps,\n    metrics=validation_metrics\n)\n\nm.fit(input_fn=lambda: input_fn(train_file_list),\n      max_steps=FLAGS.train_steps,\n      monitors=[validation_monitor])\n```\n", "It appears this has been fixed in newer versions (past r0.10), so if you don't feel like upgrading you can fix it by changing the function `_tp_fn_tn_fp` in `tensorflow/contrib/metrics/python/ops/metric_ops.py` (doesn't require rebuilding, of course) as follows.\r\n### Original\r\n```\r\ndef _tp_fn_tn_fp(predictions, labels, thresholds, ignore_mask=None):\r\n  ...\r\n  pred_tiled = math_ops.cast(\r\n      math_ops.greater(\r\n          array_ops.tile(\r\n              array_ops.transpose(predictions), [num_thresholds, 1]),\r\n          thresh_tiled),\r\n      dtype=dtypes.int32)\r\n```\r\n### Modified\r\n```\r\ndef _tp_fn_tn_fp(predictions, labels, thresholds, ignore_mask=None):\r\n  ...\r\n  pred_tiled = math_ops.cast(\r\n      math_ops.greater(\r\n          math_ops.to_float(array_ops.tile(\r\n              array_ops.transpose(predictions), [num_thresholds, 1])),\r\n          math_ops.to_float(thresh_tiled)),\r\n      dtype=dtypes.int32)\r\n```", "Still getting this error. In r0.12, the original is not as above, but is:\r\n\r\n```\r\n  pred_is_pos = math_ops.greater(\r\n      array_ops.tile(array_ops.transpose(predictions_2d), [num_thresholds, 1]),\r\n      thresh_tiled)\r\n  pred_is_neg = math_ops.logical_not(pred_is_pos)\r\n```"]}, {"number": 4174, "title": "tf.nn.bias_add does not support multiple derivatives (using with contrib.learn)", "body": "Apparently, `tf.nn.bias_add` does not support differentiating twice: I created a neural network with `tf.contrib.layers`, and attempting to compute a Hessian matrix (actually Hessian-vector product, based on https://en.wikipedia.org/wiki/Hessian_automatic_differentiation) yields a `LookupError` saying that `BiasAddGrad` has no gradient.\n\nFrom what I understand, `bias_add` is basically `add` with support for different types.\n\nIs there any way to allow `bias_add` to be differentiated multiple times? Perhaps at the very least we could make the use of `bias_add` optional in `layers`.\n", "comments": ["FYI a related issue is https://github.com/tensorflow/tensorflow/issues/2598\n\n@rmlarsen can you comment on whether multiple derivatives of `bias_add` would be feasible?  Thanks!\n", "I think the best solution is to explicitly add a second derivative function in Python, similar to what I did for InvGrad here:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L255\n\n@tatatodd would you be willing to send a PR for that?\n", "I meant @admcrae would you be willing to send a PR? @tatatodd is welcome to as well ;-)\n", "Yes, I can do it.\n\nOut of curiousity (I'm extremely new to the inner workings of TensorFlow ops), why do ops like `BiasAddGrad` and `InvGrad` need C++ kernels?  It looks as though both could easily be implemented with more elementary operations (e.g. a reduction and possibly a casting in the former case, and a simple squaring + multiplication in the latter case). For that matter, why do `inv` and `div` need separate kernels at all? Is it all for performance optimization?\n", "@admcrae Sounds great! \n\nWe added C++ kernels for them purely for performance, since they appear often in large DL  models and/or their gradients. For example, div(scalar, tensor) would first broadcast scalar to a tensor then divide the two tensors elementwise, whereas inv(tensor) will apply the unary transformation x = 1/x elementswise. The actual division will of course used vectorized/SIMD instructions on most hardware.\n", "Just submitted a PR. It's a bit ugly for what is basically an expansion (I generated a zero tensor and applied `bias_add` to get the required broadcasting). I couldn't find anything already implemented that would do this while conforming to `bias_add`'s particular features. I'd be happy to hear if anyone has a better suggestion.\n", "I'll wait to close this until the PR gets merged.\n"]}, {"number": 4173, "title": "error shows when doing the command 'python -m tensorflow.models.image.mnist.convolutional' #3674", "body": "After I install tensorflow using pip, I do the simplest command to train minst. But it shows error:\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally Extracting data/train-images-idx3-ubyte.gz Traceback (most recent call last): File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 326, in <module> tf.app.run() File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run sys.exit(main(sys.argv)) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 138, in main train_data = extract_data(train_data_filename, 60000) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\", line 85, in extract_data buf = bytestream.read(IMAGE_SIZE \\* IMAGE_SIZE \\* num_images \\* NUM_CHANNELS) File \"/usr/lib/python2.7/gzip.py\", line 261, in read self._read(readsize) File \"/usr/lib/python2.7/gzip.py\", line 308, in _read self._read_eof() File \"/usr/lib/python2.7/gzip.py\", line 347, in _read_eof hex(self.crc))) IOError: CRC check failed 0x56d16c09 != 0x39bbe345L \n", "comments": ["What should I do to solve this problem\uff1f\n", "try and unpack `data/train-images-idx3-ubyte.gz` manually - does it work?\n", "Otherwise you could try and delete `train-images-idx3-ubyte.gz` and the script should try and download it again from the source. Hopefully without any errors. It could be a problem with gzip library on your machine but that is less likely.\n", "@hholst80  I delete train-images-idx3-ubyte.gz and download it again from the source. And it works. Thank you very much !\n"]}, {"number": 4172, "title": "Missing files and flag for building linkable archives with `make` on Linux (Ubuntu)", "body": "Trying to build under Linux, I have faced two issues, one for PIC missing, and one for a missing file in the source build list.\n\nI guess the origin of the problem is the `Makefile` depends on Bazel's build, and changes may be missing.\n\nHow does this looks for you? Here is a temporary [patch](https://github.com/tensorflow/tensorflow/compare/master...ic:master).\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System: Linux 16_04\n\nInstalled version of CUDA and cuDNN: None\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): 461caa86137aee3d2e40843ea308c216f10c4655\n2. The output of `bazel version`: It does not apply here (using `make` from the `tensor flow/contrib/makefile` project.\n### If possible, provide a minimal reproducible example:\n\n```\ncd TF_HOME\n./tensorflow/contrib/makefile/download_dependencies.sh\ncd tensorflow/contrib/makefile/downloads/protobuf\nmake && make check && sudo make install && sudo ldconfig\ncd -\nmake -f tensorflow/contrib/makefile/Makefile\n\n# Long build\n# Leading to error when building `benchmark`\n```\n### What other attempted solutions have you tried?\n\nI have tried to build on Ubuntu 14_04 and Mac OS X (10.10). It worked only on Mac OS X. Then I realized that the build chokes on my Linux boxes for PIC, and then for a missing file. Adding them (see a [solution here](https://github.com/tensorflow/tensorflow/compare/master...ic:master)) fixes all.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\nThe build fails with (added breaks for readability):\n\n```\ngcc --std=c++11 -DIS_SLIM_BUILD -O0 -fPIC -I. \n-I./tensorflow/contrib/makefile/downloads/ \n-I./tensorflow/contrib/makefile/downloads/eigen-eigen-9e1b48c333aa \n-I./tensorflow/contrib/makefile/gen/proto/ \n-I./tensorflow/contrib/makefile/gen/proto_text/ -I/usr/local/include \n-o ./tensorflow/contrib/makefile/gen/bin/benchmark\n./tensorflow/contrib/makefile/gen/obj/tensorflow/core/util/reporter.o\n./tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model.o\n./tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model_main.o -Wl,--allow-multiple-definition -Wl,--whole-archive ./tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a -L/usr/local/lib -lstdc++ -lprotobuf -lz -lm -ldl -lpthread\n./tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(conv_ops.o): In function `tensorflow::LaunchDeepConvOp<Eigen::ThreadPoolDevice, float>::Run(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, int, int, int, int, int, int, int, int, int, int, int, int, int, tensorflow::Tensor*, tensorflow::TensorFormat)':\nconv_ops.cc:\n(.text._ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE[_ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE]+0x70): \nundefined reference to `tensorflow::CanUseDeepConv2D(int, int, int, int, int, int, int, int)'\nconv_ops.cc:\n(.text._ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE[_ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE]+0x184): \nundefined reference to `tensorflow::functor::DeepConv2D<Eigen::ThreadPoolDevice, float>::operator()(tensorflow::OpKernelContext*, tensorflow::Conv2DArgs const&, float const*, float const*, float*)'\ncollect2: error: ld returned 1 exit status\n```\n", "comments": ["Send a PR with the fixes? @petewarden can you review that PR?\n", "[PR](https://github.com/tensorflow/tensorflow/pull/4260) ready.\n", "Thanks for the PR @ic! Closing since this appears to be fixed.\n"]}, {"number": 4171, "title": "distribute tensorflow read data problem", "body": "Hi,\nwhen I want to build a distribute model with cifar-10  (https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/),\nit hanged at sess.run , I think it is the input queue problem. It can not read image and label from file.\nBut when run with no distribute, everything is OK.\nAny suggestions?\n\nin train():\nimages, labels = model.distorted_inputs()\n\nin model.py:\ndef distorted_inputs():\n  batch_size=FLAGS.batch_size)\n  if not FLAGS.data_dir:\n    raise ValueError('Please supply a data_dir')\n  data_dir = os.path.join(FLAGS.data_dir, '')\n  return model_input.distorted_inputs(data_dir=data_dir,\n                                        batch_size=FLAGS.batch_size)\n\nin model_input.py:\ndef distorted_inputs(data_dir, batch_size):\n  filenames = [os.path.join(data_dir, '%d.bin' % i)\n               for i in xrange(1, 5)]\n  for f in filenames:\n    if not gfile.Exists(f):\n      raise ValueError('Failed to find file: ' + f)\n  filename_queue = tf.train.string_input_producer(filenames)\n  read_input = read_cifar10(filename_queue)\n  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n  reshaped_image = tf.Print(reshaped_image, [reshaped_image], 'this is float image')\n  min_fraction_of_examples_in_queue = 0.4\n  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n                           min_fraction_of_examples_in_queue)\n  print ('Filling queue with %d CIFAR images before starting to train. '\n         'This will take a few minutes.' % min_queue_examples)\n  return _generate_image_and_label_batch(reshaped_image, read_input.label,\n                                         min_queue_examples, batch_size,test)\n\ndef _generate_image_and_label_batch(image, label, min_queue_examples,\n                                    batch_size,test):\n  num_preprocess_threads = 16\n  images, label_batch = tf.train.shuffle_batch(\n      [image, label],\n      batch_size=batch_size,\n      num_threads=num_preprocess_threads,\n      capacity=min_queue_examples + 3 \\* batch_size,\n      min_after_dequeue=min_queue_examples)\n  return images, tf.reshape(label_batch, [batch_size])\n", "comments": ["I run one ps job and one work job at localhost.\n", "seems found the problem, it's the queue problem. Now everything is OK.\n"]}, {"number": 4170, "title": "Update the version string to 0.10.0", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @jendap, @tensorflower-gardener and @keveman to be potential reviewers\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 4169, "title": "My project worked well before r0.8, but got NAN after updating to r0.10", "body": "I can not figure out what has been changed in r0.10.\n\nAfter updating to r0.10, I modify some of the import to fit r0.10. But when after run \"sess.run([train_op, loss])\" for one iteration, the loss is NAN. However, all these code work well before r0.8.\n\nAnd the trained model also produced different result (random precision).\n\nAny suggestions?\n\nThe code for building all ops is:\n\n```\n    print(\"Preparing GPU %d ......\"%(FLAGS.gpu))\n    with tf.device('/gpu:%d' % FLAGS.gpu):\n        with tf.name_scope('%s' % (FLAGS.TOWER_NAME)):\n            # Calculate the loss for one tower of the model. This function\n            # constructs the entire model but shares the variables across\n            # all towers.\n            temp_istates = []\n            for i in xrange(FLAGS.num_lstm):\n                initial_state = tf.placeholder(tf.float32, shape=(batch_size_per_gpu/FLAGS.n_steps, 1*num_lstm_units[i]),\n                                                name='train_lstm_%d_istate'%i)\n                train_initial_states[initial_state] = []\n                temp_istates.append(initial_state)\n            train_net = model.net_class({'data':train_data}, trainable=True, TOWER_NAME=FLAGS.TOWER_NAME,\n                    NUM_CLASSES=FLAGS.NUM_CLASSES, n_steps=FLAGS.n_steps,\n                    initial_state=temp_istates, lstm_num_units=num_lstm_units)\n            logits = train_net.get_output()\n            train_net.slim_loss(logits, train_label, FLAGS.NUM_CLASSES)\n\n            # Reuse variables for the next tower.\n            tf.get_variable_scope().reuse_variables()\n            losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\n\n            # Calculate the total loss for the current tower.\n            regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            total_loss = tf.add_n(losses + regularization_losses, name='total_loss')\n\n            # Compute the moving average of all individual losses and the total loss.\n            loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n            loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n            # Attach a scalar summary to all individual losses and the total loss; do the\n            # same for the averaged version of the losses.\n            for l in losses + [total_loss]:\n                # Name each loss as '(raw)' and name the moving average version of the loss\n                # as the original loss name.\n                tf.scalar_summary(l.op.name +' (raw)', l)\n                tf.scalar_summary(l.op.name, loss_averages.average(l))\n\n            with tf.control_dependencies([loss_averages_op]):\n                loss = tf.identity(total_loss)\n\n            # Retain the Batch Normalization updates operations only from the\n            # final tower. Ideally, we should grab the updates from all towers\n            # but these stats accumulate extremely fast so we can ignore the\n            # other stats from the other towers without significant detriment.\n            batchnorm_updates = tf.get_collection(slim.ops.UPDATE_OPS_COLLECTION)\n\n            with tf.control_dependencies([loss_averages_op]):\n                # Create an optimizer that performs gradient descent.\n                if FLAGS.optimizer == 'SGD':\n                    opt = tf.train.GradientDescentOptimizer(lr)\n                elif FLAGS.optimizer == 'Momentum':\n                    opt = tf.train.MomentumOptimizer(lr, momentum=FLAGS.momentum)\n                elif FLAGS.optimizer == 'Adagrad':\n                    opt = tf.train.AdagradOptimizer(lr)\n                elif FLAGS.optimizer == 'Adam':\n                    opt = tf.train.AdamOptimizer(lr)\n                elif FLAGS.optimizer == 'Ftrl':\n                    opt = tf.train.FtrlOptimizer(lr)\n                elif FLAGS.optimizer == 'RMSProp':\n                    opt = tf.train.RMSPropOptimizer(lr, FLAGS.RMSPROP_DECAY,\n                                                    momentum=FLAGS.RMSPROP_MOMENTUM,\n                                                    epsilon=FLAGS.RMSPROP_EPSILON)\n                else:\n                    raise NotImplementedError('Optimizer %s not implemented.'%FLAGS.optimizer)\n                # We must calculate the mean of each gradient. Note that this is the\n                # synchronization point across all towers.\n                grads = opt.compute_gradients(loss, colocate_gradients_with_ops=True)\n\n            grads = decay_grads(grads)\n            # Apply the gradients to adjust the shared variables.\n            apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n            # Add histograms for gradients.\n            for grad, var in grads:\n                if grad is not None:\n                    tf.histogram_summary(var.op.name + '/gradients', grad)\n\n            # Add histograms for trainable variables.\n            for var in tf.trainable_variables():\n                tf.histogram_summary(var.op.name, var)\n\n            # Track the moving averages of all trainable variables.\n            variable_averages = tf.train.ExponentialMovingAverage(\n                FLAGS.MOVING_AVERAGE_DECAY, global_step)\n\n            # Another possiblility is to use tf.slim.get_variables().\n            variables_to_average = (tf.trainable_variables() +\n                                    tf.moving_average_variables())\n            variables_averages_op = variable_averages.apply(variables_to_average)\n\n            # Group all updates to into a single train op.\n            batchnorm_updates_op = tf.group(*batchnorm_updates)\n            train_op = tf.group(apply_gradient_op, variables_averages_op,\n                                batchnorm_updates_op)\n\n            # Build the summary operation from the last tower summaries.\n            summary_op = tf.merge_all_summaries()\n```\n", "comments": ["Could you please paste your full error trace.\nAlso it could be due to your machine being only cuda3.0 compatible. ref: https://github.com/tensorflow/tensorflow/issues/25 . if this is the problem then just build tf from source and give cuda compatibility 3.0 in ./configure\n", "I am not sure of the reason. However, after a lot of failures, I change CUDNN from 4.0.4 to 5.1 and the problem is gone. Everything is fine now.\nThere might be some bugs to use CUDNN 4.0.4?\n\nBTW, r0.10 has a big chance to get \"BUILD file not found on package path\" when run configure, which is annoying.\n"]}, {"number": 4168, "title": "Cannot find cudnn, but cudnn (5005) works with my Theano install", "body": "I just installed tensorflow from source with no errors but upon running the example from the \"get started\" page...\n\n`$ cd tensorflow/models/image/mnist`\n`$ python convolutional.py`\n\nI receive the error that cudnn cannot be found. Here is the full output:\n\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: \nI tensorflow/stream_executor/cuda/cuda_dnn.cc:3304] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7465\npciBusID 0000:01:00.0\nTotal memory: 7.92GiB\nFree memory: 7.40GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\nInitialized!\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:208] could not find cudnnCreate in cudnn DSO; dlerror: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cudnnCreate\nAborted (core dumped)`\n\nI am not sure what to try because cudnn works with Theano on this machine, it also states that it finds the GPU.\n", "comments": ["Reinstalled and specified the cudnn version when prompted. Seems to work now.\n"]}, {"number": 4167, "title": "how to batch learning when using input builder function, in skflow", "body": "I have got some problem in modifying wide_n_deep_tutorial.py\nThe problem is the same as \nhttp://nticformation.com/solutions_problems.php?tuto=145994&subCategory=python+tensorflow+skflow&Category=python\n\nSorry for my poor english. And many thanks in advance.\n", "comments": ["This sort of question is better asked on stackoverflow, using the #tensorflow tag.\n"]}, {"number": 4166, "title": "tensorboard error, segement fault core dump", "body": "Hi, i got an error when using tensorboard.\n\nenvirnoment is:\n- python: 2.7\n- os: docker on centos 7, 64bit\n- tensorflow : 0.9.8\n\ni ran the tensorboard on mnist example,and get log file as `/tmp/mnist_logs/` .When i ran tensorboard directily as below ,error as `segement fault, core dump`\n\n```\ntensoboard --logdir=/tmp/mnist_logs/\n\n```\n\nif the command changed to \n\n```\npython /usr/lib/python2.7/site-packages/tensorflow/tensoboard/tensorboard.py --logdir=/tmp/mnist_logs/\n```\n\nerror same. Can anybody help me?\n", "comments": ["this may occur when there are too many logs/ checkpoints/ huge meta in that directory then tensorboard could go out of memory and throw this error. try keeping only 1 run in logdir.\n", "I checked the logdir ,there is only one file `events.out.tfevents.1472783342.docker01` , and the file size is 1.1MB .\n", "It seems a version bug, after updated to 0.10.0rc0  the tensorboard ran successful.Thank you anyway! \n"]}, {"number": 4165, "title": "WARNING: /home/dl/.cache/bazel/_bazel_dl/5c17eaa27118632cc679170e55a5e91b/external/boringssl_git/WORKSPACE:1", "body": "WARNING: /home/dl/.cache/bazel/_bazel_dl/5c17eaa27118632cc679170e55a5e91b/external/boringssl_git/WORKSPACE:1: Workspace name in /home/dl/.cache/bazel/_bazel_dl/5c17eaa27118632cc679170e55a5e91b/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: cuda7.5 cuDNN5\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n ls -l /usr/local/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189170  6\u6708 15 13:58 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16  6\u6708 15 13:58 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19  6\u6708 15 13:58 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596  6\u6708 15 13:58 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020  6\u6708 15 13:58 /usr/local/cuda/lib/libcudart_static.a\n\nI install tensorflow from the latest source\n1. The commit hash (`git rev-parse HEAD`)\n1f681d207f56fe3f2ef684873484011e8189ec05\n1. The output of `bazel version`\n   0.3.1\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nthis warning produced when I run ./configure. I worry that this will cause some trouble if I upgrade my tensorflow someday\n", "comments": ["It's just a warning, and it's an easy fix. Just rename the boringssl repo in the tensorflow workspace, and the dependent rules accordingly. We jus haven't gotten around to it. PRs welcome.\n\nAn update to bazel may break the TensorFlow build, at which point we will be very motivated to fix this quickly.\n", "hi, @jiqiujia @martinwicke Sorry to ping you. A naive question, how could I reproduce this warning.\nI run\n\n```\n./configure\nbazel build -c opt //tensorflow:libtensorflow.so\n```\n\nin CentOS 7 and could not find this warning.\n", "Yup, I think we fixed it, just forgot to close this bug. Thanks!\n"]}, {"number": 4164, "title": "Performance issue for Distributed TF", "body": "Hi All,\n\nWe just enable distributed TF to train our model,  but we get very slow performance comparing to train on single machine. We just setup two machines one ps, one worker, and each machine is Xeon CPU with nvidia K40 GPU:\nGPU 0: Quadro K420 (UUID: GPU-954fc4b7-6198-f5f9-1efb-69e73d001d5c)\nGPU 1: Tesla K40c (UUID: GPU-ac243789-45bd-448f-87d9-db21c1c88c87)\nGPU 2: Tesla K40c (UUID: GPU-d7b7783e-9041-e4e2-91f0-dc4188fd84e9)\n\nIf we train on one machine we can meet 200+examples/sec, but on two machines, it is just 20+ examples/sec. 10 times slower....\n\nOur two machines are connected in 1GBits network, and we find the network bandwidth is just 40%.\n\nSome topic discussed this issue, but I haven't got any solutions... \n\nOne machine:\n2016-09-02 09:04:35.330931: step 240, lr = 0.0001, loss = 1.59115 (107.4 examples/sec; 0.14900 sec/batch)\n2016-09-02 09:04:36.968806: step 250, lr = 0.0001, loss = 1.60836 (225.6 examples/sec; 0.07091 sec/batch)\n2016-09-02 09:04:39.037962: step 260, lr = 0.0001, loss = 1.56989 (212.2 examples/sec; 0.07541 sec/batch)\n2016-09-02 09:04:40.491946: step 270, lr = 0.0001, loss = 1.68502 (226.8 examples/sec; 0.07056 sec/batch)\n2016-09-02 09:04:42.754090: step 280, lr = 0.0001, loss = 1.65222 (214.8 examples/sec; 0.07450 sec/batch)\n2016-09-02 09:04:44.510914: step 290, lr = 0.0001, loss = 1.64218 (241.8 examples/sec; 0.06617 sec/batch)\n2016-09-02 09:04:47.301761: step 300, lr = 0.0001, loss = 1.64549 (219.8 examples/sec; 0.07278 sec/batch)\n2016-09-02 09:04:50.331641: step 310, lr = 0.0001, loss = 1.56824 (208.1 examples/sec; 0.07689 sec/batch)\n2016-09-02 09:04:52.485210: step 320, lr = 0.0001, loss = 1.49187 (226.4 examples/sec; 0.07067 sec/batch)\n2016-09-02 09:04:53.878589: step 330, lr = 0.0001, loss = 1.57825 (216.5 examples/sec; 0.07389 sec/batch)\n2016-09-02 09:04:55.613968: step 340, lr = 0.0001, loss = 1.57355 (227.0 examples/sec; 0.07047 sec/batch)\n2016-09-02 09:04:57.887825: step 350, lr = 0.0001, loss = 1.58644 (231.2 examples/sec; 0.06920 sec/batch)\n2016-09-02 09:05:00.791740: step 360, lr = 0.0001, loss = 1.57324 (224.1 examples/sec; 0.07139 sec/batch)\n2016-09-02 09:05:03.385293: step 370, lr = 0.0001, loss = 1.56881 (214.0 examples/sec; 0.07476 sec/batch)\n2016-09-02 09:05:05.684695: step 380, lr = 0.0001, loss = 1.57652 (223.3 examples/sec; 0.07166 sec/batch)\n2016-09-02 09:05:07.671139: step 390, lr = 0.0001, loss = 1.59098 (219.5 examples/sec; 0.07288 sec/batch)\n2016-09-02 09:05:09.384184: step 400, lr = 0.0001, loss = 1.57292 (222.9 examples/sec; 0.07177 sec/batch)\n2016-09-02 09:05:12.028866: step 410, lr = 0.0001, loss = 1.59291 (224.0 examples/sec; 0.07144 sec/batch)\n2016-09-02 09:05:13.912951: step 420, lr = 0.0001, loss = 1.56065 (206.7 examples/sec; 0.07741 sec/batch)\n2016-09-02 09:05:15.567746: step 430, lr = 0.0001, loss = 1.48168 (220.1 examples/sec; 0.07270 sec/batch)\n2016-09-02 09:05:17.913313: step 440, lr = 0.0001, loss = 1.52358 (220.5 examples/sec; 0.07256 sec/batch)\n2016-09-02 09:05:20.215508: step 450, lr = 0.0001, loss = 1.37813 (163.3 examples/sec; 0.09800 sec/batch)\n2016-09-02 09:05:21.685596: step 460, lr = 0.0001, loss = 1.66846 (237.3 examples/sec; 0.06741 sec/batch)\n2016-09-02 09:05:24.225214: step 470, lr = 0.0001, loss = 1.70520 (219.6 examples/sec; 0.07287 sec/batch)\n2016-09-02 09:05:26.203767: step 480, lr = 0.0001, loss = 1.58498 (234.3 examples/sec; 0.06827 sec/batch)\n2016-09-02 09:05:28.258299: step 490, lr = 0.0001, loss = 1.50158 (226.4 examples/sec; 0.07067 sec/batch)\n\ntwo machines:\nNFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2016-09-01 17:16:20.459047: step 12450, loss = 0.66(21.7 examples/sec; 0.737  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:16:43.294597: step 12480, loss = 0.50(20.7 examples/sec; 0.773  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:06.094232: step 12510, loss = 0.54(21.8 examples/sec; 0.733  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:29.405047: step 12540, loss = 0.51(20.5 examples/sec; 0.781  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:52.529421: step 12570, loss = 0.37(20.8 examples/sec; 0.768  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:18:15.177700: step 12600, loss = 0.49(21.3 examples/sec; 0.750  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:18:38.046958: step 12630, loss = 0.43(21.9 examples/sec; 0.731  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:19:00.978593: step 12660, loss = 0.48(20.7 examples/sec; 0.773  sec/batch)\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2016-09-01 17:19:24.747284: step 12690, loss = 0.47(20.7 examples/sec; 0.772  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:10.874324: step 12750, loss = 0.58(20.3 examples/sec; 0.788  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:33.881611: step 12780, loss = 0.49(21.0 examples/sec; 0.762  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:56.813524: step 12810, loss = 0.63(21.2 examples/sec; 0.755  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:21:19.631475: step 12840, loss = 0.36(21.0 examples/sec; 0.763  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:21:42.239483: step 12870, loss = 0.30(20.7 examples/sec; 0.773  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:22:04.912129: step 12900, loss = 0.75(21.3 examples/sec; 0.750  sec/batch)\n", "comments": ["You haven't said anything specific about the structure of your model, or exactly what about this situation surprises you.  \n\nMany or most models will run faster on a single GPU than they will when split across multiple GPUs, if it's possible to fit on a single GPU.\n\nHere's why: The latency per step is lower bounded by the most expensive path through a DAG where each node is a kernel execution, whose cost is the time that kernel takes, and each edge is a data dependency, whose cost is the time it takes to transfer the data from the computation that produces it to the computation where it is consumed.  If the model runs completely on one GPU, the data transfer times are all zero, and the overall execution time is just the sum of the kernel execution times.  GPU computations can be very fast, from a few microseconds to a few milliseconds.  By contrast, data transfers by RPC and even via PCIe bus are relatively slow.  \n\nConsider the following example.  We'd like to do a dot product of two large vectors, of 10^9 float32 values, all of which reside on GPU0 to begin with.  We also have GPU1 available, on the same PCIe bus with a bandwidth of 6GB/s.  Naively it seems like partitioning the computation between devices might be a good idea.  Suppose both GPUs have an internal memory bandwidth of 200GB/s.  Based on that limitation, doing the dot product just inside GPU0 should take (10^9 \\* 4 / 200x10^9) secs = 0.02 sec.  Half of the computation would take 0.01 sec.  But copying 1/2 of the data between GPUs at 6GB/s would take 0.5 \\* 4 / 6 = 0.33 sec, much longer than the on-GPU computation.  If the data transfer time is even slower, say 10Gbit/sec over a typical RPC network link, then the disadvantage is even greater.\n\nBack to your specific case: It sounds like you're trying a separate ps/worker architecture.  For the reasons just cited, it should be clear that when using a single worker it's going to be slower; this sort of architecture is only advantageous if you use a lot of workers, so that the wall-clock training time is reduced.  \n\nBut maybe you're wondering why the time is 10x slower, and yet by your measurement the network is only 40% busy?  There could be lots of reasons.  1Gb/s is a slow network by contemporary standards.  It's usually hard to get more than 80% of the theoretical capacity of a network, and maybe there's other traffic going through it competing with your workload.  There may be other jobs timesharing with your TF jobs on both machines that are competing for kernel resources that interfere with RPC latency.  If you're getting 80% of the theoretical capacity going in each direction, when there's something to send, and measuring overall capacity as bidirectional, then you could see 40% bandwidth consumption when your job is completely network bound.\n", "I am AlvinChen13's colleague and I am going to give more details on the structure. We uses CustomNet CNN to do image classification such as gender detection. We execute our CNN model under the following two cases.\n\n(1) Single Server Case - We mimic the google cifar muti-GPU model to modify our CNN to be able to execute on two GPUs (both GPUs are K40 as mentioned by AlvinChen13 installed on a single server)\nThe training performance is ~210 examples/sec if we train the CNN on both GPUs and 150 examples/sec if we train CNN only on a single GPU (GPU0 in our case).\n\n(2) Distributed Case: We mimic the google Inception model to modify our CNN to be able to execute on one server with one PS and one worker (worker uses only one GPU for training).\nThe performance is almost halved (~60 examples/sec).\nThen, we further extend our case, in which we use one PS and two workers. PS and one worker is placed on one server and the other worker is placed on another server. Two servers are connected by Gigabyte network switch. During training, we disabled almost all other network traffic within the network. \nThe performance is getting worse again, where worker0 and worker1 are both trained at ~20 examples/sec. (We use sync training method)\n\nThe number surprised us. From our experiment, the performance keeps getting worse if we add more servers. In our case, single server with multiple GPU trains at hundreds examples/sec. One worker trains at 60 examples/sec (worse than single server case). Two workers train at 40 examples/sec combined (20 examples/sec \\* 2 workers) which is even worse than single worker. Do you have any idea on how it happens?\n", "Most likely, somewhere, some resource is periodically over-committed, causing everything else to wait.  You might try using the tools described here #1824 \nto look at the time intervals between operations.  However, you could also use other tools to monitor memory use (is anything about your setup causing swapping), hack the code to time individual RPCs and operations at the PS server, and other techniques to look for operations that violate your assumptions about how they should be behaving.  If you're doing synchronous steps, look for which operation is last to complete at join points, and try to figure out why.\n", "@heroleap \nI think I am having the exact same problem as yours, and my approach is also similar to yours. BTW, I am now using 40Gb network, I do notice some performance increase compare to 1G (improve from 10 times slower to maybe 4 times slower), and my network usage is only about 10% I believe. There could be a lot of different reasons to cause this slowness. It's good if we can do some joint experiment and figure this out together? \n", "@AlvinChen13 \nHi Alvin~\n\nI have tried distributed inception model in https://github.com/tensorflow/models.git with 1 worker\uff0c2 workers\uff0c4 workers on GPU(K20C) server without infinityband.\ncompared with single GPU version, distributed looks like can't reduce any training time to achieve the same error rate.\n"]}, {"number": 4163, "title": "Add more capabilities to the CMake build", "body": "This commit includes changes that enable the following:\n- Building protobuf from source.\n- Building grpc from source.\n- Building the TensorFlow distributed runtime.\n- Building the TensorFlow Python bindings (excluding tf.contrib).\n\nThis commit also includes minor changes to remove the implicit\ndependency between a CPU-only build of the runtime and stream executor,\nand makes minor changes for compatibility with Protobuf 3.0.0 (regarding\ncompatibility between Protobuf's and TensorFlow's int64 representation).\n", "comments": ["@mrry, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ebrevdo, @tensorflower-gardener and @ageron to be potential reviewers\n", "Thanks, @mention-bot, but I think @keveman would be a better reviewer :).\n\n@keveman: Can you pay particular attention to the C++ changes? Each of them was done mostly to get things to build, but I think they are all uncontroversial.\n", "C++ changes look good to me. Feel free to merge.\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 4162, "title": "contrib.layers: TypeError: optimize_loss() got an unexpected keyword argument 'summaries'", "body": "Trying to provide a custom list of summaries to `tf.contrib.layers.optimize_loss` fails \n\n```\n>>> import tensorflow as tf\n>>> tf.contrib.layers.optimize_loss(tf.constant(1), tf.constant(1), 0.01, tf.train.AdamOptimizer, summaries=['loss', 'gradients'])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: optimize_loss() got an unexpected keyword argument 'summaries'\n>>> \n```\n\nTested on a fresh 0.10.0rc0 install, as well as in the `tensorflow/tensorflow:nightly` docker image. \n\nLittle mystified by how this is occuring since the it's correct in source. I suspect it has something to do with everything being brought into the layers namespace. \n", "comments": ["Just checked with current version (0.11rc1) - doesn't seem to have this issue. Please re-open if it still persists on your side.\n"]}, {"number": 4161, "title": "Update installation instructions to use cuDNN v5 across builds.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke, @johann-petrak and @vrv to be potential reviewers\n", "Please don't merge until binaries are updated.\n", "Unfortunately conflicts due to r0.10 rename I think\n", "@yifeif What's the status of this PR?\n", "closing this one and reopen a new one.\n"]}]