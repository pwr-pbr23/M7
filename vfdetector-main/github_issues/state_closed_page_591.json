[{"number": 35946, "title": "[TFLite int16] Symmetric 16-bit activations and 8-bit weights: reference kernel CONV_2D", "body": "This PR is one of steps to extend 8-bit quantization to support symmetric 16-bit activations.\r\n\r\nEach activation is of type int16 and symmetric around zero. The weight tensor precision remains at 8-bit signed values. The bias is set to int64 precision.\r\n\r\nIn this PR we introduce implementation and tests for CONV_2D kernel reference function.\r\nThe specification of this operator:\r\n\r\nCONV_2D \r\n\u202f Input 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f Input 1 (Weight): \r\n\u202f \u202f data_type \u202f: int8 \r\n\u202f \u202f range \u202f \u202f \u202f: [-127, 127] \r\n\u202f \u202f granularity: per-axis (dim = 0), zero_point=0 \r\n\u202f Input 2 (Bias): \r\n\u202f \u202f data_type \u202f: int64 \r\n\u202f \u202f range \u202f \u202f \u202f: [-(1<<39), (1<<39)-1] \r\n\u202f \u202f granularity: per-axis (dim = 0), zero_point=0 \r\n\u202f \u202f restriction: (scale, zero_point) = (input0_scale * input1_scale[...], 0) \r\n\u202f Output 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768,32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0", "comments": ["Hi @suharshs , @jdduke ! \r\nAs we have discussed, I have created a separate PR for CONV_2D kernel reference functions.\r\nCould you please review it ? \r\nThanks!\r\n", "Hi @ravikyram, the label \"awaiting review\" has been deleted from this PR, although this PR is still under review.\r\nCould you please check this ? Thanks ", "> Hi @ravikyram, the label \"awaiting review\" has been deleted from this PR, although this PR is still under review.\r\n> Could you please check this ? Thanks\r\n\r\n@wwwind thanks for the heads up.\r\n", "@wwwind can you please resolve conflicts? Thanks!", "Hi @jianlijianli Could you please re-approve this PR ? I had to push a fix for the merge conflict that has removed approval. Thanks!", "Hi @jianlijianli, Sorry to bother, but could you please re-approve this PR ? I had to resolve a merge conflict. Thanks!", "@wwwind Can you please address Ubuntu Sanity errors? Thanks!", "Hi @jianlijianli Sorry to bother you again, but could you please re-approve this PR for CONV_2D ? I had to push a fix due to the sanity check(do_buildifier). \r\nThanks!", "Thanks!", "Hi @rthadur Could you please check this PR ? Is it stuck internally.\r\nIt has been approved ages ago. Thanks ", "@wwwind it is struck with internal error , can you please fix this \r\n\r\n`./third_party/tensorflow/lite/kernels/internal/reference/integer_ops/conv.h:135:15: error: unused variable 'input_offset' [-Werror,-Wunused-variable]\r\n  const int32 input_offset = params.input_offset;  // r = s(q - Z)\r\n              ^\r\n./third_party/tensorflow/lite/kernels/internal/reference/integer_ops/conv.h:142:15: error: unused variable 'output_offset' [-Werror,-Wunused-variable]\r\n  const int32 output_offset = params.output_offset;`", "Hi @jianlijianli , Could you please re-approve this PR ? I had to push a fix for a warning.\r\nThanks!", "\r\n@wwwind it is struck with internal error , can you please fix this\r\n\r\n```\r\n./third_party/tensorflow/lite/kernels/test_util.h:269:19: error: unused variable 'num_inputs' [-Werror,-Wunused-variable]\r\n    const int32_t num_inputs = input_data.size();\r\n                  ^\r\n1 error generated.\r\n\r\nIn file included from third_party/tensorflow/lite/kernels/test_util.cc:15:\r\n./third_party/tensorflow/lite/kernels/test_util.h:269:19: error: unused variable 'num_inputs' [-Werror,-Wunused-variable]\r\n    const int32_t num_inputs = input_data.size();\r\n                  ^\r\n1 error generated.\r\nBroken by missing target //knowledge/cerebra/sense/text_classifier/lib3/utils/tflite:dist_diversification_test\r\nIn file included from knowledge/cerebra/sense/text_classifier/lib3/utils/tflite/dist_diversification_test.cc:6:\r\n./third_party/tensorflow/lite/kernels/test_util.h:269:19: error: unused variable 'num_inputs' [-Werror,-Wunused-variable]\r\n    const int32_t num_inputs = input_data.size();\r\n                  ^\r\n1 error generated.\r\n```", "Hi @rthadur ! I pushed a small fix for the reported failures. Could you please take a look and re-approve this PR ? Thanks!", "Hi @rthadur It looks that this PR has got the wrong label \"Reviewer Requested Changes\". \r\nI updated it and did small fixes. ", "Hi @jianlijianli ! Could you please re-approve this PR ? I updated it with the master and pushed a small fixes. Thanks!", "We're seeing a test failure:\r\n\r\n```\r\n[----------] 1 test from QuantizedConvPerChannelTest\r\n[ RUN      ] QuantizedConvPerChannelTest.FastKernelTest\r\nthird_party/tensorflow/lite/kernels/internal/common.h:175:46: runtime error: shift exponent 32 is too large for 32-bit type 'int'\r\n    #0 0x7f32e82b959d in tflite::MultiplyByQuantizedMultiplier(long, int, int) third_party/tensorflow/lite/kernels/internal/common.h:175:46\r\n    #1 0x7f32e82b6b76 in tflite::reference_integer_ops::ConvPerChannel(tflite::ConvParams const&, int const*, int const*, tflite::RuntimeShape const&, short const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, long const*, tflite::RuntimeShape const&, short*) third_party/tensorflow/lite/kernels/internal/reference/integer_ops/conv.h:202:32\r\n    #2 0x7f32e82b3066 in TryTestOneConvFilter third_party/tensorflow/lite/kernels/internal/conv_per_channel_quantized_16x8_test.cc:268:3\r\n    #3 0x7f32e82b3066 in tflite::(anonymous namespace)::QuantizedConvPerChannelTest_FastKernelTest_Test::TestBody() third_party/tensorflow/lite/kernels/internal/conv_per_channel_quantized_16x8_test.cc:327:5\r\n```", "Hi @jdduke, Thanks! I pushed a fix. Could you please re-approve ?\r\n\r\nSorry to bother, but this error looks due to -fsanitizer=undefined with clang.\r\nIs there any easy way to build and run these kernels tests with clang ?   \r\n\r\nThanks", "Hi @gbaned ! Sorry to bother, but is this PR stuck internally with errors ? Thanks", "> Hi @gbaned ! Sorry to bother, but is this PR stuck internally with errors ? Thanks\r\n\r\nHi @wwwind,  It is processing internally. We will let you know if we get internal errors. Thanks!"]}, {"number": 35945, "title": "Fix broken numa build", "body": "After 37f0ac13bdaf6f5c0015885ab70d49b4094feca5 my build was broken with the following error:\r\n\r\n```\r\n[0 / 21] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\nERROR: missing input file '//third_party/hwloc:include/private/autogen/config.h.in'\r\nERROR: /tensorflow_src/third_party/hwloc/BUILD.bazel:212:1: //third_party/hwloc:include_private_hwloc_autogen__config_h: missing input file '//third_party/hwloc:include/private/autogen/config.h.in'\r\nERROR: /tensorflow_src/tensorflow/cc/BUILD:507:1: Creating runfiles tree bazel-out/host/bin/tensorflow/cc/ops/parsing_ops_gen_cc.runfiles [for host] failed: build-runfiles failed: error executing command \r\n  (cd /home/byronyi/.cache/bazel/_bazel_byronyi/8c9ffbe253caf5ffa98d6d412a0cbaa1/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/bin:/usr/bin:/bin \\\r\n  /home/byronyi/.cache/bazel/_bazel_byronyi/install/84defa6eb1e9416bf92d6f89ab2d4f31/build-runfiles bazel-out/host/bin/tensorflow/cc/ops/parsing_ops_gen_cc.runfiles_manifest bazel-out/host/bin/tensorflow/cc/ops/parsing_ops_gen_cc.runfiles): Process terminated by signal 15: Process terminated by signal 15\r\nERROR: /tensorflow_src/third_party/hwloc/BUILD.bazel:212:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 39.257s, Critical Path: 1.42s\r\nINFO: 4 processes: 4 remote cache hit.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nThis PR fixes the error above.", "comments": ["Ping @mihaimaruseac for review.", "Although I think this has to be fixed internally as it seems to be caused by copybara", "Will fix internally, copybara doesn't catch all changes.\r\n\r\nFix should land in at most ~1hr.", "Should be fixed by d46b0e80e487f2cdb9463504a1ca8787352dab6f sorry for the delay"]}, {"number": 35943, "title": "fix libjpeg_turbo in valid names", "body": "Commit 6f9c242cd088bf2017566997c08aab95ff7dcee9 changed the name of the rule from jpeg to libjpeg_turbo. In order to use the system lib, this name has also be changed for the valid system libs.", "comments": []}, {"number": 35942, "title": "Loading model with tf.keras.models.load_model not working on multi GPU", "body": "**System information**\r\n- Custom code\r\n- TensorFlow version 2.1.0\r\n- Python version: 3.7\r\n- GPU model: 4 V100 GPUs on Kubernetes Engine\r\n\r\n**Describe the current behavior**\r\nOn multi GPU loading the model from a h5 file is not working. \r\n\r\n**Describe the expected behavior**\r\nSaving and reloading the model from a h5 file using model.save and  keras.models.load_model should work on both single and multi GPU.\r\n\r\n**Code to reproduce the issue**\r\n``` python \r\nimport tensorflow as tf \r\nimport os\r\nimport contextlib\r\nimport numpy as np\r\nimport tensorflow.keras as keras  \r\n\r\ndef get_model():\r\n    model = keras.Sequential([\r\n        keras.layers.Flatten(input_shape=(28, 28)),\r\n        keras.layers.Dense(10, activation=tf.nn.softmax)\r\n    ])\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                      loss='sparse_categorical_crossentropy')\r\n    return model\r\n\r\ndef get_model_path():\r\n    model_dir = '/tmp/m' + str(np.random.randint(0, 1000000))\r\n    os.makedirs(model_dir)\r\n    model_path = os.path.join(model_dir, 'model')\r\n    return model_path + \".h5\"\r\n\r\ndef attempt_save_and_reload(model_path, distributed_training=False):\r\n    fashion_mnist = keras.datasets.fashion_mnist\r\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n    train_images = train_images / 255.0\r\n    test_images = test_images / 255.0\r\n\r\n    with strategy.scope() if distributed_training else contextlib.nullcontext():\r\n        model = get_model()\r\n        model.fit(\r\n            train_images,\r\n            train_labels,\r\n            epochs=1,\r\n        )\r\n        model.save(model_path)\r\n        model = tf.keras.models.load_model(model_path)\r\n\r\nif __name__ == '__main__':\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    for distributed_training in [False, True]:\r\n        print('distributed training: ', distributed_training)\r\n        model_path = get_model_path()\r\n        try:\r\n            attempt_save_and_reload(model_path, distributed_training)\r\n        except Exception as e:\r\n            print('Exception raised: \\n', e)\r\n        print()\r\n```\r\n\r\n\r\n\r\n**other info/ logs**\r\nI need to use h5 files since saving the optimizer state does not work otherwise (see #33424).  The logs I get are: \r\n```\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\r\ndistributed training:  False\r\nTrain on 60000 samples\r\n60000/60000 [==============================] - 3s 52us/sample - loss: 0.5991\r\n\r\ndistributed training:  True\r\nTrain on 60000 samples\r\nINFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n60000/60000 [==============================] - 9s 152us/sample - loss: 0.6016\r\nException raised: \r\n `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.\r\n```\r\n\r\n", "comments": ["@ravikyram @ymodak The failure can also be observed in this [colab notebook](https://colab.research.google.com/drive/1nXw4V5LwtxrVNB8XlJ5TaOxbAOXRS1O3).", "The loading works if one saves and loads the model outside the distribution scope (i.e outside the with strategy.scope() block). Looks like one is not allowed to save or load models when in a cross device context. Is it safe to assume that from tf 2.1+ one can save and load keras models only outside the distribution scope? ", "@pidajay How would you fit the reloaded model using multiple GPUs then? Could you provide a code example?", "Loading and saving keras models inside a distribution strategy scope as TF SavedModel / Checkpoint format is supported and should work as expected. However loading .h5 format model inside a strategy scope is not yet supported.\r\n\r\nHowever as mentioned in one of the early thread ([#33424](https://github.com/tensorflow/tensorflow/issues/33424)), loading a SavedModel / Checkpoint doesn't restore the optimizer state. Before the issue is fixed, only model weights will be carried over and the training won't be resumed exactly where you left off.\r\n", "This appears not to work across different distribution strategies. I saved a model under a tpu distribution strategy and tried to load it with just cpu and it didn't work, producing the following error below. The model was saved with tf version 2.2.0-dev20200309 (in 2.1 save_model with a tpu distribution strategy didn't work) and tried to load with tf version 2.1.0-rc0.\r\n```\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/input_layer.py in __init__(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, **kwargs)\r\n     84                          'batch_input_shape argument to '\r\n     85                          'InputLayer, not both at the same time.')\r\n---> 86       batch_size = batch_input_shape[0]\r\n     87       input_shape = batch_input_shape[1:]\r\n     88     if kwargs:\r\n\r\nKeyError: 0\r\n```\r\n\r\nAlso loading weights appears not to work between the two environments.\r\n\r\n```\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in load_weights(self, filepath, by_name, skip_mismatch)\r\n    232         raise ValueError('Load weights is not yet supported with TPUStrategy '\r\n    233                          'with steps_per_run greater than 1.')\r\n--> 234     return super(Model, self).load_weights(filepath, by_name, skip_mismatch)\r\n\r\n...\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)\r\n   1108     \"\"\"\r\n   1109     if not self.is_compatible_with(other):\r\n-> 1110       raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\n   1111 \r\n   1112   def most_specific_compatible_shape(self, other):\r\n\r\nValueError: Shapes (512,) and (1,) are incompatible\r\n```\r\nIt looks like the number of layers is different between the two environments.\r\n\r\n\r\nThe  net of this is that there is no way to restore a saved model or checkpoints from 2.2.0-dev20200309 on tpu to 2.1 on cpu.\r\n\r\nThe model did load with `tf.saved_model.load` but it didn't work for inference, producing the following error. I tried loading the model under `strategy.scope()` to no avail.\r\n\r\n```\r\nInvalidArgumentError: Cannot assign a device for operation ... was explicitly assigned to /job:worker/replica:0/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.\r\n```", "training on tpu with 2.2.0-dev20200311 and loading under strategy.scope() on a different machine with gpu only runs on the cpu.\r\n\r\ntried `strategy = tf.distribute.OneDeviceStrategy('gpu:0')` to no avail.\r\n\r\ntried loading with `tf.saved_model.load` and inferencing with `model.signatures['serving_default']` and got the following error.\r\n\r\n```\r\nInvalidArgumentError: Cannot assign a device for operation StatefulPartitionedCall/sequential_6/efficientnet-b0/stem_bn/FusedBatchNormV3/AddN: {{node StatefulPartitionedCall/sequential_6/efficientnet-b0/stem_bn/FusedBatchNormV3/AddN}} was explicitly assigned to /job:worker/replica:0/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[StatefulPartitionedCall/sequential_6/efficientnet-b0/stem_bn/FusedBatchNormV3/AddN]] [Op:__inference_signature_wrapper_313855]\r\n```", "tried loading the model from `tf.keras.models.model_from_json` and got the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-72d87b8e325d> in <module>\r\n      2     model_config = json.load(json_file)\r\n      3 \r\n----> 4 model = tf.keras.models.model_from_json(model_config)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/model_config.py in model_from_json(json_string, custom_objects)\r\n    114   config = json.loads(json_string)\r\n    115   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n--> 116   return deserialize(config, custom_objects=custom_objects)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n    107       module_objects=globs,\r\n    108       custom_objects=custom_objects,\r\n--> 109       printable_module_name='layer')\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    371             custom_objects=dict(\r\n    372                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 373                 list(custom_objects.items())))\r\n    374       with CustomObjectScope(custom_objects):\r\n    375         return cls.from_config(cls_config)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py in from_config(cls, config, custom_objects)\r\n    398     for layer_config in layer_configs:\r\n    399       layer = layer_module.deserialize(layer_config,\r\n--> 400                                        custom_objects=custom_objects)\r\n    401       model.add(layer)\r\n    402     if (not model.inputs and build_input_shape and\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n    107       module_objects=globs,\r\n    108       custom_objects=custom_objects,\r\n--> 109       printable_module_name='layer')\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    371             custom_objects=dict(\r\n    372                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 373                 list(custom_objects.items())))\r\n    374       with CustomObjectScope(custom_objects):\r\n    375         return cls.from_config(cls_config)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in from_config(cls, config, custom_objects)\r\n    980     \"\"\"\r\n    981     input_tensors, output_tensors, created_layers = reconstruct_from_config(\r\n--> 982         config, custom_objects)\r\n    983     model = cls(inputs=input_tensors, outputs=output_tensors,\r\n    984                 name=config.get('name'))\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in reconstruct_from_config(config, custom_objects, created_layers)\r\n   2012   # First, we create all layers and enqueue nodes to be processed\r\n   2013   for layer_data in config['layers']:\r\n-> 2014     process_layer(layer_data)\r\n   2015   # Then we process nodes in order of layer depth.\r\n   2016   # Nodes that cannot yet be processed (if the inbound node\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in process_layer(layer_data)\r\n   1994       from tensorflow.python.keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\r\n   1995 \r\n-> 1996       layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n   1997       created_layers[layer_name] = layer\r\n   1998 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n    107       module_objects=globs,\r\n    108       custom_objects=custom_objects,\r\n--> 109       printable_module_name='layer')\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    360     config = identifier\r\n    361     (cls, cls_config) = class_and_config_for_serialized_keras_object(\r\n--> 362         config, module_objects, custom_objects, printable_module_name)\r\n    363 \r\n    364     if hasattr(cls, 'from_config'):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)\r\n    319   cls = get_registered_object(class_name, custom_objects, module_objects)\r\n    320   if cls is None:\r\n--> 321     raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\n    322 \r\n    323   cls_config = config['config']\r\n\r\nValueError: Unknown layer: FixedDropout\r\n```", "Hello All,\r\n\r\nSame issue using Colab (TF 2.2 RC2).\r\nMy test-model run and then it was saved without implementing TPU or GPU.  I wanted to test the behaviour. Now loading it, I have:\r\n\r\n```\r\nnew_model = tf.keras.models.load_model('model.h5')\r\nnew_model.summary()\r\n```\r\n\r\n```\r\n    319   cls = get_registered_object(class_name, custom_objects, module_objects)\r\n    320   if cls is None:\r\n--> 321     raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\n    322 \r\n    323   cls_config = config['config']\r\n\r\nValueError: Unknown layer: FixedDropout\r\n\r\n```\r\n\r\n", "@CalebEverett For saving and loading across different strategies, can you try it in tf 2.2? It might haven been fixed. BTW when using load_weights / checkpoint.restore, if the model that you are restoring into was created in a scope (potentially different than the save scope) then you need to call restore within that scope.\r\n", "@murdav can you post the colab link that leads to this error?", "@ckkuang sorry for later reply. It was my fault. Basically I'm using https://github.com/qubvel/efficientnet and I have to import the `load_model` like:\r\n\r\n```\r\nimport efficientnet.tfkeras\r\nfrom tensorflow.keras.models import load_model\r\n\r\nmodel = load_model('path/to/model.h5')\r\n```\r\n\r\nOtherwise and logically I have `ValueError: Unknown layer: FixedDropout`.\r\n\r\nDavide\r\n", "Having the same issue as CalebEverett \r\nIt seems to be that loading expects the input shape to be a list and it is a tupel?", "@MariaHeuss This is fixed with TensorFlow 2.2 Thanks!\r\n\r\nRest of the users - Can you please post a new github issue explaining your problem.\r\nWe would like to keep each issue thread specific to one problem. Thank you!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "<!--\n/* Font Definitions */\n@font-face\n\t{font-family:\"Cambria Math\";\n\tpanose-1:2 4 5 3 5 4 6 3 2 4;}\n@font-face\n\t{font-family:Calibri;\n\tpanose-1:2 15 5 2 2 2 4 3 2 4;}\n/* Style Definitions */\np.MsoNormal, li.MsoNormal, div.MsoNormal\n\t{margin:0cm;\n\tmargin-bottom:.0001pt;\n\tfont-size:11.0pt;\n\tfont-family:\"Calibri\",sans-serif;}\na:link, span.MsoHyperlink\n\t{mso-style-priority:99;\n\tcolor:blue;\n\ttext-decoration:underline;}\n.MsoChpDefault\n\t{mso-style-type:export-only;}\n@page WordSection1\n\t{size:612.0pt 792.0pt;\n\tmargin:72.0pt 72.0pt 72.0pt 72.0pt;}\ndiv.WordSection1\n\t{page:WordSection1;}\n-->\u00a0\u00a0\u00a0\u00a0From: Chenkai KuangSent: jeudi 16 avril 2020 23:34To: tensorflow/tensorflowCc: Eric Fourni\u00e9; ManualSubject: Re: [tensorflow/tensorflow] Loading model with tf.keras.models.load_model not working on multi GPU (#35942)\u00a0@CalebEverett For saving and loading across different strategies, can you try it in tf 2.2? It might haven been fixed. BTW when using load_weights / checkpoint.restore, if the model that you are restoring into was created in a scope (potentially different than the save scope) then you need to call restore within that scope.\u2014You are receiving this because you are subscribed to this thread.Reply to this email directly, view it on GitHub, or unsubscribe.\u00a0", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35942\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35942\">No</a>\n"]}, {"number": 35941, "title": "[XLA] Small update on the vectorisation of elementwise operations", "body": "- XLA was unrolling the sin, cos, power and atan2 operations, but LLVM doesn't vectorize them as they have branches inside the elementwise computation.\r\nAs the unrolling hurt performance, block the unrolling for those operations.\r\nBy default on a Titan V, when doing the Sin of 16m elements, it takes 274us. Now it takes 224us.\r\n- Also fix XLA profiler to display correctly some operation as TFLOPS isntead of FLOPS.", "comments": ["> Also fix XLA profiler to display correctly some operation as TFLOPS isntead of FLOPS.\r\n\r\nI think this is not in this PR?\r\n\r\nAlso would it be really hard to write FileCheck tests?\r\n", "for the profiler bug, it is in the second commit of this PR: https://github.com/tensorflow/tensorflow/pull/35941/commits/d062c9a26ef4df7ed3db190eecbfe5bae85b9f23\r\n\r\nI'll check to add tests.", "I pushed the requested extra test."]}, {"number": 35940, "title": "Incorrect mention of dataset....", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c05_exercise_rock_paper_scissors.ipynb\r\n---------------------------------------------------------------\r\n![Screenshot from 2020-01-16 18-37-30](https://user-images.githubusercontent.com/29497701/72527695-5d004900-388f-11ea-84f8-57ed0c12c915.png)\r\n----------------------------------------------------------------\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI think this exercise doesn't make use of cats_vs_dogs dataset, right??\r\n\r\n### Clear description\r\n\r\nIn place of `cats_vs_dogs` dataset, `rock_paper_scissors` dataset should be mentioned.\r\n\r\n### Submit a pull request?\r\n\r\nYes, shortly", "comments": ["the dataset is not loading when opening the code in google colab. both dog vs cats and rock_paper_scissors.", " Number 5 is an exercise with parts the reader needs to complete. Number 6 is the solution, it works fine in colab.\r\n\r\n@Rinkal2  can you be more specific? what is the problem you're seeing? ", "Closing this issue since the associated PR has merged. \r\nFeel free to reopen if necessary. Thanks!"]}, {"number": 35939, "title": "Problems when compiling with Clang", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.10\r\n- TensorFlow installed from (source or binary): source (for inference), python (for training)\r\n- Tensorflow version (commit SHA if source): 1768c8f2fa155d4c6406e8ff7addf374c83de7ad for inference, release 2.0 or 2.1 for training.\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): [RIOT](https://github.com/RIOT-OS/RIOT), ARM Cortex-M3/4/7\r\n\r\n**Describe the problem**\r\n\r\nI recently ported TensorFlow Lite to RIOT, an operating system for microcontrollers, using the package mechanism provided by the RIOT build system: this allows to build TensorFlow-Lite on the fly when generating a RIOT firmware and to use it's API from a RIOT application.\r\nSee https://github.com/RIOT-OS/RIOT/pull/12847 for details.\r\n\r\nRIOT provides support for many boards including many ARM based boards. It also supports both the GCC and the LLVM (Clang) toolchains.\r\n\r\nOur CI showed problems when building with Clang, see below for details and [this issue](https://github.com/RIOT-OS/RIOT/issues/13133). In short, the firmware is crashing when evaluating the FullyConnected operator on ARM Cortex-M but the same thing works fine with GCC.\r\n\r\nThe [example application in RIOT](https://github.com/RIOT-OS/RIOT/tree/master/tests/pkg_tensorflow-lite) is just running a very basic MLP model (with Dense and Softmax layers) on an image taken from the MNIST dataset. For details of the application, you can have a look at the [main_functions.cc file](https://github.com/RIOT-OS/RIOT/blob/master/tests/pkg_tensorflow-lite/mnist/main_functions.cc) and [the script](https://github.com/RIOT-OS/RIOT/blob/master/tests/pkg_tensorflow-lite/mnist/mnist_mlp.py) used to generate the flatbuffers file containing the model.\r\n\r\nNote that `hello_world` example is running fine, even when built with LLVM so I don't know what's wrong here: is the Python script or is it the way the `MicroMutableOpResolver` is built ?\r\n\r\nSorry for the long description but I wanted to make as complete as possible.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n- Software required on the host:\r\n  - **clang**: on Ubuntu, can be installed with `apt install clang`\r\n  - **ARM Cortex-M gdb**, from the [GNU ARM toolchain](https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm) (this is for the debug command below).\r\n  - pyocd, can be installed with `pip3 install --user pyocd`\r\n  - socat, on Ubuntu, can be installed with `apt install socat`\r\n- Build/flash/run the default tensorflow-lite example of RIOT on an [nrf52832-mdk](https://wiki.makerdiary.com/nrf52832-mdk/):\r\n```\r\nDEVELHELP=1 RIOT_TERMINAL=socat TOOLCHAIN=llvm make BOARD=nrf52832-mdk -C tests/pkg_tensorflow-lite flash term\r\n```\r\n- You can check where the program crashed using the `debug` target (just follow the instructions reported by the RIOT crash):\r\n```\r\nmake BOARD=nrf52832-mdk -C tests/pkg_tensorflow-lite debug\r\n```\r\nThen in gdb:\r\n```gdb\r\nset $pc=0x8006b04\r\nframe 0\r\nbt\r\n```\r\nYou get the following output:\r\n```gdb\r\nReading symbols from /work/riot/RIOT/tests/pkg_tensorflow-lite/bin/stm32f723e-disco/tests_pkg_tensorflow-lite.elf...\r\nRemote debugging using :3333\r\nhard_fault_handler (sp=0x20001bb8 <setup()::static_interpreter+16>, corrupted=1132396544, \r\n    exc_return=536882079, r4_to_r11_stack=0x310) at vectors_cortexm.c:393\r\n393\t    __BKPT(1);\r\n(gdb) set $pc=0x8006bbe\r\n(gdb) frame 0\r\n#0  tflite::GetOptionalInputTensor (context=0x20001bb8 <setup()::static_interpreter+16>, node=0xc46c, \r\n    index=2)\r\n    at /work/riot/RIOT/tests/pkg_tensorflow-lite/bin/pkg/stm32f723e-disco/tensorflow-lite/tensorflow/lite/kernels/kernel_util.h:80\r\n80\t  const bool use_tensor = index < node->inputs->size &&\r\n(gdb) bt\r\n#0  tflite::GetOptionalInputTensor (context=0x20001bb8 <setup()::static_interpreter+16>, node=0xc46c, \r\n    index=2)\r\n    at /work/riot/RIOT/tests/pkg_tensorflow-lite/bin/pkg/stm32f723e-disco/tensorflow-lite/tensorflow/lite/kernels/kernel_util.h:80\r\n#1  tflite::ops::micro::fully_connected::Eval (context=0x20001bb8 <setup()::static_interpreter+16>, \r\n    node=0xc46c) at fully_connected.cc:172\r\n#2  0x08002276 in tflite::MicroInterpreter::Invoke (this=0x20001ba8 <setup()::static_interpreter>)\r\n    at micro_interpreter.cc:201\r\n#3  0x080018b0 in setup () at main_functions.cc:100\r\n#4  0x0800164e in main (argc=5, argv=0xc46c) at main.cpp:28\r\n(gdb) quit\r\n```\r\n\r\nThe board is not very important, this command can be adapted for a lot of other ARM based boards supported by RIOT: STM32 nucleo, kinetis, etc. Note that you'll have to install the right tool for flashing the boards (OpenOCD, JLink, etc) depending on the board configuration.\r\n\r\n", "comments": ["Hi Alexandre,\r\n\r\nI wonder if we can have more specific error other than hard_fault_handler.\r\n\r\nLooking at the backtrace\r\n\r\n#0  tflite::GetOptionalInputTensor (context=0x20001bb8 <setup()::static_interpreter+16>, node=0xc46c, \r\n    index=2)\r\n    at /work/riot/RIOT/tests/pkg_tensorflow-lite/bin/pkg/stm32f723e-disco/tensorflow-lite/tensorflow/lite/kernels/kernel_util.h:80\r\n80\t  const bool use_tensor = index < node->inputs->size &&\r\n\r\nProbably this line triggered invalid memory access? Could you check to make sure none of these pointers are nullptr?\r\n\r\nThere could be a problem with the model as well. Is your model running fine on x64 and other embedded platform?\r\n\r\n> In short, the firmware is crashing when evaluating the FullyConnected operator on ARM Cortex-M but the same thing works fine with GCC.\r\nI didn't find arm-none-eabi-clang++  is this something being developed?\r\n\r\nThanks,", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35939\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35939\">No</a>\n", "@aabadie It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest stable version of TF 2.5.0 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35939\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35939\">No</a>\n"]}, {"number": 35938, "title": "tf.cast on native python float to dtype=tf.float64 leads to loss of precision", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary (pip install)\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0, v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.cast()` on a python float (e.g, a literal float constant, not a numpy float/array type) does an implicit conversion to the TensorFlow default dtype tf.float32, which can result in loss of precision when intending to cast to tf.float64:\r\n```python\r\n>>> tf.cast(0.2, tf.float64)\r\n<tf.Tensor: id=37, shape=(), dtype=float64, numpy=0.20000000298023224>\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n```python\r\n>>> tf.cast(0.2, tf.float64)\r\n<tf.Tensor: id=37, shape=(), dtype=float64, numpy=0.2>\r\n```\r\n\r\n**Code to reproduce the issue**\r\nSee above.\r\n\r\n**Other info / logs**\r\nThis was discovered as a bug in GPflow, which we built a work-around for in https://github.com/GPflow/GPflow/pull/1211 - but this is a pervading issue, and it would be good to fix this upstream instead of having to write and use a `gpflow.cast()` everywhere just to work around having potentially passed in a python float.", "comments": ["The simple workaround would be to add\r\n```python\r\n    if not tf.is_tensor(value):\r\n        return tf.convert_to_tensor(value, dtype)\r\n```\r\nto the beginning of the definition of `tf.cast()`.\r\n\r\nThis is related to tensorflow#26033 - not having to support our own \"default dtype\" implementation would remove the need for a lot of the calls to tf.cast.", "Added PR #35961 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35938\">No</a>\n"]}, {"number": 35937, "title": "Added Usage Example for Keras Dropout Layer", "body": "", "comments": ["Hi! Unfortunately, this PR is a duplicate of #35715."]}, {"number": 35936, "title": "GPU Support Instructions for CUDA 10 (Ubuntu 16.04) refer to non-existent package", "body": "The current GPU Support instructions for CUDA 10 on Ubuntu 16.04 refers to a package version that does not exist in the [NVIDIA ML repo](https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu1604/x86_64/): `libnvinfer5=6.0.1-1+cuda10.1`\r\n\r\n#### Ubuntu 16.04 (CUDA 10)\r\n\r\n<pre class=\"prettyprint lang-bsh\">\r\n...\r\n# Install TensorRT. Requires that libcudnn7 is installed above.\r\n<code class=\"devsite-terminal\">sudo apt-get install -y --no-install-recommends libnvinfer5=6.0.1-1+cuda10.1 \\\r\n    libnvinfer-dev=6.0.1-1+cuda10.1\r\n</code>\r\n</pre>\r\n\r\nThis results in an error when trying to install:\r\n```\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Version \u20186.0.1-1+cuda10.1\u2019 for \u2018libnvinfer5\u2019 was not found\r\n```\r\n\r\nChanging this to `libnvinfer6=6.0.1-1+cuda10.1` seems to work and run happily.", "comments": ["@JMMarchant , Close this issue if the doc is already corrected with the alternative PR merge.", "@JMMarchant, Associated PR has been merged, can we close this issue. Thanks!", "Looks good! Thanks all!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35936\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35936\">No</a>\n"]}, {"number": 35935, "title": "conda install tensorflow or any command to install the Tensroflow is not working.", "body": "(base) C:\\Users\\vishwasnarayan>conda install -c anaconda tensorflow\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: C:\\Users\\vishwasnarayan\\AppData\\Local\\Continuum\\anaconda3\r\n\r\n  added / updated specs:\r\n    - tensorflow\r\n\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    _tflow_select-2.3.0        |              mkl           3 KB  anaconda\r\n    absl-py-0.8.1              |           py37_0         162 KB  anaconda\r\n    astor-0.8.0                |           py37_0          45 KB  anaconda\r\n    gast-0.2.2                 |           py37_0         138 KB  anaconda\r\n    google-pasta-0.1.8         |             py_0          43 KB  anaconda\r\n    keras-applications-1.0.8   |             py_0          33 KB  anaconda\r\n    keras-preprocessing-1.1.0  |             py_1          36 KB  anaconda\r\n    libmklml-2019.0.5          |                0        21.4 MB  anaconda\r\n    libprotobuf-3.11.2         |       h7bd577a_0         2.3 MB  anaconda\r\n    markdown-3.1.1             |           py37_0         132 KB  anaconda\r\n    opt_einsum-3.1.0           |             py_0          54 KB  anaconda\r\n    protobuf-3.11.2            |   py37h33f27b4_0         597 KB  anaconda\r\n    tensorboard-2.0.0          |     pyhb38c66f_1         3.3 MB  anaconda\r\n    tensorflow-2.0.0           |mkl_py37he1bbcac_0           4 KB  anaconda\r\n    tensorflow-base-2.0.0      |mkl_py37hd1d5974_0        41.9 MB  anaconda\r\n    tensorflow-estimator-2.0.0 |     pyh2649769_0         272 KB  anaconda\r\n    termcolor-1.1.0            |           py37_1           7 KB  anaconda\r\n    ------------------------------------------------------------\r\n                                           Total:        70.4 MB\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  _tflow_select      anaconda/win-64::_tflow_select-2.3.0-mkl\r\n  absl-py            anaconda/win-64::absl-py-0.8.1-py37_0\r\n  astor              anaconda/win-64::astor-0.8.0-py37_0\r\n  gast               anaconda/win-64::gast-0.2.2-py37_0\r\n  google-pasta       anaconda/noarch::google-pasta-0.1.8-py_0\r\n  grpcio             pkgs/main/win-64::grpcio-1.16.1-py37h351948d_1\r\n  keras-applications anaconda/noarch::keras-applications-1.0.8-py_0\r\n  keras-preprocessi~ anaconda/noarch::keras-preprocessing-1.1.0-py_1\r\n  libmklml           anaconda/win-64::libmklml-2019.0.5-0\r\n  libprotobuf        anaconda/win-64::libprotobuf-3.11.2-h7bd577a_0\r\n  markdown           anaconda/win-64::markdown-3.1.1-py37_0\r\n  opt_einsum         anaconda/noarch::opt_einsum-3.1.0-py_0\r\n  protobuf           anaconda/win-64::protobuf-3.11.2-py37h33f27b4_0\r\n  tensorboard        anaconda/noarch::tensorboard-2.0.0-pyhb38c66f_1\r\n  tensorflow         anaconda/win-64::tensorflow-2.0.0-mkl_py37he1bbcac_0\r\n  tensorflow-base    anaconda/win-64::tensorflow-base-2.0.0-mkl_py37hd1d5974_0\r\n  tensorflow-estima~ anaconda/noarch::tensorflow-estimator-2.0.0-pyh2649769_0\r\n  termcolor          anaconda/win-64::termcolor-1.1.0-py37_1\r\n\r\n\r\nProceed ([y]/n)? y\r\n\r\n\r\nDownloading and Extracting Packages\r\ngast-0.2.2           | 138 KB    | ############################################################################ | 100%\r\ntensorflow-estimator | 272 KB    | ############################################################################ | 100%\r\ntensorflow-base-2.0. | 41.9 MB   | ############################################################################ | 100%\r\ntensorflow-2.0.0     | 4 KB      | ############################################################################ | 100%\r\nastor-0.8.0          | 45 KB     | ############################################################################ | 100%\r\nabsl-py-0.8.1        | 162 KB    | ############################################################################ | 100%\r\n_tflow_select-2.3.0  | 3 KB      | ############################################################################ | 100%\r\nlibmklml-2019.0.5    | 21.4 MB   | ############################################################################ | 100%\r\ntensorboard-2.0.0    | 3.3 MB    | ############################################################################ | 100%\r\nlibprotobuf-3.11.2   | 2.3 MB    | ############################################################################ | 100%\r\nopt_einsum-3.1.0     | 54 KB     | ############################################################################ | 100%\r\nmarkdown-3.1.1       | 132 KB    | ############################################################################ | 100%\r\ngoogle-pasta-0.1.8   | 43 KB     | ############################################################################ | 100%\r\nkeras-applications-1 | 33 KB     | ############################################################################ | 100%\r\ntermcolor-1.1.0      | 7 KB      | ############################################################################ | 100%\r\nkeras-preprocessing- | 36 KB     | ############################################################################ | 100%\r\nprotobuf-3.11.2      | 597 KB    | ############################################################################ | 100%\r\nPreparing transaction: done\r\nVerifying transaction: failed\r\n\r\nCondaVerificationError: The package for tensorflow-base located at C:\\Users\\vishwasnarayan\\AppData\\Local\\Continuum\\anaconda3\\pkgs\\tensorflow-base-2.0.0-mkl_py37hd1d5974_0\r\nappears to be corrupted. The path 'Lib/site-packages/tensorflow-2.0.0.data/purelib/tensorflow_core/include/tensorflow_core/core/grappler/optimizers/generic_layout_optimizer_transposer_factory.h'\r\nspecified in the package manifest cannot be found.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n\r\n", "I am getting the same error. \r\nOS -  Windows 10- 64 bit\r\n\r\ncommand ran is\r\n\"conda install tensorflow\" (python 3.7 version )\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: C:\\Users\\xxxxxx\\AppData\\Local\\Continuum\\anaconda3\r\n\r\n  added / updated specs:\r\n    - tensorflow\r\n\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    _tflow_select-2.3.0        |              mkl           3 KB\r\n    absl-py-0.8.1              |           py37_0         165 KB\r\n    astor-0.8.0                |           py37_0          47 KB\r\n    conda-4.8.1                |           py37_0         2.8 MB\r\n    gast-0.2.2                 |           py37_0         155 KB\r\n    google-pasta-0.1.8         |             py_0          43 KB\r\n    grpcio-1.16.1              |   py37h351948d_1         850 KB\r\n    keras-applications-1.0.8   |             py_0          33 KB\r\n    keras-preprocessing-1.1.0  |             py_1          36 KB\r\n    libmklml-2019.0.5          |                0        17.4 MB\r\n    libprotobuf-3.11.2         |       h7bd577a_0         1.8 MB\r\n    markdown-3.1.1             |           py37_0         132 KB\r\n    opt_einsum-3.1.0           |             py_0          54 KB\r\n    protobuf-3.11.2            |   py37h33f27b4_0         544 KB\r\n    tensorboard-2.0.0          |     pyhb38c66f_1         3.3 MB\r\n    tensorflow-2.0.0           |mkl_py37he1bbcac_0           4 KB\r\n    tensorflow-base-2.0.0      |mkl_py37hd1d5974_0        30.9 MB\r\n    tensorflow-estimator-2.0.0 |     pyh2649769_0         250 KB\r\n    termcolor-1.1.0            |           py37_1           8 KB\r\n    ------------------------------------------------------------\r\n                                           Total:        58.6 MB\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  _tflow_select      pkgs/main/win-64::_tflow_select-2.3.0-mkl\r\n  absl-py            pkgs/main/win-64::absl-py-0.8.1-py37_0\r\n  astor              pkgs/main/win-64::astor-0.8.0-py37_0\r\n  gast               pkgs/main/win-64::gast-0.2.2-py37_0\r\n  google-pasta       pkgs/main/noarch::google-pasta-0.1.8-py_0\r\n  grpcio             pkgs/main/win-64::grpcio-1.16.1-py37h351948d_1\r\n  keras-applications pkgs/main/noarch::keras-applications-1.0.8-py_0\r\n  keras-preprocessi~ pkgs/main/noarch::keras-preprocessing-1.1.0-py_1\r\n  libmklml           pkgs/main/win-64::libmklml-2019.0.5-0\r\n  libprotobuf        pkgs/main/win-64::libprotobuf-3.11.2-h7bd577a_0\r\n  markdown           pkgs/main/win-64::markdown-3.1.1-py37_0\r\n  opt_einsum         pkgs/main/noarch::opt_einsum-3.1.0-py_0\r\n  protobuf           pkgs/main/win-64::protobuf-3.11.2-py37h33f27b4_0\r\n  tensorboard        pkgs/main/noarch::tensorboard-2.0.0-pyhb38c66f_1\r\n  tensorflow         pkgs/main/win-64::tensorflow-2.0.0-mkl_py37he1bbcac_0\r\n  tensorflow-base    pkgs/main/win-64::tensorflow-base-2.0.0-mkl_py37hd1d5974_0\r\n  tensorflow-estima~ pkgs/main/noarch::tensorflow-estimator-2.0.0-pyh2649769_0\r\n  termcolor          pkgs/main/win-64::termcolor-1.1.0-py37_1\r\n\r\nThe following packages will be UPDATED:\r\n\r\n  conda                                       4.7.12-py37_0 --> 4.8.1-py37_0\r\n\r\n\r\nProceed ([y]/n)? y\r\n\r\n\r\nDownloading and Extracting Packages\r\nkeras-applications-1 | 33 KB     | ############################################################################ | 100%\r\ntensorflow-estimator | 250 KB    | ############################################################################ | 100%\r\ntensorflow-2.0.0     | 4 KB      | ############################################################################ | 100%\r\nabsl-py-0.8.1        | 165 KB    | ############################################################################ | 100%\r\nconda-4.8.1          | 2.8 MB    | ############################################################################ | 100%\r\nastor-0.8.0          | 47 KB     | ############################################################################ | 100%\r\nprotobuf-3.11.2      | 544 KB    | ############################################################################ | 100%\r\nlibprotobuf-3.11.2   | 1.8 MB    | ############################################################################ | 100%\r\ngrpcio-1.16.1        | 850 KB    | ############################################################################ | 100%\r\ntensorboard-2.0.0    | 3.3 MB    | ############################################################################ | 100%\r\ntermcolor-1.1.0      | 8 KB      | ############################################################################ | 100%\r\nopt_einsum-3.1.0     | 54 KB     | ############################################################################ | 100%\r\nmarkdown-3.1.1       | 132 KB    | ############################################################################ | 100%\r\ngast-0.2.2           | 155 KB    | ############################################################################ | 100%\r\ntensorflow-base-2.0. | 30.9 MB   | ############################################################################ | 100%\r\n_tflow_select-2.3.0  | 3 KB      | ############################################################################ | 100%\r\ngoogle-pasta-0.1.8   | 43 KB     | ############################################################################ | 100%\r\nlibmklml-2019.0.5    | 17.4 MB   | ############################################################################ | 100%\r\nkeras-preprocessing- | 36 KB     | ############################################################################ | 100%\r\nPreparing transaction: done\r\nVerifying transaction: failed\r\n\r\nCondaVerificationError: The package for tensorflow-base located at C:\\Users\\srmetlakunta\\AppData\\Local\\Continuum\\anaconda3\\pkgs\\tensorflow-base-2.0.0-mkl_py37hd1d5974_0\r\nappears to be corrupted. The path 'Lib/site-packages/tensorflow-2.0.0.data/purelib/tensorflow_core/include/tensorflow_core/core/grappler/optimizers/generic_layout_optimizer_transposer_factory.h'\r\nspecified in the package manifest cannot be found.\r\n", "@vishwas1234567 \r\n\r\nCan you try installing with pip instead of conda and see how it progresses.Please, follow the instructions from [here](https://www.tensorflow.org/install/pip).Thanks!", "I am looking for the performance form the conda and the time it commands\nalso have stopped working after doing it with the pip.\n\nOn Wed, 29 Jan 2020 at 16:16, ravikyram <notifications@github.com> wrote:\n\n> @vishwas1234567 <https://github.com/vishwas1234567>\n>\n> Can you try installing with pip instead of conda and see how it\n> progresses.Please, follow the instructions from here\n> <https://www.tensorflow.org/install/pip>.Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35935?email_source=notifications&email_token=ALSXLOWIRFHFJPQXP3BIXLLRAFNBBA5CNFSM4KHSVDBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKGYBOQ#issuecomment-579698874>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALSXLOVKYSMSABEBKG6JAI3RAFNBBANCNFSM4KHSVDBA>\n> .\n>\n", "I traced this issue to Windows not allowing, by default, file paths longer than 256 characters. To solve, you should set the LongPathsEnabled key in your registry to 1, as explained [here](https://knowledge.autodesk.com/search-result/caas/sfdcarticles/sfdcarticles/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html).\r\nI'm currently checking out what happens when I rename that file to be shorter, in case you, like me, don't have admin access to your computer.", "What is a workaround here other than registry key change? \r\n@migliorinie you mentioned, you are checking with smaller filename. Does that change come in the next version release of tensorflow?\r\nCan I install some older tensorflow version as a workaround? ", "Hi @vishwas1234567 , Please i am encountering the same error while installing tensorflow on anaconda, how were you able to solve it? thanks.", "> I traced this issue to Windows not allowing, by default, file paths longer than 256 characters. To solve, you should set the LongPathsEnabled key in your registry to 1, as explained [here](https://knowledge.autodesk.com/search-result/caas/sfdcarticles/sfdcarticles/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html).\r\n> I'm currently checking out what happens when I rename that file to be shorter, in case you, like me, don't have admin access to your computer.\r\n\r\nThis solution worked for me, I recommend others to try this. Thank you!", "> I traced this issue to Windows not allowing, by default, file paths longer than 256 characters. To solve, you should set the LongPathsEnabled key in your registry to 1, as explained [here](https://knowledge.autodesk.com/search-result/caas/sfdcarticles/sfdcarticles/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html).\r\n> I'm currently checking out what happens when I rename that file to be shorter, in case you, like me, don't have admin access to your computer.\r\n\r\nWorked for me too. Thank you.", "This seems solved, closing.\r\n\r\nPlease don't reply with \"works for me\" as that hides the solution and then causes more duplicated issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35935\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35935\">No</a>\n", "Our enterprise machines that can't edit registry keys continuously run into this max path issue, in our win-64 conda `tensorflow=2.7` builds we have to build from `c:\\c` as even `c:\\conda` will exceed the max path length while building tf. \r\n\r\nAdditionally we have to remove the entire `_virtual_includes` directory or anyone who installs the package on a windows machine that can't edit registry keys will get a `conda verification error` where conda will say certain far-nested paths inside `_virtual_includes` can't be found (even though they exist), this is due to the max path issue.\r\n\r\nHopefully this helps someone else!"]}, {"number": 35934, "title": "Keras Model Errors on Loading - 'list' object has no attribute 'items'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n**No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- TensorFlow installed from (source or binary): **pip**\r\n- TensorFlow version (use command below): **2.1.0**\r\n- Python version: **3.5.2**\r\n- CUDA/cuDNN version: **CUDA 10.1 cuDNN 7.6**\r\n- GPU model and memory: **Quadro M2000M 4.00GiB**\r\n\r\n**Describe the current behavior**\r\n\r\nWhen trying to load one of my sequential models using `tf.keras.models.load_model` an error is thrown at the following location:\r\n```\r\ntensorflow_core\\python\\keras\\utils\\generic_utils.py\", line 254, in class_and_config_for_serialized_keras_object\r\nfor key, item in cls_config.items():\r\n```\r\n\r\nThis code expects `cls_config` to be a dictionary, while for this model it is a list of dictionaries.\r\nI can successfully load and run this model using TensorFlow versions `2.0.0`, `1.14.0` and `1.4.0` (the version is was trained with)\r\n\r\nThis section of code was introduced when adding support for [passive serialization in Keras](https://github.com/tensorflow/tensorflow/commit/a4fe7d8b6919281e26e2970e530cbff886a778ae#diff-ca2f2579ed6d04b0be9c2bacfa1a4d38)\r\n\r\n**Describe the expected behavior**\r\n\r\nCan successfully load a model from a hdf5 file when its config is in the list format\r\n\r\n**Code to reproduce the issue**\r\n\r\nExecute the `class_and_config_for_serialized_keras_object` function with a config of the following form:\r\n\r\n```JSON\r\n{\r\n    \"config\": [\r\n        {\r\n            \"config\": \r\n            {\r\n                ...\r\n            },\r\n            \"class_name\": \"Conv2D\"\r\n        },\r\n        ...\r\n    ],\r\n    \"class_name\": \"Sequential\"\r\n}\r\n```\r\n\r\nWhich is different from the form when I create a new model using TF v2.1.0 or 2.0.0 (I haven't verified the form of the config with the other versions):\r\n\r\n```JSON\r\n{\r\n    \"config\": {\r\n        \"name\": \"sequential\",\r\n        \"layers\": [\r\n            {\r\n                \"config\": \r\n                {\r\n                    ...\r\n                },\r\n                \"class_name\": \"Conv2D\"\r\n            },\r\n            ...\r\n        ]\r\n    },\r\n    \"class_name\": \"Sequential\"\r\n}\r\n```\r\n\r\n**Other info / logs**\r\n\r\nWhen loaded with tf.keras in v2.0.0 the layers, model config, inputs, outputs, summary etc. are all parsed correctly, as well as being able to run data through the model.", "comments": ["Please minimal reproducible code snippet. Thanks!", "**Minimal reproducible code snippets**\r\n\r\nUsing TensorFlow v1.4.0, Keras v2.1.1, CUDA v8.0.1 and cuDNN v6.0\r\n\r\n```Python\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Conv2D\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), input_shape=(96, 96, 3)))\r\nmodel.save('model.hdf5')\r\n```\r\n\r\n\r\nUsing TensorFlow v2.1.0, CUDA v10.1.105 and cuDNN v7.6\r\n\r\n```Python\r\nfrom tensorflow.keras.models import load_model\r\n\r\nmodel = load_model('model.hdf5', compile=False)\r\n```", "@ymodak \r\nAny updates on this? I've encountered the same bug as well.\r\nWould really appreciate a fix for this.", "I had the same issue and I downgrade tensorflow to 2.0.0 and it works fine.", "> I had the same issue and I downgrade tensorflow to 2.0.0 and it works fine.\r\n\r\nSame here but a rollback should be a last resort imo.", "I was not able to reproduce the behavior. Can you change your imports from `keras` to `tensorflow.keras` in all snippets and confirm if issue persists?\r\nI saved the model using TensorFlow 1.14 and loaded the same `h5` successfully using tf-nightly version and TF 2.1.", "I'm having similar problem here.\r\n\r\nUsing `from tensorflow.keras.models` throws an error, but `from keras.models ... ` doesn't\r\n\r\n```python\r\n# Doesnt work\r\nfrom tensorflow.keras.models import model_from_config\r\nmodel = model_from_config(model_dict)\r\n\r\n# Work\r\nfrom keras.models import model_from_config\r\nmodel = model_from_config(model_dict)\r\n```\r\n\r\nmodel_dict is as follow:\r\n\r\n```json\r\n{\"class_name\": \"Sequential\", \"config\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"batch_input_shape\": [null, 52], \"dtype\": \"float32\", \"units\": 128, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": {\"class_name\": \"L1L2\", \"config\": {\"l1\": 0.0, \"l2\": 0.009999999776482582}}, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"BatchNormalization\", \"config\": {\"name\": \"batch_normalization_1\", \"trainable\": true, \"axis\": -1, \"momentum\": 0.99, \"epsilon\": 0.001, \"center\": true, \"scale\": true, \"beta_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"gamma_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"moving_mean_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"moving_variance_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"beta_regularizer\": null, \"gamma_regularizer\": null, \"beta_constraint\": null, \"gamma_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"rate\": 0.2, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"units\": 128, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"BatchNormalization\", \"config\": {\"name\": \"batch_normalization_2\", \"trainable\": true, \"axis\": -1, \"momentum\": 0.99, \"epsilon\": 0.001, \"center\": true, \"scale\": true, \"beta_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"gamma_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"moving_mean_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"moving_variance_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"beta_regularizer\": null, \"gamma_regularizer\": null, \"beta_constraint\": null, \"gamma_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_2\", \"trainable\": true, \"rate\": 0.2, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_3\", \"trainable\": true, \"units\": 64, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"BatchNormalization\", \"config\": {\"name\": \"batch_normalization_3\", \"trainable\": true, \"axis\": -1, \"momentum\": 0.99, \"epsilon\": 0.001, \"center\": true, \"scale\": true, \"beta_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"gamma_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"moving_mean_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"moving_variance_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"beta_regularizer\": null, \"gamma_regularizer\": null, \"beta_constraint\": null, \"gamma_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_3\", \"trainable\": true, \"rate\": 0.2, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_4\", \"trainable\": true, \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"BatchNormalization\", \"config\": {\"name\": \"batch_normalization_4\", \"trainable\": true, \"axis\": -1, \"momentum\": 0.99, \"epsilon\": 0.001, \"center\": true, \"scale\": true, \"beta_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"gamma_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"moving_mean_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"moving_variance_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"beta_regularizer\": null, \"gamma_regularizer\": null, \"beta_constraint\": null, \"gamma_constraint\": null}}, {\"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_4\", \"trainable\": true, \"rate\": 0.2, \"noise_shape\": null, \"seed\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_5\", \"trainable\": true, \"units\": 1, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"keras_version\": \"2.2.2\", \"backend\": \"tensorflow\"}\r\n```\r\n\r\nI'm using tensorflow 2.1 and keras 2.3.1", "Closing this issue since it cannot be reproduced. Feel free to reopen a new issue with a repro example. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35934\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35934\">No</a>\n", "@ymodak Will `tf.keras` only be kept backwards compatible with `tf.keras` and not the original `keras` package?\r\nAs given Keras officially says `tf.keras` is its successor, this could cause difficulties for people", "@ymodak What is recommended path forward on this. I have similar issue when using >2.0 tensorflow.\r\n```\r\nfrom tensorflow.keras.models import load_model\r\nmodel = load_model()\r\nAttributeError: 'list' object has no attribute 'items\r\n```\r\nWhen I downgrade to 2.0 this works fine.\r\n\r\n\r\n\r\n", "@ymodak : Same issue I am facing. Please suggest a resolution.", "@robfreund @tripathysa Please open a new issue with a minimal reproducible example and I will be happy to take a look.\r\nAt this moment I cannot reproduce the reported behavior. Thanks!", "I'm able to reproduce this issue when converting an h5 model from this package:\r\nhttps://zenodo.org/record/3466980\r\n(This is the model data for this system: https://github.com/mahehu/TUT-live-age-estimator)\r\n\r\nInput model for tflite_convert: recognizers\\alignment\\model-056-0.716316-0.967865.h5\r\n(File from the package mentioned above)\r\n\r\nCommand line: \r\ntflite_convert --output_file=model.tflite --keras_model_file=model-056-0.716316-0.967865.h5\r\n\r\nOutput:\r\n  File \"c:\\users\\matthias\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 325, in class_and_config_for_serialized_keras_object\r\n    for key, item in cls_config.items():\r\nAttributeError: 'list' object has no attribute 'items'\r\n\r\nPython: 3.7.7\r\nTF version: 2.1.0 and 2.2.0-rc2, installed via pip\r\nOS: Windows 8.1\r\nNo CUDA or configured GPU", "@ymodak I have the same issue, it's easy to reproduce because it's happened when I have old models created and saved with Keras (with TF 1.14 as backend) I try to import in TF 2.1 with tf.keras. \r\n\r\nThe tf.keras import works with TF 2.0.\r\n\r\nTo reproduce, train and save anything with Keras (not tf.keras) with TF 1.14 as backend, and try to import with tf.keras under TF 2.1 ", "Any resolution on this ?\r\n", "It looks like this will be fixed in TensorFlow 2.3: see #38135.", "this problem can be due to, while saving the file you used a different version of tensorflow(or maybe you used someone else's .h5 file that was using some different version of tensorflow).", "Hello! I have the same issue here... \r\ninvoice_candidate_model = tf.keras.models.load_model('../model/Invoice_Extraction_Model_AU')\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-15-1d8766d6d9d0> in <module>\r\n----> 1 invoice_candidate_model = tf.keras.models.load_model('../model/Invoice_Extraction_Model_AU')\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py in load_model(filepath, custom_objects, compile, options)\r\n    210       if isinstance(filepath, six.string_types):\r\n    211         loader_impl.parse_saved_model(filepath)\r\n--> 212         return saved_model_load.load(filepath, compile, options)\r\n    213 \r\n    214   raise IOError(\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py in load(path, compile, options)\r\n    136   # Recreate layers and metrics using the info stored in the metadata.\r\n    137   keras_loader = KerasObjectLoader(metadata, object_graph_def)\r\n--> 138   keras_loader.load_layers()\r\n    139 \r\n    140   # Generate a dictionary of all loaded nodes.\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py in load_layers(self)\r\n    372         continue\r\n    373 \r\n--> 374       self.loaded_nodes[node_metadata.node_id] = self._load_layer(\r\n    375           node_metadata.node_id, node_metadata.identifier,\r\n    376           node_metadata.metadata)\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py in _load_layer(self, node_id, identifier, metadata)\r\n    403     # Detect whether this object can be revived from the config. If not, then\r\n    404     # revive from the SavedModel instead.\r\n--> 405     obj, setter = self._revive_from_config(identifier, metadata, node_id)\r\n    406     if obj is None:\r\n    407       obj, setter = revive_custom_object(identifier, metadata)\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py in _revive_from_config(self, identifier, metadata, node_id)\r\n    421       obj = (\r\n    422           self._revive_graph_network(metadata, node_id) or\r\n--> 423           self._revive_layer_from_config(metadata, node_id))\r\n    424 \r\n    425     if obj is None:\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py in _revive_layer_from_config(self, metadata, node_id)\r\n    480 \r\n    481     try:\r\n--> 482       obj = layers_module.deserialize(\r\n    483           generic_utils.serialize_keras_class_and_config(class_name, config))\r\n    484     except ValueError:\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py in deserialize(config, custom_objects)\r\n    171   \"\"\"\r\n    172   populate_deserializable_objects()\r\n--> 173   return generic_utils.deserialize_keras_object(\r\n    174       config,\r\n    175       module_objects=LOCAL.ALL_OBJECTS,\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    352 \r\n    353       if 'custom_objects' in arg_spec.args:\r\n--> 354         return cls.from_config(\r\n    355             cls_config,\r\n    356             custom_objects=dict(\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py in from_config(cls, config, custom_objects)\r\n   1017   def from_config(cls, config, custom_objects=None):\r\n   1018     config = config.copy()\r\n-> 1019     function = cls._parse_function_from_config(\r\n   1020         config, custom_objects, 'function', 'module', 'function_type')\r\n   1021 \r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py in _parse_function_from_config(cls, config, custom_objects, func_attr_name, module_attr_name, func_type_attr_name)\r\n   1063     if function_type == 'function':\r\n   1064       # Simple lookup in custom objects\r\n-> 1065       function = generic_utils.deserialize_keras_object(\r\n   1066           config[func_attr_name],\r\n   1067           custom_objects=custom_objects,\r\n\r\nC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    373       obj = _GLOBAL_CUSTOM_OBJECTS[object_name]\r\n    374     else:\r\n--> 375       obj = module_objects.get(object_name)\r\n    376       if obj is None:\r\n    377         raise ValueError(\r\n\r\nAttributeError: 'NoneType' object has no attribute 'get'\r\n\r\nMy tensorflow version is 2.4.0 .... So annoying!"]}, {"number": 35933, "title": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'ravel'", "body": "I am using tensorflow 2.1\r\n\r\n\r\n    `def`` generate_sequence(epoch_num, initial_index, seq_length):\r\n    with open(os.path.join(data_directory, charIndex_json)) as f:\r\n        char_to_index = json.load(f)\r\n    index_to_char = {i:ch for ch, i in char_to_index.items()}\r\n    unique_chars = len(index_to_char)\r\n    \r\n    model = make_model(unique_chars)\r\n    model.load_weights(model_weights_directory + \"Weights_{}.h5\".format(epoch_num))\r\n     \r\n    sequence_index = [initial_index]\r\n    \r\n    for _ in range(seq_length):\r\n        batch = np.zeros((1, 1))\r\n        batch[0, 0] = sequence_index[-1]\r\n        \r\n        predicted_probs = model.predict_on_batch(batch).ravel()\r\n        sample = np.random.choice(range(unique_chars), size = 1, p = predicted_probs)\r\n        \r\n        sequence_index.append(sample[0])\r\n    \r\n    seq = ''.join(index_to_char[c] for c in sequence_index)\r\n    \r\n    cnt = 0\r\n    for i in seq:\r\n        cnt += 1\r\n        if i == \"\\n\":\r\n            break\r\n    seq1 = seq[cnt:]\r\n    #above code is for ignoring the starting string of a generated sequence. This is because we are passing any arbitrary \r\n    #character to the model for generating music. Now, the model start generating sequence from that character itself which we \r\n    #have passed, so first few characters before \"\\n\" contains meaningless word. Model start generating the music rhythm from\r\n    #next line onwards. The correct sequence it start generating from next line onwards which we are considering.\r\n    \r\n    cnt = 0\r\n    for i in seq1:\r\n        cnt += 1\r\n        if i == \"\\n\" and seq1[cnt] == \"\\n\":\r\n            break\r\n    seq2 = seq1[:cnt]\r\n    #Now our data contains three newline characters after every tune. So, the model has leart that too. So, above code is used for\r\n    #ignoring all the characters that model has generated after three new line characters. So, here we are considering only one\r\n    #tune of music at a time and finally we are returning it..\r\n    \r\n    return seq2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nerror :\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-48-1c976bcea2e8> in <module>\r\n      3 ln = int(input(\"\\n3. Enter the length of music sequence you want to generate. Typical number is between 300-600. Too small number will generate hardly generate any sequence: \"))\r\n      4 \r\n----> 5 music = generate_sequence(ep, ar, ln)\r\n      6 \r\n      7 print(\"\\nMUSIC SEQUENCE GENERATED: \\n\")\r\n\r\n<ipython-input-47-a1565ba804d1> in generate_sequence(epoch_num, initial_index, seq_length)\r\n     14         batch[0, 0] = sequence_index[-1]\r\n     15 \r\n---> 16         predicted_probs = model.predict_on_batch(batch).ravel()\r\n     17         sample = np.random.choice(range(unique_chars), size = 1, p = predicted_probs)\r\n     18 \r\n\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'ravel'", "comments": ["@indexraja \r\n\r\nI have tried on colab with TF version 2.1.0-rc2 . I am seeing the below error.\r\n`NameError: name 'make_model' is not defined`\r\nRequest you to provide colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@indexraja \r\n\r\nAny update on this issue please. Thanks!", "try this: \r\nnp.squeeze(model.predict_on_batch(batch))\r\n\r\nbtw. model.predict_on_batch should return numpy -> ravel should work . \r\nbut if input mismatch or model  arch is wrong he will raise error", "@indexraja\r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35932, "title": "Could not find matching function to call loaded from the SavedModel", "body": "My code works in `GPU` based tensorflow environment without any fuss but fails in `CPU` based environments. Some other people also are facing the same issue. `Training` works without any issues but it's the `predict` method that's failing.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary): Source\r\n- Other system related information below:\r\n\r\n```\r\nCollecting system information...\r\n/tmp/check_os.py:18: DeprecationWarning: dist() and linux_distribution() functions are deprecated in Python 3.5\r\n  platform.linux_distribution(),\r\n/tmp/check_os.py:19: DeprecationWarning: dist() and linux_distribution() functions are deprecated in Python 3.5\r\n  platform.dist(),\r\ncat: /proc/1/cgroup: No such file or directory\r\n2020-01-16 15:58:53.661018: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-01-16 15:58:53.661360: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\r\n')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 10.0.1 (clang-1001.0.46.4)\r\nTarget: x86_64-apple-darwin18.7.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy                         1.17.4             \r\nprotobuf                      3.11.2             \r\ntensorflow                    2.1.0              \r\ntensorflow-estimator          2.1.0              \r\ntensorflow-hub                0.7.0              \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.0.0\r\ntf.version.GIT_VERSION = unknown\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_cololect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.1.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /Users/sardarmrinal/anaconda3/lib/python3.7/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 6, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\nBuild label: 1.2.1\r\nBuild time: Tue Nov 26 15:27:31 2019 (1574782051)\r\nBuild timestamp: 1574782051\r\nBuild timestamp as int: 1574782051\r\n```\r\n\r\n**Describe the current behavior**\r\nThe code is running successfully in `GPU` based environment and failing in `CPU` based environments.\r\n\r\n**Describe the expected behavior**\r\nIt should run/fail in the same way in both `GPU` and `CPU` based environments.\r\n\r\n**Code to reproduce the issue**\r\nCode in the following link has the same behavior:\r\nhttps://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\r\n\r\nAnd also people are talking about this issue in the comments.\r\n\r\n\r\n**Other info / logs**\r\n```\r\nMaking predictions\r\nTraceback (most recent call last):\r\n  File \"/Users/sardarmrinal/Egnyte/Private/sardar.mrinal/workspace/competitions/kaggle_nlp_disaster/working/NLP_disaster_bert.py\", line 218, in <module>\r\n    df_sub = model_bert.predict(df_eval)\r\n  File \"/Users/sardarmrinal/Egnyte/Private/sardar.mrinal/workspace/competitions/kaggle_nlp_disaster/working/NLP_disaster_bert.py\", line 186, in predict\r\n    prediction = self.model.predict(x=X)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 909, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 462, in predict\r\n    steps=steps, callbacks=callbacks, **kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 444, in _model_iteration\r\n    total_epochs=1)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 73, in distributed_function\r\n    per_replica_function, args=(model, x, y, sample_weights))\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 760, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1787, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2132, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 162, in _predict_on_batch\r\n    return predict_on_batch(model, x)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 370, in predict_on_batch\r\n    return model(inputs)  # pylint: disable=not-callable\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 847, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 708, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 860, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 847, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\", line 218, in call\r\n    lambda: f(training=False))\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py\", line 56, in smart_cond\r\n    return false_fn()\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\", line 218, in <lambda>\r\n    lambda: f(training=False))\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 436, in _call_attribute\r\n    return instance.__call__(*args, **kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 494, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1822, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 262, in restored_function_body\r\n    \"\\n\\n\".join(signature_descriptions)))\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (3 total):\r\n    * [<tf.Tensor 'inputs:0' shape=(None, 3) dtype=int64>, <tf.Tensor 'inputs_1:0' shape=(None, 3) dtype=int64>, <tf.Tensor 'inputs_2:0' shape=(None, 3) dtype=int64>]\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n```\r\n", "comments": ["@mrinalsardar ,\r\nCan you please provide the code used by you to replicate the above reported error?Thanks!", "Here it goes:\r\n\r\n\r\n```\r\nimport re\r\nimport string\r\nimport logging\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom datetime import datetime\r\n\r\nfrom spellchecker import SpellChecker\r\nimport tokenization\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import accuracy_score\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\r\n\r\n\r\n# Logger setup\r\nlogging.basicConfig(\r\n    format=\"%(asctime)s {} %(levelname)s: %(message)s\".format(__name__),\r\n    level=logging.DEBUG,\r\n)\r\nlogger = logging.getLogger(name=__name__)\r\nlogging.getLogger(\"py4j\").setLevel(logging.ERROR)\r\n\r\n\r\nclass ModelBERT(object):\r\n    def __init__(\r\n        self, \r\n        pre_train_handle, \r\n        column_text=\"text\", \r\n        column_target=\"target\", \r\n        data_prep=False,\r\n        vector_len=200,\r\n        tokenizer=None\r\n    ):\r\n        logger.info(\"Initialize ModelBERT along with regex pattarens\")\r\n        self.pre_train_handle = pre_train_handle\r\n        self.column_text = column_text\r\n        self.column_target = column_target\r\n        self.data_prep = data_prep\r\n        self.vector_len = vector_len\r\n        self.tokenizer = tokenizer\r\n        self.model_weight_path = \"model.h5\"\r\n        self.func_list = [self.translate_smiley, self.remove_things]\r\n\r\n        self.pattern_hashtag = re.compile(pattern=r\"#\\w+\", flags=re.IGNORECASE)\r\n        self.pattern_uppercase_words = re.compile(pattern=r\"[A-Z]+\")\r\n        self.pattern_lowercase_words = re.compile(pattern=r\"[a-z]+\")\r\n        self.pattern_url = re.compile(\r\n            pattern=r\"\\b[.*]?http[s]?[a-zA-Z0-9_:/\\.]+\\b\", \r\n            flags=re.IGNORECASE\r\n        )\r\n        self.pattern_handle = re.compile(pattern=r\"@\\w+\", flags=re.IGNORECASE)\r\n        self.pattern_smiley_happy = re.compile(pattern=r\"[:|;]\\)+\")\r\n        self.pattern_smiley_sad = re.compile(pattern=r\"[:|;]\\(+\")\r\n        self.pattern_punctuation = re.compile(r\"[{}]\".format(string.punctuation))\r\n\r\n\r\n    def translate_smiley(self, df):\r\n        logger.info(\"Translating smileys\")\r\n        df[self.column_text] = df[self.column_text] \\\r\n            .apply(lambda t: self.pattern_smiley_happy.sub(repl=\"happy\", string=t)) \\\r\n            .apply(lambda t: self.pattern_smiley_sad.sub(repl=\"sad\", string=t))\r\n\r\n        return df\r\n\r\n\r\n    def remove_things(self, df):\r\n        \"\"\"Remove URL | handles | punctuations\"\"\"\r\n        logger.info(\"Remove URL, handles and punctuations\")\r\n        df[self.column_text] = df[self.column_text] \\\r\n            .apply(lambda t: self.pattern_url.sub(repl=\"\", string=t)) \\\r\n            .apply(lambda t: self.pattern_handle.sub(repl=\"\", string=t)) \\\r\n            .apply(lambda t: self.pattern_punctuation.sub(repl=\"\", string=t))\r\n        \r\n        return df\r\n\r\n\r\n    def correct_spelling(self, df):\r\n        logger.info(\"Correcting spellings\")\r\n        corrector = SpellChecker().correction\r\n        def correct_me(text):\r\n            return \" \".join(list(map(corrector, text.split())))\r\n        df[self.column_text] = df[self.column_text].apply(correct_me)\r\n        \r\n        return df\r\n\r\n\r\n    def setup_pre_train_layer(self):\r\n        logger.info(\"Setting up pretrain BERT layer\")\r\n        self.pre_train_layer = hub.KerasLayer(handle=self.pre_train_handle, trainable=True)\r\n        logger.debug(f\"Pre trained layer: {self.pre_train_layer}\")\r\n\r\n\r\n    def setup_tokenizer(self):\r\n        logger.info(\"Setting up tokenizer\")\r\n        self.setup_pre_train_layer()\r\n        \r\n        vocab_file = self.pre_train_layer.resolved_object.vocab_file.asset_path.numpy()\r\n        do_lower_case = self.pre_train_layer.resolved_object.do_lower_case.numpy()\r\n\r\n        logger.debug(f\"vocab_file: {vocab_file}\")\r\n        logger.debug(f\"do_lower_case: {do_lower_case}\")\r\n\r\n        self.tokenizer = tokenization.FullTokenizer(\r\n            vocab_file=vocab_file, \r\n            do_lower_case=do_lower_case\r\n        )\r\n\r\n\r\n    def encode_text(self, texts):\r\n        if not self.tokenizer:\r\n            self.setup_tokenizer()\r\n\r\n        logger.debug(f\"Encoding text with vector length: {self.vector_len}\")\r\n        all_tokens = []\r\n        all_masks = []\r\n        all_segments = []\r\n\r\n        for text in texts:\r\n            text = self.tokenizer.tokenize(text)\r\n\r\n            text = text[:self.vector_len - 2]\r\n            input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\r\n            pad_len = self.vector_len - len(input_sequence)\r\n\r\n            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\r\n            tokens += [0] * pad_len\r\n            pad_masks = [1] * len(input_sequence) + [0] * pad_len\r\n            segmen_ids = [0] * self.vector_len\r\n\r\n            all_tokens.append(tokens)\r\n            all_masks.append(pad_masks)\r\n            all_segments.append(segmen_ids)\r\n\r\n        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\r\n\r\n\r\n    def create_model(self):\r\n        logger.info(\"Creating model\")\r\n        input_word_ids = Input(shape=(self.vector_len,), name=\"input_word_ids\", dtype=tf.int32)\r\n        input_masks = Input(shape=(self.vector_len,), name=\"input_masks\", dtype=tf.int32)\r\n        segment_ids = Input(shape=(self.vector_len,), name=\"segment_ids\", dtype=tf.int32)\r\n\r\n        _, sequence_output = self.pre_train_layer([input_word_ids, input_masks, segment_ids])\r\n        clf_output = sequence_output[:, 0, :]\r\n        outputs = Dense(1, activation=\"sigmoid\")(clf_output)\r\n\r\n        self.model = Model(inputs=[input_word_ids, input_masks, segment_ids], outputs=outputs)\r\n        self.model.compile(\r\n            optimizer=Adam(learning_rate=3e-6), \r\n            loss=\"binary_crossentropy\", \r\n            metrics=[\"accuracy\"]\r\n        )\r\n        logger.info(self.model.summary())\r\n\r\n\r\n    def prepare_data(self, df):\r\n        if self.func_list:\r\n            for func in self.func_list:\r\n                logger.debug(f\"Applying function: {str(func)}\")\r\n                df = func(df)\r\n\r\n        return df\r\n\r\n\r\n    def train_model(self, X, y, batch_size=10, epochs=1, verbose=1, validation_split=0.2):\r\n        if self.data_prep:\r\n            df = self.prepare_data(df)\r\n\r\n        X = self.encode_text(X[self.column_text].values)\r\n        y = np.asarray(y)\r\n\r\n        logger.debug(f\"Model checkpoint: {self.model_weight_path}\")\r\n        self.checkpoint = ModelCheckpoint(\r\n            filepath=self.model_weight_path, \r\n            monitor=\"val_loss\", \r\n            save_best_only=True\r\n        )\r\n\r\n        # log_dir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n        # tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\r\n\r\n        self.create_model()\r\n        logger.info(\"Training model\")\r\n        self.model.fit(\r\n            x=X, \r\n            y=y, \r\n            batch_size=batch_size, \r\n            epochs=epochs, \r\n            verbose=verbose, \r\n            callbacks=[self.checkpoint],\r\n            validation_split=validation_split\r\n        )\r\n\r\n\r\n    def predict(self, X):\r\n        if self.data_prep:\r\n            X = self.prepare_data(X)\r\n\r\n        X = self.encode_text(X[self.column_text].values)\r\n        self.model.load_weights(self.model_weight_path)\r\n\r\n        logger.info(\"Making predictions\")\r\n        prediction = self.model.predict(x=X)\r\n\r\n        return prediction\r\n\r\n\r\n\r\npath_train = \"input/nlp-getting-started/train.csv\"\r\npath_test = \"input/nlp-getting-started/test.csv\"\r\n\r\ndf_train = pd.read_csv(path_train)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(\r\n    df_train.drop(\"target\", axis=1), \r\n    df_train[\"target\"], \r\n    test_size=0.1, \r\n    random_state=999\r\n)\r\n\r\npre_train_handle = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\r\n# pre_train_handle = \"/Users/sardarmrinal/Downloads/bert_en_uncased_L-12_H-768_A-12\"\r\n\r\nmodel_bert = ModelBERT(pre_train_handle=pre_train_handle)\r\nmodel_bert.vector_len = 2\r\n# model_bert.data_prep = True\r\nmodel_bert.func_list = [model_bert.translate_smiley, model_bert.remove_things]\r\n\r\n# model_bert.train_model(df_train)\r\nmodel_bert.train_model(\r\n    X=X_train, \r\n    y=y_train, \r\n    batch_size=500, \r\n    epochs=1, \r\n    verbose=1, \r\n    validation_split=0.1\r\n)\r\n\r\ny_predict = model_bert.predict(X=X_test)\r\naccuracy = accuracy_score(np.asarray(y_test), y_predict.round().astype(int).reshape(len(X_test)))\r\nprint(f\"Test accuracy score: {accuracy}\")\r\n\r\n\r\n\r\n# Preparing submission\r\ndf_eval = pd.read_csv(path_test)\r\n\r\nprediction = model_bert.predict(df_eval)\r\n\r\ndf_sub = pd.DataFrame(\r\n    data={\r\n        \"id\": df_eval[\"id\"].values,\r\n        \"target\": prediction.round().astype(int).reshape(df_eval.shape[0])\r\n    }\r\n)\r\n\r\ndf_sub.to_csv(\"./submission_bert.csv\", header=True, index=False)\r\n```\r\n\r\nNB: The tokenizer came from here - \r\nhttps://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\r\n", "@mrinalsardar Please provide us a minimal sample to reproduce this issue. Thanks!", "@gowthamkpr here you go:\r\n[tf_sample_input.zip](https://github.com/tensorflow/tensorflow/files/4101475/tf_sample_input.zip)\r\n", "I am running into multiple errors while running on colab. Please find the gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/5877ce3a0d443f94ed482ec7ea82c41a/untitled18.ipynb). Please correct them and share it. Thanks!", "The  `pyspellchecker` library is not installed in Colab/Kaggle `docker` image. You just have to do a `python -m pip install pyspellchecker` and that's it.\r\n\r\nAnd please use the  web `URL` for `pre_train_handle` instead of the local machine `URI`. I have edited the sample code accordingly.", "@mrinalsardar Please go ahead and make changes in the colab and share the gist. Thank you!", "@mrinalsardar Please correct the gist and share it so that we can take a look at your issue. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Hi @mrinalsardar \r\nI was facing a similar issue. What worked for me was changing the way I was saving and loading the model as suggested by another user. Instead of model.save(), I saved the weights and then loaded the model as per guidelines in Part II-Approach 1 here: https://www.tensorflow.org/guide/keras/save_and_serialize.\r\nSeems to be working now.", "Yes I tested it as mentioned in the above comment and it is working now. Thanks for sharing @hepbc. @mrinalsardal Please try as mentioned in the above comment and let me know if its working for you. ", "@gowthamkpr @hepbc \r\nIt did not resolve the issue for me. Can it be specific to `Mac`?", "Did you follow the approach 1 as mentioned [here](https://www.tensorflow.org/guide/keras/save_and_serialize#approach_1) @mrinalsardar ", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments for us to open this issue again. Thanks!", "Same issue, TF 2.1 eager mode, using SavedModel format to restore and failed in `.forward`. You can save the model with h5 format (it works fine) or using TF 2.2\r\n\r\n![image](https://user-images.githubusercontent.com/9346460/81286714-14fe6f80-9094-11ea-9953-55b8078f9742.png)\r\n\r\n\r\nI think the main reason is the TF.Keras != Keras and many TF ops are not compatible with TF.Keras.\r\n", "I seem to get the same error when incorrectly formatted data is fed into a loaded model. Using TF 2.5.0. I get this both on CPU and GPU.\r\n\r\n<img width=\"1281\" alt=\"Screenshot 2021-06-28 at 11 39 40\" src=\"https://user-images.githubusercontent.com/32125321/123615492-a59e8e80-d805-11eb-8935-3224744a8836.png\">\r\n\r\n`test_row` look like this:\r\n\r\n<img width=\"1274\" alt=\"Screenshot 2021-06-28 at 11 40 26\" src=\"https://user-images.githubusercontent.com/32125321/123615676-d2eb3c80-d805-11eb-8542-b37a34e0fff7.png\">\r\n\r\nIn this case it can easily be solved by converting `test_row` into a list before feeding it into the model. \r\n\r\n`result = loaded_model(list(test_row))`\r\n\r\n", "I am facing the same error and I am using my laptop not the colab!\r\n"]}, {"number": 35931, "title": "No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ,macos\r\n- TensorFlow installed from (source or binary): source\r\n\r\nI have successfully compiled AAR through bazel and tensorflow source code:\r\n**1.14-tensorflow-lite-with-select-tf-ops.aar** and **1.15-tensorflow-lite-with-select-tf-ops.aar**\r\n\r\n**What i want to do:**\r\nSince my tflite model lacks the **RandomUniform** operator, I compiled and generated the AAR.Then I followed the official tips from tensorflow to introduce AAR dependencies in Android Studio.\r\n![image](https://user-images.githubusercontent.com/43409147/72513097-ae0c3f00-3887-11ea-80cd-d0cf4b4d6818.png)\r\nNo error was reported during synchronization, but an error occurred when running the .kt file.\r\n**As follows, I get an error when I generate the interpreter\uff1a**\r\n![image](https://user-images.githubusercontent.com/43409147/72515495-f5e09580-388a-11ea-8854-aac284518626.png)\r\n\r\n\r\n**My error message is as follows**\r\n\r\n> 01/16 17:41:17: Launching 'MainActivity' on Pixel 2 API 29.\r\n$ adb shell am start -n \"com.example.test_aar2/com.example.test_aar2.MainActivity\" -a android.intent.action.MAIN -c android.intent.category.LAUNCHER\r\nWaiting for process to come online...\r\nConnected to process 4911 on device 'emulator-5554'.\r\nCapturing and displaying logcat messages from application. This behavior can be disabled in the \"Logcat output\" section of the \"Debugger\" settings page.\r\nW/RenderThread: type=1400 audit(0.0:123): avc: denied { write } for name=\"property_service\" dev=\"tmpfs\" ino=6939 scontext=u:r:untrusted_app:s0:c118,c256,c512,c768 tcontext=u:object_r:property_socket:s0 tclass=sock_file permissive=0 app=com.example.test_aar2\r\nD/libEGL: Emulator has host GPU support, qemu.gles is set to 1.\r\nW/libc: Unable to set property \"qemu.gles\" to \"1\": connection failed; errno=13 (Permission denied)\r\nD/libEGL: loaded /vendor/lib64/egl/libEGL_emulation.so\r\nD/libEGL: loaded /vendor/lib64/egl/libGLESv1_CM_emulation.so\r\nD/libEGL: loaded /vendor/lib64/egl/libGLESv2_emulation.so\r\nW/ample.test_aar: Accessing hidden method Landroid/view/View;->computeFitSystemWindows(Landroid/graphics/Rect;Landroid/graphics/Rect;)Z (greylist, reflection, allowed)\r\nW/ample.test_aar: Accessing hidden method Landroid/view/ViewGroup;->makeOptionalFitsSystemWindows()V (greylist, reflection, allowed)\r\nD/: HostConnection::get() New Host Connection established 0x705aa57140c0, tid 4942\r\nD/: HostComposition ext ANDROID_EMU_CHECKSUM_HELPER_v1 ANDROID_EMU_native_sync_v2 ANDROID_EMU_native_sync_v3 ANDROID_EMU_native_sync_v4 ANDROID_EMU_dma_v1 ANDROID_EMU_direct_mem ANDROID_EMU_host_composition_v1 ANDROID_EMU_host_composition_v2 ANDROID_EMU_vulkan ANDROID_EMU_deferred_vulkan_commands ANDROID_EMU_vulkan_create_resources_with_requirements ANDROID_EMU_async_unmap_buffer GL_OES_EGL_image_external_essl3 GL_OES_vertex_array_object GL_KHR_texture_compression_astc_ldr ANDROID_EMU_gles_max_version_3_1 \r\nW/OpenGLRenderer: Failed to choose config with EGL_SWAP_BEHAVIOR_PRESERVED, retrying without...\r\nD/eglCodecCommon: setVertexArrayObject: set vao to 0 (0) 0 0\r\nD/EGL_emulation: eglCreateContext: 0x705aa5714340: maj 3 min 1 rcv 4\r\nD/EGL_emulation: eglMakeCurrent: 0x705aa5714340: ver 3 1 (tinfo 0x705aa5814400)\r\nE/eglCodecCommon: glUtilsParamSize: unknow param 0x000082da\r\n    glUtilsParamSize: unknow param 0x000082da\r\nW/Gralloc3: mapper 3.x is not supported\r\nD/: createUnique: call\r\n    HostConnection::get() New Host Connection established 0x705aa57145c0, tid 4942\r\nD/: HostComposition ext ANDROID_EMU_CHECKSUM_HELPER_v1 ANDROID_EMU_native_sync_v2 ANDROID_EMU_native_sync_v3 ANDROID_EMU_native_sync_v4 ANDROID_EMU_dma_v1 ANDROID_EMU_direct_mem ANDROID_EMU_host_composition_v1 ANDROID_EMU_host_composition_v2 ANDROID_EMU_vulkan ANDROID_EMU_deferred_vulkan_commands ANDROID_EMU_vulkan_create_resources_with_requirements ANDROID_EMU_async_unmap_buffer GL_OES_EGL_image_external_essl3 GL_OES_vertex_array_object GL_KHR_texture_compression_astc_ldr ANDROID_EMU_gles_max_version_3_1 \r\nD/eglCodecCommon: allocate: Ask for block of size 0x1000\r\n    allocate: ioctl allocate returned offset 0x3ff708000 size 0x2000\r\nD/EGL_emulation: eglMakeCurrent: 0x705aa5714340: ver 3 1 (tinfo 0x705aa5814400)\r\nD/eglCodecCommon: setVertexArrayObject: set vao to 0 (0) 1 0\r\nW/System.err: TensorFlowLite: failed to load native library: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/com.example.test_aar2-GJboUyCuRGe7EMGh-GZalA==/base.apk\"],nativeLibraryDirectories=[/data/app/com.example.test_aar2-GJboUyCuRGe7EMGh-GZalA==/lib/x86_64, /system/lib64, /system/product/lib64]]] couldn't find \"libtensorflowlite_jni.so\"\r\nW/System.err: TensorFlowLite: failed to load native library: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/com.example.test_aar2-GJboUyCuRGe7EMGh-GZalA==/base.apk\"],nativeLibraryDirectories=[/data/app/com.example.test_aar2-GJboUyCuRGe7EMGh-GZalA==/lib/x86_64, /system/lib64, /system/product/lib64]]] couldn't find \"libtensorflowlite_jni.so\"\r\nE/ample.test_aar: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)\r\nD/AndroidRuntime: Shutting down VM\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.test_aar2, PID: 4911\r\n    java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at com.example.test_aar2.MainActivity.action(MainActivity.kt:47)\r\n        at com.example.test_aar2.MainActivity.access$action(MainActivity.kt:23)\r\n        at com.example.test_aar2.MainActivity$onCreate$1.onClick(MainActivity.kt:30)\r\n        at android.view.View.performClick(View.java:7140)\r\n        at android.view.View.performClickInternal(View.java:7117)\r\n        at android.view.View.access$3500(View.java:801)\r\n        at android.view.View$PerformClick.run(View.java:27351)\r\n        at android.os.Handler.handleCallback(Handler.java:883)\r\n        at android.os.Handler.dispatchMessage(Handler.java:100)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7356)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)\r\nProcess 4911 terminated.\r\n\r\n**Then I introduced two dependencies at the same time according to the official tips of tensorflow\uff1a**\r\n![image](https://user-images.githubusercontent.com/43409147/72513782-7b167b00-3888-11ea-97e2-876c3e1734e8.png)\r\n**As follows\uff1a**\r\n![image](https://user-images.githubusercontent.com/43409147/72513984-d8aac780-3888-11ea-88fa-0164d51508a5.png)\r\n\r\n**But I encountered the error again\uff1a**\r\n\r\n> Duplicate class org.tensorflow.lite.DataType found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)\r\nDuplicate class org.tensorflow.lite.DataType$1 found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)\r\nDuplicate class org.tensorflow.lite.Delegate found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)\r\nDuplicate class org.tensorflow.lite.Interpreter found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)\r\nDuplicate class org.tensorflow.lite.Interpreter$Options found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)\r\nDuplicate class org.tensorflow.lite.NativeInterpreterWrapper found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)\r\nDuplicate class org.tensorflow.lite.Tensor found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)\r\nDuplicate class org.tensorflow.lite.TensorFlowLite found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)\r\nDuplicate class org.tensorflow.lite.nnapi.NnApiDelegate found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)\r\n\r\nI have tried other versions of tensorflow-lite, such as 1.10.0. This error also appears.\r\nI checked a lot of information and made a lot of changes, but still can't use this AAR and tflite that requires RandomUniform operator\r\n\r\nSo I urgently need your help ~~~", "comments": ["Your latest gradle file doesn't conform to the guidance:\r\n```\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n}\r\n```\r\n\r\nHave you given that a try?\r\n\r\nThe `tensorflow-lite-with-select-tf-ops` variant is deprecated, and was intended to replace the base `tensorflow-lite` dependency. However, the new `tensorflow-lite-select-tf-ops` dependency is intended to be used *in addition* to the `tensorflow-lite` dependency. I also noticed that there was a bad rebase in the docs for this feature, which i'll be reverting. However, I would give the approach mentioned above a try. Note that you might need to [clear your gradle cache](https://stackoverflow.com/questions/23025433/how-to-clear-gradle-cache).\r\n\r\n", "Thank you very much, I tried some methods and finally solved the problem. "]}, {"number": 35930, "title": "load_weights does not work when weights are saved in h5 format", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nmodel can't restore weights from h5 file\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n```\r\nclass Model(keras.Model):\r\n    def __init__(self, inp1, inp2):\r\n        super(Model, self).__init__()\r\n        self.x1 = self.add_weight('w1',[inp1])\r\n        self.x2 = self.add_weight('w2',[inp2])\r\n    def call(self,x):\r\n        return x\r\n# load_weights method works when save format is 'tf\r\nx = Model(100,200)\r\nx.save_weights('temp.tmp',save_format='tf')\r\nold = x.weights[0][0].numpy()\r\nprint(old)\r\nx = Model(100,200)\r\nx.load_weights('temp.tmp')\r\nnew = x.weights[0][0].numpy()\r\nprint(new)\r\nprint(old==new)\r\n\r\n# load_weights method does not work when save format is 'h5'\r\nx = Model(100,200)\r\nx.save_weights('temp.h5',save_format='h5')\r\nold = x.weights[0][0].numpy()\r\nprint(old)\r\nx = Model(100,200)\r\nx.load_weights('temp.h5')\r\nnew = x.weights[0][0].numpy()\r\nprint(new)\r\nprint(old==new)\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@bj1123 ,\r\nWhen tried running the given code in both [2.1](https://colab.sandbox.google.com/drive/1rqcqkeDpD0TG3brFJtyq1x8WEfET0vcU#scrollTo=RbZaVocMg-u_) and [tf-nightly](https://colab.sandbox.google.com/drive/19sCVnZyjsRYzxeCs5M5IjMGli_nP-PnK#scrollTo=0FawR_S5fFm8), I didn't face any error.Thanks!", "> @bj1123 ,\r\n> When tried running the given code in both [2.1](https://colab.sandbox.google.com/drive/1rqcqkeDpD0TG3brFJtyq1x8WEfET0vcU#scrollTo=RbZaVocMg-u_) and [tf-nightly](https://colab.sandbox.google.com/drive/19sCVnZyjsRYzxeCs5M5IjMGli_nP-PnK#scrollTo=0FawR_S5fFm8), I didn't face any error.Thanks!\r\n\r\n@oanush It doesn't raise error. The problem is that the saved weights are not restored. The weights of the model remain the same even after load_weights are called.", "@bj1123 ,\r\nI just modified few lines of code and I am able to load the model weights successfully,kindly refer the [gist of colab](https://colab.sandbox.google.com/gist/oanush/8b8802b2743fbaf47149c5941da8e213/35930_tf-nightly.ipynb). Please refer these links of [github](https://github.com/keras-team/keras/issues/12094#issuecomment-481397688) and [stackoverflow](https://stackoverflow.com/questions/41859997/keras-model-load-weights-for-neural-net) for more information.", "@oanush \r\nI think your modification can not assure the weights are successfully loaded. If you comment out the code for initializing the model, the model weights remain the same and printing variables \"old\" and \"new\" happen to be the same whether or not the weights are successfully loaded. I slightly changed the codes for better understanding, and it still shows the same problem.\r\n```\r\nclass Model(keras.Model):\r\n    def __init__(self, inp1, inp2):\r\n        super(Model, self).__init__()\r\n        self.x1 = self.add_weight('w1',[inp1])\r\n        self.x2 = self.add_weight('w2',[inp2])\r\n    def call(self,x):\r\n        return x\r\n# load_weights method works when save format is 'tf\r\nx = Model(100,200)\r\nx.save_weights('temp.tmp',save_format='tf')\r\nold = x.weights[0][0].numpy()\r\nx.weights[0].assign(tf.zeros_like(x.weights[0]))\r\nprint(old)\r\nx.load_weights('temp.tmp')\r\nnew = x.weights[0][0].numpy()\r\nprint(new)\r\nprint(old==new)\r\n\r\n# load_weights method does not work when save format is 'h5'\r\nx = Model(100,200)\r\nx.save_weights('temp.h5',save_format='h5')\r\nold = x.weights[0][0].numpy()\r\nx.weights[0].assign(tf.zeros_like(x.weights[0]))\r\nprint(old)\r\nx.load_weights('temp.h5')\r\nnew = x.weights[0][0].numpy()\r\nprint(new)\r\nprint(old==new)\r\n```\r\n", "Any update regarding this issue ? \r\n\r\nI am having the same problem trying to load inceptionV3 weights trained with TF1.13  using TF2.1.", "This issue is so annoying. I am using both Anaconda's TF 2.1 GPU and MKL on both Linux and Windows. After training loss is for example 0.2 loading the weights error increases significantly as if the model is not trained.", "I also have this issue, anyone know a solution yet?", "@cs-mac \r\nI hope this might help\r\n```\r\ndef load_weights(model,save_path):\r\n    hf = h5py.File(save_path, 'r')\r\n    for i in model.trainable_weights:\r\n        res = hf.get(i.name)\r\n        res = tf.convert_to_tensor(np.array(res))\r\n        if res.shape == i.shape:\r\n            i.assign(res)\r\n```", "@bj1123 \r\nThank you for your reply! \r\nI managed to fix the mistake, embarrassingly enough it was a user error on my part.\r\nI remade the whole trained model to access some layer I needed. Then I used this newly created model, which of course was not trained yet, instead of the trained model. Therefore, all attention weights seemed random. \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35930\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35930\">No</a>\n"]}, {"number": 35929, "title": "`ValueError: Unable to decode config: None` when load_model", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: Python 3.7\r\n\r\n**Describe the current behavior**\r\nI save the model using ModelCheckpoint in model.fit(), and after the training, I can load model weights  sucessfully if I set `save_weights_only=True`. but come across the error when save both the weights and model architecture by set `save_weights_only=True` using `tf.keras.models.load_model` \r\n\r\n**Describe the expected behavior**\r\n`ValueError: Unable to decode config: None` when load_model\r\n\r\n", "comments": ["The error is because I overwrite __call__(...) function in my customed layer class.\r\nThe correct way is to firstly define __call__(...) function and then return super()__call__(...)"]}, {"number": 35928, "title": "Low performance when using persistent mode GradientTape with LSTM layers", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Conda\r\n- TensorFlow version (use command below): 2.0.0 and 2.1.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.0.130 / 7.6.4\r\n- GPU model and memory: GTX 980 Ti, GTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nThe performance was very low when using persistent mode tf.GradientTape or create multi-GradientTape objects in one ```with``` block.\r\nThis phenomenon only happens when the model includes a LSTM layer.\r\n\r\n**Code to reproduce the issue**\r\nHere is a sample code to reproduce the problem.\r\n```python\r\nimport timeit\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel0 = tf.keras.models.Sequential(\r\n    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n)\r\nmodel1 = tf.keras.models.Sequential(\r\n    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))\r\n)\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\n@tf.function\r\ndef train_step_0():\r\n  with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    y_p1 = model1(tf.random.normal((256, 128)))\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n  grad1 = tape1.gradient(loss1, model1.trainable_variables)\r\n\r\ntrain_step_0()\r\nprint(timeit.timeit('train_step_0()', globals=globals(), number=100))\r\n```\r\nIn this case, the GPU-Util was only about 30 % and the time was 37 seconds.\r\nThe following message was shown in screen.\r\n```\r\n2020-01-16 17:16:22.519978: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 0 of node sequential/lstm/zeros_like was passed int32 from sequential/lstm/StatefulPartitionedCall:9 incompatible with expected variant.\r\n2020-01-16 17:16:22.667364: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 0 of node sequential/lstm/zeros_like was passed int32 from sequential/lstm/StatefulPartitionedCall:9 incompatible with expected variant.\r\n2020-01-16 17:16:22.739393: W tensorflow/core/common_runtime/process_function_library_runtime.cc:675] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node sequential/lstm/zeros_like was passed int32 from sequential/lstm/StatefulPartitionedCall:9 incompatible with expected variant.\r\n```\r\n\r\n```python\r\n@tf.function\r\ndef train_step_1():\r\n  with tf.GradientTape(persistent=True) as tape:\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    y_p1 = model1(tf.random.normal((256, 128)))\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n  grad0 = tape.gradient(loss0, model0.trainable_variables)\r\n  grad1 = tape.gradient(loss1, model1.trainable_variables)\r\n\r\ntrain_step_1()\r\nprint(timeit.timeit('train_step_1()', globals=globals(), number=100))\r\n```\r\nIn this case, the performance was similar to the previous one.\r\n\r\n```python\r\n@tf.function\r\ndef train_step_2():\r\n  with tf.GradientTape() as tape0:\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n  with tf.GradientTape() as tape1:\r\n    y_p1 = model1(tf.random.normal((256, 128)))\r\n    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n  grad1 = tape1.gradient(loss1, model1.trainable_variables)\r\n\r\ntrain_step_2()\r\nprint(timeit.timeit('train_step_2()', globals=globals(), number=100))\r\n```\r\nIn this case, the GPU-Util was almost 100 % and the time was only 4 seconds.\r\n\r\nIn my opinion, ```train_step_0```, ```train_step_1``` and ```train_step_2``` should have similar performance.\r\nI am wandering why the GPU-Util and process time were so that different.\r\nThe strangest thing is that this phenomenon only happens when the model includes a LSTM layer.\r\nIf we exchange the LSTM layer to Conv or Dense layer, the process time will be all same.\r\nHere is a colaboratory page to reproduce this problem.\r\nhttps://colab.research.google.com/drive/1sluVFuW1yYtH0Ye4reoOEmLUHYHDSGn7\r\n", "comments": ["There's some thought, or at least hope, that https://github.com/tensorflow/tensorflow/commit/3a03164a24ea7dbfe221e84ca62e2a39b745c379 fixes the issue. Would you mind waiting until that's in a nightly (presumably tomorrow) and trying again?\r\n\r\nIf that fixes it then the performance issue was that this dtype mismatch meant that Grappler wasn't choosing the CuDNN LSTM implementation, and presumably the alternative was much slower.", "I can trigger the same warning messages and performance degradation by removing the @tf.function decorator from the GradientTape (only decorating its body) Example here with TF 2.0.0:\r\n\r\n```\r\nimport timeit\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel0 = tf.keras.models.Sequential(\r\n    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n)\r\nmodel1 = tf.keras.models.Sequential(\r\n    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))\r\n)\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\n\r\n@tf.function\r\ndef body0():\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n    return loss0\r\n\r\ndef train_step_3():\r\n  with tf.GradientTape() as tape0:\r\n    loss0 = body0()\r\n  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n\r\n\r\ntrain_step_3()\r\nprint(timeit.timeit('train_step_3()', globals=globals(), number=100))\r\n```\r\nOutput:\r\n```\r\n2020-03-11 15:39:48.084997: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 0 of node sequential_lstm_statefulpartitionedcall_28_RetVal was passed int32 from sequential/lstm/StatefulPartitionedCall:29 incompatible with expected variant.\r\n2020-03-11 15:39:48.138852: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 0 of node sequential_lstm_statefulpartitionedcall_8_RetVal was passed int32 from sequential/lstm/StatefulPartitionedCall:9 incompatible with expected variant.\r\n2020-03-11 15:39:48.168850: W tensorflow/core/common_runtime/process_function_library_runtime.cc:675] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node sequential_lstm_statefulpartitionedcall_8_RetVal was passed int32 from sequential/lstm/StatefulPartitionedCall:9 incompatible with expected variant.\r\n2020-03-11 15:39:48.726501: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] implementation_selector failed: Invalid argument: Invalid format of input node name:  Expected: {forward_node_name}:{index}\r\n2020-03-11 15:39:48.795141: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_1162_1633' and '__inference___backward_standard_lstm_1162_1633_specialized_for_gradients_sequential_lstm_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference___backward_body0_1074_1694' both implement 'lstm_bcb537ae-8579-4358-b3be-f7945b56523d' but their signatures do not match.\r\n40.24392790393904\r\n```\r\n\r\nAfter adding the @tf.function decorator to the outside of gradienttape I do not get the warnings and it is faster:\r\n```\r\nimport timeit\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel0 = tf.keras.models.Sequential(\r\n    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n)\r\nmodel1 = tf.keras.models.Sequential(\r\n    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))\r\n)\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\n\r\n@tf.function\r\ndef body0():\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n    return loss0\r\n\r\n@tf.function\r\ndef train_step_3():\r\n  with tf.GradientTape() as tape0:\r\n    loss0 = body0()\r\n  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n\r\n\r\ntrain_step_3()\r\nprint(timeit.timeit('train_step_3()', globals=globals(), number=100))\r\n```\r\nOutput: \r\n```\r\n2020-03-11 15:41:23.242231: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_855_1035' and '__inference___backward_cudnn_lstm_with_fallback_855_1035_specialized_for_StatefulPartitionedCall_1_gradients_sequential_lstm_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_train_step_3_1756' both implement 'lstm_a139ee92-2e42-4749-93b3-0b4af6926e59' but their signatures do not match.\r\n8.711773838382214\r\n```", "@Andreas5739738, would you mind trying with the latest nightly? 2.2.0.dev20200311 should have https://github.com/tensorflow/tensorflow/commit/3a03164a24ea7dbfe221e84ca62e2a39b745c379 which we hope fixes both the Grappler messages and performance.", "I can still see the same warning messages with tf-nightly:\r\nhttps://colab.research.google.com/gist/Andreas5739738/c3ec84dba75adb6880124c840c0be12f/untitled0.ipynb", "I don't see the Grappler messages in the colab. Which messages?", "If you run the code again and go to \"Runtime\" \u2192 \"show logs\", you will see them (they don't seem to be stored across runs).", "Ah, I see. @rmlarsen any ideas? Those warnings do explicitly mention the implementation selector:\r\n\r\n\r\n> Mar 11, 2020, 10:39:03 AM | WARNING | 2020-03-11 17:39:03.239186: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] implementation_selector failed: Invalid argument: Invalid format of input node name: Expected: {forward_node_name}:{index}\r\n> -- | -- | --\r\n> Mar 11, 2020, 10:39:01 AM | WARNING | 2020-03-11 17:39:01.391898: W tensorflow/core/common_runtime/process_function_library_runtime.cc:697] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node sequential_lstm_statefulpartitionedcall_8_RetVal was passed float from sequential/lstm/StatefulPartitionedCall:9 incompatible with expected variant.\r\n> Mar 11, 2020, 10:39:01 AM | WARNING | 2020-03-11 17:39:01.383694: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] function_optimizer failed: Invalid argument: Input 0 of node sequential_lstm_statefulpartitionedcall_10_RetVal was passed float from sequential/lstm/StatefulPartitionedCall:11 incompatible with expected resource.\r\n> Mar 11, 2020, 10:39:01 AM | WARNING | 2020-03-11 17:39:01.350266: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] function_optimizer failed: Invalid argument: Input 0 of node sequential_lstm_statefulpartitionedcall_10_RetVal was passed float from sequential/lstm/StatefulPartitionedCall:11 incompatible with expected resource.\r\n> \r\n> ", "@zhuzhi-fairy Could you please check with latest TF version and let us know if the issue stil persists. Thanks!. ", "> @zhuzhi-fairy Could you please check with latest TF version and let us know if the issue stil persists. Thanks!.\r\n\r\nI have checked the codes with tf 2.2.0, and the problem has been solved, I think. \r\nThank you very much.", "@saikumarchalla  I can still reproduce the problem with tf_nightly-2.3.0.dev20200518-cp36-cp36m-manylinux2010_x86_64.whl and the [colab](https://colab.research.google.com/gist/Andreas5739738/c3ec84dba75adb6880124c840c0be12f/untitled0.ipynb) linked above (run & view session logs):\r\n```\r\n2020-05-18 13:07:33.807689: W tensorflow/core/common_runtime/process_function_library_runtime.cc:773] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node sequential_lstm_partitionedcall_5_RetVal was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n```", "I am having the same error in version 2.4.1\r\nOP's code no longer replicates the eror. Instead, it appears when you add an optimizer\r\n\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel0 = tf.keras.models.Sequential(\r\n    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n)\r\nmodel1 = tf.keras.models.Sequential(\r\n    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))\r\n)\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\noptimizer=tf.keras.optimizers.Adam()\r\n\r\n@tf.function\r\ndef train_step_0():\r\n  with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    y_p1 = model1(tf.random.normal((256, 128)))\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n  grad1 = tape1.gradient(loss1, model1.trainable_variables)\r\n  optimizer.apply_gradients(zip(grad0,model0.trainable_variables))\r\n  optimizer.apply_gradients(zip(grad1,model1.trainable_variables))\r\n\r\nt0=time.time()\r\nfor i in range(100):\r\n  train_step_0()\r\nprint(time.time()-t0)\r\n```\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel0 = tf.keras.models.Sequential(\r\n    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n)\r\nmodel1 = tf.keras.models.Sequential(\r\n    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))\r\n)\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\noptimizer=tf.keras.optimizers.Adam()\r\n\r\n@tf.function\r\ndef train_step_1():\r\n  with tf.GradientTape(persistent=True) as tape:\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    y_p1 = model1(tf.random.normal((256, 128)))\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n  grad0 = tape.gradient(loss0, model0.trainable_variables)\r\n  grad1 = tape.gradient(loss1, model1.trainable_variables)\r\n  optimizer.apply_gradients(zip(grad0,model0.trainable_variables))\r\n  optimizer.apply_gradients(zip(grad1,model1.trainable_variables))\r\n\r\nt0=time.time()\r\nfor i in range(100):\r\n  train_step_1()\r\nprint(time.time()-t0)\r\n```\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel0 = tf.keras.models.Sequential(\r\n    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n)\r\nmodel1 = tf.keras.models.Sequential(\r\n    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))\r\n)\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\noptimizer=tf.keras.optimizers.Adam()\r\n\r\n@tf.function\r\ndef train_step_2():\r\n  with tf.GradientTape() as tape0:\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n  with tf.GradientTape() as tape1:\r\n    y_p1 = model1(tf.random.normal((256, 128)))\r\n    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n  grad1 = tape1.gradient(loss1, model1.trainable_variables)\r\n  optimizer.apply_gradients(zip(grad0,model0.trainable_variables))\r\n  optimizer.apply_gradients(zip(grad1,model1.trainable_variables))\r\n\r\nt0=time.time()\r\nfor i in range(100):\r\n  train_step_2()\r\nprint(time.time()-t0)\r\n```\r\nBoth train_step_0 and train_step_1 show the error, while train_step_2 doesn't. In my GPU, the first 2 approaches take around 11.5s in doing 100 training steps, while the third one takes 3.5s.\r\n\r\nEdit: I tried with Tensorflow 2.5 and the gap is even bigger\r\n\r\nThis issue should be reoppened @ymodak "]}, {"number": 35927, "title": "error when set converter.experimental_new_converter = True", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (or github SHA if from source): tf-nightly == 2.1.0-dev20200101\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\ndef model():\r\n    input_image = keras.layers.Input(shape=[])\r\n    C3, C4, C5 = resnet_graph(input_image)\r\n    P3, P4, P5, P6, P7 = FPN_graph(C3, C4, C5)\r\n    loc_data, conf_data, mask_data = predict_graph(P3, P4, P5, P6, P7)\r\n    proto_data = protonet(P3)\r\n    anchors = get_priors(config.IMAGE_SHAPE)\r\n    refined_boxes = DecodeBoxes(loc_data, anchors)\r\n    # batch_multiclass_non_max_suppression() is in object_detection.core.post_processing.py\r\n    boxes, scores, class_ids, mask_coef, num_detection =   batch_multiclass_non_max_suppression()\r\n    masks = AssemblyMask(mask_coef, proto_out, boxes)\r\n    model = keras.models.Model([input_image],[boxes, class_ids, scores, masks, num_detections])\r\n   return model\r\n\r\nmodel = model()\r\nmodel.load_weights('.h5')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()    \r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-01-16 14:11:43.696917: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 14:11:43.696927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-16 14:11:43.696936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-16 14:11:43.696944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-16 14:11:43.696952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-16 14:11:43.696960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-16 14:11:43.696968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-16 14:11:43.697006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:43.697323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:43.697611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-16 14:11:43.697635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 14:11:43.698527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-16 14:11:43.698539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-01-16 14:11:43.698544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-01-16 14:11:43.698621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:43.698944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:43.699248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8422 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-01-16 14:11:58.365227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:58.365704: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-01-16 14:11:58.365842: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-01-16 14:11:58.366283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:58.366583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-01-16 14:11:58.366616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 14:11:58.366627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-16 14:11:58.366636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-16 14:11:58.366645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-16 14:11:58.366655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-16 14:11:58.366664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-16 14:11:58.366674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-16 14:11:58.366709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:58.367016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:58.367298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-16 14:11:58.367315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-16 14:11:58.367319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-01-16 14:11:58.367323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-01-16 14:11:58.367372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:58.367682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:11:58.367971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8422 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-01-16 14:11:58.687280: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize\r\n2020-01-16 14:11:58.687309: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 1511 nodes (0), 3113 edges (0), time = 51.53ms.\r\n2020-01-16 14:11:58.687313: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 1511 nodes (0), 3113 edges (0), time = 55.71ms.\r\n2020-01-16 14:11:58.687315: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: yolact_yolact_detection_map_while_cond_24503\r\n2020-01-16 14:11:58.687319: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-16 14:11:58.687322: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-16 14:11:58.687324: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: yolact_yolact_detection_map_while_body_24504\r\n2020-01-16 14:11:58.687327: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-01-16 14:11:58.687330: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-16 14:12:00.043387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:00.043733: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-01-16 14:12:00.043796: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-01-16 14:12:00.044138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:00.044457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-01-16 14:12:00.044485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 14:12:00.044496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-16 14:12:00.044505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-16 14:12:00.044515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-16 14:12:00.044524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-16 14:12:00.044533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-16 14:12:00.044542: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-16 14:12:00.044572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:00.044887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:00.045176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-16 14:12:00.045194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-16 14:12:00.045199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-01-16 14:12:00.045202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-01-16 14:12:00.045251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:00.045555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:00.045994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8422 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-01-16 14:12:06.926724: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize\r\n2020-01-16 14:12:06.926752: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 955 nodes (-556), 2636 edges (-445), time = 208.331ms.\r\n2020-01-16 14:12:06.926756: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 955 nodes (0), 2636 edges (0), time = 80.347ms.\r\n2020-01-16 14:12:06.926759: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: yolact_yolact_detection_map_while_cond_24503_frozen\r\n2020-01-16 14:12:06.926762: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 19 nodes (0), 8 edges (0), time = 0.333ms.\r\n2020-01-16 14:12:06.926765: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 19 nodes (0), 8 edges (0), time = 0.201ms.\r\n2020-01-16 14:12:06.926767: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: yolact_yolact_detection_map_while_body_24504_frozen\r\n2020-01-16 14:12:06.926770: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 4210 nodes (-252), 5496 edges (-407), time = 6253.95508ms.\r\n2020-01-16 14:12:06.926773: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 4210 nodes (0), 5496 edges (0), time = 99.047ms.\r\nTraceback (most recent call last):\r\n  File \"/home/chengxu/deeplearning/tensorflow-2.0-study/yolact.py\", line 1567, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 490, in convert\r\n    **converter_kwargs)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 476, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 215, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\nWARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .\r\n2020-01-16 14:12:08.249581: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.\r\n2020-01-16 14:12:08.249608: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:114] Ignored drop_control_dependency.\r\n2020-01-16 14:12:08.674483: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-16 14:12:08.698096: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 3600000000 Hz\r\n2020-01-16 14:12:08.698483: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bf05e1f380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-16 14:12:08.698496: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-16 14:12:08.700080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-16 14:12:08.752405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:08.752789: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bf05e3f110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-01-16 14:12:08.752801: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-01-16 14:12:08.752929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:08.753249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-01-16 14:12:08.753413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 14:12:08.754500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-16 14:12:08.755559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-16 14:12:08.755751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-16 14:12:08.756823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-16 14:12:08.757307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-16 14:12:08.759489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-16 14:12:08.759597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:08.759997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:08.760301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-16 14:12:08.760338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 14:12:08.760970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-16 14:12:08.760980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-01-16 14:12:08.760999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-01-16 14:12:08.761074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:08.761435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:12:08.761758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8039 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nloc(callsite(\"yolact/yolact_detection/map/TensorArrayV2_5\"(\"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\":425:0) at callsite(\"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\":574:0 at callsite(\"/home/chengxu/deeplearning/tensorflow-2.0-study/utils/shape_utils.py\":228:0 at callsite(\"/home/chengxu/deeplearning/tensorflow-2.0-study/core/post_processing.py\":476:0 at callsite(\"/home/chengxu/deeplearning/tensorflow-2.0-study/yolact.py\":1118:0 at callsite(\"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\":308:0 at callsite(\"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\":785:0 at callsite(\"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\":918:0 at callsite(\"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\":744:0 at \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\":785:0)))))))))): error: operand type 'tensor<i32>' is not compatible with preceding operands; expected rank: 1\r\nTraceback (most recent call last):\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: /home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py:425:7: error: operand type 'tensor<i32>' is not compatible with preceding operands; expected rank: 1\r\n      name=name)\r\n      ^\r\n/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py:574:7: note: called from\r\n      return func(*args, **kwargs)\r\n      ^\r\n/home/chengxu/deeplearning/tensorflow-2.0-study/utils/shape_utils.py:228:9: note: called from\r\n        return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)\r\n        ^\r\n/home/chengxu/deeplearning/tensorflow-2.0-study/core/post_processing.py:476:7: note: called from\r\n      parallel_iterations=parallel_iterations)\r\n      ^\r\n/home/chengxu/deeplearning/tensorflow-2.0-study/yolact.py:1118:98: note: called from\r\n                                                                                                 masks=mask_data)\r\n                                                                                                 ^\r\n/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py:308:7: note: called from\r\n      return func(*args, **kwargs)\r\n      ^\r\n/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:785:19: note: called from\r\n                  outputs = call_fn(cast_inputs, *args, **kwargs)\r\n                  ^\r\n/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py:918:11: note: called from\r\n          output_tensors = layer(computed_tensors, **kwargs)\r\n          ^\r\n/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py:744:9: note: called from\r\n        convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n        ^\r\n/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:785:19: note: called from\r\n                  outputs = call_fn(cast_inputs, *args, **kwargs)\r\n                  ^\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nif remove converter.experimental_new_converter = True\r\n\r\nWARNING:absl:Please consider switching to use new converter by setting experimental_new_converter to true. Old converter (TOCO) is deprecated and flow will be switched on by default to use new converter soon.\r\nTraceback (most recent call last):\r\n  File \"/home/chengxu/deeplearning/tensorflow-2.0-study/yolact.py\", line 1567, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 490, in convert\r\n    **converter_kwargs)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 476, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 215, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\nWARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .\r\n2020-01-16 14:54:09.455905: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-16 14:54:09.477996: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 3600000000 Hz\r\n2020-01-16 14:54:09.478678: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55641b1a91a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-16 14:54:09.478691: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-16 14:54:09.480333: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-16 14:54:09.539873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:54:09.540262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55641b23d750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-01-16 14:54:09.540278: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-01-16 14:54:09.540443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:54:09.540744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-01-16 14:54:09.540885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 14:54:09.542169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-16 14:54:09.543344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-16 14:54:09.543537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-16 14:54:09.544789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-16 14:54:09.545460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-16 14:54:09.548110: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-16 14:54:09.548224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:54:09.548649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:54:09.548942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-16 14:54:09.548974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-16 14:54:09.549580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-16 14:54:09.549591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 \r\n2020-01-16 14:54:09.549598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N \r\n2020-01-16 14:54:09.549669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:54:09.550038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-16 14:54:09.550342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8020 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-01-16 14:54:09.621430: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2020-01-16 14:54:09.621470: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.621481: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2020-01-16 14:54:09.621489: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.621497: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2020-01-16 14:54:09.621505: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.621513: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2020-01-16 14:54:09.621520: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.621527: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2020-01-16 14:54:09.621535: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.621542: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2020-01-16 14:54:09.621553: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622587: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2020-01-16 14:54:09.622602: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622657: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2020-01-16 14:54:09.622667: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622707: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2020-01-16 14:54:09.622716: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622737: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2020-01-16 14:54:09.622744: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622759: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2020-01-16 14:54:09.622776: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622782: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622788: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622793: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622798: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622804: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622809: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622814: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622819: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622824: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-16 14:54:09.622833: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2020-01-16 14:54:09.622843: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2020-01-16 14:54:09.622852: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2020-01-16 14:54:09.622861: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2020-01-16 14:54:09.622871: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2020-01-16 14:54:09.641969: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 655 operators, 1254 arrays (0 quantized)\r\n2020-01-16 14:54:09.652164: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 655 operators, 1254 arrays (0 quantized)\r\n2020-01-16 14:54:09.701043: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 236 operators, 568 arrays (0 quantized)\r\n2020-01-16 14:54:09.704528: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 233 operators, 562 arrays (0 quantized)\r\n2020-01-16 14:54:09.707893: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 233 operators, 562 arrays (0 quantized)\r\n2020-01-16 14:54:09.711235: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 233 operators, 562 arrays (0 quantized)\r\n2020-01-16 14:54:09.713664: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 233 operators, 562 arrays (0 quantized)\r\n2020-01-16 14:54:09.715367: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 233 operators, 562 arrays (0 quantized)\r\n2020-01-16 14:54:09.726982: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 18249152 bytes, theoretical optimal value: 11829248 bytes.\r\n2020-01-16 14:54:09.727774: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 8641389\r\n2020-01-16 14:54:09.729089: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, DIV, EXP, EXPAND_DIMS, FULLY_CONNECTED, GREATER_EQUAL, LESS, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, REDUCE_MAX, REDUCE_MIN, RELU, RESHAPE, RESIZE_BILINEAR, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\nTraceback (most recent call last):\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, DIV, EXP, EXPAND_DIMS, FULLY_CONNECTED, GREATER_EQUAL, LESS, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, REDUCE_MAX, REDUCE_MIN, RELU, RESHAPE, RESIZE_BILINEAR, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\nProcess finished with exit code 1\r\n\r\n", "comments": ["Your model contains control flow and therefore is not supported in our old converter.\r\n\r\nRegarding the new converter (i.e. when `experimental_new_converter = True`), it is not obvious where your error is coming from. Can you provide a reproducible example with functions like `resnet_graph` defined. Alternatively, can you provide the model file so we can reproduce your conversion on our end.\r\n\r\nReassigning to @karimnosseir who has more familiarity with the new converter errors.", "Hi, the wights are too big, so I just post the minimal code which can produce same problem. [\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/4075261/model.zip)\r\n](url)\r\n\r\nIn the old tensorflow version like tf1.12, tf1.14, NonMaxSuppressionV3 is not supported when convert  the model to tflite, but in tf-nightly, there is no error about NonMaxSuppression, is it supported now?\r\n\r\nFor the latest tensorflow, is it supported to convert object detection model rather than SSD to tflite? ", "Can you please share the TF graph def or at least instructions on running the scripts you shared\r\n\r\nThanks", "Just need tf-nightly, python 3.7, and run model.py, it would print the same error as I mentioned.", "Having a similar issue with the new converter erroring out. python/keras/engine/network.py seems to be the common element. Using nightly-tf from today and new converter. \r\n\r\n2020-01-21 17:20:47.380130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4119 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\nloc(callsite(\"mask_rcnn/mrcnn_detection/map/TensorArrayV2_1\"(\"/home/testuser/projects/cleancahome/venv37/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\":425:0) at callsite(\"/home/testuser/projects/cleancahome/venv37/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\":574:0 at callsite(\"/home/testuser/projects/cleancahome/CamAi/CamAi/mrcnn_model.py\":786:0 at callsite(\"/home/testuser/projects/cleancahome/CamAi/CamAi/mrcnn_model.py\":850:0 at callsite(\"/home/testuser/projects/cleancahome/CamAi/CamAi/mrcnn_utils.py\":845:0 at callsite(\"/home/testuser/projects/cleancahome/CamAi/CamAi/mrcnn_model.py\":851:0 at callsite(\"/home/testuser/projects/cleancahome/venv37/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\":308:0 at callsite(\"/home/testuser/projects/cleancahome/venv37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\":779:0 at callsite(\"/home/testuser/projects/cleancahome/venv37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\":918:0 at \"/home/testuser/projects/cleancahome/venv37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\":744:0)))))))))): error: operand type 'tensor<i32>' is not compatible with preceding operands; expected rank: 1\r\nTraceback (most recent call last):", "@haozha111 Can you please have a look, specifically on TensorList issues.\r\n\r\nThanks", "Sure. I will need to take some time to repro the problem. This might not relate to tensorlist though.\r\n\r\n(note that you can't get rid of converter.experimental_new_converter = True), since your graph contains control flows which can *only* be handled by the new converter.", "Hmm,\r\nThe failure from the converter is because of concat op failing verification\r\nAccording to the tensorflow op\r\nhttps://www.tensorflow.org/api_docs/python/tf/concat\r\n\r\n\"The number of dimensions of the input tensors must match, and all dimensions except axis must be equal.\"\r\n\r\nBut the Concat has inputs\r\n(tensor<i32>, tensor<1xi32>, tensor<i32>) \r\n\r\nCan you please confirm that you are expecting concat to receive these shapes ?\r\n\r\nThanks", "Do you mean error is in line 517 detections = tf.concat(..)? It is not necessary, so if i remove those lines, it will be successful?", "The error is\r\n\"error: operand type 'tensor<i32>' is not compatible with preceding operands; expected rank: 1\r\n        for dt in dtype_flat]\"\r\n\r\nIf you looked at the stack trace presented in failure\r\n... map_fn.py:242:9: note: called from for dt in dtype_flat]\r\n... yolact_test.py:602:32: note: called from dtype=tf.float32)\r\n...yolact_test.py:676:46: note: called from\r\n                                             lambda x, y, w, m, p: refine_detections_graph_3(x, y, w, m, p, self.config),\r\n...yolact_test.py:88:9: note: called from\r\n        output_slice = graph_fn(*inputs_slice)\r\n...yolact_test.py:677:46: note: called from\r\n                                             self.config.IMAGES_PER_GPU)\r\n\r\nYou need to fix your model as this is requirement of concat op, not specific to the converter.\r\n\r\n", "If you still having problems after fixing the model, please reopen the issue and we will be happy to help.\r\n\r\nThanks for the feedback", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35927\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35927\">No</a>\n", "@karimnosseir Hi, thank you for your answer. There is a problem, if the error is just in concat op, it should print error after model.build rather than converter. And this code works fine in prediction. So i wonder if there are any differences of concat op between tensorflow and tensorflow Lite?", "I was checking some other issue and i think it might be related. Reopening and checking.\r\n", "Hey, I found that if i use map_fn in subclass of keras.layers.Layer which would print same error as above. As shown in code\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, \"Not enough GPU handware devices available\"\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\ndef batch_slice(inputs, graph_fn, batch_size, names=None):\r\n    if not isinstance(inputs, list):\r\n        inputs = [inputs]\r\n\r\n    outputs = []\r\n    for i in range(batch_size):\r\n        inputs_slice = [x[i] for x in inputs]\r\n        output_slice = graph_fn(*inputs_slice)\r\n        if not isinstance(output_slice, (tuple, list)):\r\n            output_slice = [output_slice]\r\n        outputs.append(output_slice)\r\n\r\n    outputs = list(zip(*outputs))\r\n\r\n    if names is None:\r\n        names = [None] * len(outputs)\r\n\r\n    result = [tf.stack(o, axis=0, name=n)\r\n              for o, n in zip(outputs, names)]\r\n    if len(result) == 1:\r\n        result = result[0]\r\n\r\n    return result\r\n\r\ndef refine_detections_graph(conf_data):\r\n    def test(class_id):\r\n        return conf_data\r\n    class_id = tf.range(10)\r\n    r = tf.map_fn(test, class_id, dtype=tf.float32)\r\n    return r\r\n\r\nclass DetectionLayer(keras.layers.Layer):\r\n    def __init__(self):\r\n        super(DetectionLayer, self).__init__()\r\n\r\n    def call(self, inputs):\r\n        conf_data = inputs[0]\r\n\r\n        r = batch_slice([conf_data],\r\n                        lambda x: refine_detections_graph(x),1)\r\n        return r\r\n\r\ninput_image = keras.layers.Input([300,300,3])\r\nx = keras.layers.Conv2D(64, (3,3), strides=2, padding='same')(input_image)\r\nx = keras.layers.Conv2D(128, (3,3), strides=2, padding='same')(x)\r\nx = keras.layers.Conv2D(256, (3,3), strides=2, padding='same')(x)\r\npred_score = keras.layers.Conv2D(10, (3,3), strides=2, padding='same', activation='softmax')(x)\r\npred_score = keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 10]))(pred_score)\r\n\r\nresult = DetectionLayer()([pred_score])\r\n\r\nmodel = keras.models.Model(inputs = input_image, outputs = result)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                                        tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n\r\n```\r\n\r\nHowever, if I just use function rather than use subclass of keras.layers.Layer, it can be converted successfully.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, \"Not enough GPU handware devices available\"\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\ndef batch_slice(inputs, graph_fn, batch_size, names=None):\r\n    if not isinstance(inputs, list):\r\n        inputs = [inputs]\r\n\r\n    outputs = []\r\n    for i in range(batch_size):\r\n        inputs_slice = [x[i] for x in inputs]\r\n        output_slice = graph_fn(*inputs_slice)\r\n        if not isinstance(output_slice, (tuple, list)):\r\n            output_slice = [output_slice]\r\n        outputs.append(output_slice)\r\n    outputs = list(zip(*outputs))\r\n\r\n    if names is None:\r\n        names = [None] * len(outputs)\r\n\r\n    result = [tf.stack(o, axis=0, name=n)\r\n              for o, n in zip(outputs, names)]\r\n    if len(result) == 1:\r\n        result = result[0]\r\n\r\n    return result\r\n\r\ndef refine_detections_graph(conf_data):\r\n    def test(class_id):\r\n        return conf_data\r\n    class_id = tf.range(10)\r\n    r = tf.map_fn(test, class_id, dtype=tf.float32)\r\n    return r\r\n\r\nclass DetectionLayer(keras.layers.Layer):\r\n    def __init__(self):\r\n        super(DetectionLayer, self).__init__()\r\n\r\n    def call(self, inputs):\r\n        conf_data = inputs[0]\r\n\r\n        r = batch_slice([conf_data],\r\n                        lambda x: refine_detections_graph(x),1)\r\n        return r\r\n\r\ninput_image = keras.layers.Input([300,300,3])\r\nx = keras.layers.Conv2D(64, (3,3), strides=2, padding='same')(input_image)\r\nx = keras.layers.Conv2D(128, (3,3), strides=2, padding='same')(x)\r\nx = keras.layers.Conv2D(256, (3,3), strides=2, padding='same')(x)\r\n\r\npred_score = keras.layers.Conv2D(10, (3,3), strides=2, padding='same', activation='softmax')(x)\r\npred_score = keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 10]))(pred_score)\r\n\r\nresult = batch_slice([pred_score], lambda x: refine_detections_graph(x), 1)\r\n\r\nmodel = keras.models.Model(inputs = input_image, outputs = result)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\nAnd one more question, in old version of keras, output tensors to a Model must be the output of a TensorFlow 'Layer' while in new version, there seems to be no such restriction?", "Hi,\r\n\r\nWhen you pass pure python function to output, keras will create a layer per tensorflow op. Please see:\r\nhttps://github.com/tensorflow/tensorflow/blob/dd662a9228b0f94ed2527f4117f2bef62e0aad00/tensorflow/python/keras/engine/base_layer.py#L2756\r\n\r\nEssentially they shouldn't have semantic differences but the generated graph may have some differences. ", "I will close this issue now since you already have a walk-around.\r\n\r\nI also did an investigation about why your model can't convert before, it seems that this hits an unsupported case in TF lite converter:\r\nhttps://github.com/tensorflow/tensorflow/blob/efc60f17fa8662049912469c7dc3e6fd04f27faf/tensorflow/compiler/mlir/lite/transforms/lower_static_tensor_list.cc#L350\r\n\r\nTensorArrays in TF can be dynamic shaped, but in TF Lite we don't support this. I added a check here to return this error during the pass, rather than silently generating the wrong graph (the ill-formed concat op).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35927\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35927\">No</a>\n"]}, {"number": 35926, "title": "Install Tensorflow 1.15 on CUDA 10.2 and cudnn 7.6", "body": "**System information**\r\n- OS Platform and Distribution: **Linux Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version: **1.15**\r\n- Python version: **3.6**\r\n- Installed using virtualenv? pip? conda?: **Conda**\r\n- CUDA/cuDNN version: CUDA 10.2 and cuDNN 7.6\r\n- GPU model and memory: Tesla K80 \r\n\r\n\r\n**Describe the problem**\r\n\r\nI have an exisiting code base that uses Tensorflow 1.15. I want to train my model on Azure Instance that uses Tesla K80. I've also installed CUDA and cuDNN versions mentioned above, globally as well as through anaconda. I want to install  the specific version of the tensorflow, is this possible? Can I get prebuilt-binaries for the same?\r\n\r\nInitially I tried installing through **conda install tensorflow-gpu**, I get the following error while running my script.\r\n\r\n`Loaded runtime CuDNN library: 7605 (compatibility version 7600) but source was compiled with 7102`\r\n\r\n**Any other info / logs**\r\n`System info:\r\n--------------------------------------------------------------------------------\r\n__Time Stamp__\r\n2020-01-16 06:18:49.664813\r\n\r\n__Hardware Information__\r\nMachine                                       : x86_64\r\nCPU Name                                      : haswell\r\nCPU count                                     : 24\r\nCFS restrictions                              : None\r\nCPU Features                                  :\r\n64bit aes avx avx2 bmi bmi2 cmov cx16 f16c fma fsgsbase invpcid lzcnt mmx movbe\r\npclmul popcnt rdrnd sahf sse sse2 sse3 sse4.1 sse4.2 ssse3 xsave xsaveopt\r\n\r\n__OS Information__\r\nPlatform                                      : Linux-4.15.0-1066-azure-x86_64-with-debian-stretch-sid\r\nRelease                                       : 4.15.0-1066-azure\r\nSystem Name                                   : Linux\r\nVersion                                       : #71-Ubuntu SMP Thu Dec 12 20:35:32 UTC 2019\r\nOS specific info                              : debianstretch/sid\r\nglibc info                                    : glibc 2.9\r\n\r\n__Python Information__\r\nPython Compiler                               : GCC 7.3.0\r\nPython Implementation                         : CPython\r\nPython Version                                : 3.6.10\r\nPython Locale                                 : en_US UTF-8\r\n\r\n__LLVM information__\r\nLLVM version                                  : 8.0.0\r\n\r\n__CUDA Information__\r\nFound 4 CUDA devices\r\nid 0            b'Tesla K80'                              [SUPPORTED]\r\n                      compute capability: 3.7\r\n                           pci device id: 0\r\n                              pci bus id: 0\r\nid 1            b'Tesla K80'                              [SUPPORTED]\r\n                      compute capability: 3.7\r\n                           pci device id: 0\r\n                              pci bus id: 0\r\nid 2            b'Tesla K80'                              [SUPPORTED]\r\n                      compute capability: 3.7\r\n                           pci device id: 0\r\n                              pci bus id: 0\r\nid 3            b'Tesla K80'                              [SUPPORTED]\r\n                      compute capability: 3.7\r\n                           pci device id: 0\r\n                              pci bus id: 0\r\nSummary:\r\n        4/4 devices are supported\r\nCUDA driver version                           : 10020\r\nCUDA libraries:\r\nFinding cublas from Conda environment\r\n        named  libcublas.so.10.2.2.89\r\n        trying to open library...       ok\r\nFinding cusparse from Conda environment\r\n        named  libcusparse.so.10.3.1.89\r\n        trying to open library...       ok\r\nFinding cufft from Conda environment\r\n        named  libcufft.so.10.1.2.89\r\n        trying to open library...       ok\r\nFinding curand from Conda environment\r\n        named  libcurand.so.10.1.2.89\r\n        trying to open library...       ok\r\nFinding nvvm from Conda environment\r\n        named  libnvvm.so.3.3.0\r\n        trying to open library...       ok\r\nFinding libdevice from Conda environment\r\n        searching for compute_20...     ok\r\n        searching for compute_30...     ok\r\n        searching for compute_35...     ok\r\n        searching for compute_50...     ok\r\n\r\n__ROC Information__\r\nROC available                                 : False\r\nError initialising ROC due to                 : No ROC toolchains found.\r\nNo HSA Agents found, encountered exception when searching:\r\nError at driver init:\r\nNUMBA_HSA_DRIVER /opt/rocm/lib/libhsa-runtime64.so is not a valid file path.  Note it must be a filepath of the .so/.dll/.dylib or the driver:\r\n\r\n__SVML Information__\r\nSVML state, config.USING_SVML                 : False\r\nSVML library found and loaded                 : False\r\nllvmlite using SVML patched LLVM              : True\r\nSVML operational                              : False\r\n\r\n__Threading Layer Information__\r\nTBB Threading layer available                 : True\r\nOpenMP Threading layer available              : False\r\n+--> Disabled due to                          : Unknown import problem.\r\nWorkqueue Threading layer available           : True\r\n\r\n__Numba Environment Variable Information__\r\nNone set.\r\n\r\n__Conda Information__\r\nconda_build_version                           : 3.18.9\r\nconda_env_version                             : 4.8.1\r\nplatform                                      : linux-64\r\npython_version                                : 3.7.4.final.0\r\nroot_writable                                 : True\r\n\r\n__Current Conda Env__\r\n_libgcc_mutex             0.1                        main\r\nabsl-py                   0.9.0                    pypi_0    pypi\r\nastor                     0.8.1                    pypi_0    pypi\r\nca-certificates           2019.11.27                    0    anaconda\r\ncertifi                   2019.11.28               py36_0    anaconda\r\ncudatoolkit               10.2.89              hfd86e86_0    anaconda\r\ncudnn                     7.6.5                cuda10.2_0    anaconda\r\ngast                      0.2.2                    pypi_0    pypi\r\ngoogle-pasta              0.1.8                    pypi_0    pypi\r\ngrpcio                    1.26.0                   pypi_0    pypi\r\nh5py                      2.10.0                   pypi_0    pypi\r\nkeras-applications        1.0.8                    pypi_0    pypi\r\nkeras-preprocessing       1.1.0                    pypi_0    pypi\r\nld_impl_linux-64          2.33.1               h53a641e_7\r\nlibedit                   3.1.20181209         hc058e9b_0\r\nlibffi                    3.2.1                hd88cf55_4\r\nlibgcc-ng                 9.1.0                hdf63c60_0\r\nlibstdcxx-ng              9.1.0                hdf63c60_0\r\nllvmlite                  0.31.0                   pypi_0    pypi\r\nmarkdown                  3.1.1                    pypi_0    pypi\r\nncurses                   6.1                  he6710b0_1\r\nnumba                     0.47.0                   pypi_0    pypi\r\nnumpy                     1.18.1                   pypi_0    pypi\r\nopenssl                   1.1.1                h7b6447c_0    anaconda\r\nopt-einsum                3.1.0                    pypi_0    pypi\r\npip                       19.3.1                   py36_0\r\nprotobuf                  3.11.2                   pypi_0    pypi\r\npython                    3.6.10               h0371630_0\r\nreadline                  7.0                  h7b6447c_5\r\nsetuptools                44.0.0                   py36_0\r\nsix                       1.14.0                   pypi_0    pypi\r\nsqlite                    3.30.1               h7b6447c_0\r\ntensorboard               1.15.0                   pypi_0    pypi\r\ntensorflow-estimator      1.15.1                   pypi_0    pypi\r\ntermcolor                 1.1.0                    pypi_0    pypi\r\ntk                        8.6.8                hbc83047_0\r\nwerkzeug                  0.16.0                   pypi_0    pypi\r\nwheel                     0.33.6                   py36_0\r\nwrapt                     1.11.2                   pypi_0    pypi\r\nxz                        5.2.4                h14c3975_4\r\nzlib                      1.2.11               h7b6447c_3\r\n--------------------------------------------------------------------------------`", "comments": ["@craterkamath, Looks like multiple cuDNN versions installed on instance.\r\nPlease follow the instructions mentioned [here](https://www.tensorflow.org/install/gpu).\r\nDownload the cuDNN 7.6.5 from nvidia website https://developer.nvidia.com/rdp/cudnn-archive \r\nand add it to your LD_LIBRARY_PATH.\r\nThanks!", "TF 1.15 was compiled against CUDA 10.0\r\n\r\nFor other versions of CUDA you have to compile from source.", "@craterkamath, Is this still an issue!", "@gadagashwini  The issue is resolved. I installed CUDA 9.x for which the GPU had support. I think I will close this now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35926\">No</a>\n", "Tesla K80 doesnt have support for cuda 10.2 is that what you mean?"]}, {"number": 35925, "title": "Dataset with Keras Functional Model: tuple index out of range uin steps_per_epoch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu, Newest\r\n- TensorFlow installed from (source or binary): **Pip (Binary)**\r\n- TensorFlow version (use command below): **2.1**\r\n- Python version: Python **3.7.6**\r\n- CUDA/cuDNN version: **10.2**\r\n- GPU model and memory: **GeForce 1070 8Gb**\r\n\r\n**Describe the current behavior**\r\nPassing in a dataset to model.fit from a model generated with tf.Keras layers results in **IndexError: tuple index out of range.** Error both with custom TFRecord dataset and datasets derived from tensorflow-datasets installed via pip. Looks like it is in the standardize_input_data function but since it is an instance of DatasetV2 it should not be hitting that if statement...\r\n\r\n**Describe the expected behavior**\r\nKeras models should accept tf DataSets.\r\n\r\n**Code to reproduce the issue**\r\n```Python\r\nfrom tensorflow.keras.layers import Dense, Embedding, Flatten, Lambda, Subtract, Input, Concatenate, Average, Reshape, GlobalAveragePooling1D, Dot, Dropout\r\nfrom tensorflow.keras.models import Model, Sequential\r\nfrom tensorflow.keras.utils import Sequence\r\nfrom tensorflow.keras import initializers\r\n\r\nimport tensorflow_datasets as tfds\r\ntfds.list_builders()\r\ndataset, info = tfds.load(\"mnist\", with_info=True)\r\ninputs = Input((28, 28, 1), name=\"image\")\r\nFirst = Dense(128, activation=\"relu\")\r\nSecond = Dropout(0.2)\r\nThird = Dense(10, activation=\"softmax\", name=\"label\")\r\n\r\nfirst = First(inputs)\r\nsecond = Second(first)\r\nthird = Third(second)\r\nmodel = Model(inputs=[inputs], outputs=[third])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(dataset['train'].batch(4096))\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,\r\n**kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    613       # This is the first call of __call__, so we have to initialize.\r\n    614       initializers = []\r\n--> 615       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    616     finally:\r\n    617       # At this point we know that the initialization is complete (or less\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    495     self._concrete_stateful_fn = (\r\n    496         self._stateful_fn._get_concrete_function_internal_garbage_collected( \r\n# pylint: disable=protected-access\r\n--> 497             *args, **kwds))\r\n    498 \r\n    499     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args,\r\n**kwargs)    2387       args, kwargs = None, None    2388     with self._lock:\r\n-> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)    2390     return graph_function    2391 \r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)    2701     2702       self._function_cache.missed.add(call_context_key)\r\n-> 2703       graph_function = self._create_graph_function(args, kwargs)    2704       self._function_cache.primary[cache_key] = graph_function    2705       return graph_function, args, kwargs\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)    2591             arg_names=arg_names,    2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2593             capture_by_value=self._capture_by_value),    2594         self._function_attributes,    2595         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    976                                           converted_func)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    438         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    441 \r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)\r\n     83     args = _prepare_feed_values(model, input_iterator, mode, strategy)\r\n     84     outputs = strategy.experimental_run_v2(\r\n---> 85         per_replica_function, args=args)\r\n     86     # Out of PerReplica outputs reduce or pick values to return.\r\n     87     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n    761       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n    762                                 convert_by_default=False)\r\n--> 763       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    764 \r\n    765   def reduce(self, reduce_op, value, axis):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)    1817       kwargs\r\n= {}    1818     with self._container_strategy().scope():\r\n-> 1819       return self._call_for_each_replica(fn, args, kwargs)    1820     1821   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)    2162         self._container_strategy(),    2163         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2164       return fn(*args, **kwargs)    2165     2166   def _reduce_to(self, reduce_op, value, destinations):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics, standalone)\r\n    414   x, y, sample_weights = model._standardize_user_data(\r\n    415       x, y, sample_weight=sample_weight, class_weight=class_weight,\r\n--> 416       extract_tensors_from_dataset=True)\r\n    417   batch_size = array_ops.shape(nest.flatten(x, expand_composites=True)[0])[0]\r\n    418   # If `model._distribution_strategy` is True, then we are in a replica context\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)    2381         is_dataset=is_dataset,   2382         class_weight=class_weight,\r\n-> 2383         batch_size=batch_size)    2384     2385   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)    2467           shapes=None,    2468           check_batch_axis=False,  # Don't enforce the batch size.\r\n-> 2469           exception_prefix='target')    2470     2471       # Generate sample-wise weight values given the `sample_weight` and\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    510                        'for each key in: ' + str(names))\r\n    511   elif isinstance(data, (list, tuple)):\r\n--> 512     if isinstance(data[0], (list, tuple)):\r\n    513       data = [np.asarray(d) for d in data]\r\n    514     elif len(names) == 1 and isinstance(data[0], (float, int)):\r\n\r\nIndexError: tuple index out of range\r\n```", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/2316d244e316227b4448556c96334c3f/35925.ipynb). Thanks!", "I don't think this feature is supposed to work in 2.0 but can't confirm. But here is the error from there:\r\n\r\n```\r\n   1/Unknown - 0s 26ms/step\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-14-5486743d1f94> in <module>\r\n      1 # Because it's confusing\r\n      2 # This fn is made for loss here...\r\n----> 3 model.fit(batched_dataset)\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    501       # This is the first call of __call__, so we have to initialize.\r\n    502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    504     finally:\r\n    505       # At this point we know that the initialization is complete (or less\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)\r\n     71     strategy = distribution_strategy_context.get_strategy()\r\n     72     outputs = strategy.experimental_run_v2(\r\n---> 73         per_replica_function, args=(model, x, y, sample_weights))\r\n     74     # Out of PerReplica outputs reduce or pick values to return.\r\n     75     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n    759                                 convert_by_default=False)\r\n--> 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    761 \r\n    762   def reduce(self, reduce_op, value, axis):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1785       kwargs = {}\r\n   1786     with self._container_strategy().scope():\r\n-> 1787       return self._call_for_each_replica(fn, args, kwargs)\r\n   1788 \r\n   1789   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   2130         self._container_strategy(),\r\n   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2132       return fn(*args, **kwargs)\r\n   2133 \r\n   2134   def _reduce_to(self, reduce_op, value, destinations):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n    262       y,\r\n    263       sample_weights=sample_weights,\r\n--> 264       output_loss_metrics=model._output_loss_metrics)\r\n    265 \r\n    266   if reset_metrics:\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)\r\n    309           sample_weights=sample_weights,\r\n    310           training=True,\r\n--> 311           output_loss_metrics=output_loss_metrics))\r\n    312   if not isinstance(outs, list):\r\n    313     outs = [outs]\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    250               output_loss_metrics=output_loss_metrics,\r\n    251               sample_weights=sample_weights,\r\n--> 252               training=training))\r\n    253       if total_loss is None:\r\n    254         raise ValueError('The model cannot be run '\r\n\r\n~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    164 \r\n    165         if hasattr(loss_fn, 'reduction'):\r\n--> 166           per_sample_losses = loss_fn.call(targets[i], outs[i])\r\n    167           weighted_losses = losses_utils.compute_weighted_loss(\r\n    168               per_sample_losses,\r\n\r\nIndexError: list index out of range\r\n```", "I tried changing my model to Sequential to circumvent this error, and the error still occurs. Has anyone found a way around this?", "@dendrondal The only way I've found around it is not using tf.data at all and using a plain Python generator. @amahendrakar Should I submit a PR for the tf.keras.model doc's since they say they support tf.data datasets but that is incorrect?\r\n", "Hi. I faced the same problem. Is there any solution or workaround?", "Perhaps this thread can help. \r\nhttps://github.com/tensorflow/datasets/issues/720", "I experienced the same problem when trying to train an autoencoder with only a single input. In my case the problem was solved my mapping the function \r\n\r\n```python\r\ndef autoencoder_sample(x):\r\n    return x, x\r\n```\r\nin a final step, even though the second example is basically ignored, keras `.fit` seems to expect an input and a label.\r\n\r\nIn the OP's example the issue might be solved with by using\r\n`dataset, info = tfds.load(\"mnist\", with_info=True,  as_supervised=True)`\r\nwhich leads the dataset to return tuples rather than dictionaries. \r\n\r\nHowever, it is clear that the current error message is cryptic and does not help the user to find a solution.\r\n\r\n", "I faced the same problem. Is there any solution or workaround update ?\r\n\r\n", "Also struggling with this! Any workarounds?", "No solution, I've fallen back to using python generators instead of trying to use the tf.dataset. In reference to an earlier comment (thanks for the advice though!) \r\n\r\nI was using this with custom code, following the tensorflow guide to creating datasets, but used the tfds lib here to make a really small working example so no need for extensive custom code in the issue.", "@jguhlin Hi, i faced the same issue a few days ago, could you please provide some info on how did you manage to use plain python generator function? Did you need to create your own batching mechanism and yield batches from the generator? thx.", "@FabHub Yes, I'm just using a generator and skipping the tf datasets integration. In the generator, I'm creating the batches and yielding those. Have to handle shuffling and the like manually.", "Ok, just wanted to make sure, whether there is not some more advanced solution :) , thanks a lot for quick reply.", "@FabHub Check the keras docs, it will tell you what to provide from the generator.\r\nhttps://keras.io/models/model/\r\nGood luck!", "any progress?", "I also have the same issue.\r\nIs there a solution to integrating the tf.data API and the tf.Dataset (with tfrecords) with fit function of tf.keras.Model ?", "@F-29 @MihailMihaylov97, I only used @jguhlins solution. I created simple data generator function that iterates over dataset, yields tuple (x, y) and created dataset like this: tf.data.Dataset.from_generator(generator_function ...). Unfortunately in my case, I had to create data that are in uniform shape and store them on a disk prior to train process, couldn't cut the samples in my case on-the-fly. I needed to do this in order to perform shuffling properly and also use full potential of dataset. Hope it helps.", "@FabHub thank you for the reply. I switched to tensorflow estimators, and it works just fine.", "Encountered the same issue with another tfds dataset. Adding `as_supervised=True` to `tfds.load()` solves the problem. Looks like this issue occurs when FeaturesDict is used.", "While trying to reproduce your issue in Tf Nightly, encountered different error, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/615f20eeb9e0058c0f2cfff1212e9fa8/35650.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35925\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35925\">No</a>\n"]}, {"number": 35924, "title": "[ROCm] Optimized training apply ops", "body": "This PR implements GPU kernels ApplyAdagrad, ApplyAdagradV2, ApplyAdadelta, ApplyRMSProp, and ApplyCenteredRMSProp for ROCm, and enables corresponding unit tests.", "comments": ["@ekuznetsov139 Can you please resolve conflicts? Thanks!", "gentle ping", "I'm not very familiar with ROCM implementations, @chsigg might know better.", "gentle ping", "@ekuznetsov139 Can you please check @cheshire's comments and keep us posted. Thanks!"]}, {"number": 35923, "title": "Here is a list of operators for which you will need custom implementations: BatchMatMul, Erf", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): tf-nightly 1.15\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n    input_arrays = [\"inputs/input_ids\", \"inputs/segment_ids\", \"inputs/input_mask\"]\r\n    output_arrays = [\"loss/prob\"]\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\n    # Weight quantization\r\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n    tflite_quant_model = converter.convert()\r\n    tflite_model = converter.convert()\r\n\r\n**The output from the converter invocation**\r\n\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, FULLY_CONNECTED, GATHER, MEAN, MUL, ONE_HOT, PACK, RESHAPE, RSQRT, SLICE, SOFTMAX, SQUARED_DIFFERENCE, SQUEEZE, STRIDED_SLICE, SUB, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul, Erf.\r\n```\r\n\r\nI got the error, then I add  \"converter.allow_custom_ops = True\".  It's success.\r\nBut when i use the .lite model to interpreter, I got the new error:\r\n\r\n```\r\nRuntimeError: Encountered unresolved custom op: Erf.Node number 191 (Erf) failed to prepare.\r\n```\r\n\r\nPlease help me! Thanks\r\n", "comments": ["You will need to implement `Erf` as a custom operator because it is not supported in TensorFlow Lite. There is some instruction [here](https://www.tensorflow.org/lite/guide/ops_custom).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Does BatMatMul is supported in TensorFlow Lite? I encountered this problem too using TF1.15", "Does **erf** is supported in TensorFlow Lite? I encountered this problem too using TF1.14", "Has any done any custom implementation of this: \r\nRuntimeError: Encountered unresolved custom op: Erf.Node number 97 (Erf) failed to prepare.", "Is there a specific reason the erf operator isn't supported or is it just because no one has contributed it?"]}, {"number": 35922, "title": "build failed at master branch", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master 2.1.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source):7.4.0\r\n- CUDA/cuDNN version: 10.2 / 7.6.5\r\n- GPU model and memory:\r\nGTX1080Ti GDDR5X 11GB\r\n\r\n\r\n**Describe the problem**\r\n \r\nBuild error\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \u2018-Wno-self-assign\u2019\r\nERROR: /home/wmind/repo/tensorflow/tensorflow/python/keras/api/BUILD:116:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\n2020-01-15 18:41:58.956645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\nTraceback (most recent call last):\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 776, in <module>\r\n    main()\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 772, in main\r\n    lazy_loading, args.use_relative_imports)\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 629, in create_api_files\r\n    compat_api_versions, lazy_loading, use_relative_imports)\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 503, in get_api_init_text\r\n    _, attr = tf_decorator.unwrap(attr)\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 219, in unwrap\r\n    elif _has_tf_decorator_attr(cur):\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 124, in _has_tf_decorator_attr\r\n    hasattr(obj, '_tf_decorator') and\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/home/wmind/anaconda3/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 28, in <module>\r\n    _wrap_py_utils = swig_import_helper()\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)\r\n  File \"/home/wmind/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/wmind/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/wmind/repo/tensorflow/tensorflow/tools/pip_package/BUILD:41:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nINFO: Elapsed time: 3197.242s, Critical Path: 395.57s\r\nINFO: 18184 processes: 18184 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n", "comments": ["no problem without tensorrt suppot ( ver 7.0 )", "@alanpurple, can you include ./configure output. Thanks!", "The is the same as https://github.com/tensorflow/tensorflow/issues/35115.", "@alanpurple, Did you get a chance to look at @freedomtan's comment. Thanks!", "The above problem has been found to compile the tensorflow version for Python 3.6. The following three files are logs containing the compilation process. You can derive the target version and environment from the name of the file:\r\n\r\n[tf2.1_py36_cuda10.0.zip (cuda 10.0.130, nccl 2.5.6, cudnn 7.6.5.32 tensorrt 7.0.0.11)](https://github.com/tensorflow/tensorflow/files/4137358/tf2.1_py36_cuda10.0.zip)\r\n[tf2.1_py36_cuda10.1.zip (cuda 10.1.243, nccl 2.5.6, cudnn 7.6.5.32 tensorrt 6.0.1.5)](https://github.com/tensorflow/tensorflow/files/4137359/tf2.1_py36_cuda10.1.zip)\r\n[tf2.1_py36_cuda10.2.zip (cuda 10.2.89, nccl 2.5.6, cudnn 7.6.5.32 tensorrt 7.0.0.11)](https://github.com/tensorflow/tensorflow/files/4137357/tf2.1_py36_cuda10.2.zip)\r\n\r\nThe compilation configuration is as follows:\r\n\r\nhttps://github.com/vistart/Dockerfiles/blob/build_tensorflow/build_tensorflow/.tf_configure.bazelrc.py27\r\nhttps://github.com/vistart/Dockerfiles/blob/build_tensorflow/build_tensorflow/.tf_configure.bazelrc.py36\r\nhttps://github.com/vistart/Dockerfiles/blob/build_tensorflow/build_tensorflow/.tf_configure.bazelrc.py37\r\n\r\nCurrently, tensorflow for python 2.7 compiles fine.\r\n\r\nEveryone can try the compilation process themselves by following the instructions in the link below:\r\n\r\nhttps://hub.docker.com/r/vistart/build_tensorflow", "It seems this is a python 2 vs. python 3 problem. I've tested python 3.[5-7]  environments, none of them works.", "@alanpurple, Is this still an issue!", "@gadagashwini \r\n\r\nI'll try again and let you know asap", "Stop sending this crap\n\nOn Mon, Feb 3, 2020, 12:34 AM Alan Anderson <notifications@github.com>\nwrote:\n\n> @gadagashwini <https://github.com/gadagashwini>\n>\n> I'll try again and let you know asap\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35922?email_source=notifications&email_token=AKXF6BRZZKI36CF4NBMAEZLRA7JJ7A5CNFSM4KHL7WR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKS5LRA#issuecomment-581293508>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKXF6BQP35HT6AYAAHOYAU3RA7JJ7ANCNFSM4KHL7WRQ>\n> .\n>\n", "@Lloyd57 \r\n\r\nI didn't send you anything\r\n\r\nIt's you who needs to stop commenting strange message", "I also tried to compile TF source code in Python 3.6 and 3.7, but also reported same errors. It seems that this version can not use tensorrt.", "ImportError: /home/phillweston/.cache/bazel/_bazel_phillweston/116338b0ad1de73f45727b0ef63c0bc9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/phillweston/git-repository/tensorflow/tensorflow/python/tools/BUILD:141:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)", "Please use ` ``` ` around computer generated blocks (errors, code, output)", "@gadagashwini \r\n\r\nsame error", "as I said in comments of #35115\r\nIt seems bazel 1.x + Python 3 is problematic when building TensorRT is enabled. Workarounds:\r\n\r\n1. use bazel 0.2.x, e.g., bazel 0.26.1, see comments in #35584\r\n2. use Python 2.7.x instead of Python 3", "@alanpurple, Is this still an issue!", "I'm observing something similar: \r\n```\r\nERROR: /workspace/tensorflow/tensorflow/python/keras/api/BUILD:13:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1) bash failed: error executing command \r\n...\r\nImportError: /home/default/.cache/bazel/_bazel_root/2c92b5569ddded7b3a6bd5e139451b60/sandbox/processwrapper-sandbox/14748/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN15stream_executor3gpu17ExtractGpuContextEPNS0_11GpuExecutorE\r\n```\r\nwith bazel 0.26.1 and python 3.6 ", "@gadagashwini \r\nyes, never been resolved\r\n\r\nalso failed yesterday", "successfully build an image with the latest master, closing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35922\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35922\">No</a>\n"]}, {"number": 35921, "title": "Conv2DTranspose shape gets None when exporting SavedModel", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF2.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.2/7.6\r\n- GPU model and memory: V100, 32Gb\r\n\r\n**Describe the current behavior**\r\nThe shape information after Conv2DTranspose layer is incomplete and the spatial dimensions are missing when exporting the saved model.\r\n\r\n**Describe the expected behavior**\r\nThe shape information should include the spatial dimensions when exporting the saved model.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndef _crop_and_concat(inputs, residual_input):\r\n  factor = inputs.get_shape().dims[1].value / residual_input.get_shape().dims[1].value\r\n  return tf.concat([inputs, tf.image.central_crop(residual_input, factor)], axis=-1)\r\n\r\nclass UNet(tf.keras.Model):\r\n  def __init__(self, name):\r\n    super(UNet, self).__init__(name)\r\n    self.conv1 = tf.keras.layers.Conv2D(filters=8,\r\n                                        kernel_size=(3, 3),\r\n                                        activation=tf.nn.relu)\r\n    self.conv2 = tf.keras.layers.Conv2D(filters=8,\r\n                                        kernel_size=(3, 3),\r\n                                        activation=tf.nn.relu)\r\n    self.maxpool = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\r\n                                             strides=2)\r\n    self.deconv = tf.keras.layers.Conv2DTranspose(filters=16,\r\n                                                  kernel_size=(2, 2),\r\n                                                  strides=(2, 2),\r\n                                                  padding='same',\r\n                                                  activation=tf.nn.relu)\r\n    self.conv3 = tf.keras.layers.Conv2D(filters=8,\r\n                                        kernel_size=(3, 3),\r\n                                        activation=tf.nn.relu)\r\n\r\n  @tf.function\r\n  def call(self, x):\r\n    print(\">>> Input Shape\", x.shape)\r\n    out = self.conv1(x)\r\n    print(\">>> conv1 Shape\", out.shape)\r\n    skip = self.conv2(out)\r\n    print(\">>> conv2 Shape\", skip.shape)\r\n    out = self.maxpool(skip)\r\n    print(\">>> maxpool Shape\", out.shape)\r\n    out = self.deconv(out)\r\n    # the deconv shape will be (None, None, None, 16) when exporting saved model\r\n    print(\">>> deconv Shape\", out.shape)\r\n    out = self.conv3(out)\r\n    out = _crop_and_concat(out, skip)\r\n\r\n    return out\r\n\r\n\r\nmodel = UNet(\"dummy\")\r\n\r\nres = model.predict(tf.ones((1, 400, 400, 1)))\r\n\r\nprint(\"Finish prediction\")\r\n\r\ntf.keras.models.save_model(model, \"/results/SavedModel\",\r\n    save_format=\"tf\", overwrite=True, include_optimizer=False)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n>>> Input Shape (None, 400, 400, 1)\r\n>>> conv1 Shape (None, 398, 398, 8)\r\n>>> conv2 Shape (None, 396, 396, 8)\r\n>>> maxpool Shape (None, 198, 198, 8)\r\n>>> deconv Shape (None, 396, 396, 16)\r\n>>> Input Shape (None, 400, 400, 1)\r\n>>> conv1 Shape (None, 398, 398, 8)\r\n>>> conv2 Shape (None, 396, 396, 8)\r\n>>> maxpool Shape (None, 198, 198, 8)\r\n>>> deconv Shape (None, 396, 396, 16)\r\nFinish prediction\r\n>>> Input Shape (None, 400, 400, 1)\r\n>>> conv1 Shape (None, 398, 398, 8)\r\n>>> conv2 Shape (None, 396, 396, 8)\r\n>>> maxpool Shape (None, 198, 198, 8)\r\n>>> deconv Shape (None, **None, None**, 16)\r\n... <Then the TF crashes and complains about the None value since we need it to compute the fraction in _crop_and_concat()> ...\r\n```\r\nThe above log shows that in the prediction, all shapes are correctly inferred. But when we are exporting the saved model: the shape becomes incomplete and the spatial info are lost only after the deconv (Conv2DTranspose) and all the other layers still looks fine with correct shape info. So, we have two questions:\r\n(1) Why do we need to calculate the shapes again when exporting the saved model?\r\n(2) Why is the spatial info lost only after deconv? And this one looks like a bug.\r\n\r\nFYI @nluehr  \r\n\r\n", "comments": ["Issue replicating for TF 2.1 and [tf-nightly.](https://colab.sandbox.google.com/gist/oanush/13d6928dcc1232b3a91217e3e28a679a/35921.ipynb)Thanks!", "Full error trace is as follows\r\n\r\n```\r\n>>> Input Shape (None, 400, 400, 1)\r\n>>> conv1 Shape (None, 398, 398, 8)\r\n>>> conv2 Shape (None, 396, 396, 8)\r\n>>> maxpool Shape (None, 198, 198, 8)\r\n>>> deconv Shape (None, 396, 396, 16)\r\n>>> Input Shape (None, 400, 400, 1)\r\n>>> conv1 Shape (None, 398, 398, 8)\r\n>>> conv2 Shape (None, 396, 396, 8)\r\n>>> maxpool Shape (None, 198, 198, 8)\r\n>>> deconv Shape (None, 396, 396, 16)\r\nFinish prediction\r\n>>> Input Shape (None, 400, 400, 1)\r\n>>> conv1 Shape (None, 398, 398, 8)\r\n>>> conv2 Shape (None, 396, 396, 8)\r\n>>> maxpool Shape (None, 198, 198, 8)\r\n>>> deconv Shape (None, None, None, 16)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-7f41394a0e0e> in <module>()\r\n     50 \r\n     51 tf.keras.models.save_model(model, \"/results/SavedModel\",\r\n---> 52     save_format=\"tf\", overwrite=True, include_optimizer=False)\r\n\r\n26 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nTypeError: in user code:\r\n\r\n    <ipython-input-2-7f41394a0e0e>:40 call  *\r\n        out = _crop_and_concat(out, skip)\r\n    <ipython-input-2-7f41394a0e0e>:4 _crop_and_concat  *\r\n        factor = inputs.get_shape().dims[1].value / residual_input.get_shape().dims[1].value\r\n\r\n    TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\r\n```", "@jvishnuvardhan Thx for the update. After reading the doc here https://www.tensorflow.org/guide/concrete_function,\r\n```\r\nNote: tf.saved_model retraces all concrete_functions when saving them. This is to ensure that the exported concrete functions capture changes in the environment on export (e.g. distribution strategy scope).\r\n```\r\nI think my first question in the description is answered. But the issue of missing spatial info after deconv is still on.", "@houtoms Saving a subclassed model is little different. Check for more details in [this response](https://github.com/tensorflow/tensorflow/issues/36362#issuecomment-581557975). In short, can you try saving the weights using `model.save_weights(\"weights.tf\", save_format='tf')` and then (1) load the model, (ii) run on some data, and (iii) load_weights. This approach is clearly mentioned in the TF website [here](https://www.tensorflow.org/guide/keras/save_and_serialize#saving_subclassed_models). Thanks!\r\n\r\nPlease let us know if you have any questions. Please close the issue if this resolved your issue. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/9dab35571b54def66aa84fa9cfb4081d/35921.ipynb) is the gist for our reference. Thanks!", "Thx. Closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35921\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35921\">No</a>\n"]}, {"number": 35920, "title": "TypeError: object of type 'NoneType' has no len()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): from pip (binary?)\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the current behavior and code to reproduce**\r\n\r\nI am running this code:\r\n\r\nhttps://gist.github.com/cossio/dc761b2731b1428267b65333dbd3f321\r\n\r\nto train an RBM by contrastive divergence. Briefly I am using the Keras sub-classing API and I view the multiple Monte Carlo samples as layers.\r\n\r\nHowever it produces the following error:\r\n\r\n```\r\nTypeError: object of type 'NoneType' has no len()\r\n```\r\n\r\nPlease find the full error message and stack-trace as a comment on the above gist link.", "comments": ["If I am reading your code correctly, you have: \r\n\r\n```\r\nclass CD(keras.Model):\r\n    def __init__(self, rbm, mcsteps = 5, **kwargs):\r\n        super().__init__(self, **kwargs)\r\n```\r\n\r\nWhereas the super() call should be formatted like so:\r\n`super(CD, self).__init__()`\r\n\r\nYou can of course add kwargs, if you wanted to.\r\n\r\nAs an example, see [this](https://www.tensorflow.org/tutorials/generative/cvae#network_architecture) Tensorflow tutorial.", "Thanks for taking a look. \r\n\r\nI get the same error using `super().__init__(args)` in all my custom classes.\r\n\r\nNote that `super(CD, self).__init__(args)` is equivalent to `super().__init__(args)`. See https://docs.python.org/3.7/library/functions.html#super. ", "@gadagashwini So you think this is a Tensorflow bug and not something wrong in my code? If you have suggestions for a workaround please let me know.", "@cossio Sorry for the delay in my response. I ran your code with `tf-nightly` and everything worked well. I can see the deprecation warning as you mentioned. Other than deprecation warning, I cannot reproduce the error you mentioned. May be the error might have been resolved with recent `tf-nightly`. Here is the full trace of the output.\r\n\r\n```\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11493376/11490434 [==============================] - 0s 0us/step\r\nEpoch 1/10\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:4291: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\r\nInstructions for updating:\r\nThis op will be removed after the deprecation date. Please switch to tf.sets.difference().\r\n6000/6000 [==============================] - 10s 2ms/step - loss: -13.3863\r\nEpoch 2/10\r\n6000/6000 [==============================] - 10s 2ms/step - loss: -8.2681\r\nEpoch 3/10\r\n6000/6000 [==============================] - 10s 2ms/step - loss: -5.7110\r\nEpoch 4/10\r\n6000/6000 [==============================] - 10s 2ms/step - loss: -4.1468\r\nEpoch 5/10\r\n6000/6000 [==============================] - 10s 2ms/step - loss: -3.1042\r\nEpoch 6/10\r\n6000/6000 [==============================] - 10s 2ms/step - loss: -2.4848\r\nEpoch 7/10\r\n6000/6000 [==============================] - 10s 2ms/step - loss: -1.8877\r\nEpoch 8/10\r\n6000/6000 [==============================] - 10s 2ms/step - loss: -1.5715\r\nEpoch 9/10\r\n6000/6000 [==============================] - 11s 2ms/step - loss: -1.2992\r\nEpoch 10/10\r\n6000/6000 [==============================] - 10s 2ms/step - loss: -1.0544\r\n<tensorflow.python.keras.callbacks.History at 0x7f9baa2238d0>\r\n```\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Great! Will this be available in the next minor version of tensorflow? When is it planned to be released? @jvishnuvardhan ", "@cossio This will be available in near future in `TF2.2` stable version. Currently TF team released `TF2.2.rc1` and in near future they may release `TF2.2` stable version. \r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan Well I cannot test for now because I only use TF stable. I will test when 2.2 is released. Thanks!", "I am closing this as this was resolved in `tf-nightly` and will be available in stable `TF2.2` which will be release in near future (currently TF2.2rc3 released more than a week ago). \r\n\r\nPlease feel free to reopen if this persists in `TF2.2` stable. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35920\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35920\">No</a>\n"]}, {"number": 35919, "title": "Use os._exit() instead of closing the session when TPU is preempted.", "body": "PiperOrigin-RevId: 274846825\r\nChange-Id: I55f26e7a5414f9d3feb97ee2775d376bdeab8d78", "comments": []}, {"number": 35918, "title": "Minor optimization", "body": "1. Remove the unused variable tensor_ids.\r\n2. Made the member_function const.", "comments": []}, {"number": 35917, "title": "Compilation of TF2.1 with CUDA and TensorRT fails on Linux ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): GCC 7.4.0\r\n- CUDA/cuDNN version: 10.1,10.2 CuDNN 7\r\n- GPU model and memory: NVidia GTX 1080 Ti\r\n- TensorRT: version 6 or 7.\r\n\r\n**Describe the current behavior**\r\nCompiling from source, as indicated in the official tensorflow manual, compilation fails, with the error listed in the log below.\r\n**Describe the expected behavior**\r\nCompilation should successfully complete\r\n\r\n**Code to reproduce the issue**\r\nNo code needed. After a checkout of the current v.2.1.0, configuration is carried out with no changes from default (other than enabling CUDA). \r\n\r\n**Other info / logs**\r\nLog: see attached log.txt\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/4068010/log.txt)\r\n\r\n                                                                                                                                                   \r\n", "comments": ["@feranick,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "Step to reproduce:\r\n`git clone https://github.com/tensorflow/tensorflow.git`\r\n`cd tensorflow`\r\n`git checkout v2.1.0`\r\n`./configure`\r\n(followed all the steps. Everything in default, besides activating CUDA support with TensorRT)\r\n`bazel build --config=opt --config=cuda --config=v2 --verbose_failures --verbose_explanations //tensorflow/tools/pip_package:build_pip_package`", "Update: Compilation is successful is TensorRT support is disabled (during the `./configure' step). \r\n\r\nHowever, tensorflow lite seems to have issues. When I run my code:\r\n\r\n   `converter = tf.lite.TFLiteConverter.from_keras_model(model)  \r\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.float32\r\n    converter.inference_output_type = tf.uint8\r\n    converter.representative_dataset = representative_dataset_gen\r\n    tflite_quant_model = converter.convert()`\r\n\r\nI get this error: \r\n` tflite_quant_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 469, in convert\r\n    self.experimental_new_quantizer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 240, in _calibrate_quantize_model\r\n    calibrate_quantize = _calibrator.Calibrator(result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 53, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/_tensorflow_lite_wrap_calibration_wrapper.so: undefined symbol: _ZTIN10tensorflow6DeviceE. `", "The issue, as hinted from the log, may be related to the fact that TF2.1.0 requires bazel > 0.27 (compared to TF2.0.0 which required version <=0.26.1). Starting from bazel 0.27 and later, the Python toolchain now enforces that targets analyzed as PY2 and PY3 run under a Python 2 and Python 3 interpreter, respectively. Since the issue at play here seems to be related with a TF2 wrapper for TensorRT which is a python2 program, while the rest of the compilation proceeds in Python 3 which seems no longer allowed with bazel 0.27 and later. Therefore, TF2.0.0 can be compiled, but not TF2.1.0.\r\n\r\nThe wrapper in question is tensorflow/python/keras/api:keras_python_api_gen_compat_v2", "I updated the bug report to highlight that the issues is present for any recent version of CUDA (10.1, 10.2).", "> I updated the bug report to highlight that the issues is present for any recent version of CUDA (10.1, 10.2).\r\n\r\nError(s) occur in the following environments:\r\n- Ubuntu 18.04.3 LTS x64\r\n- Python 3.6\r\n- CUDA 10.0/10.1/10.2, cudnn 7.6.5.32, nccl 2.5.6, TensorRT 6.0.1(cuda10.1)/7.0.0(cuda10.0/10.2)\r\n\r\n(The following three archives contain compilation logs. Please untar them to read)\r\n[tf2.1_py36_cuda10.0.tar.gz](https://github.com/tensorflow/tensorflow/files/4095816/tf2.1_py36_cuda10.0.tar.gz)\r\n[tf2.1_py36_cuda10.1.tar.gz](https://github.com/tensorflow/tensorflow/files/4095815/tf2.1_py36_cuda10.1.tar.gz)\r\n[tf2.1_py36_cuda10.2.tar.gz](https://github.com/tensorflow/tensorflow/files/4095817/tf2.1_py36_cuda10.2.tar.gz)\r\n\r\nIf compiled for Python 2.7, there will be no problems.", "I no longer have issues with TF 2.2. This bug should be closed as soon as TF 2.2.0 is stable.", "This is no longer an issue, so it should be closed. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35917\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35917\">No</a>\n"]}, {"number": 35916, "title": "Update version numbers for TensorFlow 2.0.1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 0 -> 0\nPatch: 0 -> 1\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.0.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh:52:2.0.0\ntensorflow/lite/experimental/micro/tools/make/third_party_downloads.inc:20:2.0.0\ntensorflow/lite/experimental/micro/tools/make/templates/tasks.json.mbed.tpl:4:2.\n0.0\ntensorflow/lite/experimental/micro/tools/make/templates/tasks.json.make.tpl:4:2.\n0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:8:\n2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:15\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:20\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:22\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:22\n:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:82:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:83:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:89:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:102:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:138:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/create_sine_model.ipynb:\n96:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:162:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:163:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:169:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:182:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:218:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:228:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:229:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:235:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:248:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:284:2.0.0\ntensorflow/tools/tensorflow_builder/compat_checker/compat_checker_test.py:33:2.0\n.0\ntensorflow/tools/tensorflow_builder/compat_checker/compat_checker_test.py:38:2.0\n.0\ntensorflow/tools/pip_package/setup.py:65:2.0.0\ntensorflow/tools/pip_package/setup.py:66:2.0.0\ntensorflow/tools/pip_package/setup.py:90:2.0.0\ntensorflow/tools/docs/generate2.py:141:2.0.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:46:2.0.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:47:2.0.0\ntensorflow/tools/ci_build/install/install_pip_packages.sh:57:2.0.0\ntensorflow/tools/ci_build/install/install_pip_packages.sh:58:2.0.0\ntensorflow/tools/ci_build/windows/gpu/pip/build_tf_windows.sh:116:2.0.0\ntensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh:116:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:647:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:647:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:648:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:649:2.0.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.0.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh:52:2.0.0\ntensorflow/lite/experimental/micro/tools/make/third_party_downloads.inc:20:2.0.0\ntensorflow/lite/experimental/micro/tools/make/templates/tasks.json.mbed.tpl:4:2.\n0.0\ntensorflow/lite/experimental/micro/tools/make/templates/tasks.json.make.tpl:4:2.\n0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:8:\n2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:15\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:20\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:22\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:22\n:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:82:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:83:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:89:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:102:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_vision/README.md:138:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/create_sine_model.ipynb:\n96:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:162:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:163:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:169:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:182:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:218:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:228:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:229:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:235:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:248:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:284:2.0.0\ntensorflow/tools/tensorflow_builder/compat_checker/compat_checker_test.py:33:2.0\n.0\ntensorflow/tools/tensorflow_builder/compat_checker/compat_checker_test.py:38:2.0\n.0\ntensorflow/tools/pip_package/setup.py:65:2.0.0\ntensorflow/tools/pip_package/setup.py:66:2.0.0\ntensorflow/tools/pip_package/setup.py:90:2.0.0\ntensorflow/tools/docs/generate2.py:141:2.0.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:46:2.0.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:47:2.0.0\ntensorflow/tools/ci_build/install/install_pip_packages.sh:57:2.0.0\ntensorflow/tools/ci_build/install/install_pip_packages.sh:58:2.0.0\ntensorflow/tools/ci_build/windows/gpu/pip/build_tf_windows.sh:116:2.0.0\ntensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh:116:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:647:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:647:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:648:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:649:2.0.0\n```", "comments": []}]