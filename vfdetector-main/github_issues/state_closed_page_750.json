[{"number": 31066, "title": "Update README.md", "body": "libtensorflow-core.a can only be compiled with r14b and below.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31066) for more info**.\n\n<!-- need_sender_cla -->", "@alokprasad Please sign CLA in order to proceed with next steps. Thank you!", "@alokprasad gentle ping to sign cla.", "I signed it!\r\n", "Issue raised \r\nhttps://github.com/tensorflow/tensorflow/issues/31064", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31066) for more info**.\n\n<!-- cla_yes -->", "Can one of the admins verify this patch?", "Closing as #31127 which copy-pasted this got merged"]}, {"number": 31065, "title": "Failed to clear function handle cache: Invalid argument: Failed to find FunctionLibraryRuntime", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory: Nvidia 1080 Ti Founder Edition 11GB\r\n\r\n**Describe the current behavior**\r\nAfter the program runs for a while, I receive the following error.\r\ntensorflow/core/framework/function_handle_cache.cc:30] Failed to clear function handle cache: Invalid argument: Failed to find FunctionLibraryRuntime for device /job:localhost/replica:0/task:0/device:CPU:0 when releasing multi-device function handle 1\r\n\r\n**Describe the expected behavior**\r\nShould not return this error\r\n\r\n**Code to reproduce the issue**\r\nI'm using tf_agents library\r\n\r\n```\r\ntf.compat.v1.enable_v2_behavior()\r\n\r\nnum_iterations = 2000 # @param\r\n\r\ninitial_collect_steps = 10 # @param\r\ncollect_steps_per_iteration = 1  # @param\r\nreplay_buffer_capacity = 100000  # @param\r\n\r\nfc_layer_params = (100,)\r\n\r\nbatch_size = 32  # @param\r\nlearning_rate = 1e-3  # @param\r\nlog_interval = 100  # @param\r\n\r\nnum_eval_episodes = 5  # @param\r\neval_interval = 1000  # @param\r\n\r\nenvironment = SupplyChainEnv2()\r\nenvironment2 = SupplyChainEnv2()\r\n\r\n\r\ntrain_env = tf_py_environment.TFPyEnvironment(environment)\r\neval_env = tf_py_environment.TFPyEnvironment(environment2)\r\n\r\n\r\nq_net = q_network.QNetwork(\r\n    train_env.observation_spec(),\r\n    train_env.action_spec(),\r\n    fc_layer_params=fc_layer_params)\r\n\r\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\r\n\r\ntrain_step_counter = tf.compat.v2.Variable(0)\r\n\r\ntf_agent = DqnAgent(\r\n    train_env.time_step_spec(),\r\n    train_env.action_spec(),\r\n    q_network=q_net,\r\n    optimizer=optimizer,\r\n    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\r\n    train_step_counter=train_step_counter)\r\ntf_agent.initialize()\r\n\r\neval_policy = tf_agent.policy\r\ncollect_policy = tf_agent.collect_policy\r\n\r\nrandom_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\r\n                                                train_env.action_spec())\r\n\r\n\r\ndef compute_avg_return(environment, policy, num_episodes=5):\r\n\r\n  total_return = 0.0\r\n  for _ in range(num_episodes):\r\n\r\n    time_step = environment.reset()\r\n    episode_return = 0.0\r\n\r\n    while not time_step.is_last():\r\n      action_step = policy.action(time_step)\r\n      time_step = environment.step(action_step.action)\r\n      episode_return += time_step.reward\r\n    total_return += episode_return\r\n    print(episode_return)\r\n\r\n  avg_return = total_return / num_episodes\r\n  return avg_return.numpy()[0]\r\n\r\n\r\ncompute_avg_return(eval_env, random_policy, num_eval_episodes)\r\n\r\n# Please also see the metrics module for standard implementations of different\r\n# metrics.\r\n\r\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n    data_spec=tf_agent.collect_data_spec,\r\n    batch_size=train_env.batch_size,\r\n    max_length=replay_buffer_capacity)\r\n\r\ndef collect_step(environment, policy):\r\n  time_step = environment.current_time_step()\r\n  action_step = policy.action(time_step)\r\n  next_time_step = environment.step(action_step.action)\r\n  traj = trajectory.from_transition(time_step, action_step, next_time_step)\r\n\r\n  # Add trajectory to the replay buffer\r\n  replay_buffer.add_batch(traj)\r\n\r\n\r\nfor _ in range(initial_collect_steps):\r\n  collect_step(train_env, random_policy)\r\n\r\n# This loop is so common in RL, that we provide standard implementations of\r\n# these. For more details see the drivers module.\r\n\r\n# Dataset generates trajectories with shape [Bx2x...]\r\ndataset = replay_buffer.as_dataset(\r\n    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\r\n\r\niterator = iter(dataset)\r\n\r\n# (Optional) Optimize by wrapping some of the code in a graph using TF function.\r\ntf_agent.train = common.function(tf_agent.train)\r\n\r\n# Reset the train step\r\ntf_agent.train_step_counter.assign(0)\r\n\r\n# Evaluate the agent's policy once before training.\r\neval_env.reset()\r\navg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\r\nreturns = [avg_return]\r\n\r\nfor _ in range(num_iterations):\r\n\r\n  # Collect a few steps using collect_policy and save to the replay buffer.\r\n  for _ in range(collect_steps_per_iteration):\r\n    collect_step(train_env, tf_agent.collect_policy)\r\n\r\n  # Sample a batch of data from the buffer and update the agent's network.\r\n  experience, unused_info = next(iterator)\r\n  train_loss = tf_agent.train(experience)\r\n\r\n  step = tf_agent.train_step_counter.numpy()\r\n\r\n  if step % log_interval == 0:\r\n    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\r\n\r\n  if step % eval_interval == 0:\r\n    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\r\n    print('step = {0}: Average Return = {1}'.format(step, avg_return))\r\n    returns.append(avg_return)\r\n```\r\n\r\n**Other info / logs**\r\nNone\r\n", "comments": ["@marconardelli ,\r\nHi when tried executing the given code `NameError: name 'SupplyChainEnv2' is not defined` was received, can you please provide the complete code.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 31064, "title": "Support for Latest NDK ( ndk14b above) for  tensorflow/contrib/makefile/Makefile", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:  No\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): CLANG\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:\r\n8GB\r\n\r\n\r\n**Describe the problem**\r\n tensorflow/contrib/makefile/Makefile is desingned to work with Android ndk14b only , None of the latest NDK can be used to compile libtensorflow-core.a for Android using latest ndk , Please modify the Makefile to support latest NDK \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please attach the build failure you're seeing and the NDK version you're using. TensorFlow Mobile builds are deprecated in favor of TensorFlow Lite; is there a reason you cannot use TensorFlow Lite? Or bazel builds for TensorFlow Mobile?", "Some of the model cannot be converted to TFLITE even enabling SELECT_TF_OPS flag also , So we need full tensorflow for Android ( i Understand it will be not optmized for speed) but still that will enable the model to run on Android. \r\nNDK Version 16 , there are changes in NDK sysroot and headers in NDK after 14b so current Makefile dont work with any NDK after 14b.\r\n\r\nI am following Android steps from below page to compile\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.15/tensorflow/contrib/makefile\r\n(Note : contrib has been removed from master after tensorflow 2.0)\r\n\r\nWill update the log with error what i am facing. ", "At this point, if you want to propose a fix, that would probably be the only way to fix this path for non-r14b builds.", "Yes this is still an issue.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31064\">No</a>\n"]}, {"number": 31063, "title": "[tflite] fix incorrect conv params", "body": "input shape's h,w is (2,4), and filter's is (2, 2)\r\nso stride_height must to be 0.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31063) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31063) for more info**.\n\n<!-- ok -->", "@jenselofsson Could you review it? thanks.", "Hi @petewarden,\r\nin the testcase, the input shape is {4, 2, 2, 4, 1}, so the height is 2, width is 4\r\nthe filter shape is {4, 3, 2, 2, 1}, so the height is 2, width is 2.\r\nso how to moving the filter windows when stride height is 2?\r\nin addition, according to output shape is {4, 2, 1, 2, 3}, (height=1, width =2), \r\nit looks like the filter windows only moves on width stride.\r\n\r\nI think maybe the testcase need to fixed if the height stride must large than 1.\r\nIs it?\r\n\r\nThanks.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/576977af5d3ccefe3d685889d9da133085187618/tensorflow/lite/experimental/micro/kernels/conv_test.cc#L210-L234\r\n\r\nPS. I met this issue when I changed conv op_resolver to call optimized_ops::Conv with enabling im2col.\r\n", "That is not what you want @zakk0610 . Strides are defined to down sample feature map, and it needs to have height 2 such that the input height 2 may down sample to output height 1.", "Can one of the admins verify this patch?", "@zakk0610  Could you please resolve the conflicts? Thanks!", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 31062, "title": "Why tensorflow r2.0 still load libcudart_static.a in cuda_configure.bzl ", "body": "In tensorflow r2.0, I still can see libcudart_static.a copied to local cuda repository in third_party/gpus/cuda_configure.bzl file, but looks like tensorflow using cuda shared object to access Nvidia GPUs, so why still we need load static library here? ", "comments": ["Any update?", "@chengleiwang ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information.", "My system is Ubuntu 18.04 64bit os.\r\nTensorFlow version is r2.0.\r\nCompiled and installed from source.\r\nI follow the TensorFlow document to build the tf. I am ooo at this moment.but I believe you can find the cuda build configuration in /third_part/gpus/cuda/ path. There are many library use libcudart_static.a as dependence.", "You can see the related code in /third_party/gpus/cuda_configure.bzl \r\n \"cudart_static\": _find_cuda_lib(\r\n            \"cudart_static\",\r\n            repository_ctx,\r\n            cpu_value,\r\n            cuda_config.config[\"cuda_library_dir\"],\r\n            cuda_config.cuda_version,\r\n            static = True,\r\n        ),\r\n\r\nBase on commit: 7c5aed3, tensorflow should use cuda dynamic library instead of static .", "@gunan , do you know the history of how tensorflow loading cuda library? what's the purpose for using libcuda_static.a before and now? ", "I don't think we are using those to build the pip package though. I believe those are there so that if users do want to use the static libraries they are available. @chsigg might also know more history on this.", "I agree with Yifei, the file is copied but should not be used during the actual build. I would be happy to merge a PR that removes the file copy.", "Actually I tried to remove the dependency for static libraries, but failed to build tensorflow after that. I will provide more detail fail message later.\r\n\r\nMay I know who can help provide a PR?", "I guess it is still needed by NCCL? See [here](https://github.com/tensorflow/tensorflow/blob/master/third_party/nccl/archive.BUILD#L95). \r\n\r\nNot sure if we could get rid of the static runtime library, though I'd really like to do so. Ping @chsigg to confirm. It was added in https://github.com/tensorflow/tensorflow/commit/cc3786bacbbc09f730a40915c0ec939953c141a7.", "I see the static runtime dependency has been removed in https://github.com/tensorflow/tensorflow/commit/f819114a2d9d393a60e954d3a3e42d8700ff3b19#diff-063e9e964d14400387e823d7ae9c4b2f\r\n\r\nClosing this. Let us know if you are still seeing issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31062\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31062\">No</a>\n"]}, {"number": 31061, "title": "tf.data.iterator make_initializer multiple creates new ops instead of using old", "body": "System information\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n    TensorFlow installed from (source or binary): binary\r\n    TensorFlow version (use command below): v1.13.1\r\n    Python version: 3.7.3\r\n    Bazel version (if compiling from source): NA\r\n    GCC/Compiler version (if compiling from source): NA\r\n    CUDA/cuDNN version: 9.2 / 7.6.0\r\n    GPU model and memory: GeForce GTX 1080 Ti\r\n\r\nDescribe the current behavior:\r\n   I'm using tf.data.dataset.Iterator.from_structure to make an iterator. And I defined a separate function to use iterator.make_initializer(dataset) to make new initializer after a certain time interval (because I'm doing experiment for growing dataset). The dataset is created using tf.data.Dataset.from_tensor_slices((imgs,labels)). But each time this function is called, it creates new ops causing the memory to grow. \r\n   I know i can use placeholder, but I want to use tf.data.dataset as it's faster. \r\n   I've tried using with tf.name_scope('dataset'): when creating the tf.data.Dataset (also tried tf.variable_scope). \r\n   But it simply avoid the name clashes by creating new op so instead of  dataset\\something it gives dataset_1\\something etc.  \r\n\r\n\r\nObserved behavior: new ops are created instead of reusing\r\n\r\nDescribe the expected behavior\r\nShould reuse the variables and not create new ops.", "comments": ["it seems like tf.data.dataset is having the same behaviour as tf.Variable() instead of tf.get_variable(). with tf.Variable(), even with tf.name_scope() or tf.variable_scope() it avoids the name clashes instead of reusing the variables.", "In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 31060, "title": "Fix build", "body": "", "comments": ["@mattn Could you please resolve the conflicts? Thanks!", "Can one of the admins verify this patch?", "Why no one look this?"]}, {"number": 31059, "title": "Copy missing header files", "body": "In pre-built TensorFlow packages for Windows systems, some header files are missing. These are needed for `include/tensorflow/c/c_api.h`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31059) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31059) for more info**.\n\n<!-- ok -->", "@sjamesr we may want to do this for all the other header files you added.", "@gunan @sjamesr I have added more header files.\r\n- `tensorflow/c/tf_attrtype.h`\r\n- `tensorflow/c/tf_datatype.h`\r\n- `tensorflow/c/tf_status.h`\r\n- `tensorflow/c/tf_tensor.h`\r\n\r\nIs this enough? Is `tensorflow/c/c_api_experimental.h` needed?", "I do not think we need c_api_experimental. That header is meant to be unstable, and should not be dedpended on by end users."]}, {"number": 31058, "title": "No Matching Distribution Found for Tensorflow", "body": "**System information**\r\n- Linux Ubuntu 18.04:\r\n- Nvidia Jetson Xavier\r\n- Python version: 3.6.8\r\n\r\n**Describe the problem**\r\n\r\n`pip install tensorflow` and `pip3 install tensorflow`\r\n\r\nreturns:\r\n\r\n`ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)`\r\n`ERROR: No matching distribution found for tensorflow`\r\n\r\nAnd trying to install the .whl file directly yields:\r\n\r\n`ERROR: tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl is not a supported wheel on this platform.`\r\n\r\nI can't understand why I'm getting these errors. My OS and specifications seem compatible.\r\n\r\n**Any other info / logs**\r\nhttps://imgur.com/XtowrBL", "comments": ["You have an ARM processor. PyPi seems to only have x86/amd64 wheels.", "@senorpinatta ,\r\nCan you please try the instructions mentioned in [this link ](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip) and let us know if it helps. Thanks.", "Hi @rmothukuru ,\r\nI googled solutions for this problem for many hours and tried many, many proposed solutions but none seemed to work.\r\n\r\nkung-foo seems correct in his statement that Tensorflow does not have any distribution for ARM/aarch64 processors.\r\n\r\nTo elaborate, I'm trying to install Tensorflow inside a docker container on my Nvidia Xavier device. \r\n\r\nI had hope [this](https://developer.codeplay.com/products/computecpp/ce/guides/tensorflow-guide/tensorflow-arm-setup?) and especially [this](https://www.tensorflow.org/lite/guide/build_arm64) guide would work (and it does when implemented directly on the device) but it does not work inside docker containers.\r\n\r\nIt returns:\r\n\r\n`Could not import tensorflow. Do not import tensorflow from its source directory change directory to outside the TensorFlow source tree, and relaunch your Python interpreter from there.`\r\n\r\nThen, when I move out of the directory where the tensorflow package is installed it can't find where tensorflow is.\r\n\r\nIt's kind of surprising to me if there's no support for tensorflow on an ARM device specifically geared towards AI/ML and is one of the most powerful devices out there for AI/ML.\r\n\r\nEdit: To clarify, Tensorflow is installed on the Xavier during its initial setup by Nvidia's own installer, but there's no support for using Tensorflow inside docker images which I don't think is an uncommon practice in this field.", "@senorpinatta Yes, you are correct -- the TensorFlow team does not currently offer pre-compiled ARM Python wheel binaries. However, you can compile the package yourself (for example, [here is a demonstration repository created by a TensorFlow community member](https://github.com/lhelontra/tensorflow-on-arm)).\r\n\r\nYou may also be interested in checking out [TensorFlow SIG Build](https://groups.google.com/a/tensorflow.org/forum/#!forum/build).", "Thanks for the link to the forum, I'll check it out.\r\nFor future readers, the example works for 32-bit architectures, not 64 as far as my attempts have gone.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31058)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31058)\r\n", "@senorpinatta  you saved me bro!!! I was 3 days trying to solve this problem, now I download Python for 64bits and I installed tensorflow!!\r\nTks man", "Nvidia has some pre-compiled versions of tf available on Jetson platform, works fine for me: https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html"]}, {"number": 31057, "title": "`tf.keras.Model.save` does not support subclassed model when saving model as SavedModel format", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NA\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190724\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`tf.keras.Model.save`  **DOES NOT** support subclassed model when saving model as SavedModel format\r\n**Describe the expected behavior**\r\n`tf.keras.Model.save` **SHOULD** support subclassed model when saving model as SavedModel format\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(2)\r\n\r\n    @tf.function\r\n    def call(self, x, training=True, mask=None):\r\n        return self.d(x)\r\n\r\n\r\nmodel = Model()\r\nmodel(tf.random.normal((2, 3)))\r\n# next line raises errors\r\nmodel.save(\"save/model\", save_format=\"tf\")\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I am able to reproduce the issue with TF version 2.0.0-dev20190724 on Colab. Please see the colab gist [here](https://colab.research.google.com/drive/1KQISq9_YVfIHV3S3OF5PTzk2pfdT4Uz5). \r\nThanks!", "@zakizhou The error `ValueError: Model <__main__.Model object at 0x7f7a6bad1e48> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).` clearly mention that `input_shape` is missing.\r\n\r\nPlease follow the steps mentioned in the [TF webiste](https://www.tensorflow.org/beta/guide/keras/saving_and_serializing#saving_subclassed_models) for `Saving Subclassed Models`. It clearly mentions that  \r\n\r\n```\r\nFirst of all, a subclassed model that has never been used cannot be saved.\r\n\r\nThat's because a subclassed model needs to be called on some data in order to create its weights.\r\n\r\nUntil the model has been called, it does not know the shape and dtype of the input data it should be expecting, and thus cannot create its weight variables. You may remember that in the Functional model from the first section, the shape and dtype of the inputs was specified in advance (via keras.Input(...)) -- that's why Functional models have a state as soon as they're instantiated.\r\n```\r\nAs you didn't run the your model, it does not know the shape and dtype of the input data it should be expecting, and thus cannot create its weight variables. \r\n\r\nI am closing the issue. Please feel free to open if you face any error after implementing your model following the steps outlined in TF website. There are couple of tutorials and guidelines on TF websites that will be helpful to you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31057\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31057\">No</a>\n", "hi @jvishnuvardhan ,\r\n> As you didn't run the your model\r\n\r\n> subclassed model needs to be called on some data in order to create its weights.\r\n\r\nBut  I indeed called my model, see \r\n```\r\nmodel = Model()\r\n# next line !\r\nmodel(tf.random.normal((2, 3)))\r\n```\r\n\r\nPlease reopen the issue as it is actually a bug.", "@zakizhou Please check the resource I provided in my last response. That resource has an example which is very useful. model compilation and fitting steps are missing your code. Please check the  resource for suggested way to save a subclass model. `model.save` is not correct approach. Please let me know if I am missing anything. Thanks!", "@jvishnuvardhan So do you mean that model.save can not be used if I'm writing custom training loop instead of compilation + fit? I should call tf.saved_model.save(model), is it right?", "I am having the same problem. I have defined my subclass custom model and its custom training loop. Furthermore, I am also not using .compile() nor .fit(), and I can't try to figure which saving method I could use.\r\n\r\nI have tested every single of them and could not find any way to solve it. Additionally, TF does not provide any documentation regarding saving a model with custom training (without using .compile() and .fit()).", "@gugarosa One workaround is that after finished custom training, call model.predict on some input tensors and then call model.save to save your model as SavedModel format.", "same problem here", "Same problem here too", "I am having the same problem on tensorflow 2.0", "@andreclaudino @aingo03304 @AlexFuster Can you please open a new issue with details of your model and share a simple standalone code to reproduce the issue. Thanks!", "I've been running OP's snippet and the bug is clearly reproducible. \r\nAs @jvishnuvardhan pointed, using _set_inputs does the work. However, I think it should not be necessary if the model is called and fed an input. In other words, the call function of Model should automatically call _set_inputs with the input received", "if you use model.predict(...) instead of model(...) it works.", "I can't save a custom model even though I successfully call model.predict(...) before the save. I get all kinds of error. My model is a seq2seq model. Why is tensorflow saying the subclassing keras.Model is the recommended low-level/flexible way when i is almost impossible to save it as a SavedModel?...", "@pinkponk Please create a new issue with standalone code and any other details to resolve your issue. Thanks! ", "Same problem, train with a customized loop. Can save ckpy without .meta. But can not get a frozen model. Does anyone solve it?", "> @gugarosa One workaround is that after finished custom training, call model.predict on some input tensors and then call model.save to save your model as SavedModel format.\r\n\r\ncould you give more details about the  'call model.predict on some input tensors'  deal? such as the code text?Thanks", "I followed this guide and it worked for me: https://www.tensorflow.org/guide/saved_model#exporting_custom_models\r\n\r\nIn other words, the below code should work. That said, I do agree that an explicit example for exporting a sub-classed keras model with a custom training loop is missing on the tf page.\r\n\r\n````\r\nimport tensorflow as tf\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(2)\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec([2, 3], tf.float32)])\r\n    def call(self, x, training=True, mask=None):\r\n        return self.d(x)\r\n\r\n\r\nmodel = Model()\r\nmodel(tf.random.normal((2, 3)))\r\n\r\ntf.saved_model.save(model, '/tmp/serving_path')\r\n````", "I have the same problem here and thanks @anoopkatti I get the pb model but I still don't know why. My model as follow:\r\n\r\n##python\r\n        layers = tf.keras.layers\r\n        input_feature = layers.Input(\r\n            shape=data_descriptions.sample_shape[\"input\"],\r\n            dtype=tf.float32,\r\n            name='input_feature'\r\n        )\r\n        inner = input_feature\r\n        #if self.hparams.down_sample > 1:\r\n        #    pass\r\n        for i in range(len(self.hparams.conv_filters)):\r\n            inner = layers.Conv2D(\r\n                filters=self.hparams.conv_filters[i],\r\n                kernel_size=(self.hparams.conv_kernel_size[i], self.hparams.conv_kernel_size[i]),\r\n                strides=(self.hparams.conv_frame_strides[i], self.hparams.conv_feat_deam_strides[i]),\r\n                padding=\"same\",\r\n                use_bias=False,\r\n                data_format=\"channels_last\",\r\n            )(inner)\r\n            inner = layers.BatchNormalization()(inner)\r\n            inner = tf.nn.relu6(inner)"]}, {"number": 31055, "title": "Saved model and serving preprocessing", "body": "Using: tensorflow2.0.0-beta1\r\nOS: macOS 10.14.6\r\npython: 3.7\r\n\r\nThis might be a docs or feature request - not sure, so please forgive my general posting of the issue.\r\n\r\nI used to use `estimator.export_savedmodel('export', serving_input_receiver_fn)` which would allow me to define `serving_input_receiver_fn ` where I could transform my inputs. Such transformations in my case were decode jpeg, convert to float, resize.\r\n\r\nWith Keras being the recommended high-level API with 2.0, I'm looking for information on how to do this. \r\nAn example model is:\r\n```python\r\nmobilenet_url = 'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4'\r\n\r\ninput = l.Input(shape=(224,224,3), name='input_image')\r\n\r\nmobilenet = hub.KerasLayer(mobilenet_url)(input)\r\nlogits = l.Dense(units=1, activation=tf.nn.sigmoid, name='prediction')(mobilenet)\r\n\r\nmodel = tf.keras.Model(inputs=input, outputs=logits)\r\n```\r\nTrain and then:\r\n```python\r\ntf.saved_model.save(model, 'export/mobilenet_finetuned')\r\n```\r\n\r\nCurrently the read, decode, resize is being done in `tf.data.Dataset` map functions. I can only see a way to provide `TensorSpec` describing the input, but no way to provide a preprocessing function when saving the model here for serving.\r\n\r\n- Is this a documentation issue? If so, guidance on how it's done would be nice and I'd be happy to contribute to the docs.\r\n- Is it not possible to do this preprocessing with the saved model with Keras and if I want to do this, stick to Estimators?", "comments": ["You can do this now by calling the model inside a function and exporting the function as a signature. There's some discussion of this in [the SavedModel RFC](https://github.com/tensorflow/community/blob/master/rfcs/20181116-saved-model.md#protos-in-serving-signatures-re-exporting). So the exported function takes whatever you'd like to pass when serving, transforms it to inputs the model takes, calls the model, and does any post-processing on the model's outputs.\r\n\r\nI think there was some thought around providing a higher-level Keras way to do this too. @k-w-w may know more about plans for that.", "Thank you for the insights @allenlavoie \r\n\r\nI'm now trying this \r\n```python\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\r\ndef serving(input_image):\r\n    def _input_to_feature(img):\r\n        img = tf.io.decode_jpeg(img, channels=3)\r\n        img = tf.image.convert_image_dtype(img, tf.float32)\r\n        return img\r\n    img = tf.map_fn(_input_to_feature, input_image)\r\n    img = tf.image.resize_with_pad(img, 224, 224)\r\n    return model.predict({ 'input_image': img })\r\n\r\ntf.saved_model.save(model, export_dir='export/transformed_for_serving', signatures=serving)\r\n```\r\n\r\nBut get the error: \r\n```\r\nin converted code:\r\n:11 predict  *\r\nimg = tf.map_fn(_input_to_feature, input_image)\r\n\r\nstack trace here:\r\n\r\nValueError: Trying to read from list with wrong element dtype. List has type uint8 but expected type string for 'map/TensorArrayV2Stack/TensorListStack' (op: 'TensorListStack') with input shapes: [], [3].\r\n```\r\n\r\nI've also tried wrapping this up in a `tf.Module` subclass that has the Keras model as a member property and then has a member function to predict but similar error.\r\n\r\nFor more context, this is the example I'm trying to set up: https://github.com/damienpontifex/mobilenet-classifier-transfer", "Yeah I think you just need to pass `dtype=` to `map_fn` to specify the output type. Otherwise it [defaults to the input type](https://www.tensorflow.org/api_docs/python/tf/map_fn#args).", "Thank you @allenlavoie \r\n\r\nI ended up with this which I verified using the `saved_model_cli run` command\r\n\r\n```python\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\r\ndef serving(input_image):\r\n\r\n    # Convert bytes of jpeg input to float32 tensor for model\r\n    def _input_to_feature(img):\r\n        img = tf.io.decode_jpeg(img, channels=3)\r\n        img = tf.image.convert_image_dtype(img, tf.float32)\r\n        img = tf.image.resize_with_pad(img, 224, 224)\r\n        return img\r\n    img = tf.map_fn(_input_to_feature, input_image, dtype=tf.float32)\r\n\r\n    # Predict\r\n    predictions = model(img)\r\n\r\n    with open('export/idx2class.pkl', 'rb') as f:\r\n        class_names = pickle.load(f)\r\n        class_names = tf.constant(class_names, dtype=tf.string)\r\n\r\n    # Single output for model so collapse final axis for vector output\r\n    predictions = tf.squeeze(predictions, axis=-1)\r\n\r\n    # Predictions are output from sigmoid so float32 in range 0 -> 1\r\n    # Round to integers for predicted class and string lookup for class name\r\n    prediction_integers = tf.cast(tf.math.round(predictions), tf.int32)\r\n    predicted_classes = tf.map_fn(lambda idx: class_names[idx], prediction_integers, dtype=tf.string)\r\n\r\n    # Convert sigmoid output for probability\r\n    # 1 (dog) will remain at logit output\r\n    # 0 (cat) will be 1.0 - logit to give probability\r\n    def to_probability(logit):\r\n        if logit < 0.5:\r\n            return 1.0 - logit\r\n        else:\r\n            return logit\r\n    class_probability = tf.map_fn(to_probability, predictions, dtype=tf.float32)\r\n\r\n    return {\r\n        'classes': predicted_classes,\r\n        'probabilities': class_probability\r\n    }\r\n\r\ntf.saved_model.save(model, export_dir='export/transformed_for_serving', signatures=serving)\r\n```\r\nAlso here https://github.com/damienpontifex/mobilenet-classifier-transfer/blob/master/binary_classifier_train.py#L129-L170\r\n\r\nIs there some documentation pieces I can update through a PR that might make this clearer. for others?", "Sure, if you have something you want to add you could append to the [SavedModel guide](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/saved_model.ipynb), one of the docstrings, or potentially the [Keras saving guide](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/keras/saving_and_serializing.ipynb). Right now we have a 1.x/2.x distinction which makes the 2.x guides somewhat hard to find, which will presumably be fixed once we release 2.x.", "@damienpontifex Is this issue resolved? Did you open any PR to update the docs as you mentioned? Please close the issue if it was resolved. Thanks!", "Thanks, yes it was resolved. I haven\u2019t yet opened the PR but will follow it up.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31055\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31055\">No</a>\n", "Just to share a tutorial in case someone wants to expand with a PR the Guides mentioned by @allenlavoie:\r\nhttps://sayak.dev/tf.keras/preprocessing/2020/04/13/embedding-image-preprocessing-functions.html", "> Thank you @allenlavoie\r\n> \r\n> I ended up with this which I verified using the `saved_model_cli run` command\r\n> \r\n> ```python\r\n> @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\r\n> def serving(input_image):\r\n> \r\n>     # Convert bytes of jpeg input to float32 tensor for model\r\n>     def _input_to_feature(img):\r\n>         img = tf.io.decode_jpeg(img, channels=3)\r\n>         img = tf.image.convert_image_dtype(img, tf.float32)\r\n>         img = tf.image.resize_with_pad(img, 224, 224)\r\n>         return img\r\n>     img = tf.map_fn(_input_to_feature, input_image, dtype=tf.float32)\r\n> \r\n>     # Predict\r\n>     predictions = model(img)\r\n> \r\n>     with open('export/idx2class.pkl', 'rb') as f:\r\n>         class_names = pickle.load(f)\r\n>         class_names = tf.constant(class_names, dtype=tf.string)\r\n> \r\n>     # Single output for model so collapse final axis for vector output\r\n>     predictions = tf.squeeze(predictions, axis=-1)\r\n> \r\n>     # Predictions are output from sigmoid so float32 in range 0 -> 1\r\n>     # Round to integers for predicted class and string lookup for class name\r\n>     prediction_integers = tf.cast(tf.math.round(predictions), tf.int32)\r\n>     predicted_classes = tf.map_fn(lambda idx: class_names[idx], prediction_integers, dtype=tf.string)\r\n> \r\n>     # Convert sigmoid output for probability\r\n>     # 1 (dog) will remain at logit output\r\n>     # 0 (cat) will be 1.0 - logit to give probability\r\n>     def to_probability(logit):\r\n>         if logit < 0.5:\r\n>             return 1.0 - logit\r\n>         else:\r\n>             return logit\r\n>     class_probability = tf.map_fn(to_probability, predictions, dtype=tf.float32)\r\n> \r\n>     return {\r\n>         'classes': predicted_classes,\r\n>         'probabilities': class_probability\r\n>     }\r\n> \r\n> tf.saved_model.save(model, export_dir='export/transformed_for_serving', signatures=serving)\r\n> ```\r\n> \r\n> Also here https://github.com/damienpontifex/mobilenet-classifier-transfer/blob/master/binary_classifier_train.py#L129-L170\r\n> \r\n> Is there some documentation pieces I can update through a PR that might make this clearer. for others?\r\n\r\nthank you very much. "]}, {"number": 31054, "title": "I0725 19:06:30.700653 7708 tf_logging.py:115] Saver not created because there are no variables in the graph to restore (When Training the tf estimator model )", "body": "```\r\nestimator = tf.contrib.estimator.DNNEstimator(\r\n    head=multi_label_head,\r\n    hidden_units=[64, 10],\r\n    feature_columns=[descriptions_embeddings])\r\n\r\nlabels = np.array(train_encoded)\r\nfeatures = {\r\n    'descriptions': np.array(train_descriptions)\r\n}\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    features,\r\n    labels,\r\n    shuffle=True,\r\n    batch_size=100,\r\n    num_epochs=20)\r\n\r\nestimator.train(input_fn=train_input_fn)\r\n```\r\n\r\n> INFO:tensorflow:Calling model_fn.\r\n> I0725 19:06:29.207903 7708 tf_logging.py:115] Calling model_fn.\r\n> INFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\n> I0725 19:06:30.700653 7708 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\r\n> INFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\n> I0725 19:06:31.963634 7708 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\r\n\r\n**_ValueError: Feature descriptions is not in features dictionary._**\r\n\r\noriginally defined at:\r\nFile \"C:\\Users\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py\", line 108, in dnn_logit_fn\r\nname='dnn')\r\nFile \"C:\\Users\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py\", line 143, in init\r\ncreate_scope_now=False)\r\nFile \"C:\\Users\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\", line 323, in init\r\nself._name, internal_input_layer, create_scope_now=create_scope_now)\r\nFile \"C:\\Users\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\template.py\", line 154, in make_template\r\n**kwargs)", "comments": ["@Neeraj0011 , Please provide details about TensorFlow version. Also, did you compile from source or install a binary?\r\nIn order to expedite the trouble-shooting process, please provide a complete code  to reproduce the issue reported here. Thanks!\r\n\r\n", "@gadagashwini \r\nTF Version : 1.12.0\r\nCompiled from Source:\r\n\r\n```\r\n`import numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nfrom sklearn.preprocessing import MultiLabelBinarizer\r\ndf = pd.read_csv(r\"C:\\Users\\xyz\\Downloads\\movies_data_en.csv\")\r\ndf.fillna('', inplace=True)\r\ndf.head(2)\r\n\r\nimport json\r\ndescriptions = df['overview'].values\r\ngenres = df['genres'].apply(lambda genre: list(map(lambda obj: obj['name'],json.loads(genre.replace('\\'', '\"'))))).values\r\n\r\ntrain_size = int(len(descriptions) * 0.8)\r\n\r\ntrain_descriptions = descriptions[:train_size]\r\ntrain_genres = genres[:train_size]\r\ntest_descriptions = descriptions[train_size:]\r\ntest_genres = genres[train_size:]\r\n\r\ndescriptions_embeddings = hub.text_embedding_column(\r\n    'descriptions',\r\n    module_spec='https://tfhub.dev/google/universal-sentence-encoder/2')\r\n\r\ntop_genres = ['Comedy', 'Thriller', 'Romance', 'Action', 'Horror', 'Crime', 'Documentary', 'Adventure', 'Science Fiction']\r\n[1, 0, 0, 0, 0, 0, 0, 1, 0]  # multi-hot label for a comedy and adventure movie\r\n\r\nencoder = MultiLabelBinarizer()\r\nencoder.fit_transform(train_genres)\r\ntrain_encoded = encoder.transform(train_genres)\r\ntest_encoded = encoder.transform(test_genres)\r\nnum_classes = len(encoder.classes_)\r\nnum_classes\r\nmulti_label_head = tf.contrib.estimator.multi_label_head(\r\n    num_classes,\r\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\r\nestimator = tf.contrib.estimator.DNNEstimator(\r\n    head=multi_label_head,\r\n    hidden_units=[64, 10],\r\n    feature_columns=[descriptions_embeddings])\r\n\r\nlabels = np.array(train_encoded)\r\nfeatures = {\r\n    'descriptions': np.array(train_descriptions)\r\n}\r\n\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    features,\r\n    labels,\r\n    shuffle=True,\r\n    batch_size=32,\r\n    num_epochs=5)\r\n\r\nestimator.train(input_fn=train_input_fn)\r\n\r\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    {'descriptions': np.array(test_descriptions).astype(np.str)},\r\n    test_encoded.astype(np.int32),\r\n    shuffle=False)\r\nestimator.evaluate(input_fn=eval_input_fn)`\r\n```", "@Neeraj0011 Could you please provide the sample data set to reproduce the issue. Thanks!", "@gadagashwini  Please find the dataset link \r\nhttps://drive.google.com/file/d/1PB7xnodZpT7EFcKq8d6vBmIR9EuczqAm/view?usp=sharing", "@Neeraj0011 Sorry for the delay in my response. I think this was resolved in `TF1.14.0` and `tf-nightly`. I could not reproduce the issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/9fe1b0df17e5135a36021a797edf3cc2/tf_31054.ipynb).\r\n\r\nI am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31054\">No</a>\n"]}, {"number": 31053, "title": "TFLite Interpreter, allocate_tensors() failed to prepare, not kTFLiteInt8/Uint8", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.6.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nThe graph only consist of ```tf.split()```, where I pin down to for this issue.\r\nAfter quantization with ```representative_dataset()```, the interpreter fail to `allocate_tensor()`.\r\n`RuntimeError: tensorflow/lite/kernels/dequantize.cc:62 op_context.input->type == kTFLiteUInt8 || op_context.input->type == kTFLiteInt8 was not true.Node number 0 (DEQUANTIZE) failed to prepare`.\r\nI assume ```tf.split()``` cannot be quantize, and nothing to be quantize actually, since if I add ```tf.lite.OpsSet.TFLITE_BUILTINS_INT8``` to fully quantize into INT8, it tell me SPLIT_V is not supported.\r\nSo this simple graph should not have any quantize/dequantize at tf.split ops?\r\nThus checking type if is Int8 and has dequantize layer here seems not to make sense?\r\n\r\n**Describe the expected behavior**\r\nInterpreter sucessfully\r\n\r\n**Code to reproduce the issue**\r\n```\r\ninputs_raw = tf.placeholder(tf.float32, shape=[1, 32, 32, 1], name='inputs_raw')\r\noutputs = tf.split(value = inputs_raw, num_or_size_splits = 2, axis = 1)[0]\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    converter = lite.TFLiteConverter.from_session(sess, [inputs_raw], [outputs])\r\n\r\n    converter.optimizations = [lite.Optimize.DEFAULT]\r\n    def representative_data_gen():\r\n        for i in range(1):\r\n            yield [np.random.random_sample((1, 32, 32, 1)).astype(np.float32)]\r\n            #yield [sess.run(tf.random.normal(inputs_raw.get_shape(), dtype = tf.float32, seed = 1))] # both have issue\r\n    converter.representative_dataset = representative_data_gen\r\n    tflite_model = converter.convert()\r\n    open(\"converted_model_quant_test.tflite\", \"wb\").write(tflite_model)\r\n\r\ninterpreter = lite.Interpreter(model_path = \"converted_model_quant_test.tflite\")\r\nnterpreter.allocate_tensors()\r\n```\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 532, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/proj/gpu_xxx/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 95, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/proj/gpu_xxx/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:62 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 was not true.Node number 0 (DEQUANTIZE) failed to prepare.\r\n\r\n", "comments": ["Same issue with TF 1.14.0 from PIP, Python 3.7.\r\n\r\nI have just installed tf-nightly-gpu and I cannot get that error with your script. However, my model (image to image) still fails to load with the same error (DEQUANTIZE).", "Jian, could you help take a look? Thanks!", "Having the same problem, under watching ... @jianlijianli ", "Having the same problem, under watching ,too ...", "Having the same problem, under watching ", "Same problem here. Using keras version of deeplabv3 with mobilenetv2 backbone, this error occurs on node 113, which is a dequantize node who is trying to convert the reduction index for a ReduceMax operation. \r\n\r\n`RuntimeError: tensorflow/lite/kernels/dequantize.cc:62 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 was not true.Node number 113 (DEQUANTIZE) failed to prepare.`\r\n\r\nHere is the visualization of node 113 in Netron. It has int32 as input type, that must be the problem.\r\nCould I just modify the graph to hard code a float32 there?\r\n<img width=\"473\" alt=\"dequantize_error\" src=\"https://user-images.githubusercontent.com/8995945/65073356-ffb4c380-d992-11e9-919a-4f4b044239be.PNG\">\r\n", "Similar issue", "same issue, watching", "similar issue", "same issue, watching", "same issue, watching", "> Same problem here. Using keras version of deeplabv3 with mobilenetv2 backbone, this error occurs on node 113, which is a dequantize node who is trying to convert the reduction index for a ReduceMax operation.\r\n> \r\n> `RuntimeError: tensorflow/lite/kernels/dequantize.cc:62 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 was not true.Node number 113 (DEQUANTIZE) failed to prepare.`\r\n> \r\n> Here is the visualization of node 113 in Netron. It has int32 as input type, that must be the problem.\r\n> Could I just modify the graph to hard code a float32 there?\r\n> <img alt=\"dequantize_error\" width=\"473\" src=\"https://user-images.githubusercontent.com/8995945/65073356-ffb4c380-d992-11e9-919a-4f4b044239be.PNG\">\r\n\r\nSimilar problem. Did you solve it?", "I'm having the same issue! Did anyone figure this out?", "I'm having the same issue! Kindly share the solution here.\r\n", "Any update on this? It's been a year.", "I got the same issue on `v1.14.0`, upgraded to `v1.15.0` fixed it. ", "@milinddeore Same for me! You might have to upgrade `pip` if you can\u2018t find `v1.15.0`.\r\nI believe this issue can be closed @jianlijianli ", "I upgrade my tensorflow v1.14.0 to v2.2.0  and it solved", "crashing in v2.3.0\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 353 (DEQUANTIZE) failed to prepare.", "tflite model \r\n[mb_melgan_quant.zip](https://github.com/tensorflow/tensorflow/files/5055739/mb_melgan_quant.zip)\r\n", "any update on this ? ", "Same issue here with tensorflow-cpu==2.3.0rc0", "@manmay-nakhashi I am also getting that similar issue on mb_melgan model itself. Are you still facing that issue, or were you able to resolve that somehow ? ", "> crashing in v2.3.0\r\n> RuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 353 (DEQUANTIZE) failed to prepare.\r\n\r\nCould you resolve this issue?", "Same issue here.", "Same issue \r\n\r\n[qat-mnist-2.zip](https://github.com/tensorflow/tensorflow/files/5591211/qat-mnist-2.zip)\r\n", "> Same issue here with tensorflow-cpu==2.3.0rc0\r\n\r\nMe too. Do you have solution for that problem? Please tell me the way. Thanks so much!", "Supported operations is failing I think. I finally left it like this and it's working:\r\n\r\ndef export_model_to_tflite(model, X_test):\r\n\r\n\t# Convert the model to the TensorFlow Lite format without quantization\r\n\tconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\ttflite_model = converter.convert()\r\n\r\n\t# Save the model to disk\r\n\topen(\"model.tflite\", \"wb\").write(tflite_model)\r\n\t\t\r\n\tdef representative_dataset_gen():\r\n\t\tfor sample in X_test:\r\n\t\t    sample = np.expand_dims(sample.astype(np.float32), axis=0)\r\n\t\t    yield [sample]\r\n\t\t\t\r\n\t# Convert the model to the TensorFlow Lite format with quantization\r\n\tconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\tconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\t#converter.target_spec.supported_types = [tf.float32] this fails if you uncomment\r\n\tconverter.representative_dataset = representative_dataset_gen\r\n\t\t\r\n\ttflite_model = converter.convert()\r\n\t\r\n\t# Save the model to disk\r\n\topen(\"model_quantized.tflite\", \"wb\").write(tflite_model)", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31053\">No</a>\n", "I'm still observed such issue while I'm using tensorflow==2.3.0rc0. \r\n\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 836 (DEQUANTIZE) failed to prepare.\r\n\r\n"]}, {"number": 31052, "title": "CUDA optimization: using read-only cache", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n@chsigg \r\n\r\nI wanted to propose adding `__restrict__` or `const __restrict__` tags on `input` and `output` arrays of ALL tensorflow kernels. But before I go on, I wanted to check a couple things:\r\n\r\n1. can I assume that input and output arrays given to a CUDA kernel do not overlap?\r\n2. Can I assume that input arrays are not modified, or would I have to check this per-kernel bases?\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\nSince pytorch arrays are stored in NHWC data layout, many kernels waste global memory read bandwidth. By adding `__restrict__` or `const __restrict__` tags, we can let the compiler cache the reads to read-only cache which displays about 50 ~ 100x faster clockspeed on memory reads.\r\n\r\n**Any Other info.**\r\n", "comments": ["@ThisIsIsaac ,\r\nIn order to answer your questions, \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@rmothukuru \r\n\r\nOperating system: all operating systems\r\n\r\narchitecture: any Nvidia GPU with compute capability over 3.0\r\n\r\nTensorflow version: NA\r\n\r\nI got response from @chsigg via email. Seems like I can assume the two ( any exceptions should be caught by pre-built unit tests ). I will go a head with a pull request with in a couple days.\r\n", "@ThisIsIsaac ,\r\nThank you for the details. Once you raise the pull request, can you please give reference of that Pull Request in this issue so that we can track it accordingly.", "Made a pull request [here](https://github.com/tensorflow/tensorflow/pull/31223)", "Can we close this?", "Closing this issue since the associated PR has been merged. Thanks!"]}, {"number": 31051, "title": "caching cuDNN CNN kernel choices", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nPytorch as a functionality to cache the chosen cuDNN kernels when the input dimensions are identical to previous calls:\r\n\r\ntorch.backend.cudnn.benchmark = True\r\nThis improves throughput of a network by 30~40% (again, when the input dimensions of the network do not change).\r\n\r\nHere is where the caching is implemented in Aten:\r\nhttps://github.com/pytorch/pytorch/blob/358fb51e773b8ad509ec270caee5ec1c51d82f38/aten/src/ATen/native/cudnn/Conv.cpp#L340\r\n\r\nDo you have any plans to add similar functionality? If not, I would love to send a PR.\r\n\r\n**Will this change the current api? How?**\r\n\r\nLike pytorch, it would require setting a flag.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAll forward passes that are performed with fixed input sizes and types\r\n\r\n**Any Other info.**\r\n", "comments": ["@timshen91 would you please help to take a look at this?\r\nAlso @houtoms for comments if any.", "Has TF already used similar mechanism https://github.com/tensorflow/tensorflow/blob/e3efabdcaf9359991527c4aec9544dbd1a61c67d/tensorflow/core/kernels/conv_ops.cc#L990?", "The similar caching mechanism is already implemented for all cuDNN convolutions: https://github.com/tensorflow/tensorflow/blob/e3efabdcaf9359991527c4aec9544dbd1a61c67d/tensorflow/core/kernels/conv_ops.cc#L990", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thanks for the catch! will close"]}, {"number": 31050, "title": "Can't set sprite in Keras Tensorboard Callback", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): maybe, but not at this moment\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThere is no way to specify a sprite for embedding visualizations with the Tensorboard Callback in Keras. \r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt would add two more parameters to `Tensorboard.__init__()`\r\n - `embeddings_image_path=None,`\r\n - `embeddings_image_size=None,`\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who wants to be able to see what their embedding clusters actually look like. NLP people are covered tho.\r\n\r\n**Any Other info.**\r\n\r\nHere's basically what you need. I just don't have the time to write unittests and docs. \r\n\r\n```python\r\nclass Tensorboard(Callback):\r\n\r\n    def __init__(self,\r\n                   ...\r\n                   embeddings_freq=0,\r\n                   embeddings_layer_names=None,\r\n                   embeddings_metadata=None,\r\n                   embeddings_data=None,\r\n\r\n                   ### add\r\n                   embeddings_image_path=None,\r\n                   embeddings_image_size=None,\r\n                   ...):\r\n\r\n        ### add \r\n        self.embeddings_image_path = embeddings_image_path\r\n        self.embeddings_image_size = embeddings_image_size\r\n\r\n\r\n    def set_model(self, model):\r\n        ...\r\n\r\n        ### add/rework a tad ~L282-L290\r\n        # allow embedding parameters to be passed as a dict(layer_name -> param)\r\n        # or as a single string that applies to all layers\r\n        layer_names = embeddings_vars.keys()\r\n\r\n        embeddings_metadata = (\r\n            {name: self.embeddings_metadata for name in layer_names}\r\n            if not isinstance(self.embeddings_metadata, dict) else data)\r\n            \r\n        embeddings_image_path = (\r\n            {name: self.embeddings_image_path for name in layer_names}\r\n            if not isinstance(self.embeddings_image_path, dict) else data)\r\n\r\n        embeddings_image_size = (\r\n            {name: self.embeddings_image_size for name in layer_names}\r\n            if not isinstance(self.embeddings_image_size, dict) else data)\r\n        ###\r\n        \r\n        try:\r\n            from tensorboard.plugins import projector\r\n        except ImportError:\r\n            raise ImportError('Failed to import TensorBoard. Please make sure that '\r\n                              'TensorBoard integration is complete.\"')\r\n\r\n        # TODO(psv): Add integration tests to test embedding visualization\r\n        # with TensorBoard callback. We are unable to write a unit test for this\r\n        # because TensorBoard dependency assumes TensorFlow package is installed.\r\n        config = projector.ProjectorConfig()\r\n        for layer_name, tensor in embeddings_vars.items():\r\n            embedding = config.embeddings.add()\r\n            embedding.tensor_name = tensor.name\r\n\r\n            if (embeddings_metadata is not None and\r\n                layer_name in embeddings_metadata):\r\n                embedding.metadata_path = embeddings_metadata[layer_name]\r\n\r\n            ### add \r\n            if (embeddings_image_path is not None and\r\n                embeddings_image_size is not None and\r\n                layer_name in embeddings_image_path and\r\n                layer_name in embeddings_image_size):\r\n\r\n                # this is what I need\r\n                embedding.sprite.image_path = embeddings_image_path[layer_name]\r\n                embedding.sprite.single_image_dim.extend(embeddings_image_size)\r\n            ###\r\n\r\n        projector.visualize_embeddings(self.writer, config)\r\n", "comments": ["@beasteers  As this is more related to TF Tensorboard, please post this in [TF Tensorboard repo](https://github.com/tensorflow/tensorboard). Thanks!", "But this isn't an issue with Tensorboard. It's an issue with Tensorflow's Keras Tensorboard callback. \r\n\r\nThe changes needs to happen in this repo i.e. `tensorflow/python/keras/callbacks.py`, so I feel like it is better placed as an issue here.\r\n\r\nIt's not a change that is heavily Tensorboard knowledge-specific. Here's the introductory [tutorial](http://www.pinchofintelligence.com/simple-introduction-to-tensorboard-embedding-visualisation/) I'm trying to reproduce: \r\n\r\n```python\r\nconfig = projector.ProjectorConfig()\r\nembedding = config.embeddings.add()\r\nembedding.tensor_name = embedding_var.name\r\nembedding.metadata_path = 'metadata.tsv'\r\n\r\n####\r\n# the Tensorboard Callback is just missing this bit.\r\nembedding.sprite.image_path = 'sprite.png'\r\nembedding.sprite.single_image_dim.extend([28,28])\r\n####\r\n\r\nprojector.visualize_embeddings(summary_writer, config)\r\n```\r\n\r\n", "I haven't been using tensorflow much lately, but it looks like it is still an unresolved problem.\r\n\r\nSee: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2238-L2270\r\n\r\nAnd notice that there is no mention of `embedding.sprite.image_path`", "@beasteers \r\nIt looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.4.1 or 2.5 and let us know if the issue still persists? \r\n\r\nCan you please open this issue on kears repo in case of any queries.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I don't have any code ready to test this but looking at the source code, there is still no way to set the sprite path or size here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2159-L2169\r\n\r\nAnd there is nothing that references `embedding.sprite.image_path` here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2284-L2316\r\n\r\nSo unless I'm mistaken, this issue is still unresolved and there is no way to provide the embedding sprites", "@beasteers As development of keras moved from TF repo to keras-team/keras repository,  I am closing this from TF repository. We will track this feature request in keras-team/keras\r\n\r\nhttps://github.com/keras-team/keras/issues/15573"]}, {"number": 31049, "title": "Extract all the layers included inner layers when doing \"load_weights\u2026", "body": "For example, a model included:\r\n      -Conv2D (keras.layer)\r\n      -BatchNorm (Keras.layer)\r\n      -CustomLayer (keras.layer)\r\n      -CustomModel (keras.Model)\r\n          -Conv2D (keras.layer)\r\n          -Conv2D (keras.layer)\r\n          -etc..\r\n      -Conv2D (keras.layer)\r\n\r\nThis function will extract all the layers included the layers in CustomModel or deeper. This is helpful when load a weights by name from functional model h5 file to subclass model.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31049) for more info**.\n\n<!-- need_sender_cla -->", "#29310 ", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31049) for more info**.\n\n<!-- ok -->", "@edwardyehuang Any update on reviewer comments? Please.", "> \r\n> \r\n> @edwardyehuang Any update on reviewer comments? Please.\r\n\r\nSorry, I am very busy recently. I will finish it by 6 sept", "Can one of the admins verify this patch?", "Hi, sorry. I am still working on it. Hopefully finish in this week. Very busy on August.", "@edwardyehuang Could you please resolve the conflicts? Thanks!", "> \r\n> \r\n> @edwardyehuang Could you please resolve the conflicts? Thanks!\r\n\r\nResolved", "@edwardyehuang Could you please resolve the conflicts? Thanks!", "> @edwardyehuang Could you please resolve the conflicts? Thanks!\r\n\r\nResolved", "@edwardyehuang Could you please address Ubuntu Sanity errors? Thanks!", "tensorflow/python/keras/engine/network.py:1202: [C0301(line-too-long), ] Line too long (116/80)\r\n\r\ntensorflow/python/keras/engine/network.py:1206: [C0326(bad-whitespace), ] No space allowed before bracket\r\n\r\nShould be fixed", "@tanzhenyu Can you please take a look on this PR? Thanks!", "@edwardyehuang Can you please check tanzhenyu's comments and keep us posted. Thanks!"]}, {"number": 31048, "title": "Fix: typo in document xla/g3doc/jit.md", "body": "", "comments": ["Maybe I misunderstood the context of this sentence.\r\nThanks for the advice."]}, {"number": 31047, "title": "moved slice by class id before shape matching", "body": "Addressing issue https://github.com/tensorflow/tensorflow/issues/30983\r\n\r\nIf you passed sample weights and a class id to a keras metric it would\r\nenforce that the predictions, labels and weights had the same shape\r\nbefore slicing the labels and predictions, resulting in a shape mismatch\r\nwhen trying to apply the weights later.", "comments": ["Can one of the admins verify this patch?", "Is there a test case that you can add to verify this change?", "Sure thing, I'll write one when I get home.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31047) for more info**.\n\n<!-- need_author_cla -->", "Whoops, added the test but pushed from the wrong account. so gonna roll it back and re-push. Sorry about that.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31047) for more info**.\n\n<!-- ok -->", "Not the best test I've ever written, but it'll fail if the sample weights aren't being broadcast (or rather not broadcast) correctly.", "@simian-typist Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 31046, "title": "[TF 2.0] tf.keras.applications.MobileNetV2 can't be converted to TFLite model", "body": "I convert MobileNetV2 to TFLite from `tf.keras.applications.MobileNetV2`:\r\n\r\n```\r\nmobile_net = MobileNetV2(\r\n                input_shape=(96, 96, 3), \r\n                weights='imagenet', \r\n                alpha=0.5, \r\n                include_top=False,\r\n                pooling='avg'\r\n)\r\n```\r\n\r\nI got the following error when try to convert to TFlite:\r\n\r\n```\r\nConverterError: TOCO failed. See console for info.\r\n2019-07-26 01:35:52.292264: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 393 operators, 739 arrays (0 quantized)\r\n2019-07-26 01:35:52.303848: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 393 operators, 739 arrays (0 quantized)\r\n2019-07-26 01:35:52.467866: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1086 operators, 2217 arrays (0 quantized)\r\n2019-07-26 01:35:52.490639: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1813] Check failed: axis < input_shape.dimensions_count() (1639659256 vs. 3)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f7f05021700 (most recent call first):\r\n  File \"/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n  File \"/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/otatanov/.conda/envs/project/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/otatanov/.conda/envs/project/bin/toco_from_protos\", line 10 in <module>\r\n```\r\n\r\nMaybe it is linked with [this issue](https://github.com/tensorflow/tensorflow/issues/22109).", "comments": ["It seems that it's not a MobileNet bug, I will close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31046\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31046\">No</a>\n"]}, {"number": 31045, "title": "Validate dict returned by input_map_fn for calibration", "body": "This avoids using dictionaries that don't have tf.Tensor as their values.", "comments": ["@pooyadavoodi Can you please check Ubuntu Sanity errors? Thanks!", "> @pooyadavoodi Can you please check Ubuntu Sanity errors? Thanks!\r\n\r\nThere were a few pylint errors which I fixed. Thanks @aaroey ", "@aaroey Looks like PR is ready for merge."]}, {"number": 31044, "title": "Refactor ExperimentalParallelInterleaveDatasetOp", "body": "This PR refactors `ExperimentalParallelInterleaveDataset` and adds the tests.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa The experimental namespace has been added by this commit (https://github.com/tensorflow/tensorflow/pull/31044/commits/c9623568d03e2a821ea2fdcf648cbccd5f16485e). Could you please take a look at the change?\r\n\r\nThe other experimental operations will be moved to the experimental namespace when refactoring them. ", "I was planning to add the experimental namespace when refactoring these operations. Your suggestion is good. Will add them now.", "@jsimsa The experimental namespace is added to all the .h and .cc files under the experimental directory (including the `sql` directory). Please take another look (https://github.com/tensorflow/tensorflow/pull/31044/commits/151a5b669e115da87581c2d7e5895f735d721329)!", "@jsimsa This commit (https://github.com/tensorflow/tensorflow/pull/31044/commits/f1e2779b366ee902897a912532fc1851362f4f5a) is submitted to resolve the conflicts with the commit (https://github.com/tensorflow/tensorflow/commit/1f8787342553fad12567cf1e0a1be68713c51363). Could you please have a look at the change?\r\n\r\nAs we add a new API `bool IsStateful()`, do we need to add the related tests for all datasets?", "Thanks @feihugis. Ideally yes, we would have C++ unit tests. My PR unfortunately did not add those because I am trying to fix something before 2.0 branch cut.", "@jsimsa Got it! I will add this C++ unit test for all the refactored datasets in another PR.", "Thank you!", "@jsimsa The internal checks failed. Could you please help check the logs and paste them here?", "The test failures seem unrelated. I will re-run the internal tests.", "Changes merged internally , waiting for auto-merge to happen."]}, {"number": 31043, "title": "Feature Request: Able to store weights to disk when not in use. For sparse inputs/data/training", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.0 Beta\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nAbility to store weights to disk when not in use, for sparse inputs/data/training.\r\n\r\nAn example of is training a very large number of item embeddings (such as word2vec). Say that the number of items(words) is very large, 7 or 8 figures. If trying to train this in Tensorflow, this would cause memory issues since there's a very large number of parameters loaded into memory at a time. For example, if there are 10 million items, and the embedding size is 256, then there are \r\n\r\n10 million * 256 embedding size = 256 million parameters\r\n\r\nWhich all loaded into memory at a time. However during each training step, only a small fraction of these parameters go through a forward or backwards pass. If we can somehow have the other parameters stored to disk when not in use, that would save on a lot of memory, and therefore greatly expand the number of parameters researchers can test with. \r\n\r\n**Will this change the current api? How?**\r\n\r\nI am not sure what is the best way to implement this. Perhaps a new datatype? Or the current datatypes can have a store option. \r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone working with sparse inputs, data, or training. \r\n\r\n**Any Other info.**\r\n\r\nThis may give some inspiration. Here is a repo that uses a sqlite database to store the vectors, so not all of them have to be loaded at once.\r\n\r\nhttps://github.com/plasticityai/magnitude\r\n\r\nI am thinking, a hacky way to go about this is to have a dummy embedding layer in Keras, which holds whatever parameters that will be used in the training step, and keep the rest in sqlite database. After each training step, the sqlite database values are exchanged with the ones in keras dummy layer, and the keras dummy layer values are replaced with the next values which will be used in the next forward pass. \r\n\r\nThough things will get a little more messy for optimizers with parameter-specific momentum, like Adagrad/Adam.", "comments": ["The way people often do this is via multiple parameter servers with embeddings sharded between them, which would be my suggestion. I don't think disks are generally low enough latency for this to make sense. Although you could just make swap really big and let the operating system swap out unused parameters; again I think that's generally too slow.", "Thanks for the suggestion, I'll look into parameter servers.\r\n\r\nI am wondering, if there's any scale where the setup I mentioned wouldn't cause a slowdown, while also reducing memory. Say that the swap would be big enough for 1,000 iterations. For a negative sampling rate of 16, left/right context of 3, and batch size of 512, that might go up to ~12 million embeddings loaded into memory. But if there are say 60 million total items, then the setup might be worth it, unless the big swap in itself caused a big slow down. Maybe the next big swap could be run in parallel during the training phrase, but then again that may have it's own memory issues. ", "Given your numbers, this sounds like less than 10 GiB of embeddings. Personally I'd just buy a stick or two of RAM and stay with a single machine if those are accurate.\r\n\r\nBut I guess you could put the embedding read as part of the data input pipeline, then like you say limit writes to some small slice of the embeddings. I don't think that's something amenable to wrapping into a high-level API, but it seems doable with existing low-level APIs.", "Great, thanks for the insights! ", "I'm attempting to make a prototype for simple-ish library for this. Let me know if you have an idea for the best data store for this. At the moment I am just attempting to do embeddings, so just a single row with simple index lookup. I've been asking around but getting a variety of recommendations: hdf5, feather, parquet, postgres, Cassandra, SQLite, and simple files.  "]}, {"number": 31042, "title": "moved slice by class id before shape matching", "body": "Addressing issue https://github.com/tensorflow/tensorflow/issues/30983\r\n\r\nIf you passed sample weights and a class id to a keras metric it would\r\nenforce that the predictions, labels and weights had the same shape\r\nbefore slicing the labels and predictions, resulting in a shape mismatch\r\nwhen trying to apply the weights later.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31042) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31042) for more info**.\n\n<!-- ok -->", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 31041, "title": "1.14 cherrypicks to fix the regression of TF-TRT precision_mode regression", "body": "There is a regression on TF-TRT precision_mode in 1.14 (vs 1.13), which cause lowercase strings (e.g. 'fp32') not recognized by the conversion tool. This PR fix the regression.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31041) for more info**.\n\n<!-- need_author_consent -->", "@pooyadavoodi please help to confirm with `@googlebot I consent` as shown above.", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31041) for more info**.\n\n<!-- ok -->"]}, {"number": 31040, "title": "TFLite conversion fails when using BatchNorm after Reshape (Check failed: dim_x == dim_y)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 & Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v1.12.1-6931-g2b5ece29d3 1.15.0-dev20190724**\r\n- Python version: **3.6.8**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\nTrying to convert a graph containing a reshape layer followed by batch normalization appears to trigger an incorrect op reordering. In particular, the toco converter relocates the mul operation from the BN layer to before the reshape layer, at which point the layers do not have compatible dimensions. This results in the `Check failed: dim_x == dim_y` error. \r\n\r\nFrom looking at the Graphviz video, this change is introduced in frame 38.\r\n\r\n**Describe the expected behavior**\r\nThe converter should not reorder operations across reshape if it would cause the dimensions to no longer match.\r\n\r\nIn addition, the check failure message should provide more information about the location of the error, such as the originating layer names (which are visible in the Graphviz outputs).\r\n\r\n**Code to reproduce the issue**\r\n```\r\n#!/usr/bin/env python3.6\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.layers as KL\r\nimport tensorflow.keras.backend as K\r\n\r\nkeras.backend.set_learning_phase(0)  # Build in test mode.\r\n\r\n# Construct a minimal graph with Reshape followed by BatchNormalization\r\ninput_tensor = KL.Input(shape=[10, 15, 512], name=\"input_tensor\")\r\nr = KL.Reshape(target_shape=[10, 30, 256], name=\"reshape\")(input_tensor)\r\noutput_bn = KL.BatchNormalization(axis=3, name=\"BN\")(r)\r\nkeras_model = keras.models.Model([input_tensor], [output_bn])\r\n\r\n# Convert the graph to TFLite.\r\nconverter = tf.lite.TFLiteConverter.from_session(K.get_session(), keras_model.inputs, keras_model.outputs)\r\n#converter.dump_graphviz_dir = \"./graphviz\"\r\n#converter.dump_graphviz_video = True\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**Other info / logs**\r\nThis occurs on all version of TF I've tested (1.13.1, 1.14.0, tf-nightly). It's possible that this sequence of operations just isn't supported, but this should be explicitly stated if so. The error message is quite vague and made the problematic sequence of ops extremely difficult to track down in a large graph.\r\n\r\nRelevant log messages:\r\n```\r\n2019-07-25 11:06:27.322822: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-07-25 11:06:27.360607: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-07-25 11:06:27.370031: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node BN/gamma/Assign doesn't exist in graph\r\nTraceback (most recent call last):\r\n  File \"./bug_report.py\", line 21, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"[venv]/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 983, in convert\r\n    **converter_kwargs)\r\n  File \"[venv]/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 438, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"[venv]/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 189, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-07-25 11:06:31.071452: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 12 operators, 24 arrays (0 quantized)\r\n2019-07-25 11:06:31.071874: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 12 operators, 24 arrays (0 quantized)\r\n2019-07-25 11:06:31.072261: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (0 quantized)\r\n2019-07-25 11:06:31.072428: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:118] Check failed: dim_x == dim_y (512 vs. 256)Dimensions must match\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f026b9a0740 (most recent call first):\r\n  File \"[venv]/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"[venv]/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"[venv]/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n  File \"[venv]/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"[venv]/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"[venv]/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n```\r\n", "comments": ["I also meet this problem when use dense\u2192reshape\u2192BN\u2192activation ,when swap the order of BN and reshape it will work well. It refer to [QAQ](https://stackoverflow.com/questions/56110234/dimensions-must-match-error-in-tflite-conversion-with-toco) But I don't want this.\r\nWin10\r\ntensorflow 1.13.1\r\nand I use 'tf.lite.TFLiteConverter.from_session' to convert.\r\nDo you have any other way to solve thie problem?", "Same here.\r\nI've a sort of Unet with embedding. After embedding (linear layer) I reshape the tensor to square (16x16x162) and proceed.\r\nI faced two problems, actually: one is addressed by https://github.com/tensorflow/tensorflow/issues/30124\r\nbut just changing the [None, 16, 16, 162] to [-1, 16, 16, 162] did the trick (as follows)\r\n\r\nx = tf.reshape(x, (-1, xsize[1], xsize[2], xsize[3]))\r\ninstead of \r\nx = tf.reshape(x, xsize) \r\n\r\nBut then I add a BNorm and the  \r\nCheck failed: dim_x == dim_y (162 vs. 41472)Dimensions must match\r\nthing happened.\r\n\r\nLooking at the numbers (162 and 41472 = 16x16x162) it seems that it has something to do with the bnorm probably acting before the reshape, not after, as stated in the issue.\r\nI tried also to disentangle the two adding some variant of\r\nx = x * 1\r\nx = x + 1 (and -1)\r\nbut no luck.\r\n\r\nOf course in TF everything is fine, this happens only during the conversion.", "hey,\r\n\r\nTOCO will be deprecated soon. Could you try use the new MLIR converter and see if issue still persist?\r\n\r\nTo use it, please download the latest tf-nightly or recent stable release of tf, and then add this to your code:\r\n`converter.experimental_new_converter = True`.\r\n\r\n(see user guide here: https://www.tensorflow.org/lite/convert/python_api)", "I just tried with tf2.1 (on windows), updated a few days ago.\r\nInserting the\r\n`converter.experimental_new_converter = True`\r\nthat error disappeared.\r\n\r\nNow I have other issues like\r\n`error: 'tf.ResizeBilinear' op is neither a custom op nor a flex op`\r\nwhich I believe depends on other things.\r\n\r\nWithout the \r\n`converter.experimental_new_converter = True`\r\nthe error is still there so it definitely makes a difference (but I'm still not able to convert my model :)).\r\n\r\n", "TF lite has ResizeBilinear op:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/resize_bilinear.cc\r\n\r\nI'm guessing that the TF 2.1 doesn't contain the relevant converter changes to support this op, could you try convert your model with tf-nightly? Thanks.", "Unfortunately I can't install the tf-nightly here, so I can't help. \r\nFor me the issue was solved just by adding this (found somewhere on the internet):\r\n```\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\nAnd now the model is converted. I have yet to check for correctness, but at least I have a float32 tflite model. Float16 has some other problems which i'll try to investigate later.", "I see.\r\n\r\ntf.lite.OpsSet.SELECT_TF_OPS tells the converter to use the TF ResizeBilinear op instead. So it can convert. But the binary size may grow a bit.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31040\">No</a>\n"]}, {"number": 31039, "title": "TensorRT Slowdown Native->FP32 and FP16->INT8; File Size Increase", "body": "## **System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, [nvcr.io/nvidia/tensorrt:19.02-py3](https://docs.nvidia.com/deeplearning/sdk/tensorrt-container-release-notes/rel_19-02.html#rel_19-02)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-7024-g24b3e6cf73 1.15.0-dev20190725\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0.130 / 7.4.2 as per the above linked container\r\n- GPU model and memory: Tesla V100 32GB\r\n\r\nRelevant output from `pip freeze`:\r\n```\r\ntf-estimator-nightly==1.14.0.dev2019072201\r\ntf-nightly-gpu==1.15.0.dev20190725\r\n```\r\n\r\n## **Describe the current behavior**\r\n\r\n**Inference Speed (frames per second)**\r\n\r\n| Model | Native | FP32 | FP16 | INT8 | \r\n| - | - | - | - | - |\r\n| tiny-yolo | 348 | 333 | 402 | 415 |\r\n| big-yolo | 125 | 140 | 243 | 208 |\r\n\r\n**1. Why is there a slowdown for tiny native->FP32?** (@pooyadavoodi same as [here](https://github.com/tensorflow/tensorflow/issues/30717#issuecomment-513938562))\r\n**2. Why is this slowdown not consistent for big?**\r\n**3. Why is there a slowdown for big FP16->INT8?**\r\n\r\n**Model Size (megabytes)**\r\n\r\n| Model | Native | FP32 | FP16 | INT8 | \r\n| - | - | - | - | - |\r\n| tiny-yolo | 35 | 67 | 44 | 51 |\r\n| big-yolo | 238 | 439 | 288 | 332 |\r\n\r\nI understand that there is currently an issue where new graph weights are saved twice to the .pb file (#30717, #30789). Once the weights in the table are adjusted for this double weight saving, the resulting sizes for fp32 and fp16 seem reasonable.\r\n\r\n**1. Why is there an increase in size for fp16->int8?**\r\n\r\n## **Describe the expected behavior**\r\n\r\nI am trying to quantize two different YOLO models (one tiny, one normal) with TensorRT. The goals of this quantization are:\r\n\r\n1. speed up inference\r\n2. decrease model size\r\n\r\nAs quantization and conversion proceeds from native->fp32->fp16->int8, I expect inference time to decrease (FPS to increase), and model size to decrease.\r\n\r\n## **Code to reproduce the issue**\r\n\r\nI am using [this](https://github.com/tensorflow/models/blob/master/research/tensorrt/tensorrt.py) script and a few helper functions from [here](https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/object_detection/graph_utils.py). The two exact scripts that I use are [do.py](https://github.com/tensorflow/tensorflow/files/3432597/do.py.txt) and [utilities.py](https://github.com/tensorflow/tensorflow/files/3432598/utilities.py.txt). Here are the [tiny model](https://drive.google.com/open?id=1Bgj9h6TJLwedtrhnritRs9eYm_iczG4v) and the [big model](https://drive.google.com/open?id=19BzNDCHpDnxj9wU26BsGM4ewS9bKNmpO).\r\n\r\nMy command for running the experiments:\r\n```bash\r\npython do.py \\\r\n--frozen_graph=big-yolov3_frozen.pb \\ # or tiny-yolov3_frozen.pb\r\n--native --fp32 --fp16 --int8 \\\r\n--batch_size=32 \\ # or 128 for tiny\r\n--output_dir=/workspace \\\r\n--input_node=inputs --output_node=output_boxes\r\n```\r\n\r\n\r\n## **Other info / logs**\r\n\r\nI ran a couple of experiments just to make sure that the results were consistent.\r\n\r\n```\r\n==========================\r\nnetwork: native_tiny-yolov3_frozen.pb,   batchsize 128, steps 100\r\n  fps   median: 350.2,  mean: 348.0,    uncertainty: 1.4,       jitter: 5.1\r\n  latency       median: 0.36551,        mean: 0.36846,  99th_p: 0.42946,        99th_uncertainty: 0.01453\r\n\r\n==========================\r\nnetwork: tftrt_fp32_tiny-yolov3_frozen.pb,       batchsize 128, steps 100\r\n  fps   median: 340.9,  mean: 333.4,    uncertainty: 1.5,       jitter: 4.4\r\n  latency       median: 0.37546,        mean: 0.38470,  99th_p: 0.47469,        99th_uncertainty: 0.06110\r\n\r\n==========================\r\nnetwork: tftrt_fp16_tiny-yolov3_frozen.pb,       batchsize 128, steps 100\r\n  fps   median: 403.3,  mean: 402.3,    uncertainty: 0.6,       jitter: 3.6\r\n  latency       median: 0.31740,        mean: 0.31824,  99th_p: 0.34266,        99th_uncertainty: 0.00263\r\n\r\n==========================\r\nnetwork: tftrt_int8_tiny-yolov3_frozen.pb,       batchsize 128, steps 100\r\n  fps   median: 417.7,  mean: 414.9,    uncertainty: 1.1,       jitter: 4.4\r\n  latency       median: 0.30641,        mean: 0.30873,  99th_p: 0.35451,        99th_uncertainty: 0.01511\r\n\r\n==========================\r\nnetwork: native_big-yolov3_frozen.pb,    batchsize 32, steps 100\r\n  fps   median: 125.2,  mean: 124.7,    uncertainty: 0.3,       jitter: 1.4\r\n  latency       median: 0.25553,        mean: 0.25677,  99th_p: 0.28257,        99th_uncertainty: 0.00308\r\n\r\n==========================\r\nnetwork: tftrt_fp32_big-yolov3_frozen.pb,        batchsize 32, steps 100\r\n  fps   median: 140.3,  mean: 140.2,    uncertainty: 0.4,       jitter: 1.9\r\n  latency       median: 0.22802,        mean: 0.22839,  99th_p: 0.25419,        99th_uncertainty: 0.00890\r\n\r\n==========================\r\nnetwork: tftrt_fp16_big-yolov3_frozen.pb,        batchsize 32, steps 100\r\n  fps   median: 237.6,  mean: 242.5,    uncertainty: 1.4,       jitter: 5.4\r\n  latency       median: 0.13469,        mean: 0.13245,  99th_p: 0.17733,        99th_uncertainty: 0.04387\r\n\r\n==========================\r\nnetwork: tftrt_int8_big-yolov3_frozen.pb,        batchsize 32, steps 100\r\n  fps   median: 210.1,  mean: 207.5,    uncertainty: 1.5,       jitter: 2.7\r\n  latency       median: 0.15231,        mean: 0.15657,  99th_p: 0.16928,        99th_uncertainty: 0.16613\r\n```\r\n\r\n```\r\n==========================\r\nnetwork: native_tiny-yolov3_frozen.pb,   batchsize 128, steps 100\r\n  fps   median: 357.4,  mean: 354.3,    uncertainty: 1.7,       jitter: 14.5\r\n  latency       median: 0.35814,        mean: 0.36215,  99th_p: 0.44575,        99th_uncertainty: 0.00629\r\n\r\n==========================\r\nnetwork: tftrt_fp32_tiny-yolov3_frozen.pb,       batchsize 128, steps 100\r\n  fps   median: 324.9,  mean: 319.4,    uncertainty: 1.4,       jitter: 2.5\r\n  latency       median: 0.39401,        mean: 0.40173,  99th_p: 0.49218,        99th_uncertainty: 0.06484\r\n\r\n==========================\r\nnetwork: tftrt_fp16_tiny-yolov3_frozen.pb,       batchsize 128, steps 100\r\n  fps   median: 376.1,  mean: 372.8,    uncertainty: 1.1,       jitter: 1.7\r\n  latency       median: 0.34036,        mean: 0.34363,  99th_p: 0.38601,        99th_uncertainty: 0.02051\r\n\r\n==========================\r\nnetwork: tftrt_int8_tiny-yolov3_frozen.pb,       batchsize 128, steps 100\r\n  fps   median: 392.1,  mean: 391.3,    uncertainty: 0.5,       jitter: 1.8\r\n  latency       median: 0.32645,        mean: 0.32717,  99th_p: 0.33765,        99th_uncertainty: 0.01967\r\n\r\n==========================\r\nnetwork: native_big-yolov3_frozen.pb,    batchsize 32, steps 100\r\n  fps   median: 124.3,  mean: 124.0,    uncertainty: 0.4,       jitter: 1.7\r\n  latency       median: 0.25737,        mean: 0.25842,  99th_p: 0.31292,        99th_uncertainty: 0.00355\r\n\r\n==========================\r\nnetwork: tftrt_fp32_big-yolov3_frozen.pb,        batchsize 32, steps 100\r\n  fps   median: 141.0,  mean: 140.7,    uncertainty: 0.3,       jitter: 0.3\r\n  latency       median: 0.22690,        mean: 0.22761,  99th_p: 0.24239,        99th_uncertainty: 0.00581\r\n\r\n==========================\r\nnetwork: tftrt_fp16_big-yolov3_frozen.pb,        batchsize 32, steps 100\r\n  fps   median: 247.4,  mean: 245.9,    uncertainty: 1.0,       jitter: 4.4\r\n  latency       median: 0.12934,        mean: 0.13044,  99th_p: 0.16018,        99th_uncertainty: 0.02011\r\n\r\n==========================\r\nnetwork: tftrt_int8_big-yolov3_frozen.pb,        batchsize 32, steps 100\r\n  fps   median: 206.0,  mean: 204.5,    uncertainty: 1.4,       jitter: 1.0\r\n  latency       median: 0.15536,        mean: 0.15885,  99th_p: 0.16454,        99th_uncertainty: 0.17423\r\n```", "comments": ["Hi @pooyadavoodi, would you help to take a look at this?", "@nseidl Is this a duplicate of https://github.com/tensorflow/tensorflow/issues/30717 ?", "> @nseidl Is this a duplicate of #30717 ?\r\n\r\nYes. Would you prefer to continue the thread in this issue or in #30717? I'd prefer to use this one (#31039)", "From the default nvidia19.02 container, I had to install the cuDNN v7.6.0 because `tf-nightly-gpu==1.15.0.dev20190801` complained about the default installed cuDNN v7.4.2 not being new enough, to match the v7.6.0 that the `tf-nightly-gpu` was built with.\r\n\r\nCUDA 10.0, cuDNN 7.6.0, Nvidia Driver 410.104, Python 3.5.2\r\n\r\n```\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.12.1-7676-g4157717 1.15.0-dev20190801\r\n```\r\n\r\nProceeding with `tf-nightly-gpu==1.15.0.dev20190801`, I found there to be negligible fps increase.\r\n\r\n```\r\npython do.py --frozen_graph=tiny-yolov3_frozen.pb --native --fp32 --fp16 --output_dir=/workspace --input_node=inputs --output_node=output_boxes\r\n\r\npython do.py --frozen_graph=big-yolov3_frozen.pb --native --fp32 --fp16 --output_dir=/workspace --input_node=inputs --output_node=output_boxes\r\n```\r\n\r\n```\r\n==========================\r\nnetwork: native_tiny-yolov3_frozen.pb,   batchsize 128, steps 100\r\n  fps   median: 862.1,  mean: 841.4,    uncertainty: 6.7,       jitter: 91.7\r\n  latency       median: 0.14848,        mean: 0.15314,  99th_p: 0.19557,        99th_uncertainty: 0.00155\r\n\r\n==========================\r\nnetwork: tftrt_fp32_tiny-yolov3_frozen.pb,       batchsize 128, steps 100\r\n  fps   median: 888.7,  mean: 865.6,    uncertainty: 7.8,       jitter: 55.0\r\n  latency       median: 0.14408,        mean: 0.14938,  99th_p: 0.19656,        99th_uncertainty: 0.02804\r\n\r\n==========================\r\nnetwork: tftrt_fp16_tiny-yolov3_frozen.pb,       batchsize 128, steps 100\r\n  fps   median: 864.9,  mean: 845.8,    uncertainty: 4.5,       jitter: 7.2\r\n  latency       median: 0.14799,        mean: 0.15179,  99th_p: 0.17967,        99th_uncertainty: 0.01009\r\n\r\n==========================\r\nnetwork: native_big-yolov3_frozen.pb,    batchsize 128, steps 100\r\n  fps   median: 161.4,  mean: 160.2,    uncertainty: 0.4,       jitter: 0.2\r\n  latency       median: 0.79308,        mean: 0.79935,  99th_p: 0.85455,        99th_uncertainty: 0.07410\r\n\r\n==========================\r\nnetwork: tftrt_fp32_big-yolov3_frozen.pb,        batchsize 128, steps 100\r\n  fps   median: 163.7,  mean: 162.7,    uncertainty: 0.4,       jitter: 0.3\r\n  latency       median: 0.78213,        mean: 0.78730,  99th_p: 0.89013,        99th_uncertainty: 0.08374\r\n\r\n==========================\r\nnetwork: tftrt_fp16_big-yolov3_frozen.pb,        batchsize 128, steps 100\r\n  fps   median: 166.3,  mean: 166.2,    uncertainty: 0.1,       jitter: 0.2\r\n  latency       median: 0.76969,        mean: 0.77026,  99th_p: 0.79509,        99th_uncertainty: 0.00254\r\n```\r\n\r\nSo, I then ran just FP16 and recorded the output. It looks like there's a problem converting `LeakyRelu`, and this causes some TensorRT nodes to not be created. This would explain the negligible FPS increase, I think.\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0802 04:02:24.227455 140013164369664 module_wrapper.py:136] From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n\r\nW0802 04:02:25.165457 140013164369664 deprecation.py:323] From do.py:67: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.gfile.GFile.\r\nW0802 04:02:25.165779 140013164369664 module_wrapper.py:136] From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\r\n\r\n2019-08-02 04:02:25.273736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.5\r\nI0802 04:02:25.273825 140013164369664 trt_convert.py:195] Linked TensorRT version: (5, 1, 5)\r\nI0802 04:02:25.273978 140013164369664 trt_convert.py:196] Loaded TensorRT version: (5, 0, 2)\r\nW0802 04:02:25.274046 140013164369664 trt_convert.py:212] TensorRT mismatch. Compiled against version 5.1.5, but loaded 5.0.2. Things may not work\r\n2019-08-02 04:02:25.527247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-08-02 04:02:26.502815: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-08-02 04:02:26.502918: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-08-02 04:02:26.503307: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-08-02 04:02:26.517468: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\r\n2019-08-02 04:02:26.521001: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5df5f50 executing computations on platform Host. Devices:\r\n2019-08-02 04:02:26.521030: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-08-02 04:02:26.911246: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e59340 executing computations on platform CUDA. Devices:\r\n2019-08-02 04:02:26.911319: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\r\n2019-08-02 04:02:26.912403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\r\npciBusID: 0000:3b:00.0\r\n2019-08-02 04:02:26.912458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-02 04:02:26.912480: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-02 04:02:26.913735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-02 04:02:26.913986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-02 04:02:26.915425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-02 04:02:26.916453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-02 04:02:26.916493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-02 04:02:26.917990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-02 04:02:26.918019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-02 04:02:27.083314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-02 04:02:27.083374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-08-02 04:02:27.083383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-08-02 04:02:27.085486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30420 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)\r\n2019-08-02 04:02:27.434240: I tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:213] Specified max_batch_size=128 is larger than maximum batch dimension of inputs (1). This can result in poor performance.\r\n2019-08-02 04:02:27.524490: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 9 ops of 6 different types in the graph that are not converted to TensorRT: ResizeNearestNeighbor, ConcatV2, SplitV, Conv2D, NoOp, Placeholder, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).\r\n2019-08-02 04:02:27.528748: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:633] Number of TensorRT candidate segments: 6\r\n2019-08-02 04:02:27.587194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.5\r\n2019-08-02 04:02:27.611198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.5\r\n2019-08-02 04:02:27.611805: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Parameter check failed at: ../builder/Network.cpp::addActivation::89, condition: int(type) >= 0 && int(type) < EnumMax<ActivationType>()\r\n2019-08-02 04:02:27.611849: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:737] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 82 nodes failed: Internal: TFTRT::ConvertLeakyRelu:2886 failed to add TRT layer, at: detector/tiny-yolo/Conv/LeakyRelu. Fallback to TF...\r\n2019-08-02 04:02:29.753906: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node TRTEngineOp_1 added for segment 1 consisting of 4 nodes succeeded.\r\n2019-08-02 04:02:29.881641: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node TRTEngineOp_2 added for segment 2 consisting of 4 nodes succeeded.\r\n2019-08-02 04:02:29.886954: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Parameter check failed at: ../builder/Network.cpp::addActivation::89, condition: int(type) >= 0 && int(type) < EnumMax<ActivationType>()\r\n2019-08-02 04:02:29.887002: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:737] TensorRT node detector/tiny-yolo/TRTEngineOp_3 added for segment 3 consisting of 54 nodes failed: Internal: TFTRT::ConvertLeakyRelu:2886 failed to add TRT layer, at: detector/tiny-yolo/Conv_6/LeakyRelu. Fallback to TF...\r\n2019-08-02 04:02:29.894176: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Parameter check failed at: ../builder/Network.cpp::addActivation::89, condition: int(type) >= 0 && int(type) < EnumMax<ActivationType>()\r\n2019-08-02 04:02:29.894212: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:737] TensorRT node detector/tiny-yolo/TRTEngineOp_4 added for segment 4 consisting of 25 nodes failed: Internal: TFTRT::ConvertLeakyRelu:2886 failed to add TRT layer, at: detector/tiny-yolo/Conv_11/LeakyRelu. Fallback to TF...\r\n2019-08-02 04:02:30.350996: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node detector/tiny-yolo/TRTEngineOp_5 added for segment 5 consisting of 29 nodes succeeded.\r\n2019-08-02 04:02:30.508788: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:183] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2019-08-02 04:02:30.524988: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:183] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2019-08-02 04:02:30.540291: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:183] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2019-08-02 04:02:30.552068: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: tf_graph\r\n2019-08-02 04:02:30.552096: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 171 nodes (-129), 186 edges (-130), time = 142.977ms.\r\n2019-08-02 04:02:30.552103: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 223 nodes (52), 238 edges (52), time = 53.212ms.\r\n2019-08-02 04:02:30.552121: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 223 nodes (0), 238 edges (0), time = 40.421ms.\r\n2019-08-02 04:02:30.552131: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 189 nodes (-34), 198 edges (-40), time = 2938.43091ms.\r\n2019-08-02 04:02:30.552138: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 181 nodes (-8), 194 edges (-4), time = 113.943ms.\r\n2019-08-02 04:02:30.552175: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: detector/tiny-yolo/TRTEngineOp_5_native_segment\r\n2019-08-02 04:02:30.552182: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 38 nodes (0), 37 edges (0), time = 2.783ms.\r\n2019-08-02 04:02:30.552188: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 38 nodes (0), 37 edges (0), time = 0.842ms.\r\n2019-08-02 04:02:30.552194: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 38 nodes (0), 37 edges (0), time = 2.768ms.\r\n2019-08-02 04:02:30.552200: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 38 nodes (0), 37 edges (0), time = 0.126ms.\r\n2019-08-02 04:02:30.552206: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 38 nodes (0), 37 edges (0), time = 2.669ms.\r\n2019-08-02 04:02:30.552212: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: TRTEngineOp_2_native_segment\r\n2019-08-02 04:02:30.552218: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 8 nodes (0), 8 edges (0), time = 2.021ms.\r\n2019-08-02 04:02:30.552224: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 8 nodes (0), 8 edges (0), time = 0.199ms.\r\n2019-08-02 04:02:30.552229: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 8 nodes (0), 8 edges (0), time = 2.1ms.\r\n2019-08-02 04:02:30.552235: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 8 nodes (0), 8 edges (0), time = 0.058ms.\r\n2019-08-02 04:02:30.552241: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 8 nodes (0), 8 edges (0), time = 2.213ms.\r\n2019-08-02 04:02:30.552247: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: TRTEngineOp_1_native_segment\r\n2019-08-02 04:02:30.552253: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 8 nodes (0), 8 edges (0), time = 2.031ms.\r\n2019-08-02 04:02:30.552259: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 8 nodes (0), 8 edges (0), time = 0.189ms.\r\n2019-08-02 04:02:30.552265: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 8 nodes (0), 8 edges (0), time = 2.102ms.\r\n2019-08-02 04:02:30.552271: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 8 nodes (0), 8 edges (0), time = 0.05ms.\r\n2019-08-02 04:02:30.552293: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 8 nodes (0), 8 edges (0), time = 2.086ms.\r\nW0802 04:02:30.686552 140013164369664 module_wrapper.py:136] From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nI0802 04:02:30.790657 140013164369664 do.py:122] Starting execution\r\nW0802 04:02:31.761751 140013164369664 deprecation.py:323] From do.py:118: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\r\n2019-08-02 04:02:35.895788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\r\npciBusID: 0000:3b:00.0\r\n2019-08-02 04:02:35.895893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-02 04:02:35.895905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-02 04:02:35.895919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-02 04:02:35.895929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-02 04:02:35.895939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-02 04:02:35.895948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-02 04:02:35.895957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-02 04:02:35.896887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-02 04:02:35.896929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-02 04:02:35.896937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-08-02 04:02:35.896943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-08-02 04:02:35.897987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30420 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)\r\nI0802 04:02:35.900057 140013164369664 do.py:141] Starting Warmup cycle\r\n2019-08-02 04:06:27.600120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nI0802 04:06:29.636768 140013164369664 do.py:146] Starting timing.\r\nI0802 04:06:45.426122 140013164369664 do.py:153] Timing loop done!\r\nRunning FP16 graph\r\n```", "This is the error with LeakyRelu:\r\n`2019-08-02 04:02:27.611805: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Parameter check failed at: ../builder/Network.cpp::addActivation::89, condition: int(type) >= 0 && int(type) < EnumMax<ActivationType>()\r\n2019-08-02 04:02:27.611849: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:737] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 82 nodes failed: Internal: TFTRT::ConvertLeakyRelu:2886 failed to add TRT layer, at: detector/tiny-yolo/Conv/LeakyRelu. Fallback to TF...\r\n`\r\nI think this is happening because the version of loaded TensorRT is different from the version used in TF compilation according to the log:\r\n```\r\nI0802 04:02:25.273825 140013164369664 trt_convert.py:195] Linked TensorRT version: (5, 1, 5)\r\nI0802 04:02:25.273978 140013164369664 trt_convert.py:196] Loaded TensorRT version: (5, 0, 2)\r\nW0802 04:02:25.274046 140013164369664 trt_convert.py:212] TensorRT mismatch. Compiled against version 5.1.5, but loaded 5.0.2. Things may not work\r\n```\r\n\r\nTensorRT 5.1 added a bunch of new activation types:\r\n- TensorRT 5.0: https://docs.nvidia.com/deeplearning/sdk/tensorrt-archived/tensorrt-502/tensorrt-api/c_api/namespacenvinfer1.html#acbd177748000d30ae0277ee980757eb6\r\n- TensorRT 5.1 https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/namespacenvinfer1.html#acbd177748000d30ae0277ee980757eb6\r\n\r\n\r\nThis kind of version mismatch will be prevented in the future: https://github.com/tensorflow/tensorflow/commit/03b6d1c6986aa62993e46114be56d9ec3276e3aa\r\n\r\n@nseidl Please use the same version of TensorRT in compilation and loading.\r\n\r\n\r\n", "@pooyadavoodi I have tested this with TensorRT 6.0.1 and did not see any problem.", "Hi @reactivetype did you try TensorRT 6.0.1 through the docker from nvcr or?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31039\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31039\">No</a>\n"]}, {"number": 31038, "title": "Override enumerate on AutoGraph", "body": "This pull request address #30802 issue.", "comments": ["Almost there! Looks like a few lint errors are blocking the pull:\r\n\r\n```\r\ntensorflow/python/autograph/operators/py_builtins_test.py:143: [C0301(line-too-long), ] Line too long (89/80)\r\ntensorflow/python/autograph/operators/py_builtins_test.py:143: [C0326(bad-whitespace), ] Exactly one space required after comma\r\ntensorflow/python/autograph/operators/py_builtins_test.py:143: [C0326(bad-whitespace), ] Exactly one space required after comma\r\ntensorflow/python/autograph/operators/py_builtins_test.py:144: [C0301(line-too-long), ] Line too long (92/80)\r\ntensorflow/python/autograph/operators/py_builtins_test.py:144: [C0326(bad-whitespace), ] Exactly one space required after comma\r\ntensorflow/python/autograph/operators/py_builtins_test.py:144: [C0326(bad-whitespace), ] Exactly one space required after comma\r\ntensorflow/python/autograph/operators/py_builtins_test.py:31: [W0611(unused-import), ] Unused tensor_shape imported from tensorflow.python.framework\r\n```", "@mdanatg Ah, yes! I've fixed it.", "merged internally , waiting for auto merge to happen, thank you all", "@mdanatg Thank you so much for guiding me on my first pull request ever to a big repository on GitHub. You have been very patient despite my glaring stupid mistakes. I want to contribute more! Please let me know if there's anything that I can do to help.", "@ilhamfp glad to hear that! Other examples of features worth adding could include adding similar support for `zip`.\r\n\r\nAlso, keep an eye out for issues marked with these two tags: \"god first issue\" and \"stat:contributions welcome\" - they are good ways to get started.\r\n\r\nCheers,\r\nDan", "@mdanatg Ah, yeah I see. I'm working on it! Expect my PR tomorrow :D  \r\n...  \r\nWait, should I open an issue first?  \r\n\r\nAnd for the last bit, actually, I've been thinking about that. I've been searching solution for me to get automated email or notification when there's a new \"good first issue\" and \"stat:contributions welcome\" label. But so far, I have found nothing. Not even [slack](https://github.com/integrations/slack/pull/797) have this feature yet. Did you know any solution for this?", "@ilhamfp It's ok to open an issue if you wish, and assign yourself to it, but it's not strictly necessary.\r\n\r\nUnfortunately I don't know if there's a way to get automated emails on filtered searches. If there is one, I'd like to use it too :)", "closing this PR as the changes have been merged by `\r\ne496ac8`"]}, {"number": 31037, "title": "Fix incorrect usage of execution plan in GPU delegate", "body": "When checking supported ops, instead of using the node id values from the execution plan, the delegate was just using node ids 0..execution_plan.size. In a case where your graph has 20 nodes, and your execution plan covers nodes 5-20, this would instead build a subgraph out of nodes 0-15.", "comments": []}, {"number": 31036, "title": "Override enumerate on AutoGraph", "body": "This pull request address [#30802](https://github.com/tensorflow/tensorflow/issues/30802) issue.", "comments": ["I just realized a terrible mistake. Please ignore this pull request."]}]