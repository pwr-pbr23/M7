[{"number": 41397, "title": "Keras Sequence Generators not compatible with ragged tensors when using mode.fit", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: cuDNN 10\r\n- GPU model and memory: V100/24GB RAM\r\n\r\n**Describe the current behavior**\r\nI have written the following generator to take numpy arrays and convert to ragged tensors within a Keras Sequence Data generator.\r\n\r\n'''python\r\nclass get_batches(tf.keras.utils.Sequence):\r\n    def __init__(self,x,y,batch_size=10,random=False):\r\n        self.batch_size = batch_size\r\n        self.random = random\r\n        self.x = x\r\n        self.y = y\r\n        self.on_epoch_end()\r\n\r\n    def on_epoch_end(self):\r\n        self.indexes = np.arange(len(self.x[0]))\r\n        if self.random == True:\r\n            np.random.shuffle(self.indexes)\r\n\r\n    def __getitem__(self, index):\r\n        'Generate one batch of data'\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\r\n\r\n        x_out = [turn_into_ragged(var[indexes]) for var in self.x]\r\n        # x_out = [var[indexes] for var in self.x]\r\n        y_out = [var[indexes] for var in self.y]\r\n\r\n        return (x_out, y_out)\r\n\r\n    def __len__(self):\r\n        x = self.x\r\n        batch_size = self.batch_size\r\n        if len(x[0]) % batch_size == 0:\r\n            n_batches = (len(x[0]) // batch_size)\r\n        else:\r\n            n_batches = (len(x[0]) // batch_size) + 1\r\n        return n_batches\r\n\r\nWhen I run this generator in a for loop, I am able to take the outputs and feed it to my model.fit command but when I put the generator into the model.fit command, I get the following error:\r\n\r\n TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was float64, but the yielded element was <tf.RaggedTensor\r\n\r\n**Describe the expected behavior**\r\nI would expect the generator to be compatible with model.fit and train.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n'''python\r\nclass get_batches(tf.keras.utils.Sequence):\r\n    def __init__(self,x,y,batch_size=10,random=False):\r\n        self.batch_size = batch_size\r\n        self.random = random\r\n        self.x = x\r\n        self.y = y\r\n        self.on_epoch_end()\r\n\r\n    def on_epoch_end(self):\r\n        self.indexes = np.arange(len(self.x[0]))\r\n        if self.random == True:\r\n            np.random.shuffle(self.indexes)\r\n\r\n    def __getitem__(self, index):\r\n        'Generate one batch of data'\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\r\n\r\n        x_out = [turn_into_ragged(var[indexes]) for var in self.x]\r\n        # x_out = [var[indexes] for var in self.x]\r\n        y_out = [var[indexes] for var in self.y]\r\n\r\n        return (x_out, y_out)\r\n\r\n    def __len__(self):\r\n        x = self.x\r\n        batch_size = self.batch_size\r\n        if len(x[0]) % batch_size == 0:\r\n            n_batches = (len(x[0]) // batch_size)\r\n        else:\r\n            n_batches = (len(x[0]) // batch_size) + 1\r\n        return n_batches\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sidhomj,\r\nIn the given code snippet you have defined the functions but you're not calling them anywhere. \r\n\r\nTo reproduce the issue reported here, could you please share the complete code or the `.ipynb`/`.py` file you are running. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41396, "title": "pylint errors on master branch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): master branch\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen I run pylint on TF python files, I get a lot of failures. The following is an example (using master branch):\r\n\r\n* Is this expected or is a bug?\r\n* Fixing pylint issues in our PRs is hard given all the existing failures. What do you suggest to make the contribution easier? \r\n\r\n```\r\ntensorflow/python~ % pylint --rcfile=../tools/ci_build/pylintrc ./keras/layers/pooling.py\r\n************* Module keras.layers.pooling\r\nkeras/layers/pooling.py:167:71: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:168:74: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:213:71: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:214:74: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:377:0: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:379:0: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:383:60: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:385:41: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:387:62: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:426:71: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:427:74: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:483:71: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:484:74: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:628:71: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:629:74: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:681:71: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:682:74: C0303: Trailing whitespace (trailing-whitespace)\r\nkeras/layers/pooling.py:72:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)\r\nkeras/layers/pooling.py:95:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:288:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)\r\nkeras/layers/pooling.py:315:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:564:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)\r\nkeras/layers/pooling.py:600:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:734:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:739:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)\r\nkeras/layers/pooling.py:792:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)\r\nkeras/layers/pooling.py:794:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:868:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:873:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)\r\nkeras/layers/pooling.py:918:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:959:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:975:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:980:2: W0221: Parameters differ from overridden 'call' method (arguments-differ)\r\nkeras/layers/pooling.py:1019:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\nkeras/layers/pooling.py:1054:4: R1705: Unnecessary \"else\" after \"return\" (no-else-return)\r\n\r\n------------------------------------------------------------------\r\nYour code has been rated at 8.54/10 (previous run: 8.81/10, -0.27)\r\n```\r\n\r\n", "comments": ["@pooyadavoodi \r\nCan you please share simple indented stand alone code to replicate the issue faced or if possible share a colab gist with the error faced for us to analyse.", "To reproduce the issue, you can simply run the command I provided above on TF source code. For the example above, I used pooling layer source code from keras.", "Just to clarify, the example I gave above is not the only TF python file that has pylint errors. Pretty much every Python file I chose from tensorflow/python has pylint errors, and for some of them score is as low as 6.0.", "Can you send a pull request please?", "There might also be a difference between the settings we are using and the setting you are running with", "@mihaimaruseac There are a huge number of errors across the whole tensorflow/python. \r\n\r\nI also think this is due to having different settings. Could you share how you run pylint and which rc file you use?\r\n\r\nThis is how I run it which is documented here: https://www.tensorflow.org/community/contribute/code_style#pylint\r\n\r\n```pylint --rcfile=tensorflow/tools/ci_build/pylintrc tensorflow/python/keras/layers/pooling.py```", "Perhaps fixing pylint errors is not required anymore for contributing to TF, given the existing errors in the source code? \r\n\r\nIf that's the case, it would be good to indicate that in the documentation: https://www.tensorflow.org/community/contribute/code_style#pylint ", "By the way, I see almost the same failures in r2.2 branch.", "[This](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/ci_build/presubmit/ubuntu_16/sanity/build.sh) is the script we are using for the sanity build. It invokes [this script](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/ci_build/ci_sanity.sh) for the actual tests. And [this](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/ci_build/ci_sanity.sh;l=92-223;drc=7eab1f3bfe796f453b4549a0923ef954c822671c) is the code that does the actual sanitization.", "Thanks for the links. Unfortunately I am still struggling with how to run pylint properly.\r\nI think the issue is lack of good documentation here: https://www.tensorflow.org/community/contribute/code_style#pylint\r\n\r\nHere is what I did using the links provided by @mihaimaruseac \r\n* I only kept \"do_pylint\" in SANITY_STEPS and removed everything else: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh#L648\r\n* I only kept \"Python 3 pylint\" in SANITY_STEPS_DESC and removed everything else: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh#L649\r\n* Ran this command locally: `./tensorflow/tools/ci_build/ci_sanity.sh --incremental` from the TF root.\r\n* The output says `FAIL: Found 155 non-whitelisted pylint errors`.\r\n* Here is the full output log: https://gist.github.com/pooyadavoodi/d2a47e3e23fc10e5fbf473d99cad647d \r\n* As an example, this page seems to show the command used on the cloud for GitHub but I cannot run it that way locally: https://source.cloud.google.com/results/invocations/010e6117-22ed-48df-8a94-b3136583d5ef/details\r\n", "Yes this is quite a problem also for the integrated pylint support in IDEs", "What was the original rationale to not use `# pylint: disable=message-name`?", "We need to also find a solution for astroid at https://github.com/PyCQA/pylint/issues/2603", "In master you can run now `pylint  tensorflow/python/keras/layers/pooling.py`. \r\n\r\n@mihaimaruseac I think that we could close this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41396\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41396\">No</a>\n"]}, {"number": 41395, "title": "[TF2.2] tf.data.Dataset memory leak", "body": "**System information**\r\n- Have I written custom code : yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4\r\n- TensorFlow installed from (source or binary): wheel\r\n- TensorFlow version (use command below):  v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10.2  \r\n- GPU model and memory: GeForce RTX 2080 Ti\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI have a memory leak when using `tf.data.Dataset`.\r\nIt happens when I use fit on a tf.keras.Model.\r\nIt seems to happend when iterating inside a `tf.function` too.\r\n\r\n**Describe the expected behavior**\r\n\r\nMemory should not be incread\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport psutil\r\nfrom tqdm import tqdm\r\n\r\nn = 80\r\ndataset = (\r\n    tf.data.Dataset.from_tensor_slices(\r\n        (\r\n            np.random.rand(n, 416, 416, 3),\r\n            np.zeros((n, 1)),\r\n        )\r\n    )\r\n    .batch(8)\r\n    .repeat()\r\n)\r\n\r\n@tf.function\r\ndef inspect_dataset(dataset):\r\n    for _ in tqdm(dataset, \"iterating through dataset\"):\r\n        pass\r\n\r\n\r\ninspect_dataset(dataset)\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/17848620/87487333-aadaeb80-c63d-11ea-8d51-8e1c4a794abf.png)\r\n\r\n", "comments": ["I have tried in colab with TF version 2.2 ,2.3-rc1 and i am seeing the error message(`Your session crashed after using all available RAM. `).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/849620e259f735b1a72c4b95c30134dc/untitled127.ipynb).Thanks!", "@Wirg Can you please share full code with the `model.fit` so that we can reproduce and find the root-cause of the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41394, "title": "InvalidArgumentError: Operation 'TensorListPushBack_489' has no attr named '_XlaCompile'", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\nYes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nAWS EC2 instance with image of Deep Learning Base AMI (Amazon Linux)\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\nN/A\r\n-   **TensorFlow installed from (source or binary)**:\r\nSource\r\n-   **TensorFlow version (use command below)**:\r\n2.2.0\r\n-   **Python version**:\r\n3.6.6\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\ncuda = V10.0.130; cudnn =  CUDNN_MAJOR 7; CUDNN_MINOR 5; CUDNN_PATCHLEVEL 1\r\n-   **GPU model and memory**:\r\nTesla T4; 8G\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n\r\n\r\n### Describe the problem\r\nI am trying to implement a custom model following a paper. This model itself should not matter too much. The problem I encountered is that the code got stuck in ``` y_pred = self(x, training = True)``` in the train_step function in the Dynamic_Loss class. It did not return an error. After running for an hour, I interrupted the kernel and it returns the error message saying \"InvalidArgumentError: Operation 'TensorListPushBack_489' has no attr named '_XlaCompile'\" and \"InvalidArgumentError: Operation 'While' has no attr named '_XlaCompile'\". I am not sure what the issue is because the code worked before I set persistent = True in tf.Gradient_Tape\r\n### Source code / logs\r\n\r\n\r\n```python\r\n\r\nclass Tem_Agg:\r\n\r\n\t\"\"\"\r\n\tThis class is to aggregate the representations in the temporal dimension using\r\n\tan autoencoder architecture\r\n\t\"\"\"\r\n\r\n\t## the length of the time stamp\r\n\tmax_length = 300\r\n\r\n\tdef __init__(self,  length, timestamp, model = \"baseline\"):\r\n\r\n\r\n\t\t## the number of timestamp in the batch\r\n\t\tself.timestamp = timestamp\r\n\t\t## the dimension of the vectors to be aggregated\r\n\t\tself.vec_length = length\r\n\t\t## the mode to use: baseline model or dynamic model\r\n\t\tassert model in [\"baseline\", \"dynamic\"]\r\n\t\tself.model = model\r\n\r\n\r\n\tdef build_model(self):\r\n\r\n\t\t\"\"\"\r\n\t\tThis is an autoecoder model where the input is used to reconstruct the input itself\r\n\t\tand predict the future\r\n\r\n\t\tfor reference: https://arxiv.org/pdf/1502.04681.pdf\r\n\t\t\"\"\"\r\n\r\n\t\t## the input layer\r\n\r\n\t\tinp = Input(shape = (self.timestamp - 5, self.vec_length))\r\n\r\n\t\tif self.model == \"dynamic\":\r\n\r\n\t\t\tweights = Input(shape = (2, ))\r\n\r\n\t\t\t## mean\r\n\t\t\tmean_weights = Dense(inp.shape[1], activation = \"relu\")(weights)\r\n\t\t\tmean_weights = tf.expand_dims(mean_weights, axis = -1)\r\n\t\t\t## standard deviation\r\n\t\t\tstd_weights = Dense(inp.shape[1], activation = \"relu\")(weights)\r\n\t\t\tstd_weights = tf.expand_dims(std_weights, axis = -1)\r\n\r\n\t\t## encoder\r\n\r\n\t\tenc_first = LSTM(self.vec_length // 2, return_sequences = True)(inp)\r\n\r\n\t\tif self.model == \"dynamic\":\r\n\t\t\tenc_first = Tem_Agg.FiLM(enc_first, mean_weights, std_weights)\r\n\r\n\r\n\t\tenc_second = LSTM(self.vec_length // 4, return_sequences = True)(enc_first)\r\n\r\n\r\n\t\tif self.model == \"dynamic\":\r\n\t\t\tenc_second = Tem_Agg.FiLM(enc_second, mean_weights, std_weights)\r\n\r\n\t\t## the full representation has the same dimension as the input so that\r\n\t\t## the hierarchy model can use it in different levels;\r\n\t\tenc_second_full = LSTM(self.vec_length, name = \"representation\")(enc_first)\r\n\r\n\r\n\t\t## decoder for reconstruction\r\n\r\n\t\tdec_first = LSTM(self.vec_length // 2, return_sequences = True)(enc_second)\r\n\t\trecon = LSTM(inp.shape[-1], return_sequences = True)(dec_first)\r\n\r\n\t\t## predict the future vectors\r\n\t\tpredicted_vec_1 = Dense(inp.shape[-1], activation = \"relu\")(enc_second_full)\r\n\t\tpredicted_vec_2 = Dense(inp.shape[-1], activation = \"relu\")(predicted_vec_1)\r\n\t\tpredicted_vec_3 = Dense(inp.shape[-1], activation = \"relu\")(predicted_vec_2)\r\n\t\tpredicted_vec_4 = Dense(inp.shape[-1], activation = \"relu\")(predicted_vec_3)\r\n\t\tpredicted_vec_5 = Dense(inp.shape[-1], activation = \"relu\")(predicted_vec_4)\r\n\r\n\r\n\t\tif self.model == \"baseline\":\r\n\t\t\tmodel = keras.Model(inputs = inp, outputs = [recon, predicted_vec_5])\r\n\t\t\tmodel.compile(\"adam\", loss = MeanAbsolutePercentageError())\r\n\t\telse:\r\n\t\t\r\n\t\t\tmodel = Dynamic_Loss(inputs = [inp, weights], outputs = [recon, predicted_vec_5, weights])\r\n\t\t\t\r\n\r\n\t\t\tmodel.compile(\"adam\")\r\n\r\n\t\treturn model\r\n\r\n\r\n\t@staticmethod\r\n\tdef FiLM(tensor, mean_weights, std_weights):\r\n\t\t\"\"\"\r\n\t\tThis is the Feature-wise Linear Modulation (FiLM) operation from the paper\r\n\r\n\t\tInputs:\r\n\t\t\ttensor: the tensor from the layer\r\n\t\t\tmean_weight: weights to be multiplied to the tensor\r\n\t\t\tstd_weights: weights to be added to product of tensor and mean_weight\r\n\r\n\t\tOutput:\r\n\t\t\toutput: the output tensor after the operation\r\n\r\n\t\t\"\"\"\r\n\r\n\t\ttensor = Multiply()([tensor, mean_weights])\r\n\t\toutput = Add()([tensor, std_weights])\r\n\t\treturn output\r\n\r\n\r\nclass Dynamic_Loss(keras.Model):\r\n\r\n\t\"\"\"\r\n\tThis is the class to learn the distribution of the loss for the multi-tasks\r\n\tmodel. We will pass the weight vector in the training and inference stages\r\n\r\n\tfor reference: https://openreview.net/pdf?id=HyxY6JHKwr\r\n\r\n\t\"\"\"\r\n\r\n\tdef train_step(self, data):\r\n\r\n\t\t\"\"\"\r\n\t\tOverwrite the training loop\r\n\r\n\t\tInput:\r\n\t\t\tdata: the data we feed into the fit() function; a tf Dataset object\r\n\t\t\"\"\"\r\n\r\n\t\t## unpacking the data\r\n\t\tx, y = data\r\n\r\n\t\t## unpacking y\r\n\t\trecon, future = y\r\n\r\n\t\t## loss\r\n\t\tmape = MeanAbsolutePercentageError()\r\n\r\n\t\ttrainable_vars = self.trainable_variables\r\n\r\n\r\n\t\twith tf.GradientTape(persistent = True) as tape_1:\r\n\r\n\t\t\t## the predicted values\r\n\t\t\ty_pred = self(x, training = True)\r\n\t\t\t\r\n\r\n\t\t\trecon_pred, future_pred, weights = y_pred\r\n\t\t\r\n\r\n\t\t\t## the two different losses\r\n\t\t\trecon_loss = mape(recon, recon_pred)\r\n\t\t\t#recon_gradients = tape_1.gradient(recon_loss, trainable_vars)\r\n\r\n\r\n\r\n\t\t\tfuture_loss = mape(future, future_pred)\r\n\t\t\t\r\n\r\n\t\t\r\n\t\trecon_gradients = tape_1.gradient(recon_loss, trainable_vars)\r\n\t\t\r\n\t\tfuture_gradients = tape_1.gradient(future_loss, trainable_vars)\r\n\r\n\r\n\r\n\r\n\t\t## the gradient of the sum is the sum of the gradient\r\n\t\tgradients = weights.numpy()[0] * recon_gradients + weights.numpy()[1] * future_gradients\r\n\r\n\t\t## applying gradients\r\n\t\tself.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n```", "comments": ["@patrickyu1111,\r\nIssues [#38524](https://github.com/tensorflow/tensorflow/issues/38524#issuecomment-646133140) and [#34211](https://github.com/tensorflow/tensorflow/issues/34211#issuecomment-599779838) with similar error log were fixed in TF v2.3. \r\n\r\nCould you please try installing TF v2.3.0rc1 or TF-nightly and check if you are facing the same issue. Thanks!", "I tried both versions. But the issue is still here", "> @patrickyu1111,\r\n> Issues [#38524](https://github.com/tensorflow/tensorflow/issues/38524#issuecomment-646133140) and [#34211](https://github.com/tensorflow/tensorflow/issues/34211#issuecomment-599779838) with similar error log were fixed in TF v2.3.\r\n> \r\n> Could you please try installing TF v2.3.0rc1 or TF-nightly and check if you are facing the same issue. Thanks!\r\n\r\nI tried both versions. But the issue is still here.", "@patrickyu1111  From the code snippet you provided I cannot reproduce the behavior you reported. Can you please help us with minimal code reproducer? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41394\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41394\">No</a>\n", "@ymodak @patrickyu1111 Hi, is this issue solved? If yes, can you please provide more information about the solution? I am facing a similar issue"]}, {"number": 41393, "title": "Use Bazel's builtin patch support.", "body": "This removes a dependency on the system having a patch executable.", "comments": ["We had issues in the past where patching wouldn't work on Windows/Linux. We have tried `git patch`, `git apply` and `patch`.\r\n\r\nAdding gunan@ to verify if this is likely to solve all issues form above and if the diffs are also ok, as I was not expecting them to be generated given PR title/description", "I had to regenerate some of the patch files because the builtin patch is stricter than `patch(1)`. (Basically, I just did `patch -p1 old.patch` and `diff -ru`.)", "As long as presubmits pass, I think we can accept this change. ", "Only \"Windows Bazel GPU \" appears to have failed, but it also is broken on master.", "Will import this manually, something breaks"]}, {"number": 41391, "title": "exponential_avg_factor not in Op FusedBatchNormV3: model deployed using Golang Tensorflow v2.0.2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nTraining:\r\n*TensorFlow v2.2.0\r\n*GPU distributed training with 4 GPUs\r\n*Use ```strategy = tf.distribute.MirroredStrategy()``` for multiple GPU training\r\n*MobileNet model is trained using ```model.fit```\r\n*Trained model is saved using ```.keras_model.save(path)```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n*Model is trained on Machine type ```n1-standard-8 (8 vCPUs, 30 GB memory)```\r\n*GCP VM with image ```c5-deeplearning-tf2-2-2-cu101-v20200706```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n*Raspberry Pi Golang model deployment\r\n*Model optimization\r\n- TensorFlow installed from (source or binary):\r\n*Training model in GCP VM ( ```c5-deeplearning-tf2-2-2-cu101-v20200706``` )\r\n*Local using ```pip install tensorflow==2.2.0```\r\n- TensorFlow version (use command below):\r\n*Training model in GCP VM ( ```c5-deeplearning-tf2-2-2-cu101-v20200706``` )\r\n*Local using ```pip install tensorflow==2.2.0```\r\n- Python version:\r\n3.7\r\n- Bazel version (if compiling from source):\r\n0.24.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n4 x NVIDIA Tesla K80, 11GB each\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n*Model trained on GCP VM with 4 GPU using TensorFlow version as per google deep learning image ```c5-deeplearning-tf2-2-2-cu101-v20200706``` is v2.2.2\r\n*Model is saved using ```keras_model.save(path)```\r\n*I am facing issue with model freeze and optimization, so that I can deploy that model in Golang Tensorflow\r\n1. Freeze model  using ```tf.compat.v1.graph_util.convert_variables_to_constants``` \r\nI have tool the keras saved model directory from GCP VM and try to freeze the model in local.\r\n```\r\nfrom tensorflow.python.framework import graph_io\r\ntf.compat.v1.disable_v2_behavior()\r\ntf.keras.backend.set_learning_phase(0)\r\ntf.compat.v1.keras.backend.set_floatx('float16')\r\n# model_dir: keras model saved dir\r\nmodel = tf.keras.models.load_model(model_dir, compile=False)\r\noutput_node_names = [\"input\",\"out1/Softmax\",\"out2/Softmax\"]\r\nsess = tf.compat.v1.keras.backend.get_session()\r\nfrozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(sess, \r\n                                                                              sess.graph.as_graph_def(), \r\n                                                                              output_node_names)\r\ninput_names = [\"input\"]\r\noutput_names = [\"out1/Softmax\",\"out2/Softmax\"]\r\nfrozen_graph = optimize_for_inference_lib.optimize_for_inference(frozen_graph,\r\n                                                                         input_names,\r\n                                                                         output_names,\r\n                                                                         input_node.dtype.as_datatype_enum)\r\ngraph_io.write_graph(frozen_graph, output_dir, freezed_model_file, as_text=False)\r\n```\r\n2. The output model .pb (single file), is freezed using above code.\r\nwhile freezing following is the warning\r\n```\r\nWARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'depth_conv_bn_5/FusedBatchNormV3'\r\n```\r\n3. Then converted the freezed model to builder.SaveModelBuilder\r\n```\r\ntf.compat.v1.disable_v2_behavior()\r\ntf.compat.v1.keras.backend.set_floatx('float16')\r\nwith gfile.FastGFile(freezed_model.pb, \"rb\") as f:\r\n    graph_def = tf.compat.v1.GraphDef()\r\n    byte = f.read()\r\n    graph_def.ParseFromString(byte)\r\n    tf.import_graph_def(graph_def, name='')\r\ncount_nodes = 0\r\nfor node in tf.compat.v1.Session().graph_def.node:\r\n    count_nodes += 1\r\nprint(\"Number of nodes that not optimized freezed model has: \", count_nodes)\r\n# golang_tf_model_dir: where the golang specific model is prepared\r\nbuilder = saved_model_builder.SavedModelBuilder(golang_tf_model_dir)\r\nwith tf.compat.v1.Session() as sess:\r\n    builder.add_meta_graph_and_variables(sess=sess,\r\n                                    tags=[\"model\"],\r\n                                    clear_devices=True)\r\n    \r\nbuilder.save()\r\n``` \r\n4. After this when I am trying to load this model in golang tensorflow version v2.0.2\r\nI found this error\r\n```\r\n NodeDef mentions attr 'exponential_avg_factor' not in Op<name=FusedBatchNormV3; signature=x:T, scale:U, offset:U, mean:U, variance:U -> y:T, batch_mean:U, batch_variance:U, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=is_training:bool,default=true>; NodeDef: {{node depth_conv_bn_5/FusedBatchNormV3}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\nFAIL\r\n```\r\n\r\n**Describe the expected behavior**\r\nEarlier I have used tensorflow version 1.12.0 to my model training, freeze and deployment and I did not see FusedBatchNormV3\r\nbut in recent changes in TensorFlow v2.0 BatchNormalization layer is freezed to FusedBatchNormV3 which is causing issue.\r\n\r\nEarlier same trained model on Tf1.12.0 has FusedBatchNorm only no FusedBatchNormV3.\r\n\r\nMy expectation\r\n1. How to exponential_avg_factor will be considered in FusedBatchNormV3\r\n2. GraphDef-generating binary is ```tf.compat.v1.GraphDef()``` \r\n3. GraphDef-interpreting binary is ```graph_def.pb2``` i think because deployment is in TensorFlow version v2.0.2\r\nOr I am doing something wrong.\r\n\r\n**Standalone code to reproduce the issue**\r\nFor golang model loading code\r\n```\r\nmodel := tf.LoadSavedModel(<golang_tf_model_dir>, []string{<model_tag>}, nil /*options*/)\r\n```\r\nmodel_tag is default ```serve``` is not changed while freezing\r\ngolang_tf_model_dir: is mentioned earlier\r\n\r\n**Other info / logs**\r\nAttached logs earlier ", "comments": ["I have checked there is fix for tensorflow lite https://github.com/tensorflow/tensorflow/pull/29615/files\r\nBut this is not the case for keras-model", "I got some insights to this issue\r\n1. If a keras model has BatchNormalization after a Concatenation layer then the ```optimize_for_inference_lib.optimize_for_inference``` API could not fuse the batch norm\r\n2. Because it could not fuse the batch norm the FuseBatchNormV3 stays as it is after inference (freezed model)\r\n3. If you put this freezed model to graph tranformation\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=freezed_model.pb \\\r\n--out_graph=optimized_saved_model.pb \\\r\n--inputs='input' \\\r\n--outputs='out1/Softmax,out2/Softmax' \\\r\n--transforms='\r\n  obfuscate_names\r\n  add_default_attributes\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\n\r\nthis will throw error as follows\r\n```\r\nfold_constants: Ignoring error NodeDef mentions attr 'exponential_avg_factor' not in Op<name=FusedBatchNormV3; signature=x:T, scale:U, offset:U, mean:U, variance:U -> y:T, batch_mean:U, batch_variance:U, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=is_training:bool,default=true>; NodeDef: {{node 1i}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n```\r\n4. After if you try to convert the optimized model pb  to saved model for deployment in Golang environment\r\nthe same error persists\r\n\r\n**Is this behavior is correct? we can not put a batch normalization layer after a concatenation layer?**\r\n**or this need a fix**", "Any progress here?", "Did you find any solution for this?\r\n", "I'm seeing the same error when loading ssd_mobilenet_v2_mnasfpn_coco from the TF v1 model zoo using TFv1. I laid out the details here: https://github.com/onnx/tensorflow-onnx/issues/862#issuecomment-781350143", "It appears that that option appeared in TF 2.2.  Compare tf 2.1 to tf 2.2:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/nn_impl.py\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/nn_impl.py\r\n\r\nSearch for the string avg_ and you will find it new to 2.2.  I check in 1.n up to 15 and it is not in there.  I suspect model created by a newer version of TF 2.2+ and trying to load in using a pre 2.2 version.\r\n", "The last comment sounds like correct to me, thanks!  From \r\nhttps://www.tensorflow.org/guide/versions#backward_and_partial_forward_compatibility, forward compatibility is only offered within Patch releases. Closing for now, but of course report back if you think this should still work.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41391\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41391\">No</a>\n"]}, {"number": 41390, "title": "Fix FileIO tests for Python < 3.6", "body": "Fixes https://github.com/tensorflow/tensorflow/pull/41273#issuecomment-658305238\r\n\r\n/cc @mihaimaruseac ", "comments": []}, {"number": 41389, "title": "Segmentation fault in subgraph.h when running tensorflow lite in ROS Foxy", "body": "### System information \r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: I used example code to write a Ros node\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ROS Foxy Docker Container (Ubuntu 20.04), currently testing on a x86_64 system but will deploy it to a aarch64 system\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: No\r\n-   **TensorFlow installed from (source or binary)**: Source\r\n-   **TensorFlow version (use command below)**: Git tag d855adfc5a0195788bf5f92c3c7352e638aa1109\r\n-   **Python version**: None, I\u00b4m using C++\r\n-   **Bazel version (if compiling from source)**: None, I\u00b4m using Cmake 3.17\r\n-   **GCC/Compiler version (if compiling from source)**: 4:9.3.0-1ubuntu2\r\n-   **CUDA/cuDNN version**: None\r\n-   **GPU model and memory**: Google Coral Chip\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI already described the problem [here](https://github.com/Namburger/edgetpu-minimal-example/issues/2)\r\nTo make things short: I tried using [these examples](https://github.com/Namburger/edgetpu-minimal-example) in ROS but got a Segmentation fault error when i tried calling interpreter.inputs()[index];\r\nI\u00b4ve been told to ask here since it seems to be a Tensorflow API issue.\r\n\r\n### Source code / logs\r\nStart a docker container: docker run -it ros:foxy\r\nExecute `source /opt/ros/foxy/setup.bash`\r\nRun \r\n```\r\napt update\r\napt install unzip\r\napt install curl\r\napt install libusb-1.0-0\r\n```\r\nUpdate cmake like this:\r\n\r\n```\r\napt install -y wget\r\napt-get install libssl-dev\r\nwget https://github.com/Kitware/CMake/releases/download/v3.17.0/cmake-3.17.0.tar.gz\r\ntar xvf cmake-3.17.0.tar.gz\r\nrm cmake-3.17.0.tar.gz\r\ncd cmake-3.17.0/\r\n./configure\r\nmake\r\nmake install\r\n```\r\n\r\nCreate a dev_ws directory in the home folder\r\ncd into /home/dev_ws\r\ncreate a src folder\r\ncopy the zip file folder nn_tflite into the src folder\r\nDirectory structure should be\r\nhome\r\n...|dev_ws\r\n......| src\r\n.........| dev_ws\r\n............| nn_tflite\r\n\r\n\r\ncd into /home/dev_ws and execute `colcon build --packages-select nn_tflite --symlink-install`\r\nExecute  `. install/setup.bash`\r\nRun the ros node with `ros2 run nn_tflite tflite_nn`\r\n\r\nRunning gbd:\r\n`cd install/nn_tflite/lib/nn_tflite/`\r\n`gbd --args ./tflite_nn`\r\n\r\n[nn_tflite.zip](https://github.com/tensorflow/tensorflow/files/4925761/nn_tflite.zip)\r\n\r\n\r\n", "comments": ["This seems to be a system inconsistency. \r\nAfter rebooting my pc and starting a new docker container it worked fine.\r\nSorry for the trouble!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41389\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41389\">No</a>\n"]}, {"number": 41388, "title": "2020-07-14 nightlies cannot import `tensorflow`", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: pip install tf-nightly==2.4.0.dev20200714\r\n- Python version: Python 3.7.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n`import tensorflow` fails.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n$ cd \"$(mktemp -d)\"\r\n$ virtualenv -q -p python3.7 ./ve\r\n$ . ./ve/bin/activate\r\n(ve) $ pip install tf-nightly\r\nCollecting tf-nightly\r\n  Using cached tf_nightly-2.4.0.dev20200714-cp37-cp37m-manylinux2010_x86_64.whl (142.9 MB)\r\nRequirement already satisfied: six>=1.12.0 in ./ve/lib/python3.7/site-packages (from tf-nightly) (1.14.0)\r\nCollecting h5py<2.11.0,>=2.10.0\r\n  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\r\nProcessing /HOMEDIR/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2/termcolor-1.1.0-py3-none-any.whl\r\nCollecting google-pasta>=0.1.8\r\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\nRequirement already satisfied: wheel>=0.26 in ./ve/lib/python3.7/site-packages (from tf-nightly) (0.34.2)\r\nCollecting grpcio>=1.8.6\r\n  Using cached grpcio-1.30.0-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB)\r\nCollecting gast==0.3.3\r\n  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\r\nCollecting tf-estimator-nightly\r\n  Using cached tf_estimator_nightly-2.4.0.dev2020071401-py2.py3-none-any.whl (459 kB)\r\nCollecting opt-einsum>=2.3.2\r\n  Using cached opt_einsum-3.2.1-py3-none-any.whl (63 kB)\r\nCollecting tb-nightly<2.4.0a0,>=2.3.0a0\r\n  Using cached tb_nightly-2.3.0a20200713-py3-none-any.whl (6.8 MB)\r\nCollecting protobuf>=3.9.2\r\n  Using cached protobuf-3.12.2-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\r\nProcessing /HOMEDIR/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6/wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl\r\nCollecting keras-preprocessing<1.2,>=1.1.1\r\n  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\nProcessing /HOMEDIR/.cache/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e/absl_py-0.9.0-py3-none-any.whl\r\nCollecting numpy<1.19.0,>=1.16.0\r\n  Using cached numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\r\nCollecting astunparse==1.6.3\r\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\nCollecting werkzeug>=0.11.15\r\n  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\r\nCollecting tensorboard-plugin-wit>=1.6.0\r\n  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\r\nRequirement already satisfied: setuptools>=41.0.0 in ./ve/lib/python3.7/site-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (44.0.0)\r\nCollecting google-auth<2,>=1.6.3\r\n  Using cached google_auth-1.19.0-py2.py3-none-any.whl (91 kB)\r\nCollecting google-auth-oauthlib<0.5,>=0.4.1\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nRequirement already satisfied: requests<3,>=2.21.0 in ./ve/lib/python3.7/site-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)\r\nCollecting markdown>=2.6.8\r\n  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\r\nCollecting rsa<5,>=3.1.4; python_version >= \"3\"\r\n  Using cached rsa-4.6-py3-none-any.whl (47 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting importlib-metadata; python_version < \"3.8\"\r\n  Using cached importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\r\nCollecting pyasn1>=0.1.3\r\n  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nCollecting zipp>=0.5\r\n  Using cached zipp-3.1.0-py3-none-any.whl (4.9 kB)\r\nInstalling collected packages: numpy, h5py, termcolor, google-pasta, grpcio, gast, tf-estimator-nightly, opt-einsum, werkzeug, tensorboard-plugin-wit, absl-py, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, protobuf, oauthlib, requests-oauthlib, google-auth-oauthlib, zipp, importlib-metadata, markdown, tb-nightly, wrapt, keras-preprocessing, astunparse, tf-nightly\r\nSuccessfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.19.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 importlib-metadata-1.7.0 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.12.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tb-nightly-2.3.0a20200713 tensorboard-plugin-wit-1.7.0 termcolor-1.1.0 tf-estimator-nightly-2.4.0.dev2020071401 tf-nightly-2.4.0.dev20200714 werkzeug-1.0.1 wrapt-1.12.1 zipp-3.1.0\r\n(ve) $ python -c 'import tensorflow'\r\nTraceback (most recent call last):\r\n  File \"/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/tmp/tmp.VuldV9r4ny/ve/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n(ve) $ \r\n```\r\n\r\n**Any other info / logs**\r\n\r\nNope.\r\n", "comments": ["I'm seeing the same error, seems `tf-nightly` is requiring GPU support.\r\n\r\nLooks like there were some changes to nightly release scripts yesterday that may have something to do with this: https://github.com/tensorflow/tensorflow/commit/4e7eb7f5ab6fb39b0804539f960c8bad12d7d106.", "Sorry for the breakage. I've deleted 07/14 builds across all packages and will roll back the config changes. ", "We normally have a smoke test that is run before we upload. For some reason I'm unable to repro this breakage:\r\n\r\n```\r\n$ python --version\r\nPython 3.7.7\r\n(venv37) $ python\r\nPython 3.7.7 (default, Mar 10 2020, 17:25:08) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.4.0-dev20200714'\r\n>>> from tensorflow.python._pywrap_tensorflow_internal import *\r\n>>>\r\n```\r\n\r\nIf you have some repro steps please let me know. ", "Hey @av8ramit, is it possible that your machine has CUDA available?  In my case, we were using a Docker image derived from `ubuntu:18.04`, just ran `pip install tf-nightly`, and `python -c import tensorflow` would reveal the error.", "@tgaddair yes my machine had CUDA available, I'll try on one without it, good idea.\r\n\r\nI believe I've patched the bug in this [whl](https://storage.googleapis.com/tensorflow-nightly/prod/tensorflow/release/ubuntu_16/gpu_py36_full/nightly_release/331/20200715-134916/github/tensorflow/pip_pkg/tf_nightly-2.4.0.dev20200715-cp36-cp36m-manylinux2010_x86_64.whl).\r\n\r\nI've renovated the release config but haven't published it out of fear that this bug still exists.  If I can repro the old bug, I'll push the new bazelrc configs soon. ", "I was able to repro and can confirm the new config fixes the issue.", "@av8ramit: Nightlies from yesterday and today work fine on my machine,\r\nbut from your comment it sounds like we\u2019re not sure whether everything\u2019s\r\nfixed yet. Should I hold off on unpinning our CI (i.e., reverting\r\n<https://github.com/tensorflow/tensorboard/pull/3839>)?\r\n", "Ah, race condition. :-) Sounds good, thanks!\r\n", "So I had reverted my config change on 07/14, so if you want to be safe it may be worth pinning to 07/15 or trying this [custom whl](https://storage.googleapis.com/tensorflow-nightly/prod/tensorflow/release/ubuntu_16/gpu_py36_full/nightly_release/331/20200715-134916/github/tensorflow/pip_pkg/tf_nightly-2.4.0.dev20200715-cp36-cp36m-manylinux2010_x86_64.whl) (it was not uploaded to pypi) I've created with my config change and fix to the cuda issue. I've resubmitted my config change today after testing.", "Looks like we can close this?", "Yes, this is working again in latest nightlies.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41388\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41388\">No</a>\n"]}, {"number": 41387, "title": "model_server_config requires platform information", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tfx/serving/serving_config#model_server_config_details\r\n\r\n## Description of the issue (what needs changing):\r\n\r\nThe model configuration file in the TF Serving docs (link above) should include the `model_platform`. If left out, TF Serving stops with the error below:\r\n\r\n```\r\nError: Invalid argument: Illegal setting neither ModelServerConfig::model_type (deprecated) nor ModelServerConfig::model_platform.\r\n```\r\n\r\n### Clear description\r\n\r\nThe configuration example should be updated from \r\n\r\n```\r\nmodel_config_list {\r\n  config {\r\n    name: 'my_first_model'\r\n    base_path: '/tmp/my_first_model/'\r\n  }\r\n  config {\r\n    name: 'my_second_model'\r\n    base_path: '/tmp/my_second_model/'\r\n  }\r\n}\r\n```\r\n\r\nto\r\n\r\n```\r\nmodel_config_list {\r\n  config {\r\n    name: 'my_first_model'\r\n    base_path: '/tmp/my_first_model/'\r\n    model_platform: 'tensorflow'\r\n  }\r\n  config {\r\n    name: 'my_second_model'\r\n    base_path: '/tmp/my_second_model/'\r\n    model_platform: 'tensorflow'\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Parameters defined\r\n\r\nPlease see the example above\r\n\r\n\r\n### Submit a pull request?\r\n\r\nPR requested here: https://github.com/tensorflow/serving/pull/1687\r\n", "comments": ["@hanneshapke \r\n\r\nThis issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from [here.](https://github.com/tensorflow/serving/issues) Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 41386, "title": "Add complex data type support for tf.math.acos in XLA", "body": "This PR tries to address the issur raised in #41370\r\nwhere tf.math.acos throws out error with complex input data.\r\nThe issue was that in XLA the `Acos` op does not capture\r\nthe complex data types.\r\n\r\nThis PR adds complex support for tf.math.acos in XLA\r\n\r\nThis PR fixes #41370.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @thomasjoerg for the review. The PR has been updated with code comment moved to the top and changed to match the implementation. Please take a look and let me know if there are any issues."]}, {"number": 41385, "title": "[Cherrypick:r2.3] Explicitly raise a (clearer) error message when models end up in inva\u2026", "body": "\u2026lid states due to interleaving graph and eager.\r\n\r\nIn rare cases code may have run w/o crashing when in these invalid states, but it's safer to error with an explanation rather than risk silent failures/fragile behavior.\r\n\r\nPiperOrigin-RevId: 321192744\r\nChange-Id: I9e97ac3b7cea27c9b389e5202de9f1c09a4aa2b8", "comments": ["will merge it tomorrow, after collecting all the cherrypicks. "]}, {"number": 41384, "title": "Increase complexity of arg_def_fuzz", "body": "@mihaimaruseac", "comments": []}, {"number": 41383, "title": "added INT16 support in corresponding python wrappers", "body": "Continuation of #40190 as detailed by @MeghnaNatraj in the [comments](https://github.com/tensorflow/tensorflow/pull/40190#issuecomment-653698323)", "comments": []}, {"number": 41382, "title": "error trying to link to Tensorflow C library", "body": "After extracting the libraries to my `/usr/local/` directory from\r\n\r\nhttps://www.tensorflow.org/install/lang_c#download\r\n\r\nI get the following:\r\n\r\n```\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 1603 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2710 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2711 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 1603 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2710 (>= sh_info of 1)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2711 (>= sh_info of 1)\r\n```", "comments": ["@boxerab,\r\nPlease take a look at [this comment](https://stackoverflow.com/a/59916438) from a similar StackOverflow issue and let us know if it helps. Thanks!", "Yes! That worked. Thanks a lot.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41382\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41382\">No</a>\n"]}, {"number": 41381, "title": "remove file block cache", "body": "@mihaimaruseac \r\nThis PR remove `FileBlockCache` interface and `FileBlockCacheStatsInterface`. `ram_file_block_cache_test` will still pass.", "comments": []}, {"number": 41379, "title": "Add aws crypto and init for s3", "body": "@mihaimaruseac \r\nThis PR adds 2 things:\r\n- `aws crypto` which is used to initialize `s3_client` ( copy from `core` implementation )\r\n- `Init` for S3, but since we use lazy-loading for `s3`, it just initialize with `nullptr`.", "comments": []}, {"number": 41378, "title": "Deprecation of validation_data in tensorflow.keras.Callbacks", "body": "Hi,\r\n\r\nwhy the `validation_data` attribute has been dropped from  *callbacks* ?\r\n\r\nThe only alternative is to set `validation_data` as a constructor parameter in the `Callback`, I can understand that this is a solution, but why we lost that useful feature? Could be recovered in subsequent releases? Previously callbacks where able to use  validation data generated with `validation_split` on the fit call.\r\n\r\nIn the documentation  https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback there is any example that uses validation data, it is uncommon to lack such an important example in a flawless overall documentation like Tensorflow has right now.\r\n\r\nThanks.\r\nKind regards.", "comments": ["@Guillermogsjc As mentioned in the `Callbacks` [guide](https://www.tensorflow.org/guide/keras/custom_callback),\r\n\r\n> A callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include tf.keras.callbacks.TensorBoard to visualize training progress and results with TensorBoard, or tf.keras.callbacks.ModelCheckpoint to periodically save your model during training.\r\n\r\nThere are several callbacks to monitor different things during training/validation. \r\n\r\n> Previously callbacks where able to use validation data generated with validation_split on the fit call.\r\nSorry, didn't get what you meant. Currently callbacks in `.fit` uses `validation_split` as shown in the `Callbacks` guide mentioned above.\r\n\r\nPlease add more details and possibly a standalone code (if required) about the issue. thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41377, "title": "Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize", "body": "I am using tensorflow-gpu 1.15.3 and keras 2.3.1, with python 3.7 and cuda 10.0. I confirm that the tensorflow version matches up with the python and cuda (including cudnn).  The tensorflow-gpu works fine when I test it with a demon example on image classification (e.g., https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py). But when I tried a large cnn model such as Yolo-v3 (i.e., https://github.com/qqwweee/keras-yolo3), it gives the error: \r\n```\r\nUnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_1/convolution}}]]\r\n\t [[Mean/_2311]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_1/convolution}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\nMost often the above error is due to the mismatch between tensorflow-gpu and cuda versions. But it seems not to be that case.\r\n\r\nIt is very interesting to note if I first run the small image classification test, which would go smoothly without any error, and thereafter run the Yolo-v3, the Yolo training will be fine without the \"Failled to get convolution algorithm\".  But if I directly run the yolo training, it won't proceed because of the error. \r\n\r\nAnyone has similar experiences? What possible reasons for the cuda error?", "comments": ["@jingweimo,\r\nPlease try limiting GPU memory growth as shown in this guide [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if you are facing the same issue. Thanks! ", "@amahendrakar I put the following codes in the front of my script:\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  # Restrict TensorFlow to only use the first GPU\r\n  try:\r\n    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\r\n  except RuntimeError as e:\r\n    # Visible devices must be set before GPUs have been initialized\r\n    print(e)\r\n\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n```\r\n\r\nBut unfortunately, that issue still persists. To train the YOLO model, I do have to run the small example first, and then switch to the YOLO model. If I directly do the latter, the errors will occur. ", "Can you please share the full log?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41377\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41377\">No</a>\n"]}, {"number": 41376, "title": "More info on context manager/`strategy.scope` in Custom training loops with tf.distribute.MirroredStrategy()", "body": "Hi @lamberta and team,\r\n\r\nJust some ideas for the **Custom training loops** tutorial (with `tf.distribute.MirroredStrategy()`):\r\n\r\n**URLs:** \r\n\r\nhttps://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function\r\nhttps://www.tensorflow.org/tutorials/distribute/custom_training#define_the_metrics_to_track_loss_and_accuracy\r\nhttps://www.tensorflow.org/tutorials/distribute/custom_training#create_a_strategy_to_distribute_the_variables_and_the_graph\r\n\r\n**Description:**\r\n\r\nIn the tutorial, you create a new strategy (`strategy = tf.distribute.MirroredStrategy()`), which has a context manager\u2014`strategy.scope`. Similar to the **[Distributed training with Keras](https://www.tensorflow.org/tutorials/distribute/keras#define_distribution_strategy)** tutorial, maybe you can also explicitly mention what the scope does in the **Custom training loops** tutorial, because some users may not have studied that in greater depth. And, it would make the tutorial more consistent with some of the other ones under **Tutorials** > **Advanced** > **Distributed training**. \r\n\r\nSo, to ensure the users don't have to use search here, especially if some of them are new to this, it may be useful to provide a bit more background on the context manager.\r\n\r\nNote that inside the context manager\u2014`strategy.scope`\u2014we have to define a number of things: the loss function, metrics, the model itself, Adam optimizer, and checkpoint. And the tutorial separates these items into different sections and provides detailed explanations of each step. Therefore, I suggest maybe we add a few comments as follows:\r\n\r\n**Examples:**\r\n\r\n```python\r\n#  <NEW> Enclose the loss function call inside the context manager.\r\nwith strategy.scope():\r\n  # <original comment>\r\n  <define a method for loss computation>\r\n```\r\n\r\n```python\r\n# <NEW> test loss and training and test accuracy metrics must be created under `strategy.scope`\r\nwith strategy.scope():\r\n  test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n      name='train_accuracy')\r\n  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n      name='test_accuracy')\r\n```\r\n\r\nAdditionally, this note from the tutorial here may also be improved (the diff is _italicized_) as follows:\r\n\r\n> **Note:** You can put all the code below inside a single scope _\u2014the `strategy.scope` context manager that instructs how to split the training among devices_. We are dividing it into several code cells for illustration purposes.\r\n\r\nLastly, similar to this [step](https://www.tensorflow.org/tutorials/distribute/keras#define_distribution_strategy), we can add a short decriptive paragraph (_italicized_) to explain what we're doing when defining `strategy` as `tf.distribute.MirroredStrategy()`:\r\n\r\n> _Create a `MirroredStrategy` object to handle distribution, and provides a context manager (`tf.distribute.MirroredStrategy.scope`) to build your model inside:_\r\n>\r\n> ```python\r\n> # If the list of devices is not specified in the\r\n> # `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\r\n> strategy = tf.distribute.MirroredStrategy()\r\n> ```\r\n\r\n---\r\n\r\nThere is another similar example from the **Multi-worker training with Keras** tutorial:\r\n\r\n- [Train the model with MultiWorkerMirroredStrategy](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy):\r\n\r\n```python\r\nwith strategy.scope():\r\n  # Model building/compiling need to be within `strategy.scope()`.\r\n  multi_worker_model = build_and_compile_cnn_model()\r\n```\r\n\r\n**PRs:**\r\n\r\nCan submit a PR if you OK (some) of the ideas.\r\n\r\nCheers!", "comments": ["@lamberta Let me know if a small PR would make sense.\r\n\r\nThe diffs:\r\n\r\n```diff\r\n+ # Enclose the loss function call inside the context manager.\r\nwith strategy.scope():\r\n  # Set reduction to `none` so we can do the reduction afterwards and divide by\r\n  # global batch size.\r\n  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n      from_logits=True,\r\n      reduction=tf.keras.losses.Reduction.NONE)\r\n  ...\r\n```\r\n\r\n```diff\r\n+ # Instantiate the test loss and training and test accuracy metrics inside the context manager.\r\nwith strategy.scope():\r\n  test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n      name='train_accuracy')\r\n  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n      name='test_accuracy')\r\n```\r\n\r\n```diff\r\n\u2b51 Note: You can put all the code below inside a single scope\r\n+ \u2014the `strategy.scope` context manager that instructs how to split the training among devices. \r\nWe are dividing it into several code cells for illustration purposes.\r\n```\r\n\r\n```diff\r\n\u2b51 Note: You can put all the code below inside a single scope. We are dividing it into several code cells for illustration purposes.\r\n\r\n+ Create a `MirroredStrategy` object to handle distribution and provide a context manager \r\n+ (`tf.distribute.MirroredStrategy.scope`) to build your model inside:\r\n\r\n# If the list of devices is not specified in the\r\n# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\r\nstrategy = tf.distribute.MirroredStrategy()\r\n```\r\n\r\nWDYT? Thanks @lamberta ", "Hi @8bitmp3, sorry for the late response on this. I'm not sure how much (if at all) the custom training loop tutorial has changed since you wrote this issue. But if you have some small changes that you think will make it easier for users to understand, please feel free to file a PR. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@nikitamaia No worries, will do. Thank you \ud83d\udc4d "]}, {"number": 41375, "title": "ModuleNotFoundError: No module named 'tensorflow.python.util'", "body": "import tensorflow as tf\r\n2020-07-14 21:15:42.824490: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-07-14 21:15:42.827687: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\\u5ed6\u5b87\u6770\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\\u5ed6\u5b87\u6770\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 64, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\\u5ed6\u5b87\u6770\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\framework_lib.py\", line 24, in <module>\r\n    from tensorflow.python.framework.device import DeviceSpec\r\n  File \"C:\\Users\\\u5ed6\u5b87\u6770\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\device.py\", line 24, in <module>\r\n    from tensorflow.python.framework import device_spec\r\n  File \"C:\\Users\\\u5ed6\u5b87\u6770\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\device_spec.py\", line 21, in <module>\r\n    from tensorflow.python.util.tf_export import tf_export\r\nModuleNotFoundError: No module named 'tensorflow.python.util'", "comments": []}, {"number": 41374, "title": "TensorFlow 2.2.0 doesn't detect GPU with CUDA version 10.2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: Tesla K40m\r\n\r\n**Describe the current behavior**\r\nKeras (version 2.4.3) with tensorflow as backend doesn't detect the GPU. This is verified by running:\r\n`tf.config.experimental.list_physical_devices('GPU')`\r\nreturns emplty list [].\r\n\r\nAlso, checked as:\r\n`from tensorflow.python.client import device_lib`\r\n`print(device_lib.list_local_devices())`\r\n\r\nreturns: \r\n_[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 11064916497553704899\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 5592130336569042773\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n]_\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should detect the GPU as detected by PyTorch as:\r\n`torch.cuda.is_available()`\r\nOutputs: True\r\n\r\n`torch.cuda.current_device()`\r\nOutputs: 0\r\n\r\n`torch.cuda.get_device_name(0)`\r\nOutputs: 'Tesla K40m'\r\n\r\n**Standalone code to reproduce the issue**\r\nNot required in this case as it seems to be some version mismatch issue.\r\n\r\n**Other info / logs** \r\nAttaching the tf_env.txt output file for the environment settings.\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/4918521/tf_env.txt)\r\n\r\nThanks!\r\n", "comments": ["Here's the update:\r\nIt works when I install separately tensorflow with GPU support:\r\n`conda install tensorflow-gpu`\r\n\r\nThus,\r\n`tf.test.is_gpu_available()`\r\nreturns: True\r\n\r\n`K.tensorflow_backend._get_available_gpus()`\r\nreturns:\r\n['/job:localhost/replica:0/task:0/device:GPU:0']\r\n\r\nAnd, `device_lib.list_local_devices()`\r\nreturns:\r\n_[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 15279102792666353313\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 8743602476137325517\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 11315983156\r\nlocality {\r\n  bus_id: 2\r\n  numa_node: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 5952156843105137928\r\nphysical_device_desc: \"device: 0, name: Tesla K40m, pci bus id: 0000:84:00.0, compute capability: 3.5\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 17355483587532015042\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n]_\r\n\r\nI am wondering, isn't this possible with TensorFlow 2?\r\n\r\nThanks!", "@gauravmunjal13,\r\nThe `cuda libs` section from the environment settings you've provided is blank. The output should be similar to\r\n\r\n```\r\n== cuda libs  ===================================================\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart-1b201d85.so.10.1\r\n/usr/local/lib/python3.6/dist-packages/torch/lib/libcudart-1b201d85.so.10.1\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.243\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.1/doc/man/man7/libcudart.7\r\n```\r\nAttached [gist](https://colab.research.google.com/gist/amahendrakar/a3d9832cb510347cadde7ac37d5f7d37/41374.ipynb) for reference. \r\n\r\n\r\nPlease take a look at the [tested build configuration](https://www.tensorflow.org/install/source#gpu) and check if you are running the compatible CUDA and cuDNN versions. Thanks!", "@amahendrakar \r\nThanks for your response!\r\n\r\nBased on shared tested build configuration link, it does mention that TF 2.2.0 matches with CUDA 10.1. However, as stated in my query I am using Tesla K40m with Driver Version: 440.31 and CUDA Version: 10.2.\r\n\r\nHowever, now I am using TF 1.14.0 which can successfully detect this GPU. \r\n\r\nRegarding the cuda libs, I tried again but sadly the cuda libs are not captured again via that script. I am working on a high performing computing cluster, thus it requires submitting job to access GPU. But I could see that via running:\r\n`tf.test.is_gpu_available()`\r\n_2020-07-15 21:56:52.773131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:84:00.0\r\n2020-07-15 21:56:52.773189: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-15 21:56:52.773206: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2020-07-15 21:56:52.773221: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2020-07-15 21:56:52.773234: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2020-07-15 21:56:52.773248: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-07-15 21:56:52.773261: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-07-15 21:56:52.773275: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-15 21:56:52.774936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-07-15 21:56:52.774990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-15 21:56:52.775016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2020-07-15 21:56:52.775028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2020-07-15 21:56:52.776741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 10791 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:84:00.0, compute capability: 3.5)\r\nTrue_\r\n\r\n**Note** These results are with TF 1.14.0. \r\n\r\nThanks!", "> However, now I am using TF 1.14.0 which can successfully detect this GPU.\r\n\r\n@gauravmunjal13,\r\nThank you for the update. Do you want to close this issue or find out the cause of the GPU inconsistency?", "@amahendrakar \r\nI have moved on with TF 1.14.0 due to project urgency. However, if it's possible to work with TF 2.2.0 on CUDA 10.2, I would like to know what shall I do to make it work.\r\nThanks for your help!", "@gauravmunjal13,\r\nPlease check [this comment](https://github.com/tensorflow/tensorflow/issues/38194#issuecomment-657233766) from a similar issue and let us know if it helps.\r\n\r\nAlso, could you please specify the cuDNN version you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41374\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41374\">No</a>\n", "I am also facing the same issue with Tensorflow2.2 and Cuda-10.1. PyTorch is able to detect and utilize GPU, but tensorflow is not.", "@kaustubholpadkar,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "> I am also facing the same issue with Tensorflow2.2 and Cuda-10.1. PyTorch is able to detect and utilize GPU, but tensorflow is not.\r\n\r\nI also faced this problem when installing TF2.2 via `conda`, I uninstalled and reinstalled with `pip` and it worked."]}, {"number": 41372, "title": "freezing the app(.py to exe)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):  source\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI have tried to freeze the custom app with auto-py-to-exe and it is giving mw the error of \"ImportError: No module named 'tensorflow_core.python'\"\r\n\r\ncan anyone please help me with it\r\n\r\nThanks in advance\r\n![tf_error](https://user-images.githubusercontent.com/61754297/87411293-f317f080-c5e0-11ea-99d8-64a6914ace7c.PNG)\r\n", "comments": ["@daynial132 \r\n\r\ntensorflow_core only exists in 1.15, 2.x.\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "I have created a virtual environment  in anaconda on 3.5 python \r\ninstalled the packages specifically used for the my app\r\nthen installed the auto-py-to-exe and run it\r\nand finally include the script to script location used one direction option,\r\nin additional files  i added complete folder to it and all the files in the folder \r\nin advance option on runtime-hooks-dir  I added complete site-packages\r\n\r\nthats it. and press the convert button on auto-py-to-exe", "have i done something wrong ?? \r\nor missing some thing", "@daynial132 This is a conversion issue, not the tensorflow issue itself. It would be better if you post this issue in stackoverflow where there is a wider community to respond. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41371, "title": "Patch to core for RFC: Sparse Domain Isolation for Supporting Large-scale Sparse Weights", "body": "This is a patch to core for RFC: Sparse Domain Isolation for Supporting large-scale Sparse Weight\r\nPlease visit https://github.com/tensorflow/community/pull/237", "comments": ["@yuefengz @tanzhenyu @byronyi @alextp Hi, this is the code patch for RFC of [Sparse Domain Isolation for Supporting large-scale Sparse Weights Training ](https://github.com/tensorflow/community/pull/237), and hope you have time to help review, thank you!", "follow", "AFAICT it's possible to have the dynamic embedding ops entirely in a third-party repo, and we just need the trainable interface part of this PR to be in core TF. Is that true?", "> AFAICT it's possible to have the dynamic embedding ops entirely in a third-party repo, and we just need the trainable interface part of this PR to be in core TF. Is that true?\r\n\r\nThat is true -- and I have the same comments. Let's keep what needs to be changed in lookup table, and leave the rest of it in the new SIG/repo", "@alextp @tanzhenyu Yes, we need the trainable interface part of this PR to be in core especially the part of `optimizer.py` and `TrainableWrapper` which are the key to compatibility with all native optimizers without requiring extend them one by one, and I believe it's inappropriate and very difficult to be spilt them out in design considerations.", "Is there any corresponding patch in TF-Serving ? How to deal with the problem of huge memory used by hash table on a single machine ?", "@lilao might have better ideas on this for TF Serving. \r\n\r\n> Is there any corresponding patch in TF-Serving ? How to deal with the problem of huge memory used by hash table on a single machine ?", "This feature will be helpful for our recommend system, hope to use as soon as possible, follow.", "@rhdong Can you please resolve conflicts? Thanks!", "> @rhdong Can you please resolve conflicts? Thanks!\r\n\r\nYes, I will resolve them.", "@rhdong  Any update on this PR? Please. Thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41371) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41371) for more info**.\n\n<!-- need_author_consent -->", "There are several conflicts. Can you solve them, please?", "@rhdong Can you please resolve conflicts? Thanks!", "@rhdong  Can you please resolve conflicts? Thanks!", "@rhdong  Any update on this PR? Please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Looks like the author will continue the work on [recommenders-addons](https://github.com/tensorflow/recommenders-addons) project."]}, {"number": 41370, "title": "tf.math.acos raises UnimplementedError for complex tensors", "body": "**System information**\r\n- OS Platform and Distribution: Windows10 1909\r\n- TensorFlow installed from : binary\r\n- TensorFlow version: 2.2.0rc2\r\n- Python version: 3.8.0\r\n- CUDA/cuDNN version: None\r\n\r\n**Describe the current behavior**\r\nThe documentation says complex inputs are allowed, but tf.math.acos and tf.math.asin raises UnimplementedError for complex64 or complex128 inputs.\r\nI found that tf.math.acos and tf.math.asin use Atan2 which do not support complex inputs. So these ops may need a new implement without Atan2 when the input is complex.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python3\r\nimport tensorflow as tf \r\na = tf.constant([1j], dtype=tf.complex64) \r\nprint(tf.math.acos(a))\r\n```\r\n**Describe the expected behavior**\r\n```\r\ntf.Tensor([1.5708-0.88137j], shape=(1,), dtype=complex64)\r\n```\r\n**Output**\r\n```\r\n2020-07-14 15:50:38.372937: W tensorflow/core/framework/op_kernel.cc:1752] OP_REQUIRES failed at xla_compile_on_demand_op.cc:216 : Unimplemented: binary complex op 'atan2'\r\nTraceback (most recent call last):\r\n  File \"acos_err.py\", line 15, in <module>\r\n    tf.math.acos(a)\r\n  File \"C:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 193, in acos\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnimplementedError: binary complex op 'atan2' [Op:Acos]\r\n```", "comments": ["Was able to reproduce the issue with TF v2.2, TF v2.3.0rc1 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/dcb5cf2d4a0f50849bbab85534c76781/41370-tf-nightly.ipynb). Thanks!", "I think we need a PR to add functionality there.", "@jstzwj Are you interested to raise a PR? Thanks!", "The issue is actually that  `Acos` is not covered in `XLA` for complex, thus triggered the incorrect result. Added a PR #41386 to add XLA complex support of `Acos` for the fix.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "The PR #41386 is pending and is awaiting for review.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41370\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41370\">No</a>\n", "So, I am having this issue, was this solved?"]}, {"number": 41369, "title": "bazel build tensorflow failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary):  \r\n- TensorFlow version: latest version\r\n- Python version: \r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory:\r\n\r\n\r\nI want to use docker build a tensorflow-gpu container, Here is my Dockerfile:\r\n\r\n\r\n`FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\r\n\r\nRUN apt-get update\r\n\r\nRUN apt-get install -y wget \\\r\n                vim \\\r\n                cmake\r\n\r\n\r\nRUN apt-get -y install \\\r\n    python3 \\\r\n    python3-pip \\\r\n    python3-setuptools`\r\n\r\nand build command is that:\r\n\r\ndocker build -t tensorlfow_gpu -f Dockerfile .\r\n\r\ndocker run -d --name tensorflow-gpu-demo -v ***/tensorflow:tensorflow tensorflow_gpu\r\n\r\n**Describe the problem**\r\n\r\nAnd I login the container \r\ninstall bazel3.1 and some require tag and start build tensorflow\r\n\r\nThen I get these errors:\r\n\r\nC++ compilation of rule '//tensorflow/core/kernels:determinant_op_gpu' failed (Exit 1)\r\n\r\nC++ compilation of rule '//tensorflow/core/util:einsum_op_util' failed (Killed)\r\n\r\nC++ compilation of rule '//tensorflow/core/kernels:roll_op_gpu' failed (Exit 1)\r\n\r\nI found every time the error raise, the error is different, What should I do to compile tensorflow successful.\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Julius-ZCJ \r\nPlease confirm if you followed instructions from [TensorFlow](https://www.tensorflow.org/install/source) website .\r\nCan you  check this link with same error and update:\r\n#37430 [link](https://github.com/tensorflow/tensorflow/issues/39333#issuecomment-647012051) [link1](https://developer.codeplay.com/products/computecpp/ce/guides/tensorflow-guide/tensorflow-native-compilation?) ", "Please include an actionable error message, nut just the file where the compilation stopped.\r\n\r\nCould it be that you ran out of memory during compile?", "@Saduf2019 \r\nI see the link you provide, but my error is not same as them.", "@Julius-ZCJ\r\nPlease update as per comment from mihaimaruseac.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41369\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41369\">No</a>\n"]}, {"number": 41367, "title": "\"ImportError: DLL load failed: The specified module could not be found.\" Windows10 issue with Tensorflow-2.2.0", "body": "I have installed Tensorflow 2.2.0 on my Win10 PC:\r\n\r\n**System information**\r\n- Windows 10 \r\n- Tensorflow installed in Anaconda using pip\r\n- Python Version : 3.7.6\r\n- Graphics Card: NVIDIA GTX1650 4GB\r\n- Processor: Intel core i5 9th-Gen\r\n\r\nI have been facing this issue since a long time. So i uninstalled Anaconda and re-installed it again but the issue still persists\\\r\n\r\n`ImportError                               Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-4282574fb12a> in <module>\r\n      1 import numpy as np\r\n----> 2 import tensorflow as tf\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n\r\n\r\nKindly help to resolve this.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@anshpujara14,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You you running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204.\r\n\r\nThanks!", "Thanks for your response. I found out that I was using TF version that was not compatible with my Graphics Card. So i downgraded to Tensorflow 2.0 and now it works fine.\r\n\r\nSo now I am closing this thread.\r\nThanks for the support", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41367\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41367\">No</a>\n"]}, {"number": 41366, "title": "pip install tf-nightly (macOS) installs build dated 20200610", "body": "**System information**\r\n- OS Platform: macOS Catalina 13.1.1 (15609.2.9.1.2)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: tf-nightly\r\n- Python version: 3.8.3 (default, Jul  8 2020, 14:27:55) \r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nInstalling `tf-nightly` installs a old build dated from mid-June.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n~: pip3 install tf-nightly\r\nCollecting tf-nightly\r\n  Downloading tf_nightly-2.3.0.dev20200610-cp38-cp38-macosx_10_14_x86_64.whl (165.4 MB)\r\n```\r\n", "comments": ["@lutzroeder \r\nPlease follow below command and let us know:\r\npip install tf-nightly==2.4.0.dev20200712\r\n\r\nor download the whl file from here:\r\nhttps://pypi.org/project/tf-nightly/2.4.0.dev20200712/#files\r\n", "@Saduf2019 \r\n```\r\n~: pip install tf-nightly==2.4.0.dev20200712\r\nERROR: Could not find a version that satisfies the requirement tf-nightly==2.4.0.dev20200712 (from versions: 2.3.0.dev20200605, 2.3.0.dev20200608, 2.3.0.dev20200609, 2.3.0.dev20200610)\r\nERROR: No matching distribution found for tf-nightly==2.4.0.dev20200712\r\n```\r\n", "@lutzroeder \r\nPlease upgrade pip and try again, can you refer to [this comment](https://github.com/tensorflow/tensorflow/issues/27470#issuecomment-600504369) and install \"pip install tf-nightly\"  and confirm.", "Pip is latest. The issue seems to be that `brew` is now on Python 3.8. `tf-nightly` has only been building 3.7 since June 10?\r\n\r\n```\r\n~: pip install --upgrade pip\r\nRequirement already up-to-date: pip in /usr/local/lib/python3.8/site-packages (20.1.1)\r\n~: pip install tf-nightly==2.4.0.dev20200712\r\nERROR: Could not find a version that satisfies the requirement tf-nightly==2.4.0.dev20200712 (from versions: 2.3.0.dev20200605, 2.3.0.dev20200608, 2.3.0.dev20200609, 2.3.0.dev20200610)\r\nERROR: No matching distribution found for tf-nightly==2.4.0.dev20200712\r\n```\r\n\r\n@goldiegadde ", "Apparently we had a few missed nightlies. The [most recent ones](https://pypi.org/project/tf-nightly/2.4.0.dev20200715/#files) should support all 3 operating systems and all pythons, except py38 on macos.\r\n\r\nThe last py38 on macos [has been released successfully on 6/10](https://pypi.org/project/tf-nightly/2.4.0.dev20200715/#files)\r\n\r\nWe will investigate why there is no recent macos/py38 release and fix.", "During a refactoring we accidentally stopped the job that was doing this particular release. We triggered the job manually and made the needed changes so that from tomorrow the job will run at the proper time.\r\n\r\nApologies again for the breakage and thanks for pointing this out", "[Today's pips](https://pypi.org/project/tf-nightly/2.4.0.dev20200716/#files) include macos py38 too. This seems solved now, but please reopen if there are still issues.\r\n\r\nThank you for pointing out the error and for debugging.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41366\">No</a>\n"]}, {"number": 41365, "title": "Support fill_value for ImageProjectiveTransform", "body": "As per https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py#L908.\r\n\r\ncc @tanzhenyu", "comments": ["@WindQAQ If you'd like to contribute more, I'd encourage you to implement the TPU kernel for this as well, this will give us the full spectrum", "Hi @tanzhenyu, I'll try it. If I understand correctly, I can add a XLA op in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/image_ops.cc, right? Thank you for the review :-)", "> Hi @tanzhenyu, I'll try it. If I understand correctly, I can add a XLA op in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/image_ops.cc, right? Thank you for the review :-)\r\n\r\nYes that is correct", "> > Hi @tanzhenyu, I'll try it. If I understand correctly, I can add a XLA op in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/image_ops.cc, right? Thank you for the review :-)\r\n> \r\n> Yes that is correct\r\n\r\nGot it, will do it in next PR.", "- Fix typo\r\n- Add documentation\r\n- Update golden API", "> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops.py#L242 also needs to be modified to register a gradient wrt the fill value for the v3 op\r\n\r\nThanks for the reminder! I am aware of it, will do it real quick :-)", "@alextp can you approve it again? Just update some machine generated files. Thanks!", "Hi @alextp, is there any update on this? I've prepared an XLA kernel for `ImageProjectiveTransform` as per @tanzhenyu's comment and wanna register on V3 directly. Thank you!", "@gbaned what's the status of merging this internally?", "> @gbaned what's the status of merging this internally?\r\n\r\n@alextp This is waiting for internal merge. \r\nCC @tanzhenyu  Any update on this? Please. Thanks!", "> @gbaned what's the status of merging this internally?\r\n\r\nI think per our discussion, there is an issue with `pywarp_gradient_exclusion.cc` -- this seems a typo to me so it's messing up with the correct one, which is `pywrap_gradient_exclusions.cc`", "> > @gbaned what's the status of merging this internally?\r\n> \r\n> I think per our discussion, there is an issue with `pywarp_gradient_exclusion.cc` -- this seems a typo to me so it's messing up with the correct one, which is `pywrap_gradient_exclusions.cc`\r\n\r\nUmm, that's weird, seems that it's a machine generated file. I can delete it anyway.", "> > > @gbaned what's the status of merging this internally?\r\n> > \r\n> > \r\n> > I think per our discussion, there is an issue with `pywarp_gradient_exclusion.cc` -- this seems a typo to me so it's messing up with the correct one, which is `pywrap_gradient_exclusions.cc`\r\n> \r\n> Umm, that's weird, seems that it's a machine generated file. I can delete it anyway.\r\n\r\nWhat kind of cli you run to generate that file?", "> What kind of cli you run to generate that file?\r\n\r\nI forget about it. I remember I saw the CI emit some errors and I followed the instructions on it. It might be something wrong on my part.", "CI still passes after deleting that file. Sorry that it's probably my fault.\r\n\r\n@tanzhenyu and @alextp Can you review this again when time allows? Thank you!", "Would you mind checking-in (or at least help) with this as well? https://github.com/tensorflow/tensorflow/pull/42077", "> Would you mind checking-in (or at least help) with this as well? #42077\r\n\r\nI can take over it in this PR too if the author does mind. Let me ask him first.", "Hi @tanzhenyu, could you review it again when time allows? I also fix #42077.", "> Thanks for the PR!\r\n> Does it resolve the dashed line problem?\r\n\r\nyeah according to https://github.com/tensorflow/tensorflow/pull/42077#issuecomment-670958925\r\n\r\nto be honest I haven't tried his code though... but it seems his fix is better than mine", "> > Thanks for the PR!\r\n> > Does it resolve the dashed line problem?\r\n> \r\n> yeah according to [#42077 (comment)](https://github.com/tensorflow/tensorflow/pull/42077#issuecomment-670958925)\r\n> \r\n> to be honest I haven't tried his code though... but it seems his fix is better than mine\r\n\r\nGreat. I will approve once @WindQAQ moved `fill_value` to the end of arg list.", "@WindQAQ  Can you please resolve conflicts? Thanks!", "Sorry if I'm asking this.... but.... I'm wondering when this will get merged?\r\n\r\nThanks", "> Sorry if I'm asking this.... but.... I'm wondering when this will get merged?\r\n> \r\n> Thanks\r\n\r\nMy guess would be today", "> > Sorry if I'm asking this.... but.... I'm wondering when this will get merged?\r\n> > Thanks\r\n> \r\n> My guess would be today\r\n\r\nit seems.... it takes longer than expected?"]}, {"number": 41364, "title": "how to build tflite2.3 on windows", "body": "how to build tflite2.3 on windows and mac", "comments": ["@Tomhouxin \r\n\r\nCan you please go through this[ link ](https://github.com/tensorflow/tensorflow/issues/37899#issuecomment-607623498)and see if it helps you.Thanks!", "@ravikyram \r\nOh! \r\nbazel build -c opt //tensorflow/lite:tensorflowlite\r\nI get tensorflowlite.dll(16.8M), But how to distinguish the Debug and Release, x86 or x64\r\n", "\"-c opt\" is for release.\r\n\"-c dbg\" for debug build.\r\n\r\nIf your workstation is x64, bazel generates a x64 binary by default.\r\n\"--cpu=x86\" option is needed to build x86 binary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41364\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41364\">No</a>\n", "@terryheo ok, thank you, good job"]}]