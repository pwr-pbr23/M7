[{"number": 29997, "title": "libtensorflow_jni.so: libcudnn.so.7: cannot open shared object file: No such file or directory", "body": "tensorflow version=1.8.0\r\njava=1.8.0\r\ncuda=9.1\r\ncudnn=7\r\n\r\n**I used java to call the GPU when I reported the following error:**\r\n\r\njava.lang.UnsatisfiedLinkError: /tomcat/temp/tensorflow_native_libraries-1561013286760-0/libtensorflow_jni.so: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n\r\n```\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>tensorflow</artifactId>\r\n            <exclusions>\r\n                <exclusion>\r\n                    <groupId>org.tensorflow</groupId>\r\n                    <artifactId>libtensorflow_jni</artifactId>\r\n                </exclusion>\r\n            </exclusions>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>libtensorflow_jni_gpu</artifactId>\r\n        </dependency>\r\n```\r\n\r\nwhen delete as follows. This program prompts that the GPU cannot be found, forcing the CPU\r\n```\r\n            <exclusions>\r\n                <exclusion>\r\n                    <groupId>org.tensorflow</groupId>\r\n                    <artifactId>libtensorflow_jni</artifactId>\r\n                </exclusion>\r\n            </exclusions>\r\n```\r\nCan you help me? Thanks a lot.\r\n\r\n", "comments": ["Just to verify are you trying to build TensorFlow from source from steps mentioned in [TensorFlow website](https://www.tensorflow.org/install/source). Also let us know on which platform you are seeing the issue. Thanks!", "Thank you for you reply. I use tf in java 1.8 centos7.", "or, Can ask\uff1fHow to configure pom TensorFlow under GPU? The above methods have no effect.", "Hi,\r\nit seems like the cudnn library is not on your library path. You can start your application with the environment variable `LD_LIBRARY_PATH=/path/to/cuda/lib64/:/path/to/cudnn/lib64` or you could add the path to your global library path like described [here](https://www.cyberciti.biz/faq/linux-setting-changing-library-path/).", "Thank you, In fact, this path is configured.\r\nFinally, I did a lot of experiments and proved that this is ok.\r\n```\r\n <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>tensorflow</artifactId>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>libtensorflow_jni_gpu</artifactId>\r\n        </dependency>\r\n```", "install cudnn \r\nCan solve the problem", "thank you, I had solved this problem. and cudnn had installed originally.", "> thank you, I had solved this problem. and cudnn had installed originally.\r\n\r\nhow you have solved the above problem can you explain the procedure..", "and, only use gpu version tensorflow jar.", "> Thank you, In fact, this path is configured.\r\n> Finally, I did a lot of experiments and proved that this is ok.\r\n> \r\n> ```\r\n>  <dependency>\r\n>             <groupId>org.tensorflow</groupId>\r\n>             <artifactId>tensorflow</artifactId>\r\n>         </dependency>\r\n>         <dependency>\r\n>             <groupId>org.tensorflow</groupId>\r\n>             <artifactId>libtensorflow_jni_gpu</artifactId>\r\n>         </dependency>\r\n> ```\r\n\r\nCan the gpu be used in this way?", "yes."]}, {"number": 29996, "title": "TF2 - apparent memory leak when running dataset ops eagerly", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX\r\n- TensorFlow installed from (source or binary): 2.0.0beta\r\n- TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nWhen using the function `tf.autograph.to_graph`, I see a memory leak which I don't see if I use the annotation `@tf.function`\r\n\r\n**Describe the expected behavior**\r\nThere should not be a memory leak.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os\r\nimport psutil\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nprocess = psutil.Process(os.getpid())\r\n\r\n\r\n# @tf.function\r\ndef train_epoch(model, p_data):\r\n    for real_inputs in p_data:\r\n        model * real_inputs\r\n\r\n\r\ntrain_epoch = tf.autograph.to_graph(train_epoch)\r\n\r\ndata = np.random.normal(0., 1., [10000, 2])\r\np_data = tf.data.Dataset.from_tensor_slices(data).batch(32)\r\n\r\nmodel = tf.Variable([1., 1.], dtype=tf.float64)\r\n\r\nfor i in range(5000):\r\n    train_epoch(model, p_data)\r\n\r\n    if i % 50 == 0:\r\n        print(process.memory_info().rss)\r\n```\r\n", "comments": ["I have tried on Colab with TF version 2.0beta and was able to reproduce the issue. ", "I did a few tests. In short, this seems to be related to `Dataset.reduce` (which is what is generated in this case).\r\n\r\nThe leak reproduces even without `autograph.to_code`, if we run the code that would be effectively generated:\r\n\r\n```\r\nimport os\r\nimport psutil\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\ntf = tf.compat.v2\r\ntf.enable_v2_behavior()\r\n\r\nprocess = psutil.Process(os.getpid())\r\n\r\n\r\n# @tf.function\r\ndef train_epoch(model, p_data):\r\n\r\n  def reduce_func(_, real_inputs):\r\n    model * real_inputs\r\n    return (tf.constant(0),)  # These tf.constant(0) are dummy variables - reduce requires at least one var.\r\n\r\n  p_data.reduce(((tf.constant(0),), ()), reduce_func)\r\n\r\n\r\n# train_epoch = tf.autograph.to_graph(train_epoch)\r\n\r\ndata = np.random.normal(0., 1., [10000, 2])\r\np_data = tf.data.Dataset.from_tensor_slices(data).batch(32)\r\n\r\nmodel = tf.Variable([1., 1.], dtype=tf.float64)\r\n\r\nfor i in range(5000):\r\n    train_epoch(model, p_data)\r\n\r\n    if i % 50 == 0:\r\n        print(process.memory_info().rss)\r\n```\r\n\r\nThe leak only seems to happen for datasets - if we replace `p_data` with reshaped version of `data` (in effect causing the for loop to run as a `tf.while_loop`), the leak doesn't reproduce:\r\n\r\n```\r\nimport os\r\nimport psutil\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\ntf = tf.compat.v2\r\ntf.enable_v2_behavior()\r\n\r\nprocess = psutil.Process(os.getpid())\r\n\r\n\r\n# @tf.function\r\ndef train_epoch(model, data):\r\n    for real_inputs in data:\r\n        model * real_inputs\r\n\r\n\r\ntrain_epoch = tf.autograph.to_graph(train_epoch)\r\n\r\ndata = np.random.normal(0., 1., [10016, 2])  # Fit batches evenly\r\ndata = data.reshape([313, 32, 2])\r\n# p_data = tf.data.Dataset.from_tensor_slices(data).batch(32)\r\n\r\nmodel = tf.Variable([1., 1.], dtype=tf.float64)\r\n\r\nfor i in range(5000):\r\n    train_epoch(model, data)\r\n\r\n    if i % 50 == 0:\r\n        print(process.memory_info().rss)\r\n```", "@jsimsa any insight into this?", "Jiri and I spoke offline. For technical reasons, `Dataset.reduce` re-traces its function whenever called during Eager execution. This in effect adds a new trace to the default graph, which accounts for the increase in memory use.\r\n\r\nFor this reason, we recommend caution when iterating over Datasets repeatedly in Eager execution, even with AutoGraph. We recommend using `tf.function`, as in increases performance and avoids these memory build-ups. `tf.function` also applies AutoGraph, so you won't need to call it separately.\r\n\r\n```\r\n@tf.function\r\ndef train_epoch(model, p_data):\r\n    for real_inputs in p_data:\r\n        model * real_inputs\r\n\r\nfor i in range(5000):\r\n    train_epoch(model, p_data)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29996\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29996\">No</a>\n", "@mdanatg  Is there any other way than wrapping the iteration of a dataset into a tf.function. My custom training loops need eager tensors to use the tf.summary API. Can I manually free the memory from the tf.dataset? \r\n\r\nFurthermore, this should be noted in the [documentation](https://www.tensorflow.org/beta/tutorials/eager/custom_training_walkthrough#training_loop), since it iterates several times of the dataset while being in eager mode.", "@jaingaurav I agree that it's worth noting it in the docs somewhere.\r\n\r\n@2649 as far as I know, the tf.summary API should work inside tf.function as well - unless you're perhaps encountering a different bug? Unfortunately I don't know of a way to manually free up the memory. @jsimsa might know of a way.", "@mdanatg No it is not really a bug. I have some numpy operations in my summary calls, which do not work in tf.function. But I just changed them to \u00b4tf\u00b4 operations. It is just a comfort problem.", "Ok, the memory leak has a higher impact on my custom training loops than I expected. For everyone who needs a quick fix:  You can disable trace (`tf.summary.trace_off()`) before you iterate over a dataset and enable it afterwards (`tf.summary.trace_on() `).  This worked in my case.\r\n\r\n@mdanatg Do you see any problems with this workaround?\r\n\r\n", "It's hard to make a in the general case, but I don't see a problem with disabling trace, if it's not needed.", "Thanks @2649 for proposing this quick fix. I wonder if you could provide some intuitions/explanations as to why this worked?\r\n\r\nChecking as this doesn't seem to work for me.\r\n\r\nBTW, I strongly think this problem should at least be mentioned (if not fixed) in all of the official docs whenever iterating a dataset happens."]}, {"number": 29995, "title": "does the java api have used mkl to get more performance on cpu?", "body": "does the java api have used mkl to get more performance on cpu?", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 29994, "title": "Fix broken LaTeX in documentation for tf.keras.optimizers.Adam", "body": "This PR should fix this:\r\n\r\n![image](https://user-images.githubusercontent.com/4061736/59825594-8fdd6f80-9388-11e9-83f1-160912b7c0d5.png)\r\n\r\nIt's a tiny change, but I thought I may as well just go ahead and fix it.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29994) for more info**.\n\n<!-- need_sender_cla -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29994) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29994) for more info**.\n\n<!-- ok -->", "Oh my god, I have no idea why GitHub requested reviews from all of you after I rebased this branch to be on master. I'm very sorry \ud83d\ude05"]}, {"number": 29993, "title": "Clean up slice documentation", "body": "1. It seems like `input_` is used instead of `input` because `input` is a built-in function.  This PR fixes the documentation by replacing `input` with `input_` in the docstring.\r\n\r\n2. Remove a potentially superfluous `'`, changing `the 'i'th dimension` to `the i'th dimension`.", "comments": []}, {"number": 29992, "title": " module 'tensorflow' has no attribute 'init_scope'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@mabodx In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I meet the same mistake,When I execute ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/pipeline.config and the relevant model", "same error when I execute \r\npython train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_coco.config\r\n\r\n\r\nTensorflow-GPU version 1.9.0\r\nCUDA 10.0\r\n", "@oysz2016  and @VitalieStirbu  Can you please post a new issue by providing the information asked by the template?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!", "@gadagashwini  \r\nhttps://github.com/tensorflow/tensorflow/issues/30810\r\n", "how did u solve >\r\n"]}, {"number": 29991, "title": "Fix pluralization typo", "body": "`s/pos_weights/pos_weight/`, as it's `pos_weight` in the signature.", "comments": []}, {"number": 29990, "title": "TensorflowJS converter cannot convert tf_saved_model to tfjs_model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS \r\n- TensorFlow installed from (source or binary): binray\r\n- TensorFlow version (use command below): 1.14.0\r\n- TensorFlowJS version: 1.2.1\r\n- Python version: 3.6\r\n\r\nWhen I using `tensorflowjs_converter` to convert a `tf_saved_model` to `tfjs_graph_model` it gives the errors as below:\r\n\r\n```\r\n2019-06-20 10:05:21.636174: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-06-20 10:05:21.636291: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-06-20 10:05:21.636529: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-20 10:05:21.648066: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-06-20 10:05:21.648092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n2019-06-20 10:05:21.648098: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nTraceback (most recent call last):\r\n  File \"~/anaconda3/bin/tensorflowjs_converter\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/converter.py\", line 556, in main\r\n    strip_debug_ops=FLAGS.strip_debug_ops)\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py\", line 285, in convert_tf_saved_model\r\n    concrete_func)\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 203, in convert_variables_to_constants_v2\r\n    tensor_util.make_tensor_proto(value))\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 466, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 368, in _AssertCompatible\r\n    raise TypeError(\"List of Tensors when single Tensor expected\")\r\nTypeError: List of Tensors when single Tensor expected\r\n```\r\n\r\nAfter debugging the source, I found that the inputs for `_AssertCompatible(values, dtype)` are: `values=Tensor(\"StatefulPartitionedCall_1:0\", shape=(768, 1024), dtype=float32)` and `dtype=None`\r\n\r\nThe comments in the source indicate that the type of `values` is not supported!\r\n\r\n\r\n", "comments": ["I located the problem in the file`convert_to_constants.py` \r\n```\r\n  # Add identity node after the reference variable and get the tensor values\r\n  # for them.\r\n  if reference_variables:\r\n    reference_variable_tensors = []\r\n    with func.graph.as_default():\r\n      for node in reference_variables:\r\n        identity_node = array_ops.identity(\r\n            func.graph.as_graph_element(node.name + \":0\"))\r\n        reference_variable_tensors.append(identity_node.name)\r\n\r\n    reference_variable_values = func.prune([], reference_variable_tensors)()\r\n\r\n    # Add values of reference variables as constant nodes.\r\n    for node, value in zip(reference_variables, reference_variable_values):\r\n      output_node = output_graph_def.node.add()\r\n      dtype = attr_value_pb2.AttrValue()\r\n      dtype.type = value.dtype.as_datatype_enum\r\n\r\n      output_node.op = \"Const\"\r\n      output_node.name = node.name\r\n      output_node.attr[\"dtype\"].CopyFrom(dtype)\r\n      output_node.attr[\"value\"].tensor.CopyFrom(\r\n          tensor_util.make_tensor_proto(value))\r\n      how_many_converted += 1\r\n\r\n```\r\n`tensor_util.make_tensor_proto` only accepts values of types (a python scalar, a python list, a\r\n  numpy ndarray, or a numpy scalar) but this line code `output_node.attr[\"value\"].tensor.CopyFrom(\r\n          tensor_util.make_tensor_proto(value))` gives the value a type of `Tensor`!\r\nHow to solve this problem? ", "What's the available version of `tensorflow` for `tensorflowjs` ?  \r\nI tried `2.0`  `1.14` and `1.13` all are failed~\r\nSO FRUSTRATED!", "***SOLUTION***   SEE ISSUE [#29956](https://github.com/tensorflow/tensorflow/issues/29956)", "Currently, **only** `tf-nightly-2.0-preview` is compatible with `tensorflowjs==1.2.1`.\r\nif you meet the errors as in ISSUE [#29956](https://github.com/tensorflow/tensorflow/issues/29956), just fix it as it suggested."]}, {"number": 29989, "title": "Segmentation fault when saving checkpoints with saveable Dataset Iterator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.6.1810\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n- tf.version: 'v1.13.1-0-g6612da8951' 1.13.1\r\n\r\n**Describe the current behavior**\r\n\r\nSegmentation fault in saving an initializable dataset iterator when entering the tf.train.MonitoredSession context manager.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe initializable iterator is saved and restored properly, behaving the same with the one shot iterator.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\"\"\"Illustrate saveable dataset iterator\r\n\"\"\"\r\nimport tensorflow as tf\r\n\r\nDATASET_SIZE = 4\r\nSAVE_STEPS = 2\r\nTRAIN_STEP = 3\r\nCHECKPOINT_DIR = '/tmp/tf_dataset_saveable'\r\n\r\ndef test_saveable():\r\n    \"\"\"test saveable\"\"\"\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        dataset = tf.data.Dataset.range(DATASET_SIZE).repeat()\r\n#        dataset_iterator = dataset.make_one_shot_iterator()\r\n        dataset_iterator = dataset.make_initializable_iterator()\r\n        dataset_init = dataset_iterator.initializer\r\n        data = dataset_iterator.get_next()\r\n\r\n        saveable = tf.contrib.data.make_saveable_from_iterator(dataset_iterator)\r\n        tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\r\n\r\n        global_step = tf.train.get_or_create_global_step()\r\n        inc_global_step = tf.assign_add(global_step, 1)  # critical\r\n\r\n        saver = tf.train.Saver()\r\n        checkpoint_dir = CHECKPOINT_DIR\r\n        scaffold = tf.train.Scaffold(saver=saver)\r\n        checkpoint_hook = tf.train.CheckpointSaverHook(\r\n            checkpoint_dir=checkpoint_dir,\r\n            save_steps=SAVE_STEPS, scaffold=scaffold)\r\n\r\n        hooks = [checkpoint_hook]\r\n        session_creator = tf.train.ChiefSessionCreator(\r\n            scaffold=scaffold, checkpoint_dir=checkpoint_dir)\r\n        with tf.train.MonitoredSession(\r\n                session_creator=session_creator, hooks=hooks) as mon_sess:\r\n            gstep = mon_sess.run(global_step)\r\n            if not gstep:\r\n                mon_sess.run(dataset_init)\r\n            for _ in range(TRAIN_STEP):\r\n                print(mon_sess.run([global_step, data]))\r\n                mon_sess.run(inc_global_step)\r\n\r\nif __name__ == '__main__':\r\n    test_saveable()\r\n```\r\n**Other info / logs**\r\n```console\r\n(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python tf_dataset_saveable.py \r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From tf_dataset_saveable.py:20: make_saveable_from_iterator (from tensorflow.contrib.data.python.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.experimental.make_saveable_from_iterator(...)`.\r\n2019-06-20 10:43:20.947675: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-20 10:43:20.951984: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-06-20 10:43:20.952497: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3f10ca0 executing computations on platform Host. Devices:\r\n2019-06-20 10:43:20.952539: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nSegmentation fault (core dumped)\r\n(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ \r\n```", "comments": ["I tried on colab with Tensorflow 1.13.1. I am able to reproduce the issue. ", "allenl@ -- not sure if these APIs are expected to work all together. Can you advise as to whether there's a better way?", "Oops. Actually tagging @allenlavoie ", "Is the issue that the iterator isn't initialized when the first checkpoint is written? You may need to get MonitoredTrainingSession to do the initialization so it happens before saving. Otherwise the APIs should go fine together AFAIK.\r\n\r\nEither way, it probably shouldn't segfault. @saxenasaurabh may be more familiar with the serialization op itself.", "@allenlavoie is correct, you need to run the `dataset_iterator.initializer` before you can save the iterator.\r\n\r\nI am going to create a change that will produce an informative error message for this case (as opposed to segfaulting).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29989\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29989\">No</a>\n", "Many thanks to the fruitful discussions. Adding dataset initializer to the collection TABLE_INITIALIZERS fixes the segfaulting. Nevertheless, the iterator starts from the very beginning of the dataset every time after recovering from the checkpoint.\r\nIs there any solution to the problem at present? Maybe something like init_fn in tf.train.Scaffold ...", "That should not be the case. The whole point of saving the iterator is that you can checkpoint its state (and we have tests that verify that this functionality works).\r\n\r\nPlease create a new issue with instructions on how to reproduce your issue so that someone can investigate why is your program resetting the iterator state.", "I think this is a missing feature. We added [CheckpointInputPipelineHook](https://www.tensorflow.org/api_docs/python/tf/contrib/data/CheckpointInputPipelineHook) to fix exactly this [problem](https://github.com/tensorflow/tensorflow/blob/5e2a91f65cfb23f996136b9201d9312c9c36b941/tensorflow/python/data/experimental/ops/iterator_ops.py#L194). However, it requires an estimator as an arg. It doesn't necessarily have to. We just did it this way for simplicity.\r\n\r\nIt should be fairly straightforward to extend  `CheckpointInputPipelineHook` to support non-estimator use-cases e.g. by explicitly passing the required args i.e. `num_worker_replicas`, `task_type`, `task_id`, `model_dir `, `save_checkpoints_secs`, `save_checkpoints_steps`. I would be happy to review if you want to send in a PR.\r\n\r\nAs a really hacky workaround you could just build a mock Estimator object that implements the expected fields. That may be prone to breakages though.", "Thank you very much. It helps greatly! :-)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29989\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29989\">No</a>\n"]}, {"number": 29988, "title": "Tensorflow Distributed Learning Tutorial Code not working on localhost", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave Version 10.14.5 (Darwin-18.6.0-x86_64-i386-64bit)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI have used the exact same code from the Tensorflow 2.0 Distributed Training Tutorial as in [https://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_estimator](url). The model does not run on localhost and the code returns an error with **\"Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\"**\r\nThe same error was reported here: #27562 but the fix says to use \"localhost\" which I have already done and the model still won't run. Moreover, removing TF_CONFIG allows the model to run perfectly fine but adding it is what causes the error. \r\n\r\n**Describe the expected behavior**\r\nThe model runs as described in the tutorial.\r\n\r\n**Code to reproduce the issue**\r\n### Run the code below in a local machine:\r\n\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nimport os\r\nimport json\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\n\r\ndef input_fn(mode, input_context=None):\r\n    datasets, info = tfds.load(name='mnist',\r\n                                with_info=True,\r\n                                as_supervised=True)\r\n    mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else\r\n                     datasets['test'])\r\n\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n        return image, label\r\n\r\n    if input_context:\r\n        mnist_dataset = mnist_dataset.shard(input_context.num_input_pipelines,\r\n                                            input_context.input_pipeline_id)\r\n    return mnist_dataset.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n\r\n\r\nNUM_WORKERS = 1\r\nIP_ADDRS = ['localhost']\r\nPORTS = [12345]\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': ['%s:%d' % (IP_ADDRS[w], PORTS[w]) for w in range(NUM_WORKERS)]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\n\r\nLEARNING_RATE = 1e-4\r\n\r\ndef model_fn(features, labels, mode):\r\n    model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    logits = model(features, training=False)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {'logits': logits}\r\n        return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)\r\n\r\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(\r\n      learning_rate=LEARNING_RATE)\r\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(\r\n      from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(labels, logits)\r\n    loss = tf.reduce_sum(loss) * (1. / BATCH_SIZE)\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\n    return tf.estimator.EstimatorSpec(\r\n      mode=mode,\r\n      loss=loss,\r\n      train_op=optimizer.minimize(\r\n          loss, tf.compat.v1.train.get_or_create_global_step()))\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nconfig = tf.estimator.RunConfig(train_distribute=strategy)\r\n\r\nclassifier = tf.estimator.Estimator(\r\n    model_fn=model_fn, config=config)\r\neval_result = tf.estimator.train_and_evaluate(\r\n    classifier,\r\n    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\r\n    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)\r\n)\r\n\r\nprint(eval_result)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2019-06-19 18:52:26.920506: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-19 18:52:26.922213: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345}\r\n2019-06-19 18:52:26.922707: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:12345\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0619 18:52:26.923428 4497626560 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:CPU:0\r\nW0619 18:52:26.926181 4497626560 estimator.py:1811] Using temporary folder as model directory: /var/folders/g1/9kky40_94s9_5qckppqs5fs9w56_br/T/tmp6x4api2p\r\nW0619 18:52:26.928005 4497626560 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nW0619 18:52:26.928821 4497626560 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:CPU:0\r\nW0619 18:52:26.929581 4497626560 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:CPU:0\r\nW0619 18:52:27.373800 123145474756608 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1340: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0619 18:52:27.378234 4497626560 monitored_session.py:347] Collective ops may deadlock with `save_checkpoints_secs` please use `save_checkpoint_steps` instead. Clearing `save_checkpoint_secs` and setting `save_checkpoint_steps` to 1000 now.\r\nNone\r\n2019-06-19 18:52:37.840780: W tensorflow/core/common_runtime/eager/context.cc:232] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```", "comments": ["Do you resolve this problem?\r\nI also meet the same question. I found that the problem appeared in the official demon of estimator and keras when using one worker two workers. \r\nI also change the localhost according to the discussion #27562, however, it doesn't work.\r\nI don't understand that is \"Moreover, removing TF_CONFIG allows the model to run perfectly fine \".", "Hi @runner-yang ,\r\nNo, I haven't resolved it yet.\r\nWhen using Python 3.6, Tensorflow 1.13.0, I get an unimplemented error because the server destroy or shutdown function hasn't been implemented yet. But Python 3.7 and Tensorflow 2.0, gives the error that I have stated above. \r\nAnd about the TF_CONFIG variable, what I meant was that the code/model compiles and runs without the TF_CONFIG but adding it is what causes the error. So the root cause should be something related to the functions that parse and execute from the TF_CONFIG environment variable. It would be a good place to start looking...", "@AshwinAmbal : I have tried to reproduce the issue on Colab with TF version 2.0beta1 and was unable to get the mentioned error. Can you please help us to replicate the issue. For reference [Colab Link](https://colab.sandbox.google.com/drive/1z7QDcwzgKrIix1tyDrASSQ_J2_Uj9wqI#scrollTo=kHWWkBDhiCLL). Thanks! ", "Hi @achandraa , \r\nI am unable to access the Colab link that you have given. \r\nAnyway, this error occurs in localhost when configuring TF_CONFIG. In Colab, I think the Collective Ops is pre-configured. So we don't set the TF_CONFIG environment variable right? Moreover, the tutorial mentioned not to set the variable in Colab. This error occurs when we run the same piece of code in our local machine after setting the TF_CONFIG Environment variable. You can reproduce the error by running the sample code given above in this thread and running it in your local machine.", "Mixing single worker TF_CONFIG and a MultiWorkerMirrored Strategy used with Estimator seem like a very odd combination. \r\n\r\nThat said, if the tutorial says that, it needs to be fixed, and the error should really be better.", "Note that the \"unable to destroy server object\" message is a red herring.  The training does complete, however it does not run evaluate because no evaluator task was started in this cluster.  The return value of `train_and_evaluate` is `None` in this case, which is printed before the last line in the logs.\r\n\r\nIn order to run evaluation, please start a separate evaluator task.  #27857 has more details.\r\n\r\nI've updated the tutorial to provide an example of a `TF_CONFIG` with 2 workers; the single worker example wasn't great."]}, {"number": 29987, "title": "Fix the issue of tf.range where tensor with a different dtype is passed", "body": "This fix tries to address the issue raised in #29867 where the following raises error:\r\n```\r\ntf.range(tf.constant(102), dtype=tf.float32)\r\n...\r\n...\r\nValueError: Tensor conversion requested dtype float32 for Tensor \\\r\n    with dtype int32: 'tf.Tensor(102, shape=(), dtype=int32)'\r\n```\r\n\r\nThis is different from `tf.arange` where different types could be used:\r\n```\r\nnp.arange(np.int(102), dtype=np.float32)\r\n```\r\n\r\nThe issue is that in tf.range cast is only done when dtype is not passed explicitly.\r\n\r\nThis fix adds additional processing so that the above scenario is covered.\r\n\r\nThis fix fixes #29867.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Could you please check reviewer comments and keep us posted. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@tomhennigan @gbaned Sorry for the delay. The PR has been updated. Please take a look and let me know if there are any issues.", "Thanks @tomhennigan for the review. The PR has been updated with name passed to cast, please take a look."]}, {"number": 29986, "title": "Add bool support for unique_with_counts", "body": "\r\nThis fix tries to address the issue raised in #29863 where\r\nunique_with_counts does not support bool dtype yet.\r\n\r\nThis fix fixes #29863.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 29985, "title": "Add Raises in the docstring of tf.histogram_fixed_width", "body": "This fix adds exception conditions for tf.histogram_fixed_width in the docstring.\r\n\r\nThis fix fixes #29276.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 29984, "title": "`vectorized_map` does not support `IndexedSlices`", "body": "I am trying to use `tf.vectorized_map`. But, in my network, I have some weights for `tf.nn.embedding_lookup`. I find when one tries to get the gradient, the gradients for `embedding` parts become `IndiceSlices`. \r\n\r\nWhen the code meets this: `                batch_grad = tf.vectorized_map(lambda x: tf.gradients(x, network_variables), loss)\r\n`, it is stuck there and have the error.\r\n\r\n```\r\n    batch_grad = tf.vectorized_map(lambda x: tf.gradients(x, network_variables), loss)\r\n  File \"/anaconda3/envs/dl/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 338, in vectorized_map\r\n    return pfor(loop_fn, batch_size)\r\n  File \"/anaconda3/envs/dl/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 164, in pfor\r\n    return f()\r\n  File \"/anaconda3/envs/dl/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 161, in f\r\n    return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n  File \"/anaconda3/envs/dl/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 214, in _pfor_impl\r\n    outputs.append(converter.convert(loop_fn_output))\r\n  File \"/anaconda3/envs/dl/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1175, in convert\r\n    output = self._convert_helper(y)\r\n  File \"/anaconda3/envs/dl/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1208, in _convert_helper\r\n    assert isinstance(y, ops.Tensor), y\r\nAssertionError: IndexedSlices(indices=Tensor(\"compute_gradients/loop_body/gradients/predict_actions/network/embeddings/embedding_lookup_grad/Reshape_1:0\", shape=(?,), dtype=int32), values=Tensor(\"compute_gradients/loop_body/gradients/predict_actions/network/embeddings/embedding_lookup_grad/Reshape:0\", shape=(?, 10), dtype=float32), dense_shape=Tensor(\"compute_gradients/loop_body/gradients/predict_actions/network/embeddings/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))\r\n```\r\n\r\nI think the error here is because `IndexedSlices` is not `tf.Tensor`. Then, how can I still use this `tf.vectorized_map`?\r\n", "comments": ["Or if there is any way to let the output of `tf.gradients` to be `Tensor`, not `IndexedSlices`? ", "@JiahaoYao Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Please see https://github.com/tensorflow/tensorflow/commit/4136a6a3ea5796c86054b4a616a821211ad2ef31#diff-56422f894d9046679f64b2f8179aa17c for the support added for IndexedSlices.\r\n\r\nNote that this currently makes the gradients dense. Alternatively, one could flatten out the IndexedSlices returned from the gradients call into its components and then unflatten it post vectorization by vectorized_map.", "Appreciated, @agarwal-ashish ! "]}, {"number": 29983, "title": "use 'while_loop' leads to OOM", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow version (use command below): tensorflow-gpu 1.14.0\r\n- Python version: python 3.7\r\n- GPU model and memory: 4 x GTX2080\r\n\r\n### My issue \r\nI use `while_loop` in the main graph. Every time I feed a batch size of dataset, say Mnist into the graph. Does `while_loop` lead to creating newer graph every epoch I train this model? Here is the error reported about OOM. \r\n\r\n```\r\n2019-06-19 16:34:07.703179: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at batch_matmul_op_impl.h:635 : Resource exhausted: OOM when allocating tensor with shape[128,8070,8070] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nTraceback (most recent call last):\r\n  File \"/home/yue/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/home/yue/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/yue/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[128,8070,8070] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node loop_body_7/MatMul/pfor/MatMul}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"neural_network_1.py\", line 212, in <module>\r\n    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\r\n  File \"/home/yue/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/home/yue/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/yue/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/home/yue/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[128,8070,8070] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[node loop_body_7/MatMul/pfor/MatMul (defined at neural_network_1.py:119) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n```\r\n\r\nDoes anyone have idea about how to solve this OOM issue?\r\n", "comments": ["@JiahaoYao In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29982, "title": "[INTEL MKL] Use the new switch -dumpfullversion to get full gcc version number", "body": "Looks like this one didn't make it to the `v1.14.0` or `r1.14` branch \ud83d\ude41 \r\nBut we need it, otherwise we get:\r\n```\r\nStep 15/21 : RUN ${PYTHON} set-build-env.py -p ${TARGET_PLATFORM} -f /root/.mkl.bazelrc --disable-v2\r\n ---> Running in 09a31032ce3b\r\ngcc_path_cmd = command -v gcc\r\ngcc located here: /usr/bin/gcc\r\ngcc version: 7\r\n\r\n\u001b[91mTraceback (most recent call last):\r\n  File \"set-build-env.py\", line 225, in <module>\r\n    env_setter = BuildEnvSetter()\r\n  File \"set-build-env.py\", line 92, in __init__\r\n    self.go()\r\n  File \"set-build-env.py\", line 218, in go\r\n    target_platform[\"min_gcc_minor_version\"]):\r\n  File \"set-build-env.py\", line 134, in gcc_version_ok\r\n    except subprocess.CalledProcessException as e:\r\nAttributeError: 'module' object has no attribute 'CalledProcessException'\r\n\u001b[0mThe command '/bin/sh -c ${PYTHON} set-build-env.py -p ${TARGET_PLATFORM} -f /root/.mkl.bazelrc --disable-v2' returned a non-zero code: 1\r\n```", "comments": ["This is basically just porting @claynerobison 's fix here https://github.com/tensorflow/tensorflow/pull/28784 into `r1.14`", "sorry, closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac", "@penpornk this change is already in `master` and it has to be in `r1.14` as and that's what this PR is about.\r\nI'm not sure why it was closed \ud83e\udd14 ", "@ashahba I think it's because TF 1.14.0 has been [released](https://github.com/tensorflow/tensorflow/releases/tag/v1.14.0).\r\n\r\n@bananabowl Will there be a 1.14.1 release?", "Yes, there will be a 1.14.1 release from the 1.14 branch (1.14.0 is a tag on this branch). We can't include this in 1.14.0 as it is already released. I am unsure on the timeline for 1.14.1 though (likely a few months out). Thanks for catching this @ashahba!\r\n"]}, {"number": 29981, "title": "Added support to CUDNN Rnn V2 in Keras APIs", "body": "The current Keras API only supports the `gen_cudnn_rnn_ops.cudnn_rnn` operation for the RNN.\r\n\r\nIn this PR, we added the support to `gen_cudnn_rnn_ops.cudnn_rnnv2` to benefit from the autotuning on different RNN algorithms. \r\n\r\nAs in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py, we still require users to use `TF_CUDNN_RNN_USE_V2` to enable it.\r\n\r\nfyi: @nluehr @benbarsdell ", "comments": ["Adding Scott who has worked in this area. Thank you!", "@qlzh727 , Thanks for your review. The v2 does the auto-tuning and have one more output to save the selected algo id, which will be used in the backward pass. In contrast, the v1 only use the default algo. I think it's to make v2 as the default. "]}, {"number": 29980, "title": "error saving file while using tf.data.experimental.TFRecordWriter at https://www.tensorflow.org/beta/tutorials/load_data/tf_records#writing_a_tfrecord_file", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\npython notebook using python v3.7.\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta\r\n[tfrecord-example-with-tfdata.ipynb.zip](https://github.com/tensorflow/tensorflow/files/3307947/tfrecord-example-with-tfdata.ipynb.zip)\r\n\r\n\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Deep learning VM available on GCP \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am following the tutorial at https://www.tensorflow.org/beta/tutorials/load_data/tf_records#writing_a_tfrecord_file\r\n\r\n\r\n**Describe the expected behavior**\r\nI should be able to save the file. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nSee attached python notebook\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with Tensorflow 2.0.0.beta1 on my system. Thanks!", "When you write (let's say, in eager mode, since that's simpler), you need to iterate through the dataset. In the relevant cell of your notebook:\r\n\r\n```\r\nwriter = tf.compat.v1.data.experimental.TFRecordWriter(filename)\r\nfor features in serialized_features_dataset:\r\n  writer.write(features)\r\n```\r\n\r\nThis doesn't look like a bug in TensorFlow to me (of course I may be missing something). Please open a new issue if I am mistaken.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29980\">No</a>\n"]}, {"number": 29979, "title": "[Grappler] Prevent segfault in shape inference", "body": "If `src_context` is `nullptr` because the `src` was null, the error message will cause a segfault trying to access `src->name()`.\r\n\r\nAlso, src_context was used before it was checked to be null, so I moved that usage after the nullptr check.\r\n\r\nThis bug was discovered by a segfault during a call to `GraphProperties::InferStatically()` inside of TF-TRT: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc#L231", "comments": ["Hi @andyly, could you help to take a look? Thanks.", "> Moving src_ic after the null check is fine as that was the case prior to some refactoring that occurred.\r\n\r\nAny idea why `fanin.node` could be null in the first place?", "> > Moving src_ic after the null check is fine as that was the case prior to some refactoring that occurred.\r\n> \r\n> Any idea why `fanin.node` could be null in the first place?\r\n\r\nIf node is a null, then the node does not have an input at a given port, in the GraphView. The part I am not sure is why GraphView may be out of sync with the shape inference (or the node does not match up with its op def). Do you have an example of reproducing this I can look at?", "> > > Moving src_ic after the null check is fine as that was the case prior to some refactoring that occurred.\r\n> > \r\n> > \r\n> > Any idea why `fanin.node` could be null in the first place?\r\n> \r\n> If node is a null, then the node does not have an input at a given port, in the GraphView. The part I am not sure is why GraphView may be out of sync with the shape inference (or the node does not match up with its op def). Do you have an example of reproducing this I can look at?\r\n\r\nIt looks like the segfault occurs when horovod is imported (even though it isn't being used in these cases). Commenting out `import horovod.tensorflow as hvd` seems to fix the segfault."]}, {"number": 29978, "title": "Build failed after upgrading protobuf from 3.7.1 to 3.8.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: latest\r\n- Python version: 3.6.2\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.26\r\n- GCC/Compiler version (if compiling from source): LLVM version 10.0.1 (clang-1001.0.46.4)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nAfter upgrading protobuf from 3.7.1 to 3.8.0, the bazel build failed. \r\n\r\nIf reverting the upgrade commit(https://github.com/tensorflow/tensorflow/commit/508f76b1d9925304cedd56d51480ec380636cb82), the build works well.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`bazel test //tensorflow/core/kernels/data:range_dataset_op_test`\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=192\r\nINFO: Reading rc options for 'test' from /Users/fei/Documents/Github/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'test' from /Users/fei/Documents/Github/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/Users/fei/venv-py/tf-src/bin/python --action_env PYTHON_LIB_PATH=/Users/fei/venv-py/tf-src/lib/python3.6/site-packages --python_path=/Users/fei/venv-py/tf-src/bin/python --action_env TF_CONFIGURE_IOS=0\r\nINFO: Reading rc options for 'test' from /Users/fei/Documents/Github/tensorflow/.tf_configure.bazelrc:\r\n  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium --test_tag_filters=-benchmark-test,-no_oss,-oss_serial --build_tag_filters=-benchmark-test,-no_oss --test_tag_filters=-gpu,-nomac,-no_mac --build_tag_filters=-gpu,-nomac,-no_mac\r\nINFO: Build option --runs_per_test has changed, discarding analysis cache.\r\nINFO: Analyzed target //tensorflow/core/kernels/data:range_dataset_op_test (1 packages loaded, 7713 targets configured).\r\nINFO: Found 1 test target...\r\nERROR: /private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/external/com_google_protobuf/BUILD:106:1: C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1)\r\nexternal/com_google_protobuf/src/google/protobuf/arena.cc:53:13: error: use of undeclared identifier 'LifecycleId'\r\nstd::atomic<LifecycleId> ArenaImpl::lifecycle_id_generator_;\r\n            ^\r\n1 error generated.\r\nTarget //tensorflow/core/kernels/data:range_dataset_op_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.726s, Critical Path: 0.74s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\ncc: @angersson ", "comments": ["@meteorcloudy can you have a look at this?", "@angersson @meteorcloudy This issue is resolved. It seems to be caused by the protobuf version conflicts with that installed on my laptop (which is `3.7.1`). After uninstalling protobuf in my system and `bazel clean --expunge`, the bazel build works well. I'm not sure how the protobuf installed on my laptop affects the TF bazel build.", "I am closing this issue as it was resolved.Thanks", "@feihugis Thanks for the update!"]}, {"number": 29977, "title": "Distributed Tensorflow error: Check failed: DeviceNameUtils::ParseFullName(new_base, &parsed_name)", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.13.1 (PC) 1.6.0-rc0(RP)\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nTrying to run a distributed tensorflow example on CPU from:\r\n\r\nhttps://github.com/tmulc18/Distributed-TensorFlow-Guide/blob/master/Distributed-Setup/dist_setup.py\r\n\r\nCommands to run the example can be found at:\r\n\r\nhttps://github.com/tmulc18/Distributed-TensorFlow-Guide/blob/master/Distributed-Setup/run.sh\r\n\r\nIt works fine when I run it on single platform (PC-PC or laptop-laptop or RP-RP) or multiple platforms with same architecture (PC-laptop, both x86 or RP-RP, both arm64). But a combination of arm64 and x86 fails from arm64 side with the following error:\r\n```bash\r\n2019-06-15 01:20:35.179745: F tensorflow/core/common_runtime/renamed_device.cc:27] Check failed: DeviceNameUtils::ParseFullName(new_base, &parsed_name) \r\n```\r\n\r\n### Source code / logs\r\nNote that in your code, the IPs need to be set accordingly.\r\nThe command for PC is:\r\n```bash\r\npython dist_setup.py --job_name \"worker\" --task_index 0\r\n```\r\nThe output:\r\n```bash\r\n2019-06-14 18:20:35.040413: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-14 18:20:35.070714: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3593265000 Hz\r\n2019-06-14 18:20:35.071281: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4c9ce60 executing computations on platform Host. Devices:\r\n2019-06-14 18:20:35.071303: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-14 18:20:35.072829: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job ps -> {0 -> 10.1.1.2:2222}\r\n2019-06-14 18:20:35.072861: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223}\r\n2019-06-14 18:20:35.074703: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:2223\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-06-14 18:20:35.178858: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 3634afcffbd6cc2d with config: \r\n2019-06-14 18:20:45.214939: W tensorflow/core/distributed_runtime/master_session.cc:1363] Timeout for closing worker session\r\n2019-06-14 18:20:55.218267: I tensorflow/core/distributed_runtime/master.cc:267] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2019-06-14 18:21:05.218392: I tensorflow/core/distributed_runtime/master.cc:267] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2019-06-14 18:21:15.218519: I tensorflow/core/distributed_runtime/master.cc:267] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n```\r\nThe command for RP is:\r\n```bash\r\npython dist_setup.py --job_name \"ps\" --task_index 0\r\n```\r\nThe output:\r\n```bash\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py:33: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  from tensorflow.python.framework import fast_tensor_util\r\n2019-06-15 01:19:54.226102: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2019-06-15 01:19:54.226278: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.1.1.1:2223}\r\n2019-06-15 01:19:54.227740: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\r\n2019-06-15 01:20:35.179745: F tensorflow/core/common_runtime/renamed_device.cc:27] Check failed: DeviceNameUtils::ParseFullName(new_base, &parsed_name) \r\nAborted\r\n```\r\nThe source code(from github):\r\n```bash\r\n\"\"\"Simple example with one parameter server and one worker.\r\nAuthor: Tommy Mulc\r\n\"\"\"\r\n\r\n\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\nimport argparse\r\nimport time\r\nimport os\r\n\r\n\r\nFLAGS = None\r\nlog_dir = '/logdir'\r\n\r\ndef main():\r\n\t# Distributed Baggage\r\n\tcluster = tf.train.ClusterSpec({\r\n        'ps':['localhost:2222'],\r\n        'worker':['localhost:2223']\r\n        }) #lets this node know about all other nodes\r\n\tif FLAGS.job_name == 'ps': #checks if parameter server\r\n\t\tserver = tf.train.Server(cluster,\r\n          job_name=\"ps\",\r\n          task_index=FLAGS.task_index)\r\n\t\tserver.join()\r\n\telse:\r\n\t\tis_chief = (FLAGS.task_index == 0) #checks if this is the chief node\r\n\t\tserver = tf.train.Server(cluster,\r\n          job_name=\"worker\",\r\n          task_index=FLAGS.task_index)\r\n\r\n\t\t# Graph\r\n\t\twith tf.device('/cpu:0'):\r\n\t\t\ta = tf.Variable(tf.truncated_normal(shape=[2]),dtype=tf.float32)\r\n\t\t\tb = tf.Variable(tf.truncated_normal(shape=[2]),dtype=tf.float32)\r\n\t\t\tc=a+b\r\n\r\n\t\t\ttarget = tf.constant(100.,shape=[2],dtype=tf.float32)\r\n\t\t\tloss = tf.reduce_mean(tf.square(c-target))\r\n\t\t\r\n\t\t\topt = tf.train.GradientDescentOptimizer(.0001).minimize(loss)\r\n\r\n    # Session\r\n    # Monitored Training Session\r\n\t\tsess = tf.train.MonitoredTrainingSession(\r\n          master=server.target,\r\n          is_chief=is_chief)\r\n\t\tfor i in range(1000):\r\n\t\t\tif sess.should_stop(): break\r\n\t\t\tsess.run(opt)\r\n\t\t\tif i % 10 == 0:\r\n\t\t\t\tr = sess.run(c)\r\n\t\t\t\tprint(r)\r\n\t\t\ttime.sleep(.1)\r\n\t\tsess.close()\r\n\r\nif __name__ == '__main__':\r\n\tparser = argparse.ArgumentParser()\r\n\t# Flags for defining the tf.train.ClusterSpec\r\n\tparser.add_argument(\r\n    \t\"--job_name\",\r\n    \ttype=str,\r\n    \tdefault=\"\",\r\n    \thelp=\"One of 'ps', 'worker'\"\r\n    )\r\n  # Flags for defining the tf.train.Server\r\n\tparser.add_argument(\r\n    \t\"--task_index\",\r\n    \ttype=int,\r\n    \tdefault=0,\r\n    \thelp=\"Index of task within the job\"\r\n    )\r\n\tFLAGS, unparsed = parser.parse_known_args()\r\n\tmain()\r\n```", "comments": ["hello,I have the same error as you. Do you solve it? Could you give me some solution? thx..", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29977\">No</a>\n"]}, {"number": 29976, "title": "updated zeros and zeros_like", "body": "Corrected typo's, formatted, and add examples", "comments": []}, {"number": 29975, "title": "Batch size on different worker could be different?", "body": "I want to use Tensorflow for training in a distributed environment. But current Allreduce framework requires that both the tensor dimensions and batch size should the same among all the workers. I want to know if Tensorflow supports different workers have different batch size in the same batch, but their tensor size is still the same.\r\n", "comments": ["Any luck to have your answer?", "Hi, what type of distributed training are you using? You should be able to use a different batch size in each worker. all reduce is usually on the gradient which doesn't really get affected by the batch dimension. what are you trying to use the all reduce for? \r\n\r\n", "@guptapriya Yep. I mean no matter PS or allreduce, Can I change the batch size adaptively based on their capabilities? But remain their global batch size the same in each iteration. Whats the related API to do this?", "@guptapriya Any luck to have your answer?", "Do you mean you want the same global batch size each step, but split the batch unevenly across the replicas - because one of the GPUs is for e.g. more powerful than the other? If yes, then this is possible to do on your own, but not supported easily by the high level APIs.\r\nBy default, when using high level APIs like Keras, we will try to split your batch equally among the replicas. If you have your training loop etc (using strategy.experimental_run_v2), you can provide input that is skewed like you want, and it should still work. \r\n\r\n", "@guptapriya Yep!!! Thats what I want! Do u have any examples like this?", "A short example w TF2. Can this be adapted for your use case? Basically you can change mk_input to return whatever input you want to return for a given replica. Note that when using this mechanism, you will not be able to benefit from some of the tf.data performance optimizations.. \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef mk_input():\r\n  replica_id = tf.distribute.get_replica_context().replica_id_in_sync_group\r\n  return replica_id * 2 \r\n\r\ndef step_fn(i):\r\n  return i\r\n\r\nstrategy = tf.distribute.MirroredStrategy([\"/cpu:0\", \"/gpu:0\"])\r\n\r\nfor _ in range(10):\r\n  pre_replica_inputs = strategy.experimental_run_v2(mk_input)\r\n  results = strategy.experimental_run_v2(step_fn, args=(pre_replica_inputs,))\r\n  print(strategy.experimental_local_results(results))\r\n```", "Hi @guptapriya Thanks for your example. One more question: this example has two devices: cpu and gpu. From my understanding mk_input returns the desired batch size. But how can I decide which size is assigned to cpu, or gpu here? How to bind the batch size to the corresponding replica?", "I just used cpu and gpu because then i was able to run it in colab easily. you can just use whatever devices you want to use (for e.g. 2 GPUs). The order in which you specify the GPUs is the replica id order. So you can use that to determine what batch size to use on which replica. mk_input can simply return the actual input. In this toy example, I just said the input will be 2 times the replica_id. But of course in reality you will use replica_id to determine the batch size. \r\n\r\ne.g.:\r\nMirroredStrategy(devices=[\"gpu:0\", \"gpu:1\"])\r\nHere \"gpu:0\" will become your replica_id = 0, and \"gpu:1\" will be replica_id = 1. \r\n\r\nDoes that help ? \r\n", "Hi @guptapriya I am little confused by the example. I checked the [Doc](https://www.tensorflow.org/beta/tutorials/distribute/training_loops). when we use experimental_run_v2(train_step,args=(x,)), then all these two workers will consume the same batch size, x here. But for my case, I want to assign for example 32 MB to cpu and 64 MB to GPU. ", "The doc is assuming you're using \"strategy.experimental_distribute_dataset\" which does the batch splitting for you. I am recommending not using that API. I am saying create your distributed input manually with the help of experimental_run_v2 (and do not use datasets). \r\nin other words, if `x` is generated by `experimental_distribute_dataset`, that will be equally split. But if `x` is generated by you, you can make it whatever you want. ", "Ok. I got it. Your example has two devices. How to specify that 32 is assigned to /cpu:0 and 64 is assigned to /gpu:0?", "use replica_id_in_sync_group. it's value matches the order in which you specified your devices in the mirrored strategy constructor. ", "Thanks @guptapriya So, here comes my last question, where can I pass the desired batch size to my specified devices?", "Hi, sorry about the delay in responding. You need to construct the input tensors you want in this case, there is no API where you can specify the batch size. \r\n\r\nMaybe I can suggest something better if you show me your use case. Where is your input coming from ? Are you using a tf.data dataset? \r\n\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29975\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29975\">No</a>\n"]}, {"number": 29974, "title": "custom filter", "body": "I want to build custom filter with tf.nn.conv2d ", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@halbaqshi In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@halbaqshi Please provide us the complete executable code in order to expedite the trouble-shooting process.Thanks!", "I am able to reproduce the issue on Colab with Tensorflow 1.12.0. ", "@halbaqshi \r\nCould substitute this slice of code  in yours:\r\n```\r\nimport numpy as np\r\n#cc=tf.shape([9])\r\ncc=tf.Variable(tf.zeros(3*3, tf.float32))\r\n#cc[0]=tf.Variable(first_conv2[0])\r\n\r\ncc[0].assign(first_conv2[0])\r\n\r\nfor i in range(1,cc.shape[0]):\r\n    #cc[i]=tf.add(first_conv2[i],cc[i-1])\r\n    #tf.add(first_conv2[i], cc[i - 1])\r\n    cc[i].assign(tf.add(first_conv2[i], cc[i - 1]))\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29974\">No</a>\n", "@halbaqshi The error [here](https://github.com/tensorflow/tensorflow/issues/29974#issuecomment-504722486) is as expected. As `cc` is a tensor object, it doesn't allow item assignment. You need to create a list for that purpose. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29974\">No</a>\n"]}, {"number": 29973, "title": "bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\nHello Tensorflow team\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):Don't know\r\n- TensorFlow version:1.13.1\r\n- Python version:3.6.5\r\n- Installed using virtualenv? pip? conda? :pip\r\n- Bazel version (if compiling from source):0.22\r\n- GCC/Compiler version (if compiling from source):Don't know\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am following the commands Build from source on Windows and got stuck\r\n\r\nC:\\Users\\DELL\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\users\\dell\\tensorflow/tools/bazel.rc\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/DELL/Miniconda3/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\dell\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/DELL/Miniconda3/python.exe --action_env PYTHON_LIB_PATH=C:/Users/DELL/Miniconda3/lib/site-packages --python_path=C:/Users/DELL/Miniconda3/python.exe --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=0 --define grpc_no_ares=true --strip=always --config monolithic --copt=-w --host_copt=-w --verbose_failures\r\nERROR: Config value monolithic is not defined in any .rc file\r\nINFO: Invocation ID: 0388987f-5098-402f-afd1-3cf7ab4f3e40\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI installed Msys2 \r\n then used pacman -S git patch unzip\r\nset path as C:\\msys64\\usr\\bin\\bash.exe for BAZEL_SH\r\nI installed  Visual C++ Build Tools 2015\r\ngit clone https://github.com/tensorflow/tensorflow.git \r\ncd tensorflow\r\ngit checkout r1.9\r\npython ./configure.py\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nI tried many ways to resolve this but can't find the solution please help me out and if possible please make video for this\r\n\r\n", "comments": ["Please have a look on this [similar issue](https://github.com/tensorflow/tensorflow/issues/23401#issuecomment-434681778) and let us know if that resolves the error. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "while running the command git checkout branchname \r\nwhich branchname I have to used in this command??\r\n", "Where did you get the command from? There is some context which is missing in your question above", "I am following the commands given on this \r\n\r\n**Build from source on Windows**\r\nhttps://www.tensorflow.org/install/source_windows\r\n\r\n", "Quoting from there\r\n\r\n> ```\r\n> git clone https://github.com/tensorflow/tensorflow.git\r\n> cd tensorflow\r\n> ```\r\n>\r\n> The repo defaults to the master development branch. You can also checkout a release branch to build:\r\n> \r\n> ```\r\n> git checkout branch_name  # r1.9, r1.10, etc.\r\n> ```\r\n\r\nYou can use master (getting you the latest code) or the release branch corresponding to the TF version you want to build from source (e.g., `r1.13` for 1.13.* series)", "Now when I am running this command\r\n**python ./configure.py**\r\n\r\nI am getting this error:\r\n```\r\n(base) C:\\Users\\DELL\\tensorflow>python ./configure.py\r\npython: can't open file './configure.py': [Errno 2] No such file or directory\r\n```", "Do you have `configure.py` in that directory? Try `dir`/`ls`", "Yes,In tensorflow folder configure file is present but not in .py extension.", "That's strange. It is supposed to exist in both formats as [`configure` ](https://github.com/tensorflow/tensorflow/blob/master/configure) just calls `configure.py`:\r\nhttps://github.com/tensorflow/tensorflow/blob/377a4df447c3b8ca086e0b5860083f1752f416b8/configure#L12", "Most likely you didn't get a full clone of the repository, or accidentally removed files. I suggest trying again from scratch, from a new fresh clone"]}, {"number": 29972, "title": "AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'", "body": "I am using tensorflow on python 3.7, Ubuntu 16.04. The code which throws the above mentioned error is written below. It is based on the following [code](https://www.tensorflow.org/guide/datasets#applying_arbitrary_python_logic_with_tfpy_func). I am getting this error on both tensorflow 1.13 as well as 2.0.0-beta1\r\n\r\nI have a dataset folder containing millions of data pair of the form (image,timeseries). The timeseries is in numpy format. I want to use np.load() function to load the data. But the filename is in string tensor format. np.load() does not accept tensorflow.python.framework.ops.EagerTensor\r\n\r\n\timport tensorflow as tf\r\n\timport numpy as np\r\n\timport imageio\r\n\r\n\t#tf.enable_eager_execution()    # use this line if using tensorflow 1.13\r\n\r\n\timageio.imwrite('data.jpg', np.random.rand(256,256,3))\r\n\tnp.save('data.npy',np.ones(1024))\r\n\r\n\tdef load(image_file,timeseries_file):\r\n\t  image = tf.io.read_file(image_file)\r\n\t  image = tf.image.decode_jpeg(image)\r\n\t  timeseries = np.load(timeseries_file.decode())\r\n\t  timeseries = tf.convert_to_tensor(timeseries, np.float32)\r\n\t  image = tf.cast(image, tf.float32)\r\n\t  timeseries = tf.cast(timeseries, tf.float32)\r\n\t  return image, timeseries\r\n\r\n\timage_files = ['data.jpg']\r\n\ttimeseries_files = ['data.npy']\r\n\ttrain_dataset = tf.data.Dataset.from_tensor_slices((image_files, timeseries_files))\r\n\ttrain_dataset = train_dataset.map(\r\n\t    lambda image_file, timeseries_file: tuple(tf.py_function(\r\n\t        load, [image_file, timeseries_file], [tf.float32, tf.float32])))\r\n\tfor x in train_dataset.take(1):\r\n\t\tprint(x)", "comments": ["You need to add `.numpy()` if you want to use the result of a TensorFlow computation directly (i.e., in your case, to get the string from the `EagerTensor`, which you call `.decode` on).", "hi\uff0cDo you have solved this problem?I ran into the same problem", "I have a following program:\r\n\r\nfrom PIL import Image\r\nimport numpy as np\r\nfrom skimage import transform\r\n\r\nmodel2 = load_model('ip_classification.hdf5')\r\n\r\ndef accept_bright(custom_img):\r\n    \r\n    # Bright Accept Images\r\n    contrast = cv2.convertScaleAbs(custom_img, alpha=1, beta=100)\r\n    blurred = cv2.GaussianBlur(contrast, (7, 7), 0)\r\n    thresh = cv2.threshold(blurred, 225, 255, cv2.THRESH_BINARY)[1]\r\n    thresh = cv2.erode(thresh, None, iterations=3)\r\n    thresh = cv2.dilate(thresh, None, iterations=5)\r\n    return thresh\r\n\r\ndef accept_low(custom_img):\r\n    \r\n    # Low Accept Images\r\n    contrast = cv2.convertScaleAbs(custom_img, alpha=1, beta=110)\r\n    blurred = cv2.GaussianBlur(contrast, (5, 5), 0)\r\n    thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)[1]\r\n    thresh = cv2.erode(thresh, None, iterations=5)\r\n    thresh = cv2.dilate(thresh, None, iterations=5)\r\n    return thresh\r\n\r\ndef reject_bright(custom_img):\r\n    \r\n    # Bright Reject Images\r\n    contrast = cv2.convertScaleAbs(custom_img, alpha=1, beta=60)\r\n    blurred = cv2.GaussianBlur(contrast, (7, 7), 0)\r\n    thresh = cv2.threshold(blurred, 180, 255, cv2.THRESH_BINARY)[1]\r\n    thresh = cv2.erode(thresh, None, iterations=7)\r\n    thresh = cv2.dilate(thresh, None, iterations=3)\r\n    return thresh\r\n\r\ndef reject_low(custom_img):\r\n    \r\n    # Dark Reject Images\r\n    contrast = cv2.convertScaleAbs(custom_img, alpha=1, beta=50)\r\n    blurred = cv2.GaussianBlur(contrast, (3, 3), 0)\r\n    thresh = cv2.threshold(blurred, 150, 255, cv2.THRESH_BINARY)[1]\r\n    thresh = cv2.erode(thresh, None, iterations=7)\r\n    thresh = cv2.dilate(thresh, None, iterations=3)\r\n    return thresh\r\n\r\ndef load(img):\r\n    threshold = 140\r\n    if np.mean(img) > threshold:\r\n        #image = Image.open(img)\r\n        images = image.convert(mode = 'L')\r\n        image_np = np.array(images)\r\n        sharp = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])       # sharpening the image\r\n        custom_img = cv2.filter2D(image_np,-1,sharp)\r\n        \r\n        # Blob implementation\r\n        detector = cv2.SimpleBlobDetector_create()\r\n        # Detect blobs.\r\n        keypoints = detector.detect(custom_img)\r\n        num_blobs = len(keypoints)\r\n        if num_blobs > 0:\r\n            w = reject_bright(custom_img)\r\n            np_image = Image.fromarray(w)\r\n            np_image = np.array(np_image).astype('float32')/255\r\n            np_image = transform.resize(np_image, (256, 256, 1))\r\n            np_image = np.expand_dims(np_image, axis=0)                  # (1,256,256,1)\r\n            return np_image\r\n        else:\r\n            x = accept_bright(custom_img)\r\n            np_image = Image.fromarray(x)\r\n            np_image = np.array(np_image).astype('float32')/255\r\n            np_image = transform.resize(np_image, (256, 256, 1))\r\n            np_image = np.expand_dims(np_image, axis=0)                  # (1,256,256,1)\r\n            return np_image\r\n        \r\n    elif np.mean(img) < threshold:\r\n        #image = Image.open(img)\r\n        images = image.convert(mode = 'L')\r\n        image_np = np.array(images)\r\n        sharp = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])       # sharpening the image\r\n        custom_img = cv2.filter2D(image_np,-1,sharp)\r\n        # Blob implementation\r\n        detector = cv2.SimpleBlobDetector_create()\r\n        # Detect blobs.\r\n        keypoints = detector.detect(custom_img)\r\n        num_blobs = len(keypoints)\r\n        if num_blobs > 0:\r\n            y = reject_low(custom_img)\r\n            np_image = Image.fromarray(y)\r\n            np_image = np.array(np_image).astype('float32')/255\r\n            np_image = transform.resize(np_image, (256, 256, 1))\r\n            np_image = np.expand_dims(np_image, axis=0)                  # (1,256,256,1)\r\n            return np_image\r\n        else:\r\n            z = accept_low(custom_img)\r\n            np_image = Image.fromarray(z)\r\n            np_image = np.array(np_image).astype('float32')/255\r\n            np_image = transform.resize(np_image, (256, 256, 1))\r\n            np_image = np.expand_dims(np_image, axis=0)                  # (1,256,256,1)\r\n            return np_image\r\n\r\n\r\ninp = Image.open('C:\\\\Users\\\\nabhishe\\\\MITC_Project\\\\RGB_compiled_images\\\\Reject-A26491-001-Sample - 74-2.jpg')\r\nimage2 = load(inp)\r\nlabel = model2.predict(image2)\r\nplt.imshow(image2.reshape(256,256),cmap='gray')\r\nplt.show()\r\nprint(\"Predicted Class (0 - Accept , 1 - Reject): \", label[0][0])\r\n\r\nError:\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'convert'\r\n\r\ncan someone help me with this"]}, {"number": 29971, "title": "Backpropogation error with tf.math.top_k", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  \r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\n1.13\r\n- Python version:\r\n3.5.7 (Also reproduced with 3.6)\r\n- CUDA/cuDNN version:\r\n10.1\r\n- GPU model and memory:\r\nQuadro P3000\r\n\r\n**Describe the current behavior**\r\nWhen the input argument in tf.math.top_k losses is a tensor of shape [0, 0], backpropogation fails on the reshape step. \r\n\r\n**Describe the expected behavior**\r\nThere should be no backprop on collecting 0 elements from tensor with 0 values. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nTrying to perform Online Hard Example Mining with two lists of losses of shape [None, None] (Batch, num_losses)\r\nWhen there are either zero positive examples or zero negative examples, trips the error on the backprop step even though there should be no backprop for a selection of zero elements or a tensor of zero elements\r\n```\r\n(placeholders/ setup)\r\npositive_class_examples = tf.gather_nd(classification_losses, positive_class_indices)\r\nnegative_class_examples = tf.gather_nd(classification_losses, negative_class_indices)\r\nnum_pos_examples = tf.shape(positive_class_examples)[0]\r\nnum_neg_examples = tf.math.maximum(256 - num_pos_examples, 0)\r\ntop_negative_losses, top_negative_loss_indices = tf.math.top_k(negative_class_examples, k=num_neg_examples)\r\nbalanced_top_classification_losses = tf.concat([positive_class_examples, top_negative_losses], axis=-1)\r\n...\r\n(optimizer creation with prior code included)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n\t [[{{node gradients/balance_positive_and_negative_examples/TopKV2_grad/Reshape}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  <Redacted>\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n\t [[node gradients/balance_positive_and_negative_examples/TopKV2_grad/Reshape (defined at train.py:177) ]]\r\n\r\nCaused by op 'gradients/balance_positive_and_negative_examples/TopKV2_grad/Reshape', defined at:\r\n  <Redacted>\r\n    train_op = optimizer.minimize(loss)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 403, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 512, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 664, in gradients\r\n    unconnected_gradients)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 420, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 1002, in _TopKGrad\r\n    ind_2d = array_ops.reshape(op.outputs[1], array_ops.stack([-1, ind_lastdim]))\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7179, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n...which was originally created as op 'balance_positive_and_negative_examples/TopKV2', defined at:\r\n  <Redacted>\r\n  File \"train.py\", line 127, in balance_positive_and_negative_examples\r\n    top_negative_losses, top_negative_loss_indices = tf.math.top_k(negative_class_examples, k=num_neg_examples)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 3084, in top_k\r\n    return gen_nn_ops.top_kv2(input, k=k, sorted=sorted, name=name)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 8401, in top_kv2\r\n    \"TopKV2\", input=input, k=k, sorted=sorted, name=name)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n\t [[node gradients/balance_positive_and_negative_examples/TopKV2_grad/Reshape (defined at train.py:177) ]]\r\n```\r\n", "comments": ["@tokotchd In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 48, 48, 3])\r\ntarget_for_optimizer = tf.placeholder(dtype=tf.float32, shape=[None, 48, 48, 3])\r\nnum_positive_examples_chosen = tf.placeholder(dtype=tf.int32, shape=None)\r\n\r\njunk_conv_layers = tf.layers.conv2d(input_placeholder, filters=3, strides=[1,1], kernel_size=[1,1])\r\n\r\nl1_diff = tf.abs(junk_conv_layers - target_for_optimizer)\r\nlosses = tf.reduce_mean(l1_diff, axis=3)\r\nlosses = tf.reduce_mean(losses, axis=2)\r\nlosses = tf.reduce_mean(losses, axis=1)\r\n\r\ntop_loss_picks, top_pick_indices = tf.math.top_k(losses, k=num_positive_examples_chosen)\r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate=0.00001)\r\ntrain_op = optimizer.minimize(top_loss_picks)\r\n\r\nwith tf.Session() as session:\r\n    session.run(tf.initialize_all_variables())\r\n    random_inputs = np.random.rand(20, 48, 48, 3)\r\n    random_target = np.random.rand(20, 48, 48, 3)\r\n    number_of_top_losses = np.random.randint(0, 2, size=[])\r\n    _ = session.run(train_op, feed_dict={input_placeholder: random_inputs, target_for_optimizer: random_target, num_positive_examples_chosen: number_of_top_losses})\r\n```\r\n\r\nAfter building a minimum working example, it seems that the only condition required for this error is backpropogation on top_k function when k == 0.  As a result, the above code runs successfully 50% of the time and fails with the following stacktrace the other 50% of the time.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n\t [[{{node gradients/TopKV2_grad/Reshape}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"minimum_example.py\", line 32, in <module>\r\n    _ = session.run(train_op, feed_dict={input_placeholder: random_inputs, target_for_optimizer: random_target, num_positive_examples_chosen: number_of_top_losses})\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n\t [[node gradients/TopKV2_grad/Reshape (defined at minimum_example.py:25) ]]\r\n\r\nCaused by op 'gradients/TopKV2_grad/Reshape', defined at:\r\n  File \"minimum_example.py\", line 25, in <module>\r\n    train_op = optimizer.minimize(top_loss_picks)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 403, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 512, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 664, in gradients\r\n    unconnected_gradients)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 420, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 1002, in _TopKGrad\r\n    ind_2d = array_ops.reshape(op.outputs[1], array_ops.stack([-1, ind_lastdim]))\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7179, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n...which was originally created as op 'TopKV2', defined at:\r\n  File \"minimum_example.py\", line 22, in <module>\r\n    top_loss_picks, top_pick_indices = tf.math.top_k(losses, k=num_positive_examples_chosen)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 3084, in top_k\r\n    return gen_nn_ops.top_kv2(input, k=k, sorted=sorted, name=name)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 8401, in top_kv2\r\n    \"TopKV2\", input=input, k=k, sorted=sorted, name=name)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n\t [[node gradients/TopKV2_grad/Reshape (defined at minimum_example.py:25) ]]\r\n```", "I could reproduce the issue on colab with TF-gpu 1.13.1.", "I cannot reproduce this using nightly.\r\n\r\nCan you find a smaller reproducing example? Say, just the shape of the input to top_k, a call to top_k, and a call to gradients?", "@alextp you need some sort of trainable variables/layers otherwise the call to gradients will not propagate through top_k.", "I had the same problem.\r\nhow to solve this problem", "@alextp  \r\n\r\nI ran his code three times, and I get this error.\r\nmaybe you should run the code he provided a few times to get this error.", "@tokotchd This is fixed in Tf 1.14.0. Please take a look at colab gist [here](https://colab.research.google.com/drive/1Q7HkreVszvG6ygXqlQrUoZI0ZRHzsX4m). You want to give a try. Thanks! ", "I just ran this 20 times with and without GPUs on colab and did not get the error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29971\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29971\">No</a>\n"]}, {"number": 29970, "title": "Tensorflow installe using GPU but it is using only CPU", "body": "I have installed tensorflow from source using bazel on ubuntu 16.xx\r\n\r\ntensorflow version 1.11.0\r\nbazel 0.18.0rc6\r\npython 2.7.11\r\ncuda 9.2 (j'utilse celle dans dossier /home/ahmed/tmp/cuda-9.2)\r\ncudnn 7.1.4\r\nnccl 2.4.2\r\n\r\nthe build sucessfull , the installation is also OK.\r\nHowever, the tensorflow does'nt use GPU.\r\n\r\nWhe I run : \r\n\r\nimport tensorflow as tf\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\n**2019-06-19 17:49:28.775658: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: UNKNOWN ERROR (-1)\r\n2019-06-19 17:49:28.775751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: ubuntu-Precision-Tower-7910\r\n2019-06-19 17:49:28.775770: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: ubuntu-Precision-Tower-7910\r\n2019-06-19 17:49:28.775831: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: Invalid argument: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got \"1\"\r\n2019-06-19 17:49:28.775898: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.54.0\r\nDevice mapping: no known devices.\r\n2019-06-19 17:49:28.777666: I tensorflow/core/common_runtime/direct_session.cc:291] Device mapping:**\r\n\r\n\r\nWed Jun 19 17:54:39 2019\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.54                 Driver Version: 396.54                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:04:00.0  On |                  N/A |\r\n| 28%   49C    P8    19W / 250W |    180MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN Xp            Off  | 00000000:05:00.0 Off |                  N/A |\r\n| 23%   33C    P8    16W / 250W |      2MiB / 12196MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1385      G   /usr/lib/xorg/Xorg                           137MiB |\r\n|    0      2744      G   compiz                                        41MiB |\r\n+-----------------------------------------------------------------------------+\r\n", "comments": ["Could you check this [issue](https://github.com/tensorflow/tensorflow/issues/19266#issuecomment-399686258) which is similar to your issue but for older TF version. If those solutions doesn't work, could you uninstall and reinstall CUDA and cuDNN? Please let us know how it progresses. Also, try to uninstall and reinstall tensorflow-gpu and try following the instructions from [TensorFlow website](https://www.tensorflow.org/install/source). Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29969, "title": "ModuleNotFoundError: No module named 'tensorflow_model_optimization'", "body": "Occurs when attempting to import tensorflow_model_optimization. Running tf-nightly-gpu 1.14. Installed tensorflow_model_optimization package as well.", "comments": []}, {"number": 29967, "title": "Relax a few accuracy checks", "body": "Some checks failed with: AssertionError: 1.0 != 0.99999994 within 7 places\r\nwhen running on XLA GPU", "comments": []}]