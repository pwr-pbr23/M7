[{"number": 1300, "title": "Support for half-floats (float16/fp16)", "body": "This is a tracking bug for adding support for the half type (aka float16, or fp16) in TensorFlow. Half computation is supported by GPUs only, although newer Intel CPUs (Haswell and newer) have support for converting back and forth between fp16 and fp32 in hardware (F16C). CUDA has some support for half since 7.5, although it's a bit cumbersome (it's not a first-class type, but relies on macros containing asm statements; effectively intrinsics).\n\nfp16 is interesting for two primary reasons: It would allow us to fit twice as large models in available GPU RAM, and it reduces memory bandwidth use, a precious resource on the GPU. The next generation of NVIDIA GPUs (Pascal) will also be able to do computation directly on two half-floats (in a SIMD-like structure) as fast as on a single float, although that would be somewhat more intrusive in code.\n\nIt is not 100% clear exactly how much of TensorFlow we need fp16 support for; interested parties are asked to comment. CuBLAS and CuDNN already has some support for half in their latest versions (so it would be natural to provide those interfaces), and Eigen also has beginning support. \n", "comments": ["+1 for this.. other reasons include reducing data storage. (on disk only, since 64bit CPU arch still uses just as much RAM), which means the storage and reading to and from disk for large datasets is a lot faster.\n", "I currently save my input in the format of HDF5 with np.fp16, and then transform to fp32 before feeding to the network. It would be great to support fp16 ops.\n", "I'm currently working on this. There's a fair amount of work to do\u2014the CUDA \u201chalf\u201d type is just a struct of uint16_t, which doesn't really match what Eigen expects. In particular, Eigen expects all such arithmetic types to have operator overloads, converts to and from other arithmetic types (including even bool), etc. The actual arithmetic intrinsics require CUDA compute 5.3 (which right now means essentially Tegra X1 only), so mostly, we'll have to rely on casting on both sides and then doing the actual calculations in float.\n\nAlso, we're going to need conversions on the CPU because e.g. cuBLAS functions might require the alpha and beta to be in half, we often need a definition of zero, etc.. We don't need these to be fast, though; fp16 is generally not a win on CPU.\n", "An update here; the Eigen::half class (making half look roughly like any other arithmetic type) works. I've added half support to a bunch of ops (with the class in place, it's mostly only about adding the right template instantiations), including some a very few select cuDNN and cuBLAS operations. I can do training of a CNN example with most ops in float16 (I need to cast back to float32 for some operations, like maxpool and dropout, that don't have implementations yet), but if I have too many of them, training starts to go slower, I believe this has something to do with gradients, but it's hard to say without the tests.\n\nSome operations that are about composing Eigen stuff probably would be faster (and more accurate) by just being a composition of cast to float32, the operation, and then cast back. We'll see about that later.\n\nOverall I'm going to try to put things piecewise through code reviews, although it is going somewhat slowly, mostly due to boring fiddly issues about build environments.\n", "The half class is out in Eigen upstream (still some things to fix, though): https://bitbucket.org/eigen/eigen/commits/7aa6310352e789f118b28456ab14de4baf5720fa\n\nWorking on code review for the new type in TF core; after that, we'll see about getting some of the ops in.\n", "@sesse , that's fast. Great work!\n", "Good stuff. How does float16 perform in training compared to, for instance, fixed point number representations with stochastic rounding? http://arxiv.org/abs/1502.02551\n", "Base commit in 9d03824d6740be0c043c431566c917ec0cf6cf3f. There's still a lot of work left to be done, of course\u2014it's not very useful yet.\n\nFroskekongen: I don't know of any detailed analysis of float16 versus fixed16; it's probably going to be dependent on a number of factors including your model and compilation flags. TensorFlow supports quantized models for inference, but as far as I know not training them.\n", "Also some interesting \"VBR\" approach is emerging http://arxiv.org/abs/1511.06393\n", "For who is interested in RAM consumption on more constrained contexts I have also added https://github.com/tensorflow/tensorflow/issues/1592\n", "9b71f96ae83bbc7a7f8ce91d94e3c1de408ac07f contains support for half cast and const. Still working on (most of) the rest of the ops.\n", "31fd4868711f393bec74200231c2936bf3df079a contains fp16 support for all the componentwise ops, both unary (sqrt, sigmoid, etc.) and binary (add, sub, mul, \u2026).\n", "36c0475865ec103b5a3c2de4a69b17eddf7a9903 and dbd35a75bfe2dc9c98858806c3b48602281d1a46 together enable fp16 optimizers, so now it should be possible to train simple pipelines in fp16, as long as they don't have convolutions or matrix multiplications in them. (Granted, those are pretty heavy restrictions, and we're working to have them lifted.)\n", "b8e73a5d5fb9f1dc4c654f163329a9e8ffb6bd76 fp16-enables the cross-entropy op. 4640ab5d3971bcc4f20951a211bd457cdfa56bc5 fp16-enables the random ops.\n", "63fd88065526c6301569743b1cec12fd2561589f enables inf/NaN debugging (if requested) for fp16, like you already can with fp32 and fp64.\n", "Some changes from @benoitsteiner: d526527a7f2bb3d29fed58b9f759f59e9a49c890 enables fp16 support for the tiling ops, 7ae67a14e273a000e5b306d335dc0f1d7c0650c1 enables fp16 support for the l2loss kernel.\n", "523055469c8a61425e3b8f104be67787c2933ccb adds support for fp16 GEMM (matrix multiplications) to StreamExecutor. 61b12f567ad167556bf55d4375112ed262d37975 does the same for convolution operations via cuDNN. (Both require CUDA 7.5.) Unfortunately there were some late changes to how convolutions are done in TensorFlow proper, so we'll need a followup commit for updating the cuDNN use.\n\nNote that neither of these actually add support to the TensorFlow MatMul and Conv2D ops, but they're necessary building blocks for that.\n", "6350f17524f0f6f3761e84589846191a90fef061 does the required fixups to StreamExecutor.\n", "4d55cb5c55fcc85009171b6a4657cbd966fd85e5 adds fp16 support to the MatMul op (gated on CUDA 7.5 or higher). This is a major milestone for fp16 support; it means that many real graphs can probably be  trained and run in fp16.\n\nConvolutions are missing still.\n", "@sesse great job with adding all these operations! Any estimate on how long it might take to support convolutions?\n", "@kunal-bajpai: It will almost certainly happen next week.\n", "@kunal-bajpai I'm hoping to land a set of improvements necessary to make the convolution code work on fp16 ready in a couple of days. We should be able to support convolutions on fp16 shortly thereafter. \n", "Some image ops while we're waiting (I'm trying to make Inception train with fp16 end-to-end): 48b52d88a419d26beb9ee5cb3c19dba61c68cc3b makes SummarizeImageOp support fp16, c3465a857fcef34fcf2894e90af882f11007762e makes DrawBoundingBox support fp16 (although the bounding box is still fp32).\n", "That sounds good @benoitsteiner. Looking forward to the commit. Please let me know when it happens. Thanks!\n", "https://github.com/tensorflow/tensorflow/commit/36357e7e1127873165694a38e3a989df4e0b6ffe adds support for fp16 to the batch normalization operations.\n", "99671f3a705789ef217c7bca92409add3cf529e5 fp16-enables ReverseOp. 6164d02144239c58a8f19cd12ff2a3ff7b7605d4 (by @benoitsteiner) fp16-enables AddN. bb0190f6c26bf11f601102dfe2166a68a7833020 (also by @benoitsteiner) fp16-enables the softmax ops. 80da0a63200cb7c9c449188620992c7a8d18c8b9 fp16-enables the resize ops, although you should note that some of them always output float no matter the input (this was preserving existing behavior, although I'm not sure it makes sense for half or double).\n\nAnd\u2026 4f257a2427ba0414bd7513c9b61fb835870bd3cf fp16-enables convolutions on GPU, assuming you have CUDA 7.5 or newer.\n\nYou still can't MaxPool/AvgPool, but that's up for next week. After that, I will declare this feature complete, since you can actually do useful stuff with it. There's always more performance to be had, though (currently it's pretty much performance-neutral on Maxwell, so it's mostly about the memory savings), more ops that need conversion and so on, but that should probably live outside this bug. And that work will be continued by Benoit alone, as I'm leaving Google and thus TensorFlow. :-)\n", "@benoitsteiner does TensorFlow support ReLU half float yet?\n", "@kunal-bajpai Most if not all the coefficient-wise operations should work on half floats. I have checked that this is the case for ReLU.\n", "Thanks for this - looking forward to MaxPool, (not yet present on last night's .whl).\n", "Also looking forward to seeing elu added.\n", "I have added support for pooling on fp16 in https://github.com/tensorflow/tensorflow/commit/b7c416926e4d31f7d7a924bfdc97580bf7d44c04. I have also added a --use_fp16 option the convolutional mnist model at tensorflow/models/image/mnist/convolutional.py to make it train using fp16 instead of fp32.\n", "Great @benoitsteiner ! We can start working with this. \n\nCan you tell me if these operations are present in a 0.9rc or when you will be adding it to a stable version?\n\n> On 08-Jun-2016, at 12:29 am, Benoit Steiner notifications@github.com wrote:\n> \n> I have added support for pooling on fp16 in b7c4169. I have also added a --use_fp16 option the convolutional mnist model at tensorflow/models/image/mnist/convolutional.py to make it train using fp16 instead of fp32.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "I don't think the pooling operations made the 0.9 cut, but they'll be available in the release that comes after 0.9. In the meantime, you can always [install from source].(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#installing-from-sources)\n", "@benoitsteiner we will be building from source but were just looking for some clarity on when a stable release might include these operations. Any ETA for the next release?\n\n> On 08-Jun-2016, at 9:38 pm, Benoit Steiner notifications@github.com wrote:\n> \n> I don't think the pooling operations made the 0.9 cut, but they'll be available in the release that comes after 0.9. In the meantime, you can always [install from source].(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#installing-from-sources)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@rizar\n", "https://github.com/tensorflow/tensorflow/commit/bcb8f33b788bd02f1b6bdf813129d5668c6c4e5e introduces a new --use_fp16 option to train the cifar10 model using half floats instead of regular floats.\n", "It looks like fp16 is very slow:\n\n```\nAlexs-MacBook-Pro:~ alexatknit$ python3 tensorflow/tensorflow/models/image/cifar10/cifar10_train.py --use_fp16\n>> Downloading cifar-10-binary.tar.gz 100.0%\nSuccessfully downloaded cifar-10-binary.tar.gz 170052171 bytes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n2016-07-06 17:54:28.997546: step 0, loss = 2.54 (2.5 examples/sec; 50.208 sec/batch)\n2016-07-06 18:01:15.998456: step 10, loss = 2.49 (3.7 examples/sec; 34.673 sec/batch)\n...\n```\n\nvs fp32:\n\n```\nAlexs-MacBook-Pro:~ alexatknit$ python3 tensorflow/tensorflow/models/image/cifar10/cifar10_train.py\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n2016-07-06 18:02:25.860620: step 0, loss = 4.67 (18.9 examples/sec; 6.789 sec/batch)\n2016-07-06 18:02:35.978002: step 10, loss = 4.63 (122.8 examples/sec; 1.042 sec/batch)\n2016-07-06 18:02:44.049841: step 20, loss = 4.48 (150.6 examples/sec; 0.850 sec/batch)\n...\n```\n", "Thanks for work done here!\n\nI am not sure what hardware @alexatknit uses, but I can confirm the slow speed on a 1080 using CUDA 8 and cuDNN 5.0:\nfp16\n\n```\ncifar10_train.py --use_fp16\n2016-07-13 18:50:51.374002: step 0, loss = 4.66 (8.8 examples/sec; 14.572 sec/batch)\n2016-07-13 18:51:00.050748: step 10, loss = 4.62 (173.7 examples/sec; 0.737 sec/batch)\n2016-07-13 18:51:07.455344: step 20, loss = 4.47 (172.1 examples/sec; 0.744 sec/batch)\n2016-07-13 18:51:14.891447: step 30, loss = 4.52 (172.7 examples/sec; 0.741 sec/batch)\n```\n\nvs. fp32\n\n```\ncifar10_train.py\n2016-07-13 18:51:38.791134: step 0, loss = 4.67 (9.2 examples/sec; 13.928 sec/batch)\n2016-07-13 18:51:41.009791: step 10, loss = 4.62 (845.5 examples/sec; 0.151 sec/batch)\n2016-07-13 18:51:42.653312: step 20, loss = 4.52 (785.7 examples/sec; 0.163 sec/batch)\n2016-07-13 18:51:44.270884: step 30, loss = 4.42 (792.6 examples/sec; 0.161 sec/batch)\n```\n\nWhat need's to be done to make this fast?\n", "The performance of the cifar10 model is limited by the speed at which we can perform the local response normalization. This operation is currently implemented for CPU only. Since most CPUs don't support fp16, we have to emulate the corresponding computations, which is incredibly slow.\n\nThe performance should improve significantly once we add support for LRN on GPU. Another option would be to compile tensorflow with support for the f16c instruction that's been introduced with the Haswell architecture. That should reduce the performance gap between fp16 and fp32.\n", "@sbrodehl GTX 1080's fp16 performance is well under 1Tflops. See reddit discussions [here](https://www.reddit.com/r/MachineLearning/comments/4lhrfj/fp16_performance_on_gtx_1080_is_artificially/?st=iqlpu3i4&sh=0afda9d6).\n", "Thanks @thinxer , I noticed that one later. :-/ \nNVidia did not specify any half precision intrinsics either in CUDA 8.\nAnd thanks @benoitsteiner for the deeper insight.\n", "@sbrodehl my benchmark was simply done on a cpu only 2015 macbook pro 13\"\n", "In https://github.com/tensorflow/tensorflow/commit/7d3f838f76e34cde680d9815ba300a989e641e86 I extended the ptb_word_lm model used in our [RNN tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html) to train using fp16.\n", "Starting with https://github.com/tensorflow/tensorflow/commit/0e91b8b4db9c40bc8c45bcc69dd18e03d9f898a3, it's now possible to train our reference translation model using fp16.\n\nI'm closing this issue since we now have several reference models that demonstrate the use of fp16 in TensorFlow. Please file separate bugs if you encounter specific limitations or bug in the functionality.\n", "Confirm a slow speed on fp16 just like @sbrodehl .\r\n\r\n## Environment\r\n\r\nTesla K80 + CUDA 8.0 + cuDNN v5.1\r\n\r\nTensorflow v0.12.1\r\n\r\n`nvidia-smi` information:\r\n\r\n```\r\n# nvidia-smi\r\nMon Mar 13 16:14:44 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:00:08.0     Off |                  Off |\r\n| N/A   52C    P0    57W / 149W |   1543MiB / 12205MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:00:09.0     Off |                  Off |\r\n| N/A   39C    P0    68W / 149W |   1458MiB / 12205MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n## FP32 result\r\n\r\n```\r\n# python cifar10_train.py\r\n2017-03-13 16:08:27.988583: step 0, loss = 4.68 (7.6 examples/sec; 16.831 sec/batch)\r\n2017-03-13 16:08:29.203714: step 10, loss = 4.61 (805.1 examples/sec; 0.159 sec/batch)\r\n2017-03-13 16:08:30.726646: step 20, loss = 4.50 (759.7 examples/sec; 0.168 sec/batch)\r\n2017-03-13 16:08:32.171580: step 30, loss = 4.43 (965.8 examples/sec; 0.133 sec/batch)\r\n2017-03-13 16:08:33.639704: step 40, loss = 4.40 (831.2 examples/sec; 0.154 sec/batch)\r\n2017-03-13 16:08:35.092967: step 50, loss = 4.34 (964.5 examples/sec; 0.133 sec/batch)\r\n2017-03-13 16:08:36.590003: step 60, loss = 4.28 (865.6 examples/sec; 0.148 sec/batch)\r\n```\r\n\r\n`nvidia-smi` shows that GPU-util is around 50%.\r\n\r\n## FP16 result\r\n\r\n```\r\n# python cifar10_train.py --use_fp16\r\n2017-03-13 16:12:18.382315: step 0, loss = 4.67 (7.2 examples/sec; 17.864 sec/batch)\r\n2017-03-13 16:12:28.586276: step 10, loss = 4.64 (130.9 examples/sec; 0.978 sec/batch)\r\n2017-03-13 16:12:38.912454: step 20, loss = 4.44 (126.1 examples/sec; 1.015 sec/batch)\r\n2017-03-13 16:12:49.141316: step 30, loss = 4.52 (130.9 examples/sec; 0.978 sec/batch)\r\n2017-03-13 16:12:59.328050: step 40, loss = 4.43 (129.4 examples/sec; 0.989 sec/batch)\r\n2017-03-13 16:13:09.573551: step 50, loss = 4.42 (132.3 examples/sec; 0.967 sec/batch)\r\n2017-03-13 16:13:20.116336: step 60, loss = 4.49 (106.9 examples/sec; 1.197 sec/batch)\r\n```\r\n\r\n`nvidia-smi` shows that GPU-util is around 10% only.", "This is because K80 has no FP16 support.\r\nThe only card which really benefits from FP16 is a P100, all others have limited support.", "@icyblade \r\nNo. FP16 is half precision, not single precision. The stated 5591\u20138736 GFLOPS are single precision (FP32) performance. As sbrodehl mentioned: Up to now only the P100 supports FP16. Other cards have crippled FP16 support/performance", "I test an rnn model with float16 on Tesla P4, also slower than float32..", "@wodesuck I think P4 does not really support half precision natively. NVIDIA mentions only FP32 and INT8. I am observing similar behavior with P4", "@icyblade What changes did you do in Tensorflow, cuDNN5.1 to run inference for FP16 ? Can you please provide your code if possible", "See commit bcb8f33b788bd02f1b6bdf813129d5668c6c4e5e.", "@benoitsteiner  or anyone,  Please Help - **'half' is not a member of 'Eigen'**\r\n\r\n**Task :** \r\nI am trying to build TensorFlow Library with bazel using OpenCL in order to run the operations on GPU without enabling GPU  of Tensorflow (without CUDA/NVIDIA). \r\nBasically want to run on mobile GPU with the help of Android NDK and the built opencl-tensorflow library)\r\n\r\n**Steps Taken:** \r\nReplace the eigen_archive libray in /home/ashok/.cache/bazel/_bazel_ashok/337a0d8d7fbaddcaa8098e349718a6d6/external with\r\n'eigen-opencl' library of https://bitbucket.org/benoitsteiner/eigen-opencl and build TensorFlow using bazel\r\n\r\n**Issues Faced:  'half' is not a member of 'Eigen'**\r\nINFO: Found 1 target...\r\nERROR: /home/ashok/Ashok/tensorflow-1.0.1/tensorflow/core/BUILD:814:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib_lite' failed (Exit 1)\r\nIn file included from ./tensorflow/core/framework/allocator.h:25:0,\r\n                 from ./tensorflow/core/framework/op_kernel.h:22,\r\n                 from ./tensorflow/core/util/guarded_philox_random.h:19,\r\n                 from tensorflow/core/util/guarded_philox_random.cc:16:\r\n./tensorflow/core/framework/type_traits.h:69:52: error: 'half' is not a member of 'Eigen'\r\n       std::is_trivial<T>::value || std::is_same<T, Eigen::half>::value ||\r\n                                                    ^\r\n./tensorflow/core/framework/type_traits.h:69:52: error: 'half' is not a member of 'Eigen'\r\n./tensorflow/core/framework/type_traits.h:69:63: error: template argument 2 is invalid\r\n       std::is_trivial<T>::value || std::is_same<T, Eigen::half>::value ||\r\n                                                               ^\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\n\r\n\r\n\r\nKindly please provide an insight in resolving the issue\r\n", "i have a float 32 model, can i direct run it in float16 case? for pytorch it is ok. but tensorflow. i don't know how to run it as float 16, any body know about it?", "Use `dtype` `tf.float16` in your model, e.g. bcb8f33b788bd02f1b6bdf813129d5668c6c4e5e.", "do you mean train using float 16? currently I have a model which training using float 32. can I convert to float 16 model, In pytorch we just using as model.half(). I am not sure, how tensorflow support"]}, {"number": 1299, "title": "Convolution produces wrong result depending on the batch size.", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 LTS\nGraphics: GeForce GTX 770/PCIe/SSE2\nCuda compute capability: 3.0\ncuda version: 7.0\ncudnn version: 6.5\n\ninstalled from sources, commit hash: 5a30c8f07\n### Steps to reproduce\n1. Download the following numpy array in .npy format: https://www.dropbox.com/s/wljk6r83d0tee14/fail_tensor.npy?dl=0\n2. Below is shown the code to reproduce the problem. The numpy.float32 array stored in fail_tensor.npy has shape (4, 33, 33, 1). Only the first slice (a[0]) of the array holds the values that causes the problem. The values in the other slice can be any value. The original array with batch_size=4 will cause a problem, however if we use batch_size less than 4 (e.g., a[:3]), then there will be no problem. A convolution with a kernel of ones in the original array will produce a tensor with a negative number even though the input tensor has only non-negative values.\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.ndimage import convolve\n\ndef test(batch_size, set_others):\n    tt = np.load('fail_tensor.npy')\n    tt = tt[:batch_size]\n    tt[1:,...] = set_others\n    t = tf.constant(tt)\n    kernel = tf.constant(1.0, shape=[5, 5, 1, 1])\n    r = tf.nn.conv2d(t, kernel, [1, 1, 1, 1], padding='SAME')\n    sess = tf.Session()\n    print tt.shape\n    print tt[0].min()\n    rr = r.eval(session=sess)\n    print rr[0].min()\n    scipy_r = convolve(tt[0,:,:,0], np.ones((5,5), np.float32), mode='constant')\n    print np.allclose(scipy_r, rr[0,:,:,0])\n    sess.close()\n```\n\n3: On IPython:\n\n``` python\nIn [0]: import fail_example\n\n# with batch_size=3 the result is the same as scipy convolve.\nIn [1]: fail_example.test(batch_size=3, set_others=0)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n\n# with batch_size=4 the result ISN'T the same as scipy convolve.\nIn [2]: fail_example.test(batch_size=4, set_others=0)\n(4, 33, 33, 1)\n0.0\n-0.410374\nFalse\n\n# Setting set_others to 100 doesn't change the result\nIn [3]: fail_example.test(batch_size=3, set_others=100)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n\n# Setting set_others to 100 doesn't change the result\nIn [4]: fail_example.test(batch_size=4, set_others=100)\n(4, 33, 33, 1)\n0.0\n-0.410374\nFalse\n\n# smaller batch_sizes than 4 produces correct result\nIn [5]: fail_example.test(batch_size=2, set_others=0)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n\n# smaller batch_sizes than 4 produces correct result\nIn [6]: fail_example.test(batch_size=1, set_others=0)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n```\n", "comments": ["One more thing, I also tested in b2bac69ed2 that is from 11 days ago and the same problem happened there too.\n", "@zheng-xq: I vaguely recall this coming up in another context.  Have we hit anything like this?\n", "I've go through the repro instructions, and all the test cases are passing\nfor me locally.\n\ntest(batch_size=3, set_others=0)\ntest(batch_size=4, set_others=0)\ntest(batch_size=3, set_others=100)\ntest(batch_size=4, set_others=100)\ntest(batch_size=2, set_others=0)\ntest(batch_size=1, set_others=0)\n\nMy environment: Titan-X GPU, Cuda 7.5 SDK, and Cudnn R4.\n\nWhich Cudnn version are you using? I would highly recommend to try Cudnn R4\nand see if this problem would go away.\n\nOn Tue, Mar 8, 2016 at 10:54 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @zheng-xq https://github.com/zheng-xq: I vaguely recall this coming up\n> in another context. Have we hit anything like this?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1299#issuecomment-193913374\n> .\n", "I am using cudnn R2. I will try to install cudnn R4 in my machine in a few days.\n", "@cesarsalgado did you resolve your issue with cudnn r4 ? \n", "Closing for now.  @cesarsalgado: Please reopen if the issue persists with cuDNN R4. \n"]}, {"number": 1298, "title": "Removed a broken link in index.md", "body": "Removed two broken links in index.md. One link referred to CIFAR-10 samples and another referred to CIFAR- Learning rate decay. However, these were broken links. So, I removed the links and the references to these links.\n", "comments": ["Can one of the admins verify this patch?\n", "Can one of the admins verify this patch?\n"]}, {"number": 1297, "title": "Exception thrown running  example/udacity/5_word2vec", "body": "Hi all. Does anyone reproduce this? The description of the problem is below in the suggested format. A brief summary is: running the examples/udacity/4_convolutions.ipynb runs fine on gpu, but the 5_word2vec notebook fails  with an AdaGrad-related exception: \n\n`InvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'`\n\nthough I never modified the code (except to print some things out). I never modified code to assign any ops to a device, as it seems to be claiming.\n\nThis looks at least superficially related to [this issue](https://github.com/tensorflow/tensorflow/issues/505).\n\nThanks,\nJim\n### Environment info\n\nOperating System: Ubuntu 14.04 LTS\n\n**If installed from binary pip package, provide:**\nBuilt from r0.7 source.\n\n**1. Which pip package you installed.**\n`sudo pip install --upgrade /tmp/tensorflow_pkg/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`\n\n**2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".**\n\n```\njd@jd-All-Series:~/dev/thirdparty/scikit-learn/scikit-learn$ python -c \"import tensorflow; print(tensorflow.__version__)\"\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally\n\n0.7.1\n```\n\n**If installed from sources, provide the commit hash:**\n`41fcc7edd63c01dfd56382be4755c90f5a7eb565`\n### Steps to reproduce\n1. Run `jupyter notebook` in the examples/udacity/ directory.\n2. Open the 5_word2vec.ipynb notebook in the web browser (from http://localhost:8888).\n3. Execute each of the steps through the first (Google-provided) session.run().\n4. Observe the exception when you get to the first session.run() after 'Train a skip-gram model.' step, which is provided by notebook (before I have to write any code). The exception happens on the line with session.run() invocation.\n\n`InvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n     [[Node: Adagrad/update_Variable_2/SparseApplyAdagrad = SparseApplyAdagrad[T=DT_FLOAT, Tindices=DT_INT64, use_locking=false](Variable_2, Variable_2/Adagrad, Adagrad/learning_rate, gradients/concat_2, gradients/concat_3)]]`\n\nI have not modified the python code assign to do any device assignments in this notebook. \n### What have you tried?\n1. I've been working through the Udacity DeepLearning course. After encountering this error, I killed the jupyter kernel for 5_word2vec, and re-run my previously-saved notebook for the fourth exercise, 4_convolutions.ipynb, and it executes without error on the gpu. Which I think must confirm that TensorFlow is installed OK. \n\nDone for the day, but when I come back, I will probably try building without gpu support, and see if that runs OK so I can at least carry on with the course.\n### Logs or other output that would be helpful\n\nPlease let me know if there's anything you'd like me to log. Hardware is a Titan-X.\n", "comments": ["That other problem you reference should be fixed:\nhttps://github.com/tensorflow/tensorflow/issues/1117\nSo that's likely a different issue. I have never tested the class notebooks on GPU.\nJust to make sure: your setup works fine on CPU?\n", "Does using:\n`with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as session:` fix it?\n", "> Just to make sure: your setup works fine on CPU?\n\nI hadn't tried that.  So I just now tested on the cpu by adding tf.device as here:\n\n``` python\nwith graph.as_default():\n     with tf.device('/cpu:0'):\n```\n\nand it did indeed run successfully. I'm not sure if there is some other way to make it run on the cpu, other than build without gpu support.\n", "> Does using:\n> with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as session: fix it?\n\nNo, I get a different exception. Namely, I removed the tf.device() (shown above) so that it goes back to trying to run on the gpu, and specified the config argument you specify, and now I get this exception on the session.run() call:\n\n```InvalidArgumentError: AttrValue must not have reference type value of float_ref\n     for attr 'tensor_type'\n    ; NodeDef: Variable/Adagrad/_84 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_550_Variable/Adagrad\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^gradients/concat_6/_86, ^gradients/concat_7/_88); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\n     [[Node: Variable/Adagrad/_84 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_550_Variable/Adagrad\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^gradients/concat_6/_86, ^gradients/concat_7/_88)]]\n\n```\n```\n", "@mrry here is a tiny colab notebook which works fine on CPU, but fails on GPU even with soft placement in r0.7. Any idea?\n", "As a sanity check: I repeated the cases above by rotating through each of the three states of the code and reproduced the behaviors I described above: 1) original code, running on GPU fails, 2) adding tf.device('/cpu:0') runs successfully, 3) soft placement fails with the different exception.\n\nThanks for the help! \n", "I think this is a known issue (at least offline), where running the initializer will cause all variables to be assigned to the GPU, then subsequently trying to run SparseApply<FOO> on one of those variables will fail because SparseApply<FOO> doesn't have a GPU implementation. (@vrv has an idea for how to fix it, but it might break other users, so we haven't pushed it yet :(.) The workaround is to pin just the offending variable to CPU using `with tf.device(\"/cpu:0\"):`.\n\nI think for the [notebook in question](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb), this involves changing the line:\n\n``` python\nembeddings = tf.Variable(\n  tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n```\n\n...and replacing it with:\n\n``` python\nwith tf.device(\"/cpu:0\"):\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n```\n", "@mrry Thanks!\nDo you prefer me to close this, or leave it open?\n", "Please leave it open. I need to add the workaround to the notebook. (PRs gladly accepted)\n", "@mrry Hmm unfortunately just assigning `embeddings` to the cpu didn't work for me. Still got the same exception. So I resorted to the expedient of putting the whole graph on the cpu.\n", "Sorry for the confusion... it looks like you might also need to pin the `softmax_weights` and `sotmax_biases` to `\"/cpu:0\"`. I hadn't read the following code closely: these are both used in `tf.nn.sampled_softmax_loss()`, which uses `tf.nn.embedding_lookup()` internally, and thus results in sparse updates in the training step.\n", "I ran into the same problem and can confirm that the workaround works when pinning all three of embeddings, softmax_weights and softmax_biases. Thanks!\n", "Right, I confirm too.\n", "Merged a workaround that pins everything to CPU. Please follow #1310 for the actual fix to make it work on GPU as well.\n"]}, {"number": 1296, "title": "did grammatical clean up", "body": "did grammatical clean up\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks, can you update to master?\n", "Merged\n"]}, {"number": 1295, "title": "miss file after install tensorflow", "body": "After installing tensorflow, I found only one tensorflow folder in the path tensorflow/inlcude. In fact, it shoud consist of many other folders such as third_party, google, Eigen and so on. I don't known the reason why I could not find these folders. The way which I used to install tensorflow is \"sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\"\n\nThanks\n", "comments": ["Can you explain what you want to do with these files?\n\nIt's possible this is a duplicate of https://github.com/tensorflow/tensorflow/issues/720: we don't yet have a devel package that installs all of the headers -- the pip package just distributes the python interface and C++ runtime, not the code to do development on it.  You currently have to integrate with bazel and our github tree to do development, but we're hoping to get #720 resolved eventually to make it easier.\n", "@vrv, I need these files when I was adding a new Op to tensorflow with TensorFlow binary installation. Some files need heads in the path like /third_party/.., but I can not find third_party anywhere. I saw these third_party folders in others tensorflow/inlcude path, howerver, they are not in mine. As a result. when I used g++ to compile it, it told me that it could not find the files.\n\nThanks\n", "Gotcha -- yes I think this is related to https://github.com/tensorflow/tensorflow/issues/1270 -- we have a fix to the pip package installation to make these available.  Our test nodes are down for the weekend so we'll push on Monday.  Let's take discussion to #1270.\n"]}, {"number": 1294, "title": "Automated Docker image build and test", "body": "1) Breaking up the pip.sh into the install and test-on-install\n\nThis is aimed at automating testing of docker images.\nAlso, a step is added in pip.sh to uninstall existing versions of\nprotobuf and tensorflow if they exist. This addresses pip install-test\nissues on non-Docker environments (e.g., Mac)\n\n2) Using virtualenv to perform pip test-on-install\n\nThe built pip-package is installed in a virtualenv, then the pip install test and the tutorial\ntest are carried out inside the virutal env. This is done inside the same docker container as the one used for bazel build, if docker is available. \n\n3) Automating Docker image build and test\n\nci_parameterized_build now has a new env-var parameter:\nTF_BUILD_DOCKER_TEST, that can be used to trigger Docker image build\nand test following the PIP build (i.e., TF_BUILD_IS_PIP=PIP or BOTH).\n\nThe image is automatically tagged as ${USER}/tensorflow:${VERSION},\nwherein the version is extracted from version.h.\n\nThe main script for Docker build and test is the newly added:\ndocker_test.sh\n\nIt uses Dockerfiles in tools/ci_build/docker and installs the whl files\ninside the Docker image. After the pip installation step, the Python\nunit tests and tutorial tests are inside the Docker container.\n", "comments": ["Can one of the admins verify this patch?\n", "Splitting the test out of pip.sh is good. Using virtualenv is good. Testing the docker images is, imho, nice to have but not critical for now. We should not change the docker files. Also putting docker files testing into parametrized build is, imho, an overkill.\n", "Addressed @jendap's comments. Closing this PR and opening a new fresh one with all changes squashed. https://github.com/tensorflow/tensorflow/pull/1380\n"]}, {"number": 1293, "title": "Gradient for tf.diag", "body": "tf.diag doesn't support the gradient (as far as I can see here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L161)\nIs it difficult to add support for that?\nThank you\nSome simple example:\n\n```\ntSv = tf.Variable(tf.truncated_normal([r]))\nt = tf.diag(tSv)\ntrain = tf.train.GradientDescentOptimizer(0.01).minimize(t)\n\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\nsess.run(train)\n...\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-96-28cf62ca1be1> in <module>()\n      1 tSv = tf.Variable(tf.truncated_normal([r]))\n      2 t = tf.diag(tSv)\n----> 3 train = tf.train.GradientDescentOptimizer(0.0001).minimize(t)\n      4 \n      5 sess = tf.Session()\nValueError: No inputs provided\n```\n", "comments": ["Note quite sure what you meant by minimizing a matrix `t` in your example.\nI do agree that it would be great to support gradient on tf.diag.\nIf you want to optimize for matrix trace, there's a workaround as follows.\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nwith tf.Graph().as_default():\n    Y = np.random.rand(3,3).astype(np.float32)\n    X = tf.Variable(np.random.rand(3,3).astype(np.float32))\n    Z = tf.matmul(X,Y)\n\n    ones = np.ones(3).astype(np.float32) \n    eye = tf.diag(ones)\n    loss = tf.reduce_sum(tf.mul(Z, eye))\n    optimizer = tf.train.AdagradOptimizer(0.01).minimize(loss)\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    with sess.as_default():\n        for i in range(100):\n            sess.run(optimizer)\n            print sess.run(loss)\n            print X.eval()\n```\n", "Thank you very much, @cameronphchen! This is a very nice workaround which is worth mentioning in the documentation. I am new to tensorflow and it took me a while to realise that derivative on diag is not implemented.\nThanks again for your help, this resolves my issue.\n", "Once https://github.com/tensorflow/tensorflow/pull/1265 is in place this becomes very easy to fix.\n", "I should have the python code implementation (which is like 2 lines).  I'll need to add tests though.\n", "@girving Yes, I have been following that thread. It would be great to add matrix trace operator as well! I can help with these when #1265 is done. \n", "@chemelnucfin Oh, that's great! Let me know if you need an extra hand. \n", "@cameronphchen feel free to work on them!  But do the trace first.  I'm taking a look at something else at the moment.  I'll put a note here when I get back to the diag grad test.  \n", "sounds good! I'll work on trace then. \n", "I have implemented trace and planning to start working on its gradient. The current approach is essentially using `reduce_sum` and `diag_part` for calculating trace. This behavior will be consistent within tensorflow but quite different from `numpy.trace` though. `numpy.trace` output trace of 2D subarrays specified by two input arguments `axis1` and `axis2`. Is this going to be an issue? Should I open a new issue for trace discussion? Since this one is for tf.diag gradient. \n\nBtw, this is my first time contributing to tensorflow, any guidance or suggestion would be appreciated. Thanks!\n"]}, {"number": 1292, "title": "RMSPropOptimizer incompatible with embedding layers.", "body": "### Description\n\n`RMSPropOptimizer` does not work with models utilizing embedding layers, which appears to be a consequence of it not implementing `_apply_sparse`. The other optimizers all implement `_apply_sparse`, so I assume this is an oversight.\n### Environment info\n\nOperating System: Ubuntu 14.04 LTS\nInstalled from source with hash: 03bff43060229357cbe2cc1659e7d129c2799b06\n### Steps to reproduce\n\n```\nimport numpy as np\nimport tensorflow as tf\n\nsess = tf.Session()\n\nweights = tf.get_variable('weights', [100, 32], 'float32', trainable=True)\nwords = tf.constant(np.arange(100, dtype='int32'))\nlogits = tf.nn.embedding_lookup(weights, words)\nlabels = tf.constant(np.arange(100, dtype='int64') % 32)\nloss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\noptimizer = tf.train.RMSPropOptimizer(0.001)\nstep = optimizer.minimize(loss)\n```\n### Logs\n\n[log.txt](https://github.com/tensorflow/tensorflow/files/147495/log.txt)\n", "comments": ["Duplicate of #464 (contributions welcome!)\n"]}, {"number": 1291, "title": "Messy graph edges", "body": "### Steps to reproduce\n1. python tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n2. python tensorflow/tensorboard/tensorboard.py --logdir=/tmp/mnist_logs\n3. Expand 'xent' node: edge from 'y-input' to 'mul' ops has an S-like shape instead of a more appropriate  C-like.\n   See the image below:\n\n![screen shot 2016-02-25 at 2 37 18 pm](https://cloud.githubusercontent.com/assets/953399/13337087/731d612c-dbce-11e5-919a-f07ca9da6a2a.png)\n", "comments": ["@chihuahua PTAL\n", "I lack a solution ATM since the graph visualizer basically defers to dagre-d3 for graph layout. dagre-d3 treats graph layout as an optimization problem that trades off speed vs a quality layout. See https://github.com/cpettitt/dagre/wiki#recommended-reading for specifics.\n\nIn this case, it is odd how the arrow from y-input to the overall scope node is curved (and not a beeline for the node). Maybe we're somehow compelling the arrow's head to land on the left side.\n", "The code has changed substantially since the issue has opened. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 1290, "title": "Update retrain.py to be compatible with Python 3", "body": "It's PROBABLY backwards compatible with Python 2 but I haven't fully tested it\n\nFixes https://github.com/tensorflow/tensorflow/issues/1274\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it! As Arterys, Inc.\n", "Updated commit e-mail address to match my CLA e-mail address (Arterys, Inc.) and force pushed.\n", "Can you go to https://cla.developers.google.com/clas and check that the corp CLA for Arterys shows up as \"CLAs I am covered by\"?\n", "@martinwicke sorry for the confusion; _now_ I see the CLA listed at https://cla.developers.google.com/clas.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks, can you update to master?  Then we can test.\n", "@vrv Rebased onto master\n", "@tensorflow-jenkins: test this please.\n", "Our mac tests are broken, but this otherwise lookss good. merging.\n"]}, {"number": 1289, "title": "Use external repository for protobuf dependency.", "body": "Currently, TensorFlow uses a Git submodule for its dependency on\nprotobuf. This was due to the hack used in protobuf for building Python\nsupport with Bazel, which was required since Bazel's Python rules did\nnot support adding directories to `PYTHONPATH`. Now that the new\n`imports` attribute has been added to the Python rules in Bazel 0.2 and\nthe hack for Python support in protobuf has been removed, this change\nremoves the `google/protobuf` Git submodule and adds an external\nrepository for including protobuf.\n\nFixes #1069\n", "comments": ["Can one of the admins verify this patch?\n", "So this require the entire world to upgrade to bazel 0.2, right?\n\n@tensorflow-jenkins: test this please, but note that this probably will fail if the answer to the above question is \"yes\", since we haven't updated Jenkins to bazel 0.2 yet.\n", "@vrv - Yes, this would require everyone to upgrade to Bazel 0.2.\n\nAlso, the `git_repository` rule I added for including the protobuf repo references the current protobuf HEAD since my patch for removing the Python hack was just merged (see google/protobuf#1233). Is this fine or should we wait until the next protobuf release?\n", "I see that the Jenkins jobs are failing since the CI machines are running an older version of Bazel, which does not yet contain the `imports` property for Python rules.\n", "We are going to upgrade bazel to 0.2 today anyway in https://github.com/tensorflow/tensorflow/pull/1206. We will update mac as well and this will work :)\n", "I have rebased this patch on HEAD, resolved merge conflicts and made some additional changes since there are now additional targets that use protobuf.\n", "@tensorflow-jenkins : test this please\n", "Can you bump the version of required bazel https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE#L21 here and then we can re-test?\n", "Done.\n", "@tensorflow-jenkins test this please\n", "Some target in protobuf is missing a `srcs_version = \"PY2AND3\"` annotation. See \nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/262/consoleFull\n", "Also looks like build_pip_package needs to be updated.\n", "@martinwicke There seems to be a few. I'll submit a PR to protobuf to add the missing `srcs_versions`.\n\n@vrv Will do.\n", "Opened google/protobuf#1402. \n\nWhat is the Bazel command run for http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/262/consoleFull? I tried `bazel test //...` but I got an error about the BUILD file for gmock not being found:\n\n```\n\u276f\u276f\u276f bazel test //...\nERROR: /usr/local/google/home/dzc/Projects/tensorflow/tensorflow/tensorflow/workspace.bzl:21:3: no such package '@gmock_archive//': In new_http_archive rule //external:gmock_archive the 'build_file' attribute does not specify an existing file (/usr/local/google/home/dzc/Projects/tensorflow/tensorflow/google/protobuf/gmock.BUILD does not exist) and referenced by '//external:gtest'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.190s\nERROR: Couldn't start the build. Unable to run tests.\n```\n\nEdit: Found the problem. It was because it was looking for gmock.BUILD in `google/protobuf`. This will need to be changed.\n", "The exact command is in the log, but the main point is bazel test\n//tensorflow/...\n\nOn Fri, Apr 8, 2016 at 14:13 David Z. Chen notifications@github.com wrote:\n\n> Opened google/protobuf#1402 https://github.com/google/protobuf/pull/1402.\n> \n> What is the Bazel command run for\n> http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/262/consoleFull?\n> I tried bazel test //... but I got an error about the BUILD file for\n> gmock not being found:\n> \n> \u276f\u276f\u276f bazel test //...\n> ERROR: /usr/local/google/home/dzc/Projects/tensorflow/tensorflow/tensorflow/workspace.bzl:21:3: no such package '@gmock_archive//': In new_http_archive rule //external:gmock_archive the 'build_file' attribute does not specify an existing file (/usr/local/google/home/dzc/Projects/tensorflow/tensorflow/google/protobuf/gmock.BUILD does not exist) and referenced by '//external:gtest'.\n> ERROR: Loading failed; build aborted.\n> INFO: Elapsed time: 0.190s\n> ERROR: Couldn't start the build. Unable to run tests.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1289#issuecomment-207608299\n", "Can you update the commit hash for protobuf?\n\n@vrv, @keveman: Just to make sure -- this shouldn't interact with our binary protobuf distribution, right?\n", "I have updated the commit hash in `tensorflow/workspace.bzl`. I'm still debugging some build and test failures that I found when running `bazel test //tensorflow/...` on my Goobuntu machine (see below), but let's also run the CI to see if there is anything else that needs to be fixed.\n\n```\n\u276f\u276f\u276f bazel test //tensorflow/...\nWARNING: /usr/local/google/home/dzc/Projects/tensorflow/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future\nINFO: Found 709 targets and 418 test targets...\nINFO: From Compiling external/protobuf/src/google/protobuf/compiler/js/js_generator.cc [for host]:\nexternal/protobuf/src/google/protobuf/compiler/js/js_generator.cc: In function 'std::string google::protobuf::compiler::js::{anonymous}::JSByteGetterSuffix(google::protobuf::compiler::js::{anonymous}::BytesMode)':\nexternal/protobuf/src/google/protobuf/compiler/js/js_generator.cc:492:1: warning: control reaches end of non-void function [-Wreturn-type]\n }\n ^\n\nERROR: /usr/local/google/home/dzc/.cache/bazel/_bazel_dzc/97010c3bd7554ebe07410261582c4d5e/external/protobuf/BUILD:581:1: C++ compilation of rule '@protobuf//:pyext/_message.so' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,no\nw -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -iquote ... (remaining 48 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1\nIn file included from external/protobuf/python/google/protobuf/pyext/extension_dict.cc:34:0:\nexternal/protobuf/python/google/protobuf/pyext/extension_dict.h:51:7: error: expected nested-name-specifier before 'shared_ptr'\n using shared_ptr;\n       ^\nexternal/protobuf/python/google/protobuf/pyext/extension_dict.h:51:7: error: 'shared_ptr' has not been declared\nexternal/protobuf/python/google/protobuf/pyext/extension_dict.h:67:3: error: 'shared_ptr' does not name a type\n   shared_ptr<Message> owner;\n   ^\nIn file included from external/protobuf/python/google/protobuf/pyext/extension_dict.cc:43:0:\nexternal/protobuf/python/google/protobuf/pyext/message.h:56:7: error: expected nested-name-specifier before 'shared_ptr'\n using shared_ptr;\n       ^\nexternal/protobuf/python/google/protobuf/pyext/message.h:56:7: error: 'shared_ptr' not declared\nexternal/protobuf/python/google/protobuf/pyext/message.h:57:12: error: 'std::std' has not been declared\n using std::std::string;\n            ^\nIn file included from external/protobuf/python/google/protobuf/pyext/extension_dict.cc:44:0:\nexternal/protobuf/python/google/protobuf/pyext/repeated_composite_container.h:53:7: error: expected nested-name-specifier before 'shared_ptr'\n using shared_ptr;\n       ^\nexternal/protobuf/python/google/protobuf/pyext/repeated_composite_container.h:53:7: error: 'shared_ptr' not declared\nIn file included from external/protobuf/python/google/protobuf/pyext/extension_dict.cc:45:0:\nexternal/protobuf/python/google/protobuf/pyext/repeated_scalar_container.h:52:7: error: expected nested-name-specifier before 'shared_ptr'\n using shared_ptr;\n       ^\nexternal/protobuf/python/google/protobuf/pyext/repeated_scalar_container.h:52:7: error: 'shared_ptr' not declared\nexternal/protobuf/python/google/protobuf/pyext/extension_dict.cc: In function 'google::protobuf::python::ExtensionDict* google::protobuf::python::extension_dict::NewExtensionDict(google::protobuf::python::CMessage*)':\nexternal/protobuf/python/google/protobuf/pyext/extension_dict.cc:251:9: error: 'google::protobuf::python::ExtensionDict' has no member named 'owner'\n   self->owner = parent->owner;\n         ^\nexternal/protobuf/python/google/protobuf/pyext/extension_dict.cc: In function 'void google::protobuf::python::extension_dict::dealloc(google::protobuf::python::ExtensionDict*)':\nexternal/protobuf/python/google/protobuf/pyext/extension_dict.cc:258:9: error: 'google::protobuf::python::ExtensionDict' has no member named 'owner'\n   self->owner.reset();\n         ^\n\nINFO: Elapsed time: 44.875s, Critical Path: 33.67s\nFAIL: //tensorflow/contrib/ctc:ctc_decoder_ops_test\nFAIL: //tensorflow/contrib/ctc:ctc_loss_op_test\nFAIL: //tensorflow/contrib/distributions:gaussian_conjugate_posteriors_test\n\n~~~ snipped ~~~\n```\n", "@tensorflow-jenkins test this please  (to see what's broken)\n", "master tests are failing right now, so don't expect a clean build.\n", "or not. Never mind.\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please\n", "FYI google/protobuf#1586 should be merged soon. Once that PR is merged, I'll update this PR to point the git commit to the new protobuf HEAD and rebase again.\n", "ready to test?\n", "Yes, just rebased, updated, and pushed.\n", "@tensorflow-jenkins test this please\n", "that failed quickly: ERROR: /workspace/tensorflow/workspace.bzl:84:3: no such package '@gmock_archive//': In new_http_archive rule //external:gmock_archive the 'build_file' attribute does not specify an existing file (/workspace/@protobuf/:gmock.BUILD does not exist) and referenced by '//external:gtest'.\n", "Odd. Looking\n", "Seems that this is caused by a bug in this version of Bazel that is fixed in a later version. In any case, it is probably better to have `gmock.BUILD` live in the tensorflow repo for now, similar to those for other dependencies, until we eventually upgrade to a version of gmock/gtest that has its own BUILD file.\n\nCan you start the tests again?\n", "test this please\n", "Almost, looks like the pip install doesn't work\n\nPython binary path to be used in PIP install: /usr/bin/python (Major.Minor version: 2.7)\nThu May 26 05:26:43 UTC 2016 : === Using tmpdir: /tmp/tmp.PJlk9Vtu7r\nrsync: link_stat \"/workspace/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/google\" failed: No such file or directory (2)\nrsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.0]\nbuild_pip_package FAILED\n", "I'll take a look. Is the command that the script used to invoke `build_pip_package` something along the lines of `bazel run //tensorflow/tools/pip_package:build_pip_package -- /path/to/destination_dir`?\n", "Yeah see the section called Create the pip package and install on https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html\n", "Should be fixed now. Not sure if this will work but:\n\n@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Looks like all the tests passed this time, though on the GitHub UI, the MacOS CPU Tests is still marked as BUILDING even though it has succeeded on Jenkins. Could it be related to this message?\n\n```\nExecuted 492 out of 492 tests: 492 tests pass.\n\nParameterized build ends with SUCCESS at: Thu May 26 16:40:30 UTC 2016 (Elapsed time: 1249 s)\nUnable to get pull request builder trigger!!    <~~~~~\nSetting status of 278cc4698b757c63edb8248aa051218435cf6415 to SUCCESS with url http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/552/ and message: 'SUCCESS\n '\nUsing context: Linux CPU Tests (Python 3)\nFinished: SUCCESS\n```\n\nEdit: or maybe not since other logs have that message too. Perhaps there's just some delay.\n", "Sweet, this is awesome.  @martinwicke @caisq okay to merge?  (I'll have to do some work internally to bring this change in, but I'll sign up for that this week).\n", "\\o/ Thanks!\n", "Woot\n\nOn Thu, May 26, 2016 at 3:25 PM David Z. Chen notifications@github.com\nwrote:\n\n> \\o/\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1289#issuecomment-222012562,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_VGti6nugRU9vmAFc_GOhwHn_B-1ks5qFh3NgaJpZM4HjOh8\n> .\n", "Yay!\n", "I understand that Bazel is the preferred build tool for TensorFlow. However, it's worth noting that this change has some consequences for developers woking with alternative build tools:\n1. Since the git --recursive option is no longer going to clone Protobuf too, it will have to be cloned manually to get access to the required header files. The documentation needs to be updated to reflect that.\n2. The exact commit version of Protobuf to clone is buried in tensorflow/workspace.bzl rather than in the handy google/protobuf link. This is a pity as having it the original way means letting git worry about checking out the right version. The way it is configured now, future changes to the Protobuf version will need to be updated in several different build instruction files.\n3. As the google/protobuf path no longer exists, any Include Path directives will have to be modified to point to the new local Protobuf clone location. This is also a shame as having a standard folder google/protobuf in the workspace meant configuring the Include Path once in the build file and then forgetting about it. Now there'll be an extra step for each new developer to change it to point to his/her own Protobuf location.\n\nI'd also add a more general request. This kind of change to the repo structure should have a minor release number with some release notes. It took me a good while to figure out why all the Protobuf headers had disappeared from my build environment after I merged in the latest tensorflow:master changes. I thought I had deleted them somehow and went around in circles before finding this PR.\n", "We are sorry for the inconvenience @StephenOman!\n\nIt is much better to download git repos using bazel. Also for all the builds there is a lot of them, not just protobuf.\n\nWhat build tool are you using? We have also have (incomplete):\n1) [contrib/cmake](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake) - it may not work today but could be made work with some effort\n2) [contrib/makefile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) - this one is super new. It is there to build the ios example. We will create CI build for it next week.\n\nPlease take a look at those two. Please contribute if one of them would suite you. Once we get them into reasonable state we can enable builds for them in ci.tensorflow.org. That way we make sure no change break that. No more breakage for you :)\n", "Thanks for the note @jendap.\n\nI'm wrapping the current C++ API with an Objective C++ interface in order to build a Swift-based iOS app. Complicated, I know! To do this, I'd created a set of Xcode projects to build both Protobuf and TensorFlow libs and am managing the build from there.\n", "Take a look at [contrib/makefile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile). We have added it exactly for this reason. It will be recommended way of working in xcode. We will soon have automated build this so it should work in future. But it is very new (may be even broken right now).\n", "Right, I see where you are going with this. Thanks.\n"]}, {"number": 1288, "title": "Rint Operator", "body": "round to nearest operator #709 \n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Had to force push the branch and lost the comments.  I removed the google/protobuf commit, and @girving was mentioning about @martinwicke 's CL.  Also about the using directives being moved elsewhere.\n", "Yeah, can you remove the change to stream_executor? If it still works after that change (which I expect it would -- my change was almost identical except for that), I'll merge it.\n", "@martinwicke Removed from stream_executor.  Thanks.\n", "Jenkins, test this please.\n", "Ah. backwards_compatibility_test. Check the build log -- it's a simple thing to fix, the console log contains instructions. Add the changed files to this PR.\n", "@martinwicke I added manually to the ops.pbtxt (which I'm guessing is autogenerated).  However, I don't have the tensorflow/core/ops/compat/ops_history.v0.pbtxt file from this commit c1a40c7b9085719cb120ae92f991f86d0757af7e \n\nI don't have the entire tensorflow/core/ops/compat directory or the associated test with it (which is why my tests passed, I guess)\n", "You should be able to run \n\nbazel-bin/tensorflow/core/ops/compat/update_ops tensorflow/core/ops\n\nand things will work\n", "@vrv I don't have that directory either.  I have searched for update_ops.  Is that an option in configuration or maybe just an internal directory?\n", "@vrv @girving @martinwicke looks like the compat directory was put in.  I checked in the op history files and this should be ready to go!\n", "Can you rebase? Sorry I sat on this.\n", "Jenkins, test this please.\n", "@martinwicke Do you want me to update the branch still?  I don't want you to have to run another test.\n", "yes, please update, I can't merge without (we discontinued our merge script and now we're stuck with github's button)\n", "ok.  Let me see if I can update from github and merge it or if I need to\nrebase and push from my end.\n\nOn Tue, Mar 15, 2016 at 1:15 PM, Martin Wicke notifications@github.com\nwrote:\n\n> yes, please update, I can't merge without (we discontinued our merge\n> script and now we're stuck with github's button)\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1288#issuecomment-197002878\n", "if you update from github it'll make an ugly merge commit. It's really bad\nactually. and they don't allow merges any more if the PR is not rebased to\nhead. Silly.\n\nOn Tue, Mar 15, 2016 at 1:30 PM chemelnucfin notifications@github.com\nwrote:\n\n> ok. Let me see if I can update from github and merge it or if I need to\n> rebase and push from my end.\n> \n> On Tue, Mar 15, 2016 at 1:15 PM, Martin Wicke notifications@github.com\n> wrote:\n> \n> > yes, please update, I can't merge without (we discontinued our merge\n> > script and now we're stuck with github's button)\n> > \n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly or view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/pull/1288#issuecomment-197002878>\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1288#issuecomment-197007761\n", "Should be fine now.  Thanks for putting up with the hassle!\n", "@martinwicke: we can still merge when the branch is out of date, as long as there is no conflict.  It's just that they recently changed the icon to match the same one that indicates conflict, so it's hard to tell :)\n", "I guess the reasoning is that if you merge, you never know whether the merge is any good.\n", "@vrv You rolled this back, right? It's not at head -- do you remember what the problem was?\n"]}, {"number": 1287, "title": "Unable to visualize Inception v3 graph in TensorBoard with TensorFlow 0.7.1", "body": "### Summary\n\nAttempting to visualize the Inception v3 graph with TensorBoard results in an empty graph (after several minutes of loading).\n\n_Update: an earlier version of this issue indicated that the progress bar hung forever, but apparently, I just didn't wait long enough._\n### Environment info\n\nOperating System: OS X 10.11.3, Chrome 48.0.2564.116, Anaconda 1.2.2\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed: https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".: `0.7.1`\n### Steps to reproduce\n1. Downloaded and un-tar the [inception v3 model](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz). The graph protobuffer is in `/tmp/imagenet/classify_image_graph_def.pb`.\n2. Run this code to dump the graph:\n\n``` python\n    import os\n    import os.path\n    import tensorflow as tf\n    from tensorflow.python.platform import gfile\n\n    INCEPTION_LOG_DIR = '/tmp/inception_v3_log'\n\n    if not os.path.exists(INCEPTION_LOG_DIR):\n        os.makedirs(INCEPTION_LOG_DIR)\n    with tf.Session() as sess:\n        model_filename = '/tmp/imagenet/classify_image_graph_def.pb'\n        with gfile.FastGFile(model_filename, 'rb') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            _ = tf.import_graph_def(graph_def, name='')\n        writer = tf.train.SummaryWriter(INCEPTION_LOG_DIR, graph_def)\n        writer.close()\n```\n1. Run tensorboard: `tensorboard --logdir /tmp/inception_v3_log`\n2. Navigate to graphs tab at http://0.0.0.0:6006/#graphs\n\n**Expected result:** the graph\n**Actual result:** Empty graph screen (after several minutes of loading with no movement of the progress bar)\n\nA 91 MB file (same size as the graph protobuffer) called `events.out.tfevents.1456423256.[hostname]` is correctly saved to the log directory, so it seems that the graph is in there somewhere.\n### What have you tried?\n1. Installing Python 2 pip version of TensorFlow 0.7.1 in a separate conda environment; same results.\n2. Running [mnist_summaries_example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py); graph is shown in TensorBoard properly, so this is a problem with the Inception model\n", "comments": ["Maybe related to #716\n", "Hi @dgolden1,\n\nThanks for reporting and taking the time to include a clean repro. Would you mind trying this setup against the master branch? We've been doing some work in improving the pipeline for large graphs, so it might be that this is already fixed at head.\n", "Sorry, @danmane, I get the same results on the current master (b1bb0bb3dd733224f243b1eb2c4432c4977ae37e) built from source on OS X with both Anaconda Python 2.7.11 and Anaconda Python 3.5.1 (each in their own separate environments).\n", "I built from source on Ubuntu 14.04 and met the same issue. Any updates on this?\n", "dsmilkov is out for the rest of the week but I will be investigating this today\n", "From what I see locally, it seems like this was fixed in Tensorboard in commit [3212eb3](https://github.com/tensorflow/tensorflow/commit/3212eb3c874452ea928e23246e524b147f31cd15#diff-ec1b56f78fd3bd9bf0a48af0a61dd71f). Basically, the graphdef contains huge embedded constant tensors, making the graphdef size too large for the client to handle when it is based from Tensorboard server to the client browser. That commit adds server-side filtering out of large embedded constants, making the client able to handle the served graph data.\n\nSo, building Tensorboard from scratch on master should allow visualization of inception_v3. Also, the next tagged release should also include a rebuilt Tensorboard with the fix.\n\n@dgolden1 and @ffmpbgrnn, did you rebuild the Tensorboard frontend and backend explicitly? Perhaps rebuilding from the TF root doesn't rebuild the Tensorboard components?\n", "Thanks for the help, @jameswex. I tried building from source again, this time on Ubuntu 14.04 with the latest master (13ea3ca91ba5aecab6f21acc14b9cb6a9afa8630) using Anaconda Python 3.5.1. Results are the same: no graph is displayed.\n\nI built via:\n\n```\nbazel clean\n./configure  # CPU-only\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n# ...etc\n```\n\nI also tried explicitly building tensorboard and running it like:\n\n```\nbazel build tensorflow/tensorboard:tensorboard\n./bazel-bin/tensorflow/tensorboard/tensorboard --logdir /tmp/inception_v3_log/\n```\n\nwith the same result.\n", "It turns out that just a bazel build will not fully rebuild the tensorboard front-end (just the back-end of tensorboard). If you want to manually rebuilt the tensorboard front-end, its currently a multi-step process.\n\nI believe in addition to the bazel build of tensorboard, you should also run \"gulp vulcanize\" in the tensorboard directory to rebuild the front-end HTML that communicates with the tensorboard back-end (see the tensorboard README.md for dependencies for running gulp commands).\n\n@danmane, can you confirm if there are additional steps beyond gulp vulcanize and bazel build? Thanks.\n", "In general, using gulp vulcanize then bazel build will get you the latest and greatest TensorBoard..\nAlthough now that I [released a new compiled TensorBoard](https://github.com/tensorflow/tensorflow/commit/65c9124086240616747e26e0aa5ef3412a3be55d) that is more recent than the improvements that @jameswex are describing, it should be enough to just use bazel. (on master)\n", "Some progress; I did gulp vulcanize and then the bazel build with the latest master (e4add493f1020c0eb986aba21c266d9e6e6f4182). As before, I'm on Ubuntu 14.04 on Amazon EC2 with Anaconda Python 3.5.1 Now, when attempting to visualize the graph (after dumping it via the same Python snippet in my original issue), I get this TensorBoard error:\n\n```\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/socketserver.py\", line 628, in process_request_thread\n    self.finish_request(request, client_address)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/socketserver.py\", line 357, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/tensorboard/backend/handler.py\", line 93, in __init__\n    BaseHTTPServer.BaseHTTPRequestHandler.__init__(self, *args)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/socketserver.py\", line 684, in __init__\n    self.handle()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/http/server.py\", line 415, in handle\n    self.handle_one_request()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/http/server.py\", line 403, in handle_one_request\n    method()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/tensorboard/backend/handler.py\", line 454, in do_GET\n    data_handlers[clean_path](query_params)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/tensorboard/backend/handler.py\", line 259, in _serve_graph\n    large_attrs_key)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/tensorboard/backend/process_graph.py\", line 66, in prepare_graph_for_ui\n    node.attr[large_attrs_key].list.s.append(str(key))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/google/protobuf/internal/containers.py\", line 251, in append\n    self._values.append(self._type_checker.CheckValue(value))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/google/protobuf/internal/type_checkers.py\", line 108, in CheckValue\n    raise TypeError(message)\nTypeError: 'value' has type <class 'str'>, but expected one of: (<class 'bytes'>,)\n```\n\nThe TensorBoard app shows this \"Graph visualization failed\" error\n![image](https://cloud.githubusercontent.com/assets/1332812/13756027/2d7d0e4a-e9db-11e5-98b0-01db39439709.png)\n\nCould this be a Python 2 vs. 3 issue?\n", "Was able to reproduce the issue using python3. The problem comes down to `str` and `bytes` being equivalent types in python2, but not in python3. Moreover python3 bytes requires an encoding to be specified when converting a string to bytes (protobuf uses utf-8 for encoding strings).\n\nFix is on the way. The commit should appear tomorrow. If you don't want to wait, a small fix that makes it work just for python3 is to replace line 66 in `process_graph.py` from\n`node.attr[large_attrs_key].list.s.append(str(key))`\nto\n`node.attr[large_attrs_key].list.s.append(bytes(key, 'utf-8'))`\n", "And also replace line 58 in `process_graph.py` from\n`keys = node.attr.keys()`\nto\n`keys = list(node.attr.keys())`\n", "@dsmilkov, making those changes worked! Thanks!\n\nAfter the change has been pushed to master, I'll test again, and close this issue if it works.\n", "I attempted to test the current master (d868f1e8089b2cbee7c7c7f6b385dda9a13c1c7e) but now I can't even open a session; I get this error:\n\n```\n>>> tf.Session()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 727, in __init__\n    super(Session, self).__init__(target, graph, config=config)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 104, in __init__\n    opts = tf_session.TF_NewSessionOptions(target=target, config=config)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 266, in TF_NewSessionOptions\n    _TF_SetTarget(opts, target)\nTypeError: expected bytes, str found\n```\n\nWhich sounds like another Python 2 vs 3 issue, unrelated to tensorboard.\n\nI'll try separately with the commit that included the `process_graph.py` patch, 9c7be1c.\n", "That is a separate python 3 tensorflow (but not tensorboard) issue that I believe is a known issue\n", "I can't test 9c7be1cf1c4f8cead83e8137a9d0718b657995af either because of yet another non-tensorboard Python 2 vs 3 issue.\n\nOn 9c7be1cf1c4f8cead83e8137a9d0718b657995af:\n\n```\nTraceback (most recent call last):\n  File \"blah.py\", line 3, in <module>\n    import tensorflow as tf\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 82, in <module>\n    from tensorflow.python.training import training as train\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/python/training/training.py\", line 162, in <module>\n    from tensorflow.python.training.session_manager import SessionManager\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_py3_unstable/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py\", line 329\n    except errors.FailedPreconditionError, e:\n                                         ^\nSyntaxError: invalid syntax\n```\n\nMaybe some Python 3 unit tests would be helpful?\n\nI'll try this again in a few days when there will hopefully be a version that works on Python 3.\n", "So we have python 3 tests, but they are not fully integrated requiring us to run them manually. A change yesterday broke TensorFlow on python 3 and fixes are on the way.\n", "Understandable, @dsmilkov, thanks for the explanation!\n", "I can confirm the graph can now be visualized properly in f952246b1756242926d47091d09ceb7c7b963269. Thanks for working on this!\n", "i've got the similar issue in python2  \r\nwhen i ran ` writer = tf.train.SummaryWriter(INCEPTION_LOG_DIR, graph_def)`  \r\neach time i got a \"events.out.tfevents.1456423256.\"", "updating tensorflow works"]}, {"number": 1286, "title": "Make RNN cells add bias to the bias variable collection", "body": "Currently helper layers like fully_connected add their bias variables to the 'bias' collection, which is a useful convenience. The RNN cell code doesn't do that, and it would be nice if it did for consistency. Needless to say the same is true for the main weights and the 'weights' collection.\n", "comments": ["I am just confirming. The way they are added here right?\nhttps://github.com/tensorflow/tensorflow/blob/39b332ba07db732ff8e0a07459e74995b92aaa83/tensorflow/contrib/layers/python/layers/layers_test.py\n", "The way they're added [here](https://github.com/tensorflow/tensorflow/blob/39b332ba07db732ff8e0a07459e74995b92aaa83/tensorflow/contrib/layers/python/layers/layers.py#L111).\n", "Would work on it from tomorrow. :)\n", "This will happen once we stop using linear() in the rnn cells and use layers directly.  Lukasz is working on that.\n", "@lukaszkaiser: @ebrevdo says this will happen automatically as a consequence of other work you're already doing.  Is that right?\n", "Eugene is right that it'll happen when we replace linear with layers, which should happen (as part of the general move to layer/tf.learn, right?). I'm not working on it right now though.\n", "We need an ETA on the move of contrib.layers to core.\n", "Indeed. Should we mark this in some way to indicate it's blocked?\n", "@lukaszkaiser I don't know any way to do that, unfortunately.\n", "Since this is part of a general move of layers/slim into core and merging RNN with that, I'm closing it for now.\n"]}, {"number": 1285, "title": "install issue! --No module named core.framework.graph_pb2", "body": "I build with the source code.\nenvy@ub1404:~/os_pri/github/tensorflow$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\n\nenvy@ub1404:~/os_pri/github$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\nUnpacking /tmp/tensorflow_pkg/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\nRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /home/envy/.local/lib/python2.7/site-packages (from tensorflow==0.7.1)\nDownloading/unpacking protobuf==3.0.0b2 (from tensorflow==0.7.1)\n  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB): 326kB downloaded\nRequirement already satisfied (use --upgrade to upgrade): wheel in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1)\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /home/envy/.local/lib/python2.7/site-packages (from tensorflow==0.7.1)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /home/envy/.local/lib/python2.7/site-packages (from protobuf==3.0.0b2->tensorflow==0.7.1)\nInstalling collected packages: tensorflow, protobuf\nSuccessfully installed tensorflow protobuf\nCleaning up...\n## envy@ub1404:~/os_pri/github$ pip show protobuf\n\nName: protobuf\nVersion: 3.0.0b2\nLocation: /usr/local/lib/python2.7/dist-packages\nRequires: setuptools, six\n## envy@ub1404:~/os_pri/github$ pip show tensorflow\n\nName: tensorflow\nVersion: 0.7.1\nLocation: /usr/local/lib/python2.7/dist-packages\nRequires: six, protobuf, wheel, numpy\nenvy@ub1404:~/os_pri/github$ python tensorflow/tensorflow/models/image/mnist/convolutional.py\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/models/image/mnist/convolutional.py\", line 34, in <module>\n    import tensorflow as tf\n  File \"/home/envy/os_pri/github/tensorflow/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/envy/os_pri/github/tensorflow/tensorflow/python/**init**.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/home/envy/os_pri/github/tensorflow/tensorflow/python/**init**.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\nImportError: No module named core.framework.graph_pb2\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\nenvy@ub1404:~/os_pri/github$\n", "comments": ["http://stackoverflow.com/questions/33625824/importerror-no-module-named-core-framework-graph-pb2\nIs this of any help?\n", " I had tried, it doesn;t not works,\n", "The error message should be pretty helpful, I think.\n", "Closing.  The issue is that \"you should not try to import tensorflow from its source directory\", as the error message states.\n"]}, {"number": 1284, "title": "Clarify Udacity assignment #2", "body": "- expand on 'hiddent'.\n- Point to nn.relu:\n  https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu\n", "comments": []}, {"number": 1283, "title": "DEFINE_bool alias not working on Mac Python test-on-install", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System: Mac\n\nIf installed from binary pip package, provide: This error occurs in Mac Python (2 and 3) test-on-install\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Go to http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/9/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/console \n2. See failures in tests: \n   tensorflow/python/platform/default/flags_test.py\n   tensorflow/python/tools/graph_metrics_test.py\n3. Notice that both failures complain about \"DEFINE_bool\" not being an attribute. \n4. Interestingly, \"DEFINE_bool\" is found at: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_flags.py#L111\n\nBut somehow it is not recognized during Python test-on-install on Mac. \n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n(See above)\n", "comments": ["@vrv This is related to CL 115364038\n", "On closer examination, this seems to be a problem related to \"pip install\" on Mac. \n", "@caisq: Assigning to you; feel free to reassign.  What's the status currently? \n", "Unless there any updates, I will close this issue in 7 days.\n", "@aselle Only Googlers have commented on this thread so far, so awaiting response seems inapplicable.\n", "This issue has been fixed. The aforementioned two tests have been passing on Mac.\n"]}, {"number": 1282, "title": "Get an error when building the Op library", "body": "When I Build the Op library, I got the error :1234: \nIn file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:22:0,\n                 from zero_out.cc:1:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is\n #error This file was generated by a newer version of protoc which is\n  ^\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update\n #error incompatible with your Protocol Buffer headers.  Please update\n  ^\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:14:2: error: #error your headers.\n #error your headers.\n  ^\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:22:35: fatal error: google/protobuf/arena.h: No such file or directory\n #include <google/protobuf/arena.h>\n\nI install tensorflow by pip installation,branch r0.7.\n", "comments": ["Can you explain what you mean by \"when building the op library\"?  Are you trying to build new C++ code (perhaps user defined ops) that depend on TensorFlow?\n", "I originally had the same error as MisayaZ, but it went away when I removed an old Protobuf package that I had installed with yum (I installed TensorFlow and its Protobuf dependency with Pip).\n\nNow I'm getting a different error when I try to compile the ZeroOut Op from the \"Adding a New Op\" tutorial. This is my error:\n\n```\nIn file included from /home/jpaparia/anaconda2/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/op.h:22:0,\n                 from zero_out.cc:1:\n/home/jpaparia/anaconda2/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/\nframework/op_def.pb.h:9:42: fatal error: google/protobuf/stubs/common.h: No such file or directory\n #include <google/protobuf/stubs/common.h>\n                                          ^\ncompilation terminated.\n```\n\nAny idea what might cause this error? Thanks!\n", "To confirm: You can build tensorflow just fine without the `ZeroOut` op, and that error shows up when you add `ZeroOut`?  @keveman: Have you seen that kind of error before?\n", "Yes, TensorFlow worked fine before I tried adding the `ZeroOut` op.\n", "Can I see a branch with your change?\n", "The TensorFlow package I downloaded from Pip is [here](https://github.com/jackpaparian/tensorflow-package). The code for `ZeroOut` is [here](https://github.com/jackpaparian/tensorflow-package/blob/master/tensorflow/include/tensorflow/core/user_ops/zero_out.cc).\nThanks for your help!\n", "That's a strange setup.  If you're going to build from source, can you grab tensorflow via git normally rather than checking pip into git?\n\n@keveman: Are we close to replacing the `user_ops` directory with something that doesn't require modifying the tensorflow tree?\n", "I was under the impression that we don't need to build from source in order to add a new op. The tutorial says \n\n> Must have installed the TensorFlow binary, or must have downloaded TensorFlow source, and be able to build it.\n\nso I thought we could just add code to our TensorFlow package that we downloaded from Pip. Do we need to build from source in order to add a new op? \n", "@jackpaparian, you are right, you should be able to build a new op with Tensorflow binary pip installed.  Can you try installing the pip package from [here](http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl) and then try compiling your user op? The pip package for r0.7 had some issues related to user ops. See #1569 for some related discussion.\n", "@keveman, thanks for your response. I installed TensorFlow from the pip package you pointed me to. I'm now getting a new error when I compile `ZeroOut`:\n\n```\nIn file included from zero_out.cc:1:0:\n/home/jpaparia/anaconda2/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/op.h:26:62: fatal error: tensorflow/core/framework/selective_registration.h: \nNo such file or directory\n #include \"tensorflow/core/framework/selective_registration.h\"\n                                                              ^\ncompilation terminated.\n```\n\nI don't seem to have the `selective_registration.h` file that `zero_out.cc` requires. Any idea why that file didn't get installed?\n", "Looks like a bug that creeped in recently. Fix on the way. Sorry about that.\n", "1a25c92 adds selective_registration.h to the pip package. Please verify with the most recent pip package [here](http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl) and close the issue if successful.\n", "I reinstalled TensorFlow with the most recent pip package, and unfortunately I'm still getting the same error about `selective_registration.h`.\n", "I ran the following and things worked for me : \n\n```\n$ pip install -U http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastStableBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl\n$ TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\n$ g++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC -I $TF_INC\n```\n\nI ran this inside a virtualenv, on a Ubuntu 14.04. I can definitely see that the file is available\n\n```\n$ ls local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/selective_registration.h \nlocal/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/selective_registration.h\n```\n\nPerhaps you are still somehow installing an older whl file.\n", "Cool, compiling with those flags worked for me too. I had been including extra compile flags used for r0.7. Thanks for your help!\n"]}, {"number": 1281, "title": "ImportError: cannot import name tensorboard_server ", "body": "I've successfully built tensorflow \"version: 0.6.0 \" and tensor board from source on Mac OS X. The problem is I am getting this error when I try to run the tensorboard module. \n\ncommand: \n`\n./bazel-bin/tensorflow/tensorboard/tensorboard --help\n`\n\nthe produced error \n\n```\nAhmeds-MacBook-Pro:tensorflow ahmedabobakr$ ./bazel-bin/tensorflow/tensorboard/tensorboard --help\nTraceback (most recent call last):\n  File \"/Users/ahmedabobakr/tensorflow/bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/tensorflow/tensorboard/backend/tensorboard.py\", line 36, in <module>\n    from tensorflow.tensorboard.backend import tensorboard_server\nImportError: cannot import name tensorboard_server\n\n```\n", "comments": ["We would strongly suggest upgrading to 0.7.1 (sync to the 0.7.1 tag if needed, or HEAD if you are daring). tensorboard had a few bugs in 0.6.0 which should mostly be fixed now.\n", "Closing due to lack of activity.  Please reopen if it's not fixed in 0.7.1.\n"]}, {"number": 1280, "title": "Implementation of CW-RNN?", "body": "Has anyone considered implementing the CW-RNN architecture discussed here: http://arxiv.org/abs/1402.3511 \n\nThe cited results seem extremely promising, and the structure is quite simple. If it's not in the pipeline or already present, I may try implementing it myself.\n", "comments": ["@aidangomez I would like to help you out! :)\n", "@yash14123 Awesome! We'll see what Google has to say about any already present implementations.\n", "We'd love to have it in the models repo if anyone writes it: https://github.com/tensorflow/models!\n", "I'm working on it\n", "Just opened a pull request\n", "@aidangomez Can you link to the pull request?\n", "Here: https://github.com/tensorflow/models/pull/198\n", "I see. Let me know if u need any help.\n\n---\n\n\u53d1\u4ef6\u4eba: Aidan Gomez notifications@github.com\n\u53d1\u9001\u65f6\u95f4: 2016\u5e746\u670813\u65e5 23:45\n\u6536\u4ef6\u4eba: tensorflow/tensorflow\n\u6284\u9001: elvinpoon; Manual\n\u4e3b\u9898: Re: [tensorflow/tensorflow] Implementation of CW-RNN? (#1280)\n\nHere: tensorflow/models#198https://github.com/tensorflow/models/pull/198\n\n\u2015\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/issues/1280#issuecomment-225740698, or mute the threadhttps://github.com/notifications/unsubscribe/ASuxNtpeByyBt7-1Ba7aV2XMa2u33XO6ks5qLeuMgaJpZM4HiMj-.\n", "@girving The PR got updated recently and has a unit test that suggests everything is working as expected. Probably getting near what's acceptable for merger, do you mind having a look?", "@aidangomez On leave, so someone else will have to review.", "Closing.  The PR is still open.  I am not sure why it did not get resolved and the discussion should continue there.  "]}, {"number": 1279, "title": "Arch doesn't support it", "body": "After `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl` or as root, I got:\n`tensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.`\n\nIt seems it doesn't work with Python 3. What should I do?\n", "comments": ["Well that URL is for Python2.  You can use Python2 on Arch\n\n``` bash\nsudo pacman -S python2-pip\nsudo pip2 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n```\n\nFor Python3, there is only a pre-built for Python3.4, while arch is with Python3.5. You can either compile tensorflow yourself, or download the binary for 3.4 and rename \"cp34\" to \"cp35\" to make it work.\n", "- Some people said that renaming cp34 to cp35 doesn't work for them, but you are welcome to try it before installing.\n- If you want to install for python3, use the url the python3 wheel which is listed on our download page, and it should work.\n", "Like `pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp35-none-linux_x86_64.whl`? The output is:\n`HTTP error 404 while getting https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp35-none-linux_x86_64.whl\n  Could not install requirement tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp35-none-linux_x86_64.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp35-none-linux_x86_64.whl\nCould not install requirement tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp35-none-linux_x86_64.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp35-none-linux_x86_64.whl for URL https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp35-none-linux_x86_64.whl`\n", "If you want to give a try to 3.4 prebuilt, you need to download the 3.4 binary and then rename it to 3.5:\n\n``` bash\nwget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl -O tensorflow-0.7.1-cp35-none-linux_x86_64.whl \nsudo pip install --upgrade tensorflow-0.7.1-cp35-none-linux_x86_64.whl \n```\n\nThere is no binary for 3.5, so this is just a hack that may not work.\n", "OK. At least the python 2's one worked.\n", "Alternatively, install Python3.4 (e.g. from https://aur4.archlinux.org/packages/python34) then create the virtualenv for that version:\n\n``` bash\n$ virtualenv -p python3.4 --system-site-packages ~/tensorflow\n```\n"]}, {"number": 1278, "title": "In LRN docs eq, moved beta outside the parenthesis", "body": "In Krizhevsky 2012 paper, the beta in LRN equation is the exponent\nof everything inside the parenthesis and not just sqr_sum.\n", "comments": ["Can one of the admins verify this patch?\n", "Closing because it was fixed in ab48dbd4a.\n"]}, {"number": 1277, "title": "Cifar breaking", "body": "I am running cifar multiGPU example with different batch sizes and number of GPUs. The code is breaking this way:\n\ntensorflow.python.framework.errors.OutOfRangeError: RandomShuffleQueue '_1_tower_0/shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 128, current size 0)\n     [[Node: tower_0/shuffle_batch = QueueDequeueMany[component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tower_0/shuffle_batch/random_shuffle_queue, tower_0/shuffle_batch/n/_775)]]\n     [[Node: tower_1/shuffle_batch/n/_664 = _HostSend[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_170_tower_1/shuffle_batch/n\", _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/shuffle_batch/n)]]\nCaused by op u'tower_0/shuffle_batch', defined at:\n  File \"lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py\", line 224, in <module>\n    tf.app.run()\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py\", line 222, in main\n    train()\n  File \"lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py\", line 150, in train\n    loss = tower_loss(scope)\n  File \"lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_multi-gpu_train.py\", line 65, in tower_loss\n    images, labels = cifar10.distorted_inputs()\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10.py\", line 119, in distorted_inputs\n    batch_size=FLAGS.batch_size)\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_input.py\", line 153, in distorted_inputs\n    min_queue_examples, batch_size)\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_input.py\", line 104, in _generate_image_and_label_batch\n    min_after_dequeue=min_queue_examples)\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 496, in shuffle_batch\n    return queue.dequeue_many(batch_size, name=name)\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 287, in dequeue_many\n    self._queue_ref, n, self._dtypes, name=name)\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 319, in _queue_dequeue_many\n    timeout_ms=timeout_ms, name=name)\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/saoni.m/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\nThe values that are passed to shuffle_batch():\n\n```\n images, label_batch = tf.train.shuffle_batch(\n      [image, label],\n      batch_size=batch_size,\n      num_threads=num_preprocess_threads,\n      capacity=min_queue_examples + 3 * batch_size,\n      min_after_dequeue=min_queue_examples)\n```\n\nare:\nbatch size 128, num_threads 16, capacity 20384, min_after_deque 20000\n", "comments": ["@josh11b: Any ideas here? \n", "Does the CIFAR example set an epoch limit?  Looks like this might be\npoor handling of running to the end in the CIFAR code?  That is the\nnormal end-of-input exception.\n", "It has `FLAGS.max_steps`, so looks like yes.\n", "Maybe @shlens would know more about the Cifar example? Is there a standard main loop now that handles this error?\n", "If this is an issue due to poor handling of running to the end, then yes there is a better way to handle this. \nNamely, one would need to update [cifar10_multi_gpu_train.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py) to employ the [Supervisor](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py).\n\n@sherrym would be the expert for upgrading this.\n\nIn the mean time, I would regard this error as a \"non-error\" as the training session merely did not shut down properly.\n", "@saonim\nHey, I'm using Ubuntu 16.04, CUDA 7.5, CuDNN v4, Tensorflow 0.8.0 with a single NVIDIA Geforce 650M 2GB GPU to train. \n\nI am implementing the same network with cifar100 as input instead of the cifar10 that is specified. I had the exact same error at the exact same line. If you traced the flow back to the source of the error, I think it lies in the cifar10_input.py in line number 149. \n\nInside the distorted_inputs function in cifar10_input.py, I have changed this line to input cifar100 binary instead of cifar10.\n\n```\nfilenames = [os.path.join(data_dir, 'train.bin')] \n```\n\ninstead of\n\n```\nfilenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\nfor i in xrange(1, 6)]\n```\n\nThe error happened for me because there was some problem with the loop back mechanism of the string_input_producer function. It doesn't go back to the start of the first file in the list _**filenames**_, upon reaching EOF of the last file in the list, as @shlens has pointed out.\n\nSo, I changed the code to:\n\n```\nfilenames = [os.path.join(data_dir, 'train.bin') for _ in xrange(10000)]\n```\n\nIt works now! Try doing the same for your code. \n", "Looks like this was resolved?\r\nClosing the issue."]}, {"number": 1276, "title": "Deep Dream Tutorial", "body": "I wanted to make a tracking issue for the Deep Dream tutorial, as mentioned as 'Coming Soon' on this page: https://www.tensorflow.org/versions/r0.7/tutorials/index.html\n\nI'm very interested to see this.\n", "comments": ["Done! https://www.tensorflow.org/versions/r0.8/tutorials/index.html\n"]}, {"number": 1275, "title": "Adding permission x to test_tutorials.sh", "body": "", "comments": ["Can one of the admins verify this patch?\n", "This change needs to be made so that tutorial tests can work on Jenkins. I guess the file permission was somehow not set correctly when it is pushed from internal. \n"]}, {"number": 1274, "title": "'utf-8' codec can't decode byte (image_retraining)", "body": "On the Training on Flowers how to;\nhttps://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html \n\nUnable to get to get the .pb file and the .txt file to create in /tmp due to;\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 1: invalid start byte\n### Environment info\n\nOperating System:\nMac OSX El Captain 10.11.3 (15D21)\nPython 3.5.1\nTensorflow from source, commit [bfd5f0b](https://github.com/tensorflow/tensorflow/commit/bfd5f0bfaa13f397e80063ce59951033928b04b6)\nIf installed from binary pip package, provide:\n1. pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp35-none-any.whl\n2. The output from python3 -c 'import os; import inspect; import tensorflow; print(os.path.dirname(inspect.getfile(tensorflow)))'\n   /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow\n### Steps to reproduce\n1. brand new mac, fresh brew, python3, pip3 etc\n2. all tutorials so far are great, then on to;\n   https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html\n3. bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain\n   OK\n4. bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos\n   Error, see output below\n### Logs or other output that would be helpful\n\nTraceback (most recent call last):\n  File \"/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 828, in <module>\n    tf.app.run()\n  File \"/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 715, in main\n    graph = create_inception_graph()\n  File \"/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/examples/image_retraining/retrain.py\", line 301, in create_inception_graph\n    graph_def.ParseFromString(f.read())\n  File \"/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_gfile.py\", line 45, in sync\n    return fn(self, _args, *_kwargs)\n  File \"/Users/paulg/proj/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/tensorflow/python/platform/default/_gfile.py\", line 199, in read\n    return self._fp.read(n)\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/codecs.py\", line 321, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 1: invalid start byte\n", "comments": ["Thanks! Creating bottleneck with updated retrain.py\n[6a63c2d](https://github.com/dgolden1/tensorflow/commit/6a63c2df153b6717799315bcc9a304bef5b2296d)\n", "I'll make a PR shortly, once I figure out the contributing agreement. Feel free to cherry pick it you can't wait!\n\nhttps://github.com/dgolden1/tensorflow/commit/6a63c2df153b6717799315bcc9a304bef5b2296d\n"]}, {"number": 1273, "title": "docker run failed with the image", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\nDocker version: Docker version 1.10.2, build c3959b1\nsame result with or without proxy(I use tsocks for proxy)\n### What have you tried?\n1.  Failed when running tensorflow with docker using the following command\n\n```\nsudo docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow\n```\n\nthe output\n\n```\nabc@abc:~/source$ sudo docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow\nUnable to find image 'b.gcr.io/tensorflow/tensorflow:latest' locally\ndocker: Error response from daemon: unable to ping registry endpoint https://b.gcr.io/v0/\nv2 ping attempt failed with error: Get https://b.gcr.io/v2/: dial tcp 74.125.204.82:443: i/o timeout\n v1 ping attempt failed with error: Get https://b.gcr.io/v1/_ping: dial tcp 74.125.204.82:443: i/o timeout.\nSee 'docker run --help'.\n\n```\n", "comments": ["also tried `export ALL_PROXY=socks5://192.168.1.104:1080` but without luck, still failed when pulling image \n", "forget this,\nafter using vpn, all things working fine.\n", "I am hitting same issue can somebody guide what is the issue?\n", " @gangele397 @qdk0901Another easy solution. If you're under GFW, you can try the image of tensorflow in Docker Hub instead of Google's Cloud Platform. In your terminal, just run `sudo docker run -it -p 8888:8888 tensorflow/tensorflow` instead of running `sudo docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow`. It works for me. Hope it is helpful for you guys.", "@MingCHEN-Github Hi, I got following error :(, do you know how to fix? thx\r\n\r\nUnable to find image 'tensorflow/tensorflow:latest' locally\r\nlatest: Pulling from tensorflow/tensorflow\r\n297061f60c36: Downloading [===>                                               ]  3.079MB/43.03MB\r\ne9ccef17b516: Download complete \r\ndbc33716854d: Download complete \r\n8fe36b178d25: Download complete \r\n686596545a94: Download complete \r\ned66f2c5f3d9: Downloading [=>                                                 ]  2.698MB/129.6MB\r\n8405b6c3f141: Downloading [==================================================>]  4.244MB/4.244MB\r\n070615ca3a03: Waiting \r\n306ac2321f8e: Waiting \r\nc30111bc1e74: Waiting \r\n7aa552c3f7f7: Waiting \r\n4db41af3662a: Waiting \r\nbf5fbadacf01: Waiting \r\ndocker: read tcp 192.168.1.107:50572->104.18.124.25:443: read: connection reset by peer.\r\n", "@qdinfish sorry for the late reply.  I hope this blog of mine is helpful to you,[https://blog.csdn.net/Chenming_Hnu/article/details/70184543](url)"]}, {"number": 1272, "title": "Update 3_regularization.ipynb", "body": "Fixed a mistake in a variable name.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks!\n"]}, {"number": 1271, "title": "Implementing Striving for Simplicity: The All Convolutional Net in Tensorflow: results on the test set lower than expected", "body": "I posted [this issue with my code](http://stackoverflow.com/questions/35339636/erratic-training-for-all-cnn-on-cifar-10) on stackoverflow a while ago but it has not received any answer or comment.\nThe preprocessing I used is in the dataset class (that I adapted from the Deep MNIST for experts tutorial) and is exactly the same as in the neon/pylearn2 implementation: calculating the mean and Wzca matrix on training set and use it to whiten the data on the training and test set followed by a global contrast normalization step with Goodfellow scale factor of 55.  \nThe only difference of my implementation according to me (I would like to be mistaken) with the paper is the use of `tf.reduce_mean` instead of `tf.reduce_sum` (which led me to exploding ReLU grad).\nSo I divide the weight decay by a factor batch_size to keep the change.  \nWith this I got up to 85.something% on the test set instead of the 91% claimed by the authors.  \nDo you see something wrong immediately ? For instance I feel the weight decay part of the cross entropy is ugly but I could not find any better way of doing that. Do you have some ideas ?  \nIf you think that this is not an appropriate place for my question tell me I will remove it but I do not know where to look.\n", "comments": ["As the now-deleted-comment suggested, this type of detailed debugging is likely out of scope for GitHub -- you may get better help on StackOverflow or such. \n"]}]