[{"number": 21268, "title": "Failed to load the native tensorflow runtime", "body": "### System information\r\n\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6.2\r\n\r\n### Describe the problem\r\nFirst time using tensorflow, I have installed it using the command pip3 install --upgrade tensorflow on the terminal. When trying to import tensorflow as tf in python I get the error below\r\n\r\n### Source code / logs\r\n\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21267, "title": "tf.constant should support dynamic shape", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.5\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: pip binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\n\r\n### Source code / logs\r\n\r\nExample code:\r\n\r\n`tf.constant(3, shape=(3, 1, tf.constant(3) * 5))`\r\n\r\nWill throw an error (very unrelated / confusing: `ValueError: setting an array element with a sequence.`).\r\n\r\nCorresponding `tf.zeros` or `tf.ones` works, so it is a counter-intuitive that `tf.constant` does not.\r\n\r\n(Workaround for me, but this seems a bit ugly: `tf.tile(tf.constant(value, shape=[1] * len(shape)), shape)`)", "comments": ["Added a PR #21296 to support tensor type of shape (constant value only).", "Ok, well, my example was maybe badly chosen. I explicitly need this for non-constant (dynamic) shapes. I edited the question description.", "`tf.constant` is intended to produce values that are fully known at graph construction time. Perhaps I'm misunderstanding, but what does a \"constant with dynamic shape\" mean?\r\n\r\nPerhaps what you're looking for is [`tf.fill`](https://www.tensorflow.org/api_docs/python/tf/fill)?\r\n", "Thanks, `tf.fill` is actually what I wanted. But from the users perspective, why make this a difference? E.g. check inside `tf.constant` if the shape is constant, then make it a `Const`, otherwise use a `Fill`, i.e. basically merge `tf.constant` and `tf.fill`. I would have expected that `tf.constant` supports the same kind of shapes as `tf.zeros` or `tf.ones`. But maybe just because I did somehow miss `tf.fill`. If you don't like this suggestion, then maybe at least `tf.fill` should be mentioned in the documentation of `tf.constant`. Also, currently it is not totally clear to me from the documentation that `shape` must be constant.", "I don't think we want to make `tf.constant` produce a tensor that isn't a constant (i.e., one whose value might be determined only at graph execution time).\r\n\r\nHowever, updating the documentation for `tf.constant` to make it more understandable seems perfectly reasonable. I'd encourage you to create a pull request updating https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/python/framework/constant_op.py#L117\r\n\r\nThanks!"]}, {"number": 21266, "title": "[tflite][android]deeplab-v3+ runtime error on Pad Ops", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: BlackBerry KEY2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master branch last commit is 78f58629ae4a47a799e88da7a78ecfcb6a6115cc\r\n- **Python version**:3.6.3\r\n- **Bazel version (if compiling from source)**:0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: NDK r17 toolchain\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI download the pre-trained modals with MobileNet-v2 from [mobilenetv2_coco_voc_trainaug](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz). covered model to tflite then load into android application, I see internal error at OP Pad on prepare.\r\n\r\n### Source code / logs\r\nusing below command to cover the model without any errors:\r\n```\r\nbazel run //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=/tmp/frozen_inference_graph.pb \\\r\n  --output_file=/tmp/optimized_graph.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_type=QUANTIZED_UINT8 \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=SemanticPredictions \\\r\n  --input_shapes=1,513,513,3\r\n```\r\nbelow command to build tensorflow-lite.aar\r\n```\r\nbazel build --cxxopt='--std=c++11' -c opt        \\\r\n  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   \\\r\n  //tensorflow/contrib/lite/java:tensorflow-lite\r\n```\r\nThen I load optimized_graph.tflite and tensorflow-lite.aar into Android application project\r\n```\r\n  private static final int DIM_PIXEL_SIZE = 3;\r\n  static final int DIM_IMG_SIZE_X = 513;\r\n  static final int DIM_IMG_SIZE_Y = 513;\r\n\r\n    tflite = new Interpreter(loadModelFile(activity));\r\n    imgData =\r\n        ByteBuffer.allocateDirect(\r\n            DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE);\r\n    imgData.order(ByteOrder.nativeOrder());\r\n    outputs = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];\r\n```\r\n```\r\n  /** Memory-map the model file in Assets. */\r\n  private MappedByteBuffer loadModelFile(Activity activity) throws IOException {\r\n    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_PATH);\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n  }\r\n```\r\nrun Interpreter\r\n```\r\ntflite.run(imgData, outputs);\r\n```\r\nError Logs:\r\n```\r\n07-31 16:20:36.144 25819-25974/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 25819\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/contrib/lite/kernels/pad.cc:96 op_context.dims != 4 (3 != 4)Node number 24 (PAD) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:130)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:168)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:145)\r\n```\r\n", "comments": ["We will investigate. Meanwhile, could you make sure your input tensors are 4D? Are you calling resizeInput()?", "@andrehentz I use pre-trained model from model_zoo https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md, I think it's 3-D tensor with shape [height, width, channels] for PAD.\r\nI noticed from Pad.cc there's a TODO   \r\n``` \r\n  //TODO(nupurgarg): Our current implementations rely on the inputs being 4D.\r\n  TF_LITE_ENSURE_EQ(context, op_context.dims, 4);\r\n```\r\nseems Pad not support 3D tensor right now.", "Thanks @kismeter We are tracking this and will support 3D (and other) soon.", "Nagging Assignee @gargn: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hello! I see there are still some ops (SPACE_TO_BATCH_ND, SPACE_TO_BATCH_ND) which support only 4D tensors:\r\n\r\ntensorflow/contrib/lite/kernels/space_to_batch_nd.cc:96 NumDimensions(op_context.input) != kInputDimensionNum (3 != 4)\r\n\r\n3D support for this ops is highly required! Are you planning to implement it in near future? Should i create feature request?\r\n", "Just for a clarification, do you need BatchToSpace non-4D *and* SpaceToBatch non-4D, or just BatchToSpace non-4D?\r\n\r\nBatchToSpace non-4D hasn't been prioritized yet, contributions are welcome, but we are also tracking these operations requests here: https://github.com/tensorflow/tensorflow/issues/21526 to help with prioritizing. For that reason, moving forward we are closing individual issues for operation requests. Feel free to file an issue if you have more info on the specific model you are trying to convert. Thanks!", "In model im working on both BatchToSpace and SpaceToBatch are required, you can find some information in this issue #22146 about TOCO converter"]}, {"number": 21265, "title": "//tensorflow/python/kernel_tests:conv_ops_test fails on AVX512 builds", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: commit ecd8decac3d9f3c7cd772e1561b9c2d3f23aa830 on the master branch\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: [bazel release 0.15.0]\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**:\r\n\r\nbazel test --config=opt --cache_test_results=no  -- //tensorflow/python/kernel_tests:conv_ops_test\r\n\r\n### Describe the problem\r\n\r\nThe //tensorflow/python/kernel_tests:conv_ops_test test case consistently fails on AVX512 builds of tensorflow\r\n\r\n### Source code / logs\r\n```\r\n======================================================================\r\nFAIL: testConv2D3x3FilterStride1x1Same (__main__.DeepConv2DTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 1796, in testConv2D3x3FilterStride1x1Same\r\n    self._RunTestCases([1, 1], \"SAME\")\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 1790, in _RunTestCases\r\n    self._CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 1782, in _CompareFwdConv2D\r\n    self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1381, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1331, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg))\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1286, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py\", line 1396, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py\", line 779, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-05, atol=1e-05\r\nMismatched value: a is different from b. \r\n(mismatch 96.60625%)\r\n x: array([[[[[1257.2019, 1264.4261, 1258.311 , ..., 1258.0616, 1228.0815,\r\n           1245.9379],\r\n          [1875.4814, 1885.9357, 1868.8992, ..., 1865.7366, 1842.2179,...\r\n y: array([[[[[1256.2858, 1263.7555, 1257.8694, ..., 1256.4175, 1226.5343,\r\n           1244.2854],\r\n          [1874.3364, 1884.7977, 1868.6038, ..., 1864.8274, 1841.5417,...\r\n\r\n----------------------------------------------------------------------\r\nRan 73 tests in 5.034s\r\n\r\nFAILED (failures=1)\r\nnot close where =  (array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 2, 2, 2]), array([  0,   1,   2, ..., 125, 126, 127]))\r\nnot close lhs =  [1257.2019 1264.4261 1258.311  ... 1840.9    1845.5197 1843.9106]\r\nnot close rhs =  [1256.2858 1263.7555 1257.8694 ... 1839.0669 1843.0771 1841.3849]\r\nnot close dif =  [0.9161377 0.6706543 0.4416504 ... 1.8331299 2.442505  2.5257568]\r\nnot close tol =  [0.01257286 0.01264755 0.01258869 ... 0.01840067 0.01844077 0.01842385]\r\ndtype = float32, shape = (1, 5, 5, 5, 128)\r\n```\r\n\r\n\r\n```\r\nFAIL: testConv2D3x3FilterStride1x1Valid (__main__.DeepConv2DTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 1793, in testConv2D3x3FilterStride1x1Valid\r\n    self._RunTestCases([1, 1], \"VALID\")\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 1790, in _RunTestCases\r\n    self._CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 1782, in _CompareFwdConv2D\r\n    self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1381, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1331, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg))\r\n  File \"/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1286, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py\", line 1396, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py\", line 779, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-05, atol=1e-05\r\nMismatched value: a is different from b. \r\n(mismatch 94.3576388889%)\r\n x: array([[[[[2802.1404, 2796.723 , 2792.0303, ..., 2813.227 , 2780.2148,\r\n           2796.4958],\r\n          [2797.6106, 2792.4019, 2813.2893, ..., 2790.9302, 2788.8625,...\r\n y: array([[[[[2802.357 , 2796.4634, 2792.4202, ..., 2812.9595, 2780.5022,\r\n           2796.4705],\r\n          [2797.8032, 2792.7297, 2813.6294, ..., 2791.9949, 2789.6606,...\r\n\r\n----------------------------------------------------------------------\r\nRan 73 tests in 7.059s\r\n\r\nFAILED (failures=1)\r\nnot close where =  (array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 2, 2, 2]), array([0, 0, 0, ..., 1, 1, 1]), array([  0,   1,   2, ..., 125, 126, 127]))\r\nnot close lhs =  [2802.1404 2796.723  2792.0303 ... 2763.8906 2751.459  2762.4377]\r\nnot close rhs =  [2802.357  2796.4634 2792.4202 ... 2764.6665 2753.163  2764.2102]\r\nnot close dif =  [0.21655273 0.25952148 0.38989258 ... 0.7758789  1.7041016  1.7724609 ]\r\nnot close tol =  [0.02803357 0.02797463 0.0279342  ... 0.02765667 0.02754163 0.0276521 ]\r\ndtype = float32, shape = (1, 5, 3, 3, 128)\r\n```", "comments": ["The basic problem seems to be that the gemm_pack_rhs<...>::operator()\r\n(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions.h#L657)\r\ndoesn't always produce the correct output if the depth is not a multiple of the\r\npacket_size.  Looking at the code there seem to be a number of issues, each of which are caught by\r\nassertions.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions.h#L718\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions.h#L371\r\n\r\nI've created a [patch](https://bitbucket.org/eigen/eigen/pull-requests/443/fix-tensor-contraction-for-avx512-machines/diff) for Eigen that fixes the issue by modifying TensorContractionBlocking to ensure the depth passed to gemm_pack_rhs<...>::operator() is always a multiple of the packet_size for packet_sizes > 8.  I appreciate Eigen might not be the best place to fix this issue as the issue is not reproducible with the Eigen Tensor class.  However, the multiple assertions in with gemm_pack_rhs<...>::operator() in Tensorflow led me to believe that this code was not intended to work when depth is not a multiple of the packet_size and so I opted to make the change in Eigen.  The change is localised to tensor contraction and is the simplest fix that I could come up with. If a fix in Tensorflow or elsewhere would be preferable, please let me know and I'll rework my patch.\r\n", "Nagging Assignee @rmlarsen: It has been 120 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The Eigen patch has been updated to extend the fix to cover the new parallel contraction code.\r\n\r\nhttps://bitbucket.org/eigen/eigen/pull-requests/554/fix-tensor-contraction-on-avx512-builds/diff", "The patch has now been merged into Eigen and commit #be4b5c3 pulls this patch into TensorFlow.  CNNs now work on AVX512 builds.  This issue can be closed."]}, {"number": 21264, "title": "Tensorflow Lite Speech Recognition Android App not able to recognize", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: from source : https://github.com/tensorflow/tensorflow\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**: 0.15.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc 4.8.4\r\n- **Exact command to reproduce**: bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   //tensorflow/contrib/lite/examples/android:tflite_demo\r\n\r\n### the problem\r\nThe TFLite Speech Recognition App from the path https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android is not able to Recognize spoken words at all. From my debug, I have got that the below \"if\" condition is failing:\r\n\r\nif ((currentTopScore > detectionThreshold) && (timeSinceLastTop > suppressionMs)) {\r\n      previousTopLabel = currentTopLabel;\r\n      previousTopLabelTime = currentTimeMS;\r\n      previousTopLabelScore = currentTopScore;\r\n      isNewCommand = true;\r\n    } else {\r\n      isNewCommand = false;\r\n    }\r\n\r\nI'm getting currentTopScore < detectionThreshold always and I'm not able to recognize the spoken words.\r\n\r\n### My own added debug logs:\r\n07-27 10:34:15.106  7240  7262 D My_Debug: >>>>>>>>>>arraycopy of recordingBuffer\r\n07-27 10:34:15.106  7240  7262 D My_Debug: >>>>>>>>>>recordingBuffer.unlock\r\n07-27 10:34:15.135  7240  7262 D My_Debug: >>>>>>>>>>inside processLatestResults\r\n07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>currentTopLabel:_silence_\r\n**07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>currentTopScore:1.3075984\r\n07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>detectionThreshold:0.7**\r\n07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>timeSinceLastTop:9223372036854775807\r\n07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>suppressionMs:1500\r\n07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>>result:org.tensorflow.demo.RecognizeCommands$RecognitionResult@32219e3\r\n07-27 10:34:15.136  7240  7240 D My_Debug: >>>>>>>>>result.isNewCommand:true\r\n07-27 10:34:15.136  7240  7240 D My_Debug: >>>>>>>>>result.foundCommand.startsWith:true", "comments": ["@petewarden, could you PTAL.", "I've just tried to reproduce this (using `git checkout v1.10.0` since building this target on head is currently broken) and the app seemed to be working successfully. Closing this for now, but please reopen if you're still seeing problems and are able to reproduce it on multiple devices. If you email me at petewarden@google.com I can send you on the APK I built here too, to see if it's something specific to your build environment or device.\r\n\r\n"]}, {"number": 21263, "title": "C:/users/******/_bazel_quickride/obpmm2rb/external/double_conversion/BUILD.bazel:7:1: C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit -1)", "body": "I stuck with this problem i had installed cmake also but iam not able to build the the tensarflow demo application.\r\n\r\nC:/users/*****/_bazel_quickride/obpmm2rb/external/double_conversion/BUILD.bazel:7:1: C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit -1)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution -  Windows\r\nTensorFlow installed from -  https://github.com/tensorflow/\r\nTensorFlow version - I dint know exactly which version i took from the git hub latest pull.\r\nBazel version - I think  it latest \r\nCUDA/cuDNN version - NA\r\nGPU model and memory - NA\r\nExact command to reproduce - Iam building the APK in which readme file sepecified in tensarflow/android.\r\nMobile device - One Plus 3, Samsung J7, Moto g", "You may be compiling with GCC 5 or later. I faced the same issue due to this and got it fixed by passing `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"` as an argument to the `bazel build` command. It is required for GCC 5 or later as mentioned under [GPU Support](https://www.tensorflow.org/install/install_sources#gpu_support). The other alternative is to build with GCC 4.8.", "Nagging Assignee @tatianashp: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21262, "title": "Toco gives core dump on using the Speech model for conversion to tflite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**:2.7.12\r\n- **Bazel version (if compiling from source)**: 0.15.1\r\n- **GCC/Compiler version (if compiling from source)**:c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\ni am trying to convert the  wavenet tensorflow model to tflite using toco with the below command.\r\nthe link for the pre-trained  model is here https://github.com/buriburisuri/speech-to-text-wavenet \r\n\r\n\r\nmodel info is here below\r\nTensor(\"import/wav_data:0\", dtype=string)\r\nTensor(\"import/output:0\", shape=(?, ?), dtype=int64)\r\n\r\nused the below command to convert to toco\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco --input_file=q_wavenet_mobile.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=q_wavenet_mobile.tflite --inference_type=QUANTIZED_UINT8 --input_arrays=wav_data --output_arrays=output  --allow_custom_ops\r\n\r\n### Source code / logs\r\n\r\n2018-07-31 11:25:22.075935: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: DecodeWav\r\n2018-07-31 11:25:22.076164: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: AudioSpectrogram\r\n2018-07-31 11:25:22.076206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Mfcc\r\n2018-07-31 11:25:22.076270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.076371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.076416: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.076559: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.076655: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.076698: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.076827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.076925: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.076970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.077080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.077176: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.077221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.077391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.077627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.077674: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.077838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.078074: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.078123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.078236: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.078340: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.078386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.078566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.078813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.078862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.079054: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.079290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.079339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.079447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.079552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.079598: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.079791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.080031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.080081: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.080238: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.080475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.080522: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.080628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.080733: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.080778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.080978: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.081225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.081272: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.081434: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.081702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.081751: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.081854: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.081959: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.082004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.082195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.082299: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.082342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.082514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.082621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.082667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.082777: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.082881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.082926: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.083127: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.083374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.083421: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.083584: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.083835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.083882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.083992: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.084100: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.084152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.084352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.084607: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.084654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.084813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.085060: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.085108: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.085216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.085330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.085374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.085568: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.085818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.085866: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.086028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.086281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.086329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.086447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.086562: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.086606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.086802: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.087109: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.087157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.087352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.087615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.087662: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.087774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.087894: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.087939: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.088103: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.088219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.088264: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.088453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.088570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.088664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.088773: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.088891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.088935: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.089102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.089358: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.089576: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.089782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.090062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.090112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.090224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.090346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.090391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.090571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.090836: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.090884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.091077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.091996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.092046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.092158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.092943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.092993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.093199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.093484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.093534: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.093699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.093976: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.094024: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.094136: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.094267: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.094310: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.094512: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.094795: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.094844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.095003: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.095276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.095325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.095435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.095564: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.095609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.095719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.095845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference\r\n2018-07-31 11:25:22.095889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal\r\n2018-07-31 11:25:22.095987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize\r\n2018-07-31 11:25:22.096080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: CTCBeamSearchDecoder\r\n2018-07-31 11:25:22.244485: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2797 operators, 4442 arrays (0 quantized)\r\n2018-07-31 11:25:22.379827: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2797 operators, 4442 arrays (0 quantized)\r\n2018-07-31 11:25:22.400884: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:161] Identified sub-network emulating dilated convolution.\r\n2018-07-31 11:25:22.401095: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:209] Replaced with Dilated Conv2D op outputting \"block_0_2/conv_filter/atrous_conv2d\".\r\nterminate called after throwing an instance of 'std::out_of_range'\r\n  what():  _Map_base::at\r\nAborted (core dumped)\r\n\r\n", "comments": ["It looks like that model users a number of ops that are not currently supported by TF Lite, including CTCBeamSearchDecoder and DecodeWav, in addition to Reciprocal and SquaredDifference. The crash seems unrelated though. ", "Is it mandatory that all operations have to be converted for successfully building  the Tflite model?? or  is  there a fallback for these operations which will handle them gracefully and still continue the conversion ?? ", "In most cases, that is mandatory. However, sometimes the ops are not in the inference path and will be discarded early in the conversion process.", "I believe you can write custom ops for the functions.  You seem to have only a handful of unsupported functions, so it's not entirely out of the question.  Look at your graph on something like tensorboard to pin down what exactly is your inference \"path\".  Toco tries its best to remove what it thinks aren't used in inference, so working with the assumption that only your inference will remain is fair.  ", "ok Thanks  @gragundier @andrehentz i will try out... but is there any pointers to how i can debug the crash or what is causing it. this will be very helpful. Also any reference to how we can write custom ops for the operations. ", "Unfortunately there is no easy way to debug this type of crash other than using a debugger.\r\n\r\nHere's what I suggest:\r\n  - try converting your floating-point model first. It looks like the model you have has been quantized using transform_graph, which is not supported by TOCO.\r\n  - try specifying input_arrays and output_arrays so that you avoid DecodeWav at the start of your graph, and avoid CTCBeamSearchDecoder at the end of it.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21261, "title": "Tensorflow 1.9 for CPU, without GPU still requires cudNN - Windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: pip 18.0 - pip install tensorflow\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: import tensorflow after pip install tensorflow\r\n\r\n### Describe the problem\r\nI am working on a Win10 machine, with python 3.6.3 and using tensorflow 1.9, pip 18.0. I did not provide an option to install tensorflow with gpu, (i.e.), according to this\r\n\r\nhttps://www.tensorflow.org/install/install_windows, \r\n\r\nI used\r\n\r\n        pip install tensorflow\r\n\r\nand did not provide option for using GPU. However, when trying to import tensorflow, I am faced with the following error\r\n\r\n        ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nAfter following various links,\r\n \r\nhttps://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso,\r\nhttps://github.com/tensorflow/tensorflow/issues/8385,\r\n \r\nI installed the **Visual studio update 3** and also used the script provided for **tensorflow self check**,  \r\n\r\nhttps://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c, \r\n\r\nand came across this following error:\r\n\r\n        Could not load 'cudart64_80.dll'. .....\r\n        Could not load 'nvcuda.dll' .......\r\n        Could not load 'cudnn64_5.dll' ........\r\n\r\nWhy is my Tensorflow looking for these packages, when I installed it without GPU? MY system doesn't house a GPU at the moment. I tried uninstall and reinstalling with the **upgraded pip 18.0**, but the issue persists. How can this be rectified.?\r\n\r\n### Source code / logs\r\n\r\n--------------------------------------------------------------------------------------\r\nErrors in Detail. in Jupyter notebook while trying import - \r\n\r\nImportError                               Traceback (most recent call last)\r\nc:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     13         try:\r\n---> 14             return importlib.import_module(mname)\r\n     15         except ImportError:\r\n\r\nc:\\python36\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nc:\\python36\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nc:\\python36\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nc:\\python36\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nc:\\python36\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nc:\\python36\\lib\\importlib\\_bootstrap.py in module_from_spec(spec)\r\n\r\nc:\\python36\\lib\\importlib\\_bootstrap_external.py in create_module(self, spec)\r\n\r\nc:\\python36\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nc:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nc:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     16             return importlib.import_module('_pywrap_tensorflow_internal')\r\n---> 17     _pywrap_tensorflow_internal = swig_import_helper()\r\n     18     del swig_import_helper\r\n\r\nc:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     15         except ImportError:\r\n---> 16             return importlib.import_module('_pywrap_tensorflow_internal')\r\n     17     _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nc:\\python36\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-28ca7626b6fd> in <module>()\r\n      1 get_ipython().run_line_magic('pylab', 'inline')\r\n----> 2 from keras import regularizers, activations\r\n      3 from keras.models import Sequential, load_model\r\n      4 from keras.utils import np_utils,plot_model,to_categorical\r\n      5 import pandas\r\n\r\nc:\\python36\\lib\\site-packages\\keras\\__init__.py in <module>()\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\nc:\\python36\\lib\\site-packages\\keras\\utils\\__init__.py in <module>()\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 \r\n      8 # Globally-importable utils.\r\n\r\nc:\\python36\\lib\\site-packages\\keras\\utils\\conv_utils.py in <module>()\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\nc:\\python36\\lib\\site-packages\\keras\\backend\\__init__.py in <module>()\r\n     87 elif _BACKEND == 'tensorflow':\r\n     88     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 89     from .tensorflow_backend import *\r\n     90 else:\r\n     91     # Try and load external backend.\r\n\r\nc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in <module>()\r\n      3 from __future__ import print_function\r\n      4 \r\n----> 5 import tensorflow as tf\r\n      6 from tensorflow.python.framework import ops as tf_ops\r\n      7 from tensorflow.python.training import moving_averages\r\n\r\nc:\\python36\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     20 \r\n     21 # pylint: disable=g-bad-import-order\r\n---> 22 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     23 from . import app\r\n     24 from . import bitwise\r\n\r\nc:\\python36\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nc:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n--------------------------------------------------------------------------------------\r\nTensorflow Self Check output\r\n\r\nERROR: Failed to import the TensorFlow module.\r\n\r\nWARNING! This script is no longer maintained! \r\n=============================================\r\nSince TensorFlow 1.4, the self-check has been integrated with TensorFlow itself,\r\nand any missing DLLs will be reported when you execute the `import tensorflow`\r\nstatement. The error messages printed below refer to TensorFlow 1.3 and earlier,\r\nand are inaccurate for later versions of TensorFlow.\r\n\r\n- Python version is 3.6.\r\n\r\n- TensorFlow is installed at: C:\\Python36\\lib\\site-packages\\tensorflow\r\n\r\n- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Download and install CUDA 8.0 from\r\n  this URL: https://developer.nvidia.com/cuda-toolkit\r\n\r\n- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that\r\n  this DLL be installed in a directory that is named in your %PATH%\r\n  environment variable. Typically it is installed in 'C:\\Windows\\System32'.\r\n  If it is not present, ensure that you have a CUDA-capable GPU with the\r\n  correct driver installed.\r\n\r\n- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Note that installing cuDNN is a\r\n  separate step from installing CUDA, and it is often found in a\r\n  different directory from the CUDA DLLs. You may install the\r\n  necessary DLL by downloading cuDNN 5.1 from this URL:\r\n  https://developer.nvidia.com/cudnn\r\n\r\n- Could not find cuDNN.\r\n\r\n--------------------------------------------------------------------------------------", "comments": ["@gunan -- pip installation problems on Windows CPU; can you advise?", "I just confirmed that in our CI we can load the pip package without cudnn.\r\nIs it possible you are running into https://github.com/tensorflow/tensorflow/issues/19584", "I have the same problem. Windows 10 64-bit, Python 3.6.6, Pip 18.0. Attempting to import it using \"import tensorflow\" in the interactive console results in all of the same errors as in the OP. Testing with Tensor-Self-Check gives the same log as the OP. TensorFlow 1.8 works fine, but both 1.9 and 1.10 give the error. I used coreinfo to check if my CPU supports AVX, and it does.", "What is your CPU model?\r\nI confirmed that it is not cudnn we are missing. But rather having a CPU without AVX can cause similar error messages.", "Thank you for your help and information. I installed 1.5 and it worked. As stated in the issue above, I think from version 1.6 and above, it requires AVX instruction sets. I did not check for them though.\r\n\r\nI had also posted the question in stackoverflow, and here's the [link](https://stackoverflow.com/questions/51605105/tensorflow-1-9-for-cpu-without-gpu-still-requires-cudnn-windows/51606222?noredirect=1#comment90201596_51606222) to it.\r\n\r\nAccording to the only answer there, am missing **VCOMP140.dll**, which I could not find even after installing the redistributable.", "Nagging Assignee @gunan: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as duplicate of #19584"]}, {"number": 21260, "title": "Using Dataset api with Estimator in MirroredStrategy, Dst tensor is not initialized", "body": "### **System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):1.9.0\r\n- Python version: python 2.7\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source):4.8.5\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory:GeForce GTX 1080Ti * 4\r\n- Exact command to reproduce:\r\n### **Describe the problem**\r\n In order to solve the problem here:https://github.com/tensorflow/tensorflow/issues/19588, I git clone the master branch of tensorflow and compile it with bazel 0.15.0, then the  \"Non-DMA-safe string tensor error\" disappeared ,  the training went well , but when the evaluation part start, it ran into this: \r\n **'Dst tensor is not initialized'**\r\nit should be a \u201cNot enough memory\u201d problem , what i have tried:\r\n\r\n1. tune the batchsize to a very small one\r\n2. set config.gpu_options.per_process_gpu_memory_fraction to a smaller fraction like 0.6\r\n3. set config.gpu_options.allow_growth to True\r\n4. use a small dataset\r\n\r\nBut none of them can help .\r\n\r\n### **Source code / logs**\r\n![image](https://user-images.githubusercontent.com/31854634/43434228-5b6e6356-94ad-11e8-9e8d-e00a2c64bb1a.png)\r\n\r\n![image](https://user-images.githubusercontent.com/31854634/43434328-c4f53642-94ad-11e8-8113-554a1c03c5a9.png)\r\n\r\n\r\nLOG\uff1a\r\n(Jet_env_3) [lijiaji@m7-model-gpu05 classification]$ python demo.py \r\n2018-07-30 11:49:33,994:INFO:tensorflow:Begin configure dataset.\r\n2018-07-30 11:49:34,210:INFO:tensorflow:End configure dataset.\r\n2018-07-30 11:49:34,210:INFO:tensorflow:Mission hyper parameters\r\n2018-07-30 11:49:34,210:INFO:tensorflow:{'adagrad_initial_accumulator_value': 0.1, 'label_list': ['1', '0', '3', '2', '5', '4', '7', '6', '9', '8'], 'learning_rate_decay_factor': 0.94, 'learning_rate_decay_type': 'exponential', 'save_steps': 1000, 'ftrl_initial_accumulator_value': 0.1, 'ftrl_learning_rate_power': -0.5, 'num_train_samples': 48000, 'rmsprop_decay': 0.9, 'weight_decay': 4e-05, 'end_learning_rate': 0.0001, 'num_epochs': 2, 'trainable_scopes': None, 'checkpoint_path': None, 'finetune_flag': 'False', 'rmsprop_momentum': 0.9, 'model_dir': '/home/lijiaji/test_models/multi_gou_test', 'replicas_to_aggregate': 1, 'learning_rate': 0.01, 'momentum': 0.9, 'opt_epsilon': 1.0, 'optimizer': 'momentum', 'num_epochs_per_decay': 2.0, 'label_smoothing': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'batch_size': 32, 'num_classes': 10, 'ftrl_l1': 0.0, 'ftrl_l2': 0.0, 'train_image_size': 224, 'train_dir': '/home/lijiaji/project/augments/mnist', 'moving_average_decay': None, 'sync_replicas': False, 'checkpoint_exclude_scopes': None, 'ignore_missing_vars': False, 'model_name': 'inception_resnet_v2', 'adadelta_rho': 0.95}\r\n2018-07-30 11:49:34,211:INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0dcd846ed0>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': 912, '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f0dcd846b90>, '_session_config': gpu_options {\r\n  allow_growth: true\r\n}\r\nallow_soft_placement: true\r\n, '_global_id_in_cluster': 0, '_is_chief': True, '_protocol': None, '_save_checkpoints_steps': 1000, '_save_summary_steps': 1000, '_model_dir': '/home/lijiaji/test_models/multi_gou_test', '_master': ''}\r\n2018-07-30 11:49:34,212:INFO:tensorflow:Starting epoch 1 / 2\r\n2018-07-30 11:49:42,557:INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\n2018-07-30 11:49:42,559:INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\r\n2018-07-30 11:49:42,559:INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\r\n2018-07-30 11:49:42,559:INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:0\r\n2018-07-30 11:49:42,559:INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:1\r\n2018-07-30 11:49:42,559:INFO:tensorflow:Configured nccl all-reduce.\r\n2018-07-30 11:49:42,655:INFO:tensorflow:Calling model_fn.\r\n2018-07-30 11:50:09,603:INFO:tensorflow:Calling model_fn.\r\n2018-07-30 11:50:22,764:INFO:tensorflow:batch_all_reduce invoked for batches size = 496 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2018-07-30 11:50:34,055:INFO:tensorflow:Done calling model_fn.\r\n2018-07-30 11:50:34,067:INFO:tensorflow:Done calling model_fn.\r\n2018-07-30 11:50:39,149:INFO:tensorflow:Create CheckpointSaverHook.\r\n2018-07-30 11:51:11,985:INFO:tensorflow:Graph was finalized.\r\n2018-07-30 11:51:37,259:INFO:tensorflow:Running local_init_op.\r\n2018-07-30 11:51:38,022:INFO:tensorflow:Done running local_init_op.\r\n2018-07-30 11:52:26,489:INFO:tensorflow:Saving checkpoints for 0 into /home/lijiaji/test_models/multi_gou_test/model.ckpt.\r\n18/07/30 11:53:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2018-07-30 11:53:21,956:INFO:tensorflow:loss = 11.345272, step = 0\r\n2018-07-30 11:55:20,779:INFO:tensorflow:global_step/sec: 0.841569\r\n2018-07-30 11:55:20,780:INFO:tensorflow:loss = 1.7503239, step = 100 (118.825 sec)\r\n2018-07-30 11:56:24,385:INFO:tensorflow:global_step/sec: 1.5722\r\n2018-07-30 11:56:24,385:INFO:tensorflow:loss = 1.5974646, step = 200 (63.605 sec)\r\n2018-07-30 11:57:28,862:INFO:tensorflow:global_step/sec: 1.55093\r\n2018-07-30 11:57:28,863:INFO:tensorflow:loss = 1.3371584, step = 300 (64.478 sec)\r\n2018-07-30 11:58:18,151:INFO:tensorflow:Saving checkpoints for 375 into /home/lijiaji/test_models/multi_gou_test/model.ckpt.\r\n2018-07-30 11:58:28,894:INFO:tensorflow:Loss for final step: 1.3792272.\r\n2018-07-30 11:58:28,900:INFO:tensorflow:Begin evaluation of epoch 1\r\n2018-07-30 11:58:28,973:INFO:tensorflow:Calling model_fn.\r\n2018-07-30 11:58:36,515:INFO:tensorflow:Done calling model_fn.\r\n2018-07-30 11:58:36,539:INFO:tensorflow:Starting evaluation at 2018-07-30-03:58:36\r\n2018-07-30 11:58:41,904:INFO:tensorflow:Graph was finalized.\r\n2018-07-30 11:58:41,907:INFO:tensorflow:Restoring parameters from /home/lijiaji/test_models/multi_gou_test/model.ckpt-375\r\nTraceback (most recent call last):\r\n  File \"demo.py\", line 22, in <module>\r\n    run(config)\r\n  File \"/mnt/disk0/home/lijiaji/test_cvalgo/cv_algo/classification/run_classification.py\", line 473, in run\r\n    run_impl(args)   \r\n  File \"/mnt/disk0/home/lijiaji/test_cvalgo/cv_algo/classification/run_classification.py\", line 435, in run_impl\r\n    steps = args.save_steps * 2\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 464, in evaluate\r\n    output_dir=self.eval_dir(name))\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1521, in _evaluate_run\r\n    config=self._session_config)\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/evaluation.py\", line 209, in _evaluate_once\r\n    session_creator=session_creator, hooks=hooks) as session:\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 832, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 555, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1018, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1023, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 712, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 483, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 281, in prepare_session\r\n    config=config)\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 195, in _restore_checkpoint\r\n    saver.restore(sess, checkpoint_filename_with_path)\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1727, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 886, in run\r\n    run_metadata_ptr)\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1109, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1281, in _do_run\r\n    run_metadata)\r\n  File \"/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1300, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n\t [[Node: save/RestoreV2/_1311 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_1315_save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: save/RestoreV2/_1438 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_1443_save/RestoreV2\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](save/RestoreV2:719)]]\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This [issue](https://github.com/tensorflow/tensorflow/issues/19588) is resolved and it should fix current issue as well. Closing this issue since fix has been pushed. Feel free to reopen if the issue still persists.\r\nThanks!\r\n"]}, {"number": 21259, "title": "tf.data.Dataset.padded_batch() doesn't work with dataset.map", "body": "System information\r\nHave I written custom code: yes\r\nOS Platform and Distribution:  Ubuntu Linux \r\nTensorFlow installed from: conda\r\nTensorFlow version (use command below): 1.4.1\r\nPython version: 3.6.5\r\nDescribe the problem\r\nIt's a image classification task to read variable-length images from png files, to map them and to padd them. But Dataset.padded_batch() doesn't work with tf.dataset which uses map \r\n    \r\n    # source codes\r\n    data.load_data() # load data from disk\r\n    usable_data = data.get_data(use_percent=use_percent, tail=tail)\r\n    # usable_data is a tuple for features and labels\r\n    init_hook = IteratorInitHook()\r\n\r\n    def input_fn():\r\n        dataset = tf.data.Dataset.from_tensor_slices(usable_data)\r\n        dataset = dataset.shuffle(params.shuffle_buffer_size)\r\n        dataset = dataset.map(data.instance_as_tensor) # the instance_as_tensor method transfer an image into feature and label tensors, the shape of feature is (?,?,?) and label shape is ()\r\n        dataset = dataset.repeat(epochs)\r\n        dataset = dataset.padded_batch(params.batch_size, padded_shapes=([None, None, None], []))\r\n\r\n        iterator = dataset.make_initializable_iterator()\r\n        init_hook.iterator_init = iterator.initializer\r\n        next_example, next_label = iterator.get_next()\r\n        return {'sgram': next_example}, next_label\r\n\r\nException stack is \r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 414, in <module>\r\n    tf.app.run(main=train_or_predict)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 405, in train_or_predict\r\n    hparams=params,\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 218, in run\r\n    return _execute_schedule(experiment, schedule)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 46, in _execute_schedule\r\n    return task()\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 625, in train_and_evaluate\r\n    self.train(delay_secs=0)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 367, in train\r\n    hooks=self._train_monitors + extra_hooks)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 807, in _call_train\r\n    hooks=hooks)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 708, in _train_model\r\n    input_fn, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 577, in _get_features_and_labels_from_input_fn\r\n    result = self._call_input_fn(input_fn, mode)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 663, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"main.py\", line 218, in input_fn\r\n    padded_shapes=([None, None, None],[])\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 695, in padded_batch\r\n    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1292, in __init__\r\n    input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py\", line 512, in map_structure_up_to\r\n    assert_shallow_structure(shallow_tree, input_tree)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py\", line 372, in assert_shallow_structure\r\n    check_types=check_types)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py\", line 356, in assert_shallow_structure\r\n    \"Input has type: %s.\" % type(input_tree))\r\nTypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <class 'list'>.", "comments": ["Without the code of your `data.instance_as_tensor()` function it is difficult to say, but I think the bug might be in that function.\r\n\r\nFor example, the following trivial code combines `Dataset.map()` and `Dataset.padded_batch()` without error:\r\n\r\n```python\r\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])\r\ndataset = dataset.shuffle(10)\r\ndataset = dataset.map(lambda a: ([[[a]]], a))\r\ndataset = dataset.repeat(3)\r\ndataset = dataset.padded_batch(2, padded_shapes=([None, None, None], []))\r\n```\r\n\r\nThe most likely cause is that your `data.instance_as_tensor()` function does not return a pair of tensors as expected.", "Hi, Here is the instance_as_tensor code:\r\n\r\n   def instance_as_tensor(image_name, label=None):\r\n        \"\"\"Convert a single training/prediction instance into a tensor.\"\"\"\r\n        img_file = tf.read_file(image_name)\r\n        image = tf.image.decode_png(img_file, channels=0)\r\n        image = tf.cast(image, tf.float32)\r\n        image_data = tf.transpose(image[:128, :] / 256.)\r\n\r\n        if label is not None:\r\n            label = tf.cast(label, tf.int32)\r\n\r\n        return {'sgram': image_data}, label", "Right, so the problem is that the `padded_shapes` argument to `Dataset.padded_batch()` doesn't match the structure of the element. The following change should fix it:\r\n\r\n```python\r\ndataset = dataset.padded_batch(params.batch_size, padded_shapes=({'sgram': [None, None, None]}, []))\r\n```", "Hi, when I try to fix it with your solution, still fails, another error occur, type still not match\r\n\r\nTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'sgram': {'sgram': <tf.Tensor 'IteratorGetNext:0' shape=(?, ?, ?, ?) dtype=float32>}}. Consider casting elements to a supported type.", "What's the full stack trace for that error?", "The full stack trace is below:\r\n\r\nTraceback (most recent call last):\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 468, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 468, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/compat.py\", line 65, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got {'sgram': {'sgram': <tf.Tensor 'IteratorGetNext:0' shape=(?, ?, ?, ?) dtype=float32>}}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 416, in <module>\r\n    tf.app.run(main=train_or_predict)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 407, in train_or_predict\r\n    hparams=params,\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 218, in run\r\n    return _execute_schedule(experiment, schedule)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 46, in _execute_schedule\r\n    return task()\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 625, in train_and_evaluate\r\n    self.train(delay_secs=0)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 367, in train\r\n    hooks=self._train_monitors + extra_hooks)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 807, in _call_train\r\n    hooks=hooks)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 711, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 694, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/data0/hegang/utils/spoken-language-id/models/base.py\", line 28, in base_model_fn\r\n    model = model_class(features, training=mode == tf.estimator.ModeKeys.TRAIN, params=params)\r\n  File \"/data0/hegang/utils/spoken-language-id/models/cnn.py\", line 8, in __init__\r\n    batch_size = tf.shape(features)[0]\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 271, in shape\r\n    return shape_internal(input, name, optimize=True, out_type=out_type)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 295, in shape_internal\r\n    input_tensor = ops.convert_to_tensor(input)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 836, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 926, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 229, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 208, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 472, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'sgram': {'sgram': <tf.Tensor 'IteratorGetNext:0' shape=(?, ?, ?, ?) dtype=float32>}}. Consider casting elements to a supported type.", "That looks like a different problem:  the code in your `model_fn` is attempting to pass an (apparently nested) dictionary of tensors to `tf.shape()`, which expects a tensor. I think part of the problem is in your `input_fn` where you wrap `next_example` (which is already a dict mapping `'sgram'` to a tensor) in another dictionary:\r\n\r\n```python\r\n    return {'sgram': next_example}, next_label\r\n```\r\n\r\nInstead, you should be able to do:\r\n\r\n```python\r\n    return next_example, next_label\r\n```\r\n", "@mrry \r\n\r\nThanks a lot, it does works now"]}, {"number": 21258, "title": "Fix Linux XLA build status link", "body": "Was missing a `)`", "comments": []}, {"number": 21257, "title": "Request for Multi Graph composition", "body": "```\r\n\r\nIn [29]: with tf.Session() as sess:\r\n   ....:     x = tf.placeholder(tf.int32, [None])\r\n   ....:     a = x + 1\r\n   ....:     y = tf.placeholder(tf.int32, [None])\r\n   ....:     b = y + 1\r\n   ....:     print(sess.run(b, feed_dict={y:a, x:[1]}))\r\n   ....:\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-29-d72479c69760> in <module>()\r\n      4     y = tf.placeholder(tf.int32, [None])\r\n      5     b = y + 1\r\n----> 6     print(sess.run(b, feed_dict={y:a, x:[1]}))\r\n      7\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    887     try:\r\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 889                          run_metadata_ptr)\r\n    890       if run_metadata:\r\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1068\r\n   1069           if isinstance(subfeed_val, ops.Tensor):\r\n-> 1070             raise TypeError('The value of a feed cannot be a tf.Tensor object. '\r\n   1071                             'Acceptable feed values include Python scalars, '\r\n   1072                             'strings, lists, numpy ndarrays, or TensorHandles.')\r\n\r\nTypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.\r\n\r\n```\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\n\u279c  ~ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.4.0-19-ga52c8d9b01', '1.4.1')\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["You're using the API incorrectly. You have to create the session after the graph.", "Sorry, i pasted the wrong code... updated now.", "This is not how it is supposed to work and it is **not** a TensorFlow issue (edit: and not a missing feature). See [`tf.get_session_handle`](\r\nhttps://www.tensorflow.org/api_docs/python/tf/get_session_handle) for a experimental workaround or `tf.placeholder_with_default`.", "Thank you! you are right, it's not an issue, i was requesting it as a feature.\r\n", "Nagging Assignee @jart: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21256, "title": "Fix typo in docstring of is_tensor", "body": "The isinstance builtin expects a tuple of types, not list.", "comments": []}, {"number": 21255, "title": "Building TF with custom op support", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:  0.14.1\r\n- **GCC/Compiler version (if compiling from source)**:  5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9/7.1\r\n- **GPU model and memory**: 1080ti, 11gb, dual\r\n- **Exact command to reproduce**:\r\n\r\n1. Copied my custom op definitions in tensorflow/user_ops\r\n2. Modified BUILD script as follows:\r\n` tf_custom_op_library(\r\n    name = \"libsbnet.so\",\r\n    srcs = [\"reduce_mask.cu,\"reduce_mask.cu.h\",\"zero_block_counters.cu.h\",\"sparse_gather.cu\", \"sparse_blocks.cu.h\",\"sparse_gather.cc\",\"reduce_mask.cc\"],\r\n)`\r\n3. `bazel build --config=opt --config=cuda --config=monolithic //tensorflow:libtensorflow_cc.so`\r\n\r\nWhen running the graph with the new .so file in Tensorflow C++, i still get an error that the custom op is not found. Is there some documentation that i can follow to see if i am missing anything?\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@dhingratul can you please add the op-not-found error trace to the bug?\r\n\r\nAnd have you tried searching Stack Overflow with its signature?", "I was able to build the tensorflow_cc.so using cc_library in the main build file, and providing the path to the src and header files. But, my custom op also has .cu source files, if I add them to hdrs=[], it throws me an error and gives me the type of accepted file types. How do i make sure my .so is created with the cuda files ?", "@dhingratul OK, can you please show us the error when you add the .cu files to your BUILD script? ", "It gives me a list of accepted file formats(eg .cc, c, etc), of which .cu is not one of it. If i rename my file such that .cu -> .cu.cc, I am able to generate the .so file, but when i load that shared object with tf.load_op_library, it throws me an error saying that the Custom Op is not registered in this binary", "@josh11b this seems like it might be an op registration issue; can you comment on the GPU/Cuda registration process, please?", "I think the two missing pieces are:\r\n1 - all \"cu.cc\" files need to be listed under the \"gpu_srcs\" attribute of the \"tf_custom_op_library\" rule.\r\n2 - After adding your op, and creating the build rule, you will need to add the new BUILD rules you created to the dependencies of libtensorflow_cc.so", "@dhingratul,\r\nCan you please respond to the above comment? If the issue is not resolved yet, please refer this [Tensorflow Documentation regarding Custom Op](https://www.tensorflow.org/guide/create_op) and the respective [Github Repository](https://github.com/tensorflow/custom-op). Thanks!", "Wow, it's been 2 years, I don't think I have the reproducer anymore to verify that, although from what I remember i was able to resolve this back then. Feel free to close it, we can re-open this if I come across this again. Thanks."]}, {"number": 21254, "title": "InvalidArgumentError: assertion failed: [predictions must be in [0, 1]] [Condition x >= y did not hold element-wise:x (linear/head/predictions/logistic:0) = ]", "body": "### Describe the problem\r\nI am using TF 1.9. My model trains successfully but when I try to evaluate I get the error below. I was even thinking my create_test_input_fn() could be the problem so I tried to evaluate on the training data set but I got the same error.\r\n\r\n### Source code / logs\r\n\r\ndef create_train_input_fn(): \r\n    return tf.estimator.inputs.pandas_input_fn(\r\n        x=X_train.drop('indiv_key',axis=1),\r\n        y=y_train, \r\n        batch_size=32,\r\n        num_epochs=None, # Repeat forever\r\n        shuffle=True)\r\n\r\ndef create_test_input_fn():\r\n    return tf.estimator.inputs.pandas_input_fn(\r\n        x=X_test.drop('indiv_key',axis=1),\r\n        y=y_test, \r\n        num_epochs=1, # Just one epoch\r\n        shuffle=False) # Don't shuffle so we can compare to census_test_labels later\r\n\r\n\r\noutdir = 'graphs/linear'\r\nshutil.rmtree(outdir, ignore_errors = True) # start fresh each time\r\nmyopt = tf.train.AdamOptimizer(1e-4)\r\n\r\ntrain_input_fn = create_train_input_fn()\r\nestimator = tf.estimator.LinearClassifier(feature_columns ,\r\n            optimizer=myopt, \r\n            model_dir=outdir, \r\n            n_classes=2)\r\nestimator.train(train_input_fn, steps=5000)\r\n\r\n\r\ntest_input_fn = create_test_input_fn()\r\nestimator.evaluate(test_input_fn)\r\n\r\n=============\r\nERROR\r\n=============\r\nInvalidArgumentError: assertion failed: [predictions must be in [0, 1]] [Condition x >= y did not hold element-wise:x (linear/head/predictions/logistic:0) = ] [[0.0246640872][0.0240472779][0.0353220813]...] [y (linear/metrics/auc_precision_recall/Cast/x:0) = ] [0]\r\n\t [[Node: linear/metrics/auc_precision_recall/assert_greater_equal/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_FLOAT, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](linear/metrics/auc_precision_recall/assert_greater_equal/Assert/AssertGuard/Assert/Switch, linear/metrics/auc/assert_greater_equal/Assert/AssertGuard/Assert/data_0, linear/metrics/auc/assert_greater_equal/Assert/AssertGuard/Assert/data_1, linear/metrics/auc/assert_greater_equal/Assert/AssertGuard/Assert/Switch_1, linear/metrics/auc_precision_recall/assert_greater_equal/Assert/AssertGuard/Assert/data_3, linear/metrics/auc_precision_recall/assert_greater_equal/Assert/AssertGuard/Assert/Switch_2)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@tensorflowbutler\r\nHave I written custom code >> No I am using the estimator API\r\nOS Platform and Distribution >> Windows 7 enterprise\r\nTensorFlow installed from >> pip istalled\r\nTensorFlow version  >> 1.9\r\nBazel version >> No idea\r\nCUDA/cuDNN version >> CPU version\r\nGPU model and memory N/A\r\nExact command to reproduce >> I shared that above\r\nMobile device N/A", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21253, "title": "Enable int64 support for MatMul", "body": "This fix tries to address the issue raised in #21241 where there were no int64 support for MatMul. This fix adds int64 support for MatMul.\r\n\r\nThis fix fixes #21241.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @rmlarsen for the review and sorry for the bug in the test. The PR has been updated. Please take a look.", "Any update on getting this merged? Very interested in seeing it land!", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21252, "title": "tensorflow/contrib/lite/Makefile doesn't build", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Linux rodete\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.9.0\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.0\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Quadro M2000 4GB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout v1.9.0\r\n./tensorflow/contrib/lite/download_dependencies.sh\r\nmake -j $(nproc) -f tensorflow/contrib/lite/Makefile\r\n```\r\n\r\n\r\n### Describe the problem\r\n\r\nThe build fails with:\r\n\r\n```\r\n./tensorflow/contrib/lite/schema/schema_generated.h: In member function \u2018bool tflite::QuantizationParameters::Verify(flatbuffers::Verifier&) const\u2019:\r\n./tensorflow/contrib/lite/schema/schema_generated.h:1480:33: error: no matching function for call to \u2018flatbuffers::Verifier::Verify(const flatbuffers::Vector<float>*)\u2019\r\n```\r\n\r\nThe problem appears to be that `./tensorflow/contrib/lite/download_dependencies.sh` downloads the latest version of flatbuffers instead of a fixed version and [this](https://github.com/google/flatbuffers/commit/79f2adc50a23949d14b0da7d12a5d1451748e82c) commit to the flatbuffers repo renamed the `flatbuffers::Verifier::Verify` vector<T> overload to `flatbuffers::Verifier::VerifyVector`.\r\n\r\nHere's the full build error: [compile_error.txt](https://github.com/tensorflow/tensorflow/files/2242668/compile_error.txt)\r\n\r\nAll dependencies downloaded by `./tensorflow/contrib/lite/download_dependencies.sh` should use explicit versions.", "comments": ["Never mind, I see that this has been fixed with commit 474b40bc7cb33d25f9bdc187d021e94a807bf1bd"]}, {"number": 21251, "title": "TF_LoadLibrary Usage in C++", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/a\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**:9/7.1\r\n- **GPU model and memory**:1080 ti, 11gb dual\r\n- **Exact command to reproduce**:\r\n1. Create a .so file with custom ops\r\n2. Freeze the graph(with custom ops) by calling tf.load_op_library at top of the freezing graph function\r\n3. Load graph in C++ `    status = session_->Create(graph_def);`\r\n4. Load custom ops by importing .so file `    TF_Status* status_2 = TF_NewStatus();\r\n    TF_Library* lib =TF_LoadLibrary(\"libcustomOp.so\", status_2);\r\n    TF_Buffer op_list_buf = TF_GetOpList(lib);`\r\n\r\nQuestion: How do I attach these custom ops with the graph, as I still get an error that the custom op is not in the graph?\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Can you try to do something that is as close to the [unit test](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_test.cc#L203) as possible and see what the difference is?", "My question is how do I attach the operations that I get from `  TF_Buffer op_list_buf = TF_GetOpList(lib);\r\n  tensorflow::OpList op_list;` to the current session/ graph? As i am just using C++ for inference, and these custom operations are not a part of either input or output nodes, i cannot figure out this. \r\n\r\nI can also work with building tensorflow from source with the custom ops, but these have a  dependency on TF, any documented build process with .cc, .cu files would be useful", "CC @asimshankar for `C++`.\r\n\r\nCan you print out the ops before and after the load library? What exact error message are you getting?\r\n\r\nIf you do `nm --demangle libdsnet.so|grep ' b '` you should see some registrar entries.", "@dhingratul : I'm not sure I understand your question and what you mean by \"attaching the operations to the current session/graph\".\r\n\r\nWhen you invoke `TF_LoadLibrary`, any custom operations and kernels defined in that library are registered with the TensorFlow runtime. If you then import/construct a graph that uses those operations and execute in a session, it should work.\r\n\r\nCould you elaborate on what exactly you're trying to do, what you've tried so far, and what errors you're getting?", "1. Train my model using Tensorflow in Python with the custom op using tf.load_op_library(path)\r\n2. Freeze the graph by modifying the freeze script by adding tf.load_op_library at top\r\n3. Use the frozen protobuf to perform inferences with Tensorflow C++ API by providing input and output nodes\r\nWorks: Build tensorflow from source with the custom op to generate a single .so file\r\nDoes Not Work: Directly loading the custom op .so with using TF_LoadLibrary\r\nI am looking for a correct way of using TF_LoadLibrary .\r\n\r\nNote: The custom ops are neither input or the output nodes.", "Mind elaborating on what you mean by \"does not work\"? Does the `TF_LoadLibrary` call succeed (as in, `TF_GetCode(status) == TF_OK`)?  Is `TF_LoadLibrary` being called before or after the `Session` is created with the `GraphDef`? (The issue report suggests it is after, but it should be before). \r\n\r\nSee also https://stackoverflow.com/questions/50475320/executing-frozen-tensorflow-graph-that-uses-tensorflow-contrib-resampler-using-c/50538292#50538292 \r\n\r\nDo let us know", "It works if I build tensorflow without the monolithic flag, as the cutsom op has a dependancy on some of the functions from libtensorflow_framework. However, if i build with monolithic flag, it is not able to find the custom op, and throws the error that the custom op is not registered with the binary", "Ah, could you elaborate on the motivations to build with the monolithic flag?\r\nIn this case, I suspect the `TF_LoadLibrary` call is failing (and returning an error in the `TF_Status` object). Could you confirm that is the case?", "I used monolithic flag so that only one .so is created instead of handling two different ones. \r\nThe call to TF_LoadLibrary doesn't fail, but i get an error that the op is not registered with the binary", "Hey @dhingratul I'm facing the same issue but I haven't been able to build my custom op with tensorflow source as well. Can you tell me how you did it? It'll be really helpful! I added the .cc files and the BUILD rule in tensorflow/core/user_ops but the build doesn't succeed. Need the custom op for inference in C++ too.", "Add cc_library, and name it. Add that name in the BUILD file", "@asimshankar I have similar question, the difference is that I build tf and custom op in Windows. When the very simple custom op lib `zero_out.dll` is built, it can be load using python, but when I load using `TF_LoadLibrary`, it returns `TF_NOT_FOUND = 5`, the dll is not found. May some dependency dll are needed? I placed built `tensorflow_cc.dll` with `zero_out.dll` in the same folder.", "I am facing a similar issue, I am able to build my ops using Bazel, which is, in turn, creating a .so file in the Bazel-build folder but then when I load a prebuild graph(.meta file) and attach it to my sessions using in C++, it says that kernel not registered. It works using the python api. Also, I am able to load graphs without any custom ops with the same c++ api. The call to TF_LoadLibrary doesn't fail, but i get an error that the op is not registered with the binary."]}, {"number": 21249, "title": "Adding a less than constraint on numpy.", "body": "", "comments": []}, {"number": 21248, "title": "[Bug] TensorRT conversion error with conv2d_transpose", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.9.0rc0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0.176/7.0.5\r\n- **GPU model and memory**: GTX 1080 with 8GiB memory\r\n- **Exact command to reproduce**: python ./minimal_graph.py\r\n\r\n### Describe the problem\r\nWhen I use Tensorflow's TensorRT integration from tensorflow.contrib to transform my model into tensorRT compatible graph, I notice that if I have conv2d_transpose layer (which is not supported for conversion yet) following some convolution layer (or other layer that can be optimized), the latter would be transformed into a tensorRT operation during graph optimization and eventually cause Cudnn to Segfault. \r\n\r\nBelow is a minimal example I figured out to trigger this bug, which describe the process to transform a simple randomly initialized network into tensorRT inference graph, and then execute it with some dummy input.\r\n\r\nThe Tensorflow version I use comes from dockerhub's nightly build (tensorflow/tensorflow:nightly-devel-gpu) with build configuration changed to enable tensorRT. I also notice that previous tensorflow version (like 1.8.0) has the same issue. On 1.9.0 there's a bug that prevents tensorRT to run correctly so I use 1.9.0rc0 instead.\r\n\r\nThe tensorRT version I use is 3.0.4. I notice another issue from [this link](https://github.com/tensorflow/tensorflow/issues/20157) claiming conv2d_transpose is not supported, but here the problem is that the model won't correctly executed instead of not transformed.\r\n\r\n\r\n### Source code / logs\r\n\r\n#### Code\r\nminimal_graph.py\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import tensorrt as trt\r\n\r\n\r\ndef build_graph_from_def(graph_def, input_nodes, output_nodes):\r\n    \"\"\"\r\n    build the actual graph from definition\r\n    \"\"\"\r\n    tf.reset_default_graph()\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        return_tensors = [operation_name + \":0\" for operation_name in input_nodes + output_nodes]\r\n        tensors = tf.import_graph_def(graph_def=graph_def, name=\"\",\r\n                                      return_elements=return_tensors)\r\n        input_tensor_list = tensors[:len(input_nodes)]\r\n        output_tensor_list = tensors[len(input_nodes):]\r\n\r\n    return graph, input_tensor_list, output_tensor_list\r\n\r\n\r\ndef main():\r\n    with tf.variable_scope(\"Net\"):\r\n        inp = tf.placeholder(tf.float32, shape=(1, 28, 28, 3), name=\"input_image\")\r\n        deconv1 = tf.layers.conv2d_transpose(inp, filters=8, kernel_size=(3, 3), strides=(2, 2))\r\n        output = tf.layers.conv2d(deconv1, filters=8, kernel_size=(3, 3), name=\"output\")\r\n    input_nodes = [\"Net/input_image\"]\r\n    output_nodes = [\"Net/output/BiasAdd\"]\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        const_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, sess.graph.as_graph_def(), output_nodes)\r\n\r\n    optimized_graph_def = trt.create_inference_graph(\r\n        input_graph_def=const_graph_def,\r\n        outputs=output_nodes,\r\n        max_batch_size=1,\r\n        max_workspace_size_bytes=1 << 25)\r\n    graph, input_tensors, output_tensors = build_graph_from_def(\r\n        optimized_graph_def, input_nodes, output_nodes)\r\n    with tf.Session(graph=graph) as sess:\r\n        sess.run(output_tensors[0], feed_dict={input_tensors[0]: np.zeros((1, 28, 28, 3))})\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n#### Log\r\n```$ python ./minimal_graph.py\r\n2018-07-30 10:30:11.355872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-30 10:30:11.356407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1404] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 6.07GiB\r\n2018-07-30 10:30:11.356418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0\r\n2018-07-30 10:30:11.604399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-30 10:30:11.604422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 \r\n2018-07-30 10:30:11.604427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N \r\n2018-07-30 10:30:11.604573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-07-30 10:30:11.604697: E tensorflow/core/common_runtime/gpu/gpu_device.cc:228] Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=0 set to 1 instead.\r\n2018-07-30 10:30:11.669166: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-07-30 10:30:11.671202: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'Net/', converted to graph\r\n2018-07-30 10:30:11.674008: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n2018-07-30 10:30:12.200384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0\r\n2018-07-30 10:30:12.200405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-30 10:30:12.200411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 \r\n2018-07-30 10:30:12.200414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N \r\n2018-07-30 10:30:12.200490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-07-30 10:30:12.212534: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Parameter check failed at: cudnnEngine.cpp::enqueue::140, condition: bindings[x] != nullptr\r\n2018-07-30 10:30:12.212549: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:401] Failed to enqueue batch for TRT engine: Net/my_trt_op_0\r\n[1]    18050 segmentation fault (core dumped)\r\n```\r\n", "comments": [" @aaroey ", "Thanks for the report. I can reproduce this with tf r1.10rc1. Will update once we have more information.", "Any update?", "I debugged a bit but could not find the problem. The log shows that input tensor was added to the engine at the time when the engine is being built, so it looks more like a trt library problem. @pooyadavoodi do you have any ideas?", "Also @samikama @jjsjann123 ", "Same error in tensorflow 1.10.1 and tensorrt (3.0.4, 4.0.1) cuda (9.2, 9.0) cudnn 7.1.4", "I finally found the root-cause, the reason is that it set the ITensor name multiple times which changes the input binding name. I'll commit a fix later.", "When I run mobilenet_v2, same error in tensorflow 1.10.1and tensorrt 2.0.1, cuda 9.0, cudnn 7. But without error in Inception-V4.\r\n\r\n**Code**\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom threading import Lock\r\nimport os\r\nimport sys\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.tensorrt as trt\r\n\r\n\r\ndef GPU_config(rate=0.95):\r\n    gpuConfig = tf.ConfigProto()\r\n    gpuConfig.allow_soft_placement = True\r\n    gpuConfig.gpu_options.allow_growth = True\r\n    gpuConfig.gpu_options.per_process_gpu_memory_fraction = rate\r\n    gpuConfig.log_device_placement = False\r\n    return gpuConfig\r\n\r\n\r\ndef get_graph_def(pb_path):\r\n    with tf.gfile.GFile(pb_path, \"rb\") as f:\r\n        graph_read = f.read()\r\n\r\n    with tf.Graph().as_default():\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(graph_read)\r\n    return graph_def\r\n\r\ndef run_graph(gdef, dumm_inp):\r\n    gpu_options = GPU_config()\r\n    tf.reset_default_graph()\r\n    g = tf.Graph()\r\n    with g.as_default():\r\n        tf.import_graph_def(graph_def=gdef,name=\"SAMA\")\r\n    inp = g.get_tensor_by_name('SAMA/Placeholder_X:0')\r\n    out = g.get_tensor_by_name('SAMA/Predictions:0')\r\n    with tf.Session(graph=g, config=gpu_options) as sess:\r\n        _ = sess.run(out, feed_dict={inp: dumm_inp})\r\n        start_time = time.time()\r\n        for i in range(100):\r\n            val = sess.run(out, feed_dict={inp: dumm_inp})\r\n        end_time = time.time()\r\n        print(\"cost time is: \",str((end_time-start_time)*1000),' ms')\r\n    return val\r\n\r\ndef write_trt_graph_to_file(graph_def, graph_name):\r\n    with tf.gfile.GFile(graph_name, \"wb\") as f:\r\n      f.write(graph_def.SerializeToString())\r\n\r\ndef get_tftrt_name(graph_name, precision_string):\r\n    return \"tftrt_{}_{}\".format(precision_string.lower(), graph_name)\r\n\r\ndef run_calibration(gdef, dumm_inp):\r\n    \"\"\"Run given calibration graph multiple times.\"\"\"\r\n    # gpu_options = cpb2.GPUOptions(per_process_gpu_memory_fraction=0.50)\r\n    gpu_options = GPU_config()\r\n    ops.reset_default_graph()\r\n    g = ops.Graph()\r\n    with g.as_default():\r\n        tf.import_graph_def(graph_def=gdef, name=\"SAMA\")\r\n    inp = g.get_tensor_by_name('SAMA/Placeholder_X:0')\r\n    out = g.get_tensor_by_name('SAMA/Predictions:0')\r\n    with tf.Session(config=gpu_options, graph=g) as sess:\r\n        # run over real calibration data here, we are mimicking a calibration set of\r\n        # 30 different batches. Use as much calibration data as you want\r\n        for _ in range(30):\r\n            val = sess.run(out, {inp: dumm_inp})\r\n    return val\r\n\r\nif __name__ == \"__main__\":\r\n    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)\r\n    # print('begin...')\r\n    dummy_input = np.ones((4,224,224,3)).astype(np.float32)\r\n    pb_path = './core/model/model.pb'\r\n    precision_mode = \"FP32\"\r\n    org_graph = get_graph_def(pb_path)\r\n    # o1 = run_graph(org_graph, dummy_input)\r\n    # print(\"o1's done!\")\r\n    # print('o1: ', o1[0])\r\n\r\n    trt_graph = trt.create_inference_graph(\r\n      input_graph_def=org_graph,\r\n      outputs=[\"Predictions\"],\r\n      max_batch_size=224,\r\n      max_workspace_size_bytes=1 << 25,\r\n      precision_mode=\"FP32\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"\r\n      minimum_segment_size=2  # minimum number of nodes in an engine\r\n    )\r\n    o2 = run_graph(trt_graph, dummy_input)\r\n    print(\"---------------------------\")\r\n    print(\"o2's done!\")\r\n    print('o2: ', o2[0])\r\n```\r\n\r\n\r\n**Log**\r\nWARNING:tensorflow:TensorRT mismatch. Compiled against version 3.0.4, but loaded 4.0.1. Things may not work\r\n2018-09-08 16:50:50.962847: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-09-08 16:50:51.087941: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:756] MULTIPLE tensorrt candidate conversion: 36\r\n2018-09-08 16:50:51.089689: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.099422: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.101876: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.104649: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.109036: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.112530: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.116195: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.121674: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.126477: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.131572: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.138909: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.145751: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.152753: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.159423: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.165827: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.172368: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.178878: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.185448: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.192249: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.199121: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.205969: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.213014: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.221183: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.229179: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.237838: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.246279: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.255204: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.263960: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.272671: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.281564: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.290650: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.299656: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.308670: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.318329: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.327576: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.336869: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-09-08 16:50:51.347347: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:51.347402: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:53.077100: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:53.077149: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:53.229884: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:53.229922: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:54.000296: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:54.000347: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:54.290206: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:54.290252: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:54.882412: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:54.882443: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:55.481933: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:55.481966: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:55.736761: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:55.736812: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:56.043892: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:56.043917: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:56.881252: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:56.881297: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:57.164363: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:57.164412: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:58.037810: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:58.037856: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:58.286105: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:58.286143: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:50:59.044338: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:50:59.044380: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:01.582217: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:01.582272: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:02.783233: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:02.783283: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:04.294398: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:04.294431: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:06.281261: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:06.281298: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:06.728836: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:06.728879: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:07.039253: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:07.039291: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:07.873704: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:07.873759: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:09.072112: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:09.072181: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:09.341916: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:09.341955: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:09.569091: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:09.569141: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:09.836272: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:09.836310: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:10.648372: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:10.648425: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:10.872321: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:10.872359: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:11.096381: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:11.096422: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:11.312129: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:11.312185: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:11.642882: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:11.642919: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:11.850653: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:11.850691: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:12.622117: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:12.622153: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:13.361002: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:13.361034: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:13.579295: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:13.579334: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:14.092344: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:14.092391: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:14.317894: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:720]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-09-08 16:51:14.317929: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:844] Can't identify the cuda device. Running on device 0 \r\n2018-09-08 16:51:14.890015: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-08 16:51:14.890747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.52GiB\r\n2018-09-08 16:51:14.890768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-08 16:51:14.890786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-08 16:51:14.890791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-09-08 16:51:14.890799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-09-08 16:51:14.891010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10619 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2018-09-08 16:51:15.155748: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Parameter check failed at: cudnnEngine.cpp::enqueue::267, condition: bindings[x] != nullptr\r\n2018-09-08 16:51:15.155796: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:401] Failed to enqueue batch for TRT engine: SAMA/my_trt_op_16\r\nTraceback (most recent call last):\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to enqueue batch for TRT engine: SAMA/my_trt_op_16\r\n\t [[Node: SAMA/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[224], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,32,112,112]], max_cached_engines_count=1, output_shapes=[[?,32,112,112]], precision_mode=\"FP32\", segment_funcdef_name=\"my_trt_op_16_native_segment\", serialized_segment=\"h\\n\\000\\00...00\\000\\000\", static_engine=true, workspace_size_bytes=288640, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](SAMA/MobilenetV2_1/Conv/Relu6)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test_trtdemo.py\", line 103, in <module>\r\n    o2 = run_graph(trt_graph, dummy_input)\r\n  File \"test_trtdemo.py\", line 51, in run_graph\r\n    _ = sess.run(out, feed_dict={inp: dumm_inp})\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to enqueue batch for TRT engine: SAMA/my_trt_op_16\r\n\t [[Node: SAMA/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[224], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,32,112,112]], max_cached_engines_count=1, output_shapes=[[?,32,112,112]], precision_mode=\"FP32\", segment_funcdef_name=\"my_trt_op_16_native_segment\", serialized_segment=\"h\\n\\000\\00...00\\000\\000\", static_engine=true, workspace_size_bytes=288640, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](SAMA/MobilenetV2_1/Conv/Relu6)]]\r\n\r\nCaused by op 'SAMA/my_trt_op_16', defined at:\r\n  File \"test_trtdemo.py\", line 103, in <module>\r\n    o2 = run_graph(trt_graph, dummy_input)\r\n  File \"test_trtdemo.py\", line 47, in run_graph\r\n    tf.import_graph_def(graph_def=gdef,name=\"SAMA\")\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\r\n    _ProcessNewOps(graph)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\r\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3289, in _add_new_tf_operations\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3289, in <listcomp>\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3180, in _create_op_from_tf_operation\r\n    ret = Operation(c_op, self)\r\n  File \"/home/vismarty/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): Failed to enqueue batch for TRT engine: SAMA/my_trt_op_16\r\n\t [[Node: SAMA/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[224], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,32,112,112]], max_cached_engines_count=1, output_shapes=[[?,32,112,112]], precision_mode=\"FP32\", segment_funcdef_name=\"my_trt_op_16_native_segment\", serialized_segment=\"h\\n\\000\\00...00\\000\\000\", static_engine=true, workspace_size_bytes=288640, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](SAMA/MobilenetV2_1/Conv/Relu6)]]", "This is fixed by #22371", "Cool, thanks!"]}, {"number": 21247, "title": "TensorFlow Lite - Sample Android app returns Invalid handle to Interpreter on AOSP", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nAndroid.mk added for AOSP build\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nGeneric Android 8 device\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**:1.7.1\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:0.14.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nTF-Lite sample Android app returns error when it is built as a part of AOSP system app. \r\nI have run Android sample TF-Lite app and it works with no error, but if I try to build the same sample app (SpeechActivity part) in AOSP using Android.mk, I've got the \"java.lang.IllegalArgumentException: Invalid handle to Interpreter\".\r\n\r\nI have added same downloaded conv_actions_frozen.tflite and conv_action_labels.txt to \"/assets\" directory and specify it in Android.mk as follows.\r\n\r\nLOCAL_ASSET_FILES += $(call find-subdir-assets)\r\n\r\n### Source code / logs\r\njava.lang.RuntimeException: Unable to start activity ComponentInfo{com.mycompany.mmiservice/com.mycompany.mmiservice.SpeechActivity}: java.lang.IllegalArgumentException: Invalid handle to Interpreter.\r\n    at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2820)\r\n    at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895)\r\n    at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n    at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596)\r\n    at android.os.Handler.dispatchMessage(Handler.java:105)\r\n    at android.os.Looper.loop(Looper.java:164)\r\n    at android.app.ActivityThread.main(ActivityThread.java:6565)\r\n    at java.lang.reflect.Method.invoke(Native Method)\r\n    at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\n    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\r\n Caused by: java.lang.IllegalArgumentException: Invalid handle to Interpreter.\r\n    at org.tensorflow.lite.NativeInterpreterWrapper.resizeInput(Native Method)\r\n    at org.tensorflow.lite.NativeInterpreterWrapper.resizeInput(NativeInterpreterWrapper.java:155)\r\n    at org.tensorflow.lite.Interpreter.resizeInput(Interpreter.java:191)\r\n    at com.mycompany.mmiservice.SpeechActivity.onCreate(DummyActivity.java:143)", "comments": ["Could you please provide a bit more information on how to reproduce this?", "@joonblue : There should be some more information in the logcat, can you share that.", "@shashishekhar, sorry for the delay.\r\nI have added TFLite to AOSP 8 as a system preload app (for POC of my project) and built it using Android.mk.\r\n\r\nHere's my Android.mk look like,\r\nLOCAL_PATH := $(call my-dir)\r\n// TensorFlow-Lite sample app build\r\ninclude $(CLEAR_VARS)\r\nLOCAL_PACKAGE_NAME := tflite-sample\r\nLOCAL_SDK_VERSION := current\r\nLOCAL_MODULE_TAGS := optional\r\nLOCAL_SRC_FILES := $(call all-java-files-under, app/src/main/java)\r\nLOCAL_CERTIFICATE := platform\r\nLOCAL_AAPT_FLAGS := \\\r\n                  --auto-add-overlay \\\r\n                  --extra-packages android.support.v7.appcompat \\\r\n                  --extra-packages android.support.design \\\r\n                  --extra-packages android.support.constraint\r\nLOCAL_PROGUARD_FLAG_FILES := app/proguard-rules.pro\r\nLOCAL_JAVACFLAGS :=-Xlint:all -Xlint:-path\r\nLOCAL_ASSET_FILES := $(call find-subdir-assets)\r\nLOCAL_RESOURCE_DIR := $(LOCAL_PATH)/app/src/main/res\r\nLOCAL_RESOURCE_DIR += \\\r\n                        prebuilts/sdk/current/support/v7/appcompat/res \\\r\n                        prebuilts/sdk/current/support/design/res\r\nLOCAL_STATIC_JAVA_LIBRARIES := \\\r\n                               android-support-v4 \\\r\n                               android-support-v7-appcompat \\\r\n                               android-support-design \\\r\n                               constraint-layout-solver\r\nLOCAL_STATIC_JAVA_AAR_LIBRARIES := \\\r\n                                constraint-layout \\\r\n                                tensorflow-lite\r\nLOCAL_FULL_MANIFEST_FILE := $(LOCAL_PATH)/app/src/main/AndroidManifest.xml\r\ninclude $(BUILD_PACKAGE)\r\n\r\ninclude $(CLEAR_VARS)\r\nLOCAL_PREBUILT_STATIC_JAVA_LIBRARIES := tensorflow-lite:libs/tensorflow-lite-0.0.0-nightly.aar\r\ninclude $(BUILD_MULTI_PREBUILT)\r\n\r\nI have used downloaded tensorflow-lite:libs/tensorflow-lite-0.0.0-nightly.aar and copied conv_actions_labels.txt and conv_actions_frozen.tflite to <project-root>/assets/\r\n\r\nAnd here's the locgat,\r\n01-01 00:44:50.547 4652-4652/org.tensorflow.lite.demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.lite.demo, PID: 4652\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{org.tensorflow.lite.demo/org.tensorflow.demo.SpeechActivity}: java.lang.IllegalArgumentException: Invalid handle to Interpreter.\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2820)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895)\r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596)\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6565)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\r\n     Caused by: java.lang.IllegalArgumentException: Invalid handle to Interpreter.\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.resizeInput(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.resizeInput(NativeInterpreterWrapper.java:155)\r\n        at org.tensorflow.lite.Interpreter.resizeInput(Interpreter.java:191)\r\n        at org.tensorflow.demo.SpeechActivity.onCreate(SpeechActivity.java:174)\r\n        at android.app.Activity.performCreate(Activity.java:6975)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2773)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895)\u00a0\r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0)\u00a0\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\u00a0\r\n        at android.os.Looper.loop(Looper.java:164)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:6565)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\u00a0\r\n\r\nPlease let me know if you need more info.", "@joonblue : \r\nLooks like invalid handle to interpreter. Are you closing the interpreter?\r\nCan you print the value of  NativeInterpreterWrapper.interpreter (using reflection)\r\n", "@joonblue : Please reopen if you are still are facing this issues."]}, {"number": 21246, "title": "Fix test breakages for rc1.", "body": "", "comments": []}, {"number": 21245, "title": "TFLite Android: Model file will not load. startOffset and declaredLength problems", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Pixel 2XL, Android studio emulator\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: Loading TFLite model in an Android application\r\n\r\nPosted as an issue here based on comment from https://stackoverflow.com/questions/51341554/tflite-android-model-file-will-not-load-startoffset-and-declaredlength-problem?noredirect=1#comment90097492_51341554\r\n\r\nI'm having issues with loading a TFLite model using the MappedByteBuffer method from the Tensorflow-for-poets-2 TFLite tutorial.\r\n\r\n```\r\nprivate MappedByteBuffer loadModelFile(Activity activity,String MODEL_FILE) throws IOException {\r\n    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_FILE);\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n```\r\n\r\nIn particular the model I have converted with the tflite_convert (formerly toco) tools fails when the fileChannel.map is returned. The model is a floating point TFLite mode.\r\n\r\nThe problem seems to be caused by the startOffset and declaredLength variables. The error I receive in the logcat is\r\n```\r\n7525-7525/dp.thexor A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 7525 (dp.thexor), pid 7525 (dp.thexor)\r\nIf I fix these values to ones from a model that successfully loads then the method successfully returns the fileChannel.map. The values from my model are startOffset = 2846788 declaredLength = 45525464\r\n```\r\n\r\nI know that my TFlite model can successfully be mapped to memory as the tensorflow/contrib/lite/tools/benchmark:benchmark_model is able to benchmark it.\r\n\r\nI'd try to load a quantized TFLite model but currently my model contains ops that do not have quantized equivalents (transpose_conv).\r\n\r\nWhat could be causing this?\r\n\r\nMy .tflite model can be found here: https://github.com/andrewginns/CycleGAN-Tensorflow-PyTorch/releases/download/tf1.7-py3.6.4/float.tflite\r\n", "comments": ["Hi @andrewginns Have you tried using your model with the [TF Lite demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app)? (Trying to find out if this affect only tensorflow for poets or not)\r\n\r\nIt looks like your model is OK and can run with benchmark_model, for example:\r\n```\r\n$ bazel run -c opt tensorflow/contrib/lite/tools/benchmark:benchmark_model -- --graph=`realpath ~/Downloads/float.tflite`\r\n```", "Hi @andrehentz yeah I have tried with the TF Lite demo app. I went further and tried with a minimal app that should just load the model file into memory but that throws the same error.\r\n\r\nThat's what confuses me too. The TF Lite model benchmarks work on both desktop and mobile.\r\n\r\nI'm also able to view the layers of the model using the html visualiser:\r\n\r\nhttps://drive.google.com/file/d/1Ovg8e_Xe7SidhlGBGQ_epHEgnd2e4q9m/view?usp=sharing", "Maybe you can narrow down the crash to a line of native code somewhere? If so, could you share a link?", "I tried your model it fails in the model verification:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L302\r\n\r\nCan you share the original model before conversion, I will like to see what is going wrong here.", "@andrehentz \r\nMy original code is here\r\nhttps://github.com/andrewginns/CycleGAN-Tensorflow-PyTorch\r\n\r\nThe network is defined in models.py\r\nhttps://github.com/andrewginns/CycleGAN-Tensorflow-PyTorch/blob/master/models.py\r\n\r\nThanks! \r\n\r\n@shashishekhar\r\n\r\nThanks for looking into it. Here's my model before conversion. \r\n\r\nhttps://github.com/andrewginns/CycleGAN-Tensorflow-PyTorch/releases/download/tf1.7-py3.6.4/frozen-graph.pb", "@andrewginns : Quick update, I was able to reproduce the issue, I am working with @miaout17  to check if this is due to any alignment issues.", "@shashishekhar Much appreciated", "Nagging Assignees @miaout17, @jdduke, @shashishekhar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@andrewginns : We did a few alignment related fixes and I reconverted the model, I was able to load it without issues, let me know if you are still facing issues. Use tflite_convert for converting the model.\r\n\r\nPlease reopen if you are still facing the issue."]}, {"number": 21244, "title": "Make full model before calling set_model on callback", "body": "Commit 1b67ccbe8006eacffd268553abd01310e8b187d6 removed the _make_train_function calls from Keras training fit_generator for eager execution.\r\n\r\nThis breaks some callbacks that depend on the entire model to be populated on the set_model or on_train_begin methods.\r\n\r\nThis commit adds the method calls back in but guarded by an eager check.  It is not doing a revert / fix because the fix that removed the calls also put a test case in for eager fit_generator testing which we want to retain.", "comments": ["@martinwicke , would you be able to review this change? We would like to get this cherry-picked into 1.10 if possible. ", "Pinging for review", "@ tanzhenyu I have completed the changes you requested.", "Nagging Reviewer @tanzhenyu: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21243, "title": "aggregation of sparse gradient and dense gradient is unexpected", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.5\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: pip binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\n\r\n### Source code / logs\r\n\r\nExample code:\r\n\r\n```\r\n    var = tf.get_variable(\"var\", (3, 5), initializer=tf.ones_initializer())\r\n    session.run(var.initializer)\r\n    loss = tf.reduce_sum(tf.nn.embedding_lookup(var, [1]) ** 2) + tf.reduce_sum(var ** 2)\r\n    grad, = tf.gradients(loss, var)\r\n    print(\"grad:\", grad)  # It is an IndexedSlices.\r\n    grad_np = session.run(grad)\r\n    print(\"grad value:\")\r\n    print(grad_np)\r\n```\r\n\r\nOutput:\r\n```\r\n...\r\ngrad value:\r\nIndexedSlicesValue(values=array([[2., 2., 2., 2., 2.],\r\n       [2., 2., 2., 2., 2.],\r\n       [2., 2., 2., 2., 2.],\r\n       [2., 2., 2., 2., 2.]], dtype=float32), indices=array([0, 1, 2, 1], dtype=int32), dense_shape=array([3, 5], dtype=int32))\r\n```\r\n\r\nThe logic in TF is implemented in `_AggregatedGrads` and `_AggregateIndexedSlicesGradients`.\r\nAs there is one dense gradient, I would have expected also a final dense gradient.\r\nI think that the dense var updates of the optimizers (e.g. `ApplyAdam`) also perform more efficient compared to the corresponding sparse updates (implemented in `_apply_sparse`).\r\n\r\nRegarding how to accumulate the dense and sparse gradients: Probably it can stay like it is, via first accumulating them in `IndexedSlices`, and an additional flag whether there was a dense gradient, and if so, then do a final dense conversion, via `tf.convert_to_tensor`.\r\n", "comments": ["@albertz,\r\nSorry for the delayed response. Upon executing your code on **`Tensorflow Version 1.15.2`**, it is returning **`Dense Gradients`**, not  **`IndexedSlices`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/498380399d69bb013b60ba3c59bf9ef5/gh_21243.ipynb) of the working code and let us know if this is what your expectation is. Thanks!", "Yes, that is now what I would expect. Thanks!", "@albertz,\r\nThank you for the confirmation. Closing the issue as it has been resolved. Thanks! "]}, {"number": 21242, "title": "The method of open addressing is not quadratic probing", "body": "There are three method in open addressing: linear probing, quadratic probing and double hashing, they have such functions:\r\nlinear probing: h(k, i) = (h'(k) + i) mod m   (eq.1)\r\nquadratic probing: h(k, i) = (h'(k) + c1 * i + c2 * i^2) mod m  (eq.2)\r\nlinear probing: h(k, i) = (h1(k) + i * h2(k)) mod m  (eq.3)\r\nThe difference between linear probing and quadratic probing is that there are quadratic component in quadratic probing, c2 * i^2.\r\nBut I can't see it in NextIndex():\r\n```\r\ninline size_t NextIndex(size_t i, uint32 num_probes) const {\r\n    // Quadratic probing.\r\n    return (i + num_probes) & mask_;\r\n  }\r\n```\r\nThe num_probes is the number of probe, like i in eq.1, so I think NextIndex() is the implement of eq.1. But all the comments said this is quadratic probing, so I add quadratic component in NextIndex().\r\n\r\nBesides, the coefficient of quadratic component I choose is random, c1 and c2 is set manually, maybe other numbers are better.", "comments": ["Please add a more descriptive/specific PR title/message. Will make it easier to triage. Thanks for the PR.", "@case540 done, thanks for reply."]}, {"number": 21241, "title": "Feature request: matmul operation for int64 datatype", "body": "For an open source project on [sMPC](https://en.wikipedia.org/wiki/Secure_multi-party_computation) in Tensorflow I am using the int64 datatype [to represent decimal numbers](https://www1.cs.fau.de/filepool/publications/octavian_securescm/secfp-fc10.pdf). However, the matmul operations is currently not supported for this datatype. Is there any chance this could be enabled in the future? Currently we are using the [Chinese remainder theorem](https://en.wikipedia.org/wiki/Chinese_remainder_theorem) to represent the numbers, but as a result all operations are significantly slower.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n", "comments": ["Added a PR #21253 for int64 support of MatMul.", "Thanks a lot! Is it also possible to enable it for gpu?", "Nagging Reviewer @yongtang: It has been 75 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 21240, "title": "Add the document to avoid saving iterator while shuffling", "body": "Resolved https://github.com/tensorflow/tensorflow/issues/18583 by simply adding the instruction.", "comments": ["Hi @saxenasaurabh , can you help to review this?", "Looks like it have been fixed by @saxenasaurabh . If so, I will close this PR.", "Close since it was fixed."]}, {"number": 21239, "title": "GPU doesn't work when I request PoseNet API (@tensorflow-models/posenet) with node.js and express", "body": "### Problem\r\nExcuse me.\r\nI implemented demo app with tensorflow-models/posenet, node.js and express.\r\nI replaced the sample code as follows, but GPU may not work when executing estimateMultiplePoses().\r\nCould you tell me how to run my code on GPU?\r\n\r\nFor example, I got a very low response when I post my API to a localhost server.\r\n\r\n```\r\n$curl -X POST -F thumbnail=@./picture.png localhost:3000/upload\r\n```\r\n\r\n[At server log] \r\nI want to get the response less than 100 ms.\r\nI got the response less than 30 ms when I didn't use PoseNet API, so thought that this low response was caused by not running on GPU.\r\n\r\n```\r\nPOST /upload 200 2110.888 ms - 9372\r\n```\r\n\r\nHowever,  the process may be alive when building the server.\r\n\r\n```\r\n$ nvidia-smi\r\nTue Jul 31 10:27:38 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 107...  On   | 00000000:01:00.0  On |                  N/A |\r\n| 27%   40C    P8    13W / 180W |   7732MiB /  8116MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 107...  On   | 00000000:02:00.0 Off |                  N/A |\r\n| 27%   38C    P8    12W / 180W |   7723MiB /  8119MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1252      G   /usr/lib/xorg/Xorg                           281MiB |\r\n|    0      2068      G   compiz                                       226MiB |\r\n|    0      2435      G   ...-token=...                                 61MiB |\r\n|    0      2697      G   unity-control-center                          15MiB |\r\n|    0      2945      G   ...-token=...                                 92MiB |\r\n|    0     14062      G   ...-token=...                                 66MiB |\r\n|    0     26892      C   node                                        6983MiB |\r\n|    1     26892      C   node                                        7711MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\n\r\nregards.\r\n\r\n----\r\n### Custom Code\r\n\r\n./routes/index.js\r\n\r\n``` \r\n\r\nvar posenet = require('@tensorflow-models/posenet');\r\nvar tf = require('@tensorflow/tfjs');\r\nrequire('@tensorflow/tfjs-node-gpu');\r\n\r\nvar express = require('express');\r\nvar router = express.Router();\r\nvar multer = require('multer');\r\nvar upload = multer({dest: \"./uploads/\"}).single('thumbnail');\r\nvar Canvas = require('canvas');\r\nvar Image = Canvas.Image;\r\n\r\nconst state = {\r\n  input: {\r\n    imageScaleFactor: 0.5,\r\n    flipHorizontal: false,\r\n    outputStride: 16,\r\n  },\r\n  detection: {\r\n    maxPoseDetections: 5,\r\n    minPoseConfidence: 0.15,\r\n    minPartConfidence: 0.1,\r\n    nmsRadius: 30.0,\r\n  },\r\n};\r\n\r\nfunction createHTMLCanvasElementFromRequest(req){\r\n    const img = new Image;\r\n    img.src = req.file.path;\r\n    const canvas = new Canvas(img.width,img.height); \r\n    const ctx = canvas.getContext('2d');\r\n    ctx.drawImage(img, 0, 0, img.width, img.height);\r\n    return canvas;\r\n}\r\n\r\nfunction detectPoses(req, res){\r\n  const canvas = createHTMLCanvasElementFromRequest(req);\r\n  async function usePoseNet(req){\r\n    const net = await posenet.load();\r\n    const poses = await net.estimateMultiplePoses(  canvas, \r\n                                                  state.input.imageScaleFactor, \r\n                                                  state.input.flipHorizontal, \r\n                                                  state.input.outputStride, \r\n                                                  state.detection.maxPoseDetections,\r\n                                                  state.detection.minPartConfidence, \r\n                                                  state.detection.nmsRadius);\r\n    return poses;\r\n  }\r\n  usePoseNet(req).then( json_res =>{\r\n    res.send({\r\n        \"OriginalName\": req.file.originalname,\r\n        \"FileName\": req.file.filename,\r\n        \"Size\": req.file.size,\r\n        \"Width\": canvas.width,\r\n        \"Height\": canvas.height,\r\n        \"Context\": json_res\r\n    })\r\n  }).catch(e => {\r\n    res.send({\r\n        \"Error\": e\r\n    });\r\n  });\r\n}\r\n\r\nfunction failedToGetImages(req,res, err){\r\n  res.send({\r\n        \"Destination\": req.file.destination,\r\n        \"Error\": err\r\n      });\r\n}\r\n\r\nrouter.post('/upload', function(req, res){\r\n  upload(req, res, function(err){\r\n    res.header('Content-Type', 'application/json; charset=utf-8');\r\n    err? failedToGetImages(req,res,err) : detectPoses(req, res);\r\n  });\r\n});\r\n\r\nmodule.exports = router;\r\n\r\n```\r\n\r\n----\r\n\r\n./app.js\r\n\r\n```\r\n\r\nvar createError = require('http-errors');\r\nvar express = require('express');\r\nvar path = require('path');\r\nvar cookieParser = require('cookie-parser');\r\nvar logger = require('morgan');\r\n\r\nvar indexRouter = require('./routes/index');\r\n\r\nvar app = express();\r\n\r\napp.set('views', path.join(__dirname, 'views'));\r\napp.set('view engine', 'pug');\r\n\r\napp.use(logger('dev'));\r\napp.use(express.json());\r\napp.use(cookieParser());\r\napp.use(express.urlencoded({ extended: false }));\r\napp.use(express.static(path.join(__dirname, 'public')));\r\n\r\napp.use('/', indexRouter);\r\n\r\napp.use(function (req, res, next) {\r\n  res.header('Access-Control-Allow-Origin', req.headers.origin);\r\n  res.header('Access-Control-Allow-Headers', 'X-Requested-With, X-HTTP-Method-Override, Content-Type, Accept');\r\n  res.header('Access-Control-Allow-Methods', 'POST, GET, PUT, DELETE, OPTIONS');\r\n  res.header('Access-Control-Allow-Credentials', true);\r\n  res.header('Access-Control-Max-Age', '86400');\r\n  next();\r\n});\r\n\r\napp.options('*', function (req, res) {\r\n  res.sendStatus(200);\r\n});\r\n\r\n\r\n// catch 404 and forward to error handler\r\napp.use(function(req, res, next) {\r\n  next(createError(404));\r\n});\r\n\r\n// error handler\r\napp.use(function(err, req, res, next) {\r\n  // set locals, only providing error in development\r\n  res.locals.message = err.message;\r\n  res.locals.error = req.app.get('env') === 'development' ? err : {};\r\n\r\n  // render the error page\r\n  res.status(err.status || 500);\r\n  res.render('error');\r\n});\r\n\r\n\r\nmodule.exports = app;\r\n\r\n```\r\n\r\n---\r\n\r\n./bin/www\r\n\r\n```\r\n\r\n#!/usr/bin/env node\r\n\r\nvar app = require('../app');\r\nvar debug = require('debug')('roboserver:server');\r\nvar http = require('http');\r\n\r\nvar port = normalizePort(process.env.PORT || '3000');\r\napp.set('port', port);\r\n\r\nvar server = http.createServer(app);\r\n\r\nserver.listen(port);\r\nserver.on('error', onError);\r\nserver.on('listening', onListening);\r\n\r\nfunction normalizePort(val) {\r\n  var port = parseInt(val, 10);\r\n\r\n  if (isNaN(port)) {\r\n    // named pipe\r\n    return val;\r\n  }\r\n\r\n  if (port >= 0) {\r\n    // port number\r\n    return port;\r\n  }\r\n\r\n  return false;\r\n}\r\n\r\nfunction onError(error) {\r\n  if (error.syscall !== 'listen') {\r\n    throw error;\r\n  }\r\n\r\n  var bind = typeof port === 'string'\r\n    ? 'Pipe ' + port\r\n    : 'Port ' + port;\r\n\r\n  // handle specific listen errors with friendly messages\r\n  switch (error.code) {\r\n    case 'EACCES':\r\n      console.error(bind + ' requires elevated privileges');\r\n      process.exit(1);\r\n      break;\r\n    case 'EADDRINUSE':\r\n      console.error(bind + ' is already in use');\r\n      process.exit(1);\r\n      break;\r\n    default:\r\n      throw error;\r\n  }\r\n}\r\n\r\nfunction onListening() {\r\n  var addr = server.address();\r\n  var bind = typeof addr === 'string'\r\n    ? 'pipe ' + addr\r\n    : 'port ' + addr.port;\r\n  debug('Listening on ' + bind);\r\n}\r\n\r\n```\r\n\r\n-----\r\n\r\n### System information\r\n- OS Platform and Distribution:  ubuntu 16.04.4 LTS\r\n- TensorFlow installed from: https://github.com/tensorflow/tfjs\r\n- TensorFlow version: 0.12.0\r\n- Bazel version: N/A\r\n- CUDA/ cuDNN: 8.0/6.0\r\n- GPU model and memory: GeForce GTX 1070 Ti 8GB * 2\r\n- Memory:\r\nSMBIOS 3.0.0 present.\r\nHandle 0x0043, DMI type 16, 23 bytes\r\nPhysical Memory Array\r\n\tLocation: System Board Or Motherboard\r\n\tUse: System Memory\r\n\tError Correction Type: None\r\n\tMaximum Capacity: 32 GB\r\n\tError Information Handle: Not Provided\r\n\tNumber Of Devices: 2\r\n\r\nHandle 0x0044, DMI type 17, 40 bytes\r\nMemory Device\r\n\tArray Handle: 0x0043\r\n\tError Information Handle: Not Provided\r\n\tTotal Width: Unknown\r\n\tData Width: Unknown\r\n\tSize: No Module Installed\r\n\tForm Factor: Unknown\r\n\tSet: None\r\n\tLocator: ChannelA-DIMM1\r\n\tBank Locator: BANK 0\r\n\tType: Unknown\r\n\tType Detail: None\r\n\tSpeed: Unknown\r\n\tManufacturer: Not Specified\r\n\tSerial Number: Not Specified\r\n\tAsset Tag: Not Specified\r\n\tPart Number: Not Specified\r\n\tRank: Unknown\r\n\tConfigured Clock Speed: Unknown\r\n\tMinimum Voltage: Unknown\r\n\tMaximum Voltage: Unknown\r\n\tConfigured Voltage: Unknown\r\n\r\nHandle 0x0045, DMI type 17, 40 bytes\r\nMemory Device\r\n\tArray Handle: 0x0043\r\n\tError Information Handle: Not Provided\r\n\tTotal Width: 64 bits\r\n\tData Width: 64 bits\r\n\tSize: 16384 MB\r\n\tForm Factor: DIMM\r\n\tSet: None\r\n\tLocator: ChannelA-DIMM2\r\n\tBank Locator: BANK 1\r\n\tType: DDR4\r\n\tType Detail: Synchronous Unbuffered (Unregistered)\r\n\tSpeed: 2133 MHz\r\n\tManufacturer: Corsair\r\n\tSerial Number: 00000000\r\n\tAsset Tag: 9876543210\r\n\tPart Number: CMK32GX4M2B3000C15  \r\n\tRank: 2\r\n\tConfigured Clock Speed: 2133 MHz\r\n\tMinimum Voltage: Unknown\r\n\tMaximum Voltage: Unknown\r\n\tConfigured Voltage: 1.2 V\r\n\r\nHandle 0x0046, DMI type 17, 40 bytes\r\nMemory Device\r\n\tArray Handle: 0x0043\r\n\tError Information Handle: Not Provided\r\n\tTotal Width: Unknown\r\n\tData Width: Unknown\r\n\tSize: No Module Installed\r\n\tForm Factor: Unknown\r\n\tSet: None\r\n\tLocator: ChannelB-DIMM1\r\n\tBank Locator: BANK 2\r\n\tType: Unknown\r\n\tType Detail: None\r\n\tSpeed: Unknown\r\n\tManufacturer: Not Specified\r\n\tSerial Number: Not Specified\r\n\tAsset Tag: Not Specified\r\n\tPart Number: Not Specified\r\n\tRank: Unknown\r\n\tConfigured Clock Speed: Unknown\r\n\tMinimum Voltage: Unknown\r\n\tMaximum Voltage: Unknown\r\n\tConfigured Voltage: Unknown\r\n\r\nHandle 0x0047, DMI type 17, 40 bytes\r\nMemory Device\r\n\tArray Handle: 0x0043\r\n\tError Information Handle: Not Provided\r\n\tTotal Width: 64 bits\r\n\tData Width: 64 bits\r\n\tSize: 16384 MB\r\n\tForm Factor: DIMM\r\n\tSet: None\r\n\tLocator: ChannelB-DIMM2\r\n\tBank Locator: BANK 3\r\n\tType: DDR4\r\n\tType Detail: Synchronous Unbuffered (Unregistered)\r\n\tSpeed: 2133 MHz\r\n\tManufacturer: Corsair\r\n\tSerial Number: 00000000\r\n\tAsset Tag: 9876543210\r\n\tPart Number: CMK32GX4M2B3000C15  \r\n\tRank: 2\r\n\tConfigured Clock Speed: 2133 MHz\r\n\tMinimum Voltage: Unknown\r\n\tMaximum Voltage: Unknown\r\n\tConfigured Voltage: 1.2 V\r\n\r\n- Exact command to reproduce: node ./bin/www\r\n- Mobile device: N/A\r\n- Browser: Google Chrome \r\n- npm: 6.1.0\r\n- node: 10.6.0\r\n- @tensorflow-models/posenet: 0.2.2\r\n- @tensorflow/tfjs-node: 0.1.9\r\n- @tensorflow/tfjs-node-gpu: 0.1.9\r\n- gcc: 5.4.0", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I have uploaded the issue as you required.\r\nCould you give me the solution?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "OK! Thanks!"]}, {"number": 21238, "title": "DecodeBase64 is not supported for Android TensorFlowInferenceInterface", "body": "### System information\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.2\r\nprotobuf                           3.5.2.post1\r\ntensorflow                         1.8.0\r\ntensorflow-hub                     0.1.0\r\ntensorflow-model-analysis          0.6.0\r\ntensorflow-serving-api             1.8.0\r\ntensorflow-tensorboard             1.5.0\r\ntensorflow-transform               0.6.0\r\ntensorflowjs                       0.1.0\r\ntensorflowonspark                  1.0.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\nHave I written custom code:\r\nNo\r\n\r\nOS Platform and Distribution:\r\nMacOS\r\n\r\nTensorFlow installed from:\r\n`pip install tensorflow`\r\n\r\nTensorFlow version:\r\n1.8.0\r\n\r\nBazel version:\r\n0.5.4\r\n\r\nCUDA/cuDNN version\r\nN/A\r\n\r\nGPU model and memory:\r\nN/A\r\n\r\nExact command to reproduce:\r\nFollow the official and deploy the model with DecodeBase64 operator.\r\n\r\nMobile device:\r\nXiaomi MIX 2\r\n\r\n### Describe the problem\r\n\r\nWe have the problem to deploy the image model to Android platform. It throws `No OpKernel was registered to support Op 'DecodeBase64' with these attrs`. And we have tried feeding string or byte array like these.\r\n\r\n```\r\nimage_string = \"foo\"\r\n\r\nbyte[] image_bytes = image_string.getBytes();\r\n\r\nString inputName = \"model_input_b64_images\";\r\n \r\ninferenceInterface.feedString(inputName, image_bytes);\r\n\r\ninferenceInterface.feed(inputName, image_bytes, 1, 4860);\r\n```\r\n\r\nIs it possible to implement and registry the kernel for `DecodeBase64` which is really important for image models? Or we have another way to feed the data for this Operator.\r\n\r\n### Source code / logs\r\n\r\nIt is easy to re-produce the issue. Try exporting the model which has the `DecodeBase64` operator and use the `freeze_graph.py` to generated the Android model file.\r\n\r\nHere is the complete log when trying to run the inference in Android.\r\n\r\n```\r\n\r\n07-30 16:58:35.943 14616-14616/com.tobe.androidclient E/AndroidRuntime: FATAL EXCEPTION: main\r\nProcess: com.tobe.androidclient, PID: 14616\r\njava.lang.RuntimeException: Unable to start activity ComponentInfo{com.tobe.androidclient/com.tobe.androidclient.MainActivity}: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'DecodeBase64' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: DecodeBase64 = DecodeBase64[](model_input_b64_images)]]\r\n    at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2855)\r\n    at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2930)\r\n    at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n    at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1619)\r\n    at android.os.Handler.dispatchMessage(Handler.java:105)\r\n    at android.os.Looper.loop(Looper.java:171)\r\n    at android.app.ActivityThread.main(ActivityThread.java:6684)\r\n    at java.lang.reflect.Method.invoke(Native Method)\r\n    at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:246)\r\n    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:783)\r\n Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'DecodeBase64' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: DecodeBase64 = DecodeBase64[](model_input_b64_images)]]\r\n    at org.tensorflow.Session.run(Native Method)\r\n    at org.tensorflow.Session.access$100(Session.java:48)\r\n    at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n    at org.tensorflow.Session$Runner.runAndFetchMetadata(Session.java:260)\r\n    at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:220)\r\n    at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n    at com.tobe.androidclient.MainActivity.onCreate(MainActivity.java:98)\r\n    at android.app.Activity.performCreate(Activity.java:7057)\r\n    at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)\r\n    at android.app.ActivityThread.performLaun\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thanks for your reply. I have updated the basic information of this issue.", "@aselle Can you have a look at this?", "@tobegit3hub , tensorflow mobile (and tflite) have limited number of ops to keep the frameworks small. Android already has a base64 decode function in it. We recommend you take that op out of your model and handle that ias a preprocess before invoking inference.", "Thanks @aselle . It is reasonable and we can the base64 op from exported model.\r\n\r\nBut we want to train the model once and deploy to Serving/Android/iOS. It would be great if all the running devices can load the same model signature without any modification."]}]