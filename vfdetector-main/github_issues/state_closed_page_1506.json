[{"number": 7753, "title": "Udacity: Add latest Docker image based on TF 1.0.0", "body": "", "comments": []}, {"number": 7752, "title": "Fix compile warnings", "body": "Fixes:\r\n  - Warnings related to comparison of signed vs unsigned types.\r\n  - Initialized variables.", "comments": ["Can one of the admins verify this patch?", "@martinwicke looks like the jenkins checks are not being run. ", "Chad can you look at this? I think issues with it may be mostly cosmetic.", "jenkins, test this please.", "@martinwicke please review", "@cwhipkey Thanks for reviewing. Addressed review comments.", "jenkins, test this please", "@cwhipkey merge conflicts resolved", "Jenkins, test this please", "@andrewharp - can you merge this in?", "FYI this PR has a typo in it, which breaks XLA builds.  The typo is `size_typ` rather than `size_type`:\r\nhttps://github.com/tensorflow/tensorflow/blame/master/tensorflow/compiler/tf2xla/xla_compiler.cc#L228\r\n\r\nI think we need a better process for testing such changes.  I'm suspecting that \"jenkins, test this please\" isn't actually compiling / testing XLA?  @gunan might have an opinion here.", "Correct, XLA is not tested in standard PR tests, we should add that."]}, {"number": 7751, "title": "tf.contrib.crf doesn't support sequences of length 1", "body": "[`tf.contrib.crf`](https://www.tensorflow.org/versions/master/api_docs/python/contrib.crf/) doesn't support sequences of length 1.\r\n\r\nFor example, if I run the example on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf and replace `num_words = 20` by `num_words = 1`, I get the error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.UnimplementedError: TensorArray has size zero, but element shape <unknown> is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\r\n\t [[Node: gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[\"loc:@rnn/TensorArray_1\"], dtype=DT_FLOAT, element_shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3, rnn/TensorArrayUnstack/range, gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Francky\\Documents\\GitHub\\nlp\\neurodeid\\test\\CRF_v2.py\", line 47, in <module>\r\n    [unary_scores, transition_params, train_op])\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnimplementedError: TensorArray has size zero, but element shape <unknown> is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\r\n\t [[Node: gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[\"loc:@rnn/TensorArray_1\"], dtype=DT_FLOAT, element_shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3, rnn/TensorArrayUnstack/range, gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow)]]\r\n\r\nCaused by op 'gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3', defined at:\r\n  File \"C:\\Users\\Francky\\Documents\\GitHub\\nlp\\neurodeid\\test\\CRF_v2.py\", line 41, in <module>\r\n    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 288, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 354, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 482, in gradients\r\n    in_grads = grad_fn(op, *out_grads)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_grad.py\", line 186, in _TensorArrayScatterGrad\r\n    grad = g.gather(indices)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py\", line 348, in gather\r\n    element_shape=element_shape)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 2226, in _tensor_array_gather_v3\r\n    element_shape=element_shape, name=name)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n...which was originally created as op 'rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3', defined at:\r\n  File \"C:\\Users\\Francky\\Documents\\GitHub\\nlp\\neurodeid\\test\\CRF_v2.py\", line 37, in <module>\r\n    unary_scores, y_t, sequence_lengths_t)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\contrib\\crf\\python\\ops\\crf.py\", line 156, in crf_log_likelihood\r\n    log_norm = crf_log_norm(inputs, sequence_lengths, transition_params)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\contrib\\crf\\python\\ops\\crf.py\", line 123, in crf_log_norm\r\n    dtype=dtypes.float32)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 545, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 663, in _dynamic_rnn_loop\r\n    for ta, input_ in zip(input_ta, flat_input))\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 663, in <genexpr>\r\n    for ta, input_ in zip(input_ta, flat_input))\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py\", line 400, in unstack\r\n    indices=math_ops.range(0, num_elements), value=value, name=name)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py\", line 428, in scatter\r\n    name=name)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 2492, in _tensor_array_scatter_v3\r\n    name=name)\r\n  File \"C:\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n\r\nUnimplementedError (see above for traceback): TensorArray has size zero, but element shape <unknown> is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\r\n\t [[Node: gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[\"loc:@rnn/TensorArray_1\"], dtype=DT_FLOAT, element_shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3, rnn/TensorArrayUnstack/range, gradients/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow)]]\r\n```\r\n\r\nTested with TensorFlow 1.0.0 on Windows 7 SP1 x64 Ultimate and TensorFlow-GPU 1.0.0 on Ubuntu 14.04.4 LTS x64.", "comments": ["What happens with a size like 2 or 5?\r\n\r\nI think your problem might not be a general contrib.crf issue but the specific example, in particular this line:\r\n\r\n`sequence_lengths = np.full(num_examples, num_words - 1, dtype=np.int32)`\r\n", "`num_words = 2` or `num_words = 5` work. Good catch, I don't know why `num_words - 1` is used in the CRF example instead of `num_words` in the line of code you quote.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e121667dc609de978a223c56ee906368d2c4ceef/tensorflow/contrib/crf/python/ops/crf.py#L121 is already decrementing the `sequence_length` by 1:\r\n\r\n```\r\n # Compute the alpha values in the forward algorithm in order to get the\r\n  # partition function.\r\n  forward_cell = CrfForwardRnnCell(transition_params)\r\n  _, alphas = rnn.dynamic_rnn(\r\n      cell=forward_cell,\r\n      inputs=rest_of_input,\r\n      sequence_length=sequence_lengths - 1,\r\n      initial_state=first_input,\r\n      dtype=dtypes.float32)\r\n  log_norm = math_ops.reduce_logsumexp(alphas, [1])\r\n  return log_norm\r\n```\r\n\r\n\r\nHowever, changing `sequence_lengths = np.full(num_examples, num_words - 1, dtype=np.int32)` to `sequence_lengths = np.full(num_examples, num_words, dtype=np.int32)` does not solve the issue when `num_words = 1` .", "This appears to be an unmaintained contrib utility.  Would you like to propose a fix?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Sorry, I can't commit to proposing a fix in the near future.", "@aselle Do you need more information from me? (as far as I know the issue is still present, so I think it should be reopened)", "Corresponding Stack Exchange question: [How can I pass sequences of length 1 to tf.contrib.crf in TensorFlow?](http://stackoverflow.com/q/42798518/395857)", "@aselle if `contrib/crf` is not maintained, should it perhaps be deleted?", "Is there any plan for supporting CRF or CRF-like code, if the one in contrib is currently unsupported?", "I was planning to use this for Named Entity Recognition. I think `contrib/crf` will help a lot other people who are trying to do the same. If given a little guidance I can contribute as well.", "@anikdas I used the CRF layer to perform named-entity recognition in https://github.com/Franck-Dernoncourt/NeuroNER if interested. Since in named-entity recognition it's often preferable to add start and end tokens around each sequence, it avoids having sequences of length 1.", "why is tf.contrib.crf discontinued, what is the alternative then? this is such a useful lib"]}, {"number": 7750, "title": "Making the URLs in the documentation clickable", "body": "It would be nice if the URLs in the documentation were clickable.\r\n\r\n[Example](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/conectionist_temporal_classification__ctc_):\r\n\r\n\r\n![image](https://cloud.githubusercontent.com/assets/15331/23184189/d337d5b0-f84c-11e6-9c7d-821563b30356.png)\r\n\r\n", "comments": ["We've identified what's causing this and will figure out a fix.", "@dr4b Can you share what the cause of this issue is?  I may throw up a quick PR if it's a relatively straightforward problem. ", "The Markdown interpreter we are now using on tensorflow.org requires URLs to be enclosed in <...>.  I think we were hoping to come up with some logic that could identify URLs and rewrite them as part of our website publishing process -- see, e.g.,  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docs/parser.py", "Is there anyone who is still working on this, or can we just do a quick fix?", "Any quick fix for these is welcome!\r\n\r\nWe're planning on some updates to the docs' markdown parser that will catch these, but that's farther out.", "Looks like https://github.com/tensorflow/tensorflow/pull/9631 fixed this."]}, {"number": 7749, "title": "Remove sigmoid by calling sigmoid before split", "body": " Remove one sigmoid function, by calling sigmoid before split", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Jenkins, test this please.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 7748, "title": "MKL support for the transpose op", "body": "2D transpose support for float.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Why aren't the tests progressing?", "@tensorflow-jenkins test this please", "I see a strange timeout error in the Linux CPU Tests console output:\r\n\r\nINFO: Timeout connecting to https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar\r\nBuild was aborted\r\nAborted by unknown\r\nUnable to get pull request builder trigger!!\r\nSetting status of 06c0c6d0c61321de291d0fd96971cc300748579d to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/3537/ and message: 'FAILURE\r\n '\r\nUsing context: Linux CPU Tests (Python 3)\r\nFinished: ABORTED\r\n\r\nEDIT: This seems to be the issue for all failures! Please can someone take a look?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Does it only listen to you? :)", "I\u2019m not sure what is going on with the transpose op tests \u2013 I looked at the build log but didn\u2019t see any real errors. I was wondering if someone could help look at what is wrong?\r\n\r\nThere is a buildifier error in the sanity check which I will fix, but for the linux CPU tests for instance, the log (https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/3899/console) suddenly ends in error with no error message.\r\n\r\nThanks for your help.\r\n", "I was just told that the rest of the tests will stop if the sanity check fails, which failed because of this following buildifier error (see below) So if you fix that, things should progress...\r\n\r\n=== Sanity check step 3 of 7: do_buildifier (buildifier check) ===\r\n\r\nRunning do_buildifier on 179 files\r\n\r\ntensorflow/core/kernels/BUILD # reformat \r\n\r\nbuildifier took 1 s\r\n\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n694,698c694,697\r\n<         \"transpose_op.cc\"\r\n<         ]\r\n<         + if_mkl([\r\n<         \"mkl_transpose_op.cc\"\r\n<         ]),\r\n---\r\n>         \"transpose_op.cc\",\r\n>     ] + if_mkl([\r\n>         \"mkl_transpose_op.cc\",\r\n>     ]),\r\n700,703c699,701\r\n<     deps = ARRAY_DEPS\r\n<         + if_mkl([\r\n<                 \"//third_party/mkl:intel_binary_blob\",\r\n<         ]),\r\n---\r\n>     deps = ARRAY_DEPS + if_mkl([\r\n>         \"//third_party/mkl:intel_binary_blob\",\r\n>     ]),\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n", "Fixed with buildifier. Looks like the tests have restarted automatically.", "Looks like the tests need to be restarted. @andydavis1 please could you kick them off?", "@tensorflow-jenkins test this please", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "The errors here had no relation to my changes, so I rebased to the latest TensorFlow master. Not sure what the deal is with the CLA comment by @googlebot, but please can you restart the tests?", "@tensorflow-jenkins test this please", "Restart tests? ", "@tensorflow-jenkins test this please", "Looks like we're done here :)", "@vivek-rane GitHub is showing a large diff from master and this change seems to duplicate a lot of existing commits there. I couldn't figure out why it's doing that. Mind trying to fix it?\r\n\r\nProbably the simplest way is to copy the diff out, remake the branch with a fresh checkout from master, and `git push -f` to the branch to update this pull request.", "CLAs look good, thanks!\n\n<!-- ok -->", "@jhseu I don't see the issue you mentioned on the diff page (https://github.com/tensorflow/tensorflow/pull/7748/files) - am I looking at the wrong place?", "Looks like your recent master merge fixed it for me too... weird. Looks good now.\r\n\r\nJenkins, test this please.", "Error in sanity checks - any idea what happened? Testing was interrupted for some reason (I think)\r\n\r\nINFO: Loading package: @bazel_tools//tools/zip\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@org_polymer_paper_spinner' (requested by nodes 'REPOSITORY:@org_polymer_paper_spinner')\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:429)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: Invalid EvalException:\r\njava.lang.InterruptedException", "I'm guessing it's a transient error. Trying again:\r\nJenkins, test this please", "Sanity checks still show the same error, but I dug into the ci.tensorflow.org console and it shows this - maybe there's something acting up with Jenkins?\r\n\r\nBuilding image tf-make-base...\r\nError checking context: 'can't stat '/var/lib/jenkins/workspace/tensorflow-pull-requests-makefile/tensorflow/contrib/makefile/downloads/gemmlowp/test''.\r\nERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-makefile/tensorflow/contrib/makefile/../../../tensorflow/contrib/makefile/Dockerfile", "Jenkins, test this please", "Same errors (interrupted exception) ", "That's pretty unusual...\r\n\r\nJenkins, test this please", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 7747, "title": "Why restore checkpoint will effect the tfrecord reader path? ", "body": "I'm training my model with training dataset in standard format\r\n```\r\nfeatures = tf.parse_example(\r\n    batch_serialized_example,\r\n    features={\r\n        \"label\": tf.FixedLenFeature([], tf.float32),\r\n        \"ids\": tf.VarLenFeature(tf.int64),\r\n        \"values\": tf.VarLenFeature(tf.float32),\r\n    })\r\nbatch_labels = features[\"label\"]\r\nbatch_ids = features[\"ids\"]\r\nbatch_values = features[\"values\"]\r\n```\r\nthen I  save the checkpoint and want to do some predicting on my other dataset, **but add two more fields: uid and item_id (just for statistics)**\r\n\r\n```\r\npredict_features = tf.parse_example(\r\n    predict_batch_serialized_example,\r\n    features={\r\n        \"label\": tf.FixedLenFeature([], tf.float32),\r\n        \"ids\": tf.VarLenFeature(tf.int64),\r\n        \"values\": tf.VarLenFeature(tf.float32),\r\n        \"uid\": tf.FixedLenFeature([], tf.int64),\r\n        \"item_id\": tf.FixedLenFeature([], tf.int64)\r\n    })\r\npredict_batch_labels = predict_features[\"label\"]\r\npredict_batch_ids = predict_features[\"ids\"]\r\npredict_batch_values = predict_features[\"values\"]\r\npredict_batch_uids = predict_features[\"uid\"]\r\npredict_batch_item_ids = predict_features[\"item_id\"]\r\n\r\n```\r\n\r\nWhen I restore the checkpoint , the simply read-data  faild `InvalidArgumentError (see above for traceback): Name: <unknown>, Feature: item_id (data type: int64) is required but could not be found.\r\n`\r\nWhen I remove the restore function , it's seems right.\r\n\r\nWhy the checkpoint do effect the reader? it's seems that after restore the checkponit ,the reader no longer read from the new path ?\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nhttps://gist.github.com/ericyue/e694a90338b9fadf9996025719005c9d\r\n\r\nmininal code (just read data)\r\n\r\n```\r\n      if not restore_session_from_checkpoint(sess, saver,cf):\r\n        logging.error(\"No checkpoint found, exit now\")\r\n        exit(1)\r\n      try:\r\n          while not coord.should_stop():\r\n            uids, item_ids = sess.run( [predict_batch_uids, predict_batch_item_ids])\r\n      except tf.errors.OutOfRangeError:\r\n          logging.info(\"Done predicting after reading all data\")\r\n      finally:\r\n          coord.request_stop()\r\n          logging.info(\"coord stopped\")\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\n  1 Caused by op u'ParseExample_2/ParseExample', defined at:\r\n  2   File \"deepcake-tf.py\", line 139, in <module>\r\n  3     \"item_id\": tf.FixedLenFeature([], tf.int64)\r\n  4   File \"/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parsing_ops.py\", line 445, in parse_example\r\n  5     dense_types, dense_defaults, dense_shapes, name)\r\n  6   File \"/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parsing_ops.py\", line 544, in _parse_example_raw\r\n  7     name=name)\r\n  8   File \"/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 167, in _parse_example\r\n  9     dense_shapes=dense_shapes, name=name)\r\n 10   File \"/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n 11     op_def=op_def)\r\n 12   File \"/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n 13     original_op=self._default_original_op, op_def=op_def)\r\n 14   File \"/home/moon/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n 15     self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Name: <unknown>, Feature: item_id (data type: int64) is required but could not be found.\r\n\t [[Node: ParseExample_2/ParseExample = ParseExample[Ndense=3, Nsparse=2, Tdense=[DT_INT64, DT_FLOAT, DT_INT64], dense_shapes=[[], [], []], sparse_types=[DT_INT64, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch_2, ParseExample_2/ParseExample/names, ParseExample_2/ParseExample/sparse_keys_0, ParseExample_2/ParseExample/sparse_keys_1, ParseExample_2/ParseExample/dense_keys_0, ParseExample_2/ParseExample/dense_keys_1, ParseExample_2/ParseExample/dense_keys_2, ParseExample_2/Const, ParseExample_2/Const_1, ParseExample_2/Const_2)]]\r\n```", "comments": ["Hi, how did you fix it?"]}, {"number": 7746, "title": "Feature request: Unique tensorboard URLs for different frontend states", "body": "It would be nice if eg clicking on the \"GRAPHS\" tab in tensorboard would rewrite the URL to something like \"localhost:6006/graphs\". And likewise, navigating to \"localhost:6006/graphs\" would immediately load the GRAPHS tab. In general, encoding the tensorboard state in the URL would be useful.\r\n\r\nThat way, you could refresh the page and go back to the tab you were on, share your tensorboard state just by copying the URL, programmatically load a specific tensorboard state from visualization routines in your client language, etc.\r\n\r\n", "comments": ["This should be done now, at the highest level (e.g. /graphs, /scalars). Please open an issue on tensorflow/tensorboard if you need more fine-grained state in the url bar."]}, {"number": 7745, "title": "Update release note", "body": "For #7664 ", "comments": []}, {"number": 7744, "title": "tensorflow sequence to sequence models", "body": "Hello everyone,\r\nI executed the code translate.py from sequence to sequence models tutorial of tensorflow for 20 hours and did not get output.\r\nCan anyone tell me how much time will it take to display the output\r\nSystem specifications:\r\nI installed Ubuntu 16.04 on VMware with a disk space of 100GB\r\n![seqgoo](https://cloud.githubusercontent.com/assets/20130992/23177248/2078472e-f834-11e6-80ec-c264984d51a7.png)\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 7743, "title": "[feature request] tf.matrix_triangular_solve misses unit_diagonal= argument", "body": "Hello,\r\n\r\nIt would be useful to allow ```tf.matrix_triangular_solve``` have ```unit_diagonal=``` kw-argument and treat ```matrix``` argument as having unit diagonal.\r\nSure, the same may be obtained by composition ```tf.matrix_set_diag``` and ```tf.matrix_triangular_solve```. However, additional assignment here seems to be inefficient. Solving triangular matrix involves quite simple formulae where diagonal elements of ```matrix``` are presented explicitly.\r\n", "comments": ["Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7742, "title": "Improve retrained inception v3 in Android demo", "body": "I have retrained the Inception-V3 final layer with my own 20 categories. When I am using retrained model in android demo app it takes 6 to 8 seconds to predict.\r\n\r\nRunning (using `armeabi-v7a`) on \r\n\r\n- LG G4 Stylus -> 6-8 sec\r\n- S6, -> 3-4.5 sec\r\n\r\nI have done `optimize_for_inference` it takes 6-9 sec and `quantize_graph` it takes 7-11 sec. Is there any way to improve it?\r\n\r\n[Output](https://i.stack.imgur.com/lKryY.png) on LG G4 Stylus.\r\n\r\n", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag.", "@poxvoculi I have asked on [Stackoverflow](http://stackoverflow.com/questions/42367281/tensorflow-android-retrained-inception-v3-take-too-much-time) but got no help.", "@codeloverr Hi, could you please share the source code of your android app? I'm trying to do the same thing but I'm getting a lot of errors during build. Thanks", "@AtomicSpider  sure, I'll make the repo and share with you. "]}, {"number": 7741, "title": "The function 'layers/convolutional.py` does not follow `tf.GraphKeys` convention", "body": "In [convolutional.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py) bias and weight variables are created and named `kernel` and `bias`. However according to the conventions defined in [tf.GraphKeys.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.GraphKeys.md) those variables should be named  `weights` and `biases` and assigned the corresponding collections. There is some (third party) functionality which utilizes this convention, including `tf.contrib.layers.summarize_weights`. \r\n\r\nIn addition, the names to be used are also stored in the constants `tf.GraphKeys.WEIGHTS` and `tf.GraphKeys.BIASES`. So instead of writing the strings hard in the code, I would suggest utilizing those constants. Do you agree with that? I am happy to submit a pull request solving this issue, if desired. \r\n\r\n\r\n", "comments": ["@fchollet: Is a PR welcome?", "No, the proper convention is `kernel` and `bias`, which is used throughout core layers. This has been extensively discussed internally.", "ok, you are right. From what I understand by now the [tf.GraphKeys.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.GraphKeys.md) conventions are not about the names of Variabels but collections those Variabels should be added. \r\n\r\nSo  the following lines should be in there:\r\n\r\n````\r\n    tf.add_to_collection(tf.GraphKeys.WEIGHTS, self.kernel)\r\n    tf.add_to_collection(tf.GraphKeys.BIASES,  self.bias)\r\n````\r\n\r\nI am happy to put this into a pull request.", "These keys are almost never used.\r\nThe only `layer` that uses them is:  \"contrib.layers.legacy_fully_connected\".\r\n\r\n(Then a few things in `contrib.layers`, book-keeping ops and `apply_regularization`, read them. They're also mentioned in the contrib/layers/README.md)\r\n\r\nThere is no way to automatically fill these collections, like is done for `GraphKeys.TRAINABLE_VARIABLES`.\r\n\r\nFor it to actually work, and make `tf.contrib.layers.apply_regularization` work as you would expect, it would need to be set *everywhere*, not just in convolutional.\r\n\r\nSo I don't think just fixing `convolutional.py` is the right choice.\r\n\r\nCurrently the core `tf.layers` is not using them so that may be a clue that they are not part of the future."]}, {"number": 7740, "title": "GradientDescentOptimizer got wrong result", "body": "**I want use gradient descent to solve equation set, but I got wrong result everytime, so I check my code and written a numpy edition, in this edition I provide explicit loss gradient and I can get currect result.**\r\n\r\nSo I don't understand why GradientDescentOptimizer can not work.\r\n\r\n**here is my code without tf:**\r\n\r\n```python\r\nimport numpy as np\r\n\r\n\r\nclass SolveEquation:\r\n    def __init__(self, rate: float, loss_threshold: float=0.0001, max_epochs: int=1000):\r\n        self.__rate = rate\r\n        self.__loss_threshold = loss_threshold\r\n        self.__max_epochs = max_epochs\r\n        self.__x = None\r\n\r\n    def solve(self, coefficients, b):\r\n        _a = np.array(coefficients)\r\n        _b = np.array(b).reshape([len(b), 1])\r\n        _x = np.zeros([_a.shape[1], 1])\r\n        for epoch in range(self.__max_epochs):\r\n            grad_loss = np.matmul(np.transpose(_a), np.matmul(_a, _x) - _b)\r\n            _x -= self.__rate * grad_loss\r\n            if epoch % 10 == 0:\r\n                loss = np.mean(np.square(np.subtract(np.matmul(_a, _x), _b)))\r\n                print('loss = {:.8f}'.format(loss))\r\n                if loss < self.__loss_threshold:\r\n                    break\r\n        return _x\r\n\r\ns = SolveEquation(0.1, max_epochs=1)\r\nprint(s.solve([[1, 2], [1, 3]], [3, 4]))\r\n```\r\n\r\n**And here is my code with tf:**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass TFSolveEquation:\r\n    def __init__(self, rate: float, loss_threshold: float=0.0001, max_epochs: int=1000):\r\n        self.__rate = rate\r\n        self.__loss_threshold = tf.constant(loss_threshold)\r\n        self.__max_epochs = max_epochs\r\n        self.__session = tf.Session()\r\n        self.__x = None\r\n\r\n    def __del__(self):\r\n        try:\r\n            self.__session.close()\r\n        finally:\r\n            pass\r\n\r\n    def solve(self, coefficients, b):\r\n        coefficients_data = np.array(coefficients)\r\n        b_data = np.array(b)\r\n        _a = tf.placeholder(tf.float32)\r\n        _b = tf.placeholder(tf.float32)\r\n        _x = tf.Variable(tf.zeros([coefficients_data.shape[1], 1]))\r\n        loss = tf.reduce_mean(tf.square(tf.matmul(_a, _x) - _b))\r\n        optimizer = tf.train.GradientDescentOptimizer(self.__rate)\r\n        model = optimizer.minimize(loss)\r\n        self.__session.run(tf.global_variables_initializer())\r\n        for epoch in range(self.__max_epochs):\r\n            self.__session.run(model, {_a: coefficients_data, _b: b_data})\r\n            if epoch % 10 == 0:\r\n                if self.__session.run(loss < self.__loss_threshold, {_a: coefficients_data, _b: b_data}):\r\n                    break\r\n        return self.__session.run(_x)\r\n\r\ns = TFSolveEquation(0.1, max_epochs=1)\r\nprint(s.solve([[1, 2], [1, 3]], [3, 4]))\r\n```\r\n\r\nI test these 2 codes with very simple equation set:\r\n\r\nx_1 + 2 * x_2 = 3\r\nx_1 + 3 * x_3 = 4\r\n\r\nloss = 1/2 * || Ax - b ||^2\r\n\r\nInit x_1 = 0, x_2 = 0, rate = 0.1\r\n\r\n**Use gradient descent**\r\n**So at 1st compute, the delta x = (0.7, 1.8)**\r\n\r\nBut unfortunately my code with tf give the \r\ndelta x = \r\n[[ 0.69999999]\r\n [ 1.75      ]]\r\n\r\nAnd my code without tf give the\r\ndelta x = \r\n[[ 0.7]\r\n [ 1.8]]\r\n\r\n**Absolutely code without tf is right, but why tf comput gradient may less 0.05 then currect result?**\r\n**I think this is the reason my code without tf can solve the equation set, but tf edition can not solve equation set currently.**\r\n\r\n**Can someone tell me why tf give a incurrent gradiant? Thanks**\r\n\r\nMy platform is Win10 + tensorflow-gpu v1.0\r\n", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag.", "@poxvoculi http://stackoverflow.com/questions/42468292/gradientdescentoptimizer-got-wrong-result"]}, {"number": 7739, "title": "tensorflow/example/mutilbox_detector bazel build error", "body": "I think there are lots of out of date docs in this repo, for example:\r\n```\r\ntensorflow/example/mutilbox_detector\r\n```\r\nthis example show build mutilbox_detector using bazel and generate an binary executable file, however I just got bunch of errors:\r\n```\r\nWARNING: Config values are not defined in any .rc file: opt\r\nERROR: /Users/jintian/tensorflow/tensorflow/core/BUILD:1279:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/jintian/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/jintian/tensorflow/tensorflow/core/BUILD:1279:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/jintian/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/jintian/tensorflow/tensorflow/core/BUILD:1279:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/jintian/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/examples/multibox_detector:detect_objects' failed; build aborted.\r\nINFO: Elapsed time: 26.050s\r\n```\r\nSuggest develop team upgrade useless docs and fix those tutorial that makes us confuse and even lead us to the wrong road.....", "comments": ["You may follow the installation instructions on the official site, perhaps.", "Yes, you'll have to run ./configure before building anything."]}, {"number": 7738, "title": "slim doesn't play well with the new tf.layers v 1.0", "body": "We use tf.contrib.slim to build NN architecture. After upgrade to tensorflow 1.0 the are some trouble when building models with slim. For example `slim.arg_scope` doesn't work with the tf.layers components. And when we keep using slim.* layers, we got a runtime error:\r\n\r\n~~~\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Session was not created with a graph before Run()!\r\n~~~\r\n\r\nIf we use all the layers from tf.layers this runtime error goes away.\r\n\r\nAny ideas ?\r\n", "comments": ["`slim` is kind of out of maintenance in my opinion. The better option is to migrate to new `tf.layers` or use another higher level abstraction based on TensorFlow.", "You can still use all layers and slim.arg_scope with tf.contrib.layers but not with tf.layers. Slim layers, losses, and metrics have been merged into tf.contrib.layers, tf.contrib.losses, tf.contrib.metrics.", "What Sergio said."]}, {"number": 7737, "title": "AttributeError: module 'tensorflow' has no attribute 'merge_all_summaries'", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? No\r\n\r\n### Environment info\r\nOperating System: Win10\r\n\r\nInstalled version of CUDA and cuDNN: No \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide: yes\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\nTraceback (most recent call last):\r\n  File \"G:/codes/tensorflow2/Autopilot-TensorFlow/train.py\", line 22, in <module>\r\n    merged_summary_op = tf.merge_all_summaries()\r\nAttributeError: module 'tensorflow' has no attribute 'tf.merge_all_summaries'\r\n\r\n```\r\n", "comments": ["It is now called ```tf.summary.merge_all```", "Did you solved this problem?", "AttributeError: module 'tensorflow' has no attribute 'merge_summary'\r\n\r\nfacing this error\r\ncant find a soln still", "For tensorflow 2.0, try using this \r\ntf.compat.v1.summary.merge_all()"]}, {"number": 7736, "title": "Convert some tf2xla ops to use elementwise tensor XLA ops, not Map", "body": "Some binary operations were implemented as maps of a scalar computation over the input tensors.  Since the computations consist of operations that can be element-wise operations, they could also be implemented in this manner.\r\n\r\nIn the case of the CPU, it is highly likely that it is improved, as the element-wise tensor operations are highly optimized. I do not know about the GPU operations, but I suspect that they will also be optimized in the element-wise tensor form.\r\n\r\nThis change re-implements all of the ops in this class.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "cheers."]}, {"number": 7735, "title": "API Documentation with Links to Source Code", "body": "I think it would be a really great improvement if the API documentation https://www.tensorflow.org/api_docs/ would contain links to the corresponding source code file on GitHub, similar to python package documentations on readthedocs.org, e.g. https://lasagne.readthedocs.io/en/latest/modules/layers/base.html.\r\n\r\nThis would also make it much faster to create pull requests that fix errors in the documentation.", "comments": ["This is actually happening in the new docs. All the way on the bottom, e.g.: https://www.tensorflow.org/api_docs/python/tf/DType\r\n\r\nDoesn't work for all things, but close.", "Oh wow, sorry for that! I guess I was looking for it in the wrong place...\r\n\r\nThat's really nice, but sometimes it's not clickable, e.g.: https://www.tensorflow.org/api_docs/python/tf/minimum", "We're planning to move up the include line to the top of the file soon so it's more visible.\r\n\r\nNot sure what to do about the generated ops though.", "We have a plan for the generated ops, which involves rejigging our build to\nplace generated files into a predictable location. But it's a little bit\naway.\n"]}, {"number": 7734, "title": "Error setting read_batch_size in tf.contrib.learn.read_batch_examples?", "body": "I am using the API _tf.contrib.learn.read_batch_examples()_. When I set the param _read_batch_size_, it always lead to the error \"**ValueError: All shapes must be fully defined: [TensorShape([]), TensorShape([Dimension(None)])]**\".  Use the default value 1 of _read_batch_size_ is OK. I wonder what's happening behind?\r\n\r\nThe full error stack is below:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"wide_n_deep_new.py\", line 258, in <module>\r\n    tf.app.run()\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"wide_n_deep_new.py\", line 249, in main\r\n    train(estimator)\r\n  File \"wide_n_deep_new.py\", line 200, in train\r\n    monitors=[eval_monitor])\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 765, in fit\r\n    max_steps=max_steps)\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 280, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1302, in fit\r\n    loss = self._train_model_v2(input_fn=input_fn, hooks=hooks)\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1312, in _train_model_v2\r\n    features, labels = input_fn()\r\n  File \"wide_n_deep_new.py\", line 198, in <lambda>\r\n    num_epochs=None), \r\n  File \"wide_n_deep_new.py\", line 156, in input_fn\r\n    examples_dict = read_csv_examples(mode, file_names, batch_size, num_epochs)\r\n  File \"wide_n_deep_new.py\", line 129, in read_csv_examples\r\n    name='read_batch_examples_{}'.format(mode))\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py\", line 101, in read_batch_examples\r\n    name=name)\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py\", line 166, in read_keyed_batch_examples\r\n    name=name)\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py\", line 399, in _read_keyed_batch_examples_helper\r\n    allow_smaller_final_batch=allow_smaller_final_batch)\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 1017, in batch_join\r\n    name=name)\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 693, in _batch_join\r\n    capacity=capacity, dtypes=types, shapes=shapes, shared_name=shared_name)\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 666, in __init__\r\n    shapes = _as_shape_list(shapes, dtypes)\r\n  File \"/opt/rh/python27/root/usr/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 75, in _as_shape_list\r\n    raise ValueError(\"All shapes must be fully defined: %s\" % shapes)\r\nValueError: All shapes must be fully defined: [TensorShape([]), TensorShape([Dimension(None)])]\r\n```", "comments": ["It might help if you could post a small but complete reproducible example.", "I am just running the code in _wide_n_deep_tutorial.py_ with some modification to read my own **csv** file. The modified code is as below:\r\n```\r\ndef input_fn(file_names, batch_size):\r\n    # Read csv files and create examples dict\r\n    examples_dict = read_csv_examples(file_names, batch_size)\r\n\r\n    # Continuous features\r\n    feature_cols = {k: tf.string_to_number(examples_dict[k],\r\n                                           out_type=tf.float32) for k in CONTINUOUS_COLUMNS}\r\n\r\n    # Categorical features\r\n    feature_cols.update({\r\n                            k: tf.SparseTensor(\r\n                                indices=[[i, 0] for i in range(examples_dict[k].get_shape()[0])],\r\n                                values=examples_dict[k],\r\n                                shape=[int(examples_dict[k].get_shape()[0]), 1])\r\n                            for k in CATEGORICAL_COLUMNS})\r\n\r\n    label = tf.string_to_number(examples_dict[LABEL_COLUMN], out_type=tf.int32)\r\n\r\n    return feature_cols, label\r\n\r\n\r\ndef read_csv_examples(file_names, batch_size):\r\n    def parse_fn(record):\r\n        record_defaults = [tf.constant([''], dtype=tf.string)] * len(COLUMNS)\r\n\r\n        return tf.decode_csv(record, record_defaults)\r\n\r\n    examples_op = tf.contrib.learn.read_batch_examples(\r\n        file_names,\r\n        batch_size=batch_size,\r\n        queue_capacity=batch_size*2.5,\r\n        reader=tf.TextLineReader,\r\n        parse_fn=parse_fn,\r\n        #read_batch_size= batch_size,\r\n        #randomize_input=True,\r\n        num_threads=8\r\n    )\r\n\r\n    # Important: convert examples to dict for ease of use in `input_fn`\r\n    # Map each header to its respective column (COLUMNS order\r\n    # matters!\r\n    examples_dict_op = {}\r\n    for i, header in enumerate(COLUMNS):\r\n        examples_dict_op[header] = examples_op[:, i]\r\n\r\n    return examples_dict_op\r\n```\r\nIf I set the param _read_batch_size_, error occurs.", "This forum is for bug reports, not usage questions.  Usage questions are best asked on stack overflow. \r\n\r\nIf you suspect this is an actual bug, please post a small but complete example that reproduces the error, which would include a minimal complete program, any input data used, and the invocation command. \r\n\r\nEven better in a case like this would be to do a little analysis yourself first, to try to identify where the problem lies.\r\n\r\nFor example, does your 'batch_size' evenly divide the number of examples in your csv file?  Does the error arise on the very first call into _batch_join or not?  etc.", "Closing due to lack of activity.  Please reopen if necessary.", "Same error here... No idea what happens. Even with the default value 1, still has error StringToNumberOp could not correctly convert string:"]}, {"number": 7733, "title": "addcsvop", "body": "add two new op decode_csv_selected_columns_op and decode_csv_first_n_op that modified from decode_csv_op.   decode_csv_selected_columns_op allows designatings which columns to parse. decode_csv_first_n_op only parses the first n columns.\r\n\r\n[original feature request](https://github.com/tensorflow/tensorflow/issues/7515#issuecomment-281302051)", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I signed it! ", "CLAs look good, thanks!\n\n<!-- ok -->", "@cherishlc can you add these ops to contrib first? We're careful about adding new ops, especially to the python API. You can model the build etc. after the contrib/image module.", "@martinwicke I have no time to do this recently, but I'd like to contribute a table pre-processing module one or two month later.", "Ok. I will close this PR. Please make a new one when you have time. Thanks!"]}, {"number": 7732, "title": "Typo correction in resize_bicubic_op.cc", "body": "I've signed the CLA.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 7731, "title": "Provide a way to clear variable and name scope", "body": "This is a follow-up of https://github.com/tensorflow/tensorflow/issues/6189\r\n\r\nIt would be nice to be able to create a variable or name scope that does not obey all currently open scopes.\r\n\r\nAs an example:\r\n```python\r\nwith tf.variable_scope('foo'):\r\n    with tf.variable_scope('bar'):\r\n        x = tf.constant(1.0, name='x')     # foo/bar/x\r\n\r\nwith tf.variable_scope('foo'):\r\n    with tf.variable_scope('bar', fresh=True):\r\n        x = tf.constant(1.0, name='x')    # bar/x\r\n```\r\n\r\nThis would be useful for using scopes with object oriented TensorFlow graphs.\r\n\r\nConsider a slightly modified version of the example from #6189 . Assume each method pushes a scope for the instance (with the class name) and for the method.\r\n```python\r\nclass Foo(object):\r\n    def __init__(self):\r\n        self.x = tf.get_variable('x', [], initializer=tf.constant(0.0))\r\n\r\n    def meth(self, foo):\r\n        z = tf.multiply(foo, 3.14, name='z')\r\n        return self.x.assign(z)\r\n\r\n    def boom(self, foo):\r\n        return tf.multiple(2.0, self.meth(foo), name='baz')\r\n\r\nf0 = Foo()      # Foo/__init__/x\r\nf1 = Foo()      # Foo_1/__init__/x\r\nf0.meth(1.0)    # Foo/meth/*\r\nf0.boom(2.0)  # Foo/boom/baz, Foo/boom/Foo/meth/z\r\n```\r\n\r\nI'd like the variables in ``boom``'s call to ``meth`` to be prefixed with only ``Foo/meth`` rather than ``Foo/boom/Foo/meth``. This requires either releasing the variable scope within boom before calling out (hard) or providing a way for ``meth`` to clear the existing variable scope. ", "comments": ["@ebrevdo Any opinion as to whether this is desirable feature request?", "I am interested in the answer to this same question. Thanks.", "+1 Need this feature. ", "+1 Need this as well.", "You can access and pass around a scope object:\r\n\r\nwith variable_scope(\"a\") as vs:\r\n  with variable_scope(\"b\"):\r\n    with variable_scope(vs):\r\n      # back in \"a\"", "Why is this not already a top priority?", "+1 Need this feature. ", "Yeah, build that ~wall~ feature!", "+1 would like this feature. \r\n\r\nI'm trying to convert some of our legacy weights to work with our fancy new models using the imperative API. I tried to load these weights via `model.load_weights('weight_file.h5', by_name=True)` but found that some weights were not loaded due to the layer class name being added as a prefix to the layer weight names. I'm unable to use  `model.load_weights('weight_file.h5', by_name=False)` as by the nature of the model being composed of subclassed components, all `load_weights_from_hdf5_group` sees are the top level layers of the outermost model class.\r\n\r\nRather than modifying the prefix, I'd like to not use a prefix entirely so that the weight names are compatible with what was saved in the model weight. Using `with variable_scope(\"\"):` resulted in the prefix still being affixed to layer weight names. \r\n\r\nA workaround I found was to assign `self._name = ''` in my sub-classed layer like so:\r\n\r\n```\r\nclass SomeConvs(keras.layers.Layer):\r\n    def __init__(self):\r\n        super(SomeConvs, self).__init__()\r\n        self._name = ''\r\n        layer_name = \"name_i'm_forced_to_use_because_i'm_loading_weights_via_name\"\r\n        self._conv2d = keras.layers.Conv2D(filters=(2, 2),\r\n                                           name=layer_name)\r\n\r\n    def call(self, x):\r\n        x = self._conv2d(x)\r\n        return x\r\n```", "Asking Francois to take a look since the example is Keras specific, and in tf2 name and variable scopes are kinda the same thing now."]}, {"number": 7730, "title": "[Java] [Feature] Tensor clone", "body": "Allows a tensor to be cloned (shallow), resulting in a new tensor\r\nsharing the same underlying buffer.\r\n\r\nThe cloned tensor is still usable after the original is closed.  This\r\nis possible because the buffer supports reference counting.\r\n\r\n- Added `TF_Clone` to the C API.\r\n- Added `Tensor::clone` to the Java API.", "comments": ["Can one of the admins verify this patch?", "Thanks for the change. Though, looking at the [API contract for `Object.clone()`](https://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#clone()), I'm not sure if this is appropriate. Particularly, the line:\r\n\r\nBy convention, the object returned by this method should be independent of this object (which is being cloned).\r\n\r\nI realize that currently Tensor objects are essentially immutable, so it may not matter yet. But I suspect that this will change in the future (folks wanting to write directly to portions of the buffer) and then the fact that two clones share the same underlying buffer may become confusing.\r\n\r\nIIRC, your motivation to do this stems from making scala bindings more idiomatic, where there are no `try-with-resources` blocks? Would a lightweight reference counting class over Tensor be something you could use in your code instead?\r\n\r\n(FYI @jhseu)", "I agree that tensor mutability would be incompatible with this, unless a copy-on-write approach were used under the hood.  Can you think of other cases of shared tensor buffers in the core?  \r\n\r\nIf impractical, we can close this PR and I'll try something else.", "I don't want to rule this out outright yet, but I would encourage you to see if there are other options that work well for your use case.\r\n\r\nAn alternative would be to add a finalizer to the `Tensor` class that does the cleanup, allowing users to avoid using `close()` and let the resources be reclaimed on GC, or introduce the reference counts as you had proposed earlier. I'm not sure there is any perfect solution.", "I rely on a wrapper at this time, no worries you can close this.\r\n\r\nIt can't hurt to have a finalizer as a safety net.   But as you know it's dangerous to leave the cleanup of large unmanaged blocks of memory to the GC.  Apparently direct ByteBuffers suffer a similar problem (and [hacks abound](http://stackoverflow.com/questions/3496508/deallocating-direct-buffer-native-memory-in-java-for-jogl/26777380#26777380)).\r\n\r\nI do like how Netty solved this problem ([ref](http://netty.io/wiki/using-as-a-generic-library.html#buffer-api)), and it inspired the approach in #6528.  Netty-style ref counting is the most powerful yet most convenient alternative that I know of.   Netty resembles a dataflow system in some ways, with its channel pipeline of buffers, further inspiring my earlier comments.", "Thanks for the comments. I'm going to close this for now, but perhaps we will revisit this or your ref change in the near future. \r\n\r\nThanks!"]}, {"number": 7729, "title": "How to delete existed tensorflow variable?", "body": "Hello, I am using windows GPU tensorflow 1.0.\r\n I use ipython notebook to build training model because I can reuse script easily. Sometimes I just want to modify one tensorflow variable, such as W1(shape=[3,3],name=\"W1\") from a existed model. So I just modify the variable and run the model building script again. However, tensorflow add W1(shape=[5,5],name=\"W1_1\")  variable to replace old variable W1. It makes some bugs when I continue the next work with W1. Could you tell me how to delete old variable \"W1\" and add new different variable named \"W1\"?", "comments": ["This questions is suited for StackOverflow which is monitored as well under TensorFlow tag.", "as @Carmezim mentioned, these questions are better suited to stackoverflow, to make sure answers get better exposure for other people looking for the same, and to keep this list focused on things that need action of core developers"]}, {"number": 7728, "title": "compile_ios_tensorflow.sh  failed with following error", "body": " /Users/xietian/Documents/tensorflow/tensorflow/contrib/makefile/gen/lib/ios_ARMV7/libtensorflow-core-armv7.a -arch armv7 -fembed-bitcode -miphoneos-version-min=8.2 -framework Accelerate -Xlinker -S -Xlinker -x -Xlinker -dead_strip -all_load -L/Users/xietian/Documents/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib -lz -lstdc++ -lprotobuf -lz -lm\r\nUndefined symbols for architecture armv7:\r\n  \"google::protobuf::internal::WireFormatLite::WriteDoubleArray(double const*, int, google::protobuf::io::CodedOutputStream*)\", referenced from:\r\n      tensorflow::TensorProto::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const in libtensorflow-core-armv7.a(tensor.pb.o)\r\n      tensorflow::HistogramProto::SerializeWithCachedSizes(google::protobuf::io::CodedOutputStream*) const in libtensorflow-core-armv7.a(summary.pb.o)\r\n  \"google::protobuf::Any::MergeFrom(google::protobuf::Any const&)\", referenced from:\r\n      tensorflow::MetaGraphDef_MetaInfoDef::MergeFrom(tensorflow::MetaGraphDef_MetaInfoDef const&) in libtensorflow-core-armv7.a(meta_graph.pb.o)\r\n      google::protobuf::internal::GenericTypeHandler<google::protobuf::Any>::Merge(google::protobuf::Any const&, google::protobuf::Any*) in libtensorflow-core-armv7.a(meta_graph.pb.o)", "comments": ["Could you please provide more contextual data, as requested by the New Issue template?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7727, "title": "when will tf.contrib.distributions move into tf core? Is this planned?", "body": "Hi,\r\n\r\nI'm working with distributions a lot in my own library built upon tensorflow. However, I found the api provided in tf.contrib.distributions are subject to change very often, which makes it quite difficult to keep up and maintain a stable codebase.\r\nSo I'd like to ask whether tf.contrib.distributions is going to the tensorflow core library and how is this planned. Will you recommend to keep using it or just build my own distributions?\r\n\r\nBest,\r\nJiaxin", "comments": ["We're moving it in the next 2 weeks, hopefully.  There may be one final breaking change to the API; but it should happen before TF 1.2 is released.  I imagine by TF 1.2 it'll be in core.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "The move will happen by tf 1.3.\n\nOn Jun 16, 2017 2:03 PM, \"Olivia\" <notifications@github.com> wrote:\n\n> Automatically closing due to lack of recent activity. Since this issue is\n> old at this point, please reopen the issue if it still occurs when tried\n> with the latest version of Tensorflow. Thank you.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7727#issuecomment-309133536>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyjMfEHnkodS_Afg0W1y1HvkvUcfks5sEu2ygaJpZM4MG8PV>\n> .\n>\n"]}, {"number": 7726, "title": "Feature request: Allow import of incomplete graphs", "body": "It seems like there is no reason in principle not to allow this:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ng=tf.Graph()\r\nwith g.as_default():\r\n    x = tf.constant(1.0, name=\"x\")\r\n    y = tf.exp(x)\r\ng_def=tf.GraphDef()\r\ng_def.node.extend([y.op.node_def])\r\n\r\ng2 = tf.Graph()\r\nwith g2.as_default():\r\n    x = tf.constant(2.0, name=\"x\")\r\n    tf.import_graph_def(g_def)\r\n\r\n```\r\n\r\nas, after `y` is imported, the `x:0` input it refers to is valid. \r\n\r\nAs it stands however, this throws `ValueError: graph_def is invalid at node u'Exp': Input tensor 'x:0' not found in graph_def..`. \r\n\r\n", "comments": ["I don't think this feature should be added to `tf.import_graph_def()`, because it is designed to deal explicitly with correctly typed and valid graphs. The `input_map` argument is available for remapping inputs explicitly, and the following program works:\r\n\r\n```python\r\ng2 = tf.Graph()\r\nwith g2.as_default():\r\n    x = tf.constant(2.0, name=\"x\")\r\n    tf.import_graph_def(g_def, input_map={\"x:0\": x})\r\n```\r\n\r\nNotice how you had to drop to manipulating the `tf.GraphDef` directly at the protobuf level in order to *create* an incomplete graph. You could easily achieve the behavior you desire by merging the protobufs `g1.as_graph_def()` and `g2.as_graph_def()`, and then use `tf.import_graph_def()` to check the validity of the merged graph.\r\n\r\nThere might be room somewhere in the API for lower-level graph editing (`tf.contrib.graph_editor` for example?), but `tf.import_graph_def()` already packs a lot of functionality into a single function, and adding more implicit behavior would not\u2014in my opinion\u2014improve it.", "It looks like there's nothing to do here so closing this issue. Please reopen if I'm mistaken."]}, {"number": 7725, "title": "BAZEL_SH environment variable is not defined...", "body": "C:\\Users\\nmehandru>bazel build tensorflow/tensorflow/examples/image_retraining:retrain\r\nError: BAZEL_SH environment variable is not defined, cannot convert MSYS paths to Windows paths: Invalid or incomplete multibyte or wide character\r\nError: AsWindowsPathWithUncPrefix(/etc/bazel.bazelrc): AsWindowsPath failed, err=203\r\n: Invalid or incomplete multibyte or wide character\r\nError: AsWindowsPathWithUncPrefix(/home/nmehandru/.bazelrc): AsWindowsPath failed, err=203\r\n: Invalid or incomplete multibyte or wide character\r\nERROR: 'null' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n (exit code: 1)\r\nThe 'build' command is only supported from within a workspace.\r\n\r\n\r\nThis is my printout when trying to run a script within the Tensorflow directory I cloned, in cmd in Windows 10. Not sure where to go from here. ", "comments": ["> BAZEL_SH environment variable is not defined\r\n\r\nDid you [install Bazel](https://bazel.build/versions/master/docs/windows.html) and add the environment variable BAZEL_SH ?", "I actually did that, I solved that issue, added that variable. \r\nNow, I am trying to figure out how to run the configure file. \r\nI am using command prompt in Windows 10, so I can't run the command './configure' as you would in a Linux terminal\r\nWhat is the equivalent method of running the configure file in command prompt in Windows?\r\n\r\n\r\n\r\nMy output right now is the following: \r\nC:\\Users\\nmehandru\\tensorflow>bazel build tensorflow/examples/image_retraining:retrain\r\nWARNING: C:/Users/nmehandru/AppData/Local/Temp/_bazel_nmehandru/48wqf4ei/external/bazel_tools/tools/cpp/cc_configure.bzl:57:3:\r\nAuto-Configuration Warning: 'BAZEL_PYTHON' is not set, start looking for python in PATH.\r\n.\r\nWARNING: C:/Users/nmehandru/AppData/Local/Temp/_bazel_nmehandru/48wqf4ei/external/bazel_tools/tools/cpp/cc_configure.bzl:57:3:\r\nAuto-Configuration Warning: Python found at C:/Python27/python.exe\r\n.\r\nWARNING: C:/Users/nmehandru/AppData/Local/Temp/_bazel_nmehandru/48wqf4ei/external/bazel_tools/tools/cpp/cc_configure.bzl:57:3:\r\nAuto-Configuration Warning: 'BAZEL_VS' is not set, start looking for the latest Visual Studio installed.\r\n.\r\nWARNING: C:/Users/nmehandru/AppData/Local/Temp/_bazel_nmehandru/48wqf4ei/external/bazel_tools/tools/cpp/cc_configure.bzl:57:3:\r\nAuto-Configuration Warning: Visual Studio found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\r\n.\r\nERROR: C:/Users/nmehandru/tensorflow/tensorflow/core/BUILD:1279:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by C:/Users/nmehandru/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: C:/Users/nmehandru/tensorflow/tensorflow/core/BUILD:1279:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by C:/Users/nmehandru/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: C:/Users/nmehandru/tensorflow/tensorflow/core/BUILD:1279:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by C:/Users/nmehandru/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/examples/image_retraining:retrain' failed; build aborted.\r\nINFO: Elapsed time: 0.948s", "Various parts of the Bazel build for TensorFlow on Windows depend on running `bash` scripts. If you followed the [Bazel on Windows installation instructions](https://bazel.build/versions/master/docs/windows.html), you should have installed msys2, which includes a `bash` shell. You should use that shell, set the environment variables in the `bash` session, and run `./configure` and `bazel build` from there.", "@mrry a quick question if you don't mind, I configured TensorFlow successfully but during build I got [this error](https://github.com/bazelbuild/bazel/issues/2594). Have you seen this problem before and do you know if it's in fact on bazel side or perhaps TF's? \r\nThank you", "@Carmezim Sorry, I haven't seen that issue before. The only thing that's slightly fishy is `--config=opt`, which I think should be `-c opt` (an alias for `--compilation-mode=opt`).", "@mrry No problem, thank you for looking into it. Interesting you point that out because it is how the command is on [Installing TensorFlow from Sources](https://www.tensorflow.org/install/install_sources) under *Build the pip package* (with `--config=opt`).", "@Carmezim I'd say there's an equal chance that I'm wrong about how Bazel works, or there's a bug in the docs :).", "@mrry haha. I believe the way you suggested is correct as I've seen folks using `-c opt`. Sorry derailing this thread. I was going to try with `-c opt` but just faced another [issue](https://github.com/tensorflow/tensorflow/issues/7871#issuecomment-282532359) so might take a little but I can follow up after if you want.\r\n**update**: with `-c opt` still got the same error.", "@mrry When I try to use 'bazel build' in msys2 it throws '-bash: bazel: command not found'. In cmd.exe the command bazel works. How can I add bazel command to bash?\r\nI installed bazel using Chocolatey.", "It sounds like the path to `bazel.exe` isn't in Bash's `PATH` environment variable. I haven't installed Bazel using Chocolatey, so I'm not sure where it installs it, but if (as an example) it was installed in `C:\\tools\\bazel\\bazel.exe`, you would use the following command to add it to Bash's `PATH`:\r\n\r\n```\r\nexport PATH=$PATH:/c/tools/bazel\r\n```", "adding @meteorcloudy to help with bazel on windows issues.", "Did you set all the environment variables listed here:\r\nhttps://bazel.build/versions/master/docs/windows.html#requirements\r\n\r\nNamely, you will need to set the following environment variables for bazel to work correctly on your system.\r\n```\r\nBAZEL_SH\r\nJAVA_HOME\r\nBAZEL_PYTHON\r\nBAZEL_VS\r\n```", "Closing due to lack of activity. ", "@nmehandru Hi,how to slove the problem,thanks.", "@AndyLin0128 Hi Andy, have you tried installing Bazel following [these instructions](https://docs.bazel.build/versions/master/windows.html)?", "@Carmezim Thanks for u answer."]}, {"number": 7724, "title": "Add TF_CONTROL_SLOT to c_api", "body": "This lets users of `TF_ImportGraphDefOptionsAddInputMapping` remap control inputs without having to guess the magical value of the `kControlSlot` constant. \r\n\r\ncf #7508", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Actually, talking to @asimshankar offline, he came up with a different approach that seems more in line with the current API. Instead of exposing TF_CONTROL_SLOT, how about we add a new function instead:\r\n```\r\nTF_GraphImportGraphDefOptionsRemapControlDependency(\r\n  TF_ImportGraphDefOptions* opts,\r\n  const char* src_name,\r\n  TF_Operation* dst);\r\n```\r\nThis is similar to other parts of the API where we explicitly create functions for control dependencies (e.g. TF_AddControlInput, TF_ImportGraphDefOptionsAddControlDependency).", "Makes sense to me. I'll code that up today.", "OK, I took a shot at it.", "Looks good to me, thanks!", "@tensorflow-jenkins test this please", "@malmaud PR merged. Thank you for the contribution!", "Thanks guys."]}]