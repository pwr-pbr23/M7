[{"number": 11026, "title": "Update optimize_for_inference_test.py", "body": "These two should be interchanged \"self.assertNotEqual(\"ResizeBilinear\", node.op)\" with \"self.assertNotEqual(\"MirrorPad\", node.op)\"", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@petewarden could you take a look?", "Jenkins, test this please.\r\n\r\nLooks like there was some kind of transient error.", "Ping @petewarden "]}, {"number": 11025, "title": "AttributeError: module 'reader' has no attribute 'ptb_producer'", "body": "I use anaconda2 python3.5 with tensorflow1.2. I want to test tensorflow on the PennTree bank (ptb) dataset. \r\n\r\nReferring to the ptb codes provided at https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb, I run the code\r\n`reader_test.py`\r\nbut got the error:\r\nERROR: testPtbProducer (__main__.PtbReaderTest)\r\nTraceback (most recent call last):\r\n  File \"<ipython-input-15-5eff201b8159>\", line 34, in testPtbProducer\r\n       ` x, y = reader.ptb_producer(raw_data, batch_size, num_steps)\r\n`AttributeError: module 'reader' has no attribute 'ptb_producer'\r\nFAIL: testPtbRawData (__main__.PtbReaderTest)\r\nTraceback (most recent call last):\r\n  File \"<ipython-input-15-5eff201b8159>\", line 28, in testPtbRawData\r\n    self.assertEqual(len(output), 4)\r\nAssertionError: 5 != 4\r\nRan 3 tests in 0.344s\r\nFAILED (failures=1, errors=1)\r\nAn exception has occurred, use %tb to see the full traceback.\r\nSystemExit: <sitecustomize.IPyTesProgram object at 0x000001FF9FADC630>\r\nC:\\Users\\yl\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\r\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\r\n\r\nThen, running the code:\r\n`ptb_word_lm.py`\r\nalso produces errors:\r\n  File \"C:\\Users\\yl\\Anaconda2\\envs\\tensorflow-gpu\\lib\\argparse.py\", line 1506, in _handle_conflict_error\r\n    raise ArgumentError(action, message % conflict_string)\r\nArgumentError: argument --model: conflicting option string: --model\r\n\r\nDid I do something wrong or miss some steps?", "comments": ["@nealwu can you comment on this or redirect to someone who can? Thanks!", "Hi @jingweimo, could you fill out the system information form we provide at https://github.com/tensorflow/models/issues/new? I tried the test and it is working fine for me in both Python 2 and Python 3.\r\n\r\nYou may also want to try on non-Anaconda Python and also try on Python 2 to see if either of those is the cause of your issue.", "@nealwu\uff1a Thanks! I will try that later", "@nealwu: Hello! I post the problem encountered with running reader_test.py and ptb_word_lm.py at: https://stackoverflow.com/questions/44855380/tensorflow-runn-error-with-reader-test-py-in-models-tutorials-rnn\r\nYou may take a look at the error message. I am not sure if there is something wrong with anaconda environment. I follow the instructions for installing tensorflow-gpu in windows by conda installation. And the installation is free of any problem. ", "What went wrong with `reader_test.py`? It looks like it ran fine to me.\r\n\r\nAs far as `ptb_word_lm.py`, I don't really know what's going on there. Hopefully someone on StackOverflow will be familiar with the error.", "@nealwu : For the `reader_test.py`, it shows that the TensorFlow library wasn't complied to SSE and other instructions. I don't know if that implies some errors. For `ptb_word_lm.py`, it gives the error:\r\n`InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(20, 400), b.shape=(400, 800), m=20, n=800, k=400\r\n                 `", "@nealwu : I solved the problem by updating anaconda `(conda update --all)` and then restarting my laptop", "Yes, that means there are a few options you could compile with to boost your speed, but everything ran fine. As far as `ptb_word_lm.py`, I'm not sure unfortunately.", "Oh, great! Looks like we can close this then."]}, {"number": 11024, "title": "Adjust doc MNIST learning rate", "body": "The gradient descent learning rate specified in the MNIST beginner\r\ntutorial should be 0.5 instead of 0.05 to stay consistent with the\r\nactual code in the repo.\r\n\r\nA learning rate of .05 will actually decrease accuracy.\r\n\r\nCode in repo: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py#L59", "comments": ["Can one of the admins verify this patch?", "The documentation on the [website](https://www.tensorflow.org/get_started/mnist/beginners#training) still says 0.05. Some generation script needs to be run to update that? ", " Looks like the code block on the website was somehow not updated with this change, but the second instance that is not in the code block got updated. @MarkDaoust have you run into this before?", "It looks like there's an intermediate version of this page where that was the actual content, then someone updated the comment in the doc to match the code in the doc...\r\n\r\nThe root docs (that you link to) are only updated with releases.\r\n\r\nBut this fix will be in [versions/master](https://www.tensorflow.org/versions/master/get_started/mnist/beginners) next time it gets published."]}, {"number": 11023, "title": "Bug: placeholder input to tf.one_hot leads to hang", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0 / cuDNN 6.0\r\n- **GPU model and memory**: GTX 1080 Ti 11GB\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nI'm not sure if you're not supposed to feed in a placeholder to `tf.one_hot`, but if you do, it hangs and chews up 100% CPU.\r\n\r\n### Source code / logs\r\n\r\nMinimal example to reproduce bug:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\np = tf.placeholder(tf.uint8, 1)\r\nx = tf.one_hot(p, depth=10)\r\nres = sess.run(x, feed_dict={p: [3]})\r\nprint(res)\r\n```\r\n\r\nThe expected result should be either (1) it works and prints `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`, or (2) it produces some kind of error.", "comments": ["I got the output of [[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]. So this can be closed?\r\n\r\nBTW, I am running tensorflow==1.2.0.", "Oh hmm, I upgraded to 1.2.0 and now it works."]}, {"number": 11022, "title": "Major performance hit when running two processes on same GPU.", "body": "I've observed a major performance hit when running two identical models on the same GPU, with allow_growth set to True, so that each process, which only needs a small fraction of the GPU memory (~1gb/11gb are used when allow_growth is set to true). When running a single model on a single process, it consistently finished going through the data I have available in approximately 170-174 seconds, and never exceeds 50% Volatile GPU-Utilization according to nvidia-smi . However, when running with two separate, concurrent processes, (each identical to the first), they both finish in approximately 320-340 seconds, which is unexpected, since GPU utilization was not even half in the first scenario, and running two concurrently effectively slows them down to running them sequentially, despite the seemingly available processing power and memory. \r\n\r\nIs this intentional, or is there a better way to do this? (Currently launching two workers via Celery, each of which create their own TF session and load the model into it separately, and run the data through it). I would love to make maximal use of available hardware, and this seems like a very counter-intuitive outcome. \r\n\r\nI can observe each process allocate roughly 1GiB GPU memory, and each adds approximately 45-50% GPU utilization. For testing purposes, data and model used in both parallel runs is completely identical.\r\n\r\nAny thoughts? Am I misusing TF? \r\n\r\n### System information\r\n- Using Keras to load model\r\n- Ubuntu 16.04\r\n- CUDA/CUDNN 8.0/5.1\r\n- TF version 1.0.1\r\n- GTX 1080 ti (not being used to render screen, second 1080 ti is doing that)\r\n- Running in nvidia-docker with a single GPU available to worker process (the one not rendering the screen).\r\n- Running two separate sessions initiated via Celery which both create their own session, set allow_growth=True, each load the model into memory separately, and each run with data separately. \r\n\r\n", "comments": ["Without code and model and command it's hard to tell anything about it. Maybe in your code, just copying your data from CPU to GPU is already taking a lot of time, in which case adding processes wouldn't help at all.", "Please include a minimum reproducer and make sure you include the exact command if possible to produce the output included in your case. If you are unclear what to include see the issue template displayed in the [Github new issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n\r\nAlso, did you try this on the latest tensorflow version (r1.2)? ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Note that the GPU utilization shown by nvidia-smi might be inaccurate as during the model running, there might be dynamic device memory being allocated so the peak utilization might be larger. Also, like @ppwwyyxx mentioned, several other issues might affect the performance.\r\n\r\nI will close this issue due to the inactive status. Please feel free to reopen this if you have more insights/code to share. I'd be happy to take a look once more detailed information is provided."]}, {"number": 11021, "title": "bazel does not co-operate with Ubuntu's update-alternatives", "body": "### System information\r\n\r\nTF 1.2, Ubuntu 17.04 on GCE, CUDA 8.0, gcc 4.9.4\r\n\r\n### Describe the problem\r\nbazel fails to compile tensorflow when multiple versions of gcc are present and the default points to `/etc/alternatives/gcc`. Specifically, it fails to find the C and C++ header files. However, all works well when `configure` is set to use `/usr/bin/gcc`.", "comments": ["Maybe this is a more appropriate place to file this bazel issue:\r\n\r\nhttps://github.com/bazelbuild/bazel/issues", "@esafak Yes as @caisq suggests, please file a bazel bug for this instead.\r\n\r\nIf you feel there are tensorflow-specific issues, just comment on this issue and I'll re-open."]}, {"number": 11020, "title": "TensorArray `name` using a Tensor", "body": "### System information\r\npython-version: ('v1.2.0-rc1-24-gce1d6ec', '1.2.0-rc2')\r\n\r\n### Describe the problem\r\nLooks like the name of the TensorArray is a Tensor and that Tensor is being treated as a boolean in a call to `name_scope`. \r\n\r\n### Source code / logs\r\n```\r\nta = tf.TensorArray(\r\n          dtype=tf.float32,\r\n          size=100,\r\n          dynamic_size=False,\r\n          colocate_with_first_write_call=False,\r\n          clear_after_read=True,\r\n          infer_shape=False)\r\n# write a bunch of stuff\r\nout = ta.gather(0, 12)\r\n```\r\n\r\nTrace:\r\n```\r\n    out = ta.gather(0, 12)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 360, in gather\r\n    element_shape=element_shape)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1814, in _tensor_array_gather_v3\r\n    element_shape=element_shape, name=name)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 374, in apply_op\r\n    with g.as_default(), ops.name_scope(name) as scope:\r\n  File \"/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 4359, in name_scope\r\n    with g.as_default(), g.name_scope(n) as scope:\r\n  File \"/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3009, in name_scope\r\n    if name:\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 578, in __nonzero__\r\n    raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n```", "comments": ["My mistake.", "@aidangomez I'm getting a similar error when trying to define a custom op\u2014would you happen to recall what you did to fix the issue?", "@gtmtg In this case I was calling ta.gather with two Tensors, but the second argument was suppose to be the name which expects a python string. Just a silly mistake on my part."]}, {"number": 11019, "title": "Negative axis support for gradient of reduce_prod", "body": "Fixes  #10835", "comments": ["@tensorflow-jenkins bot didn't get triggered. Is there any problem?\r\n", "Jenkins, test this please.", "PyLint tests were failing. So, I fixed that. Can you run Jenkins again?", "Jenkins, test this please.\n\nOn Jun 27, 2017 5:00 AM, \"Anish Shah\" <notifications@github.com> wrote:\n\n> PyLint tests were failing. So, I fixed that. Can you run Jenkins again?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11019#issuecomment-311336756>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbTN32G1BfTNF-zO-o_EGwN36eALpks5sIO7ngaJpZM4OD8Bs>\n> .\n>\n", "All tests have passed now! Is this ready to merge?", "Yep. Congrats!"]}, {"number": 11018, "title": "Fix issue template tensorboard issues link", "body": "Reported in #10975 ", "comments": ["Jenkins, test this please.\r\n"]}, {"number": 11017, "title": "Tfdbg does not work with Coordinator/QueueRunners", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nThe Tensorflow debugger does not seem to be working with Queues; data never seems to be fetched by the QueueRunner threads, be it from a file (using `tf.TFRecordReader` and `tf.parse_single_example`) or preloaded (using `tf.train.slice_input_producer`). Instead, the `coordinator.should_stop()` is True right away. This is only the case after wrapping the session in a `tf.python.debug.LocalCLIDebugWrapperSession`. The example should make things clearer.\r\n\r\nMoreover, another error occurs at `coordinator.join(threads)`.\r\n\r\nI am aware of the [FAQ entry on Threads](https://www.tensorflow.org/programmers_guide/debugger), but that does not explain why the data fetching threads would not be working.\r\n\r\n### Source code / logs\r\nTo make it easiest to replicate, I simply took the [example on working with preloaded data](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py), and wrapped the session in there with the debugger. I uploaded the gist with two lines added to https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92.\r\n\r\nTo reproduce, run the file. Once you drop in the debugger, run once. It then exits. The full output is below:\r\n\r\n```\r\nExtracting /tmp/data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\r\nTraceback (most recent call last):\r\n  File \"ex.py\", line 191, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"ex.py\", line 143, in main\r\n    run_training()\r\n  File \"ex.py\", line 138, in run_training\r\n    coord.join(threads)\r\n  File \"/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/ruben/anaconda3/lib/python3.6/site-packages/six.py\", line 686, in reraise\r\n    raise value\r\n  File \"/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 233, in _run\r\n    enqueue_callable = sess.make_callable(enqueue_op)\r\nAttributeError: 'LocalCLIDebugWrapperSession' object has no attribute 'make_callable'\r\n```\r\n\r\nThe stacktrace is about `coord.join(threads)`, but this is only possible because `coord.should_stop()` never seems to be `False`, which would indicate there is data to load. Without the added debugger lines, the example simply works.", "comments": ["cc @mrry \r\n\r\n@rubenvereecken Thanks for reporting this issue. We are aware of it and will push a fix to it soon.", "@caisq thank you so much, I look forward to it. ", "@rubenvereecken While you wait for the fix, I want to ask you whether you are trying to debug the data input queues, or the training operation on the main thread. If the latter, there is a workaround for that.", "@caisq Ah actually the former, but I'd be working my way towards the latter. Is there a way to debug these training ops while still using data fed from queues? Either way, could you point me at the workaround? Much appreciated! ", "@rubenvereecken The workaround is based on the assumption that the train op runs on the Python main thread, while the data queues run on the child threads, which should usually be the case.\r\n\r\nYou can just use the `thread_name_filter` kwarg of the wrapper's constructor to limit the debugging to the train op.\r\n\r\n```python\r\nsess = tf_debug.LocalCLIDebugWrapperSession(sess, thread_name_filter=\"MainThread$\")\r\n```\r\n\r\nThis is talked about in the FAQ. Doing this doesn't change the source of the input data. They still come from the queues; they just don't break into the TFDBG UI when they run.", "Oops. I may have given incomplete suggestion. In your code at https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92\r\n\r\nmake sure that your wrapped Session object is used only to run the train_op. You can do something like this:\r\nMove the line `sess = tf_debug.LocalCLIDebugWrapperSession(sess)` after line 100. That makes sure that when the data input queue ops are created, the `make_callable()` method of the original session, not that of the wrapped session is called. The wrapped session doesn't have a `make_callable()` method, which was recently added. This was the root cause of the issue you are seeing.", ":tada: ", "For the user of TF-Slim,  The usage of **thread_name_filter**\r\n``` \r\nsession_wrapper_main_thread =  functools.partial(\r\n  tf_debug.LocalCLIDebugWrapperSession,\r\n  thread_name_filter=\"MainThread$\")\r\n\r\nslim.learning.train(\r\n  ... \r\n  session_wrapper=session_wrapper_main_thread,\r\n  ... ) \r\n\r\n```\r\n\r\n"]}, {"number": 11016, "title": "map_func of tf.contrib.data.Dataset.map gets dict keys instead of values when the nested structure of Dataset is dict", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: b'0.5.0-12520-g1111e06d9' 1.2.0-rc2\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0/6\r\n- **GPU model and memory**:\r\n\r\n### Describe the problem\r\n\r\nIf the nested structure of `Dataset` is `dict`, `MapDataset` will call [`map_func(*nested_args)`](https://github.com/tensorflow/tensorflow/blob/1111e06d9cd691cbdfcb67cf9f234a504f4e0f6d/tensorflow/contrib/data/python/ops/dataset_ops.py#L1463) and pass the keys of `nested_args` instead of components in the dataset to `map_func`. It seems that `nested_args` or `*nested_args.values()` need to be passed to `map_func`, so that `map_func` could transform the elements in the dataset.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef foo(*args, **kwargs):\r\n    print(args, kwargs) # ('a', 'b') {}\r\n    return 1 * 100, 2 * 200\r\n\r\ntf.contrib.data.Dataset.from_tensors({'a': [1], 'b': [2]}).map(foo)\r\n```", "comments": ["I tried reproducing and I think that Dicts are not supported as inputs in general\r\n\r\n>>> a = tf.contrib.data.Dataset.from_tensors({'a': [1], 'b': [2]})\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 460, in from_tensors\r\n    return TensorDataset(tensors)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 864, in __init__\r\n    for i, t in enumerate(nest.flatten(tensors))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 676, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 741, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 113, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 102, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 462, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <type 'dict'> to Tensor. Contents: {'a': [1], 'b': [2]}. Consider casting elements to a supported type.\r\n\r\nMy tensorflow version was 1.2.0 though.", "It seems that f3f53e8b394bdcaddc707f7bde8dcc98a73531e7 adds support for `dict` as nested structures of `Dataset`. I built master branch from source.", "This is definitely a bug. Thanks for catching it! I have a fix in the works.", "It seemed that the parameter `padded_shapes` for `tf.contrib.data.Dataset.padded_batch` can't be `dict` too.\r\n", "Thanks! That turned up when I was testing the fix. "]}, {"number": 11015, "title": "Link to README.md is broken in NativeLibrary.java", "body": "I'm trying to run TensorFlow in Java, following [this instructions](https://www.tensorflow.org/install/install_java)\r\n\r\nI'm using Ubuntu 16 and java 8. I'm not sure what did I wrong, but error message says: \r\n\r\n`$ java -cp libtensorflow-1.2.0.jar:. -Djava.library.path=~/jni HelloTF`\r\n\r\n`Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: linux, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/java/README.md `\r\n\r\n`for possible solutions (such as building the library from source).`\r\n\t`at org.tensorflow.NativeLibrary.load(NativeLibrary.java:66)`\r\n\t`at org.tensorflow.TensorFlow.init(TensorFlow.java:36)`\r\n\t`at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:40)`\r\n\t`at org.tensorflow.Graph.<clinit>(Graph.java:194)`\r\n\t`at HelloTF.main(HelloTF.java:8)`\r\n\r\nBut following  **https://github.com/tensorflow/tensorflow/tree/master/java/README.md**  gets 404 - page not found.\r\n", "comments": ["Thanks for the report, I'll update the link (the correct link is https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md)\r\n\r\nThat said, looking at your erro message it seems that ti cannot find the native library. Can do run the following commands to help verify what might be missing in your setup?\r\n\r\n```\r\nls ~/jni\r\nfile ~/jni/*\r\n```\r\n\r\nAlternatively, if it makes sense for you, I'd suggest using a build tool like maven or gradle or something as that will involve much less command-line tweaking. See [instructions on the website](https://www.tensorflow.org/install/install_java#using_tensorflow_with_a_maven_project) for maven, or if you use other build tools (like gradle), then see the corresponding dependency information section [here](http://search.maven.org/#artifactdetails%7Corg.tensorflow%7Ctensorflow%7C1.2.0%7C)\r\n\r\n", "Thanks! I solved it giving the correct path to jni :smile: ", "Cool, looks like this issue is resolved, so I'm closing it out.", "Oops I spoke too soon; the real problem is fixed, but this issue is about fixing the README link.  :)", "Closing this as it's been fixed in https://github.com/tensorflow/tensorflow/pull/11094/commits/4cc35257f1f3613aae65a42b96e5ea5bba785f66\r\n"]}, {"number": 11014, "title": "Windows: Fix CUDNN_INSTALL_PATH in ./configure", "body": "Convert `CUDNN_INSTALL_PATH` from path style like `c:\\tools\\cuda` to `c:/tools/cuda`\r\nDidn't notice this before debugging https://github.com/tensorflow/tensorflow/pull/10995, because on CI, we set CUDNN_INSTALL_PATH directly.", "comments": ["**Update:**\r\nDelete the content of this comment. It is a different issue.\r\n\r\n~~I also observe this issue. On my machine, the output of the configure without this patch looks like:~~\r\n[deleted]\r\n"]}, {"number": 11013, "title": "Bug: experiment.py continuous_train_and_eval() leads to overfitting", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.0.1-4-gbd6743e-dirty 1.0.1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nOur team has found what we believe is a critical bug in [experiment.py continuous_train_and_eval()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L506) that pretty much guarantees any model trained with this method will lead to overfitting.\r\n\r\nThe problem is that every time it alternates from evaluating to training again, it calls [self._call_train()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L569), which eventually makes it to [estimator.py _train_model()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L950).\r\n\r\nThis function first [creates a brand new graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L952) then [sets up the input pipelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L956). This necessarily means that the input pipelines are reset from scratch on every training lap, and since a training lap is typically much shorter than an epoch, this means the model is only trained with a small portion of the whole dataset.\r\n\r\nOriginally we raised this as an issue in [Google's tf-seq2seq](https://github.com/google/seq2seq/issues/262), but as far as we can tell now the bug probably belongs into Tensorflow instead, as we can't see a simple way to modify the tf-seq2seq code to avoid this issue from arising.\r\n\r\nThank you for looking into this matter.\r\n\r\n### Source code / logs\r\nN/A", "comments": ["@jhseu Can you comment on this?  Thanks!", "Yeah, that's true. Often input_fns are defined to randomize inputs, but this does seem like an issue. @ispirmustafa @xiejw ", "I see. In the [original issue](https://github.com/google/seq2seq/issues/262) the client was calling [tf.contrib.slim.dataset.Dataset()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/dataset.py#L38) which doesn't appear to have an option to thoroughly randomize its inputs.\r\n\r\nI'm pretty new to TF, so I may be missing lots of things here :)", "Shuffle=True for train input_fn is recommended. And also it is best to make the training steps for each iteration longer, similar to the epoch size or multiple epochs. I will fix the docstring to document this. ", "not familiar with the slim Dataset api, but according to the docstring, there is a provider, which support randomize. ", "@xiejw,\r\n\r\nSetting shuffle=true in a tf.RandomShuffleQueue would hide the problem a bit, but not eliminate it, because the \"random\" part of a random queue occurs [when dequeuing](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_shuffle_queue_op.cc#L116). Since queues have a limited size, and the state is reset on every training lap, the model would still not train with the whole dataset.\r\n\r\nIt seems like the way to fix this properly is by not regenerating the graph from scratch on every training / testing lap. The graphs should be generated only once and then reused.", "Experiment has a method train_and_evaluate, which does the approach you wanted. However, it has a double memory problem, which is why continous_train_and_evaluate was introduced (as experimental). Can you try make train steps for each iteration larger (so one train iteration can process one epoch training data or multiple epoch data)?  \r\n\r\nIf that does not solve the problem, you probably need to try train_and_evaluate or write a hook to do evaluation by calling train(). ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 11012, "title": "[XLA] Add a unary ops cosine test to the standard tests", "body": "Add an XLA cosine python test. ", "comments": ["Can one of the admins verify this patch?", "Thanks @DavidNorman. @tensorflow-jenkins test this please.", "hmm.  it's possible that the GPU doesn't support cosine for f64.\r\n\r\ni'll doctor up the test to avoid that for GPU.\r\n", "the 'linux CPU tests (python 3)' must be someone else's error.", "Ok:  this error\r\n\r\n```\r\nLLVM ERROR: Cannot select: 0x7f9838182938: f64 = fcos 0x7f98381828d0\r\n  0x7f98381828d0: f64,ch = load<LD8[%2(addrspace=1)](tbaa=<0x7f9838183580>)(noalias=<0x7f983814be50>)(invariant)> 0x7f983824c518, 0x7f9838182868, undef:i64\r\n    0x7f9838182868: i64 = add 0x7f9838182528, 0x7f9838182c10\r\n      0x7f9838182528: i64 = addrspacecast[0 -> 1] 0x7f9838182a08\r\n        0x7f9838182a08: i64,ch = load<LD8[null(addrspace=101)](dereferenceable)(invariant)> 0x7f983824c518, TargetExternalSymbol:i64'_cosine_param_0', undef:i64\r\n          0x7f9838182180: i64 = TargetExternalSymbol'_cosine_param_0'\r\n          0x7f9838182250: i64 = undef\r\n      0x7f9838182c10: i64 = NVPTXISD::MUL_WIDE_UNSIGNED 0x7f98381826c8, Constant:i32<8>\r\n        0x7f98381826c8: i32 = AssertZext 0x7f98381825f8, ValueType:ch:i2\r\n          0x7f98381825f8: i32 = llvm.nvvm.read.ptx.sreg.tid.x TargetConstant:i64<3350>\r\n            0x7f9838182590: i64 = TargetConstant<3350>\r\n        0x7f9838182ba8: i32 = Constant<8>\r\n    0x7f9838182250: i64 = undef\r\nIn function: _cosine\r\n================================================================================\r\n```\r\n\r\nthis indicates that LLVM cannot find a cosine function for f64 that targets the GPU.  Out LLVM person suggests that a possible solution is to add the following to the NVPTXInstrInfo.tf file:\r\n\r\n```\r\ndef SINF64:  NVPTXInst<(outs Float64Regs:$dst), (ins Float64Regs:$src),\r\n                      \"sin.approx.f64 \\t$dst, $src;\",\r\n                      [(set Float64Regs:$dst, (fsin Float64Regs:$src))]>,\r\n                      Requires<[allowUnsafeFPMath]>;\r\ndef COSF64:  NVPTXInst<(outs Float64Regs:$dst), (ins Float64Regs:$src),\r\n                      \"cos.approx.f64 \\t$dst, $src;\",\r\n                      [(set Float64Regs:$dst, (fcos Float64Regs:$src))]>,\r\n                      Requires<[allowUnsafeFPMath]>;\r\n```\r\n\r\nbut that is a quick guess about the right thing.  we have not tested it or anything.   \r\n\r\nI'm not going to try to fix this.  I will leave the pull request open for a while, in case you want to use it as a test for making the NVIDIA support in LLVM work.\r\n\r\n", "@tatatodd FYI, this is more like a bug report. I'm going to close this, assuming that we're acting on this.\r\n\r\n@DavidNorman Thanks!", "No problem.  Do you want me to file a bug report?\r\n", "@drpngx see last comment :)\r\n\r\n", "@DavidNorman thanks for trying to add the test, and the subsequent sleuthing!\r\n\r\n@sanjoy @hpucha @andydavis1 @skye all might be interested in this."]}, {"number": 11011, "title": "Install tensorflow with virtualenv failed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary with virtualenv\r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n    $ virtualenv --system-site-packages -p python3 tensorflow\r\n\r\n### Describe the problem\r\nOn my Ubuntu 16.04, I'm trying to install tensorflow with **virtualenv** for **python3** under the guide of https://www.tensorflow.org/versions/master/install/install_linux.\r\n\r\nBut I got the error message at the step2:\r\n\r\n    $ virtualenv --system-site-packages -p python3 tensorflow\r\n    Running virtualenv with interpreter /usr/local/anaconda3/bin/python3\r\n    Using base prefix '/usr/local/anaconda3'\r\n    New python executable in /home/lab/tensorflow/bin/python3\r\n    Also creating executable in /home/lab/tensorflow/bin/python\r\n    /home/lab/tensorflow/bin/python3: error while loading shared libraries: libpython3.6m.so.1.0: cannot open shared object file: No such file or directory\r\n    ERROR: The executable /home/lab/tensorflow/bin/python3 is not functioning\r\n    ERROR: It thinks sys.prefix is '/home/lab' (should be '/home/lab/tensorflow')\r\n    ERROR: virtualenv is not compatible with this system or executable\r\n\r\n\r\nNotice that I've install **anaconda3**. \r\n\r\nIs anaconda3 incompatible with tensorflow or virtualenv?\r\n\r\nShould I uninstall my anaconda3 before installing tensorflow?\r\n\r\nThanks to everybody first!\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nIn particular, this looks like a virtualenv / anaconda installation issue; you haven't reached any tensorflow-specific installation yet.", "I've asked the question on stackoverflow, but no one has responded.\r\nMaybe installing tensowflow within anaconda3 is better choice?"]}, {"number": 11010, "title": "Sort shuffled indices before indexing NumpyFeed", "body": "It is possible to use `h5py` datasets with `tf.estimator.inputs.numpy_input_fn` as there is a high level of compatibility.\r\n\r\nHowever, with `shuffle=True`, one can no longer do so due to the following error:\r\n\r\n    TypeError: Indexing elements must be in increasing order\r\n\r\nThis small change allows for one to use large HDF5 datasets via `h5py` with shuffled batch indices.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "That seems to make a lot of tests fail. Could you take a look?", "@drpngx I believe this now allows tests to pass.\r\n\r\nHowever, I'm unsure if the tests should be adjusted instead.", "Jenkins, test this please.\n\nOn Jun 27, 2017 6:48 AM, \"Seonwook Park\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> I believe this now allows tests to\n> pass.\n>\n> However, I'm unsure if the tests should be adjusted instead.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11010#issuecomment-311362785>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbTjPtyKZMwpfwaTj3T4PwSys345aks5sIQgJgaJpZM4ODcnM>\n> .\n>\n", "Now passes the tests.\r\n\r\n@xiejw would have cycles to take a look?", "ping @xiejw ", "Sorry for the delay. Just a nit. ", "@xiejw thanks for reviewing this PR. I've added a line to explain why sorting is necessary.", "Thanks for the contribution!", "Can one of the admins verify this patch?", "The timeseries failures look related. Allen, thoughts?", "Just so I'm clear, this change should ideally only be affecting the order in which the Numpy arrays are indexed, but should be returning the same values as before down the line in TensorFlow land?\r\n\r\nI can dig into them to figure out how exactly, but I think those test failures are indicating that with shuffle=False this PR is changing the output of the op (i.e. it's no longer returned in the input order). Probably indicates a bug?", "@xiejw thanks for catching the bug out. I have committed a fix to make the tests pass. I have confirmed that tests pass on my end by rebasing on upstream.\r\n\r\nYour concern on performance is something I agree with however...\r\n\r\nIf I understand right.. this issue occurs when wrapping around datasets to return to index 0. Perhaps it's better to split the indexing operations to allow for this.\r\n\r\nOr dismiss this PR as it could be argued that it's a bug in `h5py`.\r\n\r\nI leave the decision to you, the maintainers.", "For now, I think you can convert the h5py as numpy which is one time cost. If this has more cost than expected or it is extremely inconvenient, we can revisit this PR and propose some new ideas. \r\n\r\nWith that said, I will close this PR and please reopen if anything changed in future. \r\n\r\nThanks for your contribution! ", "I understand. Not to re-open the issue, but I'd just like to mention that I'm using `h5py` for large input data which cannot be held in memory. While one could write their own pre-processing routine using tensorflow queues, using the numpy feeding functions with hdf5 structures is very convenient.\r\n\r\nI attach the explanation for future reference."]}, {"number": 11009, "title": "Support specify the user name when writing data to HDFS\u3002 You can spec\u2026", "body": "When I write data to HDFS I can not specify username\r\nThis scenario is used by multiple users using the same client as a local user.\r\n\r\nYou can specify the environment variable \uff08TF_HDFS_USER\uff09 to modify the username.\r\n\r\n e.g:\r\nexport TF_HDFS_USER=\"username\"", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @Yiran-wu! @jhseu could you take a look? Thanks!", "Jenkins, test this please.", "Transient failure...\r\n\r\nJenkins, test this please.", "I use the following script to test it, HADOOP_USER_NAME can be work,  close this issue.\r\nthinks @jhseu @drpngx \r\n```\r\n# import MNIST\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n\r\nimport tensorflow as tf\r\nimport os\r\n\r\nos.environ['HADOOP_USER_NAME'] = 'test001'\r\n# set parameters\r\nlearning_rate = 0.01\r\ntraining_iteration = 30\r\nbatch_size = 100\r\ndisplay_step = 2\r\n\r\n\r\n# TF graph input\r\nx = tf.placeholder(\"float\", [None, 784])\r\ny = tf.placeholder(\"float\", [None, 10])\r\n\r\n# create a model\r\n\r\n# set model weights\r\n# 784 is the dimension of a flattened MNIST image\r\nW = tf.Variable(tf.zeros([784, 10]))\r\nb = tf.Variable(tf.zeros([10]))\r\n\r\nwith tf.name_scope(\"Wx_b\") as scope:\r\n    # construct linear model\r\n    model = tf.nn.softmax(tf.matmul(x, W) + b) #softmax\r\n\r\n# add summary ops to collect data\r\nw_h = tf.summary.histogram(\"weights\", W)\r\nb_h = tf.summary.histogram(\"biases\", b)\r\n\r\nwith tf.name_scope(\"cost_function\") as scope:\r\n    # minimize error using cross entropy\r\n    cost_function = -tf.reduce_sum(y*tf.log(model))\r\n    # create a summary to monitor the cost function\r\n    tf.summary.scalar(\"cost_function\", cost_function)\r\n\r\nwith tf.name_scope(\"train\") as scope:\r\n    # gradient descent\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n# merge all summaries into a single operator\r\nmerged_summary_op = tf.summary.merge_all()\r\n\r\n# launch the graph\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n\r\n    # set the logs writer to the folder /tmp/tensorflow_logs\r\n    summary_writer = tf.summary.FileWriter('hdfs://172.16.141.90:8020/tmp/tensorflow_logs', graph=sess.graph)\r\n\r\n    # training cycle\r\n    for iteration in range(training_iteration):\r\n        avg_cost = 0.\r\n        total_batch = int(mnist.train.num_examples/batch_size)\r\n        # loop over all batches\r\n        for i in range(total_batch):\r\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\r\n            # fit training using batch data\r\n            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\r\n            # compute the average loss\r\n            avg_cost += sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})/total_batch\r\n            # write logs for each iteration\r\n            summary_str = sess.run(merged_summary_op, feed_dict={x: batch_xs, y: batch_ys})\r\n            summary_writer.add_summary(summary_str, iteration*total_batch + i)\r\n        # display logs per iteration step\r\n        if iteration % display_step == 0:\r\n            print(\"Iteration:\", '%04d' % (iteration + 1), \"cost= \", \"{:.9f}\".format(avg_cost))\r\n\r\n    print(\"Tuning completed!\")\r\n\r\n    # test the model\r\n    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\r\n    # calculate accuracy\r\n    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\r\n    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\r\n\r\n\r\nprint(\"Success!\")\r\n```", "Jenkins, test this please."]}, {"number": 11008, "title": "AttributeError: module 'tensorflow' has no attribute 'constant'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorflow/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 11007, "title": "provide project ide setting support", "body": "I'm trying to debug the c++ source code to figure out the function call relation.But I find it's very difficult to import the code to any existing ide on Mac.I had to use the bazel command  to build and test. using gdb to debug is a bit frustrating.If there are some useful tool chains, would you please give some tips which will make it easily for developers to try such great tensorflow?Thanks so much.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11006, "title": "docs: add eval_input_fn in custom model code", "body": "", "comments": ["Can one of the admins verify this patch?", "This has recently been fixed. Thanks a lot for the contribution @demohi!"]}, {"number": 11005, "title": "[Bug] tf version 1.2 , dropout layer reject tensor type rate", "body": "version 1.2\r\n\r\nThe following code:\r\n\r\n```python\r\ntf.contrib.layers.dropout(tf.zeros((100,10)), keep_prob=tf.placeholder_with_default(0.9,(),'dp'))\r\n```\r\n\r\nor \r\n\r\n```python\r\ntf.layers.dropout(tf.zeros((100,10)), 1 - tf.placeholder_with_default(0.9,(),'dp'))\r\n```\r\n\r\nwill fail with following error:\r\n\r\n```txt\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-e27328ef830a> in <module>()\r\n----> 1 tf.contrib.layers.dropout(tf.zeros((100,10)), keep_prob=tf.placeholder_with_default(0.9,(),'dp'))\r\n\r\n~/miniconda3/envs/t12/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    179       current_args = current_scope[key_func].copy()\r\n    180       current_args.update(kwargs)\r\n--> 181     return func(*args, **current_args)\r\n    182   _add_op(func)\r\n    183   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n~/miniconda3/envs/t12/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py in dropout(inputs, keep_prob, noise_shape, is_training, outputs_collections, scope)\r\n   1214                                 noise_shape=noise_shape,\r\n   1215                                 name=sc.name,\r\n-> 1216                                 _scope=sc)\r\n   1217     outputs = layer.apply(inputs, training=is_training)\r\n   1218     return utils.collect_named_outputs(\r\n\r\n~/miniconda3/envs/t12/lib/python3.6/site-packages/tensorflow/python/layers/core.py in __init__(self, rate, noise_shape, seed, name, **kwargs)\r\n    245                **kwargs):\r\n    246     super(Dropout, self).__init__(name=name, **kwargs)\r\n--> 247     self.rate = min(1., max(0., rate))\r\n    248     self.noise_shape = noise_shape\r\n    249     self.seed = seed\r\n\r\n~/miniconda3/envs/t12/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __bool__(self)\r\n    562       `TypeError`.\r\n    563     \"\"\"\r\n--> 564     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n    565                     \"Use `if t is not None:` instead of `if t:` to test if a \"\r\n    566                     \"tensor is defined, and use TensorFlow ops such as \"\r\n\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-e27328ef830a> in <module>()\r\n----> 1 tf.contrib.layers.dropout(tf.zeros((100,10)), keep_prob=tf.placeholder_with_default(0.9,(),'dp'))\r\n\r\n~/miniconda3/envs/t12/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    179       current_args = current_scope[key_func].copy()\r\n    180       current_args.update(kwargs)\r\n--> 181     return func(*args, **current_args)\r\n    182   _add_op(func)\r\n    183   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n~/miniconda3/envs/t12/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py in dropout(inputs, keep_prob, noise_shape, is_training, outputs_collections, scope)\r\n   1214                                 noise_shape=noise_shape,\r\n   1215                                 name=sc.name,\r\n-> 1216                                 _scope=sc)\r\n   1217     outputs = layer.apply(inputs, training=is_training)\r\n   1218     return utils.collect_named_outputs(\r\n\r\n~/miniconda3/envs/t12/lib/python3.6/site-packages/tensorflow/python/layers/core.py in __init__(self, rate, noise_shape, seed, name, **kwargs)\r\n    245                **kwargs):\r\n    246     super(Dropout, self).__init__(name=name, **kwargs)\r\n--> 247     self.rate = min(1., max(0., rate))\r\n    248     self.noise_shape = noise_shape\r\n    249     self.seed = seed\r\n\r\n~/miniconda3/envs/t12/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __bool__(self)\r\n    562       `TypeError`.\r\n    563     \"\"\"\r\n--> 564     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n    565                     \"Use `if t is not None:` instead of `if t:` to test if a \"\r\n    566                     \"tensor is defined, and use TensorFlow ops such as \"\r\n\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n```\r\n\r\n\r\nThe same code run successfully on tf version 1.1.0,\r\n\r\nI check the documents are not changed : \r\n[https://www.tensorflow.org/api_docs/python/tf/contrib/layers/dropout](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/dropout)\r\n\r\n        keep_prob: A scalar Tensor with the same type as x. The probability that each element is kept.\r\n\r\n\r\nIt accepts tensor, so I guess this should be a bug.\r\n\r\n\r\n\r\n", "comments": ["@abelxie Thanks for filing the issue!  I've reproduced the problem, and agree that this looks like a bug.\r\n\r\n@fchollet Can you take a look or suggest someone to fix this?  Also marking \"contributions welcome\", in case anyone in the community would like to contribute to fix this.", "The error seems caused by using Python `min()` and `max()` on a Tensor. I also get he error in TF 1.2 but it is not in the current master as of c98dab03f0de724da81dac8218757207996d1505\r\n\r\nDo you know if we can expect an updated official version soon or do we need to build ourselves?", "@simonkamronn Thanks for tracking down the PR that fixed this!\r\n\r\n@av8ramit Probably knows the schedule for TF 1.3.  In the meantime you'll either need to pull the nightly releases, or build from source yourself.", "@simonkamronn the next official release will be sometime in July. Please pull the nightly release for a fix! Thanks for bringing this to our attention. ", "Thanks for the info @av8ramit!\r\n\r\nClosing this out, since the underlying problem has already been fixed."]}, {"number": 11004, "title": "Update DetectorActivity.java", "body": "darkflow parameter fix", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Sorry, even for trivial changes we need a CLA. Could you sign that?", "Ping @anonym24 ", "Closing due to lack of CLA and no updates in a month."]}, {"number": 11003, "title": "Typo fix in Dataset API's README", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11002, "title": "`TF_AddGradients` incorrectly reports `Add` has no gradient ", "body": "### System information\r\n\r\nOS X with a CPU libtensorflow.so built off master (2336cdf)\r\n\r\n### Describe the problem\r\n\r\n`TF_AddGradients` reports \r\n\r\n```\r\nNo gradient defined for op: Add. Please see https://www.tensorflow.org/code/tensorflow/cc/gradients/README.md for instructions on how to add C++ gradients.\r\n```\r\n\r\nwhen asked to compute dz/dx for z=x+y (see code example). \r\n\r\nand yet `Add` has a [registered gradient](https://github.com/tensorflow/tensorflow/blob/2336cdf7f00434c24f1eb394f255d7de071bdc08/tensorflow/core/ops/math_grad.cc#L372). `Mul` also doesn't work, although gradients of other operations (like `Sin`) do work. \r\n\r\n\r\n### Source code / logs\r\n\r\n```c\r\n#include \"c_api.h\"\r\n\r\nvoid check_status(TF_Status* status) {\r\n  auto code = TF_GetCode(status);\r\n  if (code != TF_OK) {\r\n    cout<<TF_Message(status)<<endl;\r\n    exit(0);\r\n  }\r\n}\r\n\r\nint main() {\r\n  auto graph = TF_NewGraph();\r\n  auto x_desc = TF_NewOperation(graph, \"Placeholder\", \"x\");\r\n  TF_SetAttrType(x_desc, \"dtype\", TF_FLOAT);\r\n  auto status = TF_NewStatus();\r\n  auto x = TF_FinishOperation(x_desc, status);\r\n  auto y_desc = TF_NewOperation(graph, \"Placeholder\", \"y\");\r\n  TF_SetAttrType(y_desc, \"dtype\", TF_FLOAT);\r\n  auto y = TF_FinishOperation(y_desc, status);\r\n  auto z_desc = TF_NewOperation(graph, \"Add\", \"z\");\r\n  TF_AddInput(z_desc, {x, 0});\r\n  TF_AddInput(z_desc, {y, 0});\r\n  auto z = TF_FinishOperation(z_desc, status);\r\n  TF_Output output = {z, 0};\r\n  TF_Output input = {x, 0};\r\n  TF_Output grad;\r\n  TF_AddGradients(graph, &output, 1, &input, 1, NULL, status, &grad);\r\n  check_status(status);\r\n  return 0;\r\n}\r\n\r\n```", "comments": ["@suharshs : Mind taking a look?\r\n\r\n(FYI @skye @iganichev )", "The cc gradients are registered in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients. What you have linked is the gradients for function defs, which do not apply to cc gradients. (This is a point of confusion that we should consolidate).\r\nSo for your specific ops, check that your gradients are in this file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/math_grad.cc\r\n\r\nIf not, please create a pull request or an issue for the desired ops following instructions at, https://www.tensorflow.org/code/tensorflow/cc/gradients/README.md\r\n\r\nThanks!", "Oh I see, `Add` is not actually a registered C++ gradient yet. Sorry for the confusion. ", "No worries, the confusion is completely understandable given the duplicate gradient registries. I'll see if we can solve that issue. Thanks!"]}, {"number": 11001, "title": "Moved tpu_config.RunConfig check to beginning", "body": "This is necessary because `wrapped_model_fn ` calls `_create_infeed_enqueue_ops_and_dequeue_fn ` which requires a TPU Config. Though the actual call is delayed to a later time, it would be better to raise this error earlier.  Also this check is only necessary inside `use_tpu` block. \r\n", "comments": []}, {"number": 11000, "title": "Upgrade to jemalloc 5.0.0", "body": "Contributions welcome! (I won't have bandwidth to do this in the short-term.)", "comments": ["change workspace.bzl:\r\n\r\ntemp_workaround_http_archive(\r\n      name = \"jemalloc\",\r\n      urls = [\r\n          \"http://mirror.bazel.build/github.com/jemalloc/jemalloc/archive/4.4.0.tar.gz\",\r\n          \"https://github.com/jemalloc/jemalloc/archive/4.4.0.tar.gz\",\r\n      ],\r\n      sha256 = \"3c8f25c02e806c3ce0ab5fb7da1817f89fc9732709024e2a81b6b82f7cc792a8\",\r\n      strip_prefix = \"jemalloc-4.4.0\",\r\n      build_file = str(Label(\"//third_party:jemalloc.BUILD\")),\r\n      repository = tf_repo_name,\r\n  )\r\n\r\nto be jemalloc-5.0.0?\r\n\r\nI found https://github.com/jemalloc/jemalloc/archive/5.0.0.tar.gz work.\r\nbut http://mirror.bazel.build/github.com/jemalloc/jemalloc/archive/5.0.0.tar.gz not exist yet.\r\n\r\nIs there other things need to be changed?\r\nThanks", "Yeah, this BUILD file has to be updated as well:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/jemalloc.BUILD\r\n\r\nThe configurations were created by running ./configure on an Ubuntu 14.04 machine and copying it over. If you don't have access to Ubuntu, you can use Docker.", "Please assign to me, Thanks", "update: I already successfully build tensorflow using jemalloc 5.0.0, need a little more time to change jemalloc.BUILD because the jemalloc.h generate method is changed.", "@joshua-xia thanks for your contribution. Any further update on this?", "It looks like jemalloc was [removed](https://github.com/tensorflow/tensorflow/commit/52574f95279d8cd5ec22cfc24668b9586e41367a) from the BUILD files recently, along with other dead configuration options. Is it okay to close this feature request, or would it still be useful to have support?", "https://github.com/tensorflow/tensorflow/blob/master/third_party/jemalloc.BUILD as the build files are removed , closing this issue. Thank you "]}, {"number": 10999, "title": "pass -O options when generating dependency", "body": "In the current implementation, when generating the dependency in [L126](https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl#L216), the optimization option is not passed to nvcc, which makes nvcc to generate a lot of warnings that looks like:\r\n\r\n```\r\n_FORTIFY_SOURCE requires compiling with optimization (-O)\r\n```\r\nThis PR clear these warnings by passing optimization option when generating dependency.\r\n\r\nThe following issues are fixed by this PR: \r\nhttps://github.com/tensorflow/tensorflow/issues/9149\r\nhttps://github.com/tensorflow/tensorflow/issues/2153", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@zasdfgbnm can you clarify a bit more about the purpose of what you are doing here?\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/10999/commits/1bd3bfa8bd9a10c6431d106799d4bada849006fb#diff-04aeac3b8c8dc82fda795b4f8da66489R191 is already setting some options for -O, and 'opt' is passed to the nvcc command line; and we figured it would apply to both the host compiler and nvcc compiler options.  Can you help us out and let us know what specifically is not working as intended, and maybe why too?", "@vrv After looking deeper again into the problem, it seems that I \"fixed\" the problem in a wrong way. I have updated the title, the commits, and description of this pull request. Now the changes are straightforward I think. Thank you very much for the hint.\r\n\r\n@keveman Hopefully this PR should be a small change and can be easy to review now. ", "This makes a lot more sense, thank you for digging deeper! :)\r\n\r\n@tensorflow-jenkins test this please ", "Seems to be unrelated fails?"]}, {"number": 10998, "title": "Updated link to use HTTPS", "body": "Howdy!\r\n\r\nI just updated a link to use https instead of http.\r\n\r\nThanks! ^ _ ^", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10997, "title": "port::AlignedMalloc() may cause a memory leak", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra Version 10.12.5\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0, Darwin, cpu\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n```\r\n# NOTE: Please install Xcode 8.3 first!\r\n$ git clone https://github.com/PerfectlySoft/Perfect-TensorFlow.git\r\n$ cd Perfect-TensorFlow\r\n$ ./test.sh\r\n``` \r\n\r\n### Describe the problem\r\nWhen developing Perfect-TensorFlow, a Swift Class Wrapper of C api [https://gihub.com/PerfectlySoft/Perfect-TensorFlow](https://gihub.com/PerfectlySoft/Perfect-TensorFlow), I found there might be a memory leak when performing such a test:\r\ntesting script: https://github.com/PerfectlySoft/Perfect-TensorFlow/blob/master/Tests/PerfectTensorFlowTests/PerfectTensorFlowTests.swift#L153-L167\r\ncurrently I am using libtensorflow 1.1.0 CPU version on both macOS / Ubuntu 16.04 LTS and found the same leakage on the both systems, hope to get solution once 1.2+ released.\r\n\r\nUsing Xcode - Instruments may find the trace as screen shot below:\r\n<p><img src='https://raw.githubusercontent.com/RockfordWei/upfiler/master/alignedmalloc.leak.png'>AlignedMalloc() leak Screenshot</img></p>\r\n\r\n\r\n### Source code / logs\r\n\r\n[Tensor Creation Testing Script](https://github.com/PerfectlySoft/Perfect-TensorFlow/blob/master/Tests/PerfectTensorFlowTests/PerfectTensorFlowTests.swift#L153-L167)", "comments": ["If I've read the stack trace correctly, it looks like these allocations were made by the `tensorflow::ConstantOp` constructor, which allocates memory for a `tensorflow::Tensor` storing the value of each constant in the graph. That memory should be owned by a `tensorflow::DirectSession` object and freed when you delete the session. Can you confirm whether this happens?", "Sorry I am not sure. Just using C API to do the TF_AllocateTensor() first then call TF_DeleteTensor() once done.", "Hmm, in that case I'm going to close this issue as \"working as intended\" for now. `port::AlignedMalloc()` is used at the lowest level to allocate tensor buffers, but these buffers may be shared with other parts of the system (e.g. stored in the `OpKernel` objects that implement the operations in the graph) in reference-counted `tensorflow::Tensor` objects. As such, it's possible that you could allocated a `TF_Tensor*`, pass it to one of the APIs, call `TF_DeleteTensor()` on it, and the memory would not be freed until later. Many tensors used in a session, such as constants, will only be deleted when the session is deleted.\r\n\r\nIf you find other evidence of a memory leak with this approach, please feel free to reopen the issue!", "Interesting. Actually the calling stack is `TF_NewSession()`, `run()` then `TF_DeleteSession()`, loop for 300+ times, and this chart indicates there is a growing memory usage and Xcode reported `AlignedMalloc` was the only leaky one, where you can see the a liner increase - possibly caused by an incomplete object release.\r\n<p><img src=\"https://github.com/RockfordWei/upfiler/blob/master/growingleaks.png?raw=true\">Suspected Memory Leak Accumulation</img></p>", "Can you share a full allocation backtrace for one of the objects that is leaked *after* the call to `TF_DeleteSession()` and *before* the next call to `TF_NewSession()`? ", "Ah! You're right! After tracing the stack my self, just found some tensors that not released **AFTER** `TF_DeleteSession()`. Just fixed it. This issue can safely close now. Thank you so much!!!", "Great - thanks for confirming!"]}]