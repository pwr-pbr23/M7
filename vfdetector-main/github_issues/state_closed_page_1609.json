[{"number": 4641, "title": "bazel build succeeds, but bazel-mobile install fails", "body": "The build succeeds on OS 10.11.6, with bazel version 0.3.1-homebrew. However, the bazel mobile-install fails. This is not a TensorFlow capitalization problem or a camera permissions issue! \nThanks || \n\nbazel mobile-install //tensorflow/examples/android:tensorflow_demo --verbose_failures\n## The main error is listed below:\n\nERROR: /private/var/tmp/_bazel_kg/16cb3f28d531847b42ceb660896dbffa/external/bazel_tools/tools/android/BUILD:132:1: Converting to Python 3: external/bazel_tools/tools/android/incremental_install.py failed: 2to3 failed: error executing command \n  (cd /private/var/tmp/_bazel_kg/16cb3f28d531847b42ceb660896dbffa/execroot/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfiles/python3/external/bazel_tools/tools/android --write-unchanged-files external/bazel_tools/tools/android/incremental_install.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\n## INFO: Elapsed time: 0.396s, Critical Path: 0.15s\n\nWARNING: Bazel Android NDK crosstools are based on Android NDK revision 11. The revision of the Android NDK given in android_ndk_repository rule 'androidndk' is '12.1.2977051'.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/debug:debug_graph_utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/debug:debug_graph_utils.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/core/BUILD:667:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /Users/kg/tensorflow/tensorflow/examples/android/BUILD:56:12: in srcs attribute of android_binary rule //tensorflow/examples/android:tensorflow_demo: please do not import '//tensorflow/contrib/android:java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java' directly. You should either move the file to this package or depend on an appropriate rule there.\nINFO: Found 1 target...\nERROR: /private/var/tmp/_bazel_kg/16cb3f28d531847b42ceb660896dbffa/external/bazel_tools/tools/android/BUILD:132:1: Converting to Python 3: external/bazel_tools/tools/android/incremental_install.py failed: 2to3 failed: error executing command \n  (cd /private/var/tmp/_bazel_kg/16cb3f28d531847b42ceb660896dbffa/execroot/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfiles/python3/external/bazel_tools/tools/android --write-unchanged-files external/bazel_tools/tools/android/incremental_install.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 0.396s, Critical Path: 0.15s\n\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["My guess is that incremental_install.py is a python2 script, but you are using python3, and the automatic conversion is failing. Does it work if you use python2 instead?\n\nIf you run:\n`bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfiles/python3/external/bazel_tools/tools/android --write-unchanged-files external/bazel_tools/tools/android/incremental_install.py`\ndoes it tell you anything else about the failure?\n\nIf you just want to go ahead and install/run the demo, here's the alternative command sequence:\n\n```\nbazel build -c opt //tensorflow/examples/android:tensorflow_demo\nadb install -r bazel-bin/tensorflow/examples/android/tensorflow_demo.apk\n```\n\nYou can even auto-start the activity with:\n`adb shell am start -n org.tensorflow.demo/.CameraActivity`\n", "Andre - thanks much for the kind note. Running the command you suggested\n\n\"bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfiles/python3/external/bazel_tools/tools/android --write-unchanged-files external/bazel_tools/tools/android/incremental_install.py\" \ndoes not return anything.\n\nI also forgot to add that  the \"adb install..\"  goes through without errors - however, that is when the App crashes on start.\n\nProbably python3 is the culprit - I will check it out and let you know.\n", "What is the runtime error you're seeing? Have you added the model and label files to tensorflow/examples/android/assets/?\n", "Yes, Andrew. I have the model and label (*.txt) files in the assets directory. I am starting over again and will let you know the outcome. Thanks so much! || \n", "Andrew, I started over and the adb install works fine now. Perhaps I was missing the labels (*.txt) file. I am sorry for the inconvenience and will close the issue. Also, thank you so much for your help in resolving this. ||\n"]}, {"number": 4640, "title": "Building TensorFlow for iOS failed on macOS Sierra and xCode 8.0", "body": "When running `compile_ios_protobuf.sh`, it says: \nchecking whether we are cross compiling... configure: error: in '.../tensorflow/tensorflow/contrib/makefile/downloads/protobuf':\nconfigure: error: cannot run C compiled programs.\nIf you meant to cross compile, use `--host'.\nSee`config.log' for more details\n", "comments": ["@satok16 - can you take a look?  Thanks.\n", "Ok, first of all, I need to find a mac running Sierra.\n", "TensorFlow doesn't officially support Sierra, I think.  Marking this as contributions welcome.\n", "I think it's not about running on Sierra. It's built on Sierra but running for iOS.\n", "Actually I can run tensorflow on Sierra using python api without any issues. \n", "Same issue here. Been trying all sorts of things to no avail.\n", "Here are some more details from config.log:\n\n```\nconfigure:3937: checking whether we are cross compiling\nconfigure:3945: gcc -o conftest -DNDEBUG -g -O0 -pipe -fPIC -fcxx-exceptions -mios-simulator-version-min=9.2 -arch x86_64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk  -arch x86_64 -mios-simulator-version-min=9.2 -stdlib=libc++ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk/usr/lib/ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk/usr/lib/system conftest.c -lc++ -lc++abi >&5\nconfigure:3949: $? = 0\nconfigure:3956: ./conftest\ndyld: mach-o, but built for simulator (not macOS)\n./configure: line 3958: 56606 Abort trap: 6           ./conftest$ac_cv_exeext\nconfigure:3960: $? = 134\nconfigure:3967: error: in `/Users/eric/Code/tensorflow/tensorflow/contrib/makefile/downloads/protobuf':\nconfigure:3969: error: cannot run C compiled programs.\nIf you meant to cross compile, use `--host'.\n```\n", "I have the same issue.\n", "Ah good point; I took off the labels.  @satok16 could you please take a look or re-assign if appropriate?  Thanks.\n", "@derekshi Are you using the latest `master` branch ? There is a lot going on regarding iOS. This might help us narrow it down.\n", "yes, I cloned the master branch about 30 hours ago. Unless there are any changes since then.\n", "Same issue. Is there a workaround to build TensorFlow static library?\n", "Downgrading to Xcode 8.0.0 -> 7.3.1 does not seem to fix it.\n", "For me, at least, the `compile_ios_protobuf.sh` script is only failing when compiling for `-arch x86_64`. The other 4 targets - i386, armv7, armv7s, and arm64 - compile without an error.\n", "@derekshi, I am sorry. I am still using El Capitan in my local machine. I have access to a Sierra machine rarely here. So I tested whether the latest `master` is working properly with El Capitan.\n\nCurrently, I have a working environment with XCode8 in El Capitan and can successfully build and run both Simple and Camera examples. Works fine with iOS 10 as well. The branch is a few weeks old and I modified some files a bit. I see that all changes I made are reflected on the master branch so the changes I made shouldn't be a problem.\n\nI downloaded the latest `master` branch today and the only file I see a change is the `download_dependencies.sh`. The bazel link was broken a few weeks back. So what I did is to use the bazel version inside `tensoflow/workspace.bzl` file and hardcoded it in the `download_dependencies.sh` script. However, I now see that the latest `master` corrected the bazel issues. I compiled the latest `master` on El Capitan now and verified the working with Simple example from ios_examples folder. The simulation works as expected.\n\nSo this might be an OS Version dependent issue itself. As @tconkling mentioned, the script is failing only for `x86_64` for him. I will get hold of a Sierra machine and will look into this in the coming days. I am sure that the TensorFlow members are looking into this as well.\n", "As a temporary workaround, if you comment out the X86_64 bits in the build scripts, you can still build the protobuf and tensorflow libraries.  The resultant libraries will work on-device, but attempts to run in the simulator will fail.\n\nhttps://github.com/alphamule/tensorflow/commit/672da4fef637c5511fd2cf78a4ac8e710d0efbda\n", "Hi alphamule,\nCould you give me more detail like which file should be changed and which line to be masked?\n", "@alphamule thanks, that worked for me. @jiny2001 you want to modify ./tensorflow/contrib/makefile/compile_ios_protobuf.sh. Comment out the block that includes line 106  and then the lines containing `${LIBDIR}/iossim_x86_64/lib/libprotobuf.a` at the bottom (in the `lipo` invocations).\n", "You must also make similar changes to `tensorflow/contrib/makefile/compile_ios_tensorflow.sh` to avoid building the X86_64 library and then also to avoid linking into the final artifact.\n\nThis is perhaps an easier split view of the necessary work-around changes:\n\nhttps://github.com/alphamule/tensorflow/commit/672da4fef637c5511fd2cf78a4ac8e710d0efbda?diff=split\n\n@arundasan91 I am also of the opinion that this is an OS issue.  My best guess is that, in the upgrade to Sierra, now when you link against the ios simulator libraries (sorry if this is the wrong nomenclature---I'm new to iOS dev) the resulting binary is not executable, whereas I presume it was in El Cap.  Even using old simulator versions does not fix the problem.  When I compile the same test binary as `./configure` but don't use the simulator for `-isysroot`, the binary runs fine.\n\nThe only links to the mach-o error message from `config.log` I could find are:\n\nhttps://curl.haxx.se/mail/lib-2016-09/0079.html\nhttp://stack1.ocim.top/39762289/issue-with-executing-a-out-file-c-macos-sierra-10-12-1.html\nhttp://stackoverflow.com/questions/39762289/issue-with-executing-a-out-file-c-macos-sierra-10-12-1\n\nBut none seem to have satisfactory answers here.\n", "Thank you so much @tachim and @alphamule !\nFollowing those changes I could go advance.\n", "i did the changes to the code, my build ended with the following\n\nfatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: Usage: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo [input_file] ... [-arch <arch_type> input_file] ... [-info] [-detailed_info] [-output output_file] [-create] [-arch_blank <arch_type>] [-thin <arch_type>] [-remove <arch_type>] ... [-extract <arch_type>] ... [-extract_family <arch_type>] ... [-verify_arch <arch_type> ...] [-replace <arch_type> <file_name>] ...\n\nany help is appreciated\n", "@usamaoDATA \n\nI'm going to guess that you tried to comment out the lines in the lipo commands.  You must delete those lines instead.  The problem is that the lipo command is multi-line:\n\n```\nlipo \\\n  ${LIBDIR}/iossim_386/lib/libprotobuf-lite.a \\\n  # ${LIBDIR}/iossim_x86_64/lib/libprotobuf-lite.a \\\n  ${LIBDIR}/ios_arm7/lib/libprotobuf-lite.a \\\n  ${LIBDIR}/ios_arm7s/lib/libprotobuf-lite.a \\\n  ${LIBDIR}/ios_arm64/lib/libprotobuf-lite.a \\\n  -create \\\n  -output ${LIBDIR}/libprotobuf-lite.a\n```\n\nso if you comment out a line in the middle, the `\\` characters at the end cause all these lines to be considered one, so it ends up being interpreted as:\n\n`lipo ${LIBDIR}/iossim_386/lib/libprotobuf-lite.a # ${LIBDIR}/iossim_x86_64/lib/libprotobuf-lite.a ${LIBDIR}/ios_arm7/lib/libprotobuf-lite.a ${LIBDIR}/ios_arm7s/lib/libprotobuf-lite.a ${LIBDIR}/ios_arm64/lib/libprotobuf-lite.a -create -output ${LIBDIR}/libprotobuf-lite.a`\n\nwhich means the trailing command line arguments are considered commented out as well, so you get the error you saw about missing required options.\n\nSo, just delete those lines instead & it should work.\n", "hi guys, I follow @alphamule's guide, however, i got the same error as usamaoDATA:\n\n```\n.././install-sh -c -d '/Users/kduy/temp/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm64/include/google/protobuf/compiler/js'\n /usr/bin/install -c -m 644  google/protobuf/compiler/js/js_generator.h '/Users/kduy/temp/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm64/include/google/protobuf/compiler/js'\n+ lipo /Users/kduy/temp/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386/lib/libprotobuf.a\n\nerror: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: one of -create, -thin <arch_type>, -extract <arch_type>, -remove <arch_type>, -replace <arch_type> <file_name>, -verify_arch <arch_type> ... , -info or -detailed_info must be specified\n\nfatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: Usage: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo [input_file] ... [-arch <arch_type> input_file] ... [-info] [-detailed_info] [-output output_file] [-create] [-arch_blank <arch_type>] [-thin <arch_type>] [-remove <arch_type>] ... [-extract <arch_type>] ... [-extract_family <arch_type>] ... [-verify_arch <arch_type> ...] [-replace <arch_type> <file_name>] ...\n```\n\nI guess the problem from xcode. Have any of you seen it before ? Thanks\n", "This is the same error as @usamaoDATA and I described the solution in the response just before yours.  Make sure that you are deleting the lines for the lipo commands and not just commenting them out.\n", "@alphamule : thank you. I really did so as  alphamule@672da4f?diff=split . Perhaps my xcode causes the error\n", "Yeah, it's not Xcode, as you're still on the step before you can use Xcode (building the libraries).\n\nDid you maybe misinterpret the deleted lines as empty lines?\n\nIt needs to look like this:\n\n```\nlipo \\\n${LIBDIR}/iossim_386/lib/libprotobuf-lite.a \\\n${LIBDIR}/ios_arm7/lib/libprotobuf-lite.a \\\n${LIBDIR}/ios_arm7s/lib/libprotobuf-lite.a \\\n${LIBDIR}/ios_arm64/lib/libprotobuf-lite.a \\\n-create \\\n-output ${LIBDIR}/libprotobuf-lite.a\n```\n\nnot\n\n```\nlipo \\\n${LIBDIR}/iossim_386/lib/libprotobuf-lite.a \\\n\n${LIBDIR}/ios_arm7/lib/libprotobuf-lite.a \\\n${LIBDIR}/ios_arm7s/lib/libprotobuf-lite.a \\\n${LIBDIR}/ios_arm64/lib/libprotobuf-lite.a \\\n-create \\\n-output ${LIBDIR}/libprotobuf-lite.a\n```\n\nOr else, maybe did you remove one of those trailing backslashes?\n\nFrom your error logs, I can tell that you are running the command:\n\n```\n+ lipo /Users/kduy/temp/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386/lib/libprotobuf.a\n```\n\nwhich means that the second half of the command is getting cut off.\n", "Thank you so much, @alphamule . It's my bad, i left one of the empty line in between. \n", "@alphamule yes i deleted those lines and i still got that error, i googled it and apparently you have to enable a setting in Xcode build settings to only build the current architecture. and then the build completed successfully. i dont know if this was a fluke or some other random error. but i was very skeptical as the build settings are only edited for the current project opened in xcode. but never the less it worked and i was able to get the the app running for my custom model i created on the following devices, ipad mini 2 running ios 9.3.3, iphone 4s running 9.3.5 and a iphone 6s running iOS10\nim still very new to this so please forgive me if i made some wrong assumptions \n", "This makes sense, as we cut out the simulator architecture when building the library.  If you tried to build your app against the library for the simulator architecture, then it will fail.  I believe that, by default, Xcode won't build for all architectures, but it will if you switch your scheme's build configuration to be release rather than debug (at least that's how it was in my Xcode).\n", "Any progress on this issue? Workaround works but I can no longer build for the Simulator, where I test screen sizes for devices I don't have.\n", "@arundasan91 any update on building this properly for `x86_64` on Sierra and Xcode 8?\n", "@levelvc , No, Sorry. I did not get my hands on one. My local machine in lab is still El Capitan and I haven't updated yet because of the issues.\n\nI just asked a friend of mine for his machine. I might be able to update back in a few hours.\n", "I ran into some errors in my friends' machine. Had little time to debug. Will look into it hopefully by the end of this week. I was in a hurry so just told him to copy the log and send it over to me.\nTake a look: [error](http://pastebin.com/re6WUua5).    \n\nI did download all the dependencies. This might be an issue with his system though. Sorry for being less of a help guys.\n", "Thanks, I'm also having this error but the workaround fixes it for me\n", "@KevinAyuque The workaround lets you run on a device but you can't do anything with a simulator. @arundasan91 any update?\n", "We were able to build on Sierra without arm64 architecture but for production we ended up building on El Capitain. \n", "@arundasan91 That error is different and it's caused by missing libtool. Running `brew install libtool` should fix it.\nIf you are working on a fresh machine, you should also have automake and xcode command line tools installed (I recently ran into these problems).\n\nI am also seeing the original error on Sierra and the config.log from my machine says this:\n\n> configure:3937: checking whether we are cross compiling\n> configure:3945: gcc -o conftest -DNDEBUG -Os -pipe -fPIC -fno-exceptions -mios-simulator-version-min=8.2 -arch x86_64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk  -arch x86_64 -mios-simulator-version-min=8.2 -stdlib=libc++ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk/usr/lib/ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.0.sdk/usr/lib/system conftest.c -lc++ -lc++abi >&5\n> configure:3949: $? = 0\n> configure:3956: ./conftest\n> dyld: mach-o, but built for simulator (not macOS)\n> ./configure: line 3958: 79140 Abort trap: 6           ./conftest$ac_cv_exeext\n> configure:3960: $? = 134\n> configure:3967: error: in '/Users/sorinlumezeanu/_root/work/cvsl/WorkPhotos/tensorflow/tensorflow/contrib/makefile/downloads/protobuf':\n> configure:3969: error: cannot run C compiled programs.\n> If you meant to cross compile, use `--host'.\n> See`config.log' for more details\n\n[Here](http://pastebin.com/Gmr79pQk) is the full config.log file. \n", "The solution that @alphamule posted worked for me.\n", "@alphamule `s solution is work for me too.\n", "With @Gubarev 's commit, it looks like we now build properly in Sierra.  But, I get a weird runtime error now when I try to run the benchmark or the simple app in the simulator (I'm assuming the camera app is failing because it can't get camera input in the simulator).  Benchmark crashes with this output:\n\n```\nI tensorflow/tensorflow/contrib/ios_examples/benchmark/BenchmarkViewController.mm:198] Session created.\nI tensorflow/tensorflow/contrib/ios_examples/benchmark/BenchmarkViewController.mm:201] Graph created.\nI tensorflow/tensorflow/contrib/ios_examples/benchmark/BenchmarkViewController.mm:207] Creating session.\nI tensorflow/core/util/stat_summarizer.cc:33] StatSummarizer found 370 nodes\nF tensorflow/tensorflow/contrib/ios_examples/benchmark/../../../../tensorflow/core/lib/core/refcount.h:79] Check failed: ref_.load() == 0 (1 vs. 0)\n(lldb) \n```\n\nThis is on latest master and I made sure to re-run `tensorflow/contrib/makefile/download_dependencies.sh`\n\nIs this happening for anyone else?\n", "@alphamule \r\nI just have built latest TF from scratch (c7edafccc793bf87e29aaec90db64471a7a4bb02). I have not tried example camera app, but I have tried to build and run my own app: it works both for ARM64 iOS device (iPad Air 2) and x86_64 Simulator (iPhone 5s) without problems.", "I tried all recipes above but stuck having build error:\r\n\r\n`Making clean in .\r\n/bin/sh: /Volumes/Macintosh: No such file or directory\r\nmake: *** [clean-recursive] Error 1\r\n`\r\n\r\nSo, I did fix that by changing path to my Xcode not to have delimiters like spaces:\r\n\r\n`/Volumes/Macintosh HD/Xcode 8.2.app -> /Volumes/Macintosh-HD/Xcode.app `\r\n\r\nand now I can succesfully build:\r\n\r\n`tensorflow/contrib/makefile/compile_ios_protobuf.sh `\r\n\r\n\r\n", "@QoT Seems like your XCode location is not default. Usually it is installed via AppStore into /Applications/Xcode.app", "@QoT That is a typical space issue of the path. Try to link to a path without any space or round the path in the script up with quotes. ", "I am iOS developer and I typically have more than one versions on my system. Usually I call them like \"Xcode 7.3\" or \"Xcode 8.2\" without any problem.\r\n\r\nBut obvious some scripts have a problem with path what I did solve by excluding spaces as I did mention.", "@alphamule's suggestion ended up working for me too. Given that you would be running the camera project on device commenting out those lines wouldnt affect your work much I guess. ", "I modified the camera sample to let it run on photos and succeeded to build library in x86_64 on Sierra by following workaround. Also confirmed the library works in iPhone simulator.\r\n(By checking config.log, I found that \"--enable-cross-compile\" option is likely to become outdated on Sierra. To enable cross-compile, specify \"--build\" and \"--host\" with different value worked.)\r\n\r\n```sh\r\n\u279c  makefile git:(r0.10) \u2717 ln -s /usr/bin/llvm-gcc /usr/local/bin/x86_64-apple-darwin15.0.0-gcc\r\n\r\n\u279c  makefile git:(r0.10) \u2717 diff -u compile_ios_protobuf.sh compile_ios_protobuf.sh_bkp\r\ndiff -u compile_ios_protobuf.sh_bkp compile_ios_protobuf.sh\r\n--- compile_ios_protobuf.sh_bkp\t2017-02-14 10:57:47.000000000 +0900\r\n+++ compile_ios_protobuf.sh\t2017-02-14 15:39:18.000000000 +0900\r\n@@ -38,7 +38,9 @@\r\n LIBDIR=${GENDIR}lib\r\n mkdir -p ${LIBDIR}\r\n \r\n-OSX_VERSION=darwin14.0.0\r\n+# OSX_VERSION=darwin14.0.0\r\n+OSX_VERSION=darwin15.0.0\r\n+OSX_VERSION2=darwin16.4.0\r\n \r\n IPHONEOS_PLATFORM=`xcrun --sdk iphoneos --show-sdk-platform-path`\r\n IPHONEOS_SYSROOT=`xcrun --sdk iphoneos --show-sdk-path`\r\n@@ -100,7 +102,7 @@\r\n make distclean\r\n ./configure \\\r\n --build=x86_64-apple-${OSX_VERSION} \\\r\n---host=x86_64-apple-${OSX_VERSION} \\\r\n+--host=x86_64-apple-${OSX_VERSION2} \\\r\n --disable-shared \\\r\n --enable-cross-compile \\\r\n --with-protoc=\"${PROTOC_PATH}\" \\\r\n```\r\n\r\nAlthough actually doesn't affect the build process, version values are picked from\r\n\"15.0.0\" => \"brew info gcc; ls /usr/local/Cellar/gcc/*/bin\"\r\n\"16.4.0\" => \"uname -r\"", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Hi,\r\nI have the same problem executing the script _compile_ios_protobufs.sh_.\r\n\r\nI got this error:\r\n`Making distclean in third_party/googletest\r\nmake[1]: *** No rule to make target 'distclean'.  Stop.\r\nmake: *** [distclean-recursive] Error 1`\r\n\r\nI am using **MacOS HighSierra 10.13.6** and the latest TensorFlow from master branch.\r\n\r\nDoes anybody have found a solution?", "@DonPex Try the solution by @carloscabanero on: https://gist.github.com/BennettSmith/9487468ae3375d0db0cc .", "I've forked off Bennett's script and fixed a few things (like wrong x86_64-simulator config args, missing multi-core build, disabling archs you don't need actually works now), bumped the protobuf version to 3.7.0. I've spent two days trying to make it work on my machine (macOS 10.15.3, Xcode 11.3.1), hate protobuf more than ever. Anyways, check my script out:\r\nhttps://github.com/witaly-iwanow/build-scripts"]}, {"number": 4639, "title": "Implement NumPy style boolean indexing", "body": "See NumPy's documentation\nhttp://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#boolean-array-indexing\n\nWe currently support basic indexing http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#basic-slicing-and-indexing.\n\nBroken off of #206.\n", "comments": ["This would still be a great feature. Is anyone working on it?", "Nobody is working on it, and I will likely not be able to in the near term. Marking as contributions welcome.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "This is now possible in tf.experimental.numpy as \r\n```\r\n>>> import tensorflow.experimental.numpy as np\r\n>>> np.array([1,2,3,4])[np.array([False,True,True,False])]\r\n<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 3])>\r\n>>> a=np.array([1,2,3,4])\r\n>>> a>2\r\n<tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False,  True,  True])>\r\n>>> a[a>2]\r\n<tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 4])>\r\n```\r\n"]}, {"number": 4638, "title": "Implement advanced indexing (and mixed basic/advanced)", "body": "NumPy style advanced indexing is documented here http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing\n\nWe currently support basic indexing http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#basic-slicing-and-indexing\nusing StridedSlice.\n\ne.g.\n\nfoo = Some tensor\nidx1=[1,2,3]\nidx2=[3,4,5]\nfoo[idx1, idx2, 3]\n", "comments": ["Broken off of #206 \n", "I mentioned this in #206, but I wanted to remind again that mixed indexing with slices/arrays has some really [strange/unpredictable behavior](https://github.com/tensorflow/tensorflow/issues/206#issuecomment-221439797) for the order of result axes in NumPy. So it might be better to hold off on implementing that in TensorFlow until we're sure we're doing it right.\n", "Yes, I am aware of how the mixed advanced/basic indexing is super nonintuitive. I could break the mixed indexing into its own issue.\n", "idx1/idx2 in your example could be Tensor objects, right?\n", "does .10 currently support negative indexing? as in X[:,-1] for example?\n", "@yaroslavvb:  yes, `idx1` and`idx2` can be tensor objects.  even in what is already implemented for baisc indexing, you can do  \n\n``` pythhon\na=tf.constant(3); \nb=tf.constant(6);\nfoo[a:b]\n```\n\n@robsync: negative indices have a bug in 0.10 and work in 0.11rc0 \n", "@aselle: I saw in #206 according to your 7/12 comment that lvalue basic indexing has been implemented. But what does that mean? Because whenever I do any kind of lvalue indexing\n\ne.g.\n\n``` python\nfoo = Some tensor\nfoo[:3] = tf.zeros(3)\n```\n\nI get `TypeError: 'Tensor' object does not support item assignment.` \nThanks.\n", "@Fenugreek Tensors are immutable. But you can do this sort of thing if `foo` is a `tf.Variable` by writing `foo[:3].assign(tf.zeros(3))`\n", "...and making sure that it gets executed:\n\n``` python\nwith tf.control_dependencies([foo[:3].assign(tf.zeros(3))]):\n    foo = tf.identity(foo)\n```\n", "Great, thanks @shoyer and @danijar . I am past that problem now, but have another error with the basic indexing:\n\n``` python\n        print pool.get_shape(), args.get_shape(), values.get_shape()\n        tf.assign(pool[args], values)\n```\n\ngives\n\n```\n(1024000,) (512000,) (512000,)\nTraceback (most recent call last):\n...\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1644, in _DelegateStridedSliceShape\n    return common_shapes.call_cpp_shape_fn(op, input_tensors_needed=[1, 2, 3])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 596, in call_cpp_shape_fn\n    raise ValueError(err.message)\nValueError: Shape must be rank 1 but is rank 2\n```\n\nThere are no rank 2 shapes anywhere, so I am confused. The same error results even if I just do a `return pool[args]`, so has nothing to do with the assignment but with the basic indexing itself. Thanks again.\n", "I'm having a similar error to @Fenugreek:\r\n\r\n```\r\nA = tf.constant([[1,2],[3,4],[5,6]])\r\nid_rows = tf.constant([0,2])\r\nA[id_rows, :]\r\n```\r\n\r\ngives an error:\r\n\r\n```\r\nValueError: Shape must be rank 1 but is rank 2 for 'strided_slice' \r\n(op: 'StridedSlice') with input shapes: [3,2], [1,2], [1,2], [1].\r\n```\r\n\r\nThis is using version 1.0.0. Thanks.", "You need to manually broadcast it i.e.   `values.get_shape()` must match `pool[args].get_shape`. This is a limitation in the current implementation.\r\n", "@aselle  Can you elaborate more on how to manually broadcast to make it work ? Thanks.", "StackOverflow would be a great place to ask for tips on how to use\nbroadcasting to make gather_nd work\n\nOn Mon, Apr 3, 2017 at 6:56 PM, Kublai-Jing <notifications@github.com>\nwrote:\n\n> @aselle <https://github.com/aselle> Can you elaborate more on how to\n> manually broadcast to make it work ? Thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4638#issuecomment-291370769>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABKS1tCXk0PpQctDC7VHNcWE18soT37sks5rsaNkgaJpZM4KJUyn>\n> .\n>\n", "I posted the issue to SO: https://stackoverflow.com/questions/44793286/slicing-tensorflow-tensor-with-tensor.", "@itsmeolivia why was the issue closed? It doesn't seem like the example in the OP works.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I'm asking why the ticket was closed given the op was in fact a feature. Was it implemented? Or decided to not implement. There is no info in the ticket as to why it was closed. ", "@aselle probably want to keep this open right?", "I have realize the advanced indexing like the same way in numpy. Please refer to:\r\nhttps://github.com/SpinachR/ubuntuTest/blob/master/beautifulCodes/tensorflow_advanced_index_slicing.ipynb", "Is there any plan to implement this in TensorFlow?", "any progress?", "cc @asimshankar ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@aselle do you know if we have plan to implement this? If not, should we mark this as contribution welcome?", "CC @saxenasaurabh ", "use gather to do advanced index need much more code than numpy/pytorch, and make code hard to read. please implement this if possible.\r\nI have implemented a [simple function](https://gist.github.com/traveller59/02695ba2ff9ada5a12b0e842174062bf) to do some numpy-style advanced indexing based on tf.gather_nd and tf.transpose.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@saxenasaurabh, could you take this issue over?", "@saxenasaurabh, could you please take this issue over?", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@aselle Is there active development on this front or is using tf Eager the less painful way to go?\r\n\r\nThanks!", "@gokul-uf - Nope, this is not being actively developed at this time. Not quite sure if using eager helps you either. You could easily convert a Tensor to numpy and use advanced indexing, but if you want to compute gradients through that indexing, that won't work.", "On a related note, it might be worth reviewing our proposal for how to\nchange indexing behavior in NumPy moving forward:\nhttp://www.numpy.org/neps/nep-0021-advanced-indexing.html\nOn Thu, Jul 26, 2018 at 10:44 AM Asim Shankar <notifications@github.com>\nwrote:\n\n> @gokul-uf <https://github.com/gokul-uf> - Nope, this is not being\n> actively developed at this time. Not quite sure if using eager helps you\n> either. You could easily convert a Tensor to numpy and use advanced\n> indexing, but if you want to compute gradients through that indexing, that\n> won't work.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4638#issuecomment-408177756>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABKS1lwYXKQeWKJ36lMhrDG_rxT9eTAtks5uKf_jgaJpZM4KJUyn>\n> .\n>\n", "> @Fenugreek Tensors are immutable. But you can do this sort of thing if `foo` is a `tf.Variable` by writing `foo[:3].assign(tf.zeros(3))`\r\n\r\nI have been trying to replace one of the `tf.py_func`. Please refer [this](https://github.com/google/seq2seq/issues/354) for clear description of the problem.\r\n\r\nI'm stuck on replacing tensor's values which is created as an Object and not a Variable.\r\nAny workarounds possible for this case?\r\n\r\nSimilar Stack Overflow [Question](https://stackoverflow.com/questions/55158952/convert-tf-py-func-to-native-tensorflow-operation)", "Is it doable something like this in tensorflow:\r\n\r\n`out[a2[y], x[:, None]] = alp[a2[y], x[:, None]]`\r\n\r\nplease refer to this [post](https://stackoverflow.com/questions/56568981/apply-where-condition-on-3d-tensor-in-tensorflow) for reproducble example.\r\n\r\nThanks", "This issue is way to broad and stale. However, anyone needing advanced indexing functionality should consider the excellent tf.experimental.numpy` module by @wangpengmit "]}, {"number": 4637, "title": "Documentation updates for 0.11 release.", "body": "", "comments": ["The new typos are unfortunate, but oh well. Ignoring the build.\n"]}, {"number": 4636, "title": "tf.contrib.learn Quickstart: float64 Warning", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nFedora release 23 (Twenty Three)\n\nInstalled version of CUDA and cuDNN: \n\nGPU does not support computation :. no CUDA\nNVIDIA Corporation GM107 [GeForce GTX 745]\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nls: No match.\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n   Linux CPU Python 2.7\n   https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n1. Copy Past code from tf.contrib.learn Quickstart\n   https://www.tensorflow.org/versions/r0.10/tutorials/tflearn/index.html#tf-contrib-learn-quickstart\n2. Run the Code\n3. observe the float64 Warnings.  \n   Note:  replace 'load_csv()' with 'load_csv_with_header()' produces the correct Prediction. but float64 warnings remain.\n\nIs there a way to work around this and remove the warnings?\n### What other attempted solutions have you tried?\n\nTried \nin 'training_set' and 'test_set'\ntarget_dtype = np.int32 ; np.float32; tf.int32; tf.float32\n\nin 'new_samples'\ndtype = np.int32 ; np.float32; tf.int32; tf.float32\n\nfeature_columns = tf.cast(feature_columns, tf.float32)\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n$ python confErr.py\nWARNING:tensorflow:load_csv (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed after 2016-09-15.\nInstructions for updating:\nPlease use load_csv_{with|without}_header instead.\nWARNING:tensorflow:load_csv (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed after 2016-09-15.\nInstructions for updating:\nPlease use load_csv_{with|without}_header instead.\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\nWARNING:tensorflow:Given features: Tensor(\"input:0\", shape=(?, 4), dtype=float64), required signatures: TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False).\nWARNING:tensorflow:Given targets: Tensor(\"output:0\", shape=(?,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False).\nAccuracy: 0.966667\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\nPredictions: [1 1]\n", "comments": ["Thanks for the report, @qweelar. Looking into this issue with the tutorial code.\n", "Hi @qweelar, the float64 warning is due to a bug with the load_csv_with_header function that was fixed in commit b6813bd. This fix isn't in TensorFlow release 0.10, but should be in the next release.\n\nIn the meantime, for the purposes of the tf.contrib.learn quickstart, you can safely ignore the float64 warning.\n\n(Side note: In terms of the other deprecation warning, I will be updating the tutorial code to use load_csv_with_header, and will update this issue when that's in place.)\n", "Cool thanks for the updates @sandersk \n"]}, {"number": 4635, "title": "Performance and memory usage improvements", "body": "Is the tensorflow computational graph a good working unit for scaling the performance? Do we need an another layer of Data Flow that can help do the resource management efficiently?\n", "comments": ["I am doing a deep analysis of tensorflow under different ML algorithms and computing frameworks. The goal is to understand the performance bottlenecks under different environments.  I asked the previous question in this context.\n", "I don't understand what it is you're asking -- could you narrow down the scope, and rephrase, please?\n", "I can rephrase the question. Do you think the tensorflow computational graphs are good enough to manage  resource efficiently? My current understanding is that it is not. I put this question to get second opinion from the tensorflow community. May be someone has strong argument/proof of work to negate my understanding.\n", "I don't know what \"resource\" specifically refers to in this context.  Feel free to email me if you have a writeup / report to share, and I can discuss it with the team.  For discussions of this nature, I'd suggest to use the [mailing list](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss) instead of Github, which is for issues and bugs.\n\nThanks!\n", "By \"resource\", I mean memory and core management etc. \n"]}, {"number": 4634, "title": "Fix stddev computation in TensorBoard examples. (#4054)", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@badesairam, thanks for your PR! By analyzing the annotation information on this pull request, we identified @petewarden, @danmane and @tensorflower-gardener to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "This is already fixed in the latest master release.\n"]}, {"number": 4633, "title": "Resolve python 3.5's urllib dependency in abalone-tutorial.", "body": "Replacing urllib with six.moves.urllib enables this tutorial for Python 3.5.\n", "comments": ["@jusjusjus, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener to be a potential reviewer\n", "Can one of the admins verify this patch?\n", "Jenkins, test this please\n"]}, {"number": 4632, "title": "Random Uniform distribution cannot have Batch as a dimension", "body": "`tf.random_uniform((None, 10))`\nResults in an error \nTypeError: Expected binary or unicode string, got (None, 10)\n\nLikewise\n`tf.random_uniform((-1, 10))`\nResults in an error\nValueError: Dimension -1 must be >= 0\n\nAlthough you could pass the random matrix in as a placeholder, this is awkward..\n", "comments": ["The `shape` argument to `tf.random_uniform()` can be any arbitrary 1-D tensor, so you can use a `tf.placeholder()` as the shape (or for some component of the shape):\n\n``` python\nshape = tf.placeholder(tf.int32, shape=[None])  # `shape` is a 1-D tensor.\nrandom_t = tf.random_uniform(shape)\n\nsess.run(random_t, feed_dict={shape: [5, 10]})\nsess.run(random_t, feed_dict={shape: [3, 37]})\n\n# Or, if you just want to vary the 0th dimension, which let's call `batch_size`...\nbatch_size = tf.placeholder(tf.int32, shape=[])  # `batch_size` is a scalar (0-D tensor).\nrandom_t = tf.random_uniform([batch_size, 10])\n\nsess.run(random_t, feed_dict={batch_size: 5})\nsess.run(random_t, feed_dict={batch_size: 17})\n```\n", "I am having a similar issue but while trying to add dropout after my convolutional layers. The dropout documentation describes that noise_shape can have a shape like noise_shape = [k, 1, 1, n], but when this gets passed to random_uniform() in the dropout code I get the following error: \"ValueError: Shape must be rank 1 but is rank 4 for 'conv_dropout/random_uniform/RandomUniform' (op: 'RandomUniform') with input shapes: [10,1,1,36].\"\r\n\r\nHere is the relevant part of the code:\r\n```\r\n        noise_shape = tf.placeholder(dtype=tf.int32, shape=[batch_size, 1, 1, output.get_shape().as_list()[3]])\r\n        output_drop = tf.nn.dropout(x=output, keep_prob=0.7, noise_shape=noise_shape, name='conv_dropout')\r\n```\r\nShould I be defining or passing noise_shape differently?", "@hannah-rae I think so. Try the following:\r\n\r\n```python\r\n        noise_shape = tf.stack([batch_size, 1, 1, tf.shape(output)[3]])\r\n```\r\n", "Hooray! That worked. Thank you \ud83d\udc4d ", "@el3ment:  you need to use square brackets for shape instead of (,) \r\n\r\nExample: \r\n\r\nshape=[2,2]\r\nu=tf.random_uniform(shape,minval=0.,maxval=1., dtype= tf.float32)\r\nprint(sess.run(u))\r\n\r\n[[ 0.47149575  0.78796875]\r\n [ 0.76750052  0.41252744]]\r\n\r\n\r\n\r\n", "I have a similar problem:\r\n\r\nI want to create a random tensor of the same shape as the input my network. The input however has variable length, and is supplied by a queue. If I use the placeholder to pass the shape values, I have to evaluate the current input batch. The problem is that when I generate the input batch using sess.run then the queue will advance one step, and therefore the actual input to the network will be next batch, with a different size. \r\n\r\nAny idea how to solve this? ", "Actually, never mind:  tf.shape(input) worked, no placeholder needed. I'm curious, why input.shape didn't work?", "> I have a similar problem:\r\n> \r\n> I want to create a random tensor of the same shape as the input my network. The input however has variable length, and is supplied by a queue. If I use the placeholder to pass the shape values, I have to evaluate the current input batch. The problem is that when I generate the input batch using sess.run then the queue will advance one step, and therefore the actual input to the network will be next batch, with a different size.\r\n> \r\n> Any idea how to solve this?\r\n\r\nI'm having essentially the same problem. Did you find a solution?", "> Actually, never mind: tf.shape(input) worked, no placeholder needed. I'm curious, why input.shape didn't work?\r\n\r\nI think `tf.shape(input)` returns a tensor whose content can be determined later while `input.shape` returns an ordinary tuple and \"dynamic\" dimensions will have a `None` there."]}, {"number": 4631, "title": "GridRNN cell uses tuples for output and states", "body": "In response to this (https://github.com/tensorflow/tensorflow/issues/2560) and recent changes in LSTMCell, this PR added `state_is_tuple=True` and `output_is_tuple=True` into the constructor of GridRNNCell:\n\n```\n> cell = tf.contrib.grid_rnn.Grid2LSTMCell(3, use_peepholes=True)\n> cell.state_size\n       (LSTMStateTuple(c=3, h=3), LSTMStateTuple(c=3, h=3))\n> cell.output_size\n       (3,)\n```\n\nThis means there are 2 LSTM cells in the grid, and the state of each cell has size of `(c=3, h=3)`. There is only one output dimension, whose size is 3.\n\nIn contrast:\n\n```\n> cell = tf.contrib.grid_rnn.Grid2BasicRNNCell(3)\n> cell.state_size\n     (3, 3)\n> cell.output_size\n     (3,)\n```\n\nThis means there are 2 BasicRNN cell in the grid, and the state of each cell has size of 3. There is only one output dimension, whose size is 3.\n\nPrevious behaviour is maintained by using `state_is_tuple=False, output_is_tuple=False` when creating the cell.\n\n```\n> cell = tf.contrib.grid_rnn.Grid2LSTMCell(3, use_peepholes=True, state_is_tuple=False, output_is_tuple=False)\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2LSTMCell object at 0x10de68f50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2LSTMCell** object at 0x10de68f50>: Using a concatenated output is slower and will soon be deprecated.  Use output_is_tuple=True.\n\n> cell.state_size\n    12\n> cell.output_size\n    3\n\n> cell = tf.contrib.grid_rnn.Grid2BasicRNNCell(3, state_is_tuple=False, output_is_tuple=False)\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2BasicRNNCell object at 0x10de4b950>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2BasicRNNCell object at 0x10de4b950>: Using a concatenated output is slower and will soon be deprecated.  Use output_is_tuple=True.\n\n> cell.state_size\n   6\n> cell.output_size\n   3\n```\n\nThis also fixes https://github.com/tensorflow/tensorflow/issues/4296\n", "comments": ["Can one of the admins verify this patch?\n", "@phvu, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ilblackdragon and @ebrevdo to be potential reviewers\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please\n", "@ebrevdo do you give an LGTM?\n", "there are many changes in this code.  before a more thorough review, can you tell me if this is fully backwards compatible with the original code?  if not, how?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Sorry I screwed up this PR (lack of water and sleep apparently). Will close this and submit another PR, with responds to your comments.\n"]}, {"number": 4630, "title": "fix issues in \"nn_layer()\" and \"cross_entropy\"", "body": "1. the act argument in nn_layer() didn't work, I remove it, and assign actions singly.\n2. the cross entropy was not correct.\n", "comments": ["Can one of the admins verify this patch?\n", "@ziliwang, thanks for your PR! By analyzing the annotation information on this pull request, we identified @danmane, @keveman and @nsthorat to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Thanks for the change! These are definitely both issues in the 0.10 release, and they're fixed in the latest master:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n\nWe're not doing anymore 0.10 releases, and 0.11rc0 will be out soon which will pick up those fixes.\n"]}, {"number": 4629, "title": "C++ ShapeInference / ShapeRefiner use without PlaceHolder/ConstantOp support", "body": "The ShapeInference for C++ engine doesn't support the PlaceholderOp.  Is this because it isn't finished, or am I missing something?\n\nIt seems to be that you can't add a node to the shape inference engine without having added its dependencies.  Since ConsantOp and PlaceholderOp do not have a shape inference functions, I don't understand how the ShapeRefiner can have any nodes added to it.\n", "comments": ["Both the [Placeholder](https://github.com/tensorflow/tensorflow/blob/38adca7ebb40b8cd56b841af2f6b8a44c71f2a82/tensorflow/core/ops/array_ops.cc#L2359) and [Const](https://github.com/tensorflow/tensorflow/blob/38adca7ebb40b8cd56b841af2f6b8a44c71f2a82/tensorflow/core/ops/array_ops.cc#L335) ops do have shape inference functions registered. Thus, they can be added to the ShapeRefiner.\n\nClosing this issue, feel free to re-open if I have misunderstood your question.\n", "Hi.  I was mistaken.  I am making a new device and I am interested in knowing about the shape of tensors flowing through the device when the device is handed the graph (perhaps a better name is the subgraph).\n\nIn this graph, constants and placeholders have been replaced by _Recv nodes, which do not have shape inference functions - this lead to my mistake.\n"]}, {"number": 4628, "title": "Compiled static library has different name than given in README.md", "body": "At url: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile in Building on Linux section of README.md file, when we run `tensorflow/contrib/makefile/build_all_linux.sh` command, then the build file name written as following :\n\"This should compile a static library in `tensorflow/contrib/makefile/gen/lib/tf_lib.a` \"\n\nBut when I tried to run `tensorflow/contrib/makefile/build_all_linux.sh` command, then it's generated file have following name:\n`/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a`\n\nCould anyone confirm me that why they are different? does docs have not updated name or something else.\n", "comments": ["I fixed at HEAD.  This will be pushed to Github within a day or two.  Thanks for noticing this!\n"]}, {"number": 4627, "title": "Resolve urllib dependency for py3.5", "body": "After this pull, <..>/tutorials/estimators/abalone.py works with python 3.5.\n\nIt resolves the dependency on urllib.urlretreive, which shows a different syntax under python 3.5.\n", "comments": ["Can one of the admins verify this patch?\n", "@jusjusjus, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener to be a potential reviewer\n", "Thanks for sending in this fix! Elsewhere in the project ([for example](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/models/image/imagenet/classify_image.py)), we use the following rename:\n\n``` python\nfrom six.moves import urllib\n```\n\n...and then use `urllib.request.urlretrieve()` throughout.\n\nCan you please revise your change to use this style instead?\n"]}, {"number": 4626, "title": "TensorFlow 0.10.0rc fails to detects CUDA device on my Macbook Pro (with nvidia 750M graphic card)", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n- This one is related, but it is for Ubuntu\n  https://github.com/tensorflow/tensorflow/issues/2882\n- This one is related also\n  https://github.com/tensorflow/tensorflow/issues/2940#issuecomment-238952433\n### Environment info\n\nOperating System: mac os 10.12\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\nls -l /usr/local/cuda/lib/libcud*\n-rwxr-xr-x@ 1 root   admin   8.1K Sep 28 14:39 /usr/local/cuda/lib/libcuda.1.dylib*\n-rwxr-xr-x@ 1 root   wheel   8.1K Apr 13 08:02 /usr/local/cuda/lib/libcuda.dylib*\nlrwxr-xr-x@ 1 root   wheel    45B Apr 13 08:03 /usr/local/cuda/lib/libcudadevrt.a@ -> /Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a\nlrwxr-xr-x@ 1 root   wheel    50B Apr 13 08:03 /usr/local/cuda/lib/libcudart.7.5.dylib@ -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib\nlrwxr-xr-x@ 1 root   wheel    46B Apr 13 08:03 /usr/local/cuda/lib/libcudart.dylib@ -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib\nlrwxr-xr-x@ 1 root   wheel    49B Apr 13 08:03 /usr/local/cuda/lib/libcudart_static.a@ -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a\n-rwxr-xr-x@ 1 qdang  staff    56M Apr 23 02:19 /usr/local/cuda/lib/libcudnn.5.dylib*\nlrwxr-xr-x@ 1 qdang  staff    16B Apr 23 04:10 /usr/local/cuda/lib/libcudnn.dylib@ -> libcudnn.5.dylib\n-rw-r--r--@ 1 qdang  staff    53M Apr 23 02:19 /usr/local/cuda/lib/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n   `TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.10.0-py2-none-any.whl`\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\npython -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\n0.10.0\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI just run the `iris_monitor.py` example at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/monitors and got the error\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\n```\n### What other attempted solutions have you tried?\n\nI ran torch on my GPU successfully before, so I could confirm that GPU computing is possible on my computer.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["@vinhqdang is there any other information returned in the failure?\n\nBased on #255, you might try running the device query binary from nvidia's samples, to see whether the GPU is properly detected.\n", "Here is the `nvcc` I have\n\n```\nnvcc --version\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Mon_Apr_11_13:23:40_CDT_2016\nCuda compilation tools, release 7.5, V7.5.26\n```\n", "Oops, I think the problem is: I am running Mac OS 10.12 with Xcode 8, clang 8 and CUDA 8.0, but CUDA 8.0 does not support clang 8\n\n```\n/Developer/NVIDIA/CUDA-8.0/samples/1_Utilities/deviceQuery$ make\n\n\n\n/Developer/NVIDIA/CUDA-8.0/bin/nvcc -ccbin clang++ -I../../common/inc  -m64  -Xcompiler -arch -Xcompiler x86_64  -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_60,code=compute_60 -o deviceQuery.o -c deviceQuery.cpp\n\nnvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n\nnvcc fatal   : The version ('80000') of the host compiler ('Apple clang') is not supported\n\n\nmake: *** [deviceQuery.o] Error 1\n\n\n```\n", "so, How to solve? I also have this problem? now, I run the mac os and Xcode8, cuda 7.5\n", "@wwxFromTju Have you followed the instructions here:\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#prepare-environment-for-mac-os-x\n\nNote the following section:\n\n---\n\nIf you want to compile tensorflow and have the XCode 7.3 installed, note that Xcode 7.3 is not yet compatible with CUDA 7.5. You will need to download Xcode 7.2 and select it as your default:\n\n```\n$ sudo xcode-select -s /Application/Xcode-7.2/Xcode.app\n```\n\n---\n\nI am not an expert on the mac environment, but perhaps you can try XCode 7.2?\n", "Closing automatically due to lack of recent activity. Please reopen when further information becomes available. Thank you.\n"]}, {"number": 4625, "title": "spelling mistake of hight is fixed", "body": "At link https://www.tensorflow.org/versions/r0.10/tutorials/image_recognition/index.html \n\nFix for the typo mistake of **hight** in line \"it holds a 299 pixel **high**, 299 pixel width,\".\n", "comments": ["Can one of the admins verify this patch?\n", "@tusharsoni08, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv and @petewarden to be potential reviewers\n", "Ohh sorry! that should be \"height\" instead of \"high\" and \"hight\" and yeah, it make sense to change that sentence. :)\n", "Jenkins, test this please\n"]}, {"number": 4624, "title": "Meta Graph Import not working when used with outputs_collections", "body": "### OS info\n\n```\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=15.10\nDISTRIB_CODENAME=wily\nDISTRIB_DESCRIPTION=\"Ubuntu 15.10\"\nNAME=\"Ubuntu\"\nVERSION=\"15.10 (Wily Werewolf)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 15.10\"\nVERSION_ID=\"15.10\"\n\nKERNEL: Linux 4.2.0-16-generic x86_64\n```\n### Cuda Version:\n\n```\n-rw-r--r-- 1 root root 189170 Nov 30  2015 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Nov 30  2015 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Nov 30  2015 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Nov 30  2015 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Nov 30  2015 /usr/local/cuda/lib/libcudart_static.a\n```\n### Tensorflow pip version:\n\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp34-cp34m-linux_x86_64.whl\n### Python `tensorflow.__version__` output\n\n```\n0.10.0\n```\n### Minimal Reproducible code\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\ntf_chk = \"model_ckpt-0\"\ntf_meta = \"model_ckpt-0.meta\"\n\nimages = tf.constant(1.2, tf.float32, shape=[100, 100])\nwith slim.arg_scope(\n        [slim.fully_connected],\n        outputs_collections=[tf.GraphKeys.ACTIVATIONS]): ## NOT working\n        # outputs_collections=[]): ## This works\n    logits = slim.fully_connected(images, 100, scope='logits')\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    saver = tf.train.Saver()\n    saver.save(sess, tf_chk)\n\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        new_saver = tf.train.import_meta_graph(tf_meta)\n        new_saver.restore(sess, tf_chk)\n```\n### Description\n\nCode above will crash the program with the following stack trace:\n\n```\nTraceback (most recent call last):\n  File \"test.py\", line 24, in <module>\n    new_saver = tf.train.import_meta_graph(tf_meta)\n  File \"/home/x/.local/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 1458, in import_m\neta_graph\n    return _import_meta_graph_def(read_meta_graph_file(meta_graph_or_file))\n  File \"/home/x/.local/lib/python3.4/site-packages/tensorflow/python/training/saver.py\", line 1369, in _import_\nmeta_graph_def\n    col_op = ops.get_default_graph().as_graph_element(value)\n  File \"/home/x/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2392, in as_graph_\nelement\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n  File \"/home/x/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2452, in _as_graph\n_element_locked\n    \"graph.\" % repr(name))\nKeyError: \"The name 'logits' refers to an Operation not in the graph.\"\n```\n\nHowever, if I create _outputs_collections_ with the empty list, the code will\nwork. Also, the code will work if I don't specify the _outputs_collections_\nargument.\n", "comments": ["Thanks for filing the bug @barty777!\n\nI'm able to reproduce on my machine.  I'm not an expert on the slim framework, but it does seem like the `outputs_collections` argument to `slim.fully_connected` causes import_meta_graph to fail.  In particular, we end up with an extra field in the MetaGraphDef:\n\n```\n  collection_def: { # (tensorflow.MetaGraphDef.CollectionDefEntry) size=25B\n    key  : \"activations\"    # size=11\n    value: {    # (tensorflow.CollectionDef) size=10B\n      node_list: {  # (tensorflow.CollectionDef.NodeList) size=8B\n        value: [ \"logits\" ] # size=6\n      } # collection_def[0].value.node_list\n    }   # collection_def[0].value\n  } # collection_def[0]\n```\n\nThis field causes the crash reported above.\n\n@sguada @nathansilberman Can you take a look at this?  Thanks!\n", "Did you guys figure out a solution?"]}, {"number": 4623, "title": "examples/image_retraining examples/android", "body": "hi guys:\ni want to use a retraining model on the android device, and i moved the model.pb(retraining Inception v3 model ) and label.txt(containing two labels) to the examples/android/assets, but i got some problem as the figure.\n\n(09-28 18:37:11.572 21326-21351/org.tensorflow.demo E/native: tensorflow_inference_jni.cc:160 Output [output:0] not found, aborting!)\n\nIs there any document which explains detail about how to predict with a model in .cc file and how should i rewrite tensorflow_inference_jni.cc file to load the model.\n<img width=\"1143\" alt=\"2016-09-28 6 38 03\" src=\"https://cloud.githubusercontent.com/assets/12976847/18910659/975ef168-85ab-11e6-8fe0-4f6eaec3ba9a.png\">\n\nthanks for your time\n", "comments": ["See the comments in [TensorFlowImageListener.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageListener.java) for instructions on updating the model parameters.\n\nYou will also need to run strip_unused.py to remove some of the ops unsupported by Android TF that were for training (e.g. DecodeJpeg):\n\n```\nbazel build tensorflow/python/tools:strip_unused\nbazel-bin/tensorflow/python/tools/strip_unused --input_graph=inception.pb --output_graph=/tmp/stripped_inception.pb --input_node_names=\"Mul\" --output_node_names=\"final_result\" --input_binary=true\n```\n", "it worked, thanks for your help and time.\n", "@andrewharp there's no strip_unused.py in the repo now. Has it been moved, or is running it no longer required?"]}, {"number": 4622, "title": "Contributing to TF", "body": "My apology if I am posting this in a wrong place.\nI was wondering if I have a valid bug fix or improvement to TF, and I want to test it before submitting a PR, what's the fastest way of doing it?\nI usually use\n\n```\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n```\n\nto re-set up TF but it always takes too much time.\nThanks\n", "comments": ["Could you try to just build & test the relevant test targets?\n\nFor instance:\n\n```\nbazel test //tensorflow/python:saver_test  # Or other Python/C++ test targets.\n```\n\nI am not sure if this works in OSS world or not.  If not, when building `build_pip_package` feel free to drop the `-c opt` to improve build times.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 4621, "title": "Update gradiesnt_descent.py", "body": "", "comments": ["@gautam1858, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman and @vrv to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for the change! In the future, please describe the reasoning for the change in your pull request. In this case, passing grad.indices is incorrect because that function is for dense tensors, but .indices only works for sparse tensors.\n"]}, {"number": 4620, "title": "new feature in cuda8.0 release", "body": "```\nNew in CUDA 8\nPascal Architecture Support\nEnhance performance out-of-the-box on Pascal GPUs\nSimplify programming using Unified Memory including support for large datasets, concurrent data access and atomics\nOptimize Unified Memory performance using new data migration APIs\nIncrease throughput at ultra-fast speeds using NVLINK, new high-speed interconnect\nDeveloper Tools\nIdentify latent system-level bottlenecks using critical path analysis\nImprove productivity up to 2x with NVCC\nTune OpenACC applications and overall host code using new profiling extensions\nLibraries\nAccelerate graph analytics algorithms with nvGRAPH\nSpeed-up Deep Learning applications using native support for FP16 and INT8, support for batch operation in cuBLAS\n```\n\nDo we have anything to improve in these places\n1.Simplify programming using Unified Memory including support for large datasets, concurrent data access and atomics\n2.Optimize Unified Memory performance using new data migration APIs\n3.Increase throughput at ultra-fast speeds using NVLINK, new high-speed interconnect\n4.Speed-up Deep Learning applications using native support for FP16 and INT8, support for batch operation in cuBLAS\n", "comments": ["@fayeshine Do you have a specific problem that you're working on, or is this just for general information?\n\n@zheng-xq might have some info on specifics.\n", "United memory is great for minimizing effort to port existing CPU application to GPU. This could change in the future, but it doesn't seem to offer much performance benefit for frameworks like TensorFlow that are optimized for GPU. Since all the buffers are in forms of explicitly managed tensors, we can already reap most of the sharing between CPU and GPU through pinned memory, if necessary.\n\nNative support for FP16 had been targeted extensively. More support for INT8 is going to be added as well. \n\nThere are also a lot of interest for high-bandwidth inter-GPU communication primitives. They are likely be to be targeted as well. \n", "Possibly unrelated, but does send/recv between multiple GPUs curently go through main memory? I've been searching in the source, and it would be very helpful if you could give a hint on this.\n", "It can use P2P transfers which don't use CPU. For instance if you have 8\nGPUs which can all transfer memory directly to each other, you'll see\nsomething like this in logs\n\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3 4 5 6 7\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 4:   Y Y Y Y Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 5:   Y Y Y Y Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 6:   Y Y Y Y Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 7:   Y Y Y Y Y Y Y Y\n\nOn Wed, Oct 19, 2016 at 5:30 PM, Bairen Yi notifications@github.com wrote:\n\n> Possibly unrelated, but does send/recv between multiple GPUs curently go\n> through main memory? I've been searching in the source, and it would be\n> very helpful if you could give a hint on this.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4620#issuecomment-254978623,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHI6ZoaZoxkt6UU2Z3uDZ15lpE67Mks5q1rY4gaJpZM4KInZR\n> .\n", "@yaroslavvb How did tensorflow choose to transfer information() across GPU directly, rather than goes to CPU as an intermediate state? \r\n\r\nSecond question, more important, internally, how does tensorflow off-load job to GPU? Does it off-load all training data + weights all together into GPU memory then computing until the end of training, then bring the final weight back to main memory? Or, does it only off-load current batch of training data + weights to GPU memory?\r\n\r\nThanks,\r\nShawn", "@pswpswpsw The user specifies where to locate weights and training data and TensorFlow adds transfers as necessary. If you place your training data and weights on GPU, and all ops have GPU implementation, then all the data will stay on GPU. You can use timeline profiling to see where ops are executed and when CPU/GPU transfers are taking place -- https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659", "Since there is no specific bug or feature request settled on, I am closing this for now. Our other performance initiatives will encompass using Cuda 8.0 features. Thank you!"]}, {"number": 4619, "title": "make variable_scope accept empty string as name_or_scope", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@OscarDPan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @ebrevdo and @theweiho to be potential reviewers\n", "issue #4576 \n", "Can one of the admins verify this patch?\n", "Thanks, this looks good to me. Please merge if possible.\n", "Jenkins, test this please\n"]}, {"number": 4618, "title": "Fix typo in 'normalization'.", "body": "This pull fixes a typo in explanations of examples/tutorials/deepdream/deepdream.ipynb.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please\n"]}, {"number": 4617, "title": "sparse lookup tables for tf.nn.embedding_lookup_sparse", "body": "Hope that \u201c**params**\u201d of **tf.nn.embedding_lookup_sparse** support **SparseTensor**!!!\n\n[tf.nn.embedding_lookup_sparse(params, sp_ids, sp_weights, partition_strategy='mod', name=None, combiner='mean')](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#embedding_lookup_sparse)\n\nThe efficiency of sparse lookup tables is low,  but it is very useful for large-scale linear regression system, especially when designing a prototype. \nFor example, the wide part of \"wide and deep learning\" use sparse features.\nAlthough I don't know when the hashing function is applied, using sparse lookup table is more accurate than hashing.  The hashing function may merge two features into one bin (share the same weight), or discard new features when the lookup table is full.\n", "comments": ["@concretevitamin might have some thoughts on this.\n", "Any update @ebrevdo? If this is not going to happen soon, I plan on closing this pending a more concrete plan here. It's not clear enough to move to contributions welcome.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 4616, "title": "Commit broke keras.", "body": "I'm running `Ubuntu 14.04.5 LTS` with `tensorflow==0.10.0` and `keras==1.1.0`.\n\n```\nls -la /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 322936 Aug  3 10:55 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug  3 10:55 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug  3 10:55 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug  3 10:55 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug  3 10:55 /usr/local/cuda/lib64/libcudart_static.a\n```\n\nAfter installing tensorflow from source on an EC2 machine and attempting to import keras, I recieved an `AttributeError: module 'tensorflow.python' has no attribute 'control_flow_ops'` error. The same error mentioned in: https://github.com/fchollet/keras/issues/3857. I noticed that after this tensorflow commit: https://github.com/tensorflow/tensorflow/commit/5563229b5b45917cd8155bc47a8356af90fb20a2#diff-6ad579dc34adb7a581b8c0bcb1a4dd79\n\nthat `tensorflow.python.control_flow_ops` was no longer accessible and thus is what is breaking the code. I got around it by doing `from tensorflow.python.ops import control_flow_ops`.\n\nShould I submit a pull request to keras or does tensorflow have a plan to fix this issue?\n", "comments": ["@drpngx - Can you take a look at this?  Thanks.\n", "We probably want to have them access our code via tf.cond, I've commented on that other bug.  importing the internal module is bypassing the public interface, sadly.\n"]}, {"number": 4615, "title": "fixed link to queues.py in slim", "body": "", "comments": ["@jonasrauber, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @lukaszkaiser and @cancan101 to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Jenkins, test this please\n"]}, {"number": 4614, "title": "It might be better to have one more return value in function `tf.unique()` showing the indexes in the original array", "body": "The function `tf.unique(x, name=None)` finds the unique elements in a 1-D tensor. And it now returns two value: `y` and `idx`. `y` contains all of the unique elements of `x` sorted inthe same order that they occur in `x`. `idx` contains the index of each value of `x` in the unique output `y`.\n\n```\n# tensor 'x' is [1, 1, 2, 3, 3, 3, 7, 8, 8]\ny, idx = tf.unique(x)\ny ==> [1, 2, 3, 7, 8]\nidx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\n```\n\nBUT i think a third return value which contains the first index of each value of `y` in the original tensor `x` is also needed. It might work like this:\n\n```\n# tensor 'x' is [1, 1, 2, 3, 3, 3, 7, 8, 8]\ny, idx, idx_ori  = tf.unique(x)\ny ==> [1, 2, 3, 7, 8]\nidx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\nidx_ori ==> [0, 2, 3, 6, 7]\n```\n\nJust like its equivalent in Numpy does:\n\n```\narray 'x' is [1, 1, 2, 3, 3, 3, 7, 8, 8]\ny, idx_ori = np.unique(x, return_index=True)\ny ==> [1, 2, 3, 7, 8]\nidx_ori ==> [0, 2, 3, 6, 7]\n```\n", "comments": ["@aselle: is this an instance of NumPy API disparity that we should look into?  \n", "Sorry this is in my notes... here are all the differences between numpy that I notice:\n\n_Not compatible. numpy accepts arbitrary shapes and tf accepts only shape=(1,). tf unconditionally returns the index array whereas numpy does this only if return_index is true. np can optionally also return inverse map or return counts of each elements usage. Workaround is tf.unqiue(tf.reshape(x, [-1])). Internally numpy may do similar since the inverse is not returned shaped_. The missing return_counts can be achieved using tf.unique_counts().\n\nTF syntax: `tf.unique(x, name=None)`\nNP syntax: `np.unique (ar, return_index=False, return_inverse=False, return_counts=False)`\n", "Looking more closely, it appears unique_with_counts does support this already\nhttps://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#unique_with_counts\n@yzsatgithub  let me know if that addresses your need.\n", "I was looking for this functionality and somehow landed here from Google search.\n@aselle `unique_with_counts` is different from what @yzsatgithub is asking.\nIn the NP syntax above, he's asking the indicies equivalent to `return_index=True` attribute in NP.\n`unique_with_counts` in tensorflow is equivalent to `return_counts=True` attribute in NP.\n", "You can definitely get the equivalent functionality using unique_with_counts()... in more detail...\n\n``` python\nimport tensorflow as tf\ns=tf.Session()\nx = np.array([1, 1, 2, 4, 4, 4, 7, 8, 8])\nprint(s.run(tf.cumsum(tf.pad(tf.unique_with_counts(x)[2],[[1,0]]))[:-1]))\nprint(np.unique(x, return_index=1)[1])\n```\n\nyields the output\n\n```\n[0 2 3 6 7]\n[0 2 3 6 7]\n```\n", "This works only if x is pre-sorted. I know this functionality is doable with a bit of coding but it'd be nice if it works even though x isn't sorted like the numpy version.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "This feature is still needed and not implemented in tensorflow", "This code gives the equivalent functionality and does not require the input array to be sorted:\r\n```\r\ndef unique_with_inverse(x):\r\n  y, idx = tf.unique(x)\r\n  num_segments = tf.shape(y)[0]\r\n  num_elems = tf.shape(x)[0]\r\n  return (y, idx,  tf.unsorted_segment_min(tf.range(num_elems), idx, num_segments))\r\n```\r\nI slightly changed [this](https://stackoverflow.com/questions/38233821/merge-duplicate-indices-in-a-sparse-tensor)."]}, {"number": 4613, "title": "Maybe there should be an argument 'unique' in the function `tf.where()`", "body": "There IS the case where we wanna find the first position of the `True` element in a tensor.\nsuppose there is a tensor `a` like this:\n\n```\n[array([[[ True, False],\n        [False, False]],\n\n       [[False,  True],\n        [ True, False]],\n\n       [[False, False],\n        [False,  True]],\n\n       [[False, False],\n        [ True, False]]], dtype=bool)\n```\n\nand u run `tf.where(a)`, u will get:\n\n```\n array([[0, 0, 0],\n       [1, 0, 1],\n       [1, 1, 0],\n       [2, 1, 1],\n       [3, 1, 0]])]\n```\n\nBut we sometimes want a result like this:\n\n```\n array([[0, 0, 0],\n       [1, 0, 1],\n       [2, 1, 1],\n       [3, 1, 0]])]\n\n```\n\nwhich show us the index of first `True` element instead of all `True` elements.\n\nI suppose the function `tf.where()` might as well add an argument `unique` , indicating whether u wanna show all true element positions or the first one. And the default value of `unique` should be False. \nthe prototype might be like this:\n\n```\ntf.where(input, unique=False, name=None)\n```\n\nand for the requirements shown above, i might run something like this:\n\n```\ntf.where(a, unique = True)\n```\n", "comments": ["I don't think this is a very intuitive extension to the API. Why is it considering uniqueness only in some subset of the dimensions (i.e. 1, 2) but not in 0. I'd expect unique to do something like return\n[[0,0,0]]. In any case, we are trying to adhere to NumPy as much as possible which does not have this.\n", "I'm closing this issue due to inactivity. If new information is provided, I'll reopen this issue.\n"]}, {"number": 4612, "title": "Remove RE2 from the CMake build.", "body": "It is no longer used internally.\n", "comments": ["@mrry, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ageron, @lilac and @ebrevdo to be potential reviewers\n"]}]