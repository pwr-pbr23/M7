[{"number": 35241, "title": "add_weight with None name generate a graph that it's not possible to save by checkpoint manager", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla V100-SXM2-16GB\r\n\r\n**Describe the current behavior**\r\nWhen creating a custom keras.layers.Layer, it is possible to add weight in a lazy fashion through the add_weight() API. One of the possible arguments is the variable name.\r\n\r\nAccording the documentation the name can be omitted, however if we have a Variable with None name and we try to save the model using the CheckpointManager API we can't generate a proper graph taxonomy. Specifically, it is impossible to obtain the path prefix for the custom variable since the ``_escape_local_name()`` function in graph_view.py line 51 would rise a NoneType error.\r\n \r\n**Describe the expected behavior**\r\nThe expected behaviour would rise an error when the custom layer is build. In my opinion I would make the Variable name a mandatory field instead of having a None value by default.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nclass CustomEmbedding(tf.keras.layers.Layer):\r\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\r\n    \"\"\"\r\n\r\n    def __init__(self, vocab_size, hidden_size,  max_position_embeddings, **kwargs):\r\n        super(CustomEmbedding, self).__init__(**kwargs)\r\n        self.vocab_size = vocab_size\r\n        self.hidden_size = hidden_size\r\n        self.max_position_embeddings = max_position_embeddings\r\n        self.pad_idx = 0\r\n        self.max_position_embeddings += 1\r\n\r\n\r\n    def build(self, input_shape):\r\n        \"\"\"Build shared word embedding layer \"\"\"\r\n        with tf.name_scope(\"position_embeddings\"):\r\n            self.position_embeddings = self.add_weight(\r\n                # Note that the name is missing\r\n                shape=(self.max_position_embeddings, self.hidden_size),\r\n                initializer=positional_encoding,\r\n                trainable=False,\r\n                dtype=self.dtype)\r\n\r\n        with tf.name_scope(\"word_embeddings\"):\r\n            # Create and initialize weights. The random normal initializer was chosen\r\n            # arbitrarily, and works well.\r\n            self.word_embeddings = self.add_weight(\r\n                \"weight\",\r\n                shape=[self.vocab_size, self.hidden_size],\r\n                initializer=get_initializer(self.initializer_range),\r\n                trainable=True,\r\n                dtype=self.dtype\r\n            )\r\n        super(CustomEmbedding, self).build(input_shape)\r\n\r\n    def call(self, inputs):\r\n        input_ids, position_ids = inputs\r\n\r\n        inputs_embeds = tf.nn.embedding_lookup(self.word_embeddings, input_ids)\r\n        position_embeddings = tf.nn.embedding_lookup(self.position_embeddings, position_ids)\r\n\r\n        embeddings = inputs_embeds + position_embeddings\r\n        return embeddings\r\n```\r\nAssuming to have a model with the over-defined CustomLayer. If we try to save the model after some training epochs\r\n```\r\nckpt = tf.train.Checkpoint(model=model, optimizer=optim)\r\nckpt_manager = tf.train.CheckpointManager(ckpt,\r\n    directory=path.join(args.ckpt_path, args.version),\r\n    max_to_keep=args.max_ckp_to_keep)\r\n\r\nif ckpt_manager.latest_checkpoint:\r\n    ckpt.restore(ckpt_manager.latest_checkpoint)\r\n    print('Latest checkpoint restored!!')\r\n\r\n# training code\r\n\r\nif self.val_accuracy.result() > best_model:\r\n    best_model = self.test_accuracy.result()\r\n    ckpt_save_path = ckpt_manager.save() # expected errors\r\n    print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))\r\n``` \r\n", "comments": ["@andompesta, Could you provide the complete standalone code to analyze the reported issue. Thanks!", "Hi, I can't give you the original code, but this is a stupid example that reproduce the error\r\n```python\r\nfrom __future__ import (absolute_import, division, print_function,\r\n                        unicode_literals)\r\n\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef positional_encoding(shape, dtype=tf.float32):\r\n    \"\"\"\r\n    positional encoding initializer. Note that we can't freeze the embedding layer because tf is shit\r\n    :param shape:\r\n    :param dtype:\r\n    :return:\r\n    \"\"\"\r\n    n_pos, dim = shape\r\n    position_enc = np.array([\r\n        [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\r\n        for pos in range(n_pos)\r\n    ])\r\n\r\n    # apply sin to even indices in the array; 2i\r\n    position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])\r\n\r\n    # apply cos to odd indices in the array; 2i+1\r\n    position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])\r\n\r\n    # pos_encoding = position_enc[np.newaxis, ...]\r\n\r\n    return tf.cast(position_enc, dtype=dtype)\r\n\r\n\r\nclass CustomEmbedding(tf.keras.layers.Layer):\r\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\r\n    \"\"\"\r\n\r\n    def __init__(self, vocab_size, hidden_size,  max_position_embeddings, **kwargs):\r\n        super(CustomEmbedding, self).__init__(**kwargs)\r\n        self.vocab_size = vocab_size\r\n        self.hidden_size = hidden_size\r\n        self.max_position_embeddings = max_position_embeddings\r\n        self.pad_idx = 0\r\n        self.max_position_embeddings += 1\r\n\r\n\r\n    def build(self, input_shape):\r\n        with tf.name_scope(\"position_embeddings\"):\r\n            self.position_embeddings = self.add_weight(\r\n                shape=(self.max_position_embeddings, self.hidden_size),\r\n                initializer=positional_encoding,\r\n                trainable=False,\r\n                dtype=self.dtype)\r\n\r\n        with tf.name_scope(\"word_embeddings\"):\r\n            self.word_embeddings = self.add_weight(\r\n                \"token_weight\",\r\n                shape=(self.vocab_size, self.hidden_size),\r\n                initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\r\n                trainable=True,\r\n                dtype=self.dtype\r\n            )\r\n        super(CustomEmbedding, self).build(input_shape)\r\n\r\n    def call(self, inputs):\r\n        input_ids, position_ids = inputs\r\n\r\n        inputs_embeds = tf.nn.embedding_lookup(self.word_embeddings, input_ids)\r\n        position_embeddings = tf.nn.embedding_lookup(self.position_embeddings, position_ids)\r\n\r\n        embeddings = inputs_embeds + position_embeddings\r\n        return embeddings\r\n\r\n\r\nclass TestModel(tf.keras.Model):\r\n    def __init__(self, vocab_size, hidden_size,  max_position_embeddings, num_class, **kwargs):\r\n        super(TestModel, self).__init__(**kwargs)\r\n        self.emb = CustomEmbedding(vocab_size, hidden_size, max_position_embeddings)\r\n        self.dense = tf.keras.layers.Dense(num_class, name=\"class_prj\")\r\n\r\n    def call(self, inputs):\r\n        word_emb = self.emb(inputs)\r\n        sent_emb = tf.reduce_mean(word_emb, axis=1)\r\n        logit = self.dense(sent_emb)\r\n        return logit\r\n\r\nVOCAB_SIZE = 100\r\nHIDDEN_SIZE = 5\r\nMAX_POSITION_EMBEDDING = 30\r\nNUM_CLASS = 3\r\nEPOCH = 1\r\nLR = 0.001\r\n\r\n\r\ndef compute_loss(label, logit):\r\n    cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=label)\r\n    return tf.reduce_mean(cross_ent)\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.random.set_seed(0)\r\n    # construct the model\r\n    model = TestModel(VOCAB_SIZE, HIDDEN_SIZE, MAX_POSITION_EMBEDDING, NUM_CLASS)\r\n\r\n    # generate dataset\r\n    seqs = np.array([[0, 0, 1], [1, 1, 1], [2, 3, 2], [3, 4, 3], [4, 4, 4]], dtype=np.int64)\r\n    poss = np.array([[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]], dtype=np.int64)\r\n    label = np.array([0, 1, 2, 1, 1], dtype=np.int64)\r\n    dataset = tf.data.Dataset.from_tensor_slices((seqs, poss, label)).batch(3)\r\n\r\n    # def optim\r\n    optim = tf.keras.optimizers.RMSprop(LR, clipnorm=1.)\r\n\r\n    # def checkpoint\r\n    ckpt = tf.train.Checkpoint(model=model, optimizer=optim)\r\n    ckpt_manager = tf.train.CheckpointManager(ckpt,\r\n                                              directory=\"./tmp/test_ckp\",\r\n                                              max_to_keep=2)\r\n\r\n\r\n    def train_step(inputs):\r\n        seq, pos, label = inputs\r\n        with tf.GradientTape() as tape:\r\n            logit = model((seq, pos))\r\n            loss = compute_loss(label, logit)\r\n\r\n        gradients = tape.gradient(loss, model.trainable_variables)\r\n        optim.apply_gradients(zip(gradients, model.trainable_variables))\r\n        return loss\r\n\r\n\r\n    for epoch in range(EPOCH):\r\n        loss = 0.\r\n        for batch in dataset:\r\n            b_loss = train_step(batch)\r\n            loss += b_loss.numpy()\r\n\r\n        print(loss)\r\n        ckpt_save_path = ckpt_manager.save()\r\n```\r\nBasically I build a classifier over the embedding layer.\r\nNote that the positional embedding (line 47) has no name, thus when I try to save the graph in the checkpoint, it will have None as name. Thus, we can't generate a path name in the graph.\r\n\r\nA simple fix is to give a proper name (str) to the variable. This is way I was suggesting to change the API and make name a required field for ``add_weight()``", "I could replicate the issue with Tf 2.0.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/0c0b9d79192ba7833486f0d7830e69d2/untitled320.ipynb). Thanks!", "Assign to Kathy who works on model saving.", "This is still an issue with `TF2.2.0rc4`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/e538a95d94444ccbd48e66f5dd536568/untitled320.ipynb) is a gist for our reference. Thanks!", "Could reproduce the issue with TF Version 2.5. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/63721ecfca0f9e22b4549ff5fd1a414d/untitled320.ipynb). Thanks!", "The issue exists in tf 2.7, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/01646cb7506969669516468de549ded8/untitled636.ipynb)"]}, {"number": 35239, "title": "Return some sort of verifying data structure for load_weights for tf.keras models", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, whenever user loads keras models using HDF5, user gets no confirmation that models were loaded successfully ([\\[1\\]](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#load_weights), [\\[2\\]](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable#load_weights) return nothing). This confirmation could be useful for verifying that weights are properly loaded. In case of transfer learning, users typically only want to load specific weights from the HDF5file. This can be currently achieved by using `load_weights(by_name=True)`, however, users don't get any confirmation about which layers were actually loaded.\r\n\r\nAlso, in case there are no matching layers in between the original model and source model, the `model.load_weights(by_name=True)` fails without raising any exception so there is no way to actually debug what went wrong with the model loading. (Note here that I'm talking about name mismatch not weight mismatch) This significantly affects the ability to write unit tests for models since the tester code cannot actually verify what layers were loaded from HDF5 file.\r\n\r\n**Will this change the current api? How?**\r\nYes, this changes the current API for [\\[1\\]](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable#load_weights) and [\\[2\\]](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#load_weights) by adding a return type to them. Specifically, we will be returning a particular data structure when loading HDF5 files [here](https://github.com/tensorflow/tensorflow/blob/c49396cf71dacc32195033507b3bbd985b12c255/tensorflow/python/keras/engine/network.py#L1131). My current idea is to return the list of layers that were loaded from HDF5 although I'm open to discussion about the return type.\r\n\r\n**Who will benefit with this feature?**\r\nUsers who are loading their weights from h5py files and want to verify/check what layers were loaded from HDF5. \r\n\r\n**Any Other info.**\r\nN/A\r\n\r\n\r\nLet me know if any more clarification/information is needed. I'm willing to contribute by working on this issue.", "comments": ["Currently, the only way to see if model loads the weights correctly is by manually setting weights for every layer from hdf5 which is very inefficient. (Discussion on keras repo [here](https://github.com/keras-team/keras/issues/5397))\r\n(To build a unit test for verifying if model is correctly getting loaded, one needs to collect initial weights of the model somewhere, manually set the weights using hdf5 and lastly compare the newly set weights with the initial weights. This is a very inefficient workflow and can be improvised by the feature I have requested above)", "@fchollet (correct me if I'm wrong) the final function that sets the values of weights in keras is [this](https://github.com/tensorflow/tensorflow/blob/ea8b6de39fc208619281f34163952ccaa134c04c/tensorflow/python/keras/saving/hdf5_format.py#L639). I can see that it contains `layer_names` which holds the list of layers for which the API will try to load weights for. \r\n\r\nIMO we can simply return this list to every caller function and that would suffice ? If you agree, please let me know. I have just setup a tensorflow development environment and I can send a PR for this.", "I agree, this feature would be very useful! \r\n", "@ashutoshbsathe,\r\nSorry for the delayed response. Documentation of [Model.Load_Weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) states that it returns:\r\n\r\n> When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model, immediately if it is already built).\r\n\r\nWe can verify if the object is of type, **`tf.train.Checkpoint.restore`**. Thanks!", "> @ashutoshbsathe,\n> Sorry for the delayed response. Documentation of [Model.Load_Weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) states that it returns:\n> \n> > When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model, immediately if it is already built).\n> \n> We can verify if the object is of type, **`tf.train.Checkpoint.restore`**. Thanks!\n\nHi, the return value exists only when restoring TF checkpoints. The issue is about return value when loading HDF5 weights. The documentation also mentions that when loading HDF5 files, the return value is `None`"]}, {"number": 35160, "title": "-D_GLIBCXX_USE_CXX11_ABI=1 increases a lot RAM usage", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: python3.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1080 ti\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nSince g++7 is now the default version on Ubuntu 18 and most distributions, most builds will use  `_GLIBCXX_USE_CXX11_ABI=1`. It seems also that when tensorflow is built with  `_GLIBCXX_USE_CXX11_ABI=0`, it implies recompiling all other libraries of the project with this flag which can be unconvenient. \r\n\r\nWe noticed that building with `_GLIBCXX_USE_CXX11_ABI=1` increases the RAM by a lot. \r\n\r\n**Describe the expected behavior**\r\n\r\nBoth packages should consume the same amount of RAM.\r\n\r\n**Code to reproduce the issue**\r\n\r\nYou can install tensorflow using `python3.7 -m pip install tensorflow==1.13.1` (related to https://github.com/tensorflow/tensorflow/issues/27078) and makes sure \r\n`python3.7 -c \"import tensorflow; print(tensorflow.sysconfig.get_compile_flags())\"` prints `-D_GLIBCXX_USE_CXX11_ABI=1`. \r\nThen you can install it in python3.6 `python3.6 -m pip install tensorflow==1.13.1` and make sure ` python3.6 -c \"import tensorflow; print(tensorflow.sysconfig.get_compile_flags())\"` prints `D_GLIBCXX_USE_CXX11_ABI=0`.\r\n\r\nNow run this script with python3.6 and python3.7 and you will see that the second one consume a lot more (x3 on the model I use). Any `saved_model.pb` should work.\r\n\r\n```python\r\nimport io\r\nimport os\r\nimport sys\r\ntry:\r\n    from urllib import urlopen\r\nexcept ImportError:\r\n    from urllib.request import urlopen\r\n\r\nimport numpy\r\nimport psutil\r\nfrom PIL import Image\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import saved_model_pb2\r\nfrom tensorflow.python.platform import gfile\r\nfrom tensorflow.python.util import compat\r\n\r\nprocess = psutil.Process(os.getpid())\r\n\r\ndef print_ram(prefix=''):\r\n    print(\"RAM\", prefix, process.memory_info().rss / 1024. / 1024.)\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    if len(sys.argv) == 1:\r\n        model_filename = 'saved_model.pb'\r\n    else:\r\n        model_filename = sys.argv[1]\r\n\r\n    with gfile.FastGFile(model_filename, 'rb') as f:\r\n        data = compat.as_bytes(f.read())\r\n        sm = saved_model_pb2.SavedModel()\r\n        sm.ParseFromString(data)\r\n        if 1 != len(sm.meta_graphs):\r\n            print('More than one graph found. Not sure which to write')\r\n            sys.exit(1)\r\n\r\n    img_url = 'https://i.dailymail.co.uk/1s/2019/11/23/09/21370544-7717313-image-a-1_1574501083030.jpg'\r\n    image_data = urlopen(img_url).read()\r\n    decoded_data = numpy.array(Image.open(io.BytesIO(image_data)))\r\n    decoded_data = numpy.expand_dims(decoded_data, axis=0)\r\n\r\n    print_ram('before graph import')\r\n    graph = tf.import_graph_def(sm.meta_graphs[0].graph_def)\r\n\r\n    print_ram('before device')\r\n    with tf.device(\"/device:GPU:0\"):\r\n        with tf.Session(graph=graph, config=None) as sess:\r\n            print_ram('after session')\r\n            output = sess.graph.get_tensor_by_name('import/predictions:0')\r\n            print_ram('before run')\r\n            for i in range(10000):\r\n                results = sess.run(output, feed_dict={\"import/image_tensor:0\": decoded_data})\r\n                print_ram('after run')\r\n```\r\n\r\n**Other info / logs**\r\n\r\npython3.6:\r\n```\r\nRAM before graph import 527.55078125\r\nRAM before device 856.59375\r\n2019-12-16 17:51:27.400388: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-16 17:51:27.425979: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399580000 Hz\r\n2019-12-16 17:51:27.426637: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x229fee0 executing computations on platform Host. Devices:\r\n2019-12-16 17:51:27.426655: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nRAM after session 860.25\r\nRAM before run 860.25\r\nRAM after run 948.12109375\r\nRAM after run 998.0\r\nRAM after run 1022.2265625\r\nRAM after run 1038.984375\r\nRAM after run 1038.984375\r\nRAM after run 1056.0\r\nRAM after run 1092.1953125\r\nRAM after run 1097.09375\r\nRAM after run 1097.09375\r\nRAM after run 1097.09375\r\nRAM after run 1097.09375\r\nRAM after run 1097.09375\r\nRAM after run 1116.42578125\r\nRAM after run 1116.42578125\r\nRAM after run 1116.42578125\r\nRAM after run 1116.42578125\r\nRAM after run 1116.42578125\r\n```\r\n\r\npython3.7:\r\n```\r\nRAM before graph import 510.33984375\r\nRAM before device 752.87890625\r\n2019-12-16 17:26:00.118658: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-16 17:26:00.141979: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399580000 Hz\r\n2019-12-16 17:26:00.142737: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x265aff0 executing computations on platform Host. Devices:\r\n2019-12-16 17:26:00.142775: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nRAM after session 756.5625\r\nRAM before run 756.5625\r\nRAM after run 2977.8515625\r\nRAM after run 3030.1640625\r\nRAM after run 3047.8046875\r\nRAM after run 3090.08203125\r\nRAM after run 3121.296875\r\nRAM after run 3122.0703125\r\nRAM after run 3123.1015625\r\nRAM after run 3123.1015625\r\nRAM after run 3135.9921875\r\n```", "comments": ["@r4nt may have some input on this, any ideas?\r\n@maingoh Could you confirm this is also the same at head, or a newer release?", "This is at least the same until 1.14.0 which are built using `-D_GLIBCXX_USE_CXX11_ABI=1` (for python3.7 package). For some reason the 1.15.0 is built with `-D_GLIBCXX_USE_CXX11_ABI=0` (and thus does not have the issue).\r\n\r\nI don't have much time to rebuild a tf 1.15 with `-D_GLIBCXX_USE_CXX11_ABI=1` but I am quite sure it will be the same issue. I think the problem comes from the COW disabled on C++ strings in CXX11 ABI. Might also be related to the way protobuf handles strings ?\r\n", "@maingoh,\r\n\r\nAs I see that the issue is related to `TF 1.x` which is not officially supported anymore. Can you try installing the latest stable version of tensorflow i.e `2.6.0` and lets us know if the issue still persists. You can follow this [guide](https://www.tensorflow.org/install/source#tested_build_configurations) to build from source. Thanks!", "I rebuilt tf 2.6.0 from source, the conclusion is the same. I am still using a saved_model trained using tf1 (I don't have one in tf2). \r\n\r\nTo be honest, I don't think it will magically be fixed without a proper debug or PR addressing it. It might come from a dependency or maybe it is not even fixable because of the way the new C++ ABI works. \r\n\r\nThe official docker images are still built using the old ABI. Probably when the tf team will build with the new one they will encounter the issue.\r\n\r\nHere is my code (might not be totally perfect as I am very new to tf 2 API):\r\n```python\r\nimport os\r\ntry:\r\n    from urllib import urlopen\r\nexcept ImportError:\r\n    from urllib.request import urlopen\r\n\r\nimport psutil\r\n\r\nimport tensorflow as tf\r\n\r\nprocess = psutil.Process(os.getpid())\r\n\r\n\r\ndef print_ram(prefix=''):\r\n    print(\"RAM\", prefix, process.memory_info().rss / 1024. / 1024.)\r\n\r\n\r\nif __name__ == '__main__':\r\n    loaded = tf.saved_model.load('.')\r\n    print_ram('after model loading')\r\n    print(loaded)\r\n    print(list(loaded.signatures.keys()))  # [\"serving_default\"]\r\n    infer = loaded.signatures[\"serving_default\"]\r\n    print(infer.structured_outputs)\r\n\r\n    img_url = 'https://i.dailymail.co.uk/1s/2019/11/23/09/21370544-7717313-image-a-1_1574501083030.jpg'\r\n    image_data = urlopen(img_url).read()\r\n    decoded_data = tf.io.decode_jpeg(image_data, channels=3)\r\n    decoded_data = tf.expand_dims(decoded_data, 0)\r\n\r\n    with tf.device(\"/device:GPU:0\"):\r\n        print_ram('after device')\r\n        for i in range(10000):\r\n            infer(decoded_data)\r\n            print_ram('after run')\r\n```\r\n\r\nNote also that the new tf seems to take around ~500MB more than tf 1.x (both with `-D_GLIBCXX_USE_CXX11_ABI=0`)."]}, {"number": 35080, "title": "Start background prefetching without calling next on dataset iterator", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently if you have a dataset with a large shuffle buffer, it doesn't start trying to populate that buffer until the first request to retrieve data from the data set. This is a lost opportunity for parallelism during training script startup - the shuffle buffer could be filling in the background while the model is compiling.  \r\nSince calling next is blocking, its not possible to trigger this overlap to happen. \r\n\r\n**Will this change the current api? How?** Maybe - could add a start_prefetching() method.  The other idea I had was to make it implicit in calling __iter__ - but that is possibly more useful for my specific use case than more generally.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who cares about the startup time of their training script and uses a large shuffle buffer.\r\n\r\n**Any Other info.**\r\n", "comments": ["I agree that it would be generally beneficial to be able to overlap the \"warming up\" of the input pipeline with other computation (such as model compilation).\r\n\r\nHowever, it is non-trivial to provide this functionality given the current tf.data implementation. Each tf.data transformation has its own C++ iterator implementation with a \"GetNext\" method which upon its first invocation starts the background activity (such as prefetching). To add support for \"start prefetching\" we would need to add a new method to all C++ iterator implementations which starts the background activity (if the iterator is asynchronous) or recursively invokes this method on the input iterators (if the iterator is synchronous).\r\n\r\nAs an additional concern, if the input pipeline used custom tf.data transformations (e.g. from [SIG I/O](https://github.com/tensorflow/io)) they would also need to implement this method in order for the \"start prefetch\" functionality to work correctly across all input pipelines.\r\n\r\nAll in all, I think that the current implementation prevents the functionality to be implemented in an effective and robust manner and for that reason, the tf.data team does not plan to support such functionality.\r\n\r\nPS: If you are writing your own custom training loop (and calling `iter` and `next` explicitly), then you should be able to use [tf.group](https://www.tensorflow.org/api_docs/python/tf/group?version=stable) or [tf.tuple](https://www.tensorflow.org/api_docs/python/tf/tuple?version=stable) + `tf.py_function` to wrap the model compilation to parallelize the model compilation and fetching of the first element.", "Thanks for the detailed analysis.\r\n\r\nJust to check - did you consider a variant where the prefetch transform takes an optional parameter 'eager prefetch'?  If the prefetch transform decouples things, (as was my understanding without having dug into the code) - when provided the eager prefetch flag it would invoke the standard GetNext in the background, in advance of the first attempt to invoke iter/GetNext on the prefetch itself.  This doesn't seem to need an extensive API redesign - although it does assume a lot about  how prefetch works.", "That's an interesting idea. `prefetch` is not the only asychronous transformation though (parallel map and parallel interleave are examples of others).\r\n\r\nHaving said that, I can imagine having a tf.data.Option that specify whether input pipeline should start background threads eagerly (similar to how you can specify whether input pipeline is allowed to execute in a non-deterministic fashion). This option would then trigger a graph rewrite at input pipeline instantiation time, which would set the execution mode for all asynchronous ops in the input pipeline.\r\n\r\nIn other words, this could work. I will re-open this issue and mark it as contributions welcome as the tf.data team will not have cycles to prioritize this in Q1."]}, {"number": 35063, "title": "No example provided for using tf.nn.ctc_loss", "body": "## URL with the issue: https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss\r\n\r\n## Description of issue:\r\nThere's no example provided for using this loss and I cannot make it work.\r\nFollowing the parameters definitions I created this toy example in tf2.0.0:\r\n\r\n```\r\nimport functools\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import (Input, Conv2D, Lambda)\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n# INPUTS\r\ninputs = Input(shape=[128, 64, 1], batch_size=32)    # [frames, num_labels, channels]\r\nlabels = Input(shape=[128], batch_size=32,  dtype=tf.int32)\r\nlabel_length = tf.constant(np.ones((32)), dtype=tf.int32)\r\nlogit_length = tf.constant(np.ones((32)),  dtype=tf.int32)\r\n# MODEL\r\nx = Conv2D(1, kernel_size=(5, 5),  padding='same')(inputs)\r\nlogits = Lambda(lambda z: tf.squeeze(z, [-1]))(x)\r\nmodel = Model(inputs, logits)\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=tf.nn.ctc_loss(\r\n    labels=labels, logits=logits, label_length=label_length,\r\n    logit_length=logit_length, logits_time_major=False,\r\n    blank_index=-1\r\n))\r\n```\r\n\r\nwhich rises: \r\n\r\n```\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    469                 dtype=dtype if dtype else None,\r\n    470                 preferred_dtype=default_dtype,\r\n--> 471                 as_ref=input_arg.is_ref)\r\n    472             if input_arg.number_attr and len(\r\n    473                 set(v.dtype.base_dtype for v in values)) > 1:\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1363             as_ref=as_ref,\r\n   1364             preferred_dtype=preferred_dtype,\r\n-> 1365             ctx=ctx))\r\n   1366   return ret\r\n   1367\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\r\n   1262     graph = get_default_graph()\r\n   1263     if not graph.building_function:\r\n-> 1264       raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\n   1265                          \"building a function.\")\r\n   1266     return graph.capture(value, name=name)\r\n\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n```\r\n\r\nThen, I tried to use it as a handle:\r\n\r\n```\r\nctc_loss = functools.partial(\r\n    tf.nn.ctc_loss,\r\n    labels,         # labels\r\n    logits,         # logits\r\n    label_length,   # label_length\r\n    logit_length,   # logit_length\r\n    False,          # logits_time_major\r\n    -1,             # blank_index\r\n)\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=ctc_loss)\r\n```\r\n\r\nwhich rises:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-147-2018e8450f34> in <module>\r\n----> 1 model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss)\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    371\r\n    372       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 373       self._compile_weights_loss_and_weighted_metrics()\r\n    374\r\n    375       # Functions for train, test and predict will\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1651       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1652       #                   layer losses.\r\n-> 1653       self.total_loss = self._prepare_total_loss(masks)\r\n   1654\r\n   1655   def _prepare_skip_target_masks(self):\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)\r\n   1732             # differentiate between use case where a custom optimizer\r\n   1733             # expects a vector loss value vs unreduced per-sample loss value.\r\n-> 1734             output_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\r\n   1735             loss_reduction = losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\r\n   1736\r\n\r\nTypeError: ctc_loss_v2() got an unexpected keyword argument 'sample_weight'\r\n```\r\n\r\nThen, I tried to embed it:\r\n\r\n```\r\ndef my_ctc_loss(\r\n    labels, logits, label_length, logit_length, logits_time_major,\r\n    blank_index, sample_weight\r\n):\r\n    return tf.nn.ctc_loss(\r\n        labels=labels, logits=logits, label_length=label_length,\r\n        logit_length=logit_length, logits_time_major=logits_time_major,\r\n        blank_index=blank_index\r\n    )\r\n\r\n\r\nctc_loss_emb = functools.partial(\r\n    my_ctc_loss,\r\n    labels,         # labels\r\n    logits,         # logits\r\n    label_length,   # label_length\r\n    logit_length,   # logit_length\r\n    False,          # logits_time_major\r\n    -1,             # blank_index\r\n    None,           # sample_weight\r\n)\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)\r\n```\r\nwhich rises: \r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-150-f20d10a91540> in <module>\r\n      9     None,           # sample_weight\r\n     10 )\r\n---> 11 model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    371\r\n    372       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 373       self._compile_weights_loss_and_weighted_metrics()\r\n    374\r\n    375       # Functions for train, test and predict will\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1651       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1652       #                   layer losses.\r\n-> 1653       self.total_loss = self._prepare_total_loss(masks)\r\n   1654\r\n   1655   def _prepare_skip_target_masks(self):\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)\r\n   1732             # differentiate between use case where a custom optimizer\r\n   1733             # expects a vector loss value vs unreduced per-sample loss value.\r\n-> 1734             output_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\r\n   1735             loss_reduction = losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\r\n   1736\r\n\r\nTypeError: my_ctc_loss() got multiple values for argument 'sample_weight'\r\n```\r\n\r\n### Usage example: Not provided\r\n\r\nSince it seems that ctc_loss has to be used differently from other losses, it will helpful to have an example that shows how to use it.\r\n\r\n### Raises listed and defined: Not defined\r\n\r\n\r\nThanks!", "comments": ["Hello. Is there any progress regarding this issue? I still haven't found a way of using this loss. \r\n\r\nThanks!\r\n\r\n", "Can you try it as part of a Lambda layer and use a loss with compile that\ndoes a reduce_sum?  CTC loss is not a keras loss and returns a per batch\nentry loss vector.\n\nOn Mon, Dec 23, 2019, 3:28 PM Yasir Modak <notifications@github.com> wrote:\n\n> Assigned #35063 <https://github.com/tensorflow/tensorflow/issues/35063>\n> to @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35063?email_source=notifications&email_token=AANWFGYWVADJ7MAWKQMUZODQ2FCRTA5CNFSM4J2DC2K2YY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOVUZUULY#event-2905819695>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG6ZWV756Z7SSORERE3Q2FCRTANCNFSM4J2DC2KQ>\n> .\n>\n", "Do you mean something like? \r\n```\r\ndef ctc_layer(labels, label_length, logit_length,\r\n                   blank_index=-1, logits_time_major=False):\r\n    def func(x):\r\n        return tf.nn.ctc_loss(\r\n                logits=x,\r\n                labels=labels,\r\n                label_length=label_length,\r\n                logit_length=logit_length,\r\n                blank_index=blank_index, logits_time_major=logits_time_major)\r\n    return Lambda(func)\r\n\r\n# INPUTS\r\ninputs = Input(shape=[128, 64, 1], batch_size=32)    # [frames, num_labels, channels]\r\nlabels = Input(shape=[128], batch_size=32,  dtype=tf.int32)\r\nlabel_length = tf.constant(np.ones((32)), dtype=tf.int32)\r\nlogit_length = tf.constant(np.ones((32)),  dtype=tf.int32)\r\n# MODEL\r\nx = Conv2D(1, kernel_size=(5, 5),  padding='same')(inputs)\r\nlogits = Lambda(lambda z: tf.squeeze(z, [-1]))(x)\r\nx = ctc_layer(labels, label_length, logit_length)(logits)\r\nmodel = Model(inputs, x)\r\nmodel.summary()\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_13 (InputLayer)        [(32, 128, 64, 1)]        0\r\n_________________________________________________________________\r\nconv2d_6 (Conv2D)            (32, 128, 64, 1)          26\r\n_________________________________________________________________\r\nlambda_10 (Lambda)           (32, 128, 64)             0\r\n_________________________________________________________________\r\nlambda_11 (Lambda)           (32,)                     0\r\n=================================================================\r\nTotal params: 26\r\nTrainable params: 26\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\nBut then, in the compiler I don't know what to use as loss. I've checked the [doc](https://www.tensorflow.org/api_docs/python/tf/keras/losses) and all the losses need a y_true (`labels` in the ctc_loss and already included in the ctc_layer). I tried with just reducing the output tensor with:\r\n\r\n```\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=tf.reduce_mean)\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=tf.reduce_sum)\r\n```\r\n\r\nBut as expected is not working:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-26-3091b5b8ce35> in <module>\r\n----> 1 model.compile(optimizer=Adam(lr=0.001), loss=tf.reduce_sum)\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    371\r\n    372       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 373       self._compile_weights_loss_and_weighted_metrics()\r\n    374\r\n    375       # Functions for train, test and predict will\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1651       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1652       #                   layer losses.\r\n-> 1653       self.total_loss = self._prepare_total_loss(masks)\r\n   1654\r\n   1655   def _prepare_skip_target_masks(self):\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)\r\n   1711\r\n   1712           if hasattr(loss_fn, 'reduction'):\r\n-> 1713             per_sample_losses = loss_fn.call(y_true, y_pred)\r\n   1714             weighted_losses = losses_utils.compute_weighted_loss(\r\n   1715                 per_sample_losses,\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py in call(self, y_true, y_pred)\r\n    219       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\r\n    220           y_pred, y_true)\r\n--> 221     return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    222\r\n    223   def get_config(self):\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py in reduce_sum(input_tensor, axis, keepdims, name)\r\n   1573       gen_math_ops._sum(\r\n   1574           input_tensor, _ReductionDims(input_tensor, axis), keepdims,\r\n-> 1575           name=name))\r\n   1576\r\n   1577\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py in _sum(input, axis, keep_dims, name)\r\n  11166   _, _, _op = _op_def_lib._apply_op_helper(\r\n  11167         \"Sum\", input=input, reduction_indices=axis, keep_dims=keep_dims,\r\n> 11168                name=name)\r\n  11169   _result = _op.outputs[:]\r\n  11170   _inputs_flat = _op.inputs\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    629               _SatisfiesTypeConstraint(base_type,\r\n    630                                        _Attr(op_def, input_arg.type_attr),\r\n--> 631                                        param_name=input_name)\r\n    632             attrs[input_arg.type_attr] = attr_value\r\n    633             inferred_from[input_arg.type_attr] = input_name\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)\r\n     58           \"allowed values: %s\" %\r\n     59           (param_name, dtypes.as_dtype(dtype).name,\r\n---> 60            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n     61\r\n     62\r\n\r\nTypeError: Value passed to parameter 'reduction_indices' has DataType float32 not in list of allowed values: int32, int64\r\n```\r\n\r\nAny other suggestion? Thanks\r\n", "Hmm; good point.  You could write a loss function that calls ctc_loss,\naccepts an extra sample_weights argument (ignores it), and returns the\nreduce_sum(ctc_loss(...)); and pass that as the loss to compile.\n\nOn Sat, Dec 28, 2019 at 10:48 AM Gabriel Meseguer-Brocal <\nnotifications@github.com> wrote:\n\n> Do you mean something like?\n>\n> def ctc_layer(labels, label_length, logit_length,\n>                    blank_index=-1, logits_time_major=False):\n>     def func(x):\n>         return tf.nn.ctc_loss(\n>                 logits=x,\n>                 labels=labels,\n>                 label_length=label_length,\n>                 logit_length=logit_length,\n>                 blank_index=blank_index, logits_time_major=logits_time_major)\n>     return Lambda(func)\n>\n> # INPUTS\n> inputs = Input(shape=[128, 64, 1], batch_size=32)    # [frames, num_labels, channels]\n> labels = Input(shape=[128], batch_size=32,  dtype=tf.int32)\n> label_length = tf.constant(np.ones((32)), dtype=tf.int32)\n> logit_length = tf.constant(np.ones((32)),  dtype=tf.int32)\n> # MODEL\n> x = Conv2D(1, kernel_size=(5, 5),  padding='same')(inputs)\n> logits = Lambda(lambda z: tf.squeeze(z, [-1]))(x)\n> x = ctc_layer(labels, label_length, logit_length)(logits)\n> model = Model(inputs, x)\n> model.summary()\n>\n> _________________________________________________________________\n> Layer (type)                 Output Shape              Param #\n> =================================================================\n> input_13 (InputLayer)        [(32, 128, 64, 1)]        0\n> _________________________________________________________________\n> conv2d_6 (Conv2D)            (32, 128, 64, 1)          26\n> _________________________________________________________________\n> lambda_10 (Lambda)           (32, 128, 64)             0\n> _________________________________________________________________\n> lambda_11 (Lambda)           (32,)                     0\n> =================================================================\n> Total params: 26\n> Trainable params: 26\n> Non-trainable params: 0\n> _________________________________________________________________\n>\n> But then, in the compiler I don't know what to use as loss. I've checked\n> the doc <https://www.tensorflow.org/api_docs/python/tf/keras/losses> and\n> all the losses need a y_true (labels in the ctc_loss and already included\n> in the ctc_layer). I tried with just reducing the output tensor with:\n>\n> model.compile(optimizer=Adam(lr=0.001), loss=tf.reduce_mean)\n> model.compile(optimizer=Adam(lr=0.001), loss=tf.reduce_sum)\n>\n> But as expected is not working:\n>\n> ---------------------------------------------------------------------------\n> TypeError                                 Traceback (most recent call last)\n> <ipython-input-26-3091b5b8ce35> in <module>\n> ----> 1 model.compile(optimizer=Adam(lr=0.001), loss=tf.reduce_sum)\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\n>     455     self._self_setattr_tracking = False  # pylint: disable=protected-access\n>     456     try:\n> --> 457       result = method(self, *args, **kwargs)\n>     458     finally:\n>     459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\n>     371\n>     372       # Creates the model loss and weighted metrics sub-graphs.\n> --> 373       self._compile_weights_loss_and_weighted_metrics()\n>     374\n>     375       # Functions for train, test and predict will\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\n>     455     self._self_setattr_tracking = False  # pylint: disable=protected-access\n>     456     try:\n> --> 457       result = method(self, *args, **kwargs)\n>     458     finally:\n>     459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\n>    1651       #                   loss_weight_2 * output_2_loss_fn(...) +\n>    1652       #                   layer losses.\n> -> 1653       self.total_loss = self._prepare_total_loss(masks)\n>    1654\n>    1655   def _prepare_skip_target_masks(self):\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)\n>    1711\n>    1712           if hasattr(loss_fn, 'reduction'):\n> -> 1713             per_sample_losses = loss_fn.call(y_true, y_pred)\n>    1714             weighted_losses = losses_utils.compute_weighted_loss(\n>    1715                 per_sample_losses,\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py in call(self, y_true, y_pred)\n>     219       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n>     220           y_pred, y_true)\n> --> 221     return self.fn(y_true, y_pred, **self._fn_kwargs)\n>     222\n>     223   def get_config(self):\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)\n>     178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\n>     179     try:\n> --> 180       return target(*args, **kwargs)\n>     181     except (TypeError, ValueError):\n>     182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py in reduce_sum(input_tensor, axis, keepdims, name)\n>    1573       gen_math_ops._sum(\n>    1574           input_tensor, _ReductionDims(input_tensor, axis), keepdims,\n> -> 1575           name=name))\n>    1576\n>    1577\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py in _sum(input, axis, keep_dims, name)\n>   11166   _, _, _op = _op_def_lib._apply_op_helper(\n>   11167         \"Sum\", input=input, reduction_indices=axis, keep_dims=keep_dims,\n> > 11168                name=name)\n>   11169   _result = _op.outputs[:]\n>   11170   _inputs_flat = _op.inputs\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\n>     629               _SatisfiesTypeConstraint(base_type,\n>     630                                        _Attr(op_def, input_arg.type_attr),\n> --> 631                                        param_name=input_name)\n>     632             attrs[input_arg.type_attr] = attr_value\n>     633             inferred_from[input_arg.type_attr] = input_name\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)\n>      58           \"allowed values: %s\" %\n>      59           (param_name, dtypes.as_dtype(dtype).name,\n> ---> 60            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n>      61\n>      62\n>\n> TypeError: Value passed to parameter 'reduction_indices' has DataType float32 not in list of allowed values: int32, int64\n>\n> Any other suggestion? Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35063?email_source=notifications&email_token=AANWFG37P4XUGSO7MGDJJVDQ26NPFA5CNFSM4J2DC2K2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHYP5OI#issuecomment-569441977>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFGYXHMX3IPPG76YP7KTQ26NPFANCNFSM4J2DC2KQ>\n> .\n>\n", "Do you mean something like this (my_ctc_loss returns a scalar -tf.reduce_mean- not a tensor)?\r\n\r\n```\r\ndef my_ctc_loss(\r\n    labels, logits, label_length, logit_length, logits_time_major,\r\n    blank_index, sample_weight\r\n):\r\n    return tf.reduce_mean(tf.nn.ctc_loss(\r\n        labels=labels, logits=logits, label_length=label_length,\r\n        logit_length=logit_length, logits_time_major=logits_time_major,\r\n        blank_index=blank_index\r\n    ))\r\n\r\n# INPUTS\r\ninputs = Input(shape=[128, 64, 1], batch_size=32)    # [frames, num_labels, channels]\r\nlabels = Input(shape=[128], batch_size=32,  dtype=tf.int32)\r\nlabel_length = tf.constant(np.ones((32)), dtype=tf.int32)\r\nlogit_length = tf.constant(np.ones((32)),  dtype=tf.int32)\r\n# MODEL\r\nx = Conv2D(1, kernel_size=(5, 5),  padding='same')(inputs)\r\nlogits = Lambda(lambda z: tf.squeeze(z, [-1]))(x)\r\nmodel = Model(inputs, logits)\r\n# LOSSS\r\nctc_loss_emb = functools.partial(\r\n    my_ctc_loss,\r\n    labels,         # labels\r\n    logits,         # logits\r\n    label_length,   # label_length\r\n    logit_length,   # logit_length\r\n    False,          # logits_time_major\r\n    -1,             # blank_index\r\n    None,           # sample_weight\r\n)\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)\r\n```\r\n\r\nNot working either:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-d100089d6156> in <module>\r\n     20     None,           # sample_weight\r\n     21 )\r\n---> 22 model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    371\r\n    372       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 373       self._compile_weights_loss_and_weighted_metrics()\r\n    374\r\n    375       # Functions for train, test and predict will\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1651       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1652       #                   layer losses.\r\n-> 1653       self.total_loss = self._prepare_total_loss(masks)\r\n   1654\r\n   1655   def _prepare_skip_target_masks(self):\r\n\r\n~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)\r\n   1732             # differentiate between use case where a custom optimizer\r\n   1733             # expects a vector loss value vs unreduced per-sample loss value.\r\n-> 1734             output_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\r\n   1735             loss_reduction = losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\r\n   1736\r\n\r\nTypeError: my_ctc_loss() got multiple values for argument 'sample_weight'\r\n```\r\n\r\nI've tried this:\r\n```\r\n@tf.function(autograph=False)\r\ndef sparse_tuple_from(data):\r\n    def py_sparse_tuple_from(sequences):\r\n        \"\"\"   \"\"\"\r\n        indices = []\r\n        values = []\r\n        for n, seq in enumerate(sequences):\r\n            indices.extend(zip([n] * len(seq), range(len(seq))))\r\n            values.extend(seq)\r\n        indices = np.asarray(indices, dtype=np.int64)\r\n        values = np.asarray(values, dtype=np.int32)\r\n        shape = np.asarray(\r\n            [len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\r\n        return indices, values, shape\r\n    return tf.py_function(\r\n        py_sparse_tuple_from, [data], (tf.int64, tf.int32, tf.int64))\r\n\r\n\r\nclass MyCTCLoss(tensorflow.losses.Loss):\r\n    def __init__(self, logit_length, blank_index=-1, logits_time_major=False):\r\n        super(MyCTCLoss, self).__init__()\r\n        self.logit_length = logit_length\r\n        self.blank_index = blank_index\r\n        self.logits_time_major = logits_time_major\r\n\r\n    def call(self, y_true, y_pred):\r\n        indices, values, shape = sparse_tuple_from(y_true)\r\n        labels_ctc = tf.sparse.SparseTensor(\r\n            indices=indices, values=values, dense_shape=shape)\r\n        return tf.reduce_mean(tf.nn.ctc_loss(\r\n            labels=labels_ctc, logits=y_pred, label_length=None,\r\n            logit_length=logit_length,\r\n            logits_time_major=self.logits_time_major,\r\n            blank_index=self.blank_index\r\n        ))\r\n\r\nmy_loss = MyCTCLoss(32)  # 32 batch size\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=my_loss)\r\n```\r\n\r\nWhich at least compiles but not sure if trains or not. Additionally, as far as I understand the [doc](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss?version=stable), using a SparseTensor force the loss to be computed in the CPU not the GPU:\r\n\r\n- On TPU and GPU: Only dense padded labels are supported.\r\n- On CPU: Caller may use SparseTensor or dense padded labels but calling with a SparseTensor will be significantly faster. \r\n", "Are you using GPU?  I believe the latter approach will work if you use\ndensified labels.\n\nOn Sat, Dec 28, 2019, 11:45 AM Gabriel Meseguer-Brocal <\nnotifications@github.com> wrote:\n\n> Do you mean something like this (my_ctc_loss returns a scalar\n> -tf.reduce_mean- not a tensor)?\n>\n> def my_ctc_loss(\n>     labels, logits, label_length, logit_length, logits_time_major,\n>     blank_index, sample_weight\n> ):\n>     return tf.reduce_mean(tf.nn.ctc_loss(\n>         labels=labels, logits=logits, label_length=label_length,\n>         logit_length=logit_length, logits_time_major=logits_time_major,\n>         blank_index=blank_index\n>     ))\n>\n> # INPUTS\n> inputs = Input(shape=[128, 64, 1], batch_size=32)    # [frames, num_labels, channels]\n> labels = Input(shape=[128], batch_size=32,  dtype=tf.int32)\n> label_length = tf.constant(np.ones((32)), dtype=tf.int32)\n> logit_length = tf.constant(np.ones((32)),  dtype=tf.int32)\n> # MODEL\n> x = Conv2D(1, kernel_size=(5, 5),  padding='same')(inputs)\n> logits = Lambda(lambda z: tf.squeeze(z, [-1]))(x)\n> model = Model(inputs, logits)\n> # LOSSS\n> ctc_loss_emb = functools.partial(\n>     my_ctc_loss,\n>     labels,         # labels\n>     logits,         # logits\n>     label_length,   # label_length\n>     logit_length,   # logit_length\n>     False,          # logits_time_major\n>     -1,             # blank_index\n>     None,           # sample_weight\n> )\n> model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)\n>\n> Not working either:\n>\n> ---------------------------------------------------------------------------\n> TypeError                                 Traceback (most recent call last)\n> <ipython-input-3-d100089d6156> in <module>\n>      20     None,           # sample_weight\n>      21 )\n> ---> 22 model.compile(optimizer=Adam(lr=0.001), loss=ctc_loss_emb)\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\n>     455     self._self_setattr_tracking = False  # pylint: disable=protected-access\n>     456     try:\n> --> 457       result = method(self, *args, **kwargs)\n>     458     finally:\n>     459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\n>     371\n>     372       # Creates the model loss and weighted metrics sub-graphs.\n> --> 373       self._compile_weights_loss_and_weighted_metrics()\n>     374\n>     375       # Functions for train, test and predict will\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\n>     455     self._self_setattr_tracking = False  # pylint: disable=protected-access\n>     456     try:\n> --> 457       result = method(self, *args, **kwargs)\n>     458     finally:\n>     459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\n>    1651       #                   loss_weight_2 * output_2_loss_fn(...) +\n>    1652       #                   layer losses.\n> -> 1653       self.total_loss = self._prepare_total_loss(masks)\n>    1654\n>    1655   def _prepare_skip_target_masks(self):\n>\n> ~/.virtualenvs/phd/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _prepare_total_loss(self, masks)\n>    1732             # differentiate between use case where a custom optimizer\n>    1733             # expects a vector loss value vs unreduced per-sample loss value.\n> -> 1734             output_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n>    1735             loss_reduction = losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\n>    1736\n>\n> TypeError: my_ctc_loss() got multiple values for argument 'sample_weight'\n>\n> I've tried this:\n>\n> @tf.function(autograph=False)\n> def sparse_tuple_from(data):\n>     def py_sparse_tuple_from(sequences):\n>         \"\"\"   \"\"\"\n>         indices = []\n>         values = []\n>         for n, seq in enumerate(sequences):\n>             indices.extend(zip([n] * len(seq), range(len(seq))))\n>             values.extend(seq)\n>         indices = np.asarray(indices, dtype=np.int64)\n>         values = np.asarray(values, dtype=np.int32)\n>         shape = np.asarray(\n>             [len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n>         return indices, values, shape\n>     return tf.py_function(\n>         py_sparse_tuple_from, [data], (tf.int64, tf.int32, tf.int64))\n>\n>\n> class MyCTCLoss(tensorflow.losses.Loss):\n>     def __init__(self, logit_length, blank_index=-1, logits_time_major=False):\n>         super(MyCTCLoss, self).__init__()\n>         self.logit_length = logit_length\n>         self.blank_index = blank_index\n>         self.logits_time_major = logits_time_major\n>\n>     def call(self, y_true, y_pred):\n>         indices, values, shape = sparse_tuple_from(y_true)\n>         labels_ctc = tf.sparse.SparseTensor(\n>             indices=indices, values=values, dense_shape=shape)\n>         return tf.reduce_mean(tf.nn.ctc_loss(\n>             labels=labels_ctc, logits=y_pred, label_length=None,\n>             logit_length=logit_length,\n>             logits_time_major=self.logits_time_major,\n>             blank_index=self.blank_index\n>         ))\n>\n> my_loss = MyCTCLoss(32)  # 32 batch size\n> model.compile(optimizer=Adam(lr=0.001), loss=my_loss)\n>\n> Which at least compiles but not sure if trains or not. Additionally, as\n> far as I understand the doc\n> <https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss?version=stable>,\n> using a SparseTensor force the loss to be computed in the CPU not the GPU:\n>\n>    - On TPU and GPU: Only dense padded labels are supported.\n>    - On CPU: Caller may use SparseTensor or dense padded labels but\n>    calling with a SparseTensor will be significantly faster.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35063?email_source=notifications&email_token=AANWFGYKXRCPYNJFEBHYCRLQ26UEXA5CNFSM4J2DC2K2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHYQ3ZQ#issuecomment-569445862>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG5FVJ7KMJ4VEETJKJTQ26UEXANCNFSM4J2DC2KQ>\n> .\n>\n", "Yes, I'm using GPU. I've already adapted it to use densified labels. But it seems that is not using the GPU because it is very slow with a lot of picks in the usage which not happening when I use a different loss. Do I need to add something in my customize CTC loss to be sure it uses the GPU?\r\n\r\nThanks\r\n\r\n```\r\n@tf.function(autograph=False)\r\ndef get_labels(data):\r\n    def py_get_labels(y_true):\r\n        y_true = y_true.numpy().astype(np.int64)\r\n        label_length = np.array([i[-1] for i in y_true]).astype(np.int32)  # the labels length is codify in the target sequence \r\n        labels = np.zeros(\r\n            (len(label_length), np.max(label_length))).astype(np.int64)\r\n        for nxd, i in enumerate(y_true):\r\n            labels[nxd, :i[-1]] = i[:i[-1]].astype(np.int64)\r\n        return labels, label_length\r\n    return tf.py_function(py_get_labels, [data], (tf.int64, tf.int32))\r\n    \r\n\r\nclass CTCLoss(tf.losses.Loss):\r\n    def __init__(self, logit_length, blank_index=-1, logits_time_major=False):\r\n        super(CTCLoss, self).__init__()\r\n        self.logit_length = tf.convert_to_tensor(logit_length)\r\n        self.blank_index = blank_index\r\n        self.logits_time_major = logits_time_major\r\n\r\n    def call(self, y_true, y_pred):\r\n        labels, label_length = get_labels(y_true)\r\n        return tf.reduce_mean(tf.nn.ctc_loss(\r\n            labels=labels, logits=y_pred, label_length=label_length,\r\n            logit_length=self.logit_length,\r\n            logits_time_major=self.logits_time_major,\r\n            blank_index=self.blank_index\r\n        ))\r\n```", "@gabolsgabs, were u able to implement CTC successfully? any updates", "@neenaloysius Now, Tensorflow has been upgraded to 2.2.0-rc0, and the release doc say:\r\n```\r\nModel.fit major improvements:\r\n- You can now use custom training logic with Model.fit by overriding Model.train_step.\r\n- Easily write state-of-the-art training loops without worrying about all of the features Model.fit \r\n  handles for you (distribution strategies, callbacks, data formats, looping logic, etc)\r\n- See the default Model.train_step for an example of what this function should look like\r\n- Same applies for validation and inference via Model.test_step and Model.predict_step\r\n```\r\nAccording to the above description, I think that can be used on custom the ctc training loop, so I try to write a train_step and override it, here is code:\r\n```python\r\n...\r\ndef train_step(self, data):\r\n    data = data_adapter.expand_1d(data)\r\n    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\r\n\r\n    with tf.GradientTape() as tape:\r\n        logits = self(x, training=True)\r\n        logit_length = tf.fill([tf.shape(logits)[0]], tf.shape(logits)[1])\r\n        loss = tf.nn.ctc_loss(\r\n            labels=y,\r\n            logits=logits,\r\n            label_length=None,\r\n            logit_length=logit_length,\r\n            logits_time_major=False,\r\n            blank_index=-1)\r\n        loss = tf.reduce_mean(loss)\r\n    trainable_variables = self.trainable_variables\r\n    grads = tape.gradient(loss, trainable_variables)\r\n    self.optimizer.apply_gradients(zip(grads, trainable_variables))\r\n    return {'loss': loss}\r\n...\r\nmodel = ...\r\nmodel.compile(optimizer='adam')\r\nmodel.train_step = functools.partial(train_step, model)\r\nmodel.fit(...)\r\n```\r\nIt works.  And I found in 2.2 version, we can pass the SparseTensor into keras loop, so maybe a good way is inherit the loss class. Here is a [example](https://github.com/FLming/CRNN.tf2/blob/master/crnn/losses.py)\r\n"]}, {"number": 35043, "title": "TF2.0: No way to create local variables in tensorflow 2.0", "body": "In tensorflow 1.x, user can create local variables, whose lifetime spans over multiple session runs, but will not be saved to checkpoints. Those local variables can be used as internal states of session (e.g., the hidden states of an online rnn). In tensorflow 2.x, session is gone in python side, but when deploy the model in a c++ program we still need to use session, and the local variables can be used to store session states of the model. However, in tensorflow 2.x, there is no standard way to create local variables in python, because all variables created in a Module/Layer are automatically tracked and stored into checkpoints.", "comments": ["@x10000year \r\n\r\nCan you please help us with the minimal standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram \r\n\r\nConsider the following TF1.x example code:\r\n\r\n```\r\n# in real application, we may have many different variables to represent rnn hidden states\r\nstate = tf.Variable(\r\n    name='state',\r\n    initial_value = tf.zeros([]),\r\n    trainable=False,\r\n    collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n\r\n# reset the rnn state\r\ninit_op = state.assign(0.0, name='init_state')\r\n\r\n# update the rnn state. in real application, it can be one step of a complex rnn model\r\nupdate_op = state.assign_add(1.0, name='update_state')\r\n```\r\n\r\nAfter exporting the model, a c++ program can load the model and use init_state and update_state operations to manipulate the hidden state of the model in a session. The state variable is, however, not saved, because it is not the parameter of the model.\r\n\r\nIn TF2.x, if we implement the model with tf.Module:\r\n\r\n```\r\nclass MyModel(tf.Module):\r\n    def __init__():\r\n        self.state = tf.Variable(name='state',\r\n            initial_value = tf.zeros([]),\r\n            trainable=False)\r\n    @tf.function(input_signature=[])\r\n    def init_state():\r\n        self.state.assign(0.0)\r\n    @tf.function(input_signature=[])\r\n    def update_state():\r\n        self.state.assign_add(1.0)\r\n```\r\n\r\nThen self.state will be tracked automatically, and will be saved to SavedModel, which is undesired. The state tensor can be very big in real application.", "I just thought about the same thing and found this class attribute from the source code: tf.Module._TF_MODULE_IGNORED_PROPERTIES\r\nMaybe it can solve the problem?", "@x10000year,\r\nSorry for the delayed response. Can you please let us know if the [above comment](https://github.com/tensorflow/tensorflow/issues/35043#issuecomment-704214576) has resolved the problem so that we can work towards the closure of this issue? Thanks!", "This issue is being worked on internally, but for now remains unresolved.\r\n\r\nFor the time being, the best workaround in TF 2 is to avoid the use of `tf.Variable`. That might prove challenging with tf.Module, but applying the tf.function at as high a level as possible might help. For example:\r\n\r\n```\r\nclass MyModel(tf.Module):\r\n    def __init__():\r\n        self.state = tf.zeros([])\r\n\r\n    def init_state():\r\n        self.state = tf.zeros([])\r\n\r\n    def update_state():\r\n        self.state += 1\r\n\r\n@tf.function\r\ndef main():\r\n  m = MyModel\r\n  # all code working on m should be called here\r\n```\r\n\r\nEven better, if at all practical for you, pure functional programming patterns are much better supported, where you use plain data structures and plain functions, and all inputs are function arguments and all outputs are return values. So for example:\r\n\r\n```\r\nMyData = collections.namedtuple('MyData', ['state'])\r\n\r\ndef init():\r\n  return MyData(tf.zeros([]))\r\n\r\ndef update(old_data):\r\n  return MyData(old_data.state + 1)\r\n```"]}, {"number": 35012, "title": "Gradient of matmul in while_loop works when run eagerly but not as tf.function", "body": "**System information**\r\n- Have I written custom code: Example provided below\r\n- OS Platform and Distribution: Both Windows 10 and Google Colab\r\n- TensorFlow installed from binary\r\n- TensorFlow version: Both 2.0.0 and 2.1.0-rc0\r\n- Python version: 3.6 and 3.7.4\r\n\r\n**Describe the current behavior**\r\nIf I multiply tensors together in a while_loop I can compute higher order gradients when running eagerly, but not when running inside a function with the @tf.function decorator. If I manually unroll the loop, it works either way, so the problem must be related to the interaction between tf.function and tf.while_loop.\r\n\r\n> TypeError: in converted code:\r\n> \r\n>     <ipython-input-17-50c1c9a72b0f>:17 func  *\r\n>         d3y_dx3 = t.gradient(d2y_dx2, x)\r\n>     /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:999 gradient\r\n>         if not backprop_util.IsTrainable(t):\r\n>     /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop_util.py:30 IsTrainable\r\n>         dtype = dtypes.as_dtype(dtype)\r\n>     /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/dtypes.py:725 as_dtype\r\n>         (type_value,))\r\n> \r\n>     TypeError: Cannot convert value None to a TensorFlow DType.\r\n\r\n**Describe the expected behavior**\r\nThe output of this function should be the same regardless of the @tf.function decorator:\r\n> [[0 0 4]]\r\n> [[[-2 1 10]]]\r\n> [[[8 2 20]]]\r\n> [[[-18 6 30]]]\r\n> [[[24 24 24]]]\r\n\r\n**Code to reproduce the issue**\r\nRun without and then with @tf.function:\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable([[[-1.0]], [[0.0]], [[1.0]]])\r\n\r\n# @tf.function\r\ndef func():\r\n    with tf.GradientTape(persistent=True) as t:\r\n        t.watch(x)\r\n        pv = x\r\n        \r\n        n = 4\r\n        c = lambda i, _: i < n\r\n        b = lambda i, pv: (i+1, tf.concat([x, tf.matmul(pv, x)], axis=1))\r\n        _, pv = tf.while_loop(c, b, (tf.constant(1), pv), shape_invariants=(tf.constant(1).shape, tf.TensorShape([3, None, 1])))\r\n\r\n        # manual loop unrolling works fine both ways:\r\n        # pv = tf.concat([x, tf.matmul(pv, x)], axis=1)\r\n        # pv = tf.concat([x, tf.matmul(pv, x)], axis=1)\r\n        # pv = tf.concat([x, tf.matmul(pv, x)], axis=1)\r\n        \r\n        y = tf.reduce_sum(pv, axis=1)\r\n        dy_dx = t.gradient(y, x)\r\n        d2y_dx2 = t.gradient(dy_dx, x)\r\n        d3y_dx3 = t.gradient(d2y_dx2, x)\r\n        d4y_dx4 = t.gradient(d3y_dx3, x)\r\n    del t\r\n\r\n    tf.print(tf.transpose(y)) # transpose to print on one line\r\n    tf.print(tf.transpose(dy_dx))\r\n    tf.print(tf.transpose(d2y_dx2))\r\n    tf.print(tf.transpose(d3y_dx3))\r\n    tf.print(tf.transpose(d4y_dx4))\r\n\r\nfunc()\r\n```\r\n\r\n", "comments": ["Issue replicating for TF 2.1, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/90b02ed9bb7cc3c9e4e404dcc93ab9bb/35012.ipynb) of colab.Thanks!", "@allenlavoie does this ring any bells?", "We have not yet fixed higher-order gradients for cond/while under a tape AFAIK. I believe they need a similar fix to the one we applied to tf.function for recording/accepting gradients of side-outputs. CC @saxenasaurabh. `tf.gradients` likely works.\r\n\r\nThat said, I'm not sure why this specific error is the one we see first. If we're not going to fix/test these soon we should probably start throwing an exception (we've had an internal bug open for a while).", "Since gradient tape with the tf.while_loop was not running with the tf.function decorator ,I move the the while loop(python while) into another function and then decorated it with the tf.function. It seems to be working.\r\n\r\nPlease find the gist here : \r\nhttps://colab.research.google.com/drive/1Y_0EfYzkA6dWjCCL02mz3vOgJRdwpqd1", "Wrapping in `tf.function` sounds reasonable till we have a proper fix. Runtime performance would be similar since we would inline the inner `tf.function` anyway.", "Fair enough, thank you all.", "Issue is replicating with TF 2.2-rc3 .Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/3ef77bf017c72028e54600191a994754/untitled820.ipynb).Thanks!", "Issue is replicating with TF 2.4.0-dev20200817 .Please, find the [gist here](https://colab.research.google.com/gist/Saduf2019/5f514ad7ab269ca09368f4ea343bab8a/untitled367.ipynb).Thanks!", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/96b5bb532a7689c334b72c5cbd11fd80/untitled119.ipynb)..Thanks !", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/92e81bf1813b8395fd3f11c148fdab4d/untitled253.ipynb) ..Thanks!", "I could reproduce the issue with TF 2.6 . Please find the [**`gist`**](https://colab.research.google.com/gist/kumariko/4c5030039b88f4c2bbf1205f46b515ae/untitled253.ipynb#scrollTo=8bSJFFrhl86E) here.Thanks!"]}, {"number": 35011, "title": "Derivative of pow(x, y) can give nan when x=0 for higher order derivatives", "body": "**System information**\r\n- Have I written custom code: Example provided below\r\n- OS Platform and Distribution: Both Windows 10 and Google Colab\r\n- TensorFlow installed from binary\r\n- TensorFlow version: Both 2.0.0 and 2.1.0-rc0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\nThe output of pow(x, y) is not correct for higher order derivatives when x is 0 and when y is other than a single value. For case 2 below I get:\r\n[0 0 4]\r\n[[-2 1 10]]\r\n[[8 -nan 20]]\r\n[[-18 -nan 30]]\r\n[[24 -nan 24]]\r\n[[0 -nan 0]]\r\n\r\nFor case 1 the nan only shows up in the 5th derivative, which is still not great, but not as bad:\r\n[1 0 1]\r\n[[-4 0 4]]\r\n[[12 0 12]]\r\n[[-24 0 24]]\r\n[[24 24 24]]\r\n[[-0 -nan 0]]\r\n\r\n**Describe the expected behavior**\r\nThe output of this example for case 1 should be:\r\n[1 0 1]\r\n[[-4 0 4]]\r\n[[12 0 12]]\r\n[[-24 0 24]]\r\n[[24 24 24]]\r\n[[-0 0 0]]\r\n\r\nThe output of this example for case 2 should be:\r\n[0 0 4]\r\n[[-2 1 10]]\r\n[[8 0 20]]\r\n[[-18 0 30]]\r\n[[24 0 24]]\r\n[[0 0 0]]\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable([[-1.0], [0.0], [1.0]])\r\n\r\nwith tf.GradientTape(persistent=True) as t:\r\n    t.watch(x)\r\n    # case 1: y = x^4\r\n    # y = tf.reduce_sum(tf.pow(x, 4), axis=1) # gives nan for 5th derivative at x=0\r\n    # case 2: y = x + x^2 + x^3 + x^4\r\n    y = tf.reduce_sum(tf.pow(x, [[1, 2, 3, 4]]), axis=1) # gives nan for 2nd to 5th derivative at x=0\r\n    dy_dx = t.gradient(y, x)\r\n    d2y_dx2 = t.gradient(dy_dx, x)\r\n    d3y_dx3 = t.gradient(d2y_dx2, x)\r\n    d4y_dx4 = t.gradient(d3y_dx3, x)\r\n    d5y_dx5 = t.gradient(d4y_dx4, x)\r\ndel t\r\n\r\ntf.print(y)\r\ntf.print(tf.transpose(dy_dx)) # transpose only to fit on one line when printed\r\ntf.print(tf.transpose(d2y_dx2))\r\ntf.print(tf.transpose(d3y_dx3))\r\ntf.print(tf.transpose(d4y_dx4))\r\ntf.print(tf.transpose(d5y_dx5))\r\n```\r\n", "comments": ["Issue replicating in for given code, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/0fb369fd96e8df3fbc789b8e662e1176/35011.ipynb) of colab.Thanks!", "Thank you for the report. I'm checking in a change that defines a gradient for 0 ** 0 wrt the base.\r\n\r\nThe higher-order gradient examples will still have NaNs. That's an interesting discussion we've had in the past about whether to \"fix\" backprop in cases like this by not propagating NaNs when multiplying by a zero derivative (so x ** 0 could \"hide\" NaNs from x ** -1). The conclusion so far has been that we should leave backprop dumb.", "I realize that it's preferable to avoid handling too many special cases, but it seems like some value/utility is lost here by not producing the correct higher-order gradients. The actual higher-order gradients are defined and are continuous, but those computed by TF have a discontinuity (at x = 0). It may be unlikely that an exact value of 0 will show up during typical training, but I ran into it and it sounds like it's happened before. What's the rationale on leaving backprop \"dumb\"?\r\nThanks!", "@rmlarsen do you have more background on why we decided not to hide NaNs in backprop?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35011\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35011\">No</a>\n", "Reopening since I'm rolling back the fix (needs investigation).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35011\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35011\">No</a>\n", "Was able to replicate the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/1128ca9668d3fc0452e703fe7625367e/untitled118.ipynb)..Thanks !", "I could reproduce the issue with TF 2.6 . Please, find the [**`gist`**](https://colab.research.google.com/gist/kumariko/d2332d8c553b5afe2e714b2f3431d244/untitled118.ipynb#scrollTo=LR8L4OwgXAfr) here.Thanks!"]}, {"number": 35010, "title": "Memory leak with tf.py_function in eager mode using TF 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.3 64bit\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI customize a Keras layer and use `tf.py_function` in the `call` of the layer. The used memory keeps increasing linearly with the number of calling the layer with inputs in eager mode. What's \r\n\r\n![image](https://user-images.githubusercontent.com/18071380/70585663-d5744a00-1bff-11ea-8d41-3649d035e9be.png)\r\n\r\n**Describe the expected behavior**\r\nThe memory should keep stable.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport psutil\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass TestLayer(tf.keras.layers.Layer):\r\n    def __init__(\r\n        self,\r\n        output_dim,\r\n        **kwargs\r\n    ):\r\n        super(TestLayer, self).__init__(**kwargs)\r\n        self.output_dim = output_dim\r\n\r\n    def build(self, input_shape):\r\n        self.built = True\r\n\r\n    def call(self, inputs):\r\n        batch_embedding = tf.py_function(\r\n            self.mock_output, inp=[inputs], Tout=tf.float64,\r\n        )\r\n        return batch_embedding\r\n\r\n    def mock_output(self, inputs):\r\n        shape = inputs.shape.as_list()\r\n        batch_size = shape[0]\r\n        return tf.constant(np.random.random((batch_size,self.output_dim)))\r\n\r\n\r\ntest_layer = TestLayer(1000)\r\n\r\nfor i in range(1000):\r\n    test_layer.call(tf.constant(np.random.randint(0,100,(256,10))))\r\n    if i % 100 == 0:\r\n        used_mem = psutil.virtual_memory().used\r\n        print('used memory: {} Mb'.format(used_mem / 1024 / 1024))\r\n```\r\n\r\n**other info**\r\nWe encounter this issue when developing a customized embedding layer in [ElasticDL](https://github.com/sql-machine-learning/elasticdl) and resolving the ElasticDL [issue 1567](https://github.com/sql-machine-learning/elasticdl/issues/1567)\r\n\r\n", "comments": ["Found some issues in [`_internal_py_func`](https://github.com/tensorflow/tensorflow/blob/2a2c812ab2330c9aac33335f10679a346436acfb/tensorflow/python/ops/script_ops.py#L273) in script_ops.py\r\n\r\nEvery time the py_function is called in the eager mode, a new `EagerFunc` is [created](https://github.com/tensorflow/tensorflow/blob/2a2c812ab2330c9aac33335f10679a346436acfb/tensorflow/python/ops/script_ops.py#L295), and a new `token` is[ added into](https://github.com/tensorflow/tensorflow/blob/2a2c812ab2330c9aac33335f10679a346436acfb/tensorflow/python/ops/script_ops.py#L311) `_py_funcs`. \r\nAlso, this `func` is[ appended into](https://github.com/tensorflow/tensorflow/blob/2a2c812ab2330c9aac33335f10679a346436acfb/tensorflow/python/ops/script_ops.py#L333) `graph._py_funcs_used_in_graph`.\r\n\r\nLooks like this logic is for graph mode, and has a side effect in the eager mode.\r\n\r\n`_py_funcs` and  `graph._py_funcs_used_in_graph` will continously increasing in the eager mode.\r\n\r\nThis only accounts for a very small portion of the memory leak in the above sample code. \r\nThere must be other reasons for the huge memory leak.\r\n", "@workingloong \r\nI have tried in colab with TF 2.0, 2.1.0-rc0. With eager execution i can see huge memory leak ([gist attached](https://colab.sandbox.google.com/gist/ravikyram/c2bba090501f9a1aab13e1e86d2aea15/untitled466.ipynb)).By disabling eager execution i am seeing very small portion of memory leak ([gist attached)](https://colab.sandbox.google.com/gist/ravikyram/451e3366ffda46c971b3b54f169d22d2/untitled467.ipynb).Please, confirm is this the expected behavior?.Thanks!\r\n", "> @workingloong\r\n> I have tried in colab with TF 2.0, 2.1.0-rc0. With eager execution i can see huge memory leak ([gist attached](https://colab.sandbox.google.com/gist/ravikyram/c2bba090501f9a1aab13e1e86d2aea15/untitled466.ipynb)).By disabling eager execution i am seeing very small portion of memory leak ([gist attached)](https://colab.sandbox.google.com/gist/ravikyram/451e3366ffda46c971b3b54f169d22d2/untitled467.ipynb).Please, confirm is this the expected behavior?.Thanks!\r\n\r\nYes, there is a very portion of memory leak in graph mode by using `@tf.function` to decorate the `call` in the customized layer.  However, i can not convert the inputs to a numpy array and use python to process the array because the inputs is a Tensor not an EagerTensor in graph mode. So i hope i can use the `tf.py_function`with eager execution.\r\n\r\n```python\r\n@tf.function\r\ndef call(self, inputs):\r\n    batch_embedding = tf.py_function(\r\n        self.mock_output, inp=[inputs], Tout=tf.float64,\r\n    )\r\n    return batch_embedding\r\n```", "@skydoorkai we have found a similar issue when using\r\n\r\n```python\r\nresults = tf.py_function(\r\n                    func=self.safe_load,\r\n                    inp=[audio_descriptor, offset, duration, sample_rate, dtype],\r\n                    Tout=(tf.float32, tf.bool)),\r\n                waveform, error = results[0]\r\n```\r\n\r\nwhere `self._safe_load` loads a wave file. This caused a leak of ~24MB (the wave file size) at every prediction, afterwards calling\r\n\r\n```python\r\ntf.reset_default_graph()\r\ntf.keras.backend.clear_session()\r\n```\r\n\r\nwe only see a leak of ~600-700KB at every call.\r\n\r\n@ravikyram thanks we have tried `tf.compat.v1.disable_eager_execution()` but did not work.", "I had a similar problem on kaggle.\r\nTLDR: try tf_nightly-2.3.0.dev20200520 (or newer)\r\n\r\nI'm using TFRecordDataset and run_eagerly on compile (sadly I am currently forced to use eargerly because of another problem that happens in a custom loss function).\r\nThe memory usage gets increased after every epoch, so i can not execute the amount of epochs i want because the kernel dies because of \"out of memory\".\r\n(Raising slowly from 6 GB to over 16 GB with each epoch)\r\nI am not shure if it is a problem about tensorflow or keras.\r\n\r\nGarbage collection, clear_session, event setting every variable to None etc. did not help.\r\nSomehow the memory was still used.\r\n\r\nSomehow it seems to be better with tf_nightly-2.3.0.dev20200520.\r\nSo maybe the bug was fixed?", "I'm experiencing a memory leakage when training Mobilenetv2,Resnet, etc on Imagenet from scratch using a tf.py_function to map my dataset with a custom augmentation. It's not a huge leakage, but after three epochs of training on Imagenet dataset, my memory consumption increased from 16Gb to 59Gb and I had to stop my execution, because it started to be too slow.\r\n\r\nI'm using tensorflow 2.3 on a p3.2xlarge instance for benchmarking purposes, I hope someone has a good way to deal with this, thank you all for your comments.\r\n\r\n", "Hi all, I've boiled down the problem and created a simpler gist to demonstrate the problem, in TF 2.4\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef test_eager(x):\r\n    return tf.py_function(func=np.cos, inp=[x], Tout=tf.float32)  # the problem, calls the below and passes `use_tape_cache=True`\r\n\r\n    # return tf.numpy_function(func=np.cos, inp=[x], Tout=tf.float32)  # works as well\r\n\r\n\r\n\r\ntest_graph = tf.function(test_eager)\r\nfor _ in range(5000):\r\n    test_eager(tf.random.uniform(shape=(100000,)))  # running the loop with this increases memory by ~a few GB\r\n    # test_graph(tf.random.uniform(shape=(100000,)))  # running this is fine\r\n\r\n```\r\n\r\nIt's with the `use_tape_cache`, at least for the above. Here we can use the `numpy_func`, maybe that works as well for the data preprocessing as no gradient is needed there.\r\n", "@workingloong ,\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I can confirm that it still exists (tested with nightlies), I've updated the snippet"]}, {"number": 34992, "title": "How to convert ByteBuffer to Bitmap Image?", "body": "I am working on creating an image super-resolution application that uses a TensorFlow Lite model. The model gives the output Image in the form of ByteBuffer and I convert the ByteBuffer to Bitmap. Next, I display this Bitmap but nothing shows up. The code I am using can be seen below:\r\n\r\n```\r\nByteBuffer out = ByteBuffer.allocate(4 * 384 * 384 * 3);\r\ntflite.run(byteBuffer,out);\r\n\r\nbyte[] imageBytes= new byte[out.remaining()];\r\nout.get(imageBytes);\r\nfinal Bitmap outPut_Image = BitmapFactory.decodeByteArray(imageBytes,0,imageBytes.length);\r\n//Toast.makeText(this,tflite.toString(),Toast.LENGTH_LONG).show();\r\nToast.makeText(this, \"Working\",Toast.LENGTH_LONG).show();\r\nImageView imageView = (ImageView) this.findViewById(R.id.imageView2);\r\nimageView.setImageBitmap(outPut_Image);\r\n```\r\nPlease advise me what I am doing wrong here.", "comments": ["This is how I convert ByteBuffer to Bitmap in Kotlin. Just convert it to Java and it should work.\r\n\r\n```\r\nprivate fun getOutputImage(): Bitmap {\r\n    output?.rewind() // Rewind the output buffer after running.\r\n\r\n    val bitmap = Bitmap.createBitmap(outputWidth, outputHeight, Bitmap.Config.ARGB_8888)\r\n    val pixels = IntArray(outputWidth * outputHeight) // Set your expected output's height and width\r\n    for (i in 0 until outputWidth * outputHeight) {\r\n        val a = 0xFF\r\n        val r: Float = output?.float!! * 255.0f\r\n        val g: Float = output?.float!! * 255.0f\r\n        val b: Float = output?.float!! * 255.0f\r\n        pixels[i] = a shl 24 or (r.toInt() shl 16) or (g.toInt() shl 8) or b.toInt()\r\n    }\r\n    bitmap.setPixels(pixels, 0, outputWidth, 0, 0, outputWidth, outputHeight)\r\n\r\n    return bitmap\r\n}\r\n```", "@bilalsoomro  I have converted the  above code to Java the code becomes\r\n\r\n```\r\nprivate Bitmap getOutputImage(ByteBuffer output){\r\n        output.rewind();\r\n\r\n        int outputWidth = 384;\r\n        int outputHeight = 384;\r\n        Bitmap bitmap = Bitmap.createBitmap(outputWidth, outputHeight, Bitmap.Config.RGB_565);\r\n        int [] pixels = new int[outputWidth * outputHeight];\r\n        for (int i = 0; i < outputWidth * outputHeight; i++) {\r\n            //val a = 0xFF;\r\n            //float a = (float) 0xFF;\r\n\r\n            //val r: Float = output?.float!! * 255.0f;\r\n            //byte val = output.get();\r\n            float r = ((float) output.get()) * 255.0f;\r\n            //float r = ((float) output.get());\r\n\r\n            //val g: Float = output?.float!! * 255.0f;\r\n            float g = ((float) output.get()) * 255.0f;\r\n            //float g = ((float) output.get());\r\n\r\n            //val b: Float = output?.float!! * 255.0f;\r\n            float b = ((float) output.get()) * 255.0f;\r\n            //float b = ((float) output.get());\r\n\r\n\r\n            //pixels[i] = a shl 24 or (r.toInt() shl 16) or (g.toInt() shl 8) or b.toInt()\r\n            pixels[i] = (((int) r) << 16) | (((int) g) << 8) | ((int) b);\r\n        }\r\n        bitmap.setPixels(pixels, 0, outputWidth, 0, 0, outputWidth, outputHeight);\r\n\r\n        return bitmap;\r\n    }\r\n```\r\n\r\nAfter running this code I do get an output Image but the output isn't meaningful i.e output is a noisy image. Please advise.", "I have posted the output on StackOverflow question have a look at it https://stackoverflow.com/questions/59284342/conversion-bytebuffer-to-image-give-noisy-output", "@nauyan I am not entirely sure about your conversion. I gave it a shot. Try this....\r\n\r\nEDIT: My bad, it is supposed to be ARGB_8888. I was just trying to fix your conversion which switched to RGB565.\r\n\r\n```\r\nprivate Bitmap getOutputImage(ByteBuffer output){\r\n        output.rewind();\r\n\r\n        int outputWidth = 384;\r\n        int outputHeight = 384;\r\n        Bitmap bitmap = Bitmap.createBitmap(outputWidth, outputHeight, Bitmap.Config.ARGB_8888);\r\n        int [] pixels = new int[outputWidth * outputHeight];\r\n        for (int i = 0; i < outputWidth * outputHeight; i++) {\r\n            int a = 0xFF;\r\n\r\n            float r = output.getFloat() * 255.0f;\r\n            float g = output.getFloat() * 255.0f;\r\n            float b = output.getFloat() * 255.0f;\r\n\r\n            pixels[i] = a << 24 | ((int) r << 16) | ((int) g << 8) | (int) b;\r\n        }\r\n        bitmap.setPixels(pixels, 0, outputWidth, 0, 0, outputWidth, outputHeight);\r\n        return bitmap;\r\n    }\r\n```", "@bilalsoomro I still I getting the same output of the generated image.", "@nauyan int is 32bit in Java. If you want to use int type for a pixel, you need to convert it to ARGB8888 instead of RGB565 which is 16bit.", "@terryheo still getting a noisy image.", "@nauyan could you share the code you used for ARGB8888?", "@terryheo here is the code\r\n\r\n```\r\nprivate Bitmap getOutputImage(ByteBuffer output){\r\n        output.rewind();\r\n\r\n        int outputWidth = 384;\r\n        int outputHeight = 384;\r\n        Bitmap bitmap = Bitmap.createBitmap(outputWidth, outputHeight, Bitmap.Config.ARGB_8888);\r\n        int [] pixels = new int[outputWidth * outputHeight];\r\n        for (int i = 0; i < outputWidth * outputHeight; i++) {\r\n            int a = 0xFF;\r\n\r\n            float r = output.getFloat() * 255.0f;\r\n            float g = output.getFloat() * 255.0f;\r\n            float b = output.getFloat() * 255.0f;\r\n\r\n            pixels[i] = a << 24 | ((int) r << 16) | ((int) g << 8) | (int) b;\r\n        }\r\n        bitmap.setPixels(pixels, 0, outputWidth, 0, 0, outputWidth, outputHeight);\r\n        return bitmap;\r\n    }\r\n```", "@nauyan I think you'd better check the output type and shape of your model again.\r\n\r\nThe following code might be helpful for you.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/support/java/src/java/org/tensorflow/lite/support/image/ImageConversions.java#L44", "@terryheo  I did use functions that you have mentioned above my code is: \r\n\r\n```\r\n       try {\r\n            tflite = new Interpreter(loadModelFile(MainActivity.this, \"model.tflite\"));\r\n            Bitmap testImage = getBitmapFromAsset(this,\"Image.png\");\r\n            TensorBuffer test_tensor = TensorBuffer.createFixedSize(new int[]{96, 96, 3}, DataType.UINT8);\r\n            Bitmap out =Bitmap.createBitmap(384,\r\n                         384, Bitmap.Config.ARGB_8888);\r\n            TensorBuffer output_tensor = TensorBuffer.createFixedSize(new int[]{384, 384, 3}, DataType.UINT8);\r\n            tflite.run(test_tensor,output_tensor);\r\n            convertTensorBufferToBitmap(output_tensor,out);\r\n        } catch (IOException e) {\r\n            Toast.makeText(this,\"Error\",Toast.LENGTH_LONG).show();\r\n            e.printStackTrace();\r\n        }\r\n\r\n    static void convertTensorBufferToBitmap(TensorBuffer buffer, Bitmap bitmap) {\r\n        if (buffer.getDataType() != DataType.UINT8) {\r\n            // We will add support to FLOAT format conversion in the future, as it may need other configs.\r\n            throw new UnsupportedOperationException(String.format(\r\n                    \"Converting TensorBuffer of type %s to Bitmap is not supported yet.\",\r\n                    buffer.getDataType()));\r\n        }\r\n        int[] shape = buffer.getShape();\r\n        if (shape.length != 3 || shape[0] <= 0 || shape[1] <= 0 || shape[2] != 3) {\r\n            throw new IllegalArgumentException(String.format(\r\n                    \"Buffer shape %s is not valid. 3D TensorBuffer with shape [w, h, 3] is required\",\r\n                    Arrays.toString(shape)));\r\n        }\r\n        int h = shape[0];\r\n        int w = shape[1];\r\n        if (bitmap.getWidth() != w || bitmap.getHeight() != h) {\r\n            throw new IllegalArgumentException(String.format(\r\n                    \"Given bitmap has different width or height %s with the expected ones %s.\",\r\n                    Arrays.toString(new int[]{bitmap.getWidth(), bitmap.getHeight()}),\r\n                    Arrays.toString(new int[]{w, h})));\r\n        }\r\n        if (!bitmap.isMutable()) {\r\n            throw new IllegalArgumentException(\"Given bitmap is not mutable\");\r\n        }\r\n        // TODO(b/138904567): Find a way to avoid creating multiple intermediate buffers every time.\r\n        int[] intValues = new int[w * h];\r\n        int[] rgbValues = buffer.getIntArray();\r\n        for (int i = 0, j = 0; i < intValues.length; i++) {\r\n            byte r = (byte) rgbValues[j++];\r\n            byte g = (byte) rgbValues[j++];\r\n            byte b = (byte) rgbValues[j++];\r\n            intValues[i] = ((r << 16) | (g << 8) | b);\r\n        }\r\n        bitmap.setPixels(intValues, 0, w, 0, 0, w, h);\r\n    }\r\n\r\n    static void convertBitmapToTensorBuffer(Bitmap bitmap, TensorBuffer buffer) {\r\n        int w = bitmap.getWidth();\r\n        int h = bitmap.getHeight();\r\n        int[] intValues = new int[w * h];\r\n        bitmap.getPixels(intValues, 0, w, 0, 0, w, h);\r\n        // TODO(b/138904567): Find a way to avoid creating multiple intermediate buffers every time.\r\n        int[] rgbValues = new int[w * h * 3];\r\n        for (int i = 0, j = 0; i < intValues.length; i++) {\r\n            rgbValues[j++] = ((intValues[i] >> 16) & 0xFF);\r\n            rgbValues[j++] = ((intValues[i] >> 8) & 0xFF);\r\n            rgbValues[j++] = (intValues[i] & 0xFF);\r\n        }\r\n        int[] shape = new int[] {h, w, 3};\r\n        buffer.loadArray(rgbValues, shape);\r\n    }\r\n\r\n```\r\n\r\nThis is a snippet of my code and I still am getting an error:\r\n```\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.edsr, PID: 14650\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.edsr/com.example.edsr.MainActivity}: java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of org.tensorflow.lite.support.tensorbuffer.TensorBufferUint8\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2817)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892)\r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593)\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6541)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\r\n     Caused by: java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of org.tensorflow.lite.support.tensorbuffer.TensorBufferUint8\r\n        at org.tensorflow.lite.Tensor.dataTypeOf(Tensor.java:255)\r\n        at org.tensorflow.lite.Tensor.throwIfTypeIsIncompatible(Tensor.java:313)\r\n        at org.tensorflow.lite.Tensor.getInputShapeIfDifferent(Tensor.java:218)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:135)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:259)\r\n        at com.example.edsr.MainActivity.onCreate(MainActivity.java:97)\r\n        at android.app.Activity.performCreate(Activity.java:6975)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1213)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2770)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892)\u00a0\r\n        at android.app.ActivityThread.-wrap11(Unknown Source:0)\u00a0\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\u00a0\r\n        at android.os.Looper.loop(Looper.java:164)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:6541)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\u00a0\r\nProcess 14650 terminated.\r\n```\r\n", "I have a model of input type float 32 how can I convert bitmap to bytebuffer for such type of model. Cos I used the convertBitmapToByteBuffer method I got from here but it is giving an arrayoutofindexbound error. So I noticed that the input type for my model (flaot32) differed from most model input types(int)"]}, {"number": 34964, "title": "Add K Medoids Estimator to tf canned estimators", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nhttps://en.wikipedia.org/wiki/K-medoids\r\nBasically same as https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/kmeans.py except when updating centroid use median rather than mean.\r\n\r\n**Will this change the current api? How?**\r\nNo, it should just add functionality\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to use K-medoids clustering\r\n\r\n**Any Other info.**\r\n", "comments": ["We're not actively developing estimator models, for future development, this should go into Keras premade models. And contributions welcome", "Hello, can I work on it? I would like to try it out.", "> Hello, can I work on it? I would like to try it out.\r\n\r\nOf course. Can you start by sending out design ideas according to RFC rule?", "> > Hello, can I work on it? I would like to try it out.\r\n> \r\n> Of course. Can you start by sending out design ideas according to RFC rule?\r\n\r\nAwesome, I'll prepare for a request. It is my first time, and I am a little lost:\r\n\r\n- Can I get early feedback on the RFC here?\r\n- Should the K Mediods better be added to the [tf canned estimator](https://www.tensorflow.org/tutorials/estimator/premade) or the [keras premade models](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/premade) as mentioned above? \r\n\r\nFor the first case, I am planning to take heavy inspiration from the existing implementation of [KMeans](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/kmeans.py) as the models are fundamentally similar in most aspects and even sklearn k_mediod model uses some parts from its k_means model.\r\n\r\nIf the Keras premade model implementation is preferred, I am thinking of incremental development as I am quite unfamiliar with both for now. I would love some pointers on this one, if possible.\r\n\r\n\r\n\r\n\r\n", "> > > Hello, can I work on it? I would like to try it out.\r\n> > \r\n> > \r\n> > Of course. Can you start by sending out design ideas according to RFC rule?\r\n> \r\n> Awesome, I'll prepare for a request. It is my first time, and I am a little lost:\r\n> \r\n> * Can I get early feedback on the RFC here?\r\n> * Should the K Mediods better be added to the [tf canned estimator](https://www.tensorflow.org/tutorials/estimator/premade) or the [keras premade models](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/premade) as mentioned above?\r\n> \r\n> For the first case, I am planning to take heavy inspiration from the existing implementation of [KMeans](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/kmeans.py) as the models are fundamentally similar in most aspects and even sklearn k_mediod model uses some parts from its k_means model.\r\n> \r\n> If the Keras premade model implementation is preferred, I am thinking of incremental development as I am quite unfamiliar with both for now. I would love some pointers on this one, if possible.\r\n\r\nIn the RFC process, it is suggested to first email developers@tensorflow.org before writing up the entire doc :-)\r\nRegarding implementation, yeah some common logic would be preferred, but we don't have any immediate implementation to export TF2 KMeans yet.", "> > > > Hello, can I work on it? I would like to try it out.\r\n> > > \r\n> > > \r\n> > > Of course. Can you start by sending out design ideas according to RFC rule?\r\n> > \r\n> > \r\n> > Awesome, I'll prepare for a request. It is my first time, and I am a little lost:\r\n> > \r\n> > * Can I get early feedback on the RFC here?\r\n> > * Should the K Mediods better be added to the [tf canned estimator](https://www.tensorflow.org/tutorials/estimator/premade) or the [keras premade models](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/premade) as mentioned above?\r\n> > \r\n> > For the first case, I am planning to take heavy inspiration from the existing implementation of [KMeans](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/kmeans.py) as the models are fundamentally similar in most aspects and even sklearn k_mediod model uses some parts from its k_means model.\r\n> > If the Keras premade model implementation is preferred, I am thinking of incremental development as I am quite unfamiliar with both for now. I would love some pointers on this one, if possible.\r\n> \r\n> In the RFC process, it is suggested to first email [developers@tensorflow.org](mailto:developers@tensorflow.org) before writing up the entire doc :-)\r\n> Regarding implementation, yeah some common logic would be preferred, but we don't have any immediate implementation to export TF2 KMeans yet.\r\n\r\nThanks for being friendly and helpful.\r\n", "we could change option or have discussion to find the answer.not my answer should be best.", "Hii @tanzhenyu, I want to follow up on this issue and add the K-Medoids api. After reading all the previous comments and discussions here and on the [mailing list](https://groups.google.com/a/tensorflow.org/g/developers/c/Jj0SORWweho/m/6awItltCAwAJ), I think there are a few things that needs to be decided before we can proceed with this issue.\r\n\r\n1. There is going to be a lot of similarities with the k-means api and before we could even think of code sharing we need to decide upon the status of [clustering_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/clustering_ops.py) w.r.t to the TODO written [here](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/kmeans.py#L17).\r\n\r\n2. If we don't want to resolve the above issue first then we should proceed with adding a class `KMedoids` in the [clustering_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/clustering_ops.py) and an estimator class `KMedoidsEstimator` in the [canned directory](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned) just like its counterpart.\r\n\r\nThough, I like the second method way more because of its simplicity and straightforwardness. Or, maybe I am completely wrong about this.\r\n\r\nHowever, I suppose the core developers might have got a different and better design for this.\r\n\r\nSo, Could you please guide me through this ?\r\n\r\nAlso, I would like to know if I have to send an RFC for this or not."]}, {"number": 34946, "title": "Does TensorFlow 2.x still need the bazel build option of --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" ?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:  v2.1.0-rc0 \r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): bazel 1.2.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7\r\n- GPU model and memory: 1080Ti, ~11GB\r\n\r\n**Describe the problem**\r\n\r\nAccording to the official doc in section **TensorFlow 2.x** from https://www.tensorflow.org/install/source#tensorflow_2x, to build the TF 2.X via bazel \r\n\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nDo we need the bazel option of --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" ?\r\n\r\nThe [subsection: Bazel build options](https://www.tensorflow.org/install/source#bazel_build_options) under section **TensorFlow 1.x** states that \r\n\r\n> The official TensorFlow packages are built with GCC 4 and use the older ABI. For GCC 5 and later, make your build compatible with the older ABI using: --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\". ABI compatibility ensures that custom ops built against the official TensorFlow package continue to work with the GCC 5 built package.\r\n\r\nHowever,  `The official TensorFlow packages ` link includes TF 2.X. \r\nSo, does TensorFlow 2.x still need the bazel option of --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" ?\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n> The official TensorFlow packages are built with GCC 4 and use the older ABI. For GCC 5 and later, make your build compatible with the older ABI using: --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\". ABI compatibility ensures that custom ops built against the official TensorFlow package continue to work with the GCC 5 built package.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["any updates on this?", "I have this question as well.  As it is now, attempting to use the C libs (libtensorflow.so) alongside anything that uses custom ops (like tensorflow-text) is extremely unpleasant due to ABI issues.  Further documentation on this would be very helpful.", "updates?", "@shizukanaskytree \r\nCould you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "Could you please post the related commit? What's changed? The default ABI or documentation?", "@Saduf2019 I don't think the recent official TensorFlow packages, including 2.7 or 2.8 have moved away from GCC 4 and the older ABI. They still seem to be using --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" for the PyPi packages as well.\r\n\r\nThe issue questions if the older ABI (i.e --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\") is still needed for the latest versions of TensorFlow. Is it being maintained so for any particular reason?\r\n\r\nFor example, this dockerfile still uses manylinux2010 which has the older toolchain with GLIC 2.12, and libstdc++ 4.4\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/daaa5dd83ac046226dc31e2fc1880ea0e2050832/tensorflow/tools/ci_build/Dockerfile.rbe.cuda11.4-cudnn8.2-ubuntu18.04-manylinux2010-multipython#L4\r\n", "> For example, this dockerfile still uses manylinux2010 which has the older toolchain with GLIC 2.12, and libstdc++ 4.4\r\nDockerfile for nightly wheels is now:\r\n\r\nhttps://discuss.tensorflow.org/t/adopting-open-source-dockerfiles-for-official-tf-nightly-ci/6050/24", "@bhack Thanks for sharing this resource. Do you know if there are plans to switch to a newer manylinux tag and toolset for the official PyPi builds? Something that preferably uses the newer ABI (i.e, one that uses --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=1\")? \r\n\r\nWhile these dockerfiles are useful and facilitate custom builds for CI and other purposes, they are still using the older devtoolset-7\r\n\r\nhttps://github.com/tensorflow/build/blob/9b07b29d077001483047239354e70013ef41f5a9/tf_sig_build_dockerfiles/Dockerfile#L10\r\n\r\nReason being: For custom ops implementations, and integrations like https://github.com/openvinotoolkit/openvino_tensorflow, that depend on libraries which are ABI-1, it's not possible to maintain ABI compatibility without recompiling tensorflow from source. ", "There is a WIP PR for manylinux2014:\n\nhttps://github.com/tensorflow/build/pull/57", "@bhack did the PR you share would change the ABI value(from 0 to 1) in the compile stage? thanks.", "You can try to ask there directly in the PR"]}, {"number": 34924, "title": "Support EIGEN_USE_MKL_ALL macro for building tensorflow", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf1.14, tf2.0\r\n- Are you willing to contribute it (Yes/No): No, I am not familiar with bazel\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIf my understanding is correct, the compiling flag `--config=mkl` for bazel only enables mkl-dnn supports which could replace several basic ops like matrix multiplication with mkl implementation using jit. However, it seems to me that this flag doesn't enable mkl linkage with Eigen. Therefore, all linear ops beyond several covered in mkl-dnn are still executed in plain eigen single-threaded implementation which is too slow to use (there can be O(10) or even O(100) speed difference for large matrix and eigh, svd, qr ops, as previously noted in https://github.com/tensorflow/tensorflow/issues/7128, https://github.com/tensorflow/tensorflow/issues/13222, etc.). \r\n\r\nCurrently, in tensorflow.bzl, `DEIGEN_USE_VML` flag is set when compiled with --config=mkl. As explained in https://github.com/tensorflow/tensorflow/issues/30592, this indicates eigen is not enabled with mkl at all. But a simple replacement of this flag with `DEIGEN_USE_MKL_ALL` leads to failure of the compiling with the error complaining <mkl.h> not found. Also as noted https://github.com/tensorflow/tensorflow/pull/12219, MKL optimized Tensorflow does not support EIGEN_USE_MKL_ALL. I know little about bazel setup, so I don't know whether turn on such support is involved or as simple as some small tweaks.\r\n\r\nIn sum, tuning tf building system to \"really\" enable mkl behind tf is of great importance and it is vital for the speed of a large range of matrix ops. And this should be the expected behavior for `--config=mkl` flag after all. Currently, so called \"intel optimized\" or \"mkl enabled\" tensorflow is somehow confusing.\r\n\r\n**Will this change the current api? How?**\r\nNot for the user level API.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone using tf in his/her workflow including matrix operations like EIG, SVD, QR etc on CPU. (One can argue that there is no problem for GPU implementation, but cusolver implementations for SVD and QR can still be much slower than mkl cpu implementations. So fast CPU implementations are critical for these matrix decomposition types op).\r\n\r\n**Any Other info.**\r\n", "comments": ["@refraction-ray Could give the full build commands which you use?", "@Leslie-Fang , I believe the building workflow is similar to [official doc](https://www.tensorflow.org/install/source#cpu-only_2) using devel docker. The only tweak I have done is changing the flag in tensorflow.bzl directly into `DEIGEN_USE_MKL_ALL`.\r\nThis fails building for sure, since no path (header or library) is specified for mkl library which eigen is required now. Since I have no time to learn about bazel building system, I didn't try adding some cxxopts or hacking more bazel files. And https://github.com/tensorflow/tensorflow/pull/12219 gives me the impression `DEIGEN_USE_MKL_ALL` support is not trivial in tf building.\r\nAnd this is the reason for this issue, either some experts tell me how to build tf with `EIGEN_USE_MKL_ALL` flag by some small tweaks or building related codebase is better to be improved accordingly for mkl support.", "@refraction-ray if that's the case in https://github.com/tensorflow/tensorflow/pull/12219 , although I don't know why but MKLDNN support and eigen with mkl support seems cann't be enabled simultaneously.\r\nWhich Eigen ops is the bottleneck of your application?\r\nI have done some Eigen op optimization previously.\r\n\r\n", "@Leslie-Fang , as I have mentioned, QR, SVD decompositions are slow with eigen implementation compared to MKL multithreaded version.", "@refraction-ray Do you know how to measure the performance of QR, SVD ops with MKL multithreaded version?", "I see some comments in the https://github.com/tensorflow/tensorflow/issues/7128 about how to measure the performance. \r\nI will try to enable the native Eigen multithread version.", "@Leslie-Fang , great! Be sure to use mkl linked numpy for the benchmark", "@refraction-ray Have checked the implementation of QR and SVD in tensorflow.\r\nhttps://github.com/tensorflow/tensorflow/blob/a74e202f4a93c19863061d18979436b0470d862d/tensorflow/core/kernels/qr_op_impl.h#L101-L120\r\nBoth of them are invoking eigen in-place decomposition.\r\nhttps://eigen.tuxfamily.org/dox/classEigen_1_1HouseholderQR.html\r\nI suspect there is no parallel optimization we can do in the tensorflow op.", "@Leslie-Fang , this is why I believe the better solution here is to enable mkl linkage with eigen when building tensorflow. If this works, all ops can enjoy multithreaded mkl implementation and no need to hack these ops one by one.", "Hi everyone, could you please tell if there has been any progress on the subject? I am not an expert, but enabling MKL support for Eigen operations sounds like a reasonable solution, and would be highly appreciated. Otherwise, it's a bit frustrating to see how TensorFlow humbly uses only one core, while NumPy enjoys running SVD with >1000% CPU load :-) ", "@refraction-ray \r\nThough Eigen supports mkl (https://eigen.tuxfamily.org/dox/TopicUsingIntelMKL.html), it not available through Tensorflow. "]}, {"number": 34908, "title": "Saving custom tensor fails", "body": "Hello everyone,\r\n\r\nThere is a bug with saving and loading of custom tensors (variables) in TF. It was discovered at https://github.com/GPflow/GPflow/issues/1127. I have created the MWE in colab [here](https://colab.research.google.com/drive/1REEmDStqGGvl9EmYRa6KB44YXT7EVZkt). There are two errors when `tf.save_model` applies to custom tensors:\r\n\r\n* The loading for custom tensor (variable) restores broken tf.Variable object\r\n* Saving of tf.Module with custom tensors (variables) fails\r\n\r\nDetails in [colab notebook](https://colab.research.google.com/drive/1REEmDStqGGvl9EmYRa6KB44YXT7EVZkt)\r\n\r\nPS: referencing @alextp and @dynamicwebpaige, because they know what I mean by custom tensor.\r\n\r\nThanks,\r\nArtem ", "comments": ["@k-w-w as far as I can tell those module-visible variables should be correctly handled by savedmodel. Do you know what is going on here?", "@k-w-w, @alextp ping ^^^", "This is an interesting use case. SavedModel should be correctly handling this, but we hadn't considered this case yet. I'll see what changes are required to resolve the issue.", "@k-w-w, @alextp any progress on the issue?", "@k-w-w, @alextp ping", "@k-w-w, @alextp Hello! Because of this issue, in [GPflow](https://github.com/GPflow/GPflow) we can't save models. My suspicion that it also affects TensorFlow Probability project. Can you make an update about it? Can you propose any workaround? Can you advise on how we can contribute to TensorFlow to make this work? \r\nThanks!", "hello @k-w-w, @alextp! any updates?", "Hello @k-w-w @alextp , any updates on this?", "Pinging @rohan100jain for FYI on something that might need a priority bump", "@alextp thanks!", "@alextp any idea when this might be fixed?", "@ccrusius FYI"]}, {"number": 34898, "title": "tf.execute_volume_patches misses one dimension when used with placeholders and None", "body": "Tested in with tf 1.12.0 and tf 2.0. Below code for use with tf 2.0.\r\n\r\np = tf.keras.backend.placeholder((1,None,None,None,1))\r\nkernel = [1,3,3,3,1]\r\nstrides = [1,1,1,1,1]\r\ntf.extract_volume_patches(p, kernes, strides, 'SAME')\r\n>>><tf.Tensor 'ExtractVolumePatches:0' shape=(1, None, None, 27) dtype=float32>\r\n\r\np = tf.keras.backend.placeholder((1,120,130,140,1))\r\nkernel = [1,3,3,3,1]\r\nstrides = [1,1,1,1,1]\r\ntf.extract_volume_patches(p, kernes, strides, 'SAME')\r\n>>><tf.Tensor 'ExtractVolumePatches:0' shape=(1, 120, 130, 140, 27) dtype=float32>\r\n\r\nIs there a reason behind this behaviour?\r\n", "comments": ["I have tried on colab with TF version 1.12,1.15, 2.0, 2.1.0-rc0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/b83ee96afa6000082d3e4ee4c3c7bf3d/untitled452.ipynb). Thanks!", "could reproduce the issue with TF version 2.2.0-dev20200319. Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/73183dbf2eb5dd7716f6e9c7bf34d95e/untitled738.ipynb).Thanks!", "@hsgkim Can you please take a look at this issue? Looks like this is possibly related to your changes in the past. \r\nThank you.  ", "I have tried this in TF version 2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/9261c352b130574b4f5833582e8debb5/untitled106.ipynb?authuser=1)  ..Thanks !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34898\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34898\">No</a>\n", "Was able to reproduce the issue on Colab using TF v2.6 & tf-nightly , please find the [gist](https://colab.research.google.com/gist/sushreebarsa/3c6bf1d0028ccf1828dcdd524e60f2ec/untitled422.ipynb) for reference.Thanks!"]}, {"number": 34841, "title": "Segmentation fault and corrupted output during inference with large 3D U-Net", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution: Linux CLE 7.0UP00\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\nI have a 3D U-Net model that I am trying to use for very large inputs (arrays as large as 1024^3). Obviously, I cannot fit this in GPU memory so I am running the network on CPU compute nodes which have a very large amount of RAM available (~350 GB). I have a \"bare-bones\" script which performs only the forward pass using the 3D convolution methods from `tf.nn`, but I am only able to process arrays of size  <= 640^3 without issues. If I go for the next feasible size (which is 704^3 since I can only increase spatial dimensions of the input by multiples of 64), the following behavior occurs:\r\n\r\n* If using tensorflow with the Intel MKL library, which is optimized for running on CPUs on my system, the network runs without crashing but the generated output is corrupted (see attached images). The corruption seems to only occur along the outermost spatial axis of the output array.\r\n\r\n* If using tensorflow-gpu or regular tensorflow (without Intel MKL), the program crashes with a segmentation fault\r\n\r\nThough the input sizes are very large, I would expect tensorflow to be able to complete the computations without issue since I have enough RAM available. The difference in behavior with or without Intel MKL is also perplexing.\r\n\r\n**Code to reproduce the issue**\r\nBelow is a script that performs the U-Net computation on random data and reproduces the segfault/bug:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsize=704\r\ndtype = np.float32\r\nsample = np.random.normal(size=(1,size,size,size,4)).astype(dtype)\r\nprint(sample.shape)\r\n\r\nconvs = [(4, 4, 4, 4, 64),\r\n         (4, 4, 4, 64, 128),\r\n         (4, 4, 4, 128, 256),\r\n         (4, 4, 4, 256, 512),\r\n         (4, 4, 4, 512, 512),\r\n         (4, 4, 4, 512, 512)]\r\nTconvs = [(4, 4, 4, 512, 512),\r\n          (4, 4, 4, 512, 1024),\r\n          (4, 4, 4, 256, 1024),\r\n          (4, 4, 4, 128, 512),\r\n          (4, 4, 4, 64, 256),\r\n          (4, 4, 4, 5, 128)]\r\n\r\nx = tf.constant(sample)\r\nskips = []\r\nshapes = []\r\n\r\n# Downsampling\r\nfor kernelshape in convs:\r\n    bias = np.zeros(shape=(kernelshape[-1]))\r\n    kernel = np.random.normal(loc=0.0, scale=0.02, size=kernelshape)\r\n    x = tf.nn.conv3d(x, filters=kernel.astype(dtype), strides=[1,2,2,2,1], padding='SAME')\r\n    x = tf.nn.bias_add(x, bias.astype(dtype))\r\n    x = tf.nn.leaky_relu(x, alpha=0.3)\r\n    skips.append(x)\r\n    shapes.append(x.shape)\r\n    print(x.shape)\r\n\r\n# Upsampling with skip connections\r\nskips = list(reversed(skips[:-1]))\r\nfor kernelshape, skip in zip(Tconvs, skips):\r\n    bias = np.zeros(shape=(kernelshape[3]))\r\n    kernel = np.random.normal(loc=0.0, scale=0.02, size=kernelshape)\r\n    out_shape = np.array(x.shape.as_list())\r\n    out_shape[1:4] = 2*out_shape[1:4]\r\n    out_shape[-1] = kernel.shape[3]\r\n    x = tf.nn.conv3d_transpose(x, filters=kernel.astype(dtype), strides=2, padding='SAME', output_shape=out_shape)\r\n    x = tf.nn.bias_add(x, bias.astype(dtype))\r\n    x = tf.nn.relu(x)\r\n    x = tf.concat([x, skip], axis=-1)\r\n    print(x.shape)\r\n\r\n# Final upsampling conv\r\nkernelshape = Tconvs[-1]\r\nbias = np.zeros(shape=(kernelshape[3]))\r\nkernel = np.random.normal(loc=0.0, scale=0.02, size=kernelshape)\r\nout_shape = np.array(x.shape.as_list())\r\nout_shape[1:4] = 2*out_shape[1:4]\r\nout_shape[-1] = kernel.shape[3]\r\nx = tf.nn.conv3d_transpose(x, filters=kernel.astype(dtype), strides=2, padding='SAME', output_shape=out_shape)\r\nx = tf.nn.bias_add(x, bias.astype(dtype))\r\nx = tf.math.tanh(x)\r\n\r\nprint(x.shape)\r\nx = x.numpy().astype(np.float32)\r\n```\r\n\r\n**Other info / logs**\r\nThe attached images show the output of the U-Net (running on random noise) for the problematic size (704^3), via 2D slices along the x-y and x-z planes: \r\n![image](https://user-images.githubusercontent.com/48932392/70189520-613e2180-16a8-11ea-970f-27bdd58546f1.png)\r\nThe images should look like uniform noise, but instead have a sharp change occur along the x-axis after index~520.\r\n\r\nOn my actual dataset, the corruption looks like: \r\n![image](https://user-images.githubusercontent.com/48932392/70190128-fc83c680-16a9-11ea-9fb1-984be9673381.png)\r\n", "comments": ["Thank you for reporting the issue! \r\n\r\nI tried this on an Ubuntu 16.04 VM + Python 3.7.4 + 360GB memory. I could reproduce the corrupted image issue with both TensorFlow-MKL (Intel-optmized TensorFlow) and regular TensorFlow. (Regular TensorFlow didn't crash for me.) Will try to fix the corrupted image issue first and then we can see if regular TensorFlow still crashes for you."]}, {"number": 34726, "title": "tensorflow/workspace: re2 dependency does not use release/master branch", "body": "tensorflow/workspace.bzl has this for the re2 dep:\r\n\r\n```\r\n    tf_http_archive(\r\n        name = \"com_googlesource_code_re2\",\r\n        sha256 = \"d070e2ffc5476c496a6a872a6f246bfddce8e7797d6ba605a7c8d72866743bf9\",\r\n        strip_prefix = \"re2-506cfa4bffd060c06ec338ce50ea3468daa6c814\",\r\n        system_build_file = clean_dep(\"//third_party/systemlibs:re2.BUILD\"),\r\n        urls = [\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/re2/archive/506cfa4bffd060c06ec338ce50ea3468daa6c814.tar.gz\",\r\n            \"https://github.com/google/re2/archive/506cfa4bffd060c06ec338ce50ea3468daa6c814.tar.gz\",\r\n        ],\r\n    )\r\n```\r\n\r\nThat commit is https://github.com/google/re2/commit/506cfa4bffd060c06ec338ce50ea3468daa6c814 which is on some random branch called abseil (https://github.com/google/re2/commits/abseil) and incompatible with master or any released version of re2. This completely breaks building against system libs. On Gentoo we have re2-2019.09.01 packaged (and i cant and wont change the package to use a random branch because other packages (eg chromium) need the real released versions not a random branch).\r\n\r\nTF wont build against master, it fails with errors about \"StringPiece\" vs \"string_view\". I cant figure out how to link properly to the diff but go here https://github.com/google/re2/compare/abseil#diff-6dc69df7618951357bb5fb674c66aa34 and click \"files changed\" then \"showing 120 changed files\" then click re2/re2.h. Its fairly obvious why its failing to build. \r\n\r\nWhat even is the abseil branch? The stats say `This branch is 166 commits ahead, 167 commits behind master.` So its clearly not just some feature branch that is going to be merged in soon.\r\nTF should be using master, if TF needs some new functionality in deps then please make a new re2 release first? I (grudgingly) understand why for eigen and llvm, but re2 has monthly release cycles\r\n\r\n@gunan @angerson @martinwicke \r\n", "comments": ["Here's where the dependency was changed: https://github.com/tensorflow/tensorflow/commit/517ad0e87f8d1f23aa68236bdc474188037347dc\r\n\r\nIt's only since 1.15 and is present in 1.15, 2.0, and 2.1rc0.", "Can we find out who owns re2 and ask them when a release with absl support\nwill be available?\n\nOn Mon, Dec 2, 2019, 14:18 Austin Anderson <notifications@github.com> wrote:\n\n> Here's where the dependency was changed: 517ad0e\n> <https://github.com/tensorflow/tensorflow/commit/517ad0e87f8d1f23aa68236bdc474188037347dc>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34726?email_source=notifications&email_token=AAEM57IBTJW4UWO2R25CXADQWWCRZA5CNFSM4JTKBAO2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEFWB6HI#issuecomment-560733981>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAEM57OP6LPXVDIXVIL44HLQWWCRZANCNFSM4JTKBAOQ>\n> .\n>\n", "Thanks to some insight from RE2 and Absl folks, I think I can explain more about these. The RE2 `abseil` branch is patched to use Abseil, or specifically:\r\n\r\n>The master branch is RE2 sans Abseil. It's RE2 with no dependencies. It's what open source users have been building, packaging, wrapping et cetera since 2010.\r\n>\r\n>The abseil branch is RE2 avec Abseil. It's RE2 with a dependency on Abseil. It's a \"soft fork\" that uses absl::string_view instead of re2::StringPiece, for example.\r\n\r\nSince Abseil is normally used at head (LTS releases for Abseil are only tagged twice per year, and Abseil is currently difficult to package partly because of that), the RE2 `abseil` branch has followed suit and is not frequently tagged.\r\n\r\nFor whatever reason, TensorFlow uses this Abseil-supporting RE2 soft-fork. It seems like the paths to support system libraries on Gentoo would be:\r\n\r\n- Switch TF to use non-Absl RE2 (which may be a bad idea, depending on the original reasons for the switch)\r\n- Wait for Abseil to become package-able, and the results of such (e.g. RE2 may use package-friendly abseil in `master`, or create a separate Gentoo RE2-with-package-friendly-abseil package). I'm not sure if the Abseil team has announced anything to do with this.", "okay thats what i suspected was happening but thanks for confirming everything. :) I'd really like some more announcements directly from re2 / absl people tho instead of second-hand here. In the mean time I can bundle re2 when packaging TF on gentoo and then when things are packagable i''ll work with the maintainers of re2 on gentoo and change things over.\r\n\r\nAnother part of using absl is that re2 should stick to using only the LTS branches so that things can be packaged easily on all distros that use it. and this means TF also needs to use the LTS absl branches. historically I've never packaged absl because TF bumped hte absl dep too often and there was no tag to package. Maybe absl should move to quarterly tags or something if TF constantly needs new updates from it. I dont actually know what TF uses from absl so not sure exactly how easily it can switch to the LTS branches.", "@sanjoy is it feasible for TF to depend on LTS version of absl?\r\nIt means updating it only twice a year.", "> @sanjoy is it feasible for TF to depend on LTS version of absl?\r\n> It means updating it only twice a year.\r\n\r\nI suspect that won't be easy.  The google3 build of TensorFlow uses absl at head, and so folks will have problems where TF builds fine internally but breaks in open source (when they use a feature available at HEAD but not in the LTS version, for instance).", "TF could not use features not available on the latest LTS. That's not hard\ntest or enforce. The bigger problem will be backwards incompatible changes,\nand although I expect those to be exceedingly rare, absl is explicitly not\nruling them out.\n", "Just so I understand: what are the problems with bundling re2 and absl with the TF gentoo package in perpetuity?  Is it mainly a resource usage (disk space etc.) concern or are there other issues?\r\n\r\n> TF could not use features not available on the latest LTS. That's not hard test or enforce.\r\n\r\nCan this can be done in a way that `blaze test` fails when someone uses an absl feature that's not available in the latest LTS?  If not, then this will introduce another point of divergence between `blaze test` and the open source bots which isn't ideal (though not a complete deal breaker).\r\n\r\nI agree with your point about the backwards incompatible changes.", "It's fundamentally the same process as we have for all other third party\nlibraries so I'm not concerned about that. If the LTS release is available\nin addition to head it might be possible to test this without using open\nsource testing, but I don't think it is.\n", "> Just so I understand: what are the problems with bundling re2 and absl with the TF gentoo package in perpetuity? Is it mainly a resource usage (disk space etc.) concern or are there other issues?\r\n\r\nFirst off, this isn't a Gentoo issue, it will be an issue for any distro that packages TensorFlow (I know arch, nix also do, debian does not yet because of bazel issues) I just happened to notice first.\r\n\r\nSome more background about why bundling libraries is bad is here: https://github.com/tensorflow/tensorflow/pull/20284 , specifically https://wiki.gentoo.org/wiki/Why_not_bundle_dependencies https://fedoraproject.org/wiki/Bundled_Libraries\r\nIts mainly a security issue and QA violation to bundle packages. Some distros are super strict about it (eg debian / redhat) I can do it in Gentoo but its not good. \r\n\r\nthe specific issue here is that chromium depends on re2-without-absl. and now TF depends on re2-with-absl. so, uh, which one would libre2.so be then? and then chromium and tensorflow become mutually incompatible on any linux distro? If it was some obscure library that only TensorFlow used then it might be fine, but many other packages depend on re2 as well so I cant just pull the rug out from everyone else.\r\n\r\nThe re2 package installs headers in `/usr/include/re2/` and the library is `/usr/lib64/libre2.so`. And that is the released version of re2 so that is what all other packages use. if re2-with-absl is a thing, it cant be installed in those paths cuz then every other package using re2 wont work. For now I just use the bundled re2 package in the TF build which works okay but its suboptimal. If re2 did a hard break, eg their next 2020 tag switched to absl then we could pin the versions and the package manager would figure things out (or complain if there was a conflict until packages were updated).\r\n\r\nThe other problem with re2 switching to absl is that in its current form, absl is not really packagable at all. I could make a package for it easy enough, but it I cannot package random git hashes, I can only package tags with actual version numbers. The LTS releases would work, but no one uses those so packaging it is currently pointless. How often would absl have to have releases for things to be fine on them? If absl made tags quarterly would that work? or monthly? \r\n"]}, {"number": 34715, "title": "Can't use assert statement inside tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro Linux\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n`assert` with a tensor inside a `tf.function` results in the following error:\r\n```\r\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe assert statement should be converted to `tf.Assert` by AutoGraph.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef f(x):\r\n    assert tf.constant(True), 'assert failed'\r\n    return x\r\n\r\nx = tf.ones(())\r\nprint(f(x))\r\n```", "comments": ["@Danmou  tf.Assert has optional parameters \"summarize\" and \"name\" which might be useful for debugging. With the expected behavior that you mentioned above, this information cannot be fed using Python's standard assert statement. What would happen in that case?", "In my opinion those can just be left at their default values. The name can be decided by AutoGraph just like for all the other ops it creates. If someone wants full control they can still use tf.Assert.\r\n\r\nThe current behavior is unintuitive - you can use a tensor as the condition in an if statement but not in an assert.", "I have tried on colab with TF version 2.0 ,2.1.0-dev20191201 and was able to reproduce the issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e8c0136d50bb52032150dfbf0a84119a/untitled428.ipynb). Thanks!", "This feature is actually available, just not enabled by default. Here's how to enable it:\r\n\r\n```\r\n@tf.function(experimental_autograph_options=tf.autograph.experimental.Feature.ASSERT_STATEMENTS)\r\ndef f(x):\r\n    assert tf.constant(False), 'assert failed'\r\n    return x\r\n```\r\n\r\nI'll leave this issue open to enable asserts by default, it's something we've been wanting to do for a while.", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ff28bb6d4c663080452d357157ebc4fd/untitled95.ipynb)..Thanks !", "Was able to replicate the issue in TF 2.6, please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/0c0a55ad93c428823333f544f62509d5/untitled95.ipynb#scrollTo=nBYiqtCuK98Z)..Thanks !"]}, {"number": 34683, "title": "TensorArray objects improperly handled as tf.function arguments or return values", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): tensorflow-gpu from binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.6.0.64\r\n- GPU model and memory: GeForce GTX  1080\r\n\r\n**Describe the current behavior**\r\nWhen you return a created TensorArray from a function decorated with tf.function, the returned object is an empty Tensor with an illegal state:\r\n  object type is **tf.Tensor(<unprintable>, shape=(), dtype=variant)**\r\n\r\nThis object cannot be used and will throw an exception when attempting to use it.\r\nSee also attached image for more details.\r\n\r\nExample of the exception, when calling concat method:\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'concat'\r\n\r\n\r\n**Describe the expected behavior**\r\nThe returned object should be a legal TensorArray instance, as is the case when you use only Eager mode functions.\r\nIn this case the object type is **<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f73b0386208>**\r\n\r\nThe code works fine if all functions are in Eager mode (see code example).\r\nThe code works fine if you concat the TensorArray before returning it (see code example).\r\n\r\n**Code to reproduce the issue**\r\n```\r\n@tf.function\r\ndef accumulate_no_error_graph(d):\r\n    arr = tf.TensorArray(tf.float32, num_rows)\r\n\r\n    for i in range(num_rows):\r\n        arr = arr.write(i, d[i])\r\n\r\n    return arr.concat()\r\n\r\ndef accumulate_no_error_eager(d):\r\n    arr = tf.TensorArray(tf.float32, num_rows)\r\n\r\n    for i in range(num_rows):\r\n        arr = arr.write(i, d[i])\r\n\r\n    return arr\r\n\r\n@tf.function\r\ndef accumulate_error_graph(d):\r\n    arr = tf.TensorArray(tf.float32, num_rows)\r\n\r\n    for i in range(num_rows):\r\n        arr = arr.write(i, d[i])\r\n\r\n    return arr\r\n\r\nnum_rows = 10\r\na = tf.random.uniform([num_rows, 2])\r\n\r\n# Works well\r\ndata = accumulate_no_error_eager(a)\r\nprint(f'From eager: {data}')\r\n\r\ndata = accumulate_no_error_graph(a)\r\nprint(f'From Graph with concat: {data}')\r\n\r\n# Doesn't work!\r\ndata = accumulate_error_graph(a)\r\nprint(f'From Graph no concat: {data}')\r\n```\r\n\r\n**Other info / logs**\r\nOutput of the code:\r\n```\r\nFrom eager: <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7ff67c5ac470>\r\nFrom Graph with concat: [0.6765479  0.28778458 0.85561883 0.41792393 0.667024   0.35702074\r\n 0.33666503 0.6575141  0.05998921 0.06930244 0.05852807 0.65689635\r\n 0.4156996  0.39695132 0.78036475 0.79794145 0.96339357 0.49462914\r\n 0.885311   0.04960382]\r\n\r\nTraceback (most recent call last):\r\nValueError: Tensorflow type 21 not convertible to numpy dtype.\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/569000/69805111-a7970a80-11e8-11ea-8233-54b2385728c0.png)\r\n\r\n\r\n", "comments": ["This issue is linked to the discussion at the end of this issue:\r\n[Issue 30409](https://github.com/tensorflow/tensorflow/issues/30409)", "A similar issue is around passing TensorArrays as arguments: https://github.com/tensorflow/tensorflow/issues/30409#issuecomment-508983979\r\n\r\nUpdating the bug title a bit to catch both cases.", "I have tried on colab with TF version 2.0 ,2.1.0-dev20191128  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/cc1b34dba4b052e81ff306fc3976507f/untitled415.ipynb). Thanks!", "TensorArrays have completely different representations in graph and eager modes. Inside graphs (tf.function) a TensorArray is a `DT_VARIANT` type tensor wrapping a `vector<Tensor*>` whereas in eager mode it is just a python list of EagerTensors so they are not really interoperable. This was done to avoid keeping a copy of the entire TensorArray when performing an operation in eager mode.\r\n\r\nA workaround for now maybe to return `arr.stack()` from the tf.function and unstack the returned value using `tf.TensorArray(tf.float32, num_rows).unstack(accumulate_error_graph(a))`. If we support returning TensorArrays from tf.function this is what we might end up doing under the hood. Even if we don't support this, we should definitely raise a better error message at function tracing time maybe.", "@diNatale,\r\nCould you please check @saxenasaurabh's workaround and let us know if you are still facing the issue. Thanks!", "> Could you please check @saxenasaurabh's workaround and let us know if you are still facing the issue. Thanks!\r\n\r\n@diNatale,\r\nAny updates regarding this? Please feel free to close the issue if resolved. Thanks!", "Hi @amahendrakar ,\r\nUnfortunately I have \"neglected\" my tf2/eager implementation due to these and a couple more issues, and had to focus hard on pushing on the older version (tf1-compatible).\r\nI will try to give it higher priority soon.\r\nCheers", "> Unfortunately I have \"neglected\" my tf2/eager implementation due to these and a couple more issues, and had to focus hard on pushing on the older version (tf1-compatible).\r\n> I will try to give it higher priority soon.\r\n> Cheers\r\n\r\n@daganesh,\r\nPlease submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "> Any updates regarding this? Please feel free to close the issue if resolved. Thanks!\r\n\r\n@diNatale,\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">No</a>\n", "Re-opening as we plan to address the issue internally.\r\n\r\n@hadifar although it raises the same error, your code describes a different problem - could you file a separate issue?", "@diNatale \r\nCan you please verify in the latest tf version and let us know if this is still an issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">No</a>\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">No</a>\n", "This is still on the roadmap, but is not being actively worked on at the moment.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">No</a>\n", "Not sure why this issue keeps getting auto-closed. It's definitely something we intend to fix.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">No</a>\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34683\">No</a>\n"]}, {"number": 34679, "title": "Missing Operations for TfLite GPU Delegate", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): \r\non Android: 'org.tensorflow:tensorflow-lite:0.0.0-nightly' and 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n- Are you willing to contribute it (Yes/No): Yes to the best of my possibilities\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm trying to use the GPU delegate for my custom tflite model. Creating the interpreter with the GPU delegate using this code:\r\n```\r\n val options = Options()\r\noptions.setUseNNAPI(false)\r\noptions.setAllowFp16PrecisionForFp32(true)\r\noptions.setNumThreads(NUM_THREADS)\r\nval gpuDelegate = GpuDelegate()\r\noptions.addDelegate(gpuDelegate)\r\n\r\nd.tfLite = Interpreter( loadModelFile(assetManager)!!, options)\r\n```\r\n results in my model being run normally, hence delivering the correct outputs but not being accelerated by the GPU in my opinion, since the execution time is exactly the same as without using the delegate. Adding the line  `d.tfLite!!.modifyGraphWithDelegate(gpuDelegate)` (I don't know if this is necessary, it would also be nice to know?) results in the following error:\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    CONV_2D: Max version supported: 1. Requested version 2.\r\n    LOCAL_RESPONSE_NORMALIZATION: Operation is not supported.\r\n    SPLIT: Operation is not supported.\r\n    First 0 operations will run on the GPU, and the remaining 12 on the CPU.ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n    java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    CONV_2D: Max version supported: 1. Requested version 2.\r\n    LOCAL_RESPONSE_NORMALIZATION: Operation is not supported.\r\n    SPLIT: Operation is not supported.\r\n    First 0 operations will run on the GPU, and the remaining 12 on the CPU.ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n```\r\n\r\nSo first of all: It would be nice to have those unsupported operations, i.e. \r\n\r\n- CONV_2D v2\r\n- LOCAL_RESPONSE_NORMALIZATION\r\n- SPLIT\r\n\r\nSecond of all: what does `ModifyGraphWithDelegate is disallowed when graph is immutable` mean? Do I have to make any changes to my tflite model?\r\n\r\n", "comments": ["@impjdi are there any plans to support dilation with GPU conv?", "@jdduke IIRC we have dilation for all our conv operators.  What is different in `CONV_2D` v2 vs. v1?\r\n\r\n@Noltibus \r\n\r\n1. We have limited resource, and probably can't accommodate individual requests of op shader implementations.  We'll keep your request in mind.\r\n\r\n2. The error message is coming from `//tensorflow/lite/core/subgraph.cc`... you can grep for that.  I have the impression you're calling `ModifyGraphWithDelegate` twice or maybe after `AllocateTensors` or something?  I'm not entirely sure when a graph is considered immutable without studying the code path.", "> IIRC we have dilation for all our conv operators. What is different in CONV_2D v2 vs. v1?\r\n\r\nv2 will have dilation values that aren't 1. If that's supported, we can update the GPU delegate to handle that version.", "Is there any way I can help with those operations. For example, the SPLIT operator? It might be somewhat similar to the SLICE operator? ", "> > IIRC we have dilation for all our conv operators. What is different in CONV_2D v2 vs. v1?\r\n> \r\n> v2 will have dilation values that aren't 1. If that's supported, we can update the GPU delegate to handle that version.\r\n\r\nDoes that mean CONV v2 is \"almost\" supported? ", "@Noltibus \r\n\r\nYeah, I have a change that will open it up which is being reviewed.  In the meantime, you can just comment out the `CONV_2D` operator's version check in `//tensorflow/lite/delegates/gpu/common/model_builder.cc`and things should just work.", "Ok, thank you!! So you will keep my request for LRN in mind? Is there any way I can help with SPLIT. I never wrote an Operation before but I learn pretty quickly ;-) ", "@Noltibus \r\n\r\nSure thing.  This is an open source project, and contributions are welcome :)\r\n\r\nBefore you dig into the shader code, I would advise to understand the DHWC4 format (or PHWC4... we renamed the format at one point) first.  It's essentially slicing a HWC tensor into 4-channel slices; that's used throughout the OpenGL delegate.  I'm not so sure about the OpenCL delegate; it has more complicated formats.\r\n\r\nThen, when it comes to the shader code, OpenGL delegate uses shader code generation to avoid code duplication, but unfortunately, it makes reading & understanding hard.  OpenCL doesn't use the code generation logic and might be easier to author one.  After that, you want to also write some unit tests to make sure things work as intended.\r\n\r\nOf course, the easiest is to find the closest op implementation and modifying that :)", "@Noltibus @impjdi \r\nhi i'm running posenet example of Tensorflow lite.\r\ni got a same issue in GPU Delegate.\r\n i found where this app select CPU,GPU, NNAPI.\r\nthat file is at android/posenet/src/main/java/org.tensorflow.lite.examples.posenet.lib/Posenet.kt\r\nUnfortunately, posenet example is written by kotlin. so i can't insert your same java code.\r\ni modified,\r\n`class Posenet(\r\n  val context: Context,\r\n  val filename: String = \"posenet_model.tflite\",\r\n  val device: Device = Device.GPU\r\n) : AutoCloseable {\r\n  var lastInferenceTimeNanos: Long = -1\r\n    private set\r\n\r\n  /** An Interpreter for the TFLite model.   */\r\n  private var interpreter: Interpreter? = null\r\n  private var gpuDelegate: GpuDelegate? = null\r\n\r\n  private fun getInterpreter(): Interpreter {\r\n    if (interpreter != null) {\r\n      return interpreter!!\r\n    }\r\n    val options = Interpreter.Options()\r\n    when (device) {\r\n      Device.CPU -> { }\r\n      Device.GPU -> {\r\n        gpuDelegate = GpuDelegate()\r\n        options.addDelegate(gpuDelegate)\r\n        options.setNumThreads(4)\r\n        options.setAllowFp16PrecisionForFp32(true)\r\n      }`\r\n\r\nbut it doesn't change fps. i guess that i just change name of device, not device's actual running.\r\nhow can i use correctly device's GPU? how can i modify?\r\nAnd.. why you insert \r\n`options.setAllowFp16PrecisionForFp32(true)\r\noptions.setNumThreads(NUM_THREADS)`\r\nthis line?\r\nif this is absent, it doesn't use GPU?", "@terryheo can help further diagnose.", "@eQueue you don't need set threads for GPU.\r\nJust changing\r\n```\r\nval device: Device = Device.CPU\r\n```\r\nto\r\n```\r\nval device: Device = Device.GPU\r\n```\r\nis enough for Posenet example.\r\nBTW, I've found that the current Posenet example depends on TFLite 2.0.0. I'll update it to use TFLite 2.2.0 which gives slightly better performance.\r\n\r\nIn my testing GPU gives about 35ms inference time with Posenet model.", "@Noltibus Is this still an issue ?\r\nPlease refer to the [comment](https://github.com/tensorflow/tensorflow/issues/34679#issuecomment-638840216) above and let us know ?Thanks! ", "According to the documentation here: https://www.tensorflow.org/lite/performance/gpu_advanced none of the operations I listed above are supported, yet, so this is an ongoing issue for me."]}, {"number": 34652, "title": "GPU unit-test infrastructure breakage (`Dockerfile.gpu`, `gpu/run_py3_core.sh`) ", "body": "Evertyime we sync (the `develop-upstream` branch) in our TF fork (with the `master` branch of this repo), we run a bunch of test-suites to qualify the changes before taking them.\r\n\r\nOne of those testsuites involves running the `GPU` tests (on the `cuda` platform) using the following command \r\n\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh GPU ./tensorflow/tools/ci_build/linux/gpu/run_py3_core.sh\r\n```\r\n\r\nThe above testsuite failed for us during our latest sync attempt. \r\n\r\nWe re-ran the CI job directly on the code in the `master` branch (as opposed to the merged code in our repo), and we see the same errors, leading us to believe that the breakage originates due to changes in the upstream repo.\r\n\r\nLink to the CI run log mentioned above : http://ml-ci.amd.com:21096/job/tensorflow-cuda/1/console\r\n\r\nOur last successful merge was about 3 weeks ago, so the breakage seems to have been introduced sometime within the last 3 weeks. \r\n\r\nErrors similar to the ones we see, have been reported in other github issues.\r\nhttps://github.com/tensorflow/tensorflow/issues/34429\r\nhttps://github.com/tensorflow/tensorflow/issues/34117\r\n\r\nWhen we look at the invocation log for the `Linux GPU` (in the **Continuous build status** section of README.md), the run is passing, but it seems to use a different script and different env-vars/options/etc as compared to those used by the `Dockerfile.gpu` and `./tensorflow/tools/ci_build/linux/gpu/*.sh` scripts that are used by our CI job.\r\n\r\nThe request here is to please identify what is causing the breakage we see on our end, and fix it. Also please update the GPU test scripts (`Dockerfile.gpu` + `./tensorflow/tools/ci_build/linux/gpu/*.sh` ) to match what is done by the GPU CI runs kicked off internally.\r\n\r\nthanks\r\n\r\n\r\n/cc @chsigg @parallelo @sunway513 @whchung \r\n\r\n", "comments": ["For one of the errors we see in our CI run\r\n\r\n```\r\nImportError: /home/ubuntu/workspace/tensorflow-cuda/bazel-ci_build-cache/.cache/bazel/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_2_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN10tensorflowlsERSoRKNS_6StatusE\r\n----------------\r\nNote: The failure of target //tensorflow:create_tensorflow.python_api_2_tf_python_api_gen_v2 (with exit code 1) may have been caused by the fact that it is a Python 2 program that was built in the host configuration, which uses Python 3. You can change the host configuration (for the entire build) to instead use Python 2 by setting --host_force_python=PY2.\r\n```\r\n\r\nwe tried using the recommended workaround, (i.e. the `--host_force_python` option) but it seemed to have no effect", "@gunan gentle ping\r\n", "@angerson Could you take a look?\r\nis this due to our dockerfiles being broken?", "The XLA tests (i.e. the tests run by the script https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/xla/linux/gpu/run_py3.sh )  seem to be similarly broken on the CUDA platform \r\n\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-cuda-xla/1/console\r\n\r\n\r\n\r\n", "I don't think this would be related to the Dockerfiles, at least not the ones in tools/dockerfiles; those aren't used by these old CI build scripts. I've been trying to get the Dockerfiles to build again (the old maintainer departed) but I'm running into trouble compiling with TensorRT.\r\n\r\nAs for the tests, ci_build/release contains tests we run internally; you should be able to find some helpful build options there. I don't think we use ci_build.sh internally.", "@gunan @angerson thanks for the answers\r\n\r\nIt would be really helpful if you could make the dockerfiles+scripts in the public repo functional again. It would give us the ability to both \r\n* experimentally verify what is currently working / broken in the upstream repo\r\n* ensure that the changes in the PRs we submit are not regressing existing functionality.\r\n\r\nIn the meantime, is there any way we can get a current pass/fail status for all the tests that would be run by these 3 scripts.\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/gpu/run_cc_core.sh\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/gpu/run_py3_core.sh\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/xla/linux/gpu/run_py3.sh\r\n\r\nThe `rocm` versions of those scritps are what we use to run our CI. When we get a regression in those tests, we need to figure out whether the cause is a change in the upstream repo, or something we changed in our fork. We have not had the ability to run these tests on the upstream repo for more than a month now, and hence do not know which tests are currently broken, and hence the request to you.\r\n\r\nthanks\r\n\r\ndeven", "@gunan @angerson any update here?", "Gentle pint here. The fact that those scripts are broken makes it hard for us `ROCm` people to do sanity checks between that of `Cuda`. We are trying to behave like good first class citizen here and not break `Cuda` build in `ROCm` features. Could you either share the way to do the `internal` build or take a look at those scripts? For the minimal, sharing the `Linux GPU` build for PR sanity check can be useful for us.", "We did update our dockerfiles, but our CI does not run using ci_build.sh anymore\r\none thing to try can be to use devel dockerfile TF is publishing on dockerhub, and run the script you run_py3_core.sh script you ran directly on it.\r\n\r\nAbout the python version isue you ran into may be due to ci_build.sh trying to override `PYTHON_BIN_PATH` environment variable.\r\n"]}, {"number": 34585, "title": "Tensorflow 2.0 does not iterate through entire dataset when tf.keras.model.fit is called", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Mac OS 10.14.6**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **2.0**\r\n- Python version: **3.6.0**\r\n\r\n**Describe the current behavior**\r\nI am training a model in tf.keras with tensorflow 2.0. **I am having an issue where my model appears to train successfully, but it is not iterating through the entire dataset.** A few things indicate that it's not going through the entire dataset: \r\n\r\n1. the model is training super fast (an epoch should take ~45 minutes, which is estimated, but it always finishes in < 1 minute),\r\n2. the validation metrics are always reported 0. and,\r\n3. the progress bar that prints out never fills up, and it stops at ~1/1000 of the data. \r\n\r\nThe progress bar during training looks something like this:\r\n\r\n```\r\nEpoch 1/300   192/162636 [..............................] - ETA: 45:42 - loss: 0.4783 - mean_squared_error: 0.4783 - val_loss: 0.0000e+00 - val_mean_squared_error: 0.0000e+00\r\nEpoch 2/300   186/162636 [..............................] - ETA: 45:42 - loss: 0.4783 - mean_squared_error: 0.4783 - val_loss: 0.0000e+00 - val_mean_squared_error: 0.0000e+00\r\n...\r\nEpoch X/300   192/162636 [..............................] - ETA: 45:42 - loss: 0.4783 - mean_squared_error: 0.4783 - val_loss: 0.0000e+00 - val_mean_squared_error: 0.0000e+00\r\n```\r\n\r\n I restructured the code into tensorflow 1.15, and I do not have this issue. When I call `tf.compat.v1.enable_v2_behavior()`, I see this behavior again. There are no errors, warnings, or info that is reported, it just stops iterating through my dataset early. I am following [this tutorial](https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/#comment-510384) for Multiple Input Series. \r\n\r\n**Describe the expected behavior**\r\nThe expected behavior is that an epoch will completely go through the dataset, and it will report a reasonable validation loss, and it will take some time to go through the entire dataset. I see the correct behavior in TF 1.15, and the epochs take ~45 minutes to complete (as expected), the validation metrics are calculated, and the progress bar looks something like this (which is nothing special :) )\r\n```\r\nEpoch 16/300\r\n162605/162636 [============================>.] - ETA: 0s - loss: 1.2883e-05\r\n162636/162636 [==============================] - 2946s 1ms/sample - loss: 1.2883e-05 - val_loss: 1.5680e-05\r\nEpoch 17/300\r\n162605/162636 [============================>.] - ETA: 0s - loss: 1.2631e-05\r\n162636/162636 [==============================] - 2688s 5ms/sample - loss: 1.2633e-05 - val_loss: 2.1342e-05\r\n```\r\n\r\n**Code to reproduce the issue**\r\nI have a time-series dataset. It is very small so I am able to load it into memory, so I do not need the dataset API. I am windowing the time-series to produce two arrays, X and Y, and it looks something like this, \r\n\r\n```\r\nX=[\r\n   [[1,2,3],[4,5,6],   [7,8,9]],\r\n   [[4,5,6],[7,8,9],   [10,11,12]],\r\n   [[7,8,9],[10,11,12],[13,14,15]],\r\n   ...\r\n  ] \r\nY = [\r\n     [4],\r\n     [7],\r\n     [10],\r\n     ...\r\n    ]\r\n```\r\n\r\n(yes, I realize that I could just as easily only include one of the features. I've tried that, i.e. `X=[[[1,2,3]], [[4,5,6]], [[7,8,9]], ...], and it still doesn't work)\r\n\r\nThen, I build my model:\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\r\nmodel.add(Dense(1))\r\nmodel.compile(optimizer='adam', loss='mse')\r\n```\r\n\r\nand then I train it:\r\n\r\n```\r\nmodel.fit([X],[Y],num_epochs=300,validation_split=0.2)\r\n```\r\n\r\nIt correctly reports the number of train and validation samples, and then the progress bar pops up... but that's where the success stops. The val_loss and val_mean_squared_error is always 0, for every epoch, and it appears to never train more than a fraction (~1/1000) of my dataset, although that fraction varies slightly between epochs. This is the print out:\r\n\r\n```\r\nEpoch 1/300   192/162636 [..............................] - ETA: 45:42 - loss: 0.4783 - mean_squared_error: 0.4783 - val_loss: 0.0000e+00 - val_mean_squared_error: 0.0000e+00\r\nEpoch 2/300   186/162636 [..............................] - ETA: 45:42 - loss: 0.4783 - mean_squared_error: 0.4783 - val_loss: 0.0000e+00 - val_mean_squared_error: 0.0000e+00\r\n...\r\nEpoch X/300   192/162636 [..............................] - ETA: 45:42 - loss: 0.4783 - mean_squared_error: 0.4783 - val_loss: 0.0000e+00 - val_mean_squared_error: 0.0000e+00\r\n```\r\n\r\nWhen I call `tf.compat.v1.enable_v2_behavior()` in TF 1.15, the behavior is the same as TF 2.0.\r\n\r\n\r\n**Other info / logs**\r\n[Here is a link to the StackOverflow question that I proposed before I had confirmed that this is a TensorFlow bug](https://stackoverflow.com/questions/58826512/tensorflow-2-0-does-not-iterate-through-entire-dataset-when-tf-keras-model-fit-i/59001781#59001781)\r\n", "comments": ["@michaelarfreed \r\n\r\nLooks like code is incomplete.Please, help us with simple stand alone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "sorry about that! here's the full code, with a simplified example dataset:\r\nlet me know if you need anything else. \r\n\r\n```\r\nimport json\r\nimport os\r\nimport pickle\r\nimport random\r\nimport sys\r\nimport yaml\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import Input, Model\r\nfrom tensorflow.keras.layers import Concatenate, Dense, LSTM, RepeatVector, Reshape, TimeDistributed\r\n\r\ndef main():\r\n    # tf.compat.v1.enable_v2_behavior()\r\n\r\n    # create the dataset\r\n    datasetD = make_dataset()\r\n\r\n    train_x = datasetD['train']['x']\r\n    train_y = datasetD['train']['y']\r\n    # test_x  = datasetD['test']['x']\r\n    # test_y  = datasetD['test']['y']\r\n    val_x   = datasetD['val']['x']\r\n    val_y   = datasetD['val']['y']\r\n\r\n    # create the model\r\n    model = create_model()\r\n    model.compile(optimizer='adam',\r\n                  loss='mse')\r\n    \r\n    print(model.summary())\r\n\r\n    history = model.fit([train_x],[train_y],\r\n                        batch_size=32,\r\n                        epochs=300, \r\n                        validation_data=([val_x],[val_y]),\r\n                        validation_freq=1)\r\n\r\ndef make_dataset():\r\n    input_window_samps  = 50\r\n    num_signals         = 1\r\n    output_window_samps = 3\r\n    returnD = {}\r\n\r\n    returnD['train'] = {}\r\n    returnD['train']['x'] = []\r\n    returnD['train']['y'] = []\r\n    \r\n    for i in range(10000):\r\n        returnD['train']['x'].append(np.arange(i,i+input_window_samps))\r\n        returnD['train']['y'].append(np.expand_dims(np.arange(i+input_window_samps,i+input_window_samps+output_window_samps),axis=1))\r\n    \r\n    returnD['val'] = {}\r\n    returnD['val']['x'] = []\r\n    returnD['val']['y'] = []\r\n    \r\n    for i in range(10000,20000):\r\n        returnD['val']['x'].append(np.arange(i,i+input_window_samps))\r\n        returnD['val']['y'].append(np.expand_dims(np.arange(i+input_window_samps,i+input_window_samps+output_window_samps),axis=1))\r\n    \r\n    returnD['test'] = {}\r\n    returnD['test']['x'] = []\r\n    returnD['test']['y'] = []\r\n    \r\n    for i in range(20000,30000):\r\n        returnD['test']['x'].append(np.arange(i,i+input_window_samps))\r\n        returnD['test']['y'].append(np.expand_dims(np.arange(i+input_window_samps,i+input_window_samps+output_window_samps),axis=1))\r\n\r\n    return returnD\r\n\r\ndef create_model():\r\n    # from https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/ \"Multiple Parallel Input and Multi-Step Output\" example \r\n    input_window_samps  = 50\r\n    num_signals         = 1\r\n    output_window_samps = 3\r\n    units0 = 10\r\n    units1 = 10\r\n\r\n    input = Input(shape=(input_window_samps*num_signals,))\r\n    x = Reshape((input_window_samps,num_signals))(input)\r\n    x = LSTM(units0,activation='relu')(x)\r\n    x = RepeatVector(output_window_samps)(x)\r\n    x = LSTM(units1,activation='relu',return_sequences=True)(x)\r\n    x = TimeDistributed(Dense(num_signals))(x)\r\n\r\n    model = Model(inputs=input,outputs=x)\r\n    return model\r\n```\r\n", "@michaelarfreed \r\n\r\nI have tried on colab with TF version 1.15 and i do not see any issue.But with TF 2.0 i am able to reproduce the issue. However it looks like it is resolved with TF nightly version (`!pip install tf-nightly==2.1.0dev20191203`). Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e751dbf44cf2051a5117caaf663ccb94/untitled447.ipynb).Thanks!", "@ravikyram \r\n\r\nThank you! I only see the error on TF 1.15 when I uncomment the line `tf.compat.v1.enable_v2_behavior()`. When it is in TF 1.15, I do not see any issue, either. Sorry for the confusion about that.\r\n\r\nI see that the epoch reports the validation metrics in the nightly build, but I noticed that the progress bar still does not completely fill up and each epoch takes significantly shorter time than it did in TF 1.15. I just want to confirm that the behavior I'm seeing on my machine matches your behavior, and that every batch is being processed. This is the output I'm getting from the training:\r\n```\r\nEpoch 1/300\r\n   64/10000 [..............................] - ETA: 8:19 - loss: 31558043.8788 - val_loss: 234850827.5200Epoch 2/300\r\n   64/10000 [..............................] - ETA: 3:31 - loss: 39555757.7273 - val_loss: 234850769.5104Epoch 3/300\r\n   64/10000 [..............................] - ETA: 3:28 - loss: 38133528.6667 - val_loss: 234850704.8192Epoch 4/300\r\n   64/10000 [..............................] - ETA: 3:25 - loss: 32333142.6818 - val_loss: 234850648.7808Epoch 5/300\r\n   64/10000 [..............................] - ETA: 3:30 - loss: 25878961.6970 - val_loss: 234850591.9232Epoch 6/300\r\n```", "I am able to see the issue with TF 1.15 when we uncomment the line `tf.compat.v1.enable_v2_behavior()`. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/7e597c68b2d988befbc9cd6f42c2a55a/untitled478.ipynb).Thanks!", "Thank you, @ravikyram ! ", "Were you able to look into the other issue that I noticed in TF 2.0, where the ETA reported for an epoch is much higher than the amount of time an epoch takes? In TF 1.15 with `tf.compat.v1.enable_v2_behavior()` commented out, it correctly estimates the ETA and an epoch takes the amount of time listed by ETA (+/- a few seconds).  In TF 2.0 (in both the nightly build and the release) and in TF 1.15 with `tf.compat.v1.enable_v2_behavior()` uncommented, it estimates the same ETA as in TF 1.15 with `tf.compat.v1.enable_v2_behavior()` commented out, but an epoch is over within <30 seconds, even when eta > 2 minutes.\r\n\r\n(From my previous comment)\r\n> I see that the epoch reports the validation metrics in the nightly build, but I noticed that the progress bar still does not completely fill up and each epoch takes significantly shorter time than it did in TF 1.15. I just want to confirm that the behavior I'm seeing on my machine matches your behavior, and that every batch is being processed.", "@rachellim could you please take a look? thanks", "To give a more concrete example and expand on my previous comment, when I use my true dataset, in TF 1.15 when `tf.compat.v1.enable_v2_behavior()` is commented out, the ETA for an epoch is ~30 minutes, and an epoch takes around 30 minutes. In TF 2.0 and in TF 1.15 with `tf.compat.v1.enable_v2_behavior()` uncommented, the ETA for each epoch is still 30 minutes, but each epoch takes < 30 seconds, and this speed-up seems too good to be true. I also see this speedup in the nightly build that is in your notebook. The effect is less noticeable in the example dataset that I provided since the ETA is low, but it is there.", "I noticed that the bug that I have described is also present in the TF 2.1 release. If I call `!pip install tensorflow==2.1` where you have called `!pip install tensorflow==2.0` in [this notebook](https://colab.sandbox.google.com/gist/ravikyram/e751dbf44cf2051a5117caaf663ccb94/untitled447.ipynb), I see the same behavior that I have been describing (1. the progress bar does not fill up for a full epoch, 2. the ETA for an epoch is 4+ minutes, but an epoch finishes in seconds). The one thing that is fixed in the 2.1 release is that val_loss is calculated.\r\n\r\nPlease let me know if you need any more information from me to resolve this bug, I really appreciate your help and work!! ", "Was able to replicate the issue in TF 2.6.0-dev20210529,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/df977b60c021ddb6a0d58d33ba4963cb/untitled91.ipynb#scrollTo=hP3UL5gK9TgP).. Thanks !", "I could reproduce the issue with TF 2.6 .Please, find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/03017ac9b78eeb91cd61e46cd4b71461/untitled91.ipynb#scrollTo=V7WkTqPT9lmF).Thanks!"]}, {"number": 34523, "title": "TensorFlow Lite schema updater loses floating-point precision", "body": "I am concerned about [tensorflow/lite/schema/upgrade_schema.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/upgrade_schema.py). I see that the schema upgrade is performed by converting flatbuffers binary to JSON and then backward.\r\nHowever, there is a problem with flatbuffers binary -> JSON step described in https://github.com/google/flatbuffers/issues/5371 which leads to float32 precision loss. `float`-s are serialized as `\"0.6f\"` and `double`-s as `\"0.12d\"` (equivalent to C++'s stringstream API). Thus, for example, float32 values that are very small, e.g. `0.123e-6`, will be written as `0.0`.", "comments": ["Hi gargn@, could you please help answer this question, it's about conversion between different versions of the schema. Thanks.", "Reassigning to @aselle who has more context.", "Has there been any movement on this? I ran into this exact problem where the upgrade script actually produced an invalid output tflite file. I use quantized models, and some of the quantization scales were below 10^-6, which got mapped to 0. Apart from the possible loss of accuracy of the model, a quantization scale of 0 can't even be loaded for some platforms because they check for it.", "@miaout17  could you ptal?\r\nThis doesn't seem to be resolved. If flatbuffers doesn't change, I suppose the best thing to do would be to generate python bindings for every version of the schema so that you could programatically move data w/o using json. Of course this is somewhat painful because there has to be more data marshaling (which is why I used the json approach in the first place).\r\n"]}, {"number": 34371, "title": "tf.io.gfile.glob does not list all files in a Google Cloud Storage bucket", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): ?\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): /\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\n**Describe the current behavior**\r\nWhen listing file with `tf.io.gfile.glob` not all images are returned. It seems it is not resolving the folders recursively. \r\nWhen using the same path with gsutils  we get the correct image count.\r\n\r\n\r\n**Describe the expected behavior**\r\nWhen using the same gs:// path with **gsutils** we get the correct amount of images. \r\n\r\n**Code to reproduce the issue**\r\nIn order to reproduce the behavior I prepared a Google Bucket with the following structure. The bucket is public accessible, please feel free to use it to reproduce the behavior on your end: `gs://tensorflow-issue-reproduction`\r\n\r\n![level0](https://user-images.githubusercontent.com/1991664/69040135-93375e80-09ed-11ea-927f-da445d2bae10.png)\r\n![level1](https://user-images.githubusercontent.com/1991664/69040139-94688b80-09ed-11ea-9e19-28af47c3bcee.png)\r\n![level2](https://user-images.githubusercontent.com/1991664/69040146-96cae580-09ed-11ea-8894-c69a82acbbaa.png)\r\n![level3](https://user-images.githubusercontent.com/1991664/69040150-9894a900-09ed-11ea-94dc-49fb2d9b3b9b.png)\r\n\r\nIn summary we have 4 jpg images nested in different folder levels.\r\n\r\nTensorFlow 2 code to reproduce\r\n```\r\nfiles = tf.io.gfile.glob('gs://tensorflow-issue-reproduction/**/*.jpg')\r\nprint('file count: ', len(files))\r\n# found files 1\r\n```\r\n\r\ngsutil command which works properly\r\n```\r\ngsutil du gs://tensorflow-issue-reproduction/**/*.jpg | wc -l\r\n# found files 4\r\n```\r\n\r\n**Other info / logs**\r\n/\r\n\r\nBest regards\r\nSascha\r\n", "comments": ["Hi @ymodak \r\n\r\nany ideas?\r\n\r\nBest regards\r\nSascha", "Will take a while until I can get to this, but it's something to be handled as part of https://github.com/tensorflow/community/pull/101", "Hi @mihaimaruseac \r\n\r\nwhat does this mean \"as part of tensorflow/community#101\"?\r\nWhat are the next steps that are required to solve this issue?\r\nWho is in charge?", "It means we are refactoring all of the infrastructure around filesystem support. Currently POSIX is moved to the new world, Windows follows then GCS and the others. I am coordinating this process", "This issue still persists. Am I missing something? Is recursive globbing support dropped or something like that? Thanks.", "Unfortunately we didn't yet get to this issue. Apologies for the delay", "@mihaimaruseac thank you very much for the response. no worries.", "See also, #39565", "I just noticed you are using `**` but TF's [globbing patterns don't include that case](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/file_system.h;l=184-215;drc=516ae286f6cc796e646d14671d94959b129130a4)\r\n\r\nIncluding `**` would complicate [an already complicated implementation](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/file_system_helper.cc;l=123-266;drc=8b5b9dc96666a3a5d27fad7179ff215e3b74b67c) but I think we can take in PRs to add support for `**`", "@SaschaHeyer Could you please refer to the above comment and let us know if you still face this issue ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Was there any relevant changes in `tf.io.gfile` to change the behavior?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 34346, "title": "ImageDataGenerator does not work with tpu", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tensorflow from colab\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nCurrently it is not possible to use fit_generator. Instead we need to use .fit function.\r\n\r\nUsing ImageDataGenetor, I tried to fit the model but I got an issue :\r\n```\r\nAssertionError                            Traceback (most recent call last)\r\n\r\n<ipython-input-18-46d4a00b20c5> in <module>()\r\n      5      train_gen.flow(X_train, y_train, batch_size=batch_size), # tf.data.Dataset.from_tensor_slices((X_train[:2000], y_train[:2000])).batch(batch_size).repeat(), #\r\n      6     steps_per_epoch=  len(X_train)//batch_size,\r\n----> 7     epochs=3,\r\n      8      )\r\n\r\n2 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    725         max_queue_size=max_queue_size,\r\n    726         workers=workers,\r\n--> 727         use_multiprocessing=use_multiprocessing)\r\n    728 \r\n    729   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    617         validation_split=validation_split,\r\n    618         shuffle=shuffle,\r\n--> 619         epochs=epochs)\r\n    620     if not dist_utils.is_distributing_by_cloning(model):\r\n    621       with model._distribution_strategy.scope():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\r\n   2312         x = ds.batch(batch_size, drop_remainder=drop_remainder)\r\n   2313       else:\r\n-> 2314         assert isinstance(x, dataset_ops.DatasetV2)\r\n   2315         training_utils.validate_dataset_input(x, y, sample_weight,\r\n   2316                                               validation_split)\r\n\r\nAssertionError: \r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nfrom the keras documentation, it should work. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nHere the link to the colab code : https://colab.research.google.com/drive/1bdNfb127n-VI6ab9_sUTOgRzkil88a9n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nDoes that mean we cannot use generator with tensorflow 1.15 ? ", "comments": ["@Shiro-LK I am not able to reproduce this error (I am seeing different error, see below). Did you find this colab link on TF website? Can you please point us to the source of the tutorial if it is on TF website? Thanks!\r\n\r\nError trace I am noticing is \r\nEpoch 1/3\r\n```\r\n---------------------------------------------------------------------------\r\nAbortedError                              Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n10 frames\r\nAbortedError: Session 33fa83563cd3de92 is not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAbortedError                              Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nAbortedError: Session 33fa83563cd3de92 is not found.\r\n```", "Hi,\r\nI thank you for your reply.\r\nI am sorry, I tried to change one thing to make the generator worked but I did not succeed. To get my error replace :\r\n```\r\ntf.data.Dataset.from_generator( lambda: map(tuple, generator(train_gen.flow(X_train, y_train, batch_size=batch_size), \r\n                                                                 len(X_train)//batch_size) ), output_types=(tf.float32, tf.float32), output_shapes = (\r\n                           tf.TensorShape([batch_size, 28, 28, 1]), \r\n                           tf.TensorShape([batch_size,10]) ) ) ,\r\n```\r\n\r\nby the ImageDataGenerator : \r\n`train_gen.flow(X_train, y_train, batch_size=batch_size),`\r\n\r\nRegarding the colab file, I just use the same presentation used in this one :\r\nhttps://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb\r\n\r\nBut I modify few things like adding a generator, and use Model API instead of Sequential for instance. My code works with array but not with generator. (you can uncomment X_train, y_train in the fit function to see the code works without generator)", "@Shiro-LK can you please create a standalone code to reproduce the issue. It will be faster to resolve the issue that has a standalone code. If this is not related to a Bug/Performance, then please post it in Stackoverflow. Thanks!", "@jvishnuvardhan  Thank you for your reply.\r\nTo reproduce this issue we need to use a generator on the fit  function. For instance we can use the ImageDataGenerator from keras. On GPU the same code works.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nimport os\r\nimg_size=28\r\nN = y_train.shape[1]\r\n\r\n# load data\r\n(X_train, y_train), (X_val, y_val) = tf.keras.datasets.mnist.load_data() \r\n\r\nX_train = np.expand_dims(X_train, axis=-1).astype(\"float32\")\r\nX_val = np.expand_dims(X_val, axis=-1).astype(\"float32\")\r\ny_train = tf.keras.utils.to_categorical(y_train).astype(\"float32\")\r\ny_val = tf.keras.utils.to_categorical(y_val).astype(\"float32\")\r\n\r\n\r\n# create model\r\nwith tpu_strategy.scope():\r\n  model = cnn_model(N, (img_size, img_size,1))\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3, ),\r\n      loss='categorical_crossentropy',\r\n      metrics=['accuracy'])\r\n\r\n\r\nmodel.summary()\r\n\r\nbatch_size = 8 * 20 \r\n\r\n# training\r\ntrain_gen = ImageDataGenerator(rotation_range=25, rescale=1/255.0)\r\nmodel.fit(train_gen.flow(X_train, y_train, batch_size=batch_size),\r\n    steps_per_epoch=  len(X_train)//batch_size,\r\n    epochs=3,    )\r\n```\r\n\r\nThe issue does not come from the code because it works well on GPU. The issue comes only when TPU is used.\r\n\r\nThe link on colab if the entire code is needed. https://colab.research.google.com/drive/1eNERfIFoDGzKOICWcj9PEu4LzUt2snZx", "I had tried `tf-nightly` and it is throwing `AssertionError `. Please take a look at the [gist is here](https://colab.sandbox.google.com/gist/jvishnuvardhan/faf6dcd23a408621edc6e1be250ede66/keras-tpu-mnist.ipynb). Thanks!", "I believe we currently don't support fit_generator in distribution strategy or TPUs, but assigning to @guptapriya to confirm.", "@frankchn the user is not using fit_generator, they are using fit with input as a generator. \r\nI am not sure about 1.15 but I believe in 2.0 we convert the generator input to dataset and then it should work. I am not sure about TPUs though. @tomerk would you be able to confirm the 2.0 status? ", "As of 2.1 (not 2.0) users should be able to use fit_generator, not just fit. @robieta  also fixed a number of generator bugs for 2.1 so it's possible these issues were fixed by that. (And a lot of keras/tpu integration bugs were fixed for 2.1 as well)\r\n\r\nThe colab also actually isn't runnable (I'm guessing when it was written/run it was out of order, because y_train is accessed before it's defined) so I'm not sure what error is getting thrown.\r\n\r\n@Shiro-LK do you run into issues with the 2.1 rc?", "Hi,\r\n\r\nI tried with the new version, the assertion is gone but a new error came with fit and fit_generator:\r\n\r\n> NotFoundError: {{function_node __inference_distributed_function_4785}} No registered 'PyFunc' OpKernel for 'CPU' devices compatible with node {{node PyFunc}}\r\n> \t.  Registered:  <no registered kernels>\r\n> \r\n> \t [[PyFunc]]\r\n> \t [[MultiDeviceIteratorGetNextFromShard]]\r\n> \t [[RemoteCall]]\r\n> \t [[IteratorGetNextAsOptional]] \r\n\r\nI  am wondering if the ImageDataGenerator is compatible with TPU ?", "The error that you're seeing is expected, but I'll admit we've not done a good job of explaining what's going on, so let me give that a shot. When you use `Dataset.from_generator` (or pass a generator to Keras which will call it under the hood), the Dataset embeds the generator in a PyFunc op in its graph, and every time that op is invoked it calls next on the generator and gets the resultant bytes. (Basically treating Python as a black box.)\r\n\r\nWhen everything is running on the same machine this is fine, but the trouble is that the ways TPUs work is that there is a separate machine controlling the TPU (called, imaginatively, the TPU host controller. ^^), and you run things on the TPU by sending it a TensorFlow graph to execute. So the graph containing that PyFunc gets sent to the TPU, and the TPU can't execute it because there is no Python on the TPU host machine. (And even if there was, it wouldn't be the same interpreter with the same state as your local machine.) So it fails by telling you it can't execute the PyFunc op, but not in a very clear way unfortunately.\r\n\r\nNow I'm guessing the pattern that you expect is that the generator (or generator-like in case of a Keras Sequence) gets executed on the local machine and then the data is streamed to the TPU for ingestion and execution RPC-style. There is actually a Dataset that supports that functionality called the StreamingFilesDataset (https://github.com/tensorflow/tensorflow/blob/9b82752179bd4a61a9d33a638b8d9cb06adcf9e0/tensorflow/python/tpu/datasets.py#L50), which despite the name can also just stream in-memory data, but it is not a public symbol and is not actively maintained AFAIK. However if we think this is a workflow we want to support out of the box maybe it's worth dusting it off and integrating it into Keras. @frankchn @tomerk @alextp WDYT? (And @saeta who wrote the thing in the first place.)", "Pinging @jsimsa for tf.data expertise.\r\n\r\nI think cases like this show we do need to handle sources we cannot move in tf.data; it should still be possible to do prefetching / buffering / etc on the TPU controller to hide most of the latency, assuming the generator produces data quickly enough.", "I think the cleanest solution to this problem would be to make the placer aware of what kernels are available across processes and have it place (and remotly execute) the PyFunc op on the TPU coordinator host.\r\n\r\n@mrry @lindong28 ", "Maybe it's worth noting that https://github.com/pytorch/xla already supports generator-like inputs for TPUs. For example: https://github.com/pytorch/xla/blob/b67c7f00fc6b778a1d864e7e896cc3a1ccb73da6/test/test_train_mnist.py#L69. Actually, it's the only API that I saw.\r\n\r\nI'd be happy to see this feature in tf 2 as well.", "@jsimsa In order for placer to know what kernels are available on each process, each progress needs to send full list of its kernels to the coordinator host. It will likely become a performance bottleneck when there are hundreds of hosts and each host has thousands of kernels. I am not sure there is a good solution to address this issue.\r\n\r\nAlternatively, can we pass just the necessary information from higher layer down to the placer, similar to how tf.distribute provides explicit APIs to copy data between devices?\r\n\r\n", "Do you have back of the envelope analysis to back your claim? A thousand kernels should be representable in <1MB, so (in the large scale case you seem to be using as a justification for not implementing sharing of the op registry) you are talking about sending less than 1GB to a single machine, which given 100Gb network would be less than a second. In other words, I would expect the cost of this to be negligible given other costs at such at large scale.", "Good point. A few months ago @qqfish and I discussed this idea of having every host send kernel registration info to the coordinator to fix an op placement bug. We didn't do that due to concern with the performance overhead.\r\n\r\nI verified that if we run a simple TF program on a host with GPU, KernelDefBuilder::Build() [1] is called 6751 times for CPU kernel and 3313 times for GPU kernels. And the sum of bytes of those serialized KernelDef objects is 434 KB. For a large production job with 256 workers, the coordinator host will need to read 256 * 434 KB= 111 MB data before it can compute the op placement. The pure network overhead seems acceptable. I don't have a good estimate of how many time is needed for protobuf encode/decode overhead.\r\n\r\n@mrry Do you have any concern with having all workers send kernel registration info to the coordinator in order to compute op placement?\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/kernel_def_builder.cc#L145", "@lindong28 434 KB/worker doesn't seem like a big deal (e.g. compared to the size of some FunctionDefs), but if necessary you could compress and/or hash the kernel registrations to reduce the network traffic. A lazy protocol, where you fetch the registrations on demand, could work as well. If you do something like that then the overhead should be manageable.", "Sounds good. I can give this a shot.", "Hi, I'm facing a similar issue with custom code on Tensorflow 2.1.0-rc1 on Google Colab. Apart from the model used, the only differences I see from the \"code to reproduce the issue\" of the creator of this topic is the Tensorflow version and the use of `flow_from_dataframe` in my case instead of `flow` (sorry, I'm not able to share code to reproduce the issue right now). Here is the traceback of the error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-36-dfef2c0bfcf8> in <module>()\r\n     39                         validation_data=val_generator,\r\n     40                         validation_steps = val_generator.samples // BATCH_SIZE,\r\n---> 41                         callbacks=[mc, es, rlr])\r\n\r\n13 frames\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1304         use_multiprocessing=use_multiprocessing,\r\n   1305         shuffle=shuffle,\r\n-> 1306         initial_epoch=initial_epoch)\r\n   1307 \r\n   1308   @deprecation.deprecated(\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInternalError: Assigned device '/job:worker/replica:0/task:0/device:TPU:0' does not have registered OpKernel support for _Arg\r\n\t [[{{node iteratorgetnextasoptional_iterator}}]] [Op:__inference_distributed_function_277613]\r\n```\r\n\r\nShould the bug described in this topic have been corrected in TF 2.1.0? I didn't get from your discussion whether you solved issues related to `ImageDataGenerator` or not.\r\n\r\nDoes someone have a workaround for now?\r\n\r\nEDIT: I get the same `AssertionError` as the creator of this topic when I switch to TF 1.15.0.\r\nMore surprisingly, staying on TF 2.1.0-rc1 and using `model.fit(X_train, y_train, ...)` instead of ImageDataGenerators, I still have the same `InternalError` as shown above.", "Hi everyone, we have pushed a commit (https://github.com/tensorflow/tensorflow/commit/7bfb8380a7c09603259f49027374a4faf4199ad2) which place all PyFuncOp on the local host's address space (defined by job/replica/task) if ops.executing_eagerly_outside_functions() returns True. This change should be able to fix the bug for this issue.\r\n\r\nThe patch should be included in TF 2.2.0 which is scheduled to have branch cut on 2/26.", "Sorry everyone for the late notice. I tested this bug using tf-nightly pip package [[1]](https://colab.sandbox.google.com/drive/1nXOTay3p6uZlBEdbbKCl0ITbG2kZHISz) a few days ago and realized that the issue still exists.\r\n\r\nPreviously I don't know how to reproduce the bug using a modified/locally-built TF binary. I learned how to do it this time. Here is what I found:\r\n\r\n1) The functions eager_py_func() and py_func_common() which we updated in https://github.com/tensorflow/tensorflow/commit/7bfb8380a7c09603259f49027374a4faf4199ad2 are not executed in [[1]](https://colab.sandbox.google.com/drive/1nXOTay3p6uZlBEdbbKCl0ITbG2kZHISz).\r\n2) The PyFunc constructor is not called which means PyFunc is not instantiated.\r\n3) The modified log in Placer::Run() shows that placer has not placed any node whose op type is PyFunc.\r\n\r\nBecause PyFunc is not even observed by the placer, it means that we can not fix the issue by changing how to place PyFunc.\r\n\r\nThe error message \"No registered 'PyFunc' OpKernel for 'CPU' devices ...\" was thrown from [[2]](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.cc#L1588). The stacktrace which shows that the previous functions mostly come from xla::pufferfish. A lot of details (e.g. which function wants to create PyFunc) are compiled out by XLA compiler.\r\n\r\nIn order to understand which function/op to create PyFunc and why that function/op is run on a TPU worker, I run the same program but on localhost. This time the program can run successfully and the stacktrace provides the following information:\r\n\r\n1) ParallelMapIterator::EnsureThreadsStarted() [[3]](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op.cc#L492) starts threads. \r\n2) The thread then calls DatasetBaseIterator::GetNext(..) [[4]](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/dataset.cc#L479).\r\n3) Then it calls GeneratorDatasetOp::Dataset::Iterator::Initialize() [[5]](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/generator_dataset_op.cc#L110)\r\n4) Then it calls CapturedFunction::Instantiate() [[6]](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/captured_function.cc#L539) which attempts to instantiate a PyFunc op.\r\n\r\nSo it appears that a tf.data thread is schedule to run on the TPU worker. And that thread attempts to instantiate PyFunc op dynamically after the op placement is done and the program starts to read data using GetNext(). This logic seems to be in the control of tf.data.\r\n\r\nI will delegate this to @jsimsa for continued investigation.", "I'm getting similar issues when attempting to use a `TFRecordDataset` as generator for the `fit` function when running on a TPU. Both when :\r\n- using the `as_numpy_generator` function of the TFRecordDataset (to know the amount of batches in an epoch, needed for the `steps_per_epoch` argument of the `fit` function)\r\n- Mapping input tensors to their expected value\r\n    `\r\n        self.dataset = self.dataset.map(lambda x, y: (\r\n            tf.py_function(func=self.load_images_from_bytestring, inp=[x], Tout=tf.int32), \r\n            tf.py_function(func=self.tokenize_label, inp=[y], Tout=tf.int32)\r\n            ))`\r\n\r\nAny suggestions and/or knowledge of how/when this can be resolved?", "Adding support for running arbitrary Python code in TPU workloads is a non-trivial feature request that requires changes to both the Python and C++ runtime. We are actively working towards making this work.\r\n\r\nTo support ImageDataGenerator (or any other Python generator with Keras) on TPUs, the following needs to happen: the TPU program is split between 1) a TPU \"coordinator\" task which has a Python interpreter and traces the user Python code to generate a TensorFlow graph and 2) TPU \"worker\" tasks which are Python-less and execute the generated TensorFlow graph on the local TPU hardware. For generator-based programs to work, the execution of pure Python code needs to be delegated to the TPU coordinator (which is the only task in the current design that can run Python code). \r\n\r\nThank you everyone for your patience as we are working on addressing this issue. ", "We know its hard but please make it work for TF2.2\r\nwith ImageDataGenerator can make a model in few minutes :)\r\n@lindong28 @jsimsa Please ", "The good news is that after https://github.com/tensorflow/tensorflow/commit/a70a66d2193a0f7cd0a7014a5f2c55d508559c56, it should be possible to use Python generator (such as `ImageDataGenerator` or `tf.data.Dataset.from_generator`) for your input pipeline on CloudTPU. The bad news is that the changes did not make it to the TF 2.2 release but should now be available in the TF nightly.\r\n\r\nIt would be great if someone on this thread could independently verify that `ImageDataGenerator` now works with TF nightly. Thank you.", "i get error with dev20200312 .\r\nshould I wait for TPU update ?\r\n\r\n\r\n!pip install tf-nightly\r\n!pip install tf-nightly-gpu\r\nimport tensorflow as tf\r\nprint(tf.\\_\\_version\\_\\_)\r\n2.2.0-dev20200312\r\n\r\n```\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\n```\r\n\r\n```\r\n\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.64.72.66:8470\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.64.72.66:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Clearing out eager caches\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-8-f9d179e80e14> in <module>()\r\n      1 resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n      2 tf.config.experimental_connect_to_cluster(resolver)\r\n----> 3 tf.tpu.experimental.initialize_tpu_system(resolver)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: '__inference__tpu_init_fn_4' is neither a type of a primitive operation nor a name of a function registered in binary running on n-4730a25e-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n```", "@rxsang could you please take a look at the above issue (`NotFoundError: '__inference__tpu_init_fn_4' is neither a type of a primitive operation nor a name of a function registered in binary running on n-4730a25e-w-0. Make sure the operation or function is registered in the binary running in this process.`)... this does not seem to be related to `ImageDataGenerator` and looks like a generic TPU initialization error", "https://github.com/googlecolab/colabtools/issues/954#issuecomment-576800796\r\n\r\n'''\r\nColab has tensorflow 2.1.0 pre-installed. In order to use it, run the magic %tensorflow_version 2.x. When you run this (before importing tensorflow), we not only select 2.x but also do some additional work to configure the cloud TPU to use 2.x. The error you're seeing results from the TPU using 1.15 and your runtime using 2.1.0. \r\n'''", "Yeah this is a coordinator and tpu worker version mismatch error, please make sure both of your vm and tpu worker are using 2.x nightly.", "@rxsang Im using Colab what should I do ?", "Hi, you can do the following trick to install nightly 2.x in both vm and tpu worker.\r\n\r\n```\r\nimport requests\r\nimport os\r\nurl = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/2.2.0-dev20200311'\r\nresp = requests.post(url)\r\nprint(resp)\r\n%pip install tf-nightly==2.2.0-dev20200311\r\n```", "https://colab.research.google.com/drive/1AO7Lz5G1gTOAV6FOR3H2azTOZzBHZwYt\r\n\r\nI get two errors first eager error and fixed it google and another \r\nsecond is this\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-21-f495897bdf8d> in <module>()\r\n      5     epochs=EPOCHS,\r\n      6     validation_data=val_data_gen,\r\n----> 7     validation_steps=int(np.ceil(total_val / float(BATCH_SIZE)))\r\n      8 )\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    783         max_queue_size=max_queue_size,\r\n    784         workers=workers,\r\n--> 785         use_multiprocessing=use_multiprocessing)\r\n    786 \r\n    787   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    617         validation_split=validation_split,\r\n    618         shuffle=shuffle,\r\n--> 619         epochs=epochs)\r\n    620     if not dist_utils.is_distributing_by_cloning(model):\r\n    621       with model._distribution_strategy.scope():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\r\n   2165         x = ds.batch(batch_size, drop_remainder=drop_remainder)\r\n   2166       else:\r\n-> 2167         assert isinstance(x, dataset_ops.DatasetV2)\r\n   2168         training_utils.validate_dataset_input(x, y, sample_weight,\r\n   2169                                               validation_split)\r\n\r\nAssertionError: \r\n```", "@ErvisTusha I just tried it too but for me it worked when I did it like this:\r\n\r\n```\r\nimport requests\r\nimport os\r\nurl = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/2.2.0-dev20200312'\r\nresp = requests.post(url)\r\nprint(resp)\r\n%pip install tf-nightly==2.2.0-dev20200312\r\n\r\nimport tensorflow as tf\r\n```", "@mgmverburg  that code works good but i get that error when i run model.fit()", "> @mgmverburg that code works good but i get that error when i run model.fit()\r\n\r\nI think that it is a sort of unrelated issue then. Maybe it is a side effect of something else that came with the tf-nightly release? Because I am currently trying to get my model to work as well, but model fit itself is not giving errors yet for me anyway.", "@mgmverburg  can I take a look of your code sample ?", "@ErvisTusha What was the first error you received? The one that you were able to fix with the line `tf.compat.v1.disable_eager_execution()`. I was playing around with your code (the one posted in #37485) because I was getting an unusual connection issue on my own code and wanted to try on someone elses. When I removed that line, we both received the same error, and it doesn't seem to be related to eager execution at all.\r\n\r\nIt seems to be a problem with connecting to the TPU but I am not really sure. I am also using Google Colab with TF2.2.0.dev20200313. Any help would be appreciated! Note: This is the traceback I have. @ErvisTusha traceback is more complicated but ends with the exact same error.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-19-d4e3791fae7c> in <module>()\r\n----> 1 history = model.fit(x=train_data, epochs=10)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     63   def _method_wrapper(self, *args, **kwargs):\r\n     64     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 65       return method(self, *args, **kwargs)\r\n     66 \r\n     67     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    786               # TODO(b/150292341): Allow multiple async steps here.\r\n    787               if not data_handler.inferred_steps:\r\n--> 788                 context.async_wait()\r\n    789               logs = tmp_logs  # No error, now safe to assign to logs.\r\n    790               callbacks.on_train_batch_end(step, logs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in async_wait()\r\n   2201   an error state.\r\n   2202   \"\"\"\r\n-> 2203   context().sync_executors()\r\n   2204 \r\n   2205 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in sync_executors(self)\r\n    636     \"\"\"\r\n    637     if self._context_handle:\r\n--> 638       pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\n    639     else:\r\n    640       raise ValueError(\"Context is not initialized.\")\r\n\r\nUnavailableError: {{function_node __inference_train_function_25261}} failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1584133372.241287474\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3959,\"referenced_errors\":[{\"created\":\"@1584133372.241285278\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n```", "Looks like it already passed the initialization part and now it hits the error inside Keras with generator.\r\n\r\n@haoyuz @jsimsa as you are working on supporting dataset from_generator in TPU, could you help to take a relook?", "https://colab.research.google.com/drive/1AO7Lz5G1gTOAV6FOR3H2azTOZzBHZwYt#scrollTo=RNnrQZ5PzqqY&line=1&uniqifier=1\r\n\r\n\r\n\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInvalidArgumentError: From /job:worker/replica:0/task:0:\r\nNo OpKernel was registered to support Op 'PyFunc' used by {{node PyFunc}} with these attrs: [Tout=[DT_INT64], Tin=[], _xla_inferred_shapes=[<unknown>], token=\"pyfunc_12\"]\r\nRegistered devices: [CPU, TPU, TPU_SYSTEM, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[PyFunc]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n```", "@rxsang the tf.data and TensorFlow runtime changes have landed. I have tested that Keras with generator works on TPU as long as TPU worker can open GRPC connections to the TPU client.\r\n\r\nSince you are more familiar with the CloudTPU set up, could you please verify whether the following use cases work (externally):\r\n1) Kaggle on TPU\r\n2) Colab + TPU\r\n3) CloudTPU \r\n\r\nFeel free to ping me internally if you need more information. Thank you.", "Is there still progress being made on this?\r\nI have read through this and many related threads. When I run a fairly minimal example in colab (https://colab.research.google.com/drive/1ukuUbVMxdsM7Ivpx0wnYqzFUNiDEB6VO?usp=sharing) I get the following error, maybe it is related to @lindong28 's comment from 24 Januari?\r\n\r\n```\r\nInvalidArgumentError: From /job:worker/replica:0/task:0:\r\nCannot assign a device for operation PyFunc: {{node PyFunc}} was explicitly assigned to /job:localhost/replica:0/task:0 but available devices are [ /job:worker/replica:0/task:0/device:CPU:0, /job:worker/replica:0/task:0/device:TPU:0, /job:worker/replica:0/task:0/device:TPU:1, /job:worker/replica:0/task:0/device:TPU:2, /job:worker/replica:0/task:0/device:TPU:3, /job:worker/replica:0/task:0/device:TPU:4, /job:worker/replica:0/task:0/device:TPU:5, /job:worker/replica:0/task:0/device:TPU:6, /job:worker/replica:0/task:0/device:TPU:7, /job:worker/replica:0/task:0/device:TPU_SYSTEM:0, /job:worker/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[PyFunc]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n```\r\n\r\nDoes anyone have a direction for me to look or is using generators not possible at all yet until a newer version of tensorflow?", "I still have the same issue with PyFunc in collab with TPU and Tensorflow 2.2\r\n\r\nAny suggest?", "Sorry unfortunately we haven't supported running PyFunc on TPUs externally, there are some technique challenges due to Cloud TPUs cannot connect to the client. We are still discussing and figuring out a plan for it, but currently it is not supported.", "Sorry, I am not understanding if there is a workaround available, I am using flow_from_dataframe, is there anything I can do to run my Keras code with the available TPU?", "@rxsang any update to support PyFunc on TPUs possibly integrated in the upcoming 2.4?", "@michaelbanfield is looking into the Cloud TPUs connection issue.", "Hey @rxsang !! I wonder if you and @michaelbanfield managed to finally fix this issue?", "@rxsang \nDid the team was able to fix the issue because I am still getting the same relatable error.\nRefer https://stackoverflow.com/q/67751478/12210908 for more information on the error which I am getting when training a CNN model using TPU. ", "Issue is still replicating in [2.8 ](https://colab.sandbox.google.com/gist/mohantym/94a9597f1e8c9ee983bb35ad6f73b106/tpu-copy-of-cats-vs-dogs-without-data-augmentation.ipynb#scrollTo=qyHrknvL0pOu)version ."]}, {"number": 34322, "title": "Collective AllGather Fails to Collect Tensors in Multi-(TF)Task Between-Graph Distributed Execution", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary whl\r\n- TensorFlow version (use command below): `tensorflow-gpu==2.0.0`\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7.6.4, recommended with the support of `nccl`\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n\r\n**Describe the current behavior**\r\nFirst, we need the two files below.\r\n`cluster.py`:\r\n```python\r\nimport time\r\nfrom multiprocessing import Process\r\n\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.training.server_lib import Server\r\n\r\nCLUSTER_SPEC = {\r\n    \"worker\": [\r\n        \"localhost:14286\",\r\n        \"localhost:14287\"\r\n    ]\r\n}\r\n\r\nGROUP_SIZE = 4\r\n\r\n\r\ndef _configure(group_size):\r\n    gpu_options = config_pb2.GPUOptions(\r\n        visible_device_list='0,1',\r\n        per_process_gpu_memory_fraction=0.7 / group_size\r\n    )\r\n    experimental = config_pb2.ConfigProto.Experimental(collective_nccl=True)\r\n    experimental.collective_group_leader = '/job:worker/replica:0/task:0'\r\n    return config_pb2.ConfigProto(gpu_options=gpu_options, experimental=experimental)\r\n\r\n\r\nclass TFCluster:\r\n    def __init__(self, cluster_spec):\r\n        self._cluster_spec = cluster_spec\r\n        self._num_worker = len(self._cluster_spec.get(\"worker\", []))\r\n        self._tf_servers = []\r\n\r\n    def start(self):\r\n        def server(job_name: str, task_index: int):\r\n            s = Server(self._cluster_spec,\r\n                       job_name=job_name,\r\n                       task_index=task_index,\r\n                       config=_configure(GROUP_SIZE))\r\n            s.join()\r\n\r\n        assert self._num_worker >= 1\r\n        for i in range(self._num_worker):\r\n            self._tf_servers.append(Process(target=server,\r\n                                            args=(\"worker\", i), daemon=True))\r\n            # break\r\n        for proc in self._tf_servers:\r\n            proc.start()\r\n\r\n    def stop(self):\r\n        for proc in self._tf_servers:\r\n            proc.terminate()\r\n\r\n\r\nif __name__ == '__main__':\r\n    cluster = TFCluster(CLUSTER_SPEC)\r\n    cluster.start()\r\n    time.sleep(5)\r\n    input('Press Enter to Stop.')\r\n    cluster.stop()\r\n```\r\n`task.py`:\r\n```python\r\nimport argparse\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python import ops\r\nfrom tensorflow.python.client.session import Session\r\nfrom tensorflow.python.ops import collective_ops\r\n\r\nfrom cluster import CLUSTER_SPEC, GROUP_SIZE\r\n\r\nVAR = np.array([0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1])\r\nVAR_TASK_INDEX = 0\r\n\r\n\r\ndef test_collective(job_name, task_index, num_gpus):\r\n    worker_device = \"/job:%s/task:%d\" % (job_name, task_index)\r\n    master_target = \"grpc://\" + CLUSTER_SPEC[job_name][task_index]\r\n    print('> Session Target:', master_target)\r\n\r\n    with ops.Graph().as_default(), Session(target=master_target) as sess:\r\n        def run(x):\r\n            run_options = config_pb2.RunOptions()\r\n            run_options.experimental.collective_graph_key = task_index + 1\r\n            # Different positive graph key for different task to avoid racing conditions.\r\n            return sess.run(x, options=run_options)\r\n\r\n        with ops.device('/job:worker/task:%d/device:CPU:0' % VAR_TASK_INDEX):  # make sure all use the same variable\r\n            var = tf.Variable(VAR, name='W')\r\n\r\n        targets = []\r\n        collectives = []\r\n        for i in range(num_gpus):\r\n            with ops.device(worker_device + '/device:GPU:' + str(i)):\r\n                t = var + 0.2 * task_index + 0.1 * i\r\n                targets.append(t)\r\n                collectives.append(\r\n                    collective_ops.all_gather(\r\n                        t,\r\n                        group_size=GROUP_SIZE,\r\n                        group_key=1, instance_key=1\r\n                    )\r\n                    # collective_ops.all_reduce(\r\n                    #     t,\r\n                    #     group_size=GROUP_SIZE,\r\n                    #     group_key=1, instance_key=1, merge_op='Add', final_op='Div'\r\n                    # )\r\n                )\r\n\r\n        run(tf.compat.v1.global_variables_initializer())\r\n\r\n        var_value = run(var)\r\n        print('> Variable Value:', var_value)\r\n\r\n        targets_value = run(targets)\r\n        print('> Targets Value:', targets_value)\r\n\r\n        collectives_value = run(collectives)\r\n        print('> Collectives Value:', collectives_value)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n\r\n    parser.add_argument(\r\n        \"--job_name\",\r\n        type=str,\r\n        default=\"\",\r\n    )\r\n    parser.add_argument(\r\n        \"--task_index\",\r\n        type=int,\r\n        default=0,\r\n    )\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    num_gpus_per_node = 2\r\n    test_collective(job_name=FLAGS.job_name, task_index=FLAGS.task_index, num_gpus=num_gpus_per_node)\r\n```\r\n\r\nThen one could use the following commands to run the experiment:\r\n```bash\r\npython cluster.py\r\npython task.py --job_name=worker --task_index=0\r\npython task.py --job_name=worker --task_index=1\r\n```\r\n\r\n* EXPERIMENT 1 (FAILURE):\r\n  * Run the above code without change. Note that the `GROUP_SIZE=4`. It is programmed to all gather the 4 tensors across `task:0/GPU:0/` `task:0/GPU:1` `task:1/GPU:0` `task:1/GPU:1`, but gets stuck. If one turns on the env `TF_CPP_MIN_VLOG_LEVEL=1` , one will notice it successfully gathers the values but gets stuck right after it, specifically after `tensorflow/core/common_runtime/collective_rma_local.cc:105] PostToPeer ...`\r\n\r\n* EXPERIMENT 2 (SUCCESS):\r\n * Run the above code with `GROUP_SIZE=2`. On each task, It is programmed to gather 2 tensors on the 2 GPUs of that task. For example, on `task 0`, it gathers `task:0/GPU:0` and `task:0/GPU:1` and it succeeds.\r\n\r\n* EXPERIMENT 3 (SUCCESS):\r\n* Run the above code with the original `GROUP_SIZE=4` but with the `all_reduce` op (as provided in the commented code) instead of `all_gather`. It succeeds.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nIt is expected that all the experiments above should succeed. However, EXPERIMENT 1 fails, but 2 and 3 succeed. 3 means the collective executor works in a multi-task way of executing the graph on `all_reduce`. 2 means `all_gather` works if only collects tensor within one graph. 1 means the bug where `all_gather` fails to collect tensors across the tasks. \r\n\r\n**Code to reproduce the issue**\r\nSee above\r\n", "comments": ["@rmothukuru Thanks for the label. But it may be a `comp:op` bug for collectiveAllGather instead of a GPU bug.", "Hi @dubey, wondering if you might have any idea on whether the above code misuses anything or something else causes that. \ud83d\ude04 ", "wonder if there is any update on this issue?", "wonder if there is any update on this issue?", "Was able to replicate the issue in TF 2.6.0-dev20210529,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5ae77e936421ae6648a77903787705e7/untitled80.ipynb#scrollTo=hXKlZ2teJKXP)..Thanks !", "I was able to reproduce the issue in TF 2.6, please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/84045750f10af051765b7902c3a9f5ec/untitled80.ipynb#scrollTo=_J8m072tMdiO)."]}, {"number": 34219, "title": "STM32F7-Disco Hello World example fails to build", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04.3 LTS (Bionic Beaver)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 18.04.3 LTS\"\r\nVERSION_ID=\"18.04\"\r\n\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: Latest\r\n- Python version: 2.7.15+\r\n- Installed using virtualenv? pip? conda?: No, git\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: Genuine Intel(R) CPU T2400  @ 1.83GHz / 2Gb mem\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am attempting to build the Hello World example for the STM32F7-Disco from the following link but it is failing to build as see below.\r\nExample link:\r\n[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/hello_world](url)\r\n\r\n**Build Error**\r\n\r\n```\r\n:~/developement/tensor-lite-mcu/tensorflow$ make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\" \"02c64880acb89dbd57eebacfd67200d8\" tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers \r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"https://github.com/ARM-software/CMSIS_5/archive/01c7adb7685da540be9297b5a93e6640ea3333ce.zip\" \"3dec53cc74f1d5d79036952137be5d5e\" tensorflow/lite/experimental/micro/tools/make/downloads/cmsis \r\ndownloading https://github.com/ARM-software/CMSIS_5/archive/01c7adb7685da540be9297b5a93e6640ea3333ce.zip\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"https://github.com/AmbiqMicro/TFLiteMicro_CustCMSIS/archive/8f63966c5692e6a3a83956efd2e4aed77c4c9949.zip\" \"4fb327201034ee0a820b72de1e807d27\" tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext \r\ndownloading https://github.com/AmbiqMicro/TFLiteMicro_CustCMSIS/archive/8f63966c5692e6a3a83956efd2e4aed77c4c9949.zip\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"https://github.com/mborgerding/kissfft/archive/v130.zip\" \"438ba1fef5783cc5f5f201395cc477ca\" tensorflow/lite/experimental/micro/tools/make/downloads/kissfft patch_kissfft\r\ndownloading https://github.com/mborgerding/kissfft/archive/v130.zip\r\nFinished patching kissfft\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_07.zip\" \"e6430de25aa92bcb807d07278a1b5b90\" tensorflow/lite/experimental/micro/tools/make/downloads/person_model_grayscale \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_07.zip\r\nmake: *** No rule to make target 'tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/third_party/gemmlowp/fixedpoint/fixedpoint.h', needed by 'generate_hello_world_mbed_project'.  Stop.\r\n```\r\n\r\n", "comments": ["I tried clone:ing a fresh copy of tensorflow and ran \r\n\r\n`\r\n$ make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\r\n`\r\n\r\nand it ran without an issue.\r\n\r\nAlthough I think I remember seeing this error message myself once or twice before. Try running (ie. remove built files and downloaded data)\r\n` make -f tensorflow/lite/experimental/micro/tools/make/Makefile clean`\r\nand\r\n`rm -rf tensorflow/lite/experimental/micro/tools/make/downloads/*`\r\n\r\nand run the command again. I think that solved it for me when I saw the error.", "Okay, thanks.   Yeah, I did a 'git pull' and noticed there were a number of changed that were pulled down so I removed the tensorflow folder and re cloned the repo.  This time the build completes without issue however I am having an issue getting the mbed portion to complete the python dependency build.  I've created a virtualenv for python 2.7.15 (although it might be best to update this to python3 eventually) since there were conflicts with other installs. However, I am still seeing a failure with the cmsis_pack_manager.\r\n\r\n1. After the build, I cd into the mbed folder\r\n`(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow$ cd tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed`\r\n\r\n2. Set the default root for mbed:\r\n\r\n`(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed$ mbed config root .\r\n[mbed] . now set as default root in program \"tensorflow\"`\r\n\r\n3. I run mbed deploy which results in an error:\r\nNOTE This results in a number of Warnings regarding file extensions and the mbed build tools; \r\n\r\n```\r\n(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed$ mbed deploy\r\n[mbed] Working path \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed\" (library)\r\n[mbed] Program path \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow\"\r\n[mbed] WARNING: File \"RTX_CM0_B.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS/RTX/LIB/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n\r\n[mbed] Updating library \"\" to rev #679d24833acf (tags: latest, mbed-os-5.14.1)\r\n[mbed] Adding library \"LCD_DISCO_F746NG\" from \"https://os.mbed.com/teams/ST/code/LCD_DISCO_F746NG\" at rev #d44525b1de98\r\n[mbed] Adding library \"BSP_DISCO_F746NG\" from \"https://os.mbed.com/teams/ST/code/BSP_DISCO_F746NG\" at rev #df2ea349c37a\r\n[mbed] Adding library \"SDRAM_DISCO_F746NG\" from \"https://os.mbed.com/teams/ST/code/SDRAM_DISCO_F746NG\" at rev #370f402a2219\r\n[mbed] Adding library \"AUDIO_DISCO_F746NG\" from \"https://os.mbed.com/teams/ST/code/AUDIO_DISCO_F746NG\" at rev #7046ce26b7ed\r\n[mbed] Adding library \"mbed-os\" from \"https://github.com/ARMmbed/mbed-os\" at rev #8ef742a49c16\r\n[mbed] Auto-installing missing Python modules (pyyaml, cmsis_pack_manager, psutil, click)...\r\n[mbed] ERROR: Missing Python modules were not auto-installed.\r\n       The Mbed OS tools in this program require the following Python modules: pyyaml, cmsis_pack_manager, psutil, click\r\n       You can install all missing modules by running \"pip install -r requirements.txt\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/mbed-os\"\r\n       On Posix systems (Linux, etc) you might have to switch to superuser account or use \"sudo\"\r\n---\r\n\r\n```\r\n\r\n4. I follow the instructions and run pip install manually but this fails for cmsis_pack_manager every time:\r\n```\r\n(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed$ cd /home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/mbed-os\r\n(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow/tensorflow/mbed-os$ pip install -r requirements.txt\r\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\r\nIgnoring hidapi: markers 'platform_system != \"Linux\"' don't match your environment\r\nIgnoring pywin32: markers 'platform_system == \"Windows\"' don't match your environment\r\nIgnoring wmi: markers 'platform_system == \"Windows\"' don't match your environment\r\nRequirement already satisfied: colorama==0.3.9 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 1)) (0.3.9)\r\nRequirement already satisfied: urllib3[secure]==1.24.2 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 2)) (1.24.2)\r\nRequirement already satisfied: prettytable==0.7.2 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 3)) (0.7.2)\r\nRequirement already satisfied: junit-xml==1.8 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 4)) (1.8)\r\nRequirement already satisfied: pyyaml==4.2b1 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 5)) (4.2b1)\r\nRequirement already satisfied: jsonschema==2.6.0 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 6)) (2.6.0)\r\nRequirement already satisfied: future==0.16.0 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 7)) (0.16.0)\r\nRequirement already satisfied: six==1.12.0 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 8)) (1.12.0)\r\nRequirement already satisfied: mbed-cloud-sdk<2.1,>=2.0.6 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 9)) (2.0.8)\r\nRequirement already satisfied: requests<2.21,>=2.20 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 10)) (2.20.1)\r\nRequirement already satisfied: idna<2.8,>=2 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 11)) (2.7)\r\nRequirement already satisfied: pyserial<=3.4,>=3 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 12)) (3.4)\r\nRequirement already satisfied: Jinja2<2.11,>=2.10.1 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 13)) (2.10.3)\r\nRequirement already satisfied: intelhex<=2.2.1,>=1.3 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 14)) (2.2.1)\r\nRequirement already satisfied: mbed-ls<1.8,>=1.5.1 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 15)) (1.7.9)\r\nRequirement already satisfied: mbed-host-tests<1.6,>=1.4.4 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 16)) (1.5.9)\r\nRequirement already satisfied: mbed-greentea<1.8,>=0.2.24 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 17)) (1.7.3)\r\nRequirement already satisfied: beautifulsoup4<=4.6.3,>=4 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 18)) (4.6.3)\r\nRequirement already satisfied: pyelftools<=0.25,>=0.24 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 19)) (0.25)\r\nRequirement already satisfied: manifest-tool==1.5.2 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 20)) (1.5.2)\r\nRequirement already satisfied: icetea<1.3,>=1.2.1 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 21)) (1.2.4)\r\nRequirement already satisfied: pycryptodome<=3.7.3,>=3.7.2 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 22)) (3.7.3)\r\nRequirement already satisfied: pyusb<2.0.0,>=1.0.0 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 23)) (1.0.2)\r\nCollecting cmsis-pack-manager<0.3.0,>=0.2.3\r\n  Using cached https://files.pythonhosted.org/packages/09/55/b972564a34d99fe9b2b828bb25899ced78e67280049fdaf5bb1f4ab62345/cmsis-pack-manager-0.2.9.tar.gz\r\nProcessing /home/jomomate/.cache/pip/wheels/17/08/ec/22b464874958c3fc91e1a75748fae2220eb704a8b1035f9a03/psutil-5.6.2-cp27-cp27mu-linux_i686.whl\r\nRequirement already satisfied: cryptography<2.5,>=2.4.x in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from -r requirements.txt (line 29)) (2.4.2)\r\nCollecting Click<7.1,>=7.0\r\n  Using cached https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl\r\nRequirement already satisfied: pyOpenSSL>=0.14; python_version == \"2.7\" and extra == \"secure\" in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from urllib3[secure]==1.24.2->-r requirements.txt (line 2)) (19.0.0)\r\nRequirement already satisfied: ipaddress; python_version == \"2.7\" and extra == \"secure\" in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from urllib3[secure]==1.24.2->-r requirements.txt (line 2)) (1.0.23)\r\nRequirement already satisfied: certifi; extra == \"secure\" in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from urllib3[secure]==1.24.2->-r requirements.txt (line 2)) (2019.9.11)\r\nRequirement already satisfied: functools32; python_version == \"2.7\" in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from jsonschema==2.6.0->-r requirements.txt (line 6)) (3.2.3.post2)\r\nRequirement already satisfied: python-dateutil>=2 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from mbed-cloud-sdk<2.1,>=2.0.6->-r requirements.txt (line 9)) (2.8.1)\r\nRequirement already satisfied: python-dotenv>=0.8.2 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from mbed-cloud-sdk<2.1,>=2.0.6->-r requirements.txt (line 9)) (0.10.3)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from requests<2.21,>=2.20->-r requirements.txt (line 10)) (3.0.4)\r\nRequirement already satisfied: MarkupSafe>=0.23 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from Jinja2<2.11,>=2.10.1->-r requirements.txt (line 13)) (1.1.1)\r\nRequirement already satisfied: mbed-os-tools<0.1.0,>=0.0.9 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from mbed-ls<1.8,>=1.5.1->-r requirements.txt (line 15)) (0.0.10)\r\nRequirement already satisfied: asn1ate>=0.5 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from manifest-tool==1.5.2->-r requirements.txt (line 20)) (0.6.0)\r\nRequirement already satisfied: pyasn1<0.3.0,>=0.2.1 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from manifest-tool==1.5.2->-r requirements.txt (line 20)) (0.2.3)\r\nRequirement already satisfied: ecdsa>=0.13 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from manifest-tool==1.5.2->-r requirements.txt (line 20)) (0.14.1)\r\nRequirement already satisfied: protobuf<3.6.0,>=3.5.0 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from manifest-tool==1.5.2->-r requirements.txt (line 20)) (3.5.2.post1)\r\nRequirement already satisfied: pyparsing>=2.1.0 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from manifest-tool==1.5.2->-r requirements.txt (line 20)) (2.4.5)\r\nRequirement already satisfied: jsonmerge>=1.4 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from icetea<1.3,>=1.2.1->-r requirements.txt (line 21)) (1.7.0)\r\nRequirement already satisfied: yattag>=1.0 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from icetea<1.3,>=1.2.1->-r requirements.txt (line 21)) (1.12.2)\r\nRequirement already satisfied: mbed-flasher<0.11,>=0.10.1 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from icetea<1.3,>=1.2.1->-r requirements.txt (line 21)) (0.10.1)\r\nRequirement already satisfied: semver>=2.0 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from icetea<1.3,>=1.2.1->-r requirements.txt (line 21)) (2.9.0)\r\nRequirement already satisfied: appdirs>=1.4 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from cmsis-pack-manager<0.3.0,>=0.2.3->-r requirements.txt (line 25)) (1.4.3)\r\nRequirement already satisfied: milksnake>=0.1.2 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from cmsis-pack-manager<0.3.0,>=0.2.3->-r requirements.txt (line 25)) (0.1.5)\r\nRequirement already satisfied: asn1crypto>=0.21.0 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from cryptography<2.5,>=2.4.x->-r requirements.txt (line 29)) (1.2.0)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.7 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from cryptography<2.5,>=2.4.x->-r requirements.txt (line 29)) (1.13.2)\r\nRequirement already satisfied: enum34; python_version < \"3\" in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from cryptography<2.5,>=2.4.x->-r requirements.txt (line 29)) (1.1.6)\r\nRequirement already satisfied: typing; python_version < \"3.5\" in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from python-dotenv>=0.8.2->mbed-cloud-sdk<2.1,>=2.0.6->-r requirements.txt (line 9)) (3.7.4.1)\r\nRequirement already satisfied: lockfile in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from mbed-os-tools<0.1.0,>=0.0.9->mbed-ls<1.8,>=1.5.1->-r requirements.txt (line 15)) (0.12.2)\r\nRequirement already satisfied: fasteners in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from mbed-os-tools<0.1.0,>=0.0.9->mbed-ls<1.8,>=1.5.1->-r requirements.txt (line 15)) (0.15)\r\nRequirement already satisfied: setuptools in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from protobuf<3.6.0,>=3.5.0->manifest-tool==1.5.2->-r requirements.txt (line 20)) (41.6.0)\r\nRequirement already satisfied: pycparser in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from cffi!=1.11.3,>=1.7->cryptography<2.5,>=2.4.x->-r requirements.txt (line 29)) (2.19)\r\nRequirement already satisfied: monotonic>=0.1 in /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages (from fasteners->mbed-os-tools<0.1.0,>=0.0.9->mbed-ls<1.8,>=1.5.1->-r requirements.txt (line 15)) (1.5)\r\nBuilding wheels for collected packages: cmsis-pack-manager\r\n  Building wheel for cmsis-pack-manager (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /home/jomomate/developement/tensor-lite-mcu/env2/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-A4cYAQ/cmsis-pack-manager/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-A4cYAQ/cmsis-pack-manager/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-DKh6vn --python-tag cp27\r\n       cwd: /tmp/pip-install-A4cYAQ/cmsis-pack-manager/\r\n  Complete output (9 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build/lib.linux-i686-2.7\r\n  creating build/lib.linux-i686-2.7/cmsis_pack_manager\r\n  copying cmsis_pack_manager/pack_manager.py -> build/lib.linux-i686-2.7/cmsis_pack_manager\r\n  copying cmsis_pack_manager/_version.py -> build/lib.linux-i686-2.7/cmsis_pack_manager\r\n  copying cmsis_pack_manager/__init__.py -> build/lib.linux-i686-2.7/cmsis_pack_manager\r\n  error: [Errno 2] No such file or directory\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for cmsis-pack-manager\r\n  Running setup.py clean for cmsis-pack-manager\r\nFailed to build cmsis-pack-manager\r\nInstalling collected packages: cmsis-pack-manager, psutil, Click\r\n    Running setup.py install for cmsis-pack-manager ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /home/jomomate/developement/tensor-lite-mcu/env2/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-A4cYAQ/cmsis-pack-manager/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-A4cYAQ/cmsis-pack-manager/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-aJa4BX/install-record.txt --single-version-externally-managed --compile --install-headers /home/jomomate/developement/tensor-lite-mcu/env2/include/site/python2.7/cmsis-pack-manager\r\n         cwd: /tmp/pip-install-A4cYAQ/cmsis-pack-manager/\r\n    Complete output (9 lines):\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build/lib.linux-i686-2.7\r\n    creating build/lib.linux-i686-2.7/cmsis_pack_manager\r\n    copying cmsis_pack_manager/pack_manager.py -> build/lib.linux-i686-2.7/cmsis_pack_manager\r\n    copying cmsis_pack_manager/_version.py -> build/lib.linux-i686-2.7/cmsis_pack_manager\r\n    copying cmsis_pack_manager/__init__.py -> build/lib.linux-i686-2.7/cmsis_pack_manager\r\n    error: [Errno 2] No such file or directory\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: /home/jomomate/developement/tensor-lite-mcu/env2/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-A4cYAQ/cmsis-pack-manager/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-A4cYAQ/cmsis-pack-manager/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-aJa4BX/install-record.txt --single-version-externally-managed --compile --install-headers /home/jomomate/developement/tensor-lite-mcu/env2/include/site/python2.7/cmsis-pack-manager Check the logs for full command output.\r\n```\r\n\r\nI'm not sure how to get pass this.\r\n\r\nThanks,\r\n\r\nJon\r\n", "I was able to get pass the pip install requirements step by manually installing cmsis_package_manager but it still fails 'mbed deploy'.\r\n\r\nThis is where I got the info to install  cmsis_pack_manager:\r\n[https://github.com/ARMmbed/cmsis-pack-manager](url)\r\n\r\nThis is the error I see now:\r\n\r\n```\r\n(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow/tensorflow$ mbed deploy\r\n[mbed] Working path \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow\" (library)\r\n[mbed] Program path \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow\"\r\n[mbed] WARNING: File \"RTX_CM0_B.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS/RTX/LIB/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM3_IFX.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS/RTX/LIB/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM0.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS/RTX/LIB/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM4_IFX.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS/RTX/LIB/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM3.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS/RTX/LIB/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM4.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS/RTX/LIB/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM3_B.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS/RTX/LIB/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM4_B.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS/RTX/LIB/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM0.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS2/RTX/Library/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM3.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS2/RTX/Library/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_V8MMN.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS2/RTX/Library/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_V8MBN.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS2/RTX/Library/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_V8MB.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS2/RTX/Library/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_CM4F.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS2/RTX/Library/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_V8MM.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS2/RTX/Library/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_V8MMFN.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS2/RTX/Library/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"RTX_V8MMF.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/RTOS2/RTX/Library/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_ARMv8MBLl_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM4lf_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM7l_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_ARMv8MMLldfsp_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM0l_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_ARMv8MMLlfsp_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM4b_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM3b_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM4bf_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM7bfdp_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_ARMv8MMLld_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM3l_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM4l_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_ARMv8MMLl_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM0b_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM7b_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM7bfsp_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM7lfdp_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] WARNING: File \"arm_cortexM7lfsp_math.lib\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Lib/ARM\" uses a non-standard .lib file extension, which is not compatible with the mbed build tools.\r\n---\r\n[mbed] Updating library \"mbed-os\" to rev #679d24833acf (tags: latest, mbed-os-5.14.1)\r\n[mbed] Updating library \"lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/LCD_DISCO_F746NG\" to rev #d44525b1de98 (tag: tip)\r\n[mbed] Updating library \"lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/BSP_DISCO_F746NG\" to rev #df2ea349c37a (tag: tip)\r\n[mbed] Updating library \"lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/SDRAM_DISCO_F746NG\" to rev #370f402a2219 (tag: tip)\r\n[mbed] Updating library \"lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/AUDIO_DISCO_F746NG\" to rev #7046ce26b7ed (tag: tip)\r\n[mbed] Updating library \"lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os\" to rev #8ef742a49c16\r\n[mbed] Auto-installing missing Python modules (pyyaml, cmsis_pack_manager, psutil, click)...\r\n[mbed] ERROR: Missing Python modules were not auto-installed.\r\n       The Mbed OS tools in this program require the following Python modules: pyyaml, cmsis_pack_manager, psutil, click\r\n       You can install all missing modules by running \"pip install -r requirements.txt\" in \"/home/jomomate/developement/tensor-lite-mcu/tensorflow/tensorflow/mbed-os\"\r\n       On Posix systems (Linux, etc) you might have to switch to superuser account or use \"sudo\"\r\n---\r\n\r\n```\r\nAll of the modules that it is complaining about are installed:\r\n```\r\n(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow/tensorflow$ ls -d /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages/yaml/\r\n/home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages/yaml/\r\n(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow/tensorflow$ ls -d /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages/cmsis_pack_manager\r\n/home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages/cmsis_pack_manager\r\n(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow/tensorflow$ ls -d /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages/psutil\r\n/home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages/psutil\r\n(env2) jomomate@jmmate:~/developement/tensor-lite-mcu/tensorflow/tensorflow$ ls -d /home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages/click/\r\n/home/jomomate/developement/tensor-lite-mcu/env2/lib/python2.7/site-packages/click/\r\n```\r\n\r\n", "Just for S&Gs, here is the pip list from my config:\r\n\r\n```\r\n$ pip list\r\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\r\nPackage            Version    \r\n------------------ -----------\r\nappdirs            1.4.3      \r\nasn1ate            0.6.0      \r\nasn1crypto         1.2.0      \r\nbeautifulsoup4     4.6.3      \r\ncargo              0.1.1      \r\ncertifi            2019.9.11  \r\ncffi               1.13.2     \r\nchardet            3.0.4      \r\nClick              7.0        \r\ncmsis-pack-manager 0.2.9      \r\ncolorama           0.3.9      \r\ncryptography       2.4.2      \r\necdsa              0.14.1     \r\nenum34             1.1.6      \r\nfasteners          0.15       \r\nfunctools32        3.2.3.post2\r\nfuture             0.16.0     \r\nicetea             1.2.4      \r\nidna               2.7        \r\nintelhex           2.2.1      \r\nipaddress          1.0.23     \r\nJinja2             2.10.3     \r\njsonmerge          1.7.0      \r\njsonschema         2.6.0      \r\njunit-xml          1.8        \r\nlockfile           0.12.2     \r\nmanifest-tool      1.5.2      \r\nMarkupSafe         1.1.1      \r\nmbed-cloud-sdk     2.0.8      \r\nmbed-flasher       0.10.1     \r\nmbed-greentea      1.7.3      \r\nmbed-host-tests    1.5.9      \r\nmbed-ls            1.7.9      \r\nmbed-os-tools      0.0.10     \r\nmilksnake          0.1.5      \r\nmonotonic          1.5        \r\npip                19.3.1     \r\nprettytable        0.7.2      \r\nprotobuf           3.5.2.post1\r\npsutil             5.6.2      \r\npyasn1             0.2.3      \r\npycparser          2.19       \r\npycryptodome       3.7.3      \r\npyelftools         0.25       \r\npyOpenSSL          19.0.0     \r\npyparsing          2.4.5      \r\npyserial           3.4        \r\npytest-runner      5.2        \r\npython-dateutil    2.8.1      \r\npython-dotenv      0.10.3     \r\npyusb              1.0.2      \r\nPyYAML             4.2b1      \r\nrequests           2.20.1     \r\nsemver             2.9.0      \r\nsetuptools         41.6.0     \r\nsix                1.12.0     \r\ntyping             3.7.4.1    \r\nurllib3            1.24.2     \r\nwheel              0.33.6     \r\nyattag             1.12.2\r\n``` \r\n", "Okay, a bit closer. I moved to a Ubuntu 18.04 64-bit VM and no virtual-env and was able to get past the mbed deploy, but it now fails during the compile.\r\nNOTE: This it is required to set the GCC_ARM_PATH before running the compile step:\r\nEx:\r\n`$ mbed config -G GCC_ARM_PATH /home/jomodev/development/stm/SoftwareToolchain/gcc-arm-none-eabi-5_4-2016q3-20160926-linux-d/gcc-arm-none-eabi-5_4-2016q3/bin/`\r\n\r\nThis is the error I seen before making a change to :\r\n```\r\n$ mbed compile -v -m DISCO_F746NG -t GCC_ARM\r\n\r\n\r\nCompile [ 97.3%]: svdf.cc\r\nCompile: /home/jomodev/development/stm/SoftwareToolchain/gcc-arm-none-eabi-5_4-2016q3-20160926-linux-d/gcc-arm-none-eabi-5_4-2016q3/bin/arm-none-eabi-g++ -std=gnu++14 -fno-rtti -Wvla -c -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -fmessage-length=0 -fno-exceptions -ffunction-sections -fdata-sections -funsigned-char -MMD -fno-delete-null-pointer-checks -fomit-frame-pointer -Os -g -DMBED_TRAP_ERRORS_ENABLED=1 -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -DMBED_ROM_START=0x8000000 -DMBED_ROM_SIZE=0x100000 -DMBED_ROM1_START=0x200000 -DMBED_ROM1_SIZE=0x100000 -DMBED_RAM_START=0x20010000 -DMBED_RAM_SIZE=0x40000 -DMBED_RAM1_START=0x20000000 -DMBED_RAM1_SIZE=0x10000 -DDEVICE_SPI=1 -DTARGET_STM32F746xG -DTARGET_STM32F746NG -DDEVICE_EMAC=1 -D__MBED__=1 -DDEVICE_USBDEVICE=1 -D__FPU_PRESENT=1 -DDEVICE_PORTOUT=1 -DUSBHOST_OTHER -DDEVICE_PORTINOUT=1 -D__MBED_CMSIS_RTOS_CM -DTARGET_DISCO_F746NG -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F7 -DDEVICE_I2C_ASYNCH=1 -DDEVICE_SERIAL_ASYNCH=1 -D__CMSIS_RTOS -DTOOLCHAIN_GCC -DDEVICE_CRC=1 -DARM_MATH_CM7 -DTARGET_CORTEX_M -DTARGET_LIKE_CORTEX_M7 -DDEVICE_RTC=1 -DDEVICE_ANALOGOUT=1 -DTARGET_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DEXTRA_IDLE_STACK_REQUIRED -DDEVICE_LPTICKER=1 -DDEVICE_PWMOUT=1 -DDEVICE_SPI_ASYNCH=1 -DMBED_TICKLESS -DUSE_FULL_LL_DRIVER -DCOMPONENT_QSPIF=1 -DTARGET_CORTEX -DDEVICE_I2C=1 -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_I2CSLAVE=1 -DDEVICE_STDIO_MESSAGES=1 -D__CORTEX_M7 -DTARGET_STM32F746 -DTARGET_FAMILY_STM32 -DUSE_HAL_DRIVER -DTARGET_FF_ARDUINO -DDEVICE_PORTIN=1 -DTARGET_RELEASE -DTARGET_STM -DTARGET_NAME=DISCO_F746NG -DDEVICE_SERIAL_FC=1 -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_USTICKER=1 -DDEVICE_WATCHDOG=1 -DDEVICE_TRNG=1 -DTARGET_LIKE_MBED -DMBED_BUILD_TIMESTAMP=1573882712.15 -DTARGET_RTOS_M4_M7 -DDEVICE_SLEEP=1 -DTOOLCHAIN_GCC_ARM -DDEVICE_RESET_REASON=1 -DDEVICE_CAN=1 -DUSB_STM_HAL -DCOMPONENT_NSPE=1 -DDEVICE_INTERRUPTIN=1 -DDEVICE_SPISLAVE=1 -DDEVICE_ANALOGIN=1 -DDEVICE_SERIAL=1 -DDEVICE_FLASH=1 -DDEVICE_QSPI=1 -DTARGET_STM_EMAC -DDEVICE_MPU=1 @./BUILD/DISCO_F746NG/GCC_ARM/.includes_6e23b25a67015532b3e2b96cc60c3bf5.txt -include ./BUILD/DISCO_F746NG/GCC_ARM/mbed_config.h -MD -MF BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/experimental/micro/kernels/svdf.d -o BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/experimental/micro/kernels/svdf.o ./tensorflow/lite/experimental/micro/kernels/svdf.cc\r\n[Error] activation_utils.h@43,23: 'signbit' was not declared in this scope\r\n[DEBUG] Return: 1\r\n[DEBUG] Output: In file included from ./tensorflow/lite/experimental/micro/kernels/svdf.cc:20:0:\r\n[DEBUG] Output: ./tensorflow/lite/experimental/micro/kernels/activation_utils.h: In function 'float tflite::ops::micro::ActivationValFloat(TfLiteFusedActivation, float)':\r\n[DEBUG] Output: ./tensorflow/lite/experimental/micro/kernels/activation_utils.h:43:23: error: 'signbit' was not declared in this scope\r\n[DEBUG] Output:        return signbit(a);\r\n[DEBUG] Output:                        ^\r\n[DEBUG] Output: ./tensorflow/lite/experimental/micro/kernels/activation_utils.h:43:23: note: suggested alternative:\r\n[DEBUG] Output: In file included from ./tensorflow/lite/experimental/micro/kernels/activation_utils.h:20:0,\r\n[DEBUG] Output:                  from ./tensorflow/lite/experimental/micro/kernels/svdf.cc:20:\r\n[DEBUG] Output: /home/jomodev/development/stm/SoftwareToolchain/gcc-arm-none-eabi-5_4-2016q3-20160926-linux-d/gcc-arm-none-eabi-5_4-2016q3/arm-none-eabi/include/c++/5.4.1/cmath:682:5: note:   'std::signbit'\r\n[DEBUG] Output:      signbit(_Tp __x)\r\n[DEBUG] Output:      ^\r\nTraceback (most recent call last):\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py\", line 78, in wrapped_build_project\r\n    *args, **kwargs\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/build_api.py\", line 595, in build_project\r\n    objects = toolchain.compile_sources(resources, sorted(resources.get_file_paths(FileType.INC_DIR)))\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 414, in compile_sources\r\n    return self._compile_sources(resources, inc_dirs=inc_dirs)\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 493, in _compile_sources\r\n    return self.compile_seq(queue, objects)\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 510, in compile_seq\r\n    res['command']\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 682, in compile_output\r\n    raise ToolException(stderr)\r\nToolException: In file included from ./tensorflow/lite/experimental/micro/kernels/svdf.cc:20:0:\r\n./tensorflow/lite/experimental/micro/kernels/activation_utils.h: In function 'float tflite::ops::micro::ActivationValFloat(TfLiteFusedActivation, float)':\r\n./tensorflow/lite/experimental/micro/kernels/activation_utils.h:43:23: error: 'signbit' was not declared in this scope\r\n       return signbit(a);\r\n                       ^\r\n./tensorflow/lite/experimental/micro/kernels/activation_utils.h:43:23: note: suggested alternative:\r\nIn file included from ./tensorflow/lite/experimental/micro/kernels/activation_utils.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/kernels/svdf.cc:20:\r\n/home/jomodev/development/stm/SoftwareToolchain/gcc-arm-none-eabi-5_4-2016q3-20160926-linux-d/gcc-arm-none-eabi-5_4-2016q3/arm-none-eabi/include/c++/5.4.1/cmath:682:5: note:   'std::signbit'\r\n     signbit(_Tp __x)\r\n     ^\r\n\r\n[mbed] ERROR: \"/usr/bin/python\" returned error.\r\n       Code: 1\r\n       Path: \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed\"\r\n       Command: \"/usr/bin/python -u /home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM -v\"\r\n       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n---\r\n```\r\n\r\n\r\nI was able to get passed this by editing the activation_utils.h file as follows:\r\n**$ view ./tensorflow/lite/experimental/micro/kernels/activation_utils.h\r\njomodev@jomoudev:~/development/stm**\r\n```\r\n     42     case kTfLiteActSignBit:\r\n     43       return std::signbit(a)\r\n```\r\nNow I see this:\r\n```\r\nCompile [ 98.8%]: arm_mult_q15.c\r\nCompile: /home/jomodev/development/stm/SoftwareToolchain/gcc-arm-none-eabi-5_4-2016q3-20160926-linux-d/gcc-arm-none-eabi-5_4-2016q3/bin/arm-none-eabi-gcc -std=gnu11 -c -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -fmessage-length=0 -fno-exceptions -ffunction-sections -fdata-sections -funsigned-char -MMD -fno-delete-null-pointer-checks -fomit-frame-pointer -Os -g -DMBED_TRAP_ERRORS_ENABLED=1 -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -DMBED_ROM_START=0x8000000 -DMBED_ROM_SIZE=0x100000 -DMBED_ROM1_START=0x200000 -DMBED_ROM1_SIZE=0x100000 -DMBED_RAM_START=0x20010000 -DMBED_RAM_SIZE=0x40000 -DMBED_RAM1_START=0x20000000 -DMBED_RAM1_SIZE=0x10000 -DDEVICE_SPI=1 -DTARGET_STM32F746xG -DTARGET_STM32F746NG -DDEVICE_EMAC=1 -D__MBED__=1 -DDEVICE_USBDEVICE=1 -D__FPU_PRESENT=1 -DDEVICE_PORTOUT=1 -DMBED_BUILD_TIMESTAMP=1573888115.9 -DDEVICE_PORTINOUT=1 -D__MBED_CMSIS_RTOS_CM -DTARGET_DISCO_F746NG -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F7 -DDEVICE_I2C_ASYNCH=1 -DDEVICE_SERIAL_ASYNCH=1 -D__CMSIS_RTOS -DTOOLCHAIN_GCC -DDEVICE_CRC=1 -DARM_MATH_CM7 -DTARGET_CORTEX_M -DDEVICE_MPU=1 -DTARGET_LIKE_CORTEX_M7 -DDEVICE_RTC=1 -DDEVICE_ANALOGOUT=1 -DTARGET_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DEXTRA_IDLE_STACK_REQUIRED -DDEVICE_LPTICKER=1 -DDEVICE_PWMOUT=1 -DDEVICE_SPI_ASYNCH=1 -DMBED_TICKLESS -DUSE_FULL_LL_DRIVER -DCOMPONENT_QSPIF=1 -DTARGET_CORTEX -DDEVICE_I2C=1 -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_I2CSLAVE=1 -DDEVICE_STDIO_MESSAGES=1 -D__CORTEX_M7 -DTARGET_STM32F746 -DTARGET_FAMILY_STM32 -DUSE_HAL_DRIVER -DTARGET_FF_ARDUINO -DDEVICE_PORTIN=1 -DTARGET_RELEASE -DTARGET_STM -DTARGET_NAME=DISCO_F746NG -DUSBHOST_OTHER -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_USTICKER=1 -DDEVICE_WATCHDOG=1 -DDEVICE_TRNG=1 -DTARGET_LIKE_MBED -DTARGET_RTOS_M4_M7 -DDEVICE_SLEEP=1 -DTOOLCHAIN_GCC_ARM -DDEVICE_RESET_REASON=1 -DDEVICE_CAN=1 -DUSB_STM_HAL -DCOMPONENT_NSPE=1 -DDEVICE_INTERRUPTIN=1 -DDEVICE_SPISLAVE=1 -DDEVICE_ANALOGIN=1 -DDEVICE_SERIAL=1 -DDEVICE_FLASH=1 -DDEVICE_QSPI=1 -DTARGET_STM_EMAC -DDEVICE_SERIAL_FC=1 @./BUILD/DISCO_F746NG/GCC_ARM/.includes_6e23b25a67015532b3e2b96cc60c3bf5.txt -include ./BUILD/DISCO_F746NG/GCC_ARM/mbed_config.h -MD -MF BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.d -o BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.o ./tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c\r\n[Error] arm_mult_q15.c@101,6: conflicting types for 'arm_mult_q15'\r\n[DEBUG] Return: 1\r\n[DEBUG] Output: ./tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:101:6: error: conflicting types for 'arm_mult_q15'\r\n[DEBUG] Output:  void arm_mult_q15(\r\n[DEBUG] Output:       ^\r\n[DEBUG] Output: In file included from ./tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:29:0:\r\n[DEBUG] Output: ./mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h:1924:8: note: previous declaration of 'arm_mult_q15' was here\r\n[DEBUG] Output:    void arm_mult_q15(\r\n[DEBUG] Output:         ^\r\nTraceback (most recent call last):\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py\", line 78, in wrapped_build_project\r\n    *args, **kwargs\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/build_api.py\", line 595, in build_project\r\n    objects = toolchain.compile_sources(resources, sorted(resources.get_file_paths(FileType.INC_DIR)))\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 414, in compile_sources\r\n    return self._compile_sources(resources, inc_dirs=inc_dirs)\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 493, in _compile_sources\r\n    return self.compile_seq(queue, objects)\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 510, in compile_seq\r\n    res['command']\r\n  File \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 682, in compile_output\r\n    raise ToolException(stderr)\r\nToolException: ./tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:101:6: error: conflicting types for 'arm_mult_q15'\r\n void arm_mult_q15(\r\n      ^\r\nIn file included from ./tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:29:0:\r\n./mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h:1924:8: note: previous declaration of 'arm_mult_q15' was here\r\n   void arm_mult_q15(\r\n        ^\r\n\r\n[mbed] ERROR: \"/usr/bin/python\" returned error.\r\n       Code: 1\r\n       Path: \"/home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed\"\r\n       Command: \"/usr/bin/python -u /home/jomodev/development/stm/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM -v\"\r\n       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n---\r\n\r\n```", "I found the fix for the 'arm_mult_q15' in the following issue:\r\n[https://github.com/tensorflow/tensorflow/issues/32037](url)\r\n\r\nThis requires editing the methods in the \"./mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h\" file.\r\n\r\nAlso, after performing a complete update/upgrade and dist-upgrade, I was able to compile the example in that config as well. That is after making the changes in the \"arm_math.h\" file. ", "@jomoengineer thanks for sharing your solution!!! :heart_eyes: :+1: ", "Is there another way to solve this? I don't want to jump through all these hoops just to compile a sample project", "@Machine-Hum the only way I was able to get it to run was applying ***both*** hacks. Let me know if you find a different workaround!! :sweat_smile: ", "Using the build instructions here https://www.tensorflow.org/lite/microcontrollers/library seem to work fine, are these the most recent build utils or is it the readme in the repo :s", "The issue is related to the Deploy to STM32F746 from the Hello World example.\r\n[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/hello_world](url)", "I just tried release Release 2.1.0 and it still has the issue with the activation_utils.h file where it is needed to change line 43 to:\r\n\r\n` return std::signbit(a)`\r\n\r\nAlso, there were a number of 'printf' errors that required adding \"#include \\<cstdio\\>\" to:\r\n  **_./tensorflow/lite/experimental/micro/micro_optional_debug_tools.cc_**\r\n\r\nErrors seen:\r\n```\r\n./tensorflow/lite/experimental/micro/micro_optional_debug_tools.cc: In function 'void tflite::PrintInterpreterState(tflite::MicroInterpreter*)':\r\n./tensorflow/lite/experimental/micro/micro_optional_debug_tools.cc:94:68: error: 'printf' was not declared in this scope\r\n          interpreter->tensors_size(), interpreter->operators_size());\r\n       \r\n\r\n[mbed] ERROR: \"/usr/bin/python\" returned error.\r\n       Code: 1\r\n       Path: \"/home/jomodev/development/stm/env2/tensorflow-2.1.0/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed\"\r\n       Command: \"/usr/bin/python -u /home/jomodev/development/stm/env2/tensorflow-2.1.0/tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM\"\r\n       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n---\r\n\r\n```\r\n\r\nAfter these changes, the code compiled fine. \r\n", "Could you please try on latest stable version of TF 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "I attempted with Tensorflow 2.5.0 and it still fails:\r\n```\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=disco_f746ng OPTIMIZED_KERNEL_DIR=cmsis_nn generate_hello_world_mbed_project\r\n\r\n\r\nKERNEL_DIR=cmsis_nn generate_hello_world_mbed_project\r\n--2021-08-08 15:53:17--  http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\r\nResolving mirror.tensorflow.org (mirror.tensorflow.org)... 216.58.194.208, 2607:f8b0:4005:805::2010\r\nConnecting to mirror.tensorflow.org (mirror.tensorflow.org)|216.58.194.208|:80... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1760478 (1.7M) [application/zip]\r\nSaving to: \u2018/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\u2019\r\n\r\n/tmp/dca12522a9f9e3 100%[===================>]   1.68M  4.26MB/s    in 0.4s    \r\n\r\n2021-08-08 15:53:18 (4.26 MB/s) - \u2018/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\u2019 saved [1760478/1760478]\r\n\r\nCloning into 'tensorflow/lite/micro/tools/make/downloads/pigweed'...\r\nremote: Sending approximately 16.66 MiB ...\r\nremote: Counting objects: 23, done\r\nremote: Finding sources: 100% (23/23)\r\nremote: Total 26938 (delta 12530), reused 26929 (delta 12530)\r\nReceiving objects: 100% (26938/26938), 16.53 MiB | 6.14 MiB/s, done.\r\nResolving deltas: 100% (12530/12530), done.\r\nNote: checking out '47268dff45019863e20438ca3746c6c62df6ef09'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b <new-branch-name>\r\n\r\nHEAD is now at 47268dff pw_hdlc_lite: Client I/O improvements\r\ntensorflow/lite/micro/tools/make/Makefile:600: tensorflow/lite/micro/tools/make/targets/disco_f746ng_makefile.inc: No such file or directory\r\nURL transformed to HTTPS due to an HSTS policy\r\n--2021-08-08 15:53:25--  https://github.com/ARM-software/CMSIS_5/archive/0d7e4fa7131241a17e23dfae18140e0b2e77728f.zip\r\nResolving github.com (github.com)... 192.30.255.113\r\nConnecting to github.com (github.com)|192.30.255.113|:443... connected.\r\nHTTP request sent, awaiting response... 302 Found\r\nLocation: https://codeload.github.com/ARM-software/CMSIS_5/zip/0d7e4fa7131241a17e23dfae18140e0b2e77728f [following]\r\n--2021-08-08 15:53:25--  https://codeload.github.com/ARM-software/CMSIS_5/zip/0d7e4fa7131241a17e23dfae18140e0b2e77728f\r\nResolving codeload.github.com (codeload.github.com)... 192.30.255.120\r\nConnecting to codeload.github.com (codeload.github.com)|192.30.255.120|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: unspecified [application/zip]\r\nSaving to: \u2018/tmp/0d7e4fa7131241a17e23dfae18140e0b2e77728f.zip\u2019\r\n\r\n/tmp/0d7e4fa7131241     [ <=>                ]  48.04M  6.95MB/s    in 7.2s    \r\n\r\n2021-08-08 15:53:32 (6.69 MB/s) - \u2018/tmp/0d7e4fa7131241a17e23dfae18140e0b2e77728f.zip\u2019 saved [50371309]\r\n\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/disco_f746ng_makefile.inc'.  Stop.\r\n```\r\n", "I attempted with Tensorflow 2.4.2 and had a similar issue. Looking at the targets folder, there was no 'disco_f746ng' reference but there was a 'stm32f4' one so I tried that.\r\nThis seemed to compile and resulted in the following folder:\r\n\r\n```\r\nCompile Command:\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=stm32f4 OPTIMIZED_KERNEL_DIR=cmsis_nn generate_hello_world_mbed_project\r\n\r\nFolder:\r\ntensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed\r\n```\r\nFrom here I was able to complete the rest of the process, but seen the following warning during the 'mbed compile' step:\r\n```\r\n$ mbed compile -m DISCO_F746NG -t GCC_ARM\r\n[mbed] Working path \"/home/jomodev/development/tensorflow/tensorflow_08082021/tensorflow-2.4.2/tensorflow/lite/micro/tools/make/gen/stm32f4_cortex-m4/prj/hello_world/mbed\" (program)\r\nWARNING: MBED_ARM_PATH set as environment variable but doesn't exist\r\n[Warning] @,: Compiler version mismatch: Have 5.4.1; expected version >= 9.0.0 and < 10.0.0\r\n```\r\n\r\nThis resulted in the following file:\r\n```\r\n./BUILD/DISCO_F746NG/GCC_ARM/mbed.bin\r\n```\r\n\r\nAfter copying this to a STM32F746G-DISCO, nothing appears on the screen and nothing is seen from the Serial output, so I am not sure what got built.\r\n\r\n", "Oh, actually the initial build resulted in the following folder:\r\n```\r\ntensorflow/lite/micro/tools/make/gen/stm32f4_cortex-m4/prj/hello_world/mbed\r\n```\r\nAfter further review, this is creating a STM32F4 Cortex M4 folder. However, from the STM Hello World example, it should be for a STM32F7 Cortex M7. The M4 code will not work on a M7 board. I am not clear as to what board this is for then, but would like to have the  STM32F746G-DISCO code back as per the example at:\r\n[https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world](url)"]}, {"number": 34200, "title": "OutOfRangeError when training estimator with filter in the input_fn", "body": "**System information**\r\n- Have I written custom code: YES\r\n- OS Platform and Distribution: WINDOWS 10, 64-bits\r\n- TensorFlow installed from (source or binary): conda install -c anaconda tensorflow-gpu\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: from conda installation\r\n- GPU model and memory: NVIDIA GE Force 730M, 4.8GB\r\n\r\n**Describe the current behavior**\r\nOutOfRangeError when training estimator with filter in the input_fn\r\nWhen I remove the `.filter(lambda x: x['label'] == 0)` in the `input_fn`, everything works fine.\r\nThe same error appears if I use:\r\n`tf.estimator.train_and_evaluate(KMeansEstimator, train_spec_kmeans, eval_spec_kmeans)`\r\nor\r\n`KMeansEstimator.train(input_fn = lambda: input_fn(data_files) , max_steps=10)`\r\n\r\n**Describe the expected behavior**\r\nNo error during the training with the filtered dataset in the `input_fn` of the estimator\r\n\r\n**Code to reproduce the issue**\r\ntfrecords_path is the path to mnist dataset as TFRecords files with the features described bellow:\r\n```\r\ndef parser(record):\r\n    features={\r\n            'label': tf.io.FixedLenFeature([], tf.int64),\r\n            'height': tf.io.FixedLenFeature([], tf.int64),\r\n            'width': tf.io.FixedLenFeature([], tf.int64),\r\n            'depth': tf.io.FixedLenFeature([], tf.int64),\r\n            'image_raw': tf.io.FixedLenFeature([], tf.string)\r\n            }\r\n    parsed = tf.io.parse_single_example(record, features)\r\n    feats = tf.io.decode_raw(parsed['image_raw'], tf.float64)\r\n    return {'image': feats, 'label': parsed['label']}\r\n\r\ndef input_fn(tfrecords_path):\r\n    dataset = (\r\n        tf.data.TFRecordDataset(tfrecords_path)\r\n        .map(\r\n            map_func=parser,\r\n            num_parallel_calls=int(multiprocessing.cpu_count() * 0.75)\r\n            )\r\n        .filter(lambda x: x['label'] == 0)\r\n        .batch(1024)\r\n        .prefetch(1024)\r\n     )\r\n    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\r\n    batch_feats = iterator.get_next()\r\n    return batch_feats\r\n\r\ntrain_spec_kmeans = tf.estimator.TrainSpec(input_fn = lambda: input_fn(data_files) , max_steps=10)\r\neval_spec_kmeans = tf.estimator.EvalSpec(input_fn = lambda: input_fn(data_files) )\r\n\r\nKMeansEstimator = tf.compat.v1.estimator.experimental.KMeans(\r\n    num_clusters=number_of_clusters,\r\n    feature_columns = [tf.feature_column.numeric_column(\r\n        key='image',\r\n        dtype=tf.float64,\r\n        shape=(784,)\r\n    )],\r\n    use_mini_batch=True)\r\n\r\ntf.estimator.train_and_evaluate(KMeansEstimator, train_spec_kmeans, eval_spec_kmeans)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1160, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1194, in _train_model_default\r\n    saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1489, in _train_with_estimator_spec\r\n    log_step_count_steps=log_step_count_steps) as mon_sess:\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 584, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 1014, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 725, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 1207, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 1212, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 885, in create_session\r\n    hook.after_create_session(self.tf_sess, self.coord)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\kmeans.py\", line 105, in after_create_session\r\n    session.run(self._init_op)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: 2 root error(s) found.\r\n  (0) Out of range: End of sequence\r\n         [[node IteratorGetNext (defined at C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\r\n         [[Shape_1/_37]]\r\n  (1) Out of range: End of sequence\r\n         [[node IteratorGetNext (defined at C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'IteratorGetNext':\r\n  File \".\\main.py\", line 144, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \".\\main.py\", line 136, in main\r\n    launch_training_testing()\r\n  File \".\\main.py\", line 100, in launch_training_testing\r\n    km_ckpt, kmeans = train_test.train_quantizers(FLAGS)\r\n  File \"C:\\Users\\CUI\\ML\\Decentralized_class_V1_tf2\\src\\model\\train_test_tf2TFR.py\", line 334, in train_quantizers\r\n    km_ckpt, kmeans = label_train_quantizers(label, q_c, kmeans, km_ckpt, config)\r\n  File \"C:\\Users\\CUI\\ML\\Decentralized_class_V1_tf2\\src\\model\\train_test_tf2TFR.py\", line 296, in label_train_quantizers\r\n    kmeans[label][q_c].train(input_fn = lambda: kmtrain_input_fn(tfr_train_files) , max_steps=config.km_steps)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1160, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1187, in _train_model_default\r\n    input_fn, ModeKeys.TRAIN))\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1024, in _get_features_and_labels_from_input_fn\r\n    self._call_input_fn(input_fn, mode))\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py\", line 65, in parse_input_fn_result\r\n    result = iterator.get_next()\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 426, in get_next\r\n    name=name)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\", line 2500, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3360, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1751, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\n", "comments": ["@DenisUllmann ,\r\nThank you for reporting the issue, looks like the code given is incomplete can you please provide standalone code to reproduce the issue? \r\nAlso can you try reducing the `batch size `and see is if the error is not faced ?", "@oanush ,\r\nPlease find bellow a standalone code with `tensorflow_datasets` that reproduces the same error with filtering, and that works fine without filtering (removing `.filter(lambda x: x['label'] == 0)`). If I put `batch size` to 1, the same error raises.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nnumber_of_clusters = 10\r\nnumber_of_epochs = 10\r\n\r\ndef input_fn():\r\n    data_builder = tfds.builder('mnist')\r\n    data_builder.download_and_prepare()\r\n    \r\n    dataset = (\r\n        data_builder.as_dataset(split=tfds.Split.TRAIN)\r\n        .filter(lambda x: x['label'] == 0)\r\n        .map(lambda x: {'image': tf.reshape(x['image'], [-1,]), 'label': x['label']})\r\n        .batch(1024)\r\n        .prefetch(1024)\r\n     )\r\n    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\r\n    batch_feats = iterator.get_next()\r\n    return batch_feats\r\n\r\ntrain_spec_kmeans = tf.estimator.TrainSpec(input_fn = input_fn , max_steps=10)\r\neval_spec_kmeans = tf.estimator.EvalSpec(input_fn = input_fn)\r\n\r\nKMeansEstimator = tf.compat.v1.estimator.experimental.KMeans(\r\n    num_clusters=number_of_clusters,\r\n    feature_columns = [tf.feature_column.numeric_column(\r\n        key='image',\r\n        dtype=tf.float64,\r\n        shape=(784,)\r\n    )],\r\n    use_mini_batch=True)\r\n\r\ntf.estimator.train_and_evaluate(KMeansEstimator, train_spec_kmeans, eval_spec_kmeans)\r\n```\r\n\r\nResult without filtering:\r\n```\r\nINFO:tensorflow:Loss for final step: 2652678100.0.\r\n({'loss': 2667317500.0, 'score': 2667317500.0, 'global_step': 10}, [])\r\n```\r\n\r\nWith filtering:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 613, in run\r\n    return self.run_local()\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 714, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1160, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1194, in _train_model_default\r\n    saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1489, in _train_with_estimator_spec\r\n    log_step_count_steps=log_step_count_steps) as mon_sess:\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 584, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 1014, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 725, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 1207, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 1212, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\", line 885, in create_session\r\n    hook.after_create_session(self.tf_sess, self.coord)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\kmeans.py\", line 105, in after_create_session\r\n    session.run(self._init_op)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: 2 root error(s) found.\r\n  (0) Out of range: End of sequence\r\n         [[node IteratorGetNext (defined at C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\r\n         [[IteratorGetNext/_33]]\r\n  (1) Out of range: End of sequence\r\n         [[node IteratorGetNext (defined at C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'IteratorGetNext':\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 613, in run\r\n    return self.run_local()\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 714, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1160, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1187, in _train_model_default\r\n    input_fn, ModeKeys.TRAIN))\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1024, in _get_features_and_labels_from_input_fn\r\n    self._call_input_fn(input_fn, mode))\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1115, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"<stdin>\", line 1, in <lambda>\r\n  File \"<stdin>\", line 13, in input_fn\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\", line 426, in get_next\r\n    name=name)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\", line 2500, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3360, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"C:\\Users\\CUI\\.conda\\envs\\py3tf2_0\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1751, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```", "Issue replicating with given code for tf-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/2bb4f461bd5f846b16b7de6d219ec8eb/34200.ipynb) colab.Thanks!", "I no longer work in this codebase. Please find appropriate owner.", "This is possibly due to the ways in which the Kmeans estimator is trying to check the number of dataset steps, but we do not currently have the availability to dig into this. @DenisUllmann , is this something you can look into and submit a fix for?", "Issue replicating with given code for tf-2.3, please find [gist here](https://colab.research.google.com/gist/Saduf2019/b63e790d96b37fa0e4ee6bf758f80de3/untitled368.ipynb)", "I was able to reproduce the issue in Tensorflow 2.4, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/7afb880722038a5aa0c844de049f7a96/34200.ipynb).", "Was able to replicate the issue with TF [v2.5 ](https://colab.research.google.com/gist/sushreebarsa/a78535d14a346f427856e778e262a253/untitled244.ipynb#scrollTo=VuLjYwPPMRYT)& TF [2.6.0-dev20210606](https://colab.research.google.com/gist/sushreebarsa/0b398d7ec9debbf4c556aff7df8b6333/untitled245.ipynb),please find the gists here ..Thanks!", "I was able to reproduce the issue in Tensorflow 2.6, please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/e565a5148e12171e18c4f9529d98a96e/untitled245.ipynb#scrollTo=g7u0Qqz_sUuG)."]}, {"number": 34171, "title": "Bug in LSTMBlockCellBpropWithCUDA when using peephole connections", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\nr2.0\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n9.0/7.4\r\n- GPU model and memory:\r\nQuadro M2200\r\n\r\n**Describe the current behavior**\r\nIn the backward propagation of the LSTM GPU code, there's a [broadcast operation included in the gradient computation](https://github.com/tensorflow/tensorflow/blob/722b96b22926dbc05881c35cb63fd342c6843112/tensorflow/core/kernels/rnn/lstm_ops_gpu.cu.cc#L398) of `cs_prev_grad` involving `wci` and `wcf`\r\n```c++\r\n  if (use_peephole) {\r\n    Eigen::array<Eigen::DenseIndex, 2> p_shape({1, cell_size});\r\n    Eigen::array<Eigen::DenseIndex, 2> p_broadcast_shape({batch_size, 1});\r\n    cs_prev_grad.device(d) =\r\n        cs_prev_grad + di * wci.reshape(p_shape).broadcast(p_broadcast_shape) +\r\n        df * wcf.reshape(p_shape).broadcast(p_broadcast_shape);\r\n    wci_grad.device(d) = (di * cs_prev).sum(Eigen::array<int, 1>({0}));\r\n    wcf_grad.device(d) = (df * cs_prev).sum(Eigen::array<int, 1>({0}));\r\n    wco_grad.device(d) = (do_ * cs).sum(Eigen::array<int, 1>({0}));\r\n  }\r\n}\r\n```\r\nJust before this code, the [GPU kernel](https://github.com/tensorflow/tensorflow/blob/722b96b22926dbc05881c35cb63fd342c6843112/tensorflow/core/kernels/rnn/lstm_ops_gpu.cu.cc#L358) `lstm_gates_bprop` is launched which calculates the gradient of `cs_prev_grad` using `wcf` and `wci`\r\n\r\n```c++\r\n__global__ void lstm_gates_bprop(\r\n    const T* cs_prev,  // [batch_size, cell_size]\r\n    const T* h_prev,   // [batch_size, cell_size]\r\n    const T* w,        // [input_size + cell_size, 4 * cell_size]\r\n    const T* wci,      // [cell_size]\r\n    const T* wcf,      // [cell_size]\r\n    const T* wco,      // [cell_size]\r\n    const T* b,        // [4 * cell_size]\r\n    const T* i,        // [batch_size, cell_size]\r\n    const T* cs,       // [batch_size, cell_size]\r\n    const T* f,        // [batch_size, cell_size]\r\n    const T* o,        // [batch_size, cell_size]\r\n    const T* ci,       // [batch_size, cell_size]\r\n    const T* co,       // [batch_size, cell_size]\r\n    const T* cs_grad,  // [batch_size, cell_size]\r\n    const T* h_grad,   // [batch_size, cell_size]\r\n    T* do_,            // [batch_size, cell_size]\r\n    T* dcs,            // [batch_size, cell_size]\r\n    T* dci,            // [batch_size, cell_size]\r\n    T* df,             // [batch_size, cell_size]\r\n    T* di,             // [batch_size, cell_size]\r\n    T* dgates,         // [input_size + cell_size, 4 * cell_size]\r\n    T* cs_prev_grad,   // [batch_size, cell_size]\r\n    const int batch_size, const int cell_size, const bool use_peephole) {\r\n  const int batch_id = blockIdx.x * blockDim.x + threadIdx.x;\r\n  const int act_id = blockIdx.y * blockDim.y + threadIdx.y;\r\n\r\n  if (batch_id >= batch_size || act_id >= cell_size) return;\r\n\r\n  const int gid = batch_id * cell_size * 4 + act_id;\r\n  const int cid = batch_id * cell_size + act_id;\r\n\r\n  // bunch of code\r\n\r\n  dgates[gid + 0 * cell_size] = di_local;\r\n  dgates[gate_c_offset(gate_layout, cell_size)] = dci_local;\r\n  dgates[gate_f_offset(gate_layout, cell_size)] = df_local;\r\n  dgates[gid + 3 * cell_size] = do_local;\r\n\r\n  cs_prev_grad[cid] = dcs_local * f_local;\r\n  if (use_peephole) {\r\n    cs_prev_grad[cid] += di_local * wci[act_id] + df_local * wcf[act_id];\r\n  }\r\n```\r\n\r\n**Describe the expected behavior**\r\nUnless I misunderstand, the gradient when using peephole connections is incorrect. The peephole gradient should be taken into account _either_ in the GPU kernel _or_ by the eigen computation, but not both.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Adding original author @ekelsen", "The original change is added in https://github.com/tensorflow/tensorflow/commit/61b521679b9468b4b4ffa792a1ebc6a5ecd28317.\r\n\r\nAnd also add true author @duckworthd", "Additionally, I also think these kernels have some slowness due to not accessing coalesced memory. The kernel launch indicates `batch_size` is being thought of as the x dimension (cartesian) when it is actually the row dimension of the matrix (CUDA uses image notation rather than matrix notation.\r\n\r\nLooking at the [definition of the cid variable](https://github.com/tensorflow/tensorflow/blob/722b96b22926dbc05881c35cb63fd342c6843112/tensorflow/core/kernels/rnn/lstm_ops_gpu.cu.cc#L312) it uses `act_id` as the offset. Yet `act_id` is defined in terms of the `threadIdx.y` which is the slower varying dimension. We should prefer it being dependent on `threadIdx.x` for coalesced memory access", "Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ? please check **[link](https://www.tensorflow.org/api_docs/python/tf/keras/experimental/PeepholeLSTMCell)** . Thanks!", "It still looks like the bug is there having just looked at the code now"]}, {"number": 34037, "title": "Support key mapping in TFRecord", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFor multiple features situation like recommendation, names of features will take a lot of spaces in tfrecord files. A simple solution is replacing name by feature's index. Do this compression automatically needs change tfrecord file. Support a user-specific option maybe better.\r\n\r\n**Will this change the current api? How?**\r\nAdd a key mapping parameter to `tf.data.TFRecordDataset`, `tf.io.TFRecordWriter` and `tf.data.FixedLengthRecordDataset`.\r\n\r\n**Who will benefit with this feature?**\r\nSave disk space for industry situations.\r\n\r\n**Any Other info.**\r\n", "comments": ["@Officium \r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature. Thanks!", "@ravikyram In CTR prediction task, one may use statistics of history over several dimensions to construct features. For example, a user's features contains the average click count on advertises with same category of users with same gender in past 7 days. A middle size advertise system may use thousands of such features. For system implementation, the feature consistence between online and offline is important. So the features used online will be recored in logs which can be converted to TFRecords by Spark for offline training. Here is a simple comparison case. In this case, mapping name to index will save 13% store spaces.\r\n```python\r\nimport itertools\r\nimport random\r\n\r\nimport tensorflow as tf\r\nfrom tqdm import trange\r\n\r\n\r\ndef construct_feature_names():\r\n    id_features = ['age', 'gender', 'category', 'city', 'system']\r\n    dense_features = ['impression', 'click', 'ctr', 'pagetime']\r\n    time_features = [1, 7, 14, 28]\r\n\r\n    total_features = []\r\n    for cross_num in range(len(id_features)):\r\n        for id_cross in itertools.combinations(id_features, cross_num):\r\n            for dense, time in itertools.product(dense_features, time_features):\r\n                total_features.append('_'.join(id_cross + (dense, str(time))))\r\n    return total_features\r\n\r\n\r\ndef write_tfrecord(write_path, num_sample=10000, use_index_key=False):\r\n    \"\"\"\r\n    Write random samples to tfrecord\r\n    Args:\r\n        write_path (str): file path to write\r\n        num_sample (int): total number of random samples\r\n        use_index_key (bool): whether use feature's index to replace its name\r\n    \"\"\"\r\n    with tf.io.TFRecordWriter(write_path, 'GZIP') as writer:\r\n        for _ in trange(num_sample):\r\n            features = {}\r\n            for name in construct_feature_names():\r\n                key = name if not use_index_key else str(len(features))\r\n                features[key] = tf.train.Feature(int64_list=tf.train.Int64List(value=[int(10000000 * random.random())]))\r\n            tfrecord_example = tf.train.Example(features=tf.train.Features(feature=features))\r\n            writer.write(tfrecord_example.SerializeToString())\r\n\r\n\r\nwrite_tfrecord('index.tfrecord', use_index_key=True)\r\nwrite_tfrecord('raw.tfrecord', use_index_key=False)\r\n```\r\n\r\n", "@ravikyram This feature will support a time-space tradeoff. In real system, TBs of data will be used for distributional training.  It saves disk space and network transmission time but increases loading time. Basically, it provides improvement."]}]