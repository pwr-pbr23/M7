[{"number": 9306, "title": "AttributeError: module 'tensorflow' has no attribute 'get_default_graph'", "body": "**Error Log:**\r\nmodel = Sequential()\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-50-24a4d47a09aa>\", line 1, in <module>\r\n    model = Sequential()\r\n...\r\n/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 47, in get_uid\r\n    if graph not in _GRAPH_UID_DICTS:\r\n\r\n**_AttributeError: module 'tensorflow' has no attribute 'get_default_graph'_**\r\n\r\n\r\n------------\r\n**Versions:**\r\n!pip3 show tensorflow\r\nName: tensorflow\r\nVersion: 1.0.1\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nRequires: numpy, protobuf, six, wheel\r\n\r\n------------\r\n!pip3 show keras\r\nName: Keras\r\nVersion: 2.0.3\r\nSummary: Deep Learning for Python\r\nHome-page: https://github.com/fchollet/keras\r\nAuthor: Francois Chollet\r\nAuthor-email: francois.chollet@gmail.com\r\nLicense: MIT\r\nLocation: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nRequires: six, theano, pyyaml\r\n\r\n------------\r\n**OS:**:  Mac OS\r\n------------\r\n\r\nI couldnt find any solution to it in \"existing issues\" nor on \"Stackoverflow\".\r\nAny suggestions?", "comments": ["That seems to be a mismatch in keras, not tensorflow. Sorry, closing this since this is not a tensorflow bug.\r\n\r\n@fchollet "]}, {"number": 9305, "title": "Add reqeueue operations.", "body": "This PR adds `requeue` and `requeue_many` operations to `QueueBase`. The operations dequeue an element/elements from the queue, add the element(s) to the queue again, and return an op that evaluates to the dequeued value.\r\n\r\nThe motivation behind these operations is to be able to reuse training data that is hard to come by after using them in training.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "I think this is a simple enough change (without enough context) that we should let clients write this code.\r\n\r\nThe sniff test for adding an API to a core library, whose implementation only has a few lines of code is:\r\n\r\n1) How often do people do this?  If rare, then let clients write the code.  I haven't seen this be used very often.\r\n\r\n2) Is it easy to write those few lines wrong?  I'd argue in this case, the only challenge is remembering to add the control dependency.  But by that definition, all code that uses control dependencies should be in TF core, which would be unmaintainable.  (We definitely made a mistake in exposing control dependencies to users by default, but that needs to be fixed in a more general way). \r\n\r\nSo I'm going to err on the side of saying \"this is a thing you can do\" but not have the burden of maintenance be transferred to the TF core team, at least until more context is available.\r\n\r\nHope that makes sense, and that I'm being reasonable in your opinion!", "Sounds good. We'll keep it in our code for the time being. "]}, {"number": 9304, "title": "Problem displaying Chinese character in Tensorboard Embedding", "body": "I am trying to display an embedding but what its show is all garbled. May i know does Tensorboard Embedding able to display Chinese character?\r\nMay i know its that possible to display more than 100K point? If yes, how to i do the configuration?\r\n![default](https://cloud.githubusercontent.com/assets/27762422/25168360/cf775996-2515-11e7-8fa1-326d06e6d045.PNG)\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@Jeffery4000 yes, you can display Chinese in tensorboard embedding. Make sure you use UTF-8 encoding in your metadata file."]}, {"number": 9303, "title": "Import additional RNNCells in tf.contrib.rnn.__init__.py ", "body": "Some RNNCells, like IntersectionRNNCell, NASCell, are implemented in tf.contrb.rnn.python.ops.rnn_cell. I would like to try these cells but found that these can't be used as the same way as LSTMCell, which I can import by ```from tf.contrib.rnn import LSTMCell```. I checked the code found that these cells are not imported by ```tf.contrib.rnn.__init__.py ```. \r\n\r\nIs there any way to try these cells? Or is it possible to import these cells in ```tf.contrib.rnn.__init__.py ```.\r\n\r\nThanks.", "comments": ["Yes, you have to add a reference in the [docstring](http://google3/third_party/tensorflow/contrib/rnn/__init__.py?l=44). Feel free to send a PR and CC @ebrevdo ", "PR, https://github.com/tensorflow/tensorflow/pull/9327 ", "Thanks!"]}, {"number": 9302, "title": "TensorFlow reverting to GPU on a CPU machine on most recent nightly build", "body": "I'm running tensorflow on windows. I continue to produce these errors around CUDA libraries and cuDNNs ,but I don't have a GPU and didn't install a GPU version of TF. \r\n\r\nI installed the latest nightly build of TF from jenkins (apr 17) but when that one failed on me I tried to pip3 uninstall and revert to [this one](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=cpu,OS=windows/134/) to no success.\r\n\r\nI was up and running last night thanks to a solution on github from Derek, but cannot stop reproducing this CUDA error after playing around with the path variables in pycharm.\r\n\r\n\r\n- Error is reproduced on both custom code and standard wide_n_deep TensorFlow examples\r\n- Windows 10 64; Python 35\r\n- TF installed from nightly build\r\n- TF versions 1.1.0 and 1.0.1\r\n- I have no GPU\r\n- C:\\Users\\me>C:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python35\\python.exe C:\\Users\\me\\Desktop\\wide_n_deep --model_type =\"wide_n_deep\"\r\n\r\n### Source Code / Logs\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cublas64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:2294] Unable to load cuBLAS DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cudnn64_5.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:3517] Unable to load cuDNN DSO\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cufft64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library nvcuda.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.cc:165] hostname: DESKTOP-HMIARON\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Larriva\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\Larriva\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Larriva\\Desktop\\wnd2.py\", line 15, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Larriva\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Larriva\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\Larriva\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Did you try to reboot? Is that with the new Windows 10 Creator update? Do you have up to date nvidia drivers?", "Thank you for your reply. I tried a reboot.\r\nI've not done any windows updates in the time between having the working code and non-working code.\r\nI don't believe I have any NVIDIA hardware or software on my computer. Intel runs the graphics.", "Are these in your path?\r\n\r\n* `cublas64_80.dll`\r\n* `cudnn64_5.dll`\r\n* `cufft64_80.dll`\r\n* `nvcuda.dll`\r\n\r\nIf not, try to locate them and add them into your path. You can use the windows dependency walker to check if you need to add more dependencies.\r\n\r\n/CC: @mrry ", "those dlls don't exist on my system anywhere, so was unable to add them to PATH.\r\nI downloaded cuda_8.0.61_win10 in hopes of grabbing those dlls...\r\nhttps://developer.nvidia.com/cuda-downloads\r\ninstaller recommended a different specific version of visual studio\r\nhttps://www.visualstudio.com/thank-you-downloading-visual-studio/?sku=Community&rel=15\r\ncuda install failed because the above version of VS is 17 and cuda wants <=2015.\r\nFound a version of 2015 \r\nhttp://download.cnet.com/Visual-Studio-Community-2015/3055-2212_4-76440611.html?tag=pdl-redir\r\nReinstalled the CUDA package\r\nCame closer and got a few more packages successfully found, but issue did not resolve...\r\n\r\n`I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cudnn64_5.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:3517] Unable to load cuDNN DSO\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library nvcuda.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.cc:165] hostname: DESKTOP-HMIARON`", "Yes, please install the rest of the DLLs.", "Do you know where I can download?\nThey didn't seem to come with the CUDA package.\n\nOn Thu, Apr 20, 2017 at 8:58 AM, drpngx <notifications@github.com> wrote:\n\n> Yes, plesae install the rest of the DLLs.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9302#issuecomment-295790311>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AaeMoce-utna36nLl_cMYS3YdybDqTiRks5rx4C7gaJpZM4NBNs9>\n> .\n>\n", "Please follow the resolution there. \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/5968", "I appreciate your help but i'm still not running. \r\nNone of the CUDA packages seemed to contain nvcuda.dll so I grabbed the ala carte  dll off a site and dropped it into the same bin as the other referenced dlls, re-added it to PATH and still am receiving the message:\r\n```\r\n\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library nvcuda.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.cc:165] hostname: \u00ea\u0002\u2261\u00b0\u00a0\u007f\r\n```\r\n\r\n", "Solved it. I believe it hints at my original issue: something somewhere in the process made TF look for GPU components despite my not having a GPU.\r\nI uninstalled everything (python, tensorflow, dumped the cached files in the pip bin) then got a nightly build of TF from Jenkins, reinstalled Python, deleted my VS17 (because the pywrap requries the15 build) and got a successful load.\r\nThank you again for your help.", "Great to hear!"]}, {"number": 9301, "title": "Quantized graph fails to work on NVIDIA Jetson TX1 architecture although it worked on a normal PC?", "body": "### System Information\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\nNo\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*:\r\nUbuntu 16.04 LTS on NVIDIA Jetson TX1 (Linux for Tegra 24.1)\r\n- *TensorFlow installed from (source or binary)?*:\r\nCompiled from source and installed via this wheel available here: https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack\r\n- *TensorFlow version* (use command below):\r\n1.0 Alpha\r\n- *CUDA/cuDNN version*:\r\n8.0/5.1\r\n- *GPU Model and Memory*:\r\nTegra X1, 4GB\r\n\r\n### Describe the problem clearly\r\nWhen I ran a quantized graph on my laptop, it works quite as expected (just slightly lower accuracy compared to the frozen graph, while much lower size). However, when I transferred the exact same quantized graph to run on an Jetson TX1, the file gives me a highly inaccurate class prediction, at a 100% probability for the random class. On the other hand, when I tried to perform inference from the frozen graph (from which the quantized graph was derived) on both my laptop and the Jetson TX1, I got the exact same answers as expected.\r\n\r\nSo I suspected it could have been an issue of data transfer causing the file to be slightly corrupted. I checked the files byte by byte at all points of transfer (from my laptop to memory stick, then memory stick to the Jetson), but I found the files are exactly the same. This is the command I used to check: `cmp $old_file $new_file || echo \"different files\"`.\r\n\r\nThus, I am suspecting it could be an issue of how tensorflow performs on the Jetson TX1 ARM architecture (aarch64). Is there anyway to verify this, and if it is indeed a performance issue on an ARM architecture, is there a way to resolve this?\r\n\r\nThank you for your help.\r\n", "comments": ["@andrewharp might be more up-to-date on the Jetson progress.", "The random single-category symptom is the same as the issue seen in https://github.com/tensorflow/tensorflow/issues/7150#issuecomment-282343046, so maybe this is related.\r\n\r\nAs @andrehentz suggests there, maybe try quantizing with both quantize_weights and quantize_nodes?", "Thanks for your reply! Do you have an example of how this could be done? I have always thought there was only one quantization function available (for the weights). What is the difference between quantizing the weights vs the nodes?\r\n\r\nThe current code I'm using for the quantization is this:\r\n\r\n```\r\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph \\\r\n--input=./frozen_model_inception_v3.pb \\\r\n--output_node_names=\"InceptionV3/Predictions/Softmax\" \\\r\n--output=./quantized_graph_inception_v3.pb \\\r\n--mode=eightbit\r\n\r\n```\r\n\r\nEDIT: I found out the quantization of the nodes come from the graph_transform tool, and I tried to quantize an inception v4 model both weights and the nodes. However, the quantization of the node give me an `IndexError`. This is the error stack:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"evaluate_IR2_pb.py\", line 32, in <module>\r\n    tf.import_graph_def(graph_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 404, in import_graph_def\r\n    ops.set_shapes_for_outputs(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1719, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1669, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 653, in _call_cpp_shape_fn_impl\r\n    v = tensor_util.constant_value(op.inputs[idx])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 703, in constant_value\r\n    ret = _ConstantValue(tensor)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 651, in _ConstantValue\r\n    dim = constant_value(tensor.op.inputs[-1])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1439, in __getitem__\r\n    return self._op._inputs[i]\r\nIndexError: list index out of range\r\n```\r\nHere is the bazel command for quantizing weights and nodes:\r\n```\r\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_model_inception_resnet_v2.pb \\\r\n--out_graph=./quantized_weights_and_nodes_inception_resnet_v2.pb \\\r\n--inputs='Placeholder_only:0' \\\r\n--outputs='InceptionResnetV2/Logits/Predictions:0' \\\r\n--transforms='quantize_weights quantize_nodes'\r\n```\r\nOnce I removed `quantize_nodes`, the error is gone. But I am left with a quantization (`quantize_weights` only) that gives me the 100% inaccuracy I saw on the Jetson. On the other hand, using the quantization method (also weights only) I first used still gave me a high accuracy I expected.\r\n\r\nHowever, when I tried this on the inception_v3 model, the quantization of weights and nodes work as expected, but not on the Jetson, which gives me a `ValueError: No op named QuantizedMul in defined operations\". Upon removing `quantize_nodes` and using the `quantize_weights` only, the model runs, but again, it gives me the highly inaccurate answers.\r\n\r\nMy conclusion is that `quantize_nodes` isn't stable for the Jetson TX1, but on the other hand, `quantize_weights` (from the graph_transform tool) is highly inaccurate. Note that the above predictions are all on the custom imagenet model.\r\n\r\nDiscounting the use of the tools from `graph_transform` and following what I did initially with the quantization, both the quantization (not from `graph_transform` tool) and freezing graph (by manually exporting the graph after restoring from a checkpoint) works very well for another non-imagenet problem. The model used for this problem came from fine-tuning the imagenet problem. This is a very strange behavior.\r\n\r\nSo from what I can conclude, I think it is that quantization tools from `transform_graphs` is rather buggy, and for the quantization from `quantize_graph`, it works quite well but one can get very bad results on a Jetson TX probably due to how the architecture handles the calculations (This is just my conjecture). The calculations could be done differently in the Jetson because of the quantization to 8 bits, but I am not experienced with the Jetson so I can't say anything for sure. I have kept the imagenet prediction model and the fine-tuned model codes almost exactly the same, and the only things differing between them are the checkpoint files and the number of classes to predict in the end(1001 for the imagenet, and 5 for the fine-tuned one). Could this have caused the error? \r\n\r\nIs there a way to get consistent results from quantization (especially on a Jetson)?", "@kwotsin \r\n\r\nThe \"official\" way to quantize a graph is described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#eight-bit-calculations), but I'm in the process of updating it slightly. In any case, I think the problem you are encountering is unrelated to the actual quantization steps.\r\n\r\nMy theory is that a different code path is triggered on a Jetson, because it supports NEON instructions. I'll try to get a fix in the nightly builds before the end of next week. Meanwhile, if you'd like to build it yourself, try:\r\n`bazel build -c opt --copt=\"-DTENSORFLOW_DISABLE_META\" //tensorflow/contrib/android:libtensorflow_inference.so --config=android_arm`\r\n(or the equivalent for your architecture)\r\n\r\n", "@andrehentz I have tried building TF from source but with not very much success. The furthest version of TF I've gotten to is the one available here: https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack. Would this be a cause of the issue? \r\n\r\nHowever, for the quantization, I run it on my host machine. Regarding the `android:libtensorflow`_inference file you mentioned to build, do I run the graph_transform tool or quantize_graph tool after building that with bazel? I have tried to use `graph_transform` and `quantize_graph` both after building the tensorflow inference on my host machine (is this correct), but they gave me the same results as before.\r\n\r\nOn the other hand, on my fine-tuned model for predicting flowers, the model actually predicted very well (nearly identical accuracy at 99% for both quantized and frozen models) for **some images**. Upon testing on images that are slightly larger, the quantized version is no better than a random guess. Could this be attributed to the accuracy of quantization? In any case, the frozen model seems to be much more robust (correct predictions for everything).\r\n\r\nRegarding the speed: I find that quantization could speed up my predictions by at least 30%, and the fastest time it took for some runs was half that required of the frozen models. I am very much looking forward to using quantization if the robustness issue can be solved. Meanwhile, thank you for helping me in this matter! I look forward to your fix! :D ", "Let's  always use `graph_transform` (because `quantize_graph` is slowly becoming obsolete).\r\n\r\nThe recent fixes affect both quantization (which you are running on your hose) and inference, which I understand you are running using the tegra-ugly_hack. Apart from updating that to get the fixes, you could try manually replicating the relevant change: https://github.com/tensorflow/tensorflow/commit/2e85e1c2c1d8d3b9f404fd110ca7ffd34a046f65\r\n", "Thanks for your fix! May I know if I need to rebuild the `graph_transform` tool via bazel with a fresh install of TF? I am still using TF 1.01, and have rebuilt the `graph_transform` tool with bazel, and used only `quantize_weights` from the `graph_transform` tool. However, I'm still getting the very random classes predicted with low accuracy.\r\n\r\nI have also tried `quantize_weights` and `quantize_nodes` together, which gives me a better performance at 43% confidence. However, what's strange is before I rebuilt the tool, using the old `graph_transform` tool, I was getting 46% confidence for my predictions. But using `quantize_graph`, I got 69% confidence instead. To compare, the frozen (not quantized) graph gives me an 81% confidence.\r\n\r\nDo I need to update the entire TF 1.01 on my host machine? If I do so, will there be huge conflicts in developing for Jetson which uses the ugly hack? Is there any way to upgrade the Jetson's ugly hack to a later version? \r\n", "1) Yes, please build the graph_transform tool with a fresh install. It is OK to keep both on your host machine, just download the fresh install to a separate directory.\r\n\r\n2) Be sure to use all the transforms for 8-bit calculations:\r\n```\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_model_inception_resnet_v2.pb \\\r\n--out_graph=./quantized_weights_and_nodes_inception_resnet_v2.pb \\\r\n--inputs='Placeholder_only' \\\r\n--outputs='InceptionResnetV2/Logits/Predictions' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\n\r\n3) Sorry I can't help with the Jetson's hack. As I said earlier, you can try applying the relevant changes manually.", "@andrehentz  I tried to building with the fresh install but have gotten this error instead:\r\n\r\n```\r\nERROR: /home/kwotsin/tensorflow-android/tensorflow/tensorflow/core/BUILD:1363:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/kwotsin/tensorflow-android/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/kwotsin/tensorflow-android/tensorflow/tensorflow/core/BUILD:1363:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/kwotsin/tensorflow-android/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/kwotsin/tensorflow-android/tensorflow/tensorflow/core/BUILD:1363:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/kwotsin/tensorflow-android/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/tools/graph_transforms:transform_graph' failed; build aborted.\r\nINFO: Elapsed time: 0.817s\r\n```\r\n\r\nAre some things left out in the BUILD files? I am not very sure about this as I've never seen how such a code is being written or how it's getting processed.\r\n", "Sorry @kwotsin . I don't know what those errors are about. I usually build like this:\r\n```\r\n  $ git clone https://github.com/tensorflow/tensorflow \r\n  $ cd tensorflow\r\n  $ ./configure\r\n  $ bazel build tensorflow/tools/graph_transforms:all\r\n```\r\nI just tried them.\r\n(The full instructions are on tensorflow's website)", "@andrehentz the transform_graph works very well currently. I am seeing a better accuracy compared to using quantize_graph, and the inference speed is also faster. \r\n\r\nAs for the Jetson, I realized the problem of running the quantized graph obtained from the transform_graph tool comes from the TF version (1.0 alpha) not having any quantized operation functions (e.g. QuantizeMul) at all. So I'm trying to update the TF version to something higher that has these operations available.", "@kwotsin I experience error \"No op named QuantizedMul\", so as you specified I tried upgrading tensorflow to 1.1 version. I am new to TensorFlow, I hope the configuration I select is okay. I need the quantized model for image classifier application (I am not using GPU). When I try to ./configure, I am leaving only default setting without enabling any support like MKL, GPU, CUDA, etc. \r\n\r\nI am using the above command given by andrehentz to do the quantization. However, I get error \"ValueError: No op named QuantizedAdd in defined operations\" now. Am I missing some necessary step that the model generated by transform_graph doesn't work for me?  \r\n\r\nI am not getting response for this problem if I post in StackOverflow. I hope to find help here. Thank you.", "What platform are you running the device on? You mentioned you tried upgrading the tensorflow version - did you succeed in the end? What is your version of TF now? IMO, I think those operations should come prepackaged with the newer versions of TF, so there shouldn't be anything extra to be built (not very sure about this though).", "@kwotsin I am running on Mac. I have Tensorflow verison 1.1, the upgrade was successful. after that I did \"bazel build tensorflow/tools/graph_transforms:all\" and did the transform_graph operation using :\r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_model_inception_resnet_v2.pb \\\r\n--out_graph=./quantized_weights_and_nodes_inception_resnet_v2.pb \\\r\n--inputs='Placeholder_only' \\\r\n--outputs='InceptionResnetV2/Logits/Predictions' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n\r\nHowever I get \"ValueError: No op named QuantizedAdd in defined operations.\" when \"tf.import_graph_def(graph_def, name='')\" is called.", "@rajnamitha Upgrade to r1.2, and you will not face this issue.  Also while using the transformed graph if you get following error in r1.2 : \r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n\r\nTake off  'remove_nodes(op=Identity, op=CheckNumerics)' from the 'transforms' part, refer issue https://github.com/tensorflow/models/issues/1721\r\n\r\n", "@rajnamitha I am using python 2.7, so cannot comment about the behaviour in python 3.5\r\n\r\nI think there is a way to verify if QuantizedAdd op is supported by your build!\r\nIn tensorflow/core/kernels:BUILD, look for the following part. \r\n\r\ntf_kernel_library(\r\n    name = \"quantized_ops\",\r\n....)\r\n\r\nIf there is a \"quantized_add_op.cc\" listed in it, then I think this op is supported by your build. And there should be a file with the same name in 'core' folder.\r\n\r\nI came to this conclusion after comparing the BUILD file of r1.1 and r1.2\r\nHowever even I am facing the accuracy drop issue with the quantized model.\r\n ", "@chandrakantkhandelwal : I have tried installing tensorflow from source code as mentioned in my issue (10739). In that I don't face QuantizedAdd during the import model step. Whereas, it throws error during inference when I quantize the model using \"Graph_transform\" tool. I was told to modify quantize node option to quantize_nodes(fallback_min=-10, fallback_max=10). Which again provides 0 accuracy in result.\r\nhttps://github.com/tensorflow/tensorflow/issues/10739\r\n\r\n And yes, drop in accuracy is expected for quantized model as it uses 8-bit calculation unlike conventional 32-bit. But for inception-resnet-v2 model accuracy is 0% (when I quantize it using quantization tool). If resnet-v2 model is not meant to be quantized is still a question. Because for inception-v3 and inception-v4 there was slight drop in accuracy but not like in resnet-v2 model.", "@rajnamitha you can try removing `remove_nodes` and `quantize_nodes` and see if the accuracy drop still persists.", "This is an old issue. Can you please retry using the newer version of TensorFlow. I will close this issue, please create a new issue using one of the issue templates if you are still facing this issue? "]}, {"number": 9300, "title": "Store step stats in benchmark model and use python timeline to visual\u2026", "body": "\u2026ize it\r\n\r\nSince [timeline](https://github.com/tensorflow/tensorflow/blob/27711108b5fce2e1692f9440631a183b3808fa01/tensorflow/python/client/timeline.py) is a python only visualization tool. To visualize benchmark_model, we need to write collected **StepStats** to a file and then use timeline to generate JSON-formatted file in Chrome Trace format.", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "@YuMS Any update on this?", "I'll close this, as it appears inactive. Please create a new PR if you would like to pick this back up."]}, {"number": 9299, "title": "Making TF consume least GPU memory as default", "body": "Hi there, \r\n\r\nBy default, TF takes all GPU memory available on the machine, even if very few memory are actually needed. This would prevent running other processes and lead to Out of Memory errors. \r\n\r\nAlthough we can use GPU options to set the GPU amount, but sometimes we are running others' codes and do not want to bother or forget to add GPU configurations to make it consumes less memory. Consider most people are using shared machines, it would bring difficulties for other users of GPU server very often.\r\n\r\nThe default configuration of using all GPU memory may have some benefits that I am not totally aware of, but I wonder if the benefits outweigh the drawbacks of doing that. Therefore, I wonder if TF could make it consume least GPU memory as default. ", "comments": ["Thanks for your input @wagonhelm ! Unfortunately that's not something that we'll do at this point.\r\n\r\nYou can use http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory (see both answers)."]}, {"number": 9298, "title": "[cmake/feature request] provide USE_SYSTEM_xxx flags to skip dependency builds", "body": "Tensorflow's cmake build will download and compile all of its dependency. Julia[1] does something similar to maximize its performance. However Julia provided some optional flags such as `USE_SYSTEM_FFTW` so users can compile Julia against the FFTW library provided by system package manager (e.g. `apt/dpkg`). I wish Tensorflow's cmake build could provide similar options to skip building its dependencies and just use the system provided one.\r\n\r\n[1] https://github.com/JuliaLang/julia/blob/master/Makefile#L243", "comments": ["This seems like a reasonable request, although it's one we're unlikely to use for releases because linking together one huge static library from the exact versions of the library dependencies is easier to reason about. (Indeed, most of the trouble with the Windows version has been \"DLL hell\" of one form or another.)\r\n\r\nHowever, I can see why this would be useful for local source development, and would be happy to review contributions to the CMake build that allow this!", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "This should be reopened."]}, {"number": 9297, "title": "Retrieve a graph or model without copying the graph construction code", "body": "Hi,\r\n\r\nLet's say I have defined a graph and trained the weights in file \"myTrain.py\" and I saved the model using `tf.train.saver()` in a directory.\r\n\r\nI want to have a tester file (let's name it \"myTest.py\") which doesn't contain the definition of the graph, but be able to load the saved graph and also the trained weights and I can use it as I was using in \"myTrain.py\". For example, pass a new batch with `feed_dict` and get the desired outputs.\r\n\r\nI know there are many sources about how to save and restore the graph, but all of them are confusing or they have the definition of the graph as a piece of code written before loading the saved model which I believe shouldn't be needed.\r\n\r\nI appreciate any help.\r\n\r\nThanks\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9296, "title": "Tensorflow 1.1 build breaks on CUDA code", "body": "### System Information\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*: Linux Ubuntu 17.04\r\n- *TensorFlow installed from (source or binary)?*: source\r\n- *TensorFlow version* (use command below): v1.1.0-rc2\r\n- *Bazel version (if compiling from source)*: 0.4.5\r\n- *CUDA/cuDNN version*: 8.0/5.1.5\r\n- *GPU Model and Memory*:M2000M\r\n\r\n### Describe the problem clearly\r\n\r\nFails to build after upgrade to Ubuntu 17.04 (from 16.10), using the same compiler (gcc 5.4.1) and compiler flags. \r\n\r\nBuild command is `bazel build --config=opt --config=cuda //tensorflow/\r\ntools/pip_package:build_pip_package`\r\n\r\nI've tried compiling with the default -march=native and -march=core-avx-i as well as using command given [here](http://stackoverflow.com/a/41584791)\r\n\r\n### Source Code / Logs\r\n`From Compiling tensorflow/core/kernels/fake_quant_ops_gpu.cu.cc:\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9218): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"`\r\n\r\nSame error is then repeated for different parameter types (ex. const long/int *)\r\n", "comments": ["Can you paste the log somewhere? Where does it trigger from `fake_quant_ops_gpu.cu.cc`?", "Ok, compiling with gcc-4.9 seems to work. Closing since I'm pretty sure this was a compiler issue which I believe is explicitly not supported by cuda 8.0 (ie. gcc 5.4.1). \r\n\r\nSorry for not trying that before posting here. I assumed it would work since I thought it was the same compiler as before the upgrade. Also apologies for the slow follow up.\r\n\r\nRegarding the question, I don't believe it was specific to fake_quant_ops_gpu.cu.cc but rather was triggered from the first gpu op the encountered (ex. gru_ops_gpu.cu.cc, l2loss_op_gpu.cu.cc).\r\n\r\nThank you for the quick response."]}, {"number": 9295, "title": "Fix a small copy/paste typo in a comment", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9294, "title": "Tutorial has error: Recurrent Neural Networks", "body": "Tutorial URL: https://www.tensorflow.org/tutorials/recurrent\r\nI'm going through the tutorial listed above and I think there is a mistake in the very first code example:\r\n```\r\nlstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\r\n# Initial state of the LSTM memory.\r\nstate = tf.zeros([batch_size, lstm.state_size])\r\n```\r\nAn error is reported for the third line:\r\n```\r\nValueError: setting an array element with a sequence.\r\n```\r\nIf one prints the `lstm.state_size` object (where say, lstm_size = 50) one finds:\r\n```\r\nLSTMStateTuple(c=50, h=50)\r\n```\r\nI'm guessing this should be:\r\n```\r\nlstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\r\n# Initial state of the LSTM memory.\r\nstate = tf.zeros([batch_size, lstm_size])\r\n```\r\nBut frankly there are numerous other errors in this tutorial as well, so I'm not sure.  I will continue to report them as I find them.\r\nVersion: tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl\r\nRunning on Ubuntu 14.04", "comments": ["Thanks for reporting! It would be great if you could update us on additional errors as you follow along.\r\n\r\n@ebrevdo FYI", "Will do.  The API change for TF1.0.1, especially for the RNN classes, has caused confusion and there is a real shortage of working RNN code samples.  The vast majority of the code examples on StackOverflow are from mid-2016 and they no longer work with the new API.  Thanks.", "Indeed, the tutorial has a one line warning:\n\n\"The basic pseudocode is as follows:\"\n\nwe should really just highlight that in red above each code example.\n\nAnd at the top we should link to the real code:\n\nhttps://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb\n\nthat code looks to be up to date.\n\nOn Tue, Apr 18, 2017 at 11:59 AM, Kevin Shaw <notifications@github.com>\nwrote:\n\n> Will do. The API change for TF1.0.1, especially for the RNN classes, has\n> caused confusion and there is a real shortage of working RNN code samples.\n> The vast majority of the code examples on StackOverflow are from mid-2016\n> and they no longer work with the new API. Thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9294#issuecomment-294946512>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1DMKf_TVeAvjH92tx9YW599CEmsks5rxQgOgaJpZM4NAvZe>\n> .\n>\n", "@kevinashaw, @ebrevdo, @drpngx, I just ran across the same problem. My concern is that `state = tf.zeros([batch_size, lstm_size])` might not capture what needs to be captured. In the current API, the `lstm_state` is a tuple of `(c, h)` or `(memory, output)`, which should have size `[lstm_size, 2]`. Would this then require us to write `state = tf.zeros([batch_size, lstm_size, 2])`?\r\n\r\nI am working through this as I type this. So far I have not quite hit on the solution.\r\n\r\n**Edit:** perhaps `state = lstm.zero_state(batch_size, dtype=tf.float32)`, or some other value for `dtype`? This creates a zeroed-out state tensor given the LSTM's existing state size, shape, etc.", "@ebrevdo : Thank you for your help!\r\nI saw the \"pseudocode\" warning, but those two lines of code look like they should be self-contained.  Also, regardless of highlighting in red, the pseudocode should be solid starting point for working code.  There is so little working TF RNN code after the API change that this one document becomes a critical resource.\r\nI can also confirm that the repo [link](https://github.com/tensorflow/models) in the tutorial does not link to the file that you [referenced](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb).  Thank you for providing.  I will note that that code, as a tutorial example, is somewhat opaque.", "@ebrevdo @drpngx : Would you like me to report additional errors on this thread, or on a new thread?", "I wonder if you could just edit the doc and send a PR when done\n\nOn Apr 18, 2017 1:41 PM, \"Kevin Shaw\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> @drpngx <https://github.com/drpngx>\n> : Would you like me to report additional errors on this thread, or on a new\n> thread?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9294#issuecomment-294974763>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbYtcZWmcfMVUuWEC6aTIdx-psZRBks5rxR_2gaJpZM4NAvZe>\n> .\n>\n", "I would be glad to, but I'm not clear on what 'truth' is at this point.\r\nHere is the second code sample:\r\n```\r\n# Placeholder for the inputs in a given iteration.\r\nnum_steps  = 20\r\nbatch_size = 1000\r\nlstm_size  = 50\r\nwords = tf.placeholder(tf.int32, [batch_size, num_steps])\r\n\r\nlstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\r\n# Initial state of the LSTM memory.\r\ninitial_state = state = tf.zeros([batch_size, lstm.state_size])\r\n\r\nfor i in range(num_steps):\r\n    # The value of state is updated after processing each batch of words.\r\n    output, state = lstm(words[:, i], state)\r\n```\r\nWhich yields: `ValueError: setting an array element with a sequence.` on the `tf.zeros` line.\r\nI'm working on finding an executable replacement, but haven't found it yet.  I'm not clear on what the proper shape for the `state` should be.", "Following @Engineero's recommendation, I can use `tf.zeros([batch_size, lstm_size, 2])` and I then get an error on the last line (`lstm()`) with:\r\n```\r\nTypeError: 'Tensor' object is not iterable.\r\n```\r\nWhich is opaque to me, since it does not look like we are iterating on a Tensor. ", "@kevinashaw I think it needs to be `state = lstm.zero_state(batch_size, dtype=tf.float32)` or some other value for `dtype`, depending on the problem. This should create a zeroed-out initial state tensor based on the network's state parameters.\r\n\r\nI am still having some other issues, but I think this got me through the initial state declaration.", "@Engineero : Thanks.\r\nI have updated the test code, but it still fails with the same error `not iterable` message:\r\n```\r\n# Placeholder for the inputs in a given iteration.\r\nnum_steps  = 20\r\nbatch_size = 1000\r\nlstm_size  = 50\r\nwords = tf.placeholder(tf.int32, [batch_size, num_steps])\r\n\r\nlstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\r\n# Initial state of the LSTM memory.\r\ninitial_state = state = tf.zeros(batch_size, dtype=tf.float32)\r\n\r\nfor i in range(num_steps):\r\n    # The value of state is updated after processing each batch of words.\r\n    output, state = lstm(words[:, i], state)\r\n```", "Not to be too nit-picky about the tutorial, but the example code uses `state_is_tuple=False` which is now deprecated.  So the sample code should probably be updated.  I, myself, am still in clear on how this changes the code, but I hope to work it out soon. ", "@kevinashaw that is not what I wrote. You are still using `tf.zeros(...)`. Should be `lstm.zero_state(...)` I think.", "My bad.  Sorry.  Retrying...", "With the new state initialization line:\r\n```\r\ninitial_state = state = lstm.zero_state(batch_size, dtype=tf.float32)\r\n```\r\nThe new error is thrown on the last line: \r\n```\r\nValueError: linear is expecting 2D arguments: [TensorShape([Dimension(1000)]), \r\n    TensorShape([Dimension(1000), Dimension(50)])]\r\n```\r\nHere is the full code sample:\r\n```\r\n# Placeholder for the inputs in a given iteration.\r\nnum_steps  = 20\r\nbatch_size = 1000\r\nlstm_size  = 50\r\nwords = tf.placeholder(tf.int32, [batch_size, num_steps])\r\n\r\nlstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\r\n# Initial state of the LSTM memory.\r\ninitial_state = state = lstm.zero_state(batch_size, dtype=tf.float32)\r\n\r\nfor i in range(num_steps):\r\n    # The value of state is updated after processing each batch of words.\r\n    output, state = lstm(words[:, i], state)\r\n```\r\nIf one does the following to determine the shape and type of `state`:\r\n```\r\nprint(tf.shape(state))\r\nprint(state)\r\n```\r\nthe following is printed:\r\n```\r\nTensor(\"Shape:0\", shape=(3,), dtype=int32)\r\nLSTMStateTuple(c=<tf.Tensor 'zeros_11:0' shape=(1000, 50) dtype=float32>, h=<tf.Tensor 'zeros_12:0' shape=(1000, 50) dtype=float32>)\r\n```", "@kevinashaw that's where I am now too. It seems like we are not presenting arguments to the `__call__` method of `lstm` correctly. I am working a different problem, but same issue.", "@Engineero : Here is a dumb question that you may be able to help with.  If TF is capable of auto-unrolling LSTMs, then why do we need to use the `for` loop (in the sample above) to feed in the words one at a time?  I've never understood this.", "You don't.  We should really change the tutorial to use tf.nn.dynamic_rnn\n-- which handles most of this for you.\n\nOn Tue, Apr 18, 2017 at 2:28 PM, Kevin Shaw <notifications@github.com>\nwrote:\n\n> @Engineero <https://github.com/Engineero> : Here is a dumb question that\n> you may be able to help with. If TF is capable of auto-unrolling LSTMs,\n> then why do we need to use the for loop (in the sample above) to feed in\n> the words one at a time? I've never understood this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9294#issuecomment-294987951>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4IAH-RQE3--SUoWa8SY301zh6GEks5rxSsagaJpZM4NAvZe>\n> .\n>\n", "It seems the unrolling should only be necessary for the training process.  At inference-time, it should function on a sample-by-sample basis.  But I guess it is just a memory vs speed trade-off question.", "@ebrevdo : could we get a short snippet of sample code using `tf.nn.dynamic_rnn`?  :-)", "While this tutorial is just a little outdated (they still point to RNNCells\nin tf.nn.rnn_cell instead of tf.contrib.rnn), a lot of the examples are\nstill very good:\n\nhttp://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n\nOn Tue, Apr 18, 2017 at 2:42 PM, Kevin Shaw <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> : could we get a short snippet of\n> sample code using tf.nn.dynamic_rnn? :-)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9294#issuecomment-294991817>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-CBYbG8zxpfR9DY5P_NHWDo-HxPks5rxS43gaJpZM4NAvZe>\n> .\n>\n", "@ebrevdo this may be a separate issue, but on that tutorial that you linked, do you have any idea how one should replace `tf.contrib.learn.run_n(...)` calls? It gives a deprecation warning with the following:\r\n\r\n> Instructions for updating:\r\n> graph_actions.py will be deleted. Use tf.train.* utilities instead. You can use learn/estimators/estimator.py as an example.\r\n\r\nThe file this sends you to does not seem to be very helpful.\r\n\r\nThanks!\r\n", "@martinwicke what's the replacement for tf.contrib.learn.run_n?", "We have none. It was never used after initial experimentation, so we decided to scrap it altogether. You can get the same behavior using `tf.contrib.training.train`, if you need feeds, with a Hook to add the feeds.", "@kevinashaw For the record, I found [this gist][1] to be helpful. He also has [a blog][2] where he talks about some of the undocumented features of an older version of TensorFlow. You still have to change out `tf.nn.rnn_cell` for `tf.contrib.rnn`, but this helped me get something going.\r\n\r\nThat said, I am now having an issue where my network is not updating at all. I train it for several epochs and get the exact same results every time.\r\n\r\n[1]: https://gist.github.com/danijar/61f9226f7ea498abce36187ddaf51ed5\r\n[2]: http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/", "@Engineero : Thanks!", "Following the discussion in #9699, I've at least found a solution to the initial_state problem.  It would probably make sense to reference it in the new tutorial/documentation.   The problem with the initial_state is not that `zero_state` produces incorrect results :\r\n```\r\ninitial_state = lstm_cells.zero_state(batch_size, tf.float32)\r\n```\r\nIts that the batch_size value is a constant.  For the training data, the size of the batch will *always* match the batch_size.  But for Testing data it usually never will.  Testing is done on a much large set, or atleast a set that has a size that varies from the batch_size.  Hence, for testing data, the `initial_state` from the test data will still have a size of `batch_size` but the test data itself will not.\r\nThe solution is surprisingly simple, just let batch_size vary according to the data set brought in:\r\n```\r\nbatch_size_var  = tf.shape(Xin)[0]\r\ninitial_state = lstm_cells.zero_state(batch_size_var, tf.float32) \r\n```\r\nWhere Xin is the data input tensor.  Now `initial_state` will always match the size of the data set.", "Right. tf.nn.dynamic_rnn will call zero_state for you with the right value\nof you pass in just the dtype argument and not an initial state.\n\nOn May 10, 2017 12:12 AM, \"Kevin Shaw\" <notifications@github.com> wrote:\n\n> Following the discussion in #9699\n> <https://github.com/tensorflow/tensorflow/issues/9699>, I've at least\n> found a solution to the initial_state problem. It would probably make sense\n> to reference it in the new tutorial/documentation. The problem with the\n> initial_state is not that zero_state produces incorrect results :\n>\n> initial_state = lstm_cells.zero_state(batch_size, tf.float32)\n>\n> Its that the batch_size value is a constant. For the training data, the\n> size of the batch will *always* match the batch_size. But for Testing\n> data it usually never will. Testing is done on a much large set, or atleast\n> a set that has a size that varies from the batch_size. Hence, for testing\n> data, the initial_state from the test data will still have a size of\n> batch_size but the test data itself will not.\n> The solution is surprisingly simple, just let batch_size vary according to\n> the data set brought in:\n>\n> batch_size_var  = tf.shape(Xin)[0]\n> initial_state = lstm_cells.zero_state(batch_size_var, tf.float32)\n>\n> Where Xin is the data input tensor. Now initial_state will always match\n> the size of the data set.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9294#issuecomment-300395826>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6FFXxRWKcofk4unjdfMrJVZ1DmGks5r4WNjgaJpZM4NAvZe>\n> .\n>\n", "Thanks!  The documentation regarding that feature is somewhat thin for tf.nn.dynamic_rnn. \n\n> On May 10, 2017, at 2:30 PM, ebrevdo <notifications@github.com> wrote:\n> \n> Right. tf.nn.dynamic_rnn will call zero_state for you with the right value\n> of you pass in just the dtype argument and not an initial state.\n> \n> On May 10, 2017 12:12 AM, \"Kevin Shaw\" <notifications@github.com> wrote:\n> \n> > Following the discussion in #9699\n> > <https://github.com/tensorflow/tensorflow/issues/9699>, I've at least\n> > found a solution to the initial_state problem. It would probably make sense\n> > to reference it in the new tutorial/documentation. The problem with the\n> > initial_state is not that zero_state produces incorrect results :\n> >\n> > initial_state = lstm_cells.zero_state(batch_size, tf.float32)\n> >\n> > Its that the batch_size value is a constant. For the training data, the\n> > size of the batch will *always* match the batch_size. But for Testing\n> > data it usually never will. Testing is done on a much large set, or atleast\n> > a set that has a size that varies from the batch_size. Hence, for testing\n> > data, the initial_state from the test data will still have a size of\n> > batch_size but the test data itself will not.\n> > The solution is surprisingly simple, just let batch_size vary according to\n> > the data set brought in:\n> >\n> > batch_size_var = tf.shape(Xin)[0]\n> > initial_state = lstm_cells.zero_state(batch_size_var, tf.float32)\n> >\n> > Where Xin is the data input tensor. Now initial_state will always match\n> > the size of the data set.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/9294#issuecomment-300395826>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/ABtim6FFXxRWKcofk4unjdfMrJVZ1DmGks5r4WNjgaJpZM4NAvZe>\n> > .\n> >\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/9294#issuecomment-300617582>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AGz330YtgDPyVRCxft8ZrEt0wNMS-Rnpks5r4iyFgaJpZM4NAvZe>.\n> \n\n", "@martinwicke , for https://github.com/dennybritz/tf-rnn/blob/master/sequence_example.ipynb  as mentioned eariler changed  tf.contrib.learn.run_n(...) to tf.contrib.training.train in the code, but it seems running into the inifinite loop.", "Yes, you'll have to limit the number of steps using a [`StepCounterHook`](https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook) or make sure your input function throws an appropriate exception after the training data is depleted (either `StopIteration` or `OutOfRangeError`).", "@kevinashaw Did you finally find a solution for the linear problem?\r\n\r\n    ValueError: linear is expecting 2D arguments: [TensorShape([Dimension(1000)]), \r\n    TensorShape([Dimension(1000), Dimension(50)])]", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@MarkDaoust @mhyttsten, something to consider when thinking about the fate of our tutorials.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "While it could use some modernization, the tutorial is working fine now. ", "Thanks to @kevinashaw for pointing out this problem. \r\nAs far as I can see @MarkDaoust @drpngx  \r\n the tutorial code posted here and at [TF site](https://www.tensorflow.org/tutorials/recurrent) still has the same errors after nearly 12 months\r\n\r\nI did try making some of the modifications suggested above, they don't appear to solve the issue\r\n\r\n\r\n```\r\nXin =tf.random_normal((100,100))\r\n\r\nbatch_size_var  = tf.shape(Xin)[0]\r\nlstm = tf.nn.rnn_cell.LSTMCell(lstm_size) #with dynamic rnn the reference to .LSTMCell is invalid\r\nhidden_state = lstm_cells.zero_state(batch_size_var, tf.float32) \r\ncurrent_state = lstm_cells.zero_state(batch_size_var, tf.float32)\r\n```\r\nIs there another RNN tutorial anyone can recommend?", "How about try to use `lstm.state_size[0]` or `lstm.state_size[1]` instead of just using `lstm.state_size`"]}, {"number": 9293, "title": "Force new instance creation in MultiRNNCell", "body": "This change was missed when other similar ones were made in https://github.com/tensorflow/tensorflow/commit/54d50ffec8df4f748694632dbe5ebde9971e2c9e", "comments": ["Can one of the admins verify this patch?", "See also discussion of another similar issue with @nealwu at https://github.com/tensorflow/tensorflow/pull/5599#issuecomment-293514292", "Looks reasonable. Adding @ebrevdo for review.", "LGTM", "@tensorflow-jenkins test this please", "The test failure in MacOS CPU Tests is unrelated. Merging PR."]}, {"number": 9292, "title": "xorshift128+ version of (stateless) random ops", "body": "Currently, TensorFlow's random numbers use the Philox counter mode generator, which is extremely easy to parallelize on both CPU and GPU.  This applies to both the normal stateful ops and the new [`tf.contrib.stateless`](https://github.com/tensorflow/tensorflow/commit/cc45456e4ad0eff16127d1727d0cf48afb71ca0e) versions with custom seeding.\r\n\r\nxorshift128+ is a simpler generator that could conceivably speed up random number generation.  Unfortunately, it is not a counter mode generator, and is thus difficult to parallelize or use safely in a random access setting.\r\n\r\nUntil now!  Commit https://github.com/girving/tensorflow/commit/60abb26f528f53e7692edb3e89489a69b59ae83e on branch https://github.com/girving/tensorflow/tree/xorshift implements random access into the xorshift128+ generator in a reasonably efficient manner, using some finite field machinery.  Specifically, jumps in xorshift128+ are represented as elements of the finite field GF(2^128), composed to produce other jumps, then mapped through linear maps to produce xorshift128+ values.\r\n\r\nHowever, the code is a proof of concept.  A decent amount of further work would have to be done to get committed to TensorFlow.  In particular, the parallelism code on both CPU and GPU would have to be written, by computing one jump per thread of execution (many jumps can be computed more cheaply vs. one at a time).  The current code is also nonportable: it assumes special  instructions for carryless multiplication of polynomials over GF(2).  These instructions are available on recent Intel and AMD CPUs, but a slow path would need to be written to handle everything else.\r\n\r\nAlso, whether the result would actually be faster is an open question.\r\n\r\nI don't have time to do the remaining work, so I am leaving this here as a project in case someone wants to take it on with my help.", "comments": ["Is this still valid? Was this related only to old conrtib?", "This was somewhat of a lark.  Code would still work with minor changes if anyone ever wants to use it, but definitely fine to close this issue.  I'll leave my branch up in case people feel logic exotic use of xorshift in the future."]}, {"number": 9291, "title": "Docs: Fix #9074 in release branch", "body": "Change: 153414133", "comments": ["Not 100% certain about documentation changes, over to @MarkDaoust  for a quick glance.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 9290, "title": "Update adding_an_op.md", "body": "The variable name for the output should better be consistent with the name below.", "comments": ["Can one of the admins verify this patch?", "@kumasento, can you explain what \"below\" refers to? I can't find any instances of \"output_flat\" or \"flat\" in this markdown file. So I'm confused about what is inconsistent.", "@caisq Thanks for your comments. Maybe you could try to search with `output\\_flat`? Or you could find it in the section \"Op registration > Attrs\", the code snippet in that section contains an example that uses `output_flat`.\r\n\r\nCheers!", "Just above this section: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/extend/adding_an_op.md#attr-types-attr-types", "@kumasento ok - I overlooked that. Thanks.", "Cheers!"]}, {"number": 9289, "title": "Branch 153438895", "body": "", "comments": ["The test failure in \"Linux CPU Test (Python 3)\" build is a known flake."]}, {"number": 9288, "title": "Java API - sophisticated example", "body": "Dear All,\r\n\r\nI have seen the [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java) for importing model created and trained in Python imported into Java code and used for predictions.  However, I had some problems understanding what was actually going on especially in [this block](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java#L92-L101) and [this block](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java#L156-L207). Would it be possible to get some more documentation on it?\r\nMoreover, I know the Java API is still under construction. However, I would be interested in if it is possible to see some more sophisticated examples, if it is possible including:\r\n\r\n- importing model into Java and then performing training on the model\r\n\r\n- implementing, training, evaluating, saving, loading a model from scratch in Java\r\n\r\nThank you for any help!\r\n\r\nCheers,\r\nPeter", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\n@asimshankar "]}, {"number": 9287, "title": "Update adding_an_op.md to suppress logging", "body": "This `TF_INC` variable will contain redundant logging messages like: \r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n```\r\n... without redirecting the output.", "comments": ["Can one of the admins verify this patch?", "I have double checked and found that the logging output will not pollute the `TF_INC` variable's content. Sorry for the disturbance."]}, {"number": 9286, "title": "Multiplicative Integration Recurrent Neural Networks", "body": "I implemented Multiplicative Integration variants of recurrent neural networks\r\n(RNN, GRU and LSTM) proposed in \r\n\r\n  Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov,\r\n  On Multiplicative Integration with Recurrent Neural Networks. NIPS, 2016.\r\n  https://arxiv.org/abs/1606.06630\r\n\r\nThese RNNs are: \r\n   - MultiplicativeIntegrationRNNCell\r\n   - MultiplicativeIntegrationGRUCell\r\n   - MultiplicativeIntegrationLSTMCell\r\n   - _multiplicative_integration as a helper function\r\nin\r\ntensorflow/contrib/rnn/python/ops/rnn_cell.py\r\n\r\nTest codes are in\r\ntensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "As googlebot said, I changed my email address of github just before I signed the CLA with the changed email address.\r\nI guess googlebot added cla: no, because I did not log out before signing CLA.\r\n\r\nWhat should I do?", "You need to change the email address in your commits. You can do that with git commit --amend, but depending on how good your git-fu is, that may be too much trouble. If it is, I would recommend making a new PR with a single, clean (new email) commit.", "There's also conflicts here too.  I'd suggest making a new PR with the correct email after syncing to HEAD.  Sorry about this!"]}, {"number": 9285, "title": "TensorBoard filter regresson", "body": "### System information\r\n\r\nDocker image `tensorflow/tensorflow:nightly` (or 1.1.0rc2)\r\n\r\n### Describe the problem\r\n\r\nStart a tensorboard process\r\n\r\n```\r\ntensorboard --logdir /efs/log/atari\r\n```\r\n\r\nand try and filter. It does not have any effect.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/6200749/25130135/6499f98c-2441-11e7-8050-34cce130fef0.png)\r\n", "comments": ["This isn't a performance bug; filtering on rc2 for tensorboard is just broken.", "@dandelionmane It looks like TF 1.1 is shipping. Did this get fixed?", "Ping on this. Looks like filtering is broken in the TF1.1 wheel on Pypi.", "This is a pretty significant usability problem with TensorBoard. We often run TensorBoard on directories with many runs, and being unable to filter runs by name makes it very difficult to do useful things with TensorBoard.", "Confirming filtering is broken in TF 1.1 Linux CPU PyPI wheel.", "Also reported in #9519.", "Broken for me as well.\r\n\r\nOS: Linux Ubuntu 16.04\r\ninstalled binary via pip3", "This regression happened somewhere between rc1 and rc2, BTW.", "Thanks for the report. This is fixed at master, so the bug won't be present in the next release.", "I am seeing this fixed on the nightly. Thanks!", "@Yup, installed the nightly, seems to be fixed. Maybe it's that crazy html5lib-0.9999 dependency that's fixing it?"]}, {"number": 9284, "title": "Broadcasting support in `tf.where`", "body": "`tf.where` does not support broadcasting like its numpy equivalent at the moment. How easy would it be to add broadcasting? \r\n\r\nHere are some examples.\r\n\r\n```python\r\ncondition = np.random.normal(0, 1, (3, 5, 1, 1)) < 0\r\nx = np.zeros((7, 11))\r\ny = np.ones((7, 11))\r\n\r\nnp.where(condition, x, y).shape  # (3, 5, 7, 11)\r\ntf.where(condition, x, y)\r\n\r\n>>> InvalidArgumentError: Shapes must be equal rank, but are 2 and 4 for 'Select_2' \r\n>>> (op: 'Select')  with input shapes: [3,5,1,1], [7,11], [7,11].\r\n```\r\n\r\n```python\r\ncondition = np.random.normal(0, 1, (3, 5, 1, 1)) < 0\r\nx = np.zeros((1, 1, 7, 11))\r\ny = np.ones((1, 1, 7, 11))\r\n\r\nnp.where(condition, x, y).shape  # (3, 5, 7, 11)\r\ntf.where(condition, x, y)\r\n\r\n>>> InvalidArgumentError: Dimension 0 in both shapes must be equal, but are 1 and 3 \r\n>>> for 'Select_3' (op: 'Select') with input shapes: [3,5,1,1], [1,1,7,11], [1,1,7,11].\r\n```\r\n", "comments": ["A workaround might be the following but I don't know how bad the overheads are.\r\n\r\n```python\r\ndef broadcastable_where(condition, x=None, y=None, *args, **kwargs):\r\n    if x is None and y is None:\r\n        return tf.where(condition, x, y, *args, **kwargs)\r\n    else:\r\n        _shape = tf.broadcast_dynamic_shape(tf.shape(condition), tf.shape(x))\r\n        _broadcaster = tf.ones(_shape)\r\n        return tf.where(\r\n            condition & (_broadcaster > 0.0), \r\n            x * _broadcaster,\r\n            y * _broadcaster,\r\n            *args, **kwargs\r\n        )\r\n```", "@aselle is that something we'd want?", "There is also not full support for broadcasting in the other way, when `condition` is smaller than `x` or `y`; as indicated in the docs it only works if `condition` is a vector, but it could be extended to work with `condition` matching an arbitrary number of first dimensions of `x` and `y` (I'm not sure if this is considered also \"broadcasting\") and/or broadcasting singleton dimensions.\r\nNumPy requires `condition` to always have the same rank as `x` and `y`, but it does broadcast singleton dimensions.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any volunteer to send a PR?", "I was trying to add the implementation but noticed that there are some incompatibilities between the current behavior of `tf.where` and the `broadcast` used by other ops in TensorFlow.\r\n\r\nThe current behavior of `tf.where` is to `broadcast` of vector starts with the beginning of the dimension. But the broadcast for other ops is from the end of the dimension.\r\n\r\nFor example, `[16]` and `[16, 2, 8]` is a valid op for `tf.where`. See:\r\nhttps://github.com/tensorflow/tensorflow/blob/faf7f05f5ed3d92405656a318fb2d571a7d31532/tensorflow/python/kernel_tests/cwise_ops_test.py#L1602-L1614\r\n\r\nBut `[16]` and `[16, 2, 8]` will throw out an error for `np.where`:\r\n```python\r\n>>> import numpy as np\r\n>>> \r\n>>> c = np.random.randint(0, 2, 16).astype(np.bool)\r\n>>> x = np.random.rand(16, 2, 8) * 100\r\n>>> y = np.random.rand(16, 2, 8) * 100\r\n>>> \r\n>>> np.where(c, x, y)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nValueError: operands could not be broadcast together with shapes (16,) (16,2,8) (16,2,8) \r\n>>> \r\n```\r\n", "@drpngx Any suggestions on how to resolve this issue?", "Ah, that's unfortunate. We should use a `@compatibility(numpy)` tag to indicate where behavior is different, then implement the feature in a way that's consistent with the current, tensorflow behavior.\r\n", "@drpngx Added a PR #15982 that adds `broadcast=True|False` to switch to bumpy behavior. Not sure this is the best way but please take a look.", "Thanks!"]}, {"number": 9283, "title": "Save model every fixed number of steps with MonitoredTrainingSession", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 0.12\r\n- **CUDA/cuDNN version**: 7.5 / 5.1\r\n- **GPU model and memory**: GTX 1080\r\n\r\n### Describe the problem\r\n\r\nIs it possible to save model every fixed number of steps with MonitoredTrainingSession?\r\nIf not, would be happy to see it as a feature.\r\n", "comments": ["`tf.train.CheckpointSaverHook` can answer your quesion", "Thanks!"]}, {"number": 9282, "title": "Check failed: NDIMS == dims() (2 vs. 1)", "body": "I get the following error after building the graph of my NN. when I try to execute :\r\n\r\nloss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)\r\n\r\nI get this error :\r\n\r\n`F tensorflow/core/framework/tensor_shape.cc:36] Check failed: NDIMS == dims() (2 vs. 1) Asking for tensor of 2 dimensions from a tensor of 1 dimensions`\r\n\r\nI don't succeed to know to which part of the graph this error is related to ... Because, normaly the graph is build, so shapes should be correct ... are not ? \r\n\r\nI am using Tensorflow version 1.0.0 with python API.\r\n\r\nIt could be a bug according to the first reply here : http://stackoverflow.com/questions/43413293/tensorflow-check-failed-ndims-dims-2-vs-1", "comments": ["That seems like a bug. Could you share the simple possible repro you can find?", "I succeed to find the source of the error.\r\nIt was caused by the bad shape of an input tensor of a custom op.\r\nI have create a custom op, coded in C++, used in Python. \r\nI think an improvement could be done in the error message. Indeed, with this line only, it is difficult to find the source of this error. "]}, {"number": 9281, "title": "Close FileWriterCache before moving training directory in estimator_test", "body": "Hopefully this fixes #9185 by releasing some stray file handle.", "comments": ["I triggered http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/3/console\r\n\r\nI hope the setup is the same as tf-master-win-bzl.\r\n\r\n@meteorcloudy FYI.", "@martinwicke I tested on my local machine as well, it doesn't seem to fix the problem.\r\n\r\nBut I found `shutil.copytree(model_dir1, model_dir2)` works, can we use this instead of `os.rename`?", "The problem with that would be that this does not delete the old tree. This test is to ensure that no absolute paths creep in anywhere, so this would defang the test.\r\n\r\nI cannot see what hangs on to a file handle. ", "Perhaps the easiest approach would be to install the current nightly on one of our Windows test VMs, run a modified version of `estimator_test.py` that blocks before the rename, and apply https://technet.microsoft.com/en-us/sysinternals/processexplorer.aspx to the Python interpreter to see what handles are open?", "@mrry It shows this file is in use by python\r\n`c:/tmp/tmp3y2vhvzi/model_dir1/events.out.tfevents.1492671112.TENSORFLOW-JENK`\r\n\r\n\r\n![image](https://cloud.githubusercontent.com/assets/4171702/25217358/d8040456-25a6-11e7-82fe-96477d9d1a13.png)\r\n", "Thanks for tracking that down, Yun! Then I think we have a smoking gun: the `tf.summary.FileWriter` is holding open a file handle after `est1.train()` returns.\r\n\r\nIt looks like this happens deep in the `MonitoredTrainingSession`... and there seems to be a cache(!!) involved:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/summary/writer/writer_cache.py#L27\r\n\r\nAs a hack, you might try invoking `FileWriterCache.clear()` before the rename, but I'm not sure that that's sufficient because deleting a `FileWriter` doesn't appear to `close()` it, so you might need to add a `writer.close()` call somewhere (or maybe add it to the destructor?).", "@mrry Thanks for the hint, Derek! \r\nI tested adding `writer_cache.FileWriterCache._cache[model_dir1].close()` before rename also solves the problem. Does this sounds good to you?", "That'll fix the test, but I'm wondering whether we can get put that line (or some equivalent) somewhere were it will be called automatically when the Estimator owning the model_dir is deleted?", "I'm even wondering whether we should even keep the cache live when any of train(), eval(), predict() exit. They're all meant to be long running and ideally shouldn't leave open file handles hanging around.", "@martinwicke Yes, I totally agree! But I am lack of knowledge how these function works. Can someone with more experience look into this?", "Jenkins, test this please.", "Triggered another Windows build: http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/4/", "I edited FileWriterCache to `close()` all caches it has, and call that in the test. Cleaner than closing the cache directly.", "I'll merge to fix build."]}, {"number": 9280, "title": "Cannot cross compile tf so to android with ndk-r14b", "body": "Some basic infos:\r\n1. Linux 4.10.10-1-ARCH #1 SMP PREEMPT Wed Apr 12 18:50:28 CEST 2017 x86_64 GNU/Linux\r\n2. tensorflow-r1.1 source code\r\n3. bazel 0.4.5\r\n4. android-ndk-r14b\r\n5. clang-3.9\r\n6. gcc 6.3.1\r\nWHEN I cross compile with :\"bazel build //tensorflow/contrib/android: libtensorflow_inference.so --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a --verbose_failures\"\r\nI got an issue:\r\nERROR: /home/mae/tensorflow-r1.1/tensorflow/core/kernels/BUILD:3869:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: clang failed: error executing command \r\n  (cd /home/mae/.cache/bazel/_bazel_mae/991fa79c990635f28b632a15ab1879cf/execroot/tensorflow-r1.1 && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/bin:/opt/cuda/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/meta_support.d '-frandom-seed=bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/meta_support.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/genfiles -iquote external/protobuf -iquote bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/genfiles/external/local_config_sycl -iquote external/gemmlowp -iquote bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/genfiles/external/gemmlowp -isystem external/protobuf/src -isystem bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/genfiles/external/eigen_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-mfpu=neon' '-std=c++11' -DTF_LEAN_BINARY -O2 '--sysroot=external/androidndk/ndk/platforms/android-16/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/kernels/meta_support.cc -o bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-py3-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/meta_support.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/core/kernels/meta_support.cc:18:\r\nIn file included from ./tensorflow/core/kernels/meta_support.h:23:\r\nIn file included from external/gemmlowp/meta/transform_kernels.h:239:\r\nexternal/gemmlowp/meta/transform_kernels_arm_32.h:7925:7: error: inline assembly requires more registers than available\r\n      \"ldr r0, %[input_range_min]\\n\"\r\n      ^\r\nexternal/gemmlowp/meta/transform_kernels_arm_32.h:5506:7: error: inline assembly requires more registers than available\r\n      \"ldr r0, %[input_range_min]\\n\"\r\n      ^\r\n2 errors generated.\r\nTarget //tensorflow/contrib/android: libtensorflow_inference.so failed to build\r\n\r\nTensorflow works well on my PC ,but I need to port my codes to android proj.\r\nAnyone help ? Thanks a lot.\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@MaeThird I meet the same problem as you, then I use `android-ndk-r12b` and it built successful\r\nsee this #8641 ", "@YaoC Thanks.Finally I get this work by your method.Maybe ndk 13+ is incompatible with inline asm somewhere but who knows.", "Just to follow-up here, building for Android with NDK r14b should work as of https://github.com/tensorflow/tensorflow/commit/d410a83207bdf63ead87a145f97377b7eb1f96bf"]}, {"number": 9279, "title": "Add BestSplits op to common windows installation problems.", "body": "Fix #9273 ", "comments": ["yea, only for 1.0.1 and its RCs.", "Here's a link to the other FAQ (about SSE instructions): http://stackoverflow.com/q/43134753", "Added to the table as well!"]}, {"number": 9278, "title": "Ability to Create New Device Context (GPU)", "body": "If I destroyed GPU context for any reason and tried to start new tensorflow session I got this error `CUDA_ERROR_CONTEXT_IS_DESTROYED ` and notebook kernel crashes. So, it will be a nice feature for python users if they can create a new context for tensorflow if one is destroyed.", "comments": ["How did you destroy the GPU context in the first place? Also, what is the stack trace of the error?", "numba.cuda.close()\r\n\r\nhttp://numba.pydata.org/numba-doc/0.32.0/cuda-reference/host.html?highlight=reset#numba.cuda.close", "I don't think you can recover from a context close. This is not a supported scenario, so I am closing this.", "I don't want to recover to a closed context, I want a new context to process tensoerflow operations. As I understand that Tensorflow uses more than one stream within the same context to accelerate data processing from host to device and vice-versa. Someone needs to set these up manually, it will be nice to be handled in one call. \r\n\r\nThanks anyway", "I am not sure I understand. Are you talking about `with tf.device`?", "No....I'll try to respond later in more detail."]}, {"number": 9277, "title": "Fixing anaconda install command", "body": "3.6 is the standard python version of anaconda. In order to get tensorflow installed correctly, version 3.5 is needed. Passing the exact version of python to the anaconda environment is necessary.", "comments": ["Can one of the admins verify this patch?", "@caisq, does this look right to you?", "@martinwicke I'll take a look sometime today or tomorrow.", "@tensorflow-jenkins test this please"]}]