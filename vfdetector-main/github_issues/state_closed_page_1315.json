[{"number": 13656, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 13655, "title": "AttributeError: 'GFile' object has no attribute 'writelines", "body": "Hi when i try to execute this code:\r\n\r\n`from tensorflow.python.platform import gfile\r\n... with gfile.GFile(\"%s_%s\" % (target_path, len_d + len_q), mode=\"w\") as tokens_file:\r\n        tokens_file.writelines(results)...`\r\n\r\nI obtain the following error:\r\n\r\n`AttributeError: 'GFile' object has no attribute 'writelines'`\r\n\r\nWhy GFile haven't writelines attribute in verision 1.2? What method should i use instead? Thanks a lot.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "honestly this would be a great place to put this or maybe even linking the fix for the depreciation instead of just saying we wont help you.", "You're right, this should be treated as a feature request. What do you mean by \"fix for the depreciation\"?\r\n\r\n/CC @rohan100jain @vrv, any plans to add this, to match the interface of Python files?\r\n", "well, I have fixed this problem by refactoring the code to get it to *kinda* run on the newest version of TF. I forked the project and I believe the fix has been made there to at least get around this problem. I currently am working on updating the rest of this to be able to run on the newest version of TF with no problems, however that is proving to be a bit of a task.", "@reedwm I have no personal plans to add this, but it seems easy enough and I'd like to encourage external contributions.\r\n\r\n@kuthedk is it sufficient to just implement this via:\r\n\r\n    def writelines(self, seq):\r\n        for line in seq:\r\n          self.write(line)\r\n\r\n?\r\n\r\n\r\n", "(Specifically, adding that function to file_io.py with tests, etc.)", "@vrv omg I've been commenting in regards to the wrong project. my bad. please forgive me as I've been sick for a while and a lot of stuff has been blurring together. What you have added looks like it would work but what I ended up doing was just a simple 2 liner in the https://github.com/carpedm20/attentive-reader-tensorflow project. there is a PR that I made on that project that also kinda addresses this similar issue."]}, {"number": 13654, "title": "Enable setting HDFS user", "body": "Following issue https://github.com/tensorflow/tensorflow/issues/13627 add enable TensorFlow to access HDFS as a specific HDFS user instead of the the system user owning the Tf process", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@jhseu Pushed the change", "Are you sure it doesn't already work without this change? I vaguely recall testing it and it picked up the username from HADOOP_USER_NAME.", "We have a use case where we run TensorFlowOnSpark from Apache Livy. When we set HADOOP_USER_NAME then we get an impersonation error saying that User_A cannot impersonate User_A, which is correct since we don't give permissions to User_A to impersonate other users, hence not itself.\r\n\r\nNevertheless, when I set HADOOP_USER_NAME Tensorflow does access HDFS as that user, so I suppose there is no need for a patch.", "Thanks. As I understand it, we can close this out. Feel free to reopen if you disagree."]}, {"number": 13653, "title": "Android: Error:(68, 13) Failed to resolve: org.tensorflow:tensorflow-android:+", "body": "Error:(68, 13) Failed to resolve: org.tensorflow:tensorflow-android:+\r\n\r\nAre there some problems with remote repository?\r\n\r\n```\r\nbuildscript {\r\n    repositories {\r\n        jcenter()\r\n        google()\r\n    }\r\n    dependencies {\r\n        classpath 'com.android.tools.build:gradle:3.0.0-beta7'\r\n    }\r\n}\r\n\r\nallprojects {\r\n    repositories {\r\n        jcenter()\r\n        maven {\r\n            url 'https://maven.google.com'\r\n        }\r\n        maven { url \"https://jitpack.io\" }\r\n    }\r\n}\r\n```\r\n\r\nhttps://bintray.com/google/tensorflow/tensorflow-android\r\n\r\n![screenshot_1](https://user-images.githubusercontent.com/8851301/31483788-91917476-af36-11e7-93cd-d9749967982d.png)\r\n", "comments": ["temporary quick solution (without building) to add tensorflow to our android projects\r\n\r\ndownload jar file https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastStableBuild/artifact/out/libandroid_tensorflow_inference_java.jar\r\nand put to **project_name\\app\\libs** folder\r\n\r\nadd `compile files('libs/libandroid_tensorflow_inference_java.jar')` to you app `build.gradle` file\r\n\r\nor just `compile fileTree(include: ['*.jar'], dir: 'libs')`:\r\n\r\n```\r\ndependencies {\r\n    compile fileTree(include: ['*.jar'], dir: 'libs')\r\n    ...\r\n}\r\n```\r\n\r\ndownload **libtensorflow_inference.so** for armeabi-v7a (or for other android device processor architecture) \r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-android/lastStableBuild/artifact/out/native/libtensorflow_inference.so/armeabi-v7a/libtensorflow_inference.so\r\nand put to **project_name\\app\\src\\main\\jniLibs\\armeabi-v7a** folder (or you may need different path, read https://github.com/tensorflow/tensorflow/issues/13653#issuecomment-336516671)\r\n\r\n", "Also getting this problem. \r\n\r\nFor anybody using android make sure you pull the android nightly build from here: http://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/\r\n\r\nNot sure what the difference is since the file structure appears to be the same, but I couldn't get the files that anonym24 suggested working all morning but things magically started working when i found the android nightly build.", "+1\r\nSame here...\r\n", " Failed to resolve: org.tensorflow:tensorflow-android:+ \r\n\r\nsame here... ", "My apologies folks - while re-organizing the bintray repository the tensorflow-android aar artifact was incorrectly disconnected from jcenter. We're working to resolve it ASAP.\r\n\r\nIn the interim, please use the workaround in https://github.com/tensorflow/tensorflow/issues/13653#issuecomment-336038164 or you can temporarily add the tensorflow bintray maven repository  in your build.gradle\r\n\r\n```\r\n...\r\nrepositories {\r\n  jcenter()\r\n  maven {\r\n    url 'https://google.bintray.com/tensorflow'\r\n  }\r\n  ...\r\n```\r\n", "This solution help to compile, but unfortunately the speech demo is not working ", "Same here helps to compile, TF Detect app is crashing.", "@vclteam @vamshikhoslalabs : Could you elaborate a bit on the speech demo not working? For example, can you share some device logs that might shed some light on why the demo may be crashing?\r\n\r\n@andrewharp FYI", "@asimshankar\r\n~~Caused by: java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK~~\r\n~~Same error on all apps built with [this workaround](https://github.com/tensorflow/tensorflow/issues/13653#issuecomment-336038164)~~\r\nI put libtensorflow_inference.so in the wrong folder (it belongs in /android-project/libs/armeabi-v7a/ by default). Now everything works fine. \r\n", "@asimshankar \r\nnative libraries:\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-android/lastStableBuild/artifact/out/native/libtensorflow_inference.so/\r\n\r\nall (not only libtensorflow_inference): https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastStableBuild/artifact/out/native/", "@anonym24 : I didn't quite follow your comment. Could you elaborate on what you meant? Are you able to run the speech demo?\r\n\r\nIt seems @MusaMahmood got things working after placing `libtensorflow_inference.so` in the right folder.\r\n\r\nSo at this point, I'm a bit confused about whether anyone is having a problem with the speech demo ( @anonym24 @vclteam @vamshikhoslalabs )\r\n\r\nIf so, please feel free to file a separate issue so we can keep this one focused on the binaries being available in jcenter (as per https://github.com/tensorflow/tensorflow/issues/13653#issuecomment-336145329 )\r\n\r\nThanks.", "guys, if you use _tensorflow example project_ (not yours)\r\nyou have to determine the correct path for native libraries\r\n\r\nfor newly created projects in Android Studio it is **project_name\\app\\src\\main\\jniLibs\\armeabi-v7a**\r\n\r\nfor _tensorflow example project_ it is **project_name\\libs\\armeabi-v7a** (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)\r\nthe project overrides default path in `build.gradle` file:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/build.gradle#L130\r\n`jniLibs.srcDirs = ['libs']`", "Rather than dealing with individual native .so files, it should be easier just to use the [AAR itself](https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/tensorflow.aar), which includes both the jar and the native libs for 32 and 64 bit architectures. You can import it according to [these instructions](https://stackoverflow.com/questions/29826717/how-to-import-a-aar-file-into-android-studio-1-1-0-and-use-it-in-my-code) on SO:\r\n```\r\nTo import a .aar library:\r\nGo to File>New>New Module\r\nSelect \"Import .JAR/.AAR Package\" and click next.\r\nEnter the path to .aar file and click finish.\r\nGo to File>Project Structure (Ctrl+Shift+Alt+S).\r\nUnder \"Modules,\" in left menu, select \"app.\"\r\nGo to \"Dependencies tab.\r\nClick the green \"+\" in the upper right corner.\r\nSelect \"Module Dependency\"\r\nSelect the new module from the list.\r\n```", "It looks like this is solved.", "please let me know when this issue was fixed.", "The original issue was missing android artifacts from jcenter, leading to compile failures like\r\n `Failed to resolve: org.tensorflow:tensorflow-android:+`\r\n\r\nThis issue was fixed over Sunday/Monday (Oct 15th/16th 2017) and at this point all tensorflow-android aar versions should accessible from jcenter again.", "so what do we do know. If we compile should it work know automatically.", "@asimshankar, I got this when run the speech demo in Android Studio3:\r\nCaused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Shape must be rank 1 but is rank 0 for 'Mfcc' (op: 'Mfcc') with input shapes: [16000,0,257], [].\r\n\r\nand I unzip the apk file I generated in Android Studio found that there's no libtensorflow_demo.so in libs directory.\r\n\r\nHow ever the \"tensorflow_demo.apk\" (which is the prebuilt Android demo applications) I download from \"https://ci.tensorflow.org/view/Nightly/job/nightly-android/\" runs ok.\r\n\r\nconfuse a lot about that ", "@imaoldboy libtensorflow_demo.so is an optional library, so there is no issue due to it being missing (it only contains object tracking and faster-than-Java image color conversion code written in C).\r\n\r\nlibtensorflow_inference.so is the one that contains TensorFlow itself. Are you depending on `org.tensorflow:tensorflow-android:+` and still seeing this issue? Can you tell what version of the TF native library you're getting?", "@andrewharp I downloaded the code from github.com and follow the instruction of \"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#building-in-android-studio-using-the-tensorflow-aar-from-jcenter\", and the module's bundle.gradle default setting is \"compile 'org.tensorflow:tensorflow-android:+'\" which I never changed.\r\nI run the demo app in a Emulator, is that a problem?", "I give up this android speech demo stuff\u3002", "@imaoldboy We put compiled native libs for x86 into the AAR, but it's not tested as thoroughly so it wouldn't be too surprising if you ran into issues using an emulator.\r\n\r\nThe versioning issues we were having earlier should be resolved now, so you might try clearing your gradle caches to force it to download the latest AAR.", " tensorflow / tensorflow\r\nCode  Issues 1,308  Pull requests 199  Projects 0  Pulse\r\nJump to bottom Open\r\n[tensorflow] Android studio doesn't fetch setUseNNAPI #17924\r\n@dnpawate\r\ndnpawate opened this issue \r\nabout 2 hours ago \r\nin build.gradle below dependency will not download\r\nlatest file of ./tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java\r\n\r\ndependencies {\r\ncompile 'org.tensorflow:tensorflow-lite:+'\r\n}\r\n\r\nHence we are getting unresolved symbol for new API setUseNNAPI\r\n\r\nas gradle unable to download new API as below\r\n\r\n/** Turns on/off Android NNAPI for hardware acceleration when it is available. */\r\npublic void setUseNNAPI(boolean useNNAPI) {\r\nif (wrapper != null) {\r\nwrapper.setUseNNAPI(useNNAPI);\r\n} else {\r\nthrow new IllegalStateException(\"NativeInterpreterWrapper has already been closed.\");\r\n}\r\n}\r\n", "Nightly Android builds at https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastStableBuild/artifact/out/libandroid_tensorflow_inference_java.jar seem to have vanished. Getting not found error. Anyone know where they moved to?", "ci.tensorflow.org has been deprecated for a while.\r\nIt looks like we missed migrating this build.\r\n@petewarden do we have an equivalent of this build in the new system?\r\nWhat would be the best way to deliver the binaries for this?"]}, {"number": 13652, "title": "No OpKernel was registered to support Op 'Assign' running nightlypi stable build on raspberry pi 3", "body": "I am running the tensorflow backend to keras on a raspberry pi 3 with the lastest Stretch\r\nThe tensorflow 1.3 build is http://ci.tensorflow.org/view/Nightly/job/nightly-pi/lastStableBuild/\r\n\r\nWhen running the Adam optimizer I'm seeing the following error:\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Assign' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_INT32]\r\n\r\n         [[Node: Adam_2/iterations/Assign = Assign[T=DT_INT64, _class=[\"loc:@Adam_2/iterations\"], use_locking=true, validate_shape=true](Adam_2/iterations, Adam_2/iterations/initial_value)]]\r\n\r\nWhat can I do to solve this?", "comments": ["The error is trying to tell you that no kernel for int64 Assign is registered; only int32 and float are registered.\r\n\r\nDigging into this a bit, I see that this is controlled by this block of macros for our Pi builds:\r\nhttps://github.com/tensorflow/tensorflow/blob/3bafe0a86f67dd54197c6d60bdb5053f510de7d8/tensorflow/core/framework/register_types.h#L117\r\n\r\nSo you have two options:\r\n1) Build tensorflow from sources yourself, including support for DT_INT64:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n2) Otherwise, you can find the code that's using int64, and change it to use int32 instead.\r\n\r\nHope that helps!\r\n\r\nNote that technically this isn't a bug or feature request; in the future please ask these kinds of questions on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since there is a larger community that reads questions there. Thanks!"]}, {"number": 13651, "title": "Android -- No OpKernel was registered to support Op 'SparseToDense' with these attrs", "body": "I am trying to load a graph inside Android that I generated and frozen. I keep getting this error whenever I try to run it:\r\n\r\n```\r\nCaused by: java.lang.IllegalArgumentException: No OpKernel was registered to support\r\nOp 'SparseToDense' with these attrs.  Registered devices: [CPU], Registered kernels:\r\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]\r\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]\r\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n                                                                                            \r\n[[Node: output = SparseToDense[T=DT_INT64, Tindices=DT_INT64, validate_indices=true](CTCBeamSearchDecoder, CTCBeamSearchDecoder:2, CTCBeamSearchDecoder:1, output/default_value)]]\r\n```\r\n\r\nI also optimized the graph for inference. When I try to load it onto Android, I get this error:\r\n\r\n```\r\njava.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs \r\nspecified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; \r\nNodeDef: stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/add/y = Const[dtype=DT_INT32, \r\nvalue=Tensor<type: int32 shape: [] values: 1>](stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Switch:1)\r\n```\r\n\r\nSteps to get the frozen graph and optimized graph:\r\n1. Clone this [repository](https://github.com/selcouthlyBlue/bi_lstm_ocr):\r\n2. Run dummy_train.py producing the .pbtxt and checkpoint files\r\n3. Run dummy_freeze_and_save.py producing the frozen and optimized graphs frozen_bi_lstm_ctc_ocr.pb and optimized_frozen_bi_lstm_ctc_ocr.pb, respectively.\r\n\r\nFiles in the mentioned repository relevant to the problem:\r\n- [The Bidirectional LSTM Network](https://github.com/selcouthlyBlue/bi_lstm_ocr/blob/master/main/TFStackedBidirectionalLstmNetwork.py) (contains the network specs and training code)\r\n- [The utilities file](https://github.com/selcouthlyBlue/bi_lstm_ocr/blob/master/main/utils.py) (containing the graph freezing and optimization codes)\r\n- [The dummy configs](https://github.com/selcouthlyBlue/bi_lstm_ocr/tree/master/test_files/configs) in the test files (just to train the model for 1 epoch)\r\n\r\nHere is the Java part related to the problem:\r\n\r\n   ```\r\n public String recognizeHandwritingFrom(Bitmap bitmap) {\r\n        bitmap = Bitmap.createScaledBitmap(bitmap, 1024, 128,, true);\r\n        int[] intValues = new int[bitmap.getWidth() * bitmap.getHeight()];\r\n        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n        float[] floatValues = new float[bitmap.getWidth() * bitmap.getHeight()];\r\n        for (int i = 0; i < intValues.length; ++i) {\r\n            final int val = intValues[i];\r\n            floatValues[i] = (((val >> 16) & 0xFF));\r\n        }\r\n        float[] result = new float[80];\r\n\r\n        long[] INPUT_SIZE = new long[]{1, bitmap.getHeight(), bitmap.getWidth()};\r\n        String[] inputs = new String[]{\"input\", \"seq_len_input\"};\r\n        inferenceInterface.feed(inputs[0], floatValues, INPUT_SIZE);\r\n        inferenceInterface.feed(inputs[1], new int[]{bitmap.getWidth()}, 1);\r\n\r\n        String[] outputs = new String[]{\"output\"};\r\n        inferenceInterface.run(outputs);\r\n        inferenceInterface.fetch(outputs[0], result);\r\n\r\n        return result.toString();\r\n    }\r\n```\r\nI'm using \r\n\r\n- Python 3.5.3 :: Anaconda custom (64-bit)\r\n- The python tensorflow build is downloaded using Anaconda. Tensorflow version is 1.2.1\r\n- The compiled tensorflow for IOS is from this [nightly build](https://ci.tensorflow.org/view/Nightly/job/nightly-android/44/artifact/).\r\n\r\nI would be really grateful if anyone has an idea on why Android seems to not be able to find `SparseToDense` as this is the only thing I have to fix to make it work.\r\n\r\nIf you would like to run the android application as well, you can clone this [repository](https://github.com/selcouthlyBlue/mem2speech). Just get the files from the nightly build and place them inside app/libs folder following this structure:\r\n\r\n```\r\nlibs\r\n|____arm64-v8a\r\n| |____libtensorflow_inference.so\r\n|____armeabi-v7a\r\n| |____libtensorflow_inference.so\r\n|____libandroid_tensorflow_inference_java.jar\r\n|____x86\r\n| |____libtensorflow_inference.so\r\n|____x86_64\r\n| |____libtensorflow_inference.so\r\n```", "comments": ["@petewarden can you take a look?", "Thank you for bringing attention to this :D", "encountered the same problem.\r\nyou are sending int64 where only int32 is accepted\r\nsolved it by adding casting to the python code that is building the graph\r\ndecoded, _ = tf.nn.ctc_beam_search_decoder(logit.sg_transpose(perm=[1, 0, 2]), seq_len, merge_repeated=False)\r\n\ty = tf.sparse_to_dense( tf.to_int32(decoded[0].indices),  tf.to_int32(decoded[0].dense_shape),  tf.to_int32(decoded[0].values), name=\"output_node\" )\r\n\r\n", "I don't have an answer on the second optimization question unfortunately, but if you do want to support more than the standard float and int32 types on mobile, you can recompile TensorFlow with __ANDROID_TYPES_FULL__ defined. This will result in a larger binary though. We're working on improving the documentation around this, and there's some information in this free ebook too: http://www.oreilly.com/data/free/building-mobile-applications-with-tensorflow.csp", "I apologize for the late reply. It's weird that gmail didn't update me of your replies. I tried @eli99999 's solution and now I got this error when I ran the application:\r\n\r\n```\r\njava.lang.IllegalArgumentException: Matrix size-incompatible: In[0]: [1,1056], In[1]: [160,128]\r\n[[Node:stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/bw/basic_lstm_cell/basic_lstm_cell/\r\n\r\nMatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]\r\n\r\n(stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/bw/basic_lstm_cell/basic_lstm_cell/concat, \r\n\r\nstack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/bw/basic_lstm_cell/basic_lstm_cell/MatMul/Enter)]]\r\n```\r\n\r\n", "I did a little poking with the network parameters and it turns out, the number of hidden units influence the dimensions found in \"In[1]\". With that, here's the code building the bidirectional lstm part of the graph:\r\n\r\n```\r\n#num_layers = 3,\r\n#num_hidden = 32\r\n#num_classes = 80 \r\n\r\nlstm_fw_cells = [tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0) for _ in range(num_layers)]\r\nlstm_bw_cells = [tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0) for _ in range(num_layers)]\r\n\r\noutputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(lstm_fw_cells, lstm_bw_cells,\r\n                                                                   self.inputs, dtype=tf.float32)\r\n\r\nbatch_size = tf.shape(self.inputs)[0]\r\n\r\noutputs = tf.reshape(outputs, [-1, num_hidden])\r\n\r\nW = tf.Variable(tf.truncated_normal([num_hidden, num_classes], stddev=0.1, dtype=tf.float32))\r\nb = tf.Variable(tf.constant(0., shape=[num_classes], dtype=tf.float32))\r\n\r\nlogits = tf.matmul(outputs, W) + b\r\nlogits = tf.reshape(logits, [batch_size, -1, num_classes])\r\nlogits = tf.transpose(logits, (1, 0, 2))\r\n```", "I managed to fix the error from applying @eli99999 's solution by changing the bitmap image_width to 128 in the application's recognizer config. I don't know why but it just fixed that error.\r\n\r\n```\r\nRecognizer recognizer = new OfflineRecognizer(recognizerConfigBuilder.setAssetManager(context.getAssets())\r\n                .setImageHeight(128)\r\n                .setImageWidth(128)\r\n                .setModelFilename(\"frozen_bi_lstm_ctc_ocr.pb\")\r\n                .setCharsetFile(\"chars.txt\")\r\n                .build());\r\n```\r\n\r\nWith that change, I now encounter this new error which happens upon fetching the outputs after running:\r\n\r\n```\r\ninferenceInterface.run(outputs);\r\ninferenceInterface.fetch(outputs[0], result); //where the error happens\r\n```\r\n\r\n`java.lang.IllegalArgumentException: cannot use java.nio.FloatArrayBuffer with Tensor of type INT32`", "I already fixed the error previous error. I just changed the type of result from float to int. But I have some issues with changing the image width to 128 just to get it working. For now, I'm going to close this issue since the errors have been addressed with @eli99999 's solution + a little poking around.", "> encountered the same problem.\r\n> you are sending int64 where only int32 is accepted\r\n> solved it by adding casting to the python code that is building the graph\r\n> decoded, _ = tf.nn.ctc_beam_search_decoder(logit.sg_transpose(perm=[1, 0, 2]), seq_len, merge_repeated=False)\r\n> y = tf.sparse_to_dense( tf.to_int32(decoded[0].indices), tf.to_int32(decoded[0].dense_shape), tf.to_int32(decoded[0].values), name=\"output_node\" )\r\n\r\nHi,\r\nI had the same issue and I tried this method. But after this, I started getting the following error:\r\n2019-04-08 11:54:59.302 17408-17563/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[SeqLen], outputs:[SparseToDense]\r\n2019-04-08 11:54:59.302 17408-17563/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: Thread-3\r\n    Process: org.tensorflow.demo, PID: 17408\r\n    java.lang.IllegalArgumentException: Expects arg[0] to be int32 but float is provided\r\n        at org.tensorflow.Session.run(Native Method)\r\n        at org.tensorflow.Session.access$100(Session.java:48)\r\n        at org.tensorflow.Session$Runner.runHelper(Session.java:314)\r\n        at org.tensorflow.Session$Runner.run(Session.java:264)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)\r\n        at org.tensorflow.demo.SpeechActivity.recognize(SpeechActivity.java:229)\r\n        at org.tensorflow.demo.SpeechActivity.access$100(SpeechActivity.java:48)\r\n        at org.tensorflow.demo.SpeechActivity$3.run(SpeechActivity.java:193)\r\n        at java.lang.Thread.run(Thread.java:761)\r\nCould you help me with this?\r\n\r\ninferenceInterface.feed(INPUT_DATA_NAME, mfccInput, 1, 157, 39);\r\ninferenceInterface.run(outputScoresNames); //the error is [ointing to this line\r\ninferenceInterface.fetch(OUTPUT_SCORES_NAME, outputScores);\r\nLog.v(LOG_TAG, \"OUTPUT======> \" + Arrays.toString(outputScores));"]}, {"number": 13650, "title": "What's the difference between function eval() and sess.run()?", "body": "Codes are as follows:\r\n```python\r\n# encoding: utf-8\r\nimport load\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom sklearn.metrics import confusion_matrix\r\n\r\ndef get_chunk(samples, labels, chunkSize):\r\n    if len(samples) != len(labels):\r\n        raise Exception('Length of samples and labels must equal')\r\n    stepStart = 0\r\n    i = 0\r\n    while stepStart < len(samples):\r\n        stepEnd = stepStart + chunkSize\r\n        if stepEnd < len(samples):\r\n            yield i, samples[stepStart: stepEnd], labels[stepStart: stepEnd]\r\n            i += 1\r\n        stepStart = stepEnd\r\n\r\nclass NetWork():\r\n    def __init__(self, num_hidden, batch_size, num_labels, image_size, channel):\r\n        self.num_hidden = num_hidden\r\n        self.batch_size = batch_size\r\n        self.num_labels = num_labels\r\n\r\n        self.image_size = image_size\r\n        self.num_channel= channel\r\n\r\n        self.tf_train_samples = None\r\n        self.tf_train_labels  = None\r\n        self.tf_test_samples  = None\r\n\r\n        self.graph = tf.Graph()      \r\n        self.define_graph()\r\n        self.sess  = tf.Session(graph = self.graph)\r\n        self.writer= tf.summary.FileWriter('./board', self.graph)\r\n    \r\n    def define_graph(self):\r\n        with self.graph.as_default():\r\n            with tf.name_scope('inputs'):\r\n                self.tf_train_samples = tf.placeholder(tf.float32, shape = (self.batch_size, self.image_size, self.image_size, self.num_channel), name = 'train_samples')\r\n                self.tf_train_labels   = tf.placeholder(tf.float32, shape = (self.batch_size, self.num_labels), name = 'train_labels')\r\n                self.tf_test_samples  = tf.placeholder(tf.float32, shape = (self.batch_size, self.image_size, self.image_size, self.num_channel), name = 'test_labels')\r\n            with tf.name_scope('fc1'):\r\n                weight1 = tf.Variable(tf.truncated_normal([self.image_size * self.image_size, self.num_hidden], stddev = 0.1), name = 'weight1')\r\n                bias1   = tf.Variable(tf.truncated_normal([self.num_hidden], stddev = 0.1), name = 'bias1')\r\n                tf.summary.histogram('weight1', weight1)\r\n                tf.summary.histogram('bias1', bias1)\r\n            with tf.name_scope('fc2'):\r\n                weight2 = tf.Variable(tf.truncated_normal([self.num_hidden, self.num_labels], stddev = 0.1), name = 'weight2')\r\n                bias2   = tf.Variable(tf.truncated_normal([self.num_labels], stddev = 0.1), name = 'bias2')\r\n                tf.summary.histogram('weight2', weight2)\r\n                tf.summary.histogram('bias2', bias2)\r\n\r\n            def model(data):\r\n                shape = data.get_shape().as_list()\r\n                reshape = tf.reshape(data, [shape[0], shape[1] * shape[2] * shape[3]])\r\n                with tf.name_scope('fc1_model'):\r\n                    hidden  = tf.nn.relu(tf.matmul(reshape, weight1) + bias1)   \r\n                with tf.name_scope('fc2_model'):\r\n                    return tf.matmul(hidden, weight2) + bias2\r\n\r\n            logits = model(self.tf_train_samples)\r\n            with tf.name_scope('loss'):\r\n                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = self.tf_train_labels))\r\n                tf.summary.scalar('loss', self.loss)\r\n            with tf.name_scope('optimizer'):\r\n                self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss)\r\n            with tf.name_scope('predictions'):\r\n                self.train_prediction = tf.nn.softmax(logits)\r\n                self.test_prediction  = tf.nn.softmax(model(self.tf_test_samples))\r\n            self.merged = tf.summary.merge_all()\r\n\r\n    def run(self):\r\n        ### train\r\n        def print_confusion_matrix(confusionMatrix):\r\n            print('Consusion Matrix: ')\r\n            for i, line in enumerate(confusionMatrix):\r\n                print line, line[i]/np.sum(line)\r\n            \r\n            a = 0\r\n            for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\r\n                a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\r\n                print column[i] / np.sum(column), \r\n            print '\\n', np.sum(confusionMatrix, a)\r\n\r\n        with self.sess as sess:\r\n            tf.global_variables_initializer().run()\r\n            print 'Start Training'\r\n\r\n            train_samples, train_labels = load.loadmat_data('train_32x32.mat')\r\n            train_samples, train_labels = load.reformat(train_samples, train_labels)\r\n            train_samples = load.normalize(train_samples)\r\n\r\n            test_samples, test_labels = load.loadmat_data('test_32x32.mat')\r\n            test_samples, test_labels = load.reformat(test_samples, test_labels)\r\n            test_samples = load.normalize(test_samples)\r\n\r\n            for i, samples, labels in get_chunk(train_samples, train_labels, chunkSize = self.batch_size):\r\n                _, l, predictions, summary = sess.run([self.optimizer, self.loss, self.train_prediction, self.merged], feed_dict = {self.tf_train_samples: samples, self.tf_train_labels: labels})\r\n                self.writer.add_summary(summary, i)\r\n                accuracy, _ = self.accuracy(predictions, labels)\r\n                if i % 50 == 0:\r\n                    print 'Minibatch loss at step %d: loss is %f' % (i, l)\r\n                    print 'Minibatch accuracy: %.1f' % accuracy\r\n\r\n            accuracies = []\r\n            confusionMatrices = []\r\n            for i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.batch_size):\r\n                result = sess.run(self.test_prediction, feed_dict={self.tf_test_samples: samples})\r\n\t\t\t\t# result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})\r\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\r\n                accuracies.append(accuracy)\r\n                confusionMatrices.append(cm)\r\n                print('Test Accuracy: %.1f%%' % accuracy)\r\n            print(' Average  Accuracy:', np.average(accuracies))\r\n            print('Standard Deviation:', np.std(accuracies))\r\n            print_confusion_matrix(np.add.reduce(confusionMatrices))\r\n\r\n    def accuracy(self, predictions, labels, need_confusion_matrix = False):\r\n        _predictions = np.argmax(predictions, 1)\r\n        _labels      = np.argmax(labels, 1)\r\n        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\r\n        accuracy = (100 * np.sum(_predictions == _labels) / predictions.shape[0])\r\n        return accuracy, cm\r\n\r\nif __name__ == '__main__':\r\n    net = NetWork(num_hidden = 128, batch_size = 100, num_labels = 10, image_size = 32, channel = 1)\r\n    net.run()\r\n```\r\n\r\nWhen I use 'result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})', it works fine, while in 'result = sess.run(self.test_prediction, feed_dict={self.tf_test_samples: samples})', it shows error 'ValueError: operands could not be broadcast together with shapes (10,10) (9,9) '. SO, what's difference between function eval() and sess.run()", "comments": ["Can you please include the full traceback with the error message?", "Error show as follows:\r\nTraceback (most recent call last):\r\n  File \"/home/mm/workspace/MoHong/tensorflow_test/classification/one_label/flow.py\", line 128, in <module>\r\n    net.run()\r\n  File \"/home/mm/workspace/MoHong/tensorflow_test/classification/one_label/flow.py\", line 117, in run\r\n    print_confusion_matrix(np.add.reduce(confusionMatrices))\r\nValueError: operands could not be broadcast together with shapes (10,10) (9,9) ", "Maybe that's the problem of 'print_confusion_matrix', which i deleted, all works well."]}, {"number": 13649, "title": "LSTMBlockFusedCell  does not support using  DropoutWrapper ", "body": "I am trying to use DropoutWrapper with LSTMBlockFusedCell as follows:\r\n\r\n```\r\ncell = tf.contrib.rnn.LSTMBlockFusedCell(num_units,forget_bias) \r\ncell = tf.contrib.rnn.DropoutWrapper(cell,dropout)\r\n```\r\n I get an exception that the LSTMBlockFusedCell is not an RNNCell\r\nMessage: The parameter cell is not a RNNCell. Which is raised form _like_rnncell during DropoutWrapper initialization. \r\n\r\nIt is checking for those proprieties on the cell:\r\n\"\"Checks that a given object is an RNNCell by using duck typing.\"\"\"\r\n   conditions = [hasattr(cell, \"output_size\"), hasattr(cell, \"state_size\"),                 hasattr(cell, \"zero_state\"), callable(cell)]  LSTMBlockFusedCell does not have output_size , state_size or zero_state properties. \r\n\r\nShould LSTMBlockFusedCell  act like RNNCell to allow using various wrappers?\r\n\r\nhttps://stackoverflow.com/questions/46699985/using-tensorflow-dropoutwrapper-with-lstmblockfusedcell", "comments": ["@ebrevdo any plans to support this?", "Unfortunately, because this is cell is fused in time, adding dropout would require modifying the C++ code that performs the computation to add proper random number generation and dropout.  Work in this direction is not planned at the moment but I'll ping @ekelsen  to get his thoughts on feasibility.", "why? the http://arxiv.org/abs/1409.2329 says the dropout is only applied to input not state, why can we just do a dropout on the input first?", "You're right.  This should be sufficiently easy, by adding new\ninput_keep_prob and variational_recurrent args to the constructor.  If\nsomeone is interested in adding this, I'm happy to review the PR.\n\nOn Wed, Dec 6, 2017 at 4:41 PM, xiaoyun wu <notifications@github.com> wrote:\n\n> why? the http://arxiv.org/abs/1409.2329 says the dropout is only applied\n> to input not state, why can we just do a dropout on the input first?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13649#issuecomment-349823037>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5r3LTeI9GwX5GpALXb95gAITbckks5s9zQ6gaJpZM4P2WPu>\n> .\n>\n", "@ebrevdo Do you mean to implement dropout in `LSTMBlockFusedCell`, rather than make `DropoutWrapper` supports `LSTMBlockFusedCell`, right?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Yes, that's what I mean.\n\nOn Thu, Dec 21, 2017, 11:34 PM Alfred <notifications@github.com> wrote:\n\n> It has been 14 days with no activity and the awaiting tensorflower label\n> was assigned. Please update the label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13649#issuecomment-353540062>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6mpiEsQumzlRVrCbsYJvxaxG7szks5tC1t6gaJpZM4P2WPu>\n> .\n>\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@hanyh @xiaoyunwu Hi, we found that user can call `tf.nn.dropout` on their inputs directly (see discussion with @ebrevdo in #15843), . Does the workaround solve your problem?", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Assuming my solution is sufficient. Reopen if you find another reason to move dropout into this layer.", "What about MultiRNNCell ? I believe this was the whole point of DropoutWraper, so that higher level functions accepting rnn cells can be used.", "I agree with @godefv  the workaround is ok for dropout wrapper, but you cannot still use it MultiRNNCell and dynamic_rnn"]}, {"number": 13648, "title": "tf.string_input_producer() doesn't work with tf.placeholder", "body": "In a scenario it is intended to dynamically change the file names of tensorflow record files, as indicated by the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef test(s):\r\n    filename_queue = tf.train.string_input_producer([s])\r\n\r\n    reader = tf.TextLineReader()\r\n    key, value = reader.read(filename_queue)\r\n\r\n    record_defaults = [[1.0], [1]]\r\n    col1, col2 = tf.decode_csv(value, record_defaults = record_defaults)\r\n\r\n    return col1, col2\r\n\r\ns = tf.placeholder(tf.string, None, name = 's')\r\n# s = tf.constant('file0.csv', tf.string)\r\nss = [\"file0.csv\", \"file1.csv\"]\r\ninputs, labels = test(s)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n    for e in ss:\r\n        inputs_val, labels_val = sess.run([inputs, labels], feed_dict = {s: e})\r\n        print(\"input {} - label {}\".format(inputs_val, labels_val))\r\n\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n\r\n```\r\n\r\nit is observed the code above is working with tf.constant which is commented above, however not tf.placeholder, which is believed to be equivalent.\r\n\r\nThere is no direct error related to the malfunctioning as below:\r\n\r\n```\r\n(tensorflow)[yuming@atlas4 working-files]$ python 36.py\r\n2017-10-12 11:13:21.753318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:\r\nname: Quadro M4000 major: 5 minor: 2 memoryClockRate(GHz): 0.7725\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.87GiB\r\n2017-10-12 11:13:21.858019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 1 with properties:\r\nname: Quadro K2200 major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.54GiB\r\n2017-10-12 11:13:21.858060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:980] Device peer to peer matrix\r\n2017-10-12 11:13:21.858068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] DMA: 0 1\r\n2017-10-12 11:13:21.858072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 0:   Y N\r\n2017-10-12 11:13:21.858075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 1:   N Y\r\n2017-10-12 11:13:21.858082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro M4000, pci bus id: 0000:83:00.0, compute capability: 5.2)\r\n2017-10-12 11:13:21.858088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1042] Ignoring gpu device (device: 1, name: Quadro K2200, pci bus id: 0000:03:00.0, compute capability: 5.0) with Cuda multiprocessor count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\nTraceback (most recent call last):\r\n  File \"36.py\", line 26, in <module>\r\n    inputs_val, labels_val = sess.run([inputs, labels], feed_dict = {s: e})\r\n  File \"/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1118, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1315, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)\r\n         [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TextLineReaderV2, input_producer)]]\r\n\r\nCaused by op u'ReaderReadV2', defined at:\r\n  File \"36.py\", line 17, in <module>\r\n    inputs, labels = test(s)\r\n  File \"36.py\", line 7, in test\r\n    key, value = reader.read(filename_queue)\r\n  File \"/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 194, in read\r\n    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\r\n  File \"/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 654, in _reader_read_v2\r\n    queue_handle=queue_handle, name=name)\r\n  File \"/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 789, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3052, in create_op\r\n    op_def=op_def)\r\n  File \"/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1610, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nOutOfRangeError (see above for traceback): FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)\r\n         [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TextLineReaderV2, input_producer)]]\r\n\r\n```\r\n\r\nOS version Redhat 7.3, Python version 2.7.5, Tensorflow version 1.3\r\n\r\nfile0.csv and file1.csv are quite simple csv files just with two lines:\r\n0.1,0\r\n0.9,1", "comments": ["@mrry knows the truth here, and can correct any (all!) mistakes I make.\r\n\r\nQueues are pretty confusing (I was confused by this too).  Although your attempt to replace `tf.constant` with `tf.placeholder` seems reasonable, I've confirmed it doesn't work, with the error you've listed.  I believe the problem is that the producer side of the queue is actually started by this line:\r\n```\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n```\r\n\r\nSo your subsequent call to `sess.run(... feed_dict={s: e})` isn't actually materializing a value in the placeholder.  I believe this implies that the argument to `tf.train.string_input_producer` really needs to be a tensor with a materialized value; you can't use a placeholder here, since there's no way to set that value before the queue producers start running.\r\n\r\nNote that \"datasets\" are now the preferred mechanism to handle input data, replacing \"queues\".  More info here; I believe they do support your use-case (look for `feedable iterator`):\r\nhttps://www.tensorflow.org/programmers_guide/datasets", "@tatatodd : Many thanks for your quick respond. Let's see @mrry 's opinion. If this materializing behavior is by design, I can quite understand that and let's close it, since as you said we have better solution. Thanks again. ", "Yes, Todd's explanation is correct. The most confusing aspect of `tf.train.string_input_producer()` is that, while it looks like a regular operation, it is actually implemented using a separate set of `sess.run()` calls that run in the background in a queue-runner thread. It is these hidden calls that are failing because they don't know about the value you want to bind to the placeholder. They also start happening when you call `tf.train.start_queue_runners()`, and **before** you feed the value for the placeholder.\r\n\r\nTodd also mentioned that we have a new API that makes doing this kind of thing easier. (One nit: you can use the much simpler \"initializable\" iterators, and don't need to use \"feedable\" ones for this case.) Here's what your program would look like in TF 1.4 (for TF 1.3, using `tf.contrib.data` instead of `tf.data`):\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfilename = tf.placeholder(tf.string, [], name = 's')\r\nrecord_defaults = [[1.0], [1]]\r\n\r\nlines_dataset = tf.data.TextLineDataset(filename)\r\nparsed_dataset = lines_dataset.map(lambda line: tf.decode_csv(line, record_defaults=record_defaults))\r\niterator = parsed_dataset.make_initializable_iterator()\r\ninputs, labels = iterator.get_next()\r\n\r\nss = [\"file0.csv\", \"file1.csv\"]\r\n\r\nwith tf.Session() as sess:\r\n  for e in ss:\r\n    # Feed once per file to initialize the iterator.\r\n    sess.run(iterator.initializer, feed_dict={filename: e})\r\n\r\n    # Loop over the lines of the file until `OutOfRangeError` indicates end-of-file.\r\n    try:\r\n      while True:\r\n        inputs_val, labels_val = sess.run([inputs, labels])  # NOTE: No `feed_dict` needed here.\r\n        print(\"input {} - label {}\".format(inputs_val, labels_val))\r\n      except tf.errors.OutOfRangeError:\r\n        pass\r\n```", "@mrry I am sincerely appreciating the detailed explanation and intuitive code sample. Thanks again."]}, {"number": 13647, "title": "Extracting weight values from output_graph.pb", "body": "Hi, this isn't a bug more of request for a tutorial or guidance -  I have asked this question multiple times on Stackoverflow  and can't seem to get any responses nor views. My past questions on tensorflow also didn't get much views on Stackoverflow -- I think this is because tensorflow is still relatively new and it's hard to find knowledgeable people.\r\n\r\nI'm sorry to post, here but I don't know where else to ask -- I've been at it for a few days and looked \r\nthe tutorials on the official site but I couldn't find any. \r\n\r\nI used inceptionV-3 for transfer learning  and now I have a output_graph.pb.\r\nI want to extract all the weights and biases for each layer, but I can't seem to find a way to do this. \r\n\r\nI've gone over most the tutorials in tensorflow:\r\nhttps://www.tensorflow.org/programmers_guide/saved_model#models\r\n\r\nAnd most blogs or stackoverflows posts show how to deal with 'meta` and `ckpt` graphs.\r\n\r\nbut not on `pb` graphs.  \r\n\r\nI've attempted this:\r\n\r\nhttps://stackoverflow.com/questions/46696859/tf-graphkeys-trainable-variables-on-output-graph-pb-resulting-in-empty-list\r\n\r\n\r\n```\r\ndef create_graph(modelFullPath):\r\n    \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\r\n    # Creates graph from saved graph_def.pb.\r\n    with tf.gfile.FastGFile(modelFullPath, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        tf.import_graph_def(graph_def, name='')\r\n\r\nGRAPH_DIR = r'C:\\tmp\\output_graph.pb'\r\ncreate_graph(GRAPH_DIR)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n    print (len(all_vars))\r\n```\r\n\r\nBut I'm getting a value of 0 -- so empty return. \r\n\r\n\r\nI've also attempted using `freeze_graph.py`  after cloning tensorflow repository but couldn't understand all of the arguments (created a stackoverflow question on this as well)\r\n\r\n--input_graph    = path to the graph I want to extract.\r\n\r\nI'm assuming the above arugument is all I need to use?\r\n\r\nHowever when running this command:\r\n\r\n`python3.6 C:\\Users\\Moondra\\tensorflow\\tensorflow\\python\\tools\\freeze_graph.py --input-graph=C:\\tmp\\output_graph.pb`\r\n\r\nI'm getting a import error which is confusing me as well:\r\n\r\n```\r\nfrom tensorflow.python.tools import saved_model_util\r\n     importError: cannot import name 'saved_model_utils'\r\n```\r\n\r\n\r\nI've cloned the repository properly and the saved_model_utils is within the tools folder, so not sure \r\nwhy I\"m getting an import error.  \r\n\r\nAny help would be appreciated, and once again, sorry for posting here.  :(\r\n\r\n\r\n", "comments": ["I posted [an answer](https://stackoverflow.com/a/46699540/3574081) to your question on Stack Overflow.", "Thank you so much. It works. ", "This is really helpful, thank you very much.....!"]}, {"number": 13646, "title": "Fixes and additions to release notes.", "body": "Added line about Keras moving into core.\r\nAdded line about CUDA/cuDNN versions.\r\nAdded line about custom ops.", "comments": []}, {"number": 13645, "title": "Adding basic MKL-DNN code", "body": "This PR includes basic code of MKL-DNN integration in the graph rewrite and the convolution op. It is the base for the upcoming MKL-DNN kernels.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@gunan It seems the only failing test failed because of Timeout. I assume it is flaky. ", "Sorry for forgetting about this change, I have been OOO for a while.\r\nretesting just in case.", "@mahmoud-abuzaina Could you rebase your changes on master? I cant get a green test run again, and I am not sure if these are due to old failures, or some issues we may be introducing.", "@gunan Done.", "Looks like there are now some clang-format issues:\r\n\r\n```\r\n=== Sanity check step 11 of 11: do_clang_format_check (Clang Format Check: Check .h and .cc files with Google C++ style) ===\r\n\r\nFile tensorflow/core/graph/mkl_graph_util.h is not properly formatted with clang-format --style=google\r\nFile tensorflow/core/graph/mkl_layout_pass.cc is not properly formatted with clang-format --style=google\r\nFile tensorflow/core/graph/mkl_tfconversion_pass.cc is not properly formatted with clang-format --style=google\r\nFile tensorflow/core/kernels/mkl_conv_grad_filter_ops.cc is not properly formatted with clang-format --style=google\r\nFile tensorflow/core/kernels/mkl_conv_grad_input_ops.cc is not properly formatted with clang-format --style=google\r\nFile tensorflow/core/kernels/mkl_conv_ops.cc is not properly formatted with clang-format --style=google\r\nFile tensorflow/core/kernels/mkl_conv_ops.h is not properly formatted with clang-format --style=google\r\nFile tensorflow/core/kernels/mkl_tfconv_op.h is not properly formatted with clang-format --style=google\r\nFile tensorflow/core/util/mkl_util.h is not properly formatted with clang-format --style=google\r\nFile tensorflow/core/util/mkl_util_test.cc is not properly formatted with clang-format --style=google\r\n```\r\nCould you take a look?", "@gunan it looks like the style checker has added more constraints. Looking at it.\r\n", "@gunan I fixed clang-format style issues.", "Jenkins, test this please", "Not sure why the clang-format still reporting errors although I ran the clang-format tool on the files, and cpplint does not show errors. ", "Is it possible the difference comes from `--style=google` flag for clang-format?\r\n@yifeif in case she has more insight.", "I always use --style=google", "Looks like different clang-format version might format differently even with the same style option. #14115 to disable it for now.", "The check should be disabled now if we can rebase. Thanks!", "Thanks @yifeif. I did a rebase. ", "Jenkins, test this please.", "Jenkins, test this please.", "@gunan I am still not able to verify if the failing tests are failing because of the MKL changes. ", "I think these are infra issues. I will get them investigated as soon as possible."]}, {"number": 13643, "title": "conv2d_transpose crashes on GPU with zero size batch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (installed with `conda install tensorflow-gpu`)\r\n- **TensorFlow version (use command below)**: b'unknown' 1.3.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: GTX 980\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nExecute the script below.  It works correctly when running on a CPU, but on a GPU it crashes with this error:\r\n\r\n```\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:430] could not convert BatchDescriptor {count: 0 feature_map_count: 1 spatial: 7 7  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX} to cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\r\n```\r\n\r\nThe error happens when the first dimension of the input array is 0.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\na = tf.placeholder(dtype=tf.float32, shape=(None, 7, 7, 1))\r\nb = tf.contrib.layers.conv2d_transpose(a, num_outputs=16, kernel_size=5, stride=2)\r\nsession = tf.Session()\r\nsession.run(tf.global_variables_initializer())\r\nprint(session.run(b, feed_dict={a: np.zeros((0, 7, 7, 1))}))\r\n```", "comments": ["I encountered this in TensorFlow 1.4.0 - so looks like this fix is not included in 1.4.0 release. I installed TensorFlow nightly build (tf_nightly_gpu-1.5.0.dev20171127-cp35-cp35m-manylinux1_x86_64.whl) and the nightly build solves the issue.", "I can confirm the issue still exists in my case for the nightly build tf_nightly_gpu-1.5.0.dev20171127-cp35-cp35m-manylinux1_x86_64.whl... :(\r\n\r\nAnyone can points us the good build that fixes the issue?", "@hpnhxxwn OK you are right - it also exists in my app too. The above example runs successfully, but my tensorflow app just crashes:(", "The fix did not make it into 1.4, but should be in nightly. \r\n\r\n@hpnhxxwn, does the above example not work or is it a different example that does not work? Can you post your CUDA/cudnn/Python/Ubuntu versions and your GPU? I'm not seeing the issue on Ubuntu 14.04, Python 2.7, Cuda 8 and cuDNN 6 with `pip install tf-nightly-gpu`.\r\n\r\n@xiaoyaozhuzi what is the error message when your TensorFlow app crashes? If possible, can you post a small example that reproduces the problem?", "@reedwm \r\n\r\nIt is my code. It was working for CPU version, but for GPU it did not work and gives me such error. Mine is Ubuntu 16.04, Python 3.5, Cuda 8 and cuDNN 5. Actually I tried both tensorflow 1.14.0 with cuDNN 6 and local build from tenforflow latest source code with cnDNN 5, both have issue. I have also tried with the nightly build xiaoyongzhu pointed yesterday, also has the same issue with cuDNN6. The error message is really mysterious and I do not know where to look at. However, the code works on CPU tensorflow.\r\n\r\nMy code is below, could you suggest workaround?\r\n```\r\ndef resnet_model(bin_multiple):\r\n\r\n    #input and reshape\r\n    inputs = Input(shape=input_shape)\r\n    reshape = Reshape(input_shape_channels)(inputs)\r\n\r\n    #normal convnet layer (have to do one initially to get 64 channels)\r\n    conv = Conv2D(64,(1,bin_multiple*note_range),padding=\"same\",activation='relu')(reshape)\r\n    pool = MaxPooling2D(pool_size=(1,2))(conv)\r\n\r\n    for i in range(int(np.log2(bin_multiple))-1):\r\n        print i\r\n        #residual block\r\n        bn = BatchNormalization()(pool)\r\n        re = Activation('relu')(bn)\r\n        freq_range = (bin_multiple/(2**(i+1)))*note_range\r\n        print freq_range\r\n        conv = Conv2D(64,(1,freq_range),padding=\"same\",activation='relu')(re)\r\n\r\n        #add and downsample\r\n        ad = add([pool,conv])\r\n        pool = MaxPooling2D(pool_size=(1,2))(ad)\r\n\r\n    flattened = Flatten()(pool)\r\n    fc = Dense(1024, activation='relu')(flattened)\r\n    do = Dropout(0.5)(fc)\r\n    fc = Dense(512, activation='relu')(do)\r\n    do = Dropout(0.5)(fc)\r\n    outputs = Dense(note_range, activation='sigmoid')(do)\r\n\r\n    model = Model(inputs=inputs, outputs=outputs)\r\n\r\n    return model\r\n\r\n... other code\r\n\r\nmodel = resnet_model(bin_multiple)\r\ninit_lr = float(args['init_lr'])\r\n    model.compile(loss='binary_crossentropy',\r\n              optimizer=SGD(lr=init_lr,momentum=0.9), metrics=['accuracy', 'mae', 'categorical_accuracy'])\r\nmodel.summary()\r\n\r\nhistory = model.fit_generator(trainGen.next(),trainGen.steps(), epochs=epochs, verbose=1,validation_data=valGen.next(),validation_steps=valGen.steps(),callbacks=callbacks, workers=8, use_multiprocessing=True)\r\n```", "@hpnhxxwn I'm not sure what the issue is. Can you post the full error message that occurs? Also if possible can you post the full example? Are you calling conv2d_transpose with zero batch size?"]}, {"number": 13642, "title": "[bug?] tf.nn.embedding_lookup returns 0 when ids out of range", "body": "It seems tf.nn.embedding_lookup will simply return tensor of zeros when ids out of range (larger than the embedding table size):\r\n```\r\nimport tensorflow as tf\r\nembs = tf.ones([100, 100]) \r\nidx = tf.cast(tf.ones([1]) * 1000, tf.int32)\r\nwith tf.Session() as sess:\r\n  emb = sess.run(tf.nn.embedding_lookup(embs, idx))\r\n```\r\nThe emb will be tensor of zeros. I am not sure if this is a bug, or by design for efficiency concern? It would be nice if there is a runtime exception. That will do a big favor in avoiding hidden bugs that lead to performance degeneration.\r\n\r\n(I am running tensorflow-gpu 1.3.0 in Ubuntu 16.04)", "comments": ["This is by-design, for performance reasons.\r\n\r\nNote the docs describe this via the `validate_indices` argument, which is deprecated and always set to the `True` behavior.\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup\r\n\r\n```\r\nvalidate_indices: DEPRECATED. If this operation is assigned to CPU, values in indices are always validated to be within range. If assigned to GPU, out-of-bound indices result in safe but unspecified behavior, which may include raising an error.\r\n```", "Now, when the id is out of the range, there will be an error, instead of returning zero tensor.\r\n\r\nIf I want to get zero tensors if the ids are out of the range, what should I do? (as the validate_indices is deprecated now)\r\n\r\nThanks", "@floydluo did you ever figure out a solution? I'm hesitant to use the `embedding_lookup` function if I can't be sure it won't raise an `InvalidArgumentError`...", "@tgsmith61591 Hi, actually, I didn't find an elegant way. But I found a tentative one. Suppose `vocab_size` is N, where 0 is `pad_id`,  1 is `unknown_id`, and 2 to (N-1) are normal token ids. I construct an embedding as the snippet shows. This embedding always return zero vectors for `0` (`pad_id`) and `1` (`unknown_id`) during training, as the corresponding vectors are zero constants. \r\n\r\n```python\r\ndef get_embeddings_variable(vocab_size, embedding_size = 200):\r\n    # input_size is vocab_size.\r\n    # 0: pad id, 1: unk id, 2 to (vocab_size-1): normal token id\r\n    # I want return zero vectors for pad and unk all the time.\r\n    embed_shape = vocab_size, embedding_size\r\n    embeddings = tf.Variable(tf.truncated_normal(embed_shape,  stddev=1.0 / np.sqrt(embedding_size)))\r\n    always_zero_tokens_num = 2  # pad and unk\r\n    embeddings = tf.concat([[tf.constant([0.] * embedding_size)] * always_zero_tokens_num, embeddings], axis = 0)\r\n    return embeddings \r\n```\r\nThen I process the input indexes. If any one is larger than N, I replace it with 1, `unknown_id`. Finally, the embeddings returns zero vectors for the ids which are out of range...\r\n\r\n"]}, {"number": 13640, "title": "Branch 171836140", "body": "", "comments": ["Thanks, @yifeif "]}, {"number": 13639, "title": "Invoke get_shape() on sparse_tensor leads to feeding error", "body": "If I invoke get_shape method on sparse_tensor, the shape tensor will be added into the _unfeedable_tensors set of the current graph. Then when I feed the sparse tensor, an error occurs.\r\n\r\nThe codes below show this error\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nshape = np.array([7, 9, 2], dtype=np.int64)\r\nindices = np.array([[3, 2, 0], [4, 5, 1]], dtype=np.int64)\r\nvalues = np.array([1.0, 2.0], dtype=np.float32)\r\nx = tf.sparse_placeholder(tf.float32, shape=shape)\r\n\r\nwith tf.Session() as sess:\r\n    x.get_shape() # <-- Troublemaker\r\n    # This line leads to the exception:\r\n    # \t   Tensor Tensor(\"Const:0\", shape=(3,), dtype=int64) may not be fed.\r\n    # The side effection of this line is that \r\n    # it adds the 'shape' tensor into Graph._unfeedable_tensors, \r\n\r\n    print(sess.run(x, feed_dict={\r\n    \tx: tf.SparseTensorValue(indices, values, shape)}))\r\n```\r\n\r\nThe stacktrace\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-baac6f49a954> in <module>()\r\n     10     x.get_shape()\r\n     11     print(sess.run(x, feed_dict={\r\n---> 12     \tx: tf.SparseTensorValue(indices, values, shape)}))\r\n\r\n/Users/liqimai/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/Users/liqimai/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    944                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\r\n    945           if not self.graph.is_feedable(subfeed_t):\r\n--> 946             raise ValueError('Tensor %s may not be fed.' % subfeed_t)\r\n    947           subfeed_name = compat.as_bytes(subfeed_t.name)\r\n    948           feed_dict_string[subfeed_name] = np_val\r\n\r\nValueError: Tensor Tensor(\"Const:0\", shape=(3,), dtype=int64) may not be fed.\r\n```\r\n------------------------\r\n\r\n### System information\r\nI do not think this bug is related to my environment.\r\n== cat /etc/issue ===============================================\r\nDarwin liqimaideMacBook-Pro.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.37)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin MacBook-Pro.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.11.3)\r\nprotobuf (3.2.0)\r\ntensorflow (1.0.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.0.0\r\ntf.GIT_VERSION = v1.0.0-rc2-15-g47bba63-dirty\r\ntf.COMPILER_VERSION = v1.0.0-rc2-15-g47bba63-dirty\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================", "comments": ["The issue is that `SparseTensor.get_shape` calls [`tensor_util.constant_value_as_shape`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_util.py#L751), which calls [`tensor_util.constant_value`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_util.py#L714), which calls `Graph.prevent_feeding`.\r\n\r\n@ebrevdo, what do you think the best way of solving this is? Perhaps in the `SparseTensor` constructor, we can set a new field with the `TensorShape`.", "I'm wary of changing the SparseTensor api.  what if you create 3 placeholders and create a SparseTensor from them.  does this work?", "The API would not change, except that calling `get_shape` would not prevent feeding the `tf.sparse_placeholder`. I do not think that making that change would affect backwards compatibility.\r\n\r\n3 placeholders would work (that's what `tf.sparse_placeholder` internally does) but ideally calling get_shape on `tf.sparse_placeholder` would not cause it to be unfeedable.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Assigning to @reedwm since he's already analyzed the exact problem, and seems to have a reasonable proposed fix.  That said, I don't know the details here.", "I don't know enough about SparseTensor to fix this, or currently have time to look into it.\r\n\r\nAssigning to @ebrevdo for now.", "You can run\r\n\r\n```python\r\nsess.run(x, feed_dict={x.indices: indices, x.values: values})\r\n```\r\nwithout breaking shape semantics, right?", "Yes, that works, because the shape is not fed.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "OK great; closing.", "I have the same problem too.\r\nBut it seems that no one can solve this.\r\nAny ideas?", "BY the way. tensorflow is really full of bugs itself, which is nothing comparable to pytorch.", "@xChenSky \r\nAccording to my experience, do NOT specify the shape when declare tf.sparse_placeholder. \r\n```python\r\nx = tf.sparse_placeholder(tf.float32) # work\r\nx = tf.sparse_placeholder(tf.float32, shape=shape) # not work\r\n```"]}, {"number": 13638, "title": "Boring ssl update.", "body": "As per https://github.com/tensorflow/tensorflow/issues/13587.\r\nPing @gunan .\r\nAlso what's the standard way of getting the checksum? I used sha256sum on the file I downloaded.\r\nThe patch to the BUILD file is no longer applicable, I'm not sure if we still need that patch.. Any comments?", "comments": ["@tjingrant please take a look at the sanity build failure:\r\n\r\nERROR: /workspace/tensorflow/tools/pip_package/BUILD:101:1: no such package '@boringssl//': Traceback (most recent call last):\r\n\tFile \"/workspace/tensorflow/workspace.bzl\", line 123\r\n\t\t_apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n\tFile \"/workspace/tensorflow/workspace.bzl\", line 107, in _apply_patch\r\n\t\trepo_ctx.path(patch_file)\r\npath() can only take a string or a label. and referenced by '//tensorflow/tools/pip_package:licenses'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:licenses' failed; build aborted.\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/8948/console", "@caisq It seems to be the fault of Github. Github was experiencing issues when I submitted the changes; I did a force-update and things make sense now.", "@tensorflow-jenkins test this please", "Hi @gunan, here at IBM Research we have to constantly experiment with the master branch and it has been a slight nuisance that we have to debug the master branch every time. Although I do not speak for IBM Research officially, my manager has been telling me that he's interested in taking a more active approach to this issue. For instance, we can commit one of our [minsky](https://www.ibm.com/developerworks/community/blogs/aixpert/entry/OpenPOWER_IBM_S822LC_for_HPC_Minsky_First_Look?lang=en) machine to be a build bot. This can help prevent some mishaps (for us) that can easily be identified and corrected (i.e., https://github.com/tensorflow/tensorflow/commit/fa8c1a1f30eed9ea35e1339eb94515cefc97582f). \r\n\r\nOn the other hand, we understand there is no official support for PPC, but some of us are already constantly maintaining the specific build instructions for PPC internally and therefore we can be of some help to address some of the build failures on PPC.\r\n\r\nAgain, although I do not speak for IBM Research officially, we would like to actively explore all possible options and get the conversation started.", "@thirupalanisamy @martinwicke FYI.\r\nWe used to have an s390x build setup by @nayana-ibm with a setup similar to what you are suggesting. However, that machine seems to be offline at the moment. There are also some upcoming changes in our CI infrastructure, so we may need to come up with a longer term plan for such use cases.\r\n\r\nThiru, can we open a separate communication channel to @tjingrant 's team to discuss how we can make life easier for IBM Research?", "@gunan The vms used for s390x Tensorflow CI are up and running however disconnected by av8ramit. This is probably due to builds are continuously failing for s390x. I could see the two reasons for build failure: first one is it needs Bazel upgrade to 0.6.1 and other one is google/nsync module. \r\nI have upgraded Bazel to 0.6.1 on these vms. For google/nsync failure I  have raised an issue [here](https://github.com/tensorflow/tensorflow/issues/13777). \r\n", "Looks like only one of the VMs was connected but disabled. I reenabled this\nVM. Sorry for missing this earlier.\nfor VM-0, I see \"Ping response time is too long or timed out.\" So that one\nis actually not connected.\n\nWe can continue the discussion in the issue you filed, if you like.\n\nOn Tue, Oct 17, 2017 at 2:15 AM, Nayana Thorat <notifications@github.com>\nwrote:\n\n> @gunan <https://github.com/gunan> The vms used for s390x Tensorflow CI\n> are up and running however disconnected by av8ramit. This is probably due\n> to builds are continuously failing for s390x. I could see the two reasons\n> for build failure: first one is it needs Bazel upgrade to 0.6.1 and other\n> one is google/nsync module.\n> I have upgraded Bazel to 0.6.1 on these vms. For google/nsync failure I\n> have raised an issue here\n> <https://github.com/tensorflow/tensorflow/issues/13777>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13638#issuecomment-337168798>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOZVHh-0DkFTgOlMKR0tTYetM51kKks5stHAlgaJpZM4P1vQO>\n> .\n>\n", "@thirupalanisamy do you have any recommendations regarding how we can move forward with this? We searched around and cannot find any information regarding how to add a new build bot.", "@thirupalanisamy is right now travelling, but Let us discuss this separately.\r\nRight now, we have a s390x build:\r\nhttp://ci.tensorflow.org/view/contrib/job/tensorflow-contrib-s390x/\r\n\r\nWe can setup something similar.\r\nTF team cannot commit to keeping it green, but the failures can be easier to track with a continuous build. Please feel free to reach out to me via email using <my github username>[at]google.com", "Hi Gunan,\r\nContinuing discussion on CI vms on this thread : https://github.com/tensorflow/tensorflow/issues/13914"]}, {"number": 13637, "title": "Merge pull request #1 from tensorflow/master", "body": "pull", "comments": []}, {"number": 13636, "title": "get_shape() does not work for output of tf.image.resize_nearest_neighbor()", "body": "Hello everyone,\r\n\r\nFollowing the issue  #7932, I have also noticed that get_shape() does not work when using\r\ntf.image.resize_nearest_neighbor().\r\n\r\nI use linux 16.06 and TF in [1.3.0, 1.3.1, 1.4.0-dev20171008] and I have the same error.\r\n\r\nThe error is quite easy to understand. The fact that tf.image.resize_nearest_neighbor is unable to compute correctly **get_shape** leads to some problematic behavior.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(tf.float32, [None, 32, 32, 1])\r\n\r\nprint(\"x_shape:\", x.get_shape()) # x_shape: (?, 32, 32, 1)\r\n\r\ninput_shape = tf.shape(x)[1:3]\r\nnewsize     = tf.multiply(input_shape, (2, 2)) # gives (64, 64)\r\n\r\n####################\r\n\r\nresized_data = tf.image.resize_nearest_neighbor(\r\n    images        = x,\r\n    size          = newsize,\r\n    align_corners = None,\r\n    name          = None\r\n)\r\nprint(\"resized_data:\", resized_data.get_shape()) # resized_data: (?, ?, ?, 1)\r\n\r\n#################### NOW THE PROBLEMATIC #################\r\n\r\nflatten_tensor = tf.contrib.layers.flatten(inputs = resized_data)\r\nprint(\"flatten_tensor:\", flatten_tensor.get_shape()) # resized_data: (?, ?)\r\n\r\nfailing_layer = tf.contrib.layers.fully_connected(\r\n    inputs        = flatten_tensor,\r\n    num_outputs   = 1,\r\n    activation_fn = None\r\n)\r\n\r\n##############################################\r\n############# LAUNCH THE SESSION #############\r\n##############################################\r\n\r\nwith tf.Session() as sess:\r\n    rslt = sess.run(resized_data, feed_dict={x: np.ones((666, 32, 32,1))})\r\n    print(\"Rslt Shape:\", rslt.shape)\r\n    \r\n    failing_layer = sess.run(failing_layer, feed_dict={x: np.ones((666, 32, 32,1))})\r\n    print(\"failing_layer Shape:\", failing_layer.shape)\r\n```\r\n\r\nFor information, it is absolutely to launch the session with the FC layer in the script, it completely fails.", "comments": ["Thanks to the work in Keras, I managed to design a workaround:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef int_shape(x):\r\n    try:\r\n        return tuple(x.get_shape().as_list())\r\n    except ValueError:\r\n        return None\r\n\r\nx = tf.placeholder(tf.float32, [None, 32, 32, 1])\r\n\r\nprint(\"x_shape:\", x.get_shape()) # x_shape: (?, 32, 32, 1)\r\n\r\n####################\r\n\r\nscale_factor   = (2, 2)\r\n\r\noriginal_shape = int_shape(x)\r\n\r\nnew_shape      = tf.shape(x)[1:3]\r\nnew_shape      = tf.multiply(new_shape, scale_factor) # gives (64, 64)\r\n\r\nresized_data = tf.image.resize_nearest_neighbor(\r\n    images        = x,\r\n    size          = new_shape,\r\n    align_corners = None,\r\n    name          = None\r\n)\r\n\r\nprint(\"resized_data:\", resized_data.get_shape()) # resized_data: (?, ?, ?, 1)\r\n\r\nprint(resized_data.shape)\r\n\r\nresized_data.set_shape(\r\n    (\r\n        None, \r\n        original_shape[1] * scale_factor[0] if original_shape[1] is not None else None,\r\n        original_shape[2] * scale_factor[1] if original_shape[2] is not None else None, \r\n        None\r\n    )\r\n)\r\n\r\n#################### NOW THE PROBLEMATIC #################\r\n\r\nflatten_tensor = tf.contrib.layers.flatten(inputs = resized_data)\r\nprint(\"flatten_tensor:\", flatten_tensor.get_shape()) # resized_data: (?, ?)\r\n\r\nfailing_layer = tf.contrib.layers.fully_connected(\r\n    inputs        = flatten_tensor,\r\n    num_outputs   = 1,\r\n    activation_fn = None\r\n)\r\n\r\n##############################################\r\n############# LAUNCH THE SESSION #############\r\n##############################################\r\n\r\nwith tf.Session() as sess:\r\n    \r\n    tf.global_variables_initializer().run()\r\n    \r\n    rslt = sess.run(resized_data, feed_dict={x: np.ones((666, 32, 32,1))})\r\n    print(\"Rslt Shape:\", rslt.shape)\r\n    \r\n    failing_layer = sess.run(failing_layer, feed_dict={x: np.ones((666, 32, 32,1))})\r\n    print(\"failing_layer Shape:\", failing_layer.shape)\r\n```\r\n\r\nDo you think, a PR should fix this ? Or is it maybe normal behavior ...", "@DEKHTIARJonathan In general we don't make guarantees about static shape inference in your graph.  This is because it can become arbitrarily complex, or sometimes impossible.  E.g. in your original example, the size is determined by `newsize = tf.multiply(input_shape, (2, 2))`; in the general case this may be an arbitrary expression.\r\n\r\nSo I'd consider this normal behavior.  BTW using set_shape to explicitly set the shape is a reasonable workaround."]}, {"number": 13635, "title": "w9wang2", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 13634, "title": "Failed build tensorflow with bazel, failed with: error SQLite will not work correctly with the -ffast-math option of GCC.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: **Ubuntu 17.04**\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: **commit: 1ad5e692e2fc218ca0b2a9a461c19762fdc9674b master branch**\r\n- **Python version**: **Python 2.7.13**\r\n- **Bazel version (if compiling from source)**:  **0.6.1**\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n**#build tensorflow**\r\n_bazel build --config=mkl --copt=\"-g\" --copt=\"-DEIGEN_USE_VML\" --copt=\"-mavx2\" --copt=\"-mfma\" --copt=\"-O3\" --verbose_failures  --copt=\"-Ofast\" --copt=\"-L/opt/intel/gcc/lib64\" -s -c opt //tensorflow/tools/pip_package:build_pip_package_\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n**During the build process, it failed with the message:**\r\n\r\n_ERROR: /root/.cache/bazel/_bazel_root/33016bcb7111d180c7dd9b171742c7e7/external/sqlite_archive/BUILD.bazel:9:1: C++ compilation of rule '@sqlite_archive//:sqlite' failed (Exit 1): gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/33016bcb7111d180c7dd9b171742c7e7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DEIGEN_USE_VML -g -DEIGEN_USE_VML -mavx2 -mfma -O3 -Ofast -L/opt/intel/gcc/lib64 -MD -MF bazel-out/local-opt/bin/external/sqlite_archive/_objs/sqlite/external/sqlite_archive/sqlite3.pic.d -fPIC -iquote external/sqlite_archive -iquote bazel-out/local-opt/genfiles/external/sqlite_archive -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/sqlite_archive -isystem bazel-out/local-opt/genfiles/external/sqlite_archive -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/sqlite_archive/sqlite3.c -o bazel-out/local-opt/bin/external/sqlite_archive/_objs/sqlite/external/sqlite_archive/sqlite3.pic.o)\r\nexternal/sqlite_archive/sqlite3.c: In function 'sqlite3IsNaN':\r\nexternal/sqlite_archive/sqlite3.c:28276:3: error: #error SQLite will not work correctly with the -ffast-math option of GCC.\r\n # error SQLite will not work correctly with the -ffast-math option of GCC.\r\n   ^~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 75.602s, Critical Path: 59.11s\r\nFAILED: Build did NOT complete successfully_\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["You're setting the bazel option `--copt=\"-Ofast\"`, which sets the gcc option `-Ofast`, which implies the gcc option `-ffast-math`, which is causing your problem.", "Thanks a lot, tataodd. It seems in tensorflow 1.1.0 with this --copt=\"-Ofast\", it could be compiled. Do you what's this option would doing during the compile process. \r\nAnother question is removing this option would decrease the tensorflow performance or not? Since I am running tensorflow on a intel xeon-phi TM platform which is targeting to hogh performance calculation."]}, {"number": 13633, "title": "add the missing closing parenthesis to code snippet", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 13632, "title": "W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] ", "body": "The source code I try to compile has been changed, and now it requires tf-nightly build(1.4 version). For that reason, I created new conda environment which includes 1.4 version of tensorflow.\r\n\r\nHowever, I still use the same set-up except the version of tensorflow. I've read they anticipate releasing 1.4 version of tensorflow wih cuDNN 7 instead of cuDNN 6. Is this the reason that I get these warnings? Should I upgrade the cuDNN version from 6 to 7. \r\n\r\nI'm also getting \"RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\" although I loaded the 3.6 version of tf-nightly build.\r\n\r\nThis is what I get:\r\n```\r\n2017-10-11 15:42:05.897046: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] \r\ntrain 0 4000 0.764138 1.0965 0.604099\r\n2017-10-11 15:42:40.117190: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] \r\n2017-10-11 15:42:40.118152: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] \r\n2017-10-11 15:42:40.118270: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] \r\ntrain 0 4100 0.778095 1.09562 0.603462\r\n2017-10-11 15:43:14.819378: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] \r\n2017-10-11 15:43:14.820271: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] \r\n2017-10-11 15:43:14.820375: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] \r\ntrain 0 4200 1.55563 1.09606 0.603783\r\n```\r\n\r\n### System information\r\n- **OS Platform and Distribution** : Linux Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: tf_nightly_gpu-1.4.0.dev20171010-cp36-cp36m-manylinux1_x86_64.whl\r\n- **TensorFlow version** : 1.4.0-dev20171010\r\n- **Python version**: 3.6.1 Anaconda 64bit\r\n- **CUDA/cuDNN version**:  cuda-ga2_8.0_amd64 / cudnn-8.0-linux-x64-v6.0\r\n- **GPU model and memory**: Nvidia GeForce GT 710  1GB memory\r\n- **Exact command to reproduce**: python train_xxx --train_batch_size 16 --val_batch_size 16\r\n", "comments": ["The fast_tensor_util warning is because we only build for 3.5, then copy the pip package to a 3.6 name assuming they will be compatible (I think). This was true, but we're using a bit of Cython-compiled code now. Probably Cython is a bit overcautious with the version checking here, but we should get rid of the warning anyway.\r\n\r\n@av8ramit would it be difficult to start building nightlies (and releases?) separately for Python 3.5 and 3.6? I could look into ways to suppress the warning, but it's probably there for a reason.", "Yes, we unfortunately build for Python 3.5 and then copy to 3.6 for Linux. I'll look into separate builds.", "And to be clear I have no idea what's up with the CUDA warnings.", "@yzhwang It looks like you added these warnings, with the code looking something like this:\r\n\r\n```\r\n      auto allocated = scratch_allocator->AllocateBytes(stream, size_in_bytes);\r\n      if (allocated.ok()) {\r\n        scratch = allocated.ValueOrDie();\r\n      } else {\r\n        LOG(WARNING) << allocated.status().error_message();\r\n      }\r\n```\r\n\r\nThe code looks reasonable.  Do you have any idea why we're getting these nonsense status messages?", "This means that the previous cudnn function call: https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2195 returns a size_in_bytes that caused AllocateBytes() to fail. It's interesting that the error_message() is empty though.\r\nThe solution for this is: since we are not officially supporting cudnn7 (tests and integration in progress), I recommend you to switch back to cudnn6 for now.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "The problem has been solved when I switched back to cudnn6. It was mainly about version issue as @yzhwang stated. I really appreciated it.\r\n\r\nThanks for dropping a line.", "@Narthyard Thanks for the update. Note that TensorFlow now officially support cudnn7+CUDA9. Let me know if you think this bug still exists when switching to cudnn 7.", "@yzhwang The bug is still there. I am using Tensorflow 1.5 with CUDA 9.0 with cudnn 7 ", "Still there with 1.10.1 with CUDA 9.2 and cudnn 7.2", "Still there with 1.14 and XLA on."]}, {"number": 13631, "title": "[Feature Request] Add partial_run and partial_run_setup to MonitoredSession", "body": "Hello, \r\n\r\nas mentioned in the topic, I wonder if there is a chance in the near future for MonitoredSession to support partial graph computation. ", "comments": ["@ispirmustafa, any plans to support this?", "We're adding a capability into MonitoredSession where you can use partialrun. ", "We added MonitoredSession.run_step_fn.  We hope it satisfies the use case. "]}, {"number": 13630, "title": " INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]", "body": "Hi all,\r\nI use Python 2.7.13 and Tensorflow 1.3.0 on CPU.\r\n\r\nI want to use DensNet( [https://github.com/pudae/tensorflow-densenet](url) ) for regression problem. My data contains 60000 jpeg images with 37 float labels for each image. \r\nI saved my data into tfrecords files by: \r\n\r\n`\r\ndef Read_Labels(label_path):\r\n    labels_csv = pd.read_csv(label_path)\r\n    labels = np.array(labels_csv)\r\n    return labels[:,1:]\r\n\r\ndef load_image(addr):\r\n    # read an image and resize to (224, 224)\r\n    img = cv2.imread(addr)\r\n    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    img = img.astype(np.float32)\r\n    return img\r\n\r\ndef Shuffle_images_with_labels(shuffle_data, photo_filenames, labels):\r\n    if shuffle_data:\r\n        c = list(zip(photo_filenames, labels))\r\n        shuffle(c)\r\n        addrs, labels = zip(*c)\r\n        return addrs, labels\r\n\r\ndef image_to_tfexample_mine(image_data, image_format, height, width, label):\r\n  return tf.train.Example(features=tf.train.Features(feature={\r\n      'image/encoded': bytes_feature(image_data),    \r\n      'image/format': bytes_feature(image_format),\r\n      'image/class/label': _float_feature(label),\r\n      'image/height': int64_feature(height),\r\n      'image/width': int64_feature(width),\r\n  }))\r\n\r\ndef _convert_dataset(split_name, filenames, labels, dataset_dir):\r\n  assert split_name in ['train', 'validation']\r\n\r\n  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\r\n\r\n  with tf.Graph().as_default():\r\n        \r\n        for shard_id in range(_NUM_SHARDS):\r\n          output_filename = _get_dataset_filename(dataset_path, split_name, shard_id)\r\n         \r\n          with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\r\n              start_ndx = shard_id * num_per_shard\r\n              end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\r\n              for i in range(start_ndx, end_ndx):\r\n                  sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\r\n                          i+1, len(filenames), shard_id))\r\n                  sys.stdout.flush()\r\n\r\n                  img = load_image(filenames[i])\r\n                  image_data = tf.compat.as_bytes(img.tostring())\r\n                    \r\n                  label = labels[i]\r\n                    \r\n                  example = image_to_tfexample_mine(image_data, image_format, height, width, label)\r\n                    \r\n                  # Serialize to string and write on the file\r\n                  tfrecord_writer.write(example.SerializeToString())\r\n                \r\n  sys.stdout.write('\\n')\r\n  sys.stdout.flush()\r\n  \r\ndef run(dataset_dir):\r\n\r\n    labels = Read_Labels(dataset_dir + '/training_labels.csv')\r\n    \r\n    photo_filenames = _get_filenames_and_classes(dataset_dir + '/images_training')\r\n    \r\n    shuffle_data = True \r\n    \r\n    photo_filenames, labels = Shuffle_images_with_labels(\r\n            shuffle_data,photo_filenames, labels)\r\n    \r\n    training_filenames = photo_filenames[_NUM_VALIDATION:]\r\n    training_labels = labels[_NUM_VALIDATION:]\r\n    \r\n    validation_filenames = photo_filenames[:_NUM_VALIDATION]\r\n    validation_labels = labels[:_NUM_VALIDATION]\r\n    \r\n    _convert_dataset('train',\r\n                     training_filenames, training_labels, dataset_path)\r\n    _convert_dataset('validation',\r\n                     validation_filenames, validation_labels, dataset_path)\r\n    \r\n    print('\\nFinished converting the Flowers dataset!')` \r\n________________________________________________________________________________\r\nAnd I decode it by:\r\n\r\n`\r\nwith tf.Session() as sess:\r\n\r\n    feature = {\r\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\r\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),\r\n      'image/class/label': tf.FixedLenFeature(\r\n          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),\r\n       }\r\n\r\n    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\r\n    \r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(filename_queue)\r\n   \r\n    features = tf.parse_single_example(serialized_example, features=feature)\r\n    \r\n    image = tf.decode_raw(features['image/encoded'], tf.float32)\r\n    print(image.get_shape())\r\n    \r\n    label = tf.cast(features['image/class/label'], tf.float32)\r\n\r\n    image = tf.reshape(image, [224, 224, 3])\r\n\r\n    images, labels = tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)\r\n    \r\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\n    sess.run(init_op)\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n    \r\n    for batch_index in range(6):\r\n        img, lbl = sess.run([images, labels])\r\n        img = img.astype(np.uint8)\r\n        print(img.shape)\r\n        for j in range(6):\r\n            plt.subplot(2, 3, j+1)\r\n            plt.imshow(img[j, ...])\r\n        plt.show()\r\n    \r\n    coord.request_stop()\r\n    \r\n    coord.join(threads)`\r\n______________________________________________________________________________________________________\r\nIt's all fine up to this point. But when I use the bellow commands for decoding TFRecord files:\r\n\r\n` \r\nreader = tf.TFRecordReader\r\n\r\n keys_to_features = {\r\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\r\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='raw'),\r\n      'image/class/label': tf.FixedLenFeature(\r\n          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),\r\n     }\r\n\r\n  items_to_handlers = {\r\n      'image': slim.tfexample_decoder.Image('image/encoded'),\r\n      'label': slim.tfexample_decoder.Tensor('image/class/label'),\r\n  }\r\n  \r\ndecoder = slim.tfexample_decoder.TFExampleDecoder(\r\n      keys_to_features, items_to_handlers)`\r\n_________________________________________________________________________________________________\r\n I get the following error.\r\n\r\n> INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]\r\n         [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert = Assert[T=[DT_STRING], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp, case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert/data_0)]]\r\nINFO:tensorflow:Caught OutOfRangeError. Stopping Training.\r\nINFO:sensorflow:Finished training! Saving model to disk.\r\n_______________________________________________________________________________________________\r\nTo use Densenet for my problem, I should fix this error first. \r\nCould anybody please help me out of this problem. This code works perfectly for the datasets like flowers, MNIST and CIFAR10 available at [https://github.com/pudae/tensorflow-densenet/tree/master/datasets](url) but does not work for my data.", "comments": []}, {"number": 13629, "title": "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP] [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif", "body": "Hi all,\r\nI use Python 2.7.13 and Tensorflow 1.3.0 on CPU.\r\n\r\nI want to use DensNet( [https://github.com/pudae/tensorflow-densenet](url) ) for regression problem. My data contains 60000 jpeg images with 37 float labels for each image. \r\nI saved my data into tfrecords files by: \r\n\r\n`\r\ndef Read_Labels(label_path):\r\n    labels_csv = pd.read_csv(label_path)\r\n    labels = np.array(labels_csv)\r\n    return labels[:,1:]\r\n\r\ndef load_image(addr):\r\n    # read an image and resize to (224, 224)\r\n    img = cv2.imread(addr)\r\n    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    img = img.astype(np.float32)\r\n    return img\r\n\r\ndef Shuffle_images_with_labels(shuffle_data, photo_filenames, labels):\r\n    if shuffle_data:\r\n        c = list(zip(photo_filenames, labels))\r\n        shuffle(c)\r\n        addrs, labels = zip(*c)\r\n        return addrs, labels\r\n\r\ndef image_to_tfexample_mine(image_data, image_format, height, width, label):\r\n  return tf.train.Example(features=tf.train.Features(feature={\r\n      'image/encoded': bytes_feature(image_data),    \r\n      'image/format': bytes_feature(image_format),\r\n      'image/class/label': _float_feature(label),\r\n      'image/height': int64_feature(height),\r\n      'image/width': int64_feature(width),\r\n  }))\r\n\r\ndef _convert_dataset(split_name, filenames, labels, dataset_dir):\r\n  assert split_name in ['train', 'validation']\r\n\r\n  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\r\n\r\n  with tf.Graph().as_default():\r\n        \r\n        for shard_id in range(_NUM_SHARDS):\r\n          output_filename = _get_dataset_filename(dataset_path, split_name, shard_id)\r\n         \r\n          with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\r\n              start_ndx = shard_id * num_per_shard\r\n              end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\r\n              for i in range(start_ndx, end_ndx):\r\n                  sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\r\n                          i+1, len(filenames), shard_id))\r\n                  sys.stdout.flush()\r\n\r\n                  img = load_image(filenames[i])\r\n                  image_data = tf.compat.as_bytes(img.tostring())\r\n                    \r\n                  label = labels[i]\r\n                    \r\n                  example = image_to_tfexample_mine(image_data, image_format, height, width, label)\r\n                    \r\n                  # Serialize to string and write on the file\r\n                  tfrecord_writer.write(example.SerializeToString())\r\n                \r\n  sys.stdout.write('\\n')\r\n  sys.stdout.flush()\r\n  \r\ndef run(dataset_dir):\r\n\r\n    labels = Read_Labels(dataset_dir + '/training_labels.csv')\r\n    \r\n    photo_filenames = _get_filenames_and_classes(dataset_dir + '/images_training')\r\n    \r\n    shuffle_data = True \r\n    \r\n    photo_filenames, labels = Shuffle_images_with_labels(\r\n            shuffle_data,photo_filenames, labels)\r\n    \r\n    training_filenames = photo_filenames[_NUM_VALIDATION:]\r\n    training_labels = labels[_NUM_VALIDATION:]\r\n    \r\n    validation_filenames = photo_filenames[:_NUM_VALIDATION]\r\n    validation_labels = labels[:_NUM_VALIDATION]\r\n    \r\n    _convert_dataset('train',\r\n                     training_filenames, training_labels, dataset_path)\r\n    _convert_dataset('validation',\r\n                     validation_filenames, validation_labels, dataset_path)\r\n    \r\n    print('\\nFinished converting the Flowers dataset!')` \r\n________________________________________________________________________________\r\nAnd I decode it by:\r\n\r\n`\r\nwith tf.Session() as sess:\r\n\r\n    feature = {\r\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\r\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),\r\n      'image/class/label': tf.FixedLenFeature(\r\n          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),\r\n       }\r\n\r\n    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\r\n    \r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(filename_queue)\r\n   \r\n    features = tf.parse_single_example(serialized_example, features=feature)\r\n    \r\n    image = tf.decode_raw(features['image/encoded'], tf.float32)\r\n    print(image.get_shape())\r\n    \r\n    label = tf.cast(features['image/class/label'], tf.float32)\r\n\r\n    image = tf.reshape(image, [224, 224, 3])\r\n\r\n    images, labels = tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)\r\n    \r\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\n    sess.run(init_op)\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n    \r\n    for batch_index in range(6):\r\n        img, lbl = sess.run([images, labels])\r\n        img = img.astype(np.uint8)\r\n        print(img.shape)\r\n        for j in range(6):\r\n            plt.subplot(2, 3, j+1)\r\n            plt.imshow(img[j, ...])\r\n        plt.show()\r\n    \r\n    coord.request_stop()\r\n    \r\n    coord.join(threads)`\r\n______________________________________________________________________________________________________\r\nIt's all fine up to this point. But when I use the bellow commands for decoding TFRecord files:\r\n\r\n` \r\nreader = tf.TFRecordReader\r\n\r\n keys_to_features = {\r\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\r\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='raw'),\r\n      'image/class/label': tf.FixedLenFeature(\r\n          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),\r\n     }\r\n\r\n  items_to_handlers = {\r\n      'image': slim.tfexample_decoder.Image('image/encoded'),\r\n      'label': slim.tfexample_decoder.Tensor('image/class/label'),\r\n  }\r\n  \r\ndecoder = slim.tfexample_decoder.TFExampleDecoder(\r\n      keys_to_features, items_to_handlers)`\r\n_________________________________________________________________________________________________\r\n I get the following error.\r\n\r\n> INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]\r\n         [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert = Assert[T=[DT_STRING], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp, case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert/data_0)]]\r\nINFO:tensorflow:Caught OutOfRangeError. Stopping Training.\r\nINFO:sensorflow:Finished training! Saving model to disk.\r\n_______________________________________________________________________________________________\r\nTo use Densenet for my problem, I should fix this error first. \r\nCould anybody please help me out of this problem. This code works perfectly for the datasets like flowers, MNIST and CIFAR10 available at [https://github.com/pudae/tensorflow-densenet/tree/master/datasets](url) but does not work for my data.", "comments": []}, {"number": 13628, "title": "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]          [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif", "body": "Hi all,\r\nI use Python 2.7.13 and Tensorflow 1.3.0 on CPU.\r\n\r\nI want to use DensNet( [https://github.com/pudae/tensorflow-densenet](url) ) for regression problem. My data contains 60000 jpeg images with 37 float labels for each image. \r\nI saved my data into tfrecords files by: \r\n\r\n`\r\ndef Read_Labels(label_path):\r\n    labels_csv = pd.read_csv(label_path)\r\n    labels = np.array(labels_csv)\r\n    return labels[:,1:]\r\n\r\ndef load_image(addr):\r\n    # read an image and resize to (224, 224)\r\n    img = cv2.imread(addr)\r\n    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    img = img.astype(np.float32)\r\n    return img\r\n\r\ndef Shuffle_images_with_labels(shuffle_data, photo_filenames, labels):\r\n    if shuffle_data:\r\n        c = list(zip(photo_filenames, labels))\r\n        shuffle(c)\r\n        addrs, labels = zip(*c)\r\n        return addrs, labels\r\n\r\ndef image_to_tfexample_mine(image_data, image_format, height, width, label):\r\n  return tf.train.Example(features=tf.train.Features(feature={\r\n      'image/encoded': bytes_feature(image_data),    \r\n      'image/format': bytes_feature(image_format),\r\n      'image/class/label': _float_feature(label),\r\n      'image/height': int64_feature(height),\r\n      'image/width': int64_feature(width),\r\n  }))\r\n\r\ndef _convert_dataset(split_name, filenames, labels, dataset_dir):\r\n  assert split_name in ['train', 'validation']\r\n\r\n  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\r\n\r\n  with tf.Graph().as_default():\r\n        \r\n        for shard_id in range(_NUM_SHARDS):\r\n          output_filename = _get_dataset_filename(dataset_path, split_name, shard_id)\r\n         \r\n          with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\r\n              start_ndx = shard_id * num_per_shard\r\n              end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\r\n              for i in range(start_ndx, end_ndx):\r\n                  sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\r\n                          i+1, len(filenames), shard_id))\r\n                  sys.stdout.flush()\r\n\r\n                  img = load_image(filenames[i])\r\n                  image_data = tf.compat.as_bytes(img.tostring())\r\n                    \r\n                  label = labels[i]\r\n                    \r\n                  example = image_to_tfexample_mine(image_data, image_format, height, width, label)\r\n                    \r\n                  # Serialize to string and write on the file\r\n                  tfrecord_writer.write(example.SerializeToString())\r\n                \r\n  sys.stdout.write('\\n')\r\n  sys.stdout.flush()\r\n  \r\ndef run(dataset_dir):\r\n\r\n    labels = Read_Labels(dataset_dir + '/training_labels.csv')\r\n    \r\n    photo_filenames = _get_filenames_and_classes(dataset_dir + '/images_training')\r\n    \r\n    shuffle_data = True \r\n    \r\n    photo_filenames, labels = Shuffle_images_with_labels(\r\n            shuffle_data,photo_filenames, labels)\r\n    \r\n    training_filenames = photo_filenames[_NUM_VALIDATION:]\r\n    training_labels = labels[_NUM_VALIDATION:]\r\n    \r\n    validation_filenames = photo_filenames[:_NUM_VALIDATION]\r\n    validation_labels = labels[:_NUM_VALIDATION]\r\n    \r\n    _convert_dataset('train',\r\n                     training_filenames, training_labels, dataset_path)\r\n    _convert_dataset('validation',\r\n                     validation_filenames, validation_labels, dataset_path)\r\n    \r\n    print('\\nFinished converting the Flowers dataset!')` \r\n________________________________________________________________________________\r\nAnd I decode it by:\r\n\r\n`\r\nwith tf.Session() as sess:\r\n\r\n    feature = {\r\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\r\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),\r\n      'image/class/label': tf.FixedLenFeature(\r\n          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),\r\n       }\r\n\r\n    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\r\n    \r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(filename_queue)\r\n   \r\n    features = tf.parse_single_example(serialized_example, features=feature)\r\n    \r\n    image = tf.decode_raw(features['image/encoded'], tf.float32)\r\n    print(image.get_shape())\r\n    \r\n    label = tf.cast(features['image/class/label'], tf.float32)\r\n\r\n    image = tf.reshape(image, [224, 224, 3])\r\n\r\n    images, labels = tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)\r\n    \r\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\n    sess.run(init_op)\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n    \r\n    for batch_index in range(6):\r\n        img, lbl = sess.run([images, labels])\r\n        img = img.astype(np.uint8)\r\n        print(img.shape)\r\n        for j in range(6):\r\n            plt.subplot(2, 3, j+1)\r\n            plt.imshow(img[j, ...])\r\n        plt.show()\r\n    \r\n    coord.request_stop()\r\n    \r\n    coord.join(threads)`\r\n______________________________________________________________________________________________________\r\nIt's all fine up to this point. But when I use the bellow commands for decoding TFRecord files:\r\n\r\n` \r\nreader = tf.TFRecordReader\r\n\r\n keys_to_features = {\r\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\r\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='raw'),\r\n      'image/class/label': tf.FixedLenFeature(\r\n          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),\r\n     }\r\n\r\n  items_to_handlers = {\r\n      'image': slim.tfexample_decoder.Image('image/encoded'),\r\n      'label': slim.tfexample_decoder.Tensor('image/class/label'),\r\n  }\r\n  \r\ndecoder = slim.tfexample_decoder.TFExampleDecoder(\r\n      keys_to_features, items_to_handlers)`\r\n_________________________________________________________________________________________________\r\n I get the following error.\r\n\r\n> INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]\r\n         [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert = Assert[T=[DT_STRING], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp, case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert/data_0)]]\r\nINFO:tensorflow:Caught OutOfRangeError. Stopping Training.\r\nINFO:sensorflow:Finished training! Saving model to disk.\r\n_______________________________________________________________________________________________\r\nTo use Densenet for my problem, I should fix this error first. \r\nCould anybody please help me out of this problem. This code works perfectly for the datasets like flowers, MNIST and CIFAR10 available at [https://github.com/pudae/tensorflow-densenet/tree/master/datasets](url) but does not work for my data.", "comments": ["I've opened a question on Stackoverflow. I would be thankful if you help.\r\n\r\nhttps://stackoverflow.com/questions/46687348/decoding-tfrecord-with-tfslim", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nNote that this is part of the `New issue` template:\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow."]}, {"number": 13626, "title": "[Import Error] Tensorflow is looking for wrong shared library; libnvidia-fatbinaryloader.so.375.88", "body": "### System information\r\n- **OS Platform and Distribution**:  Ubuntu 16.04\r\n- **TensorFlow installed from**: Binary\r\n- **TensorFlow version**: 1.3.0 \r\n- **Python version**: 2.7\r\n- **CUDA/cuDNN version**: 6\r\n- **GPU model and memory**: GTX 840M, 2GB\r\n- **Exact command to reproduce**: import tensorflow as tf\r\n\r\nWhen ever I try to import tensorflow (1.3.0), I encounter a ImportError where the system is unable to locate the shared library libnvidia-fatbinaryloader.so.375.88. I've looked in /usr/lib/nvidia-375 and found that another version of the .so exists (375.66). \r\nThe issue is resolved if I downgrade to tensorflow version 1.2.1", "comments": ["@allenlavoie Do you have any thoughts on this?", "Ha. 1.3 is before the RTLD_GLOBAL change so I couldn't have broken it with that was my main thought.\r\n\r\nOtherwise those look like driver version numbers. What is printed after \"Driver Version:\" in `nvidia-smi`?", "Driver Version output in nvidia-smi is 375.66", "Sounds like a bug then. Does it work if you symlink/copy .66 to .88 in that folder?", "Symlinking .66 to .88 in the folder seems to be working, did not test extensively, but got tensorflow 1.3.0 working after symlink", "I have seen such issues reported when the driver is upgraded through apt. However, I could not reproduce this myself.\r\nDid you use to have 375.66 driver installed on this machine before?\r\n\r\n", "Actually, I'm not entirely sure of the driver version before the issue.", "The problem is not with TF versions, There was mismatch b/w libcuda and nvidia driver versions. Closing the issue"]}, {"number": 13625, "title": "Something wrong while using C++ Session.Run api. ", "body": "I use tensorflow C++ api to run my model. I flower this reference: [label_image](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc).\r\nIn my model, the outputs' shape is 2d,  I run model like this:\r\n\r\n` std::vector<Tensor> outputs;`\r\n`status = session->Run(m_feed_dict, {\"model/wav_outputs\"},{}, &outputs);`\r\nHowever, I get this:\r\n`\r\ntensorflow/core/framework/tensor_shape.cc:44] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions\r\n`\r\nCould anybody tell me how to fix this?\r\n@petewarden.  @vclteam  #\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13624, "title": "speech demo is not working on duration different from 1000ms", "body": "hi\r\n\r\nwhen i try to use a different data-set (different duration) on the speech recognition demo. the training works well.but the freeze part crash with the message\r\n\r\n**\"Assign requires shapes of both tensors to match \"**\r\n\r\ntried almost everything:\r\n- delete the temp directory\r\n- pass the same param from train to freeze\r\n- make sure that all wav file are with the same size\r\n- used the nightly docker\r\n- used the latest python\r\n\r\nnothing worked!!!!!\r\n\r\nmy guess is that the problem is in the loading part\r\n\r\nany idea what next? \r\n", "comments": ["sorry my bad"]}]