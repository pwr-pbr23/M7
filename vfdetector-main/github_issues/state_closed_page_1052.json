[{"number": 21730, "title": "The Hessian computation does not work for graphs using `tf.gather`.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Binary.\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: '3.6.5 (default, Mar 29 2018, 03:28:50) \\n[GCC 5.4.0 20160609]'\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntheta = tf.get_variable('theta', shape=[10], dtype=tf.float32, initializer=tf.zeros_initializer)\r\nindices = tf.placeholder(tf.int64, shape=(None, 3))\r\nY = tf.gather(theta, indices)\r\nloss = tf.reduce_sum(tf.square(Y))\r\ngrads = tf.gradients(loss, theta)\r\nwith tf.Session() as sess:\r\n   print(sess.run(grads, feed_dict={theta: np.arange(10), indices: np.array([[1,3,6], [2,4,5]])}))\r\n\r\nhess = tf.hessians(loss, theta)\r\nwith tf.Session() as sess:\r\n   print(sess.run(hess, feed_dict={theta: np.arange(10), indices: np.array([[1,3,6], [2,4,5]])}))\r\n```\r\n\r\n### Describe the problem\r\nWhen trying to compute Hessians with a graph that uses `tf.gather`, it throws a `TypeError: 'IndexedSlices' object is not subscriptable`. This I believe is because the gradients are returned as `IndexedSlice` objects whereas Tensorflow is expecting them to be a normal array.\r\n\r\nWhat I would expect is a sort of 2D `IndexedSlice` object for `hess` such that `hess.values[i, j]` is the second partial derivative of `loss` with respect to the `theta` indices `hess.indices[i]` and `hess.indices[j]`.\r\n\r\n### Source code / logs\r\nThe traceback:\r\n\r\n```TypeError                                 Traceback (most recent call last)\r\n<ipython-input-11-b8b35a8e0799> in <module>()\r\n----> 1 hess = tf.hessians(loss, theta)\r\n\r\n~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in hessians(ys, xs, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\r\n    999           lambda j, result: (j + 1,\r\n   1000                              result.write(j, gradients(_gradient[j], x)[0])),\r\n-> 1001           loop_vars\r\n   1002       )\r\n   1003 \r\n\r\n~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name\r\n   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n-> 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2817     return result\r\n   2818 \r\n\r\n~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2638       self.Enter()\r\n   2639       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2641     finally:\r\n   2642       self.Exit()\r\n\r\n~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2588         structure=original_loop_vars,\r\n   2589         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2590     body_result = body(*packed_vars_for_body)\r\n   2591     if not nest.is_sequence(body_result):\r\n   2592       body_result = [body_result]\r\n\r\n~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in <lambda>(j, result)\r\n    998           lambda j, _: j < n,\r\n    999           lambda j, result: (j + 1,\r\n-> 1000                              result.write(j, gradients(_gradient[j], x)[0])),\r\n   1001           loop_vars\r\n   1002       )\r\n\r\nTypeError: 'IndexedSlices' object is not subscriptable\r\n```", "comments": ["@rmlarsen can you please respond or reassign to someone who knows this code?", "@rwolst \r\n\r\nCan you try with TF version 1.15 and see if the error still persists.I am not seeing any issue with with TF version 1.15.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/22592d11c095913fe2cb0769834332d3/untitled120.ipynb).Please, verify once and close the issue.Thanks!", "@ravikyram That works for me now with 1.15. I'll close the issue, however I do think in an ideal world the result should be a 2D index slice matrix. \r\n\r\nIf the size of the gathered indices is much smaller than the total number of indices, this would provide a large performance benefit. This would of course be an enhancement as it is no longer a bug."]}, {"number": 21729, "title": "`fit` method of subclassed `tf.keras.Model` doesn't work with multi inputs `tf.data.Dataset` when validation data exsits  ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: '1.11.0-dev20180820'\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n`fit` method of subclassed `tf.keras.Model` doesn't work with multi inputs `tf.data.Dataset` when validation data exsits  \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nsource = (\r\n    ((tf.constant(np.random.normal(0, 1, (1024, 2)), dtype=tf.float32),\r\n      tf.constant(np.random.normal(0, 1, (1024, 2)), dtype=tf.float32))),\r\n    tf.constant(np.random.randint(0, 2, (1024, 2)), dtype=tf.float32)\r\n)\r\n\r\ntrain = tf.data.Dataset.from_tensor_slices(source).batch(128, drop_remainder=True).repeat(10)\r\nvalid = tf.data.Dataset.from_tensor_slices(source).batch(128, drop_remainder=True).repeat(10)\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(2, activation=\"softmax\")\r\n\r\n    def call(self, inputs, training=True, mask=None):\r\n        return self.d(inputs[0] + inputs[1])\r\n\r\n\r\nm = Model()\r\nm.compile(tf.train.AdamOptimizer(0.001),\r\n          loss=[\"categorical_crossentropy\"])\r\nm.fit(x=train, validation_data=valid, steps_per_epoch=8)\r\n```\r\nlog\r\n```\r\nValueError: ('Error when checking model input: expected no data, but got:', (<tf.Tensor 'IteratorGetNext_1:0' shape=(128, 2) dtype=float32>, <tf.Tensor 'IteratorGetNext_1:1' shape=(128, 2) dtype=float32>))\r\n```", "comments": ["This seems to apply to single input / multi output models as well.", "It has been more than one month since I created this issue, is that any plan to fix it? If there is no one working on this now, I would like to contribute", "It also does not work when the input is a dictionary:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom collections import ChainMap\r\n\r\ntf.enable_eager_execution()\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(2, activation='relu')\r\n        self.dense2 = tf.keras.layers.Dense(2, activation='relu')\r\n        self.dense3 = tf.keras.layers.Dense(1)\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        x1 = inputs['x1']\r\n        x2 = inputs['x2']\r\n\r\n        x1 = self.dense1(x1)\r\n        x2 = self.dense2(x2)\r\n\r\n        y = self.dense3(x1 + x2)\r\n\r\n        return y\r\n\r\n\r\nmodel = MyModel()\r\n\r\nx1 = tf.random.normal((10, 1), 0, 1)\r\nx2 = tf.random.normal((10, 1), 1, 2)\r\ny = x1 + 2 * x2\r\n\r\nds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices({'x1': x1}),\r\n                          tf.data.Dataset.from_tensor_slices({'x2': x2}))) \\\r\n    .map(lambda *dicts: dict(ChainMap(*dicts)))\r\n\r\ny = tf.data.Dataset.from_tensor_slices(y)\r\n\r\nds = tf.data.Dataset.zip((ds, y)) \\\r\n    .batch(5) \\\r\n    .repeat()\r\n\r\nmodel.compile(tf.train.AdamOptimizer(), 'mean_squared_error', metrics =['mae'])\r\nmodel.fit(ds, epochs=1, steps_per_epoch=10, validation_data=ds)\r\n```\r\nIt gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/Users/user/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1614, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/Users/user/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py\", line 681, in fit_loop\r\n    verbose=verbose)\r\n  File \"/Users/user/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 144, in configure_callbacks\r\n    val_data = val_inputs + val_targets\r\nTypeError: unsupported operand type(s) for +: 'dict' and 'list'\r\n```\r\n", "@hsm207 is it solved in the newer version?", "@byzhang no. Just checked with 1.12.0 and still the same problem.", "no errors with version 1.14.1-dev20190528, closed."]}, {"number": 21728, "title": "Can't compile frozen facenet graph (Proper version)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: not mobile\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: clang\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**: `./tfcompile --graph=frozen_20170512-110547.pb --config=frozen_20170512-110547.pbtxt --cpp_class=\"my_super_class\" --target_features=\"+avx2\"`\r\n\r\n### Describe the problem\r\nI've tried to compile facenet https://github.com/davidsandberg/facenet frozen graph - 20180408-102900\r\nAnd I've got this error:\r\n`INVALID ARGUMENTS: Unable to functionalize control flow in graph: Switch ('InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/Switch_1') has operands ('InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/Switch_1/Switch' and 'InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/pred_id') that have different switch depths (1 != 0)\r\n`\r\n\r\nIs there any workaround for this? Does Tensorflow support compiling BatchNorm?\r\n### Source code / logs\r\nMy .pbtxt:\r\n```\r\nfeed {\r\n  id { node_name: \"input\" }\r\n  shape {\r\n    dim { size: 160 }\r\n    dim { size: 160 }\r\n  }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"embeddings\" }\r\n}\r\n```\r\n", "comments": ["Apologies for delay in response. I think facenet repo can be better platform to address this question, since its not a bug or feature request on the lines of TensorFlow core framework. You can post this issue in facenet repo from [here](https://github.com/davidsandberg/facenet/issues). Thanks!"]}, {"number": 21727, "title": "Tensorflow compile error with win10", "body": "### System information\r\n- **OS Win10**\r\n- **TensorFlow version 1.9**:\r\n- **Python version 3.5.2**:\r\n- **swig version 3.0.12**:\r\n- **cmake version 3.12.1**:\r\n- **git version 2.18.0**:\r\n\r\n### Describe the problem\r\nWhen I compiled the Tensorflow with source code, there exist some errors: \r\n\r\n c_api.cc.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"void __cdecl tensorflow::NewRemoteDevices(class tensorflow::Env *,class tens\r\norflow::WorkerCacheInterface *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >\r\nconst &,class std::function<void __cdecl(class tensorflow::Status const &,class std::vector<class tensorflow::Device *,\r\nclass std::allocator<class tensorflow::Device *> > *)>)\" (?NewRemoteDevices@tensorflow@@YAXPEAVEnv@1@PEAVWorkerCacheInt\r\nerface@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$function@$$A6AXAEBVStatus@tensorflow@@PEAV\r\n?$vector@PEAVDevice@tensorflow@@V?$allocator@PEAVDevice@tensorflow@@@std@@@std@@@Z@5@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Stat\r\nus __cdecl `anonymous namespace'::GetAllRemoteDevices(class std::vector<class std::basic_string<char,struct std::char_t\r\nraits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char\r\n>,class std::allocator<char> > > > const &,class tensorflow::WorkerCacheInterface *,class std::unique_ptr<class tensorf\r\nlow::DeviceMgr,struct std::default_delete<class tensorflow::DeviceMgr> > *)\" (?GetAllRemoteDevices@?A0x87213361@@YA?AVS\r\ntatus@tensorflow@@AEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_stri\r\nng@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@PEAVWorkerCacheInterface@3@PEAV?$unique_ptr@VDeviceMgr@tensor\r\nflow@@U?$default_delete@VDeviceMgr@tensorflow@@@std@@@5@@Z) \u4e2d\u88ab\u5f15\u7528 [D:\\Tensorflow_20180816\\tensorflow\\tensorflow\\contrib\\\r\ncmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  c_api.cc.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: class tensorflow::Status __cdecl tensorflow::SessionMgr::CreateSessi\r\non(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class tensorflow::Se\r\nrverDef const &,bool)\" (?CreateSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@\r\n@V?$allocator@D@2@@std@@AEBVServerDef@2@_N@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE\r\n_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context *)\" (?UpdateTFE_ContextWithServerDef@?\r\nA0x87213361@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z) \u4e2d\u88ab\u5f15\u7528 [D:\\Tensorflow_20180816\\tensorflow\\tens\r\norflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  c_api.cc.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: class tensorflow::Status __cdecl tensorflow::SessionMgr::WorkerSessi\r\nonForSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std:\r\n:shared_ptr<struct tensorflow::WorkerSession> *)\" (?WorkerSessionForSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV\r\n?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$shared_ptr@UWorkerSession@tensorflow@@@5@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570\r\n\"class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef\r\n const &,struct TFE_Context *)\" (?UpdateTFE_ContextWithServerDef@?A0x87213361@@YA?AVStatus@tensorflow@@HAEBVServerDef@3\r\n@PEAUTFE_Context@@@Z) \u4e2d\u88ab\u5f15\u7528 [D:\\Tensorflow_20180816\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal\r\n.vcxproj]\r\n  c_api.cc.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"class tensorflow::eager::EagerClientCache * __cdecl tensorflow::eager::NewGr\r\npcEagerClientCache(class std::shared_ptr<class tensorflow::GrpcChannelCache>)\" (?NewGrpcEagerClientCache@eager@tensorfl\r\now@@YAPEAVEagerClientCache@12@V?$shared_ptr@VGrpcChannelCache@tensorflow@@@std@@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __\r\ncdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context\r\n*)\" (?UpdateTFE_ContextWithServerDef@?A0x87213361@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z) \u4e2d\u88ab\u5f15\u7528 [\r\nD:\\Tensorflow_20180816\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  D:\\Tensorflow_20180816\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\pywrap_tensorflow_internal.dll : fatal error\r\n LNK1120: 4 \u4e2a\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u547d\u4ee4 [D:\\Tensorflow_20180816\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcx\r\nproj]\r\n\r\ncan anyone help me, thanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "i have a same problem , how to slove it?", "@alextp Can you take a look at what's going on here? #22268 looks like the same problem.", "This issue is lacking a lot of information, such as bazel or cmake, system details, etc.\r\nI will close this.\r\nInterested parties may follow the issue #22268"]}, {"number": 21726, "title": "assign_moving_average function explodes when smoothing bounded inputs", "body": "### System information\r\n- Problem encountered when running custom code\r\n- Linux Ubuntu 16.04 LTS\r\n- TensorFlow installed from source\r\n- TensorFlow version 1.8.0\r\n- Python 3.5.2 \r\n- Bazel Version 0.11.1\r\n- GCC 4.9.4\r\n- CUDA 9.1.85, CuDNN 7.1.2\r\n- GPU: Gtx1070, 8GB memory\r\n\r\n### Describe the problem\r\nIn training a network consisting of the architecture described below, I encountered a problem with batch normalization layers. I found that the Keras batch normalization moving average parameters were unstable. While decreasing loss was observed over several iterations, it would regularly explode to a very high value. In observing the batch normalization weights in Tensorboard I notice that the moving mean in batch normalization layers explode at the same iteration of this loss explosion, while the batch statistics remain well behaved as shown in the picture below. This doesn't add up as the moving averages are a stable function of the batch statistics.\r\n\r\nI noticed that the only Keras code employed in updating these parameters is the `assign_moving_average` function in `tensorflow/tensorflow/python/training/moving_averages.py`. Looking deeper this function in turn uses the `assign_sub` function in `tensorflow/tensorflow/python/ops/state_ops.py` which by default does not use any locking and hence its behaviour may be undefined. Perhaps this is the cause, and if so, should the default be to ignore locking? How certain is it that this wont run into undefined behaviour?\r\n\r\nWhen I replace the Tensorflow `assign_moving_average` function with a simple handcrafted `(1-self.momentum)*mean + (self.momentum)*self.moving_mean` in the Keras BatchNormalization object, this problem never occurs.\r\n\r\n### Source code / logs\r\nFor reference I am training a three layer convolutional network with three max pooling layers and three batch normalization layers, followed by two dense layers and two batch normalization layers at the output. The network employs a triplet loss and so the weights are each employed three times prior to back propogation. I hope to get a chance some time in the future to reproduce this problem in a simple example that I can share here.\r\n\r\nThe `decay` parameter to the tensorflow `assign_moving_average` function in this case is 0.99.\r\n\r\n![batchnormweightexplosion](https://user-images.githubusercontent.com/18222703/44322682-51db8c00-a403-11e8-9d1b-f8554b626412.png)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I guess these parameters need to be in the specific format provided. I've provided the parameters again below, adhering to the template format:\r\n\r\n### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.4\r\n- **CUDA/cuDNN version**: CUDA 9.1.85, CuDNN 7.1.2\r\n- **GPU model and memory**: Gtx1070, 8GB memory\r\n- **Exact command to reproduce**: N/A", "@brainnoise Is there enough information here to go on?", "@MCMcCallum Is this still an issue for you? Can you please test with TF 1.13.rc0 and check? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@MCMcCallum Is your handcrafted update fix still working? I appear to be having the same problem.", "I am facing the name problem. The moving_mean and moving_variance go to nan at training time and the effect of this can only be seen during the evaluation time. I am not using keras I am coding in tensorflow APIs and I am using tf.layers.batch_normalization. Is there a workaround? I am using tensorflow 1.13"]}, {"number": 21725, "title": "Quantize model: does not have MinMax information, and is not a constant array when quantize the concat op layer", "body": "tensorflow verison: 1.10\r\nproblem:I will describle this problem in two cases(one can quantize succeed, the other can not success).\r\ntf.concat(\r\n    values,\r\n    axis,\r\n    name='concat'\r\n). This's the concat api.\r\n\r\n1.if i use concat like this\uff1a tf.concat(values=['t1', 't2'], axis=3). My model can quantize succeed.\r\n2.if i use concat like this\uff1a tf.concat(values=['t1'], axis=3).some errors occured: concat does not have MinMax information, and is not a constant array. Cannot proceed with quantization.\r\nt1 and t2 are the layer name.\r\n\r\nI quantize my model like this:\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=/home/admin_pc/model_test/output_qt1.pb \\\r\n--output_file=/home/admin_pc/model_test/mobilenet_qg.tflite \\\r\n--input_fromat=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--input_array=image \\\r\n--output_array=Pose/concat_stage7 \\\r\n--input_shape=1,224,224,3 \\\r\n--change_concat_input_ranges=false", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@suharshs : Mind taking a look?", "Hi,\r\n\r\nWhat model are you trying to quantize? And can you provide what the input nodes to your concat operation are in the success case and the failure case? And can you paste the exact error message from TOCO?\r\n\r\nThanks!", "Hi, suharshs. I send an email to you(suharshs@gmail.com) to describe the model structure in which is the success case and failure case. The error message is 'concat_stage' does not have MinMax information, and is not a constant array. Cannot proceed with quantization.\r\nAborted (core dumped)", "@suharshs , i can provide the .pb file. do you need?", "@suharshs ,hi. can you tell me this's tensorflow bug or my source bug ?  If my source bug,i will check my model and source code soon. thanks", "Hi, I have the same issue \"Array TFLite_Detection_PostProcess does not have MinMax information, and is not a constant array. Cannot proceed with quantization.\"\r\nMy tensorflow version is 1.10.1 with GPU. I've training my data with ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config in Google Machine Learning (TPU). \r\n The exported method that I use is object_detection/export_tflite_ssd_graph.py, I did TOCO with  \"pet breeds dataset\" example and TOCO works well. My problem is when I try to use TOCO with my own data set. I did the very same process for both data set (my own and TF example). The frozen graph with my own data set woks well in my computer (object_detection/export_inference_graph.py). \r\n\r\nTOCO command:\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops\r\n\r\nbest regards.", "yep ! I solve with apply fake Quatized\r\n\r\n--default_ranges_min=0 \\\r\n--default_ranges_max=6 \\\r\n\r\nI hope it will be usefull to other people :)", "Hi @1icas, \r\n\r\nThe issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the concats in your network need quantization information for TOCO and aren't supported out of the box. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after the concats, see how the rewriter does it: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\r\n\r\nThat being said this can be complicated and very error prone, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. If it doesn't provide sufficient speedup (or if it has an error, please file another issue!!) then we can consider this full quantization.", "> yep ! I solve with apply fake Quatized\r\n> \r\n> --default_ranges_min=0 \r\n> --default_ranges_max=6 \\\r\n> \r\n> I hope it will be usefull to other people :)\r\n\r\n\r\n\r\n> yep ! I solve with apply fake Quatized\r\n> \r\n> --default_ranges_min=0 \r\n> --default_ranges_max=6 \\\r\n> \r\n> I hope it will be usefull to other people :)\r\n\r\nHello @sepulm01 , thanks for your suggestion to solve this issue. I have encountered the same problem and solved it with your way. However, I am wondering how to set the --default_ranges_min and --default_ranges_max values? The 0 and 6 are obtained via some calculation or something else? \r\n\r\nThanks,\r\nYuhang", "thanks for your suggestion to solve this issue. I have encountered the same problem and solved it with your way. but when i evalute the result of converted tflite model,i get a zerodivisionerror:float (num_correct)/float(cnt_total),did you get this issues?if you have any idea about this please let me know,thaks a lot", "> Hi,\r\n> \r\n> What model are you trying to quantize? And can you provide what the input nodes to your concat operation are in the success case and the failure case? And can you paste the exact error message from TOCO?\r\n> \r\n> Thanks!\r\n\r\nhi , i meet some some problem when i want to quatize my model.(cnn+max_pool+bn+relu+fc). but it returns a error \"FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array kws_model/KWS_Model/tower_0/CNN_V1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\" i tried to add default value can solve the problem, but i know it the best solution. \r\n\r\nor i also tried to remove the bn layer , it can avoid the last error , but it return a new error message \"prediction does not have MinMax information, and is not a constant array. Cannot proceed with quantization.\" the prediction is the last layer of my network,\r\n(final_fc_weights = tf.get_variable(\"final_fc_weights\", [cfg.fc2_size, cfg.n_class], initializer=initializer)\r\n174     final_fc_bias = tf.get_variable(\"final_fc_bias\", [cfg.n_class], initializer=initializer)\r\n176     final_fc = tf.add(tf.matmul(fc2, final_fc_weights), final_fc_bias, name=\"prediction\")\r\n177     return final_fc\r\n)\r\nhope you give me some suggestions, thanks.", "Based on feedback that the contrib/quantize quantization-aware training tool is a bit brittle and hard to use on some model architectures, we have released a [post-training integer quantization tool](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba), that requires a small calibration dataset. Please take a look at the [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb) and give it a try, it should work much better! And let us know if you run into any issues.\r\n\r\nClosing this issue, since we are rethinking and working on an api to replace contrib/quantize quantization-aware-training (although post-training quantization above should be sufficient for the majority of use cases).\r\n\r\nThanks!\r\n-Suharsh\r\n", "> Hi @1icas,\r\n> \r\n> The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the concats in your network need quantization information for TOCO and aren't supported out of the box. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after the concats, see how the rewriter does it: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\r\n> \r\n> That being said this can be complicated and very error prone, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. If it doesn't provide sufficient speedup (or if it has an error, please file another issue!!) then we can consider this full quantization.\r\n\r\n@suharshs i tried clicking on the link above and I was redirected to a non existent page. I am having a similar issue where I was successfully able to quantize and produce a tflite graph but I noticed that minmaxvars node was missing after the concatV2 node. Can you please provide with the link agagin or direct me to something that might help?", "Here is an updated link https://github.com/tensorflow/tensorflow/tree/r1.8/tensorflow/contrib/quantize\r\n\r\nWe have a new Keras 2.0 QAT api that is more configurable, please take a look here: https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html", "@suharshs do you have any example of manually adding the quatize in contrib to the graph?\r\nIt is a little hard to verify the way of excecuting it"]}, {"number": 21723, "title": "I got a keras_applications ModuleNotFoundError when I compile my tensorflow 1.10", "body": "hi, I want compile my tensorflow 1.10 to update. But I got an error when I compile it:\r\nModuleNotFoundError: No module named 'keras_applications'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines failed build steps.\r\nAnyone has the same error?", "comments": ["Solution in https://github.com/tensorflow/tensorflow/issues/21518#issuecomment-411875069"]}, {"number": 21722, "title": "TensorFlow Samples Do Not Run - Can't get past documented 16358, 17393 regardless version used", "body": "Installed without error tensorflow 1.10 as instructed on the tensorflow.org site(https://www.tensorflow.org/install/install_windows)\r\n\r\nExecuted without error the following sample code from tensorflow.org site (https://www.tensorflow.org/tutorials/keras/basic_text_classification)\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nprint(tf.__version__)\r\nimdb = keras.datasets.imdb\r\n\r\nWhen the next sample line was executed it generated the errors documented under \"import tensorflow failed, \"ImportError: DLL load failed\". Even after install visual studio 2015, Microsoft Visual C++ 2015 Redistributable Update 3.  #17393\" displayed:\r\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\r\n\r\nI tried uninstalling & installing every version of tensorflow from 1.5 to present version. The error occurred for each version.\r\n\r\nIn attempting to get this short, simple tensorflow example, I have encountered not only this DLL issue but also:\r\nAttributeError: module 'numpy' has no attribute '__version___'\r\nModuleNotFoundError: No module named 'keras'\r\n(Request for updating keras/datasets files to r1.5  #16358)\r\n\r\nDoes not appear the documented tensorflow site sample(s) works.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution: Windows 8\r\nTensorFlow installed from: tensor flow.org\r\nTensorFlow version: tried 1.5 to most recent 1.10\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce:\r\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\r\nMobile device: N/A\r\n\r\nERRORS:\r\nImportError: DLL load failed \r\n(As reported in #17393)\r\nAttributeError: module 'numpy' has no attribute 'version_'\r\n(As reported in multiple postings)\r\nModuleNotFoundError: No module named 'keras'\r\n(As reported in #16358)\r\n", "The problem statement from the tensor flow sample program is:\r\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\r\n\r\nInitially, the noted statement generated a DLL error on this statement after successfully running the following statements from the posted tensorflow sample:\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nprint(tf.version)\r\nimdb = keras.datasets.imdb\r\n\r\nI installed the recommended C++ Redistributable as advised by the error displayed (17393).\r\n\r\nThe problem statement then began to generate the following error.\r\nAttributeError: module 'numpy' has no attribute \r\n\r\nI removed tensor flow 1.10 and installed tensorflow 1.5 as advised by another posting.\r\n\r\nThe problem statement then began to generate the following error.\r\n\r\nModuleNotFoundError: No module named 'eras' (#16358)\r\n\r\nI then tried different versions of tensor flow on the basis that various postings hinted this was a version issue that was reportedly fixed in the next release.\r\n\r\nSo far, none of the postings on this topic resolve this issue.", "1) DLL Error:\r\nImportError: Could not find 'msvcp140.dll'. TensorFlow requires that this DLL be\r\n installed in a directory that is named in your %PATH% environment variable. You\r\n may install this DLL by downloading Visual C++ 2015 Redistributable Update 3 fr\r\nom this URL: https://www.microsoft.com/en-us/download/details.aspx?id=53587\r\n\r\n2) NUMPY Error:\r\n>>> (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"...\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\datasets\\imdb.py\", line 77, in load_data\r\n    indices = np.arrange(len(x_train))\r\nAttributeError: module 'numpy' has no attribute 'arrange'\r\n\r\n3) KERAS Error:\r\noriginal posted sample code  \r\nimport tensorflow as tf\r\nfrom tensorflow import eras\r\n\r\nOR\r\n\r\nRecommended fix\r\nGet Keras error\r\n>>> from tensorflow.python import keras\r\n>>> from keras.datasets import imdb\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'keras'\r\n\r\n", "Regarding error 2: `indices = np.arrange(len(x_train))` appears to be a typo, but in the current version of imdb.py this has been fixed (on Aug 1).\r\n\r\nI cannot reproduce the library problem. \r\n\r\nImporting directly from tensorflow.python is definitely not a good idea. Here's a workaround (though this style of import should really work). I have tested this on 1.8, so it's not a new thing:\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> imdb = tf.keras.datasets.imdb\r\n>>> imdb\r\n<module 'tensorflow.tools.api.generator.api.keras.datasets.imdb' from '/Users/wicke/Library/Python/2.7/lib/python/site-packages/tensorflow/tools/api/generator/api/keras/datasets/imdb/__init__.pyc'>\r\n>>> dir(imdb)\r\n['__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', 'get_word_index', 'load_data']\r\n>>> \r\n```", "Nagging Assignee @martinwicke: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'll close this. Our new API generation should have resolved this. "]}, {"number": 21721, "title": "Analysis of target '//tensorflow/python/eager:core' failed; build aborted", "body": "### System information\r\n- MacOS 10.13.4\r\n- commit hash `c894b86481da31c291e6d763f68c9f60a811f7fe`\r\n- Python 2.7.14 in conda virtualenv\r\n- Bazel\r\n\r\n```\r\nBuild label: 0.15.2-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 17 13:06:19 2018 (1531832779)\r\nBuild timestamp: 1531832779\r\nBuild timestamp as int: 1531832779\r\n```\r\n- No CUDA\r\n- to reproduce `bazel test //tensorflow/python/...`\r\n\r\n### Describe the problem\r\n\r\nI have added functionality to tensorflow/tensorflow/python/ops/image_ops_impl.py and corresponding unit tests in tensorflow/tensorflow/python/ops/image_ops_test.py\r\n\r\nI originally forked tensorflow from the master branch, made these changes on my local machine, rebased and commit. No changes to C++ code. \r\n\r\nThen I created and activated a virtualenv.\r\n\r\n### Source code / logs\r\n\r\n```\r\nERROR: /Users/isaacsultan/Code/tensorflow/third_party/python_runtime/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n    File \"/Users/isaacsultan/Code/tensorflow/third_party/py/python_configure.bzl\", line 308\r\n        _create_local_python_repository(repository_ctx)\r\n    File \"/Users/isaacsultan/Code/tensorflow/third_party/py/python_configure.bzl\", line 270, in _create_local_python_repository\r\n        _check_python_lib(repository_ctx, python_lib)\r\n    File \"/Users/isaacsultan/Code/tensorflow/third_party/py/python_configure.bzl\", line 213, in _check_python_lib\r\n        _fail((\"Invalid python library path: %...))\r\n    File \"/Users/isaacsultan/Code/tensorflow/third_party/py/python_configure.bzl\", line 28, in _fail\r\n        fail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Invalid python library path: /usr/local/lib/python2.7/dist-packages\r\n and referenced by '//third_party/python_runtime:headers'\r\nERROR: Analysis of target '//tensorflow/python:control_flow_util' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 4.603s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (8 packages loaded)\r\nFAILED: Build did NOT complete successfully (8 packages loaded)\r\n    currently loading: tensorflow/core ... (2 packages)\r\n```", "comments": ["Update:\r\n\r\nEDIT: After re-running ./configure:\r\n\r\n```\r\n(tensorflow) Isaacs-MacBook:tensorflow isaacsultan$ bazel clean --expunge\r\nINFO: Starting clean.\r\n(tensorflow) Isaacs-MacBook:tensorflow isaacsultan$ bazel test //tensorflow/python/...\r\nStarting local Bazel server and connecting to it...\r\n........................\r\nERROR: /private/var/tmp/_bazel_isaacsultan/0e2667ab20883652d759a6a805575b2d/external/local_config_cc/BUILD:50:5: in apple_cc_toolchain rule @local_config_cc//:cc-compiler-darwin_x86_64: Xcode version must be specified to use an Apple CROSSTOOL. If your Xcode version has changed recently, try: \"bazel clean --expunge\" to re-run Xcode configuration\r\nERROR: Analysis of target '//tensorflow/python/eager:core' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-darwin_x86_64' failed; build aborted\r\nINFO: Elapsed time: 15.184s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (93 packages loaded)\r\nFAILED: Build did NOT complete successfully (93 packages loaded)\r\n    currently loading: tensorflow/core ... (2 packages\r\n```"]}, {"number": 21720, "title": "Build fails for versions higher than 0.10 whenever MPI support is on.", "body": "I have tried to build TF from source for r1.4, r1.6, r1.8, r1.9 with the same result. It always fails if MPI support is on.\r\n\r\nOnly r0.10 build does not fail.\r\n\r\nGoogling, for which revision and which MPI version would work,  has yielded no results.", "comments": ["@phalexo I remember I fixed a compilation error in #18907. The MPI should work after that PR. Can you share more details about the issue and provide the information that was requested (in new issue template) when you open the new issue?", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04, with 16.04 kernel\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nSupermicro 4028GR-TRT, dual Xeons, 256GB RAM\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nTried r1.4-r1.9\r\n- **Python version**:\r\n3.5\r\n- **Bazel version (if compiling from source)**:\r\n0.16\r\n- **GCC/Compiler version (if compiling from source)**:\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper\r\nOFFLOAD_TARGET_NAMES=nvptx-none\r\nOFFLOAD_TARGET_DEFAULT=1\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 7.3.0-21ubuntu1~14.04' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=gcc4-compatible --disable-libstdcxx-dual-abi --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 7.3.0 (Ubuntu 7.3.0-21ubuntu1~14.04)\r\n\r\n- **CUDA/cuDNN version**:\r\n9.2/7.1.4_9.2\r\n- **GPU model and memory**:\r\nTitan X/Maxwell X 4, 12GB RAM\r\n- **Exact command to reproduce**:\r\n bazel build -c opt --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nor\r\n\r\n bazel build  //tensorflow/tools/pip_package:build_pip_package\r\n\r\nERROR: /home/developer/tensorflow/tensorflow/contrib/mpi/BUILD:48:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_utils' failed (Exit 1)\r\nIn file included from tensorflow/contrib/mpi/mpi_utils.cc:18:0:\r\n./tensorflow/contrib/mpi/mpi_utils.h: In member function 'const int tensorflow::MPIUtils::GetSourceID(const string&) const':\r\n./tensorflow/contrib/mpi/mpi_utils.h:49:11: error: 'FATAL' was not declared in this scope\r\n       LOG(FATAL) << \"Failed to convert worker name to MPI index: \" << task_id;\r\n           ^~~~~\r\n./tensorflow/contrib/mpi/mpi_utils.h:49:11: note: suggested alternative: 'ESTALE'\r\n       LOG(FATAL) << \"Failed to convert worker name to MPI index: \" << task_id;\r\n           ^~~~~\r\n           ESTALE\r\n./tensorflow/contrib/mpi/mpi_utils.h:49:7: error: 'LOG' was not declared in this scope\r\n       LOG(FATAL) << \"Failed to convert worker name to MPI index: \" << task_id;\r\n       ^~~\r\ntensorflow/contrib/mpi/mpi_utils.cc: In constructor 'tensorflow::MPIUtils::MPIUtils(const string&)':\r\ntensorflow/contrib/mpi/mpi_utils.cc:33:3: error: 'CHECK' was not declared in this scope\r\n   CHECK(worker_name.size() < max_worker_name_length)\r\n   ^~~~~\r\ntensorflow/contrib/mpi/mpi_utils.cc:35:64: warning: format not a string literal and no format arguments [-Wformat-security]\r\n   snprintf(my_name, max_worker_name_length, worker_name.c_str());\r\n                                                                ^\r\ntensorflow/contrib/mpi/mpi_utils.cc:41:25: error: 'INFO' was not declared in this scope\r\n   if (proc_id == 0) LOG(INFO) << \"MPI process-ID to gRPC server name map: \\n\";\r\n                         ^~~~\r\ntensorflow/contrib/mpi/mpi_utils.cc:41:21: error: 'LOG' was not declared in this scope\r\n   if (proc_id == 0) LOG(INFO) << \"MPI process-ID to gRPC server name map: \\n\";\r\n                     ^~~\r\ntensorflow/contrib/mpi/mpi_utils.cc:45:11: error: 'INFO' was not declared in this scope\r\n       LOG(INFO) << \"Process: \" << i\r\n           ^~~~\r\ntensorflow/contrib/mpi/mpi_utils.cc:45:7: error: 'LOG' was not declared in this scope\r\n       LOG(INFO) << \"Process: \" << i\r\n       ^~~\r\ntensorflow/contrib/mpi/mpi_utils.cc:47:25: error: 'endl' is not a member of 'std'\r\n                 << std::endl;\r\n                         ^~~~\r\ntensorflow/contrib/mpi/mpi_utils.cc:47:25: note: suggested alternative: 'end'\r\n                 << std::endl;\r\n                         ^~~~\r\n                         end\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n", "@phalexo The issue you encounter seems to be the same as #18363 which has been fixed in PR #18907.\r\n\r\nCan you try the latest master and see if it resolves your issue?", "I had to delete the folder, clone it again, and it did compile this time. Thanks.", "Nagging Assignee @rohan100jain: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21719, "title": "Protobuf 3.6.1 import issue with tensorflow 1.10", "body": "Version: master/v3.6.1\r\nLanguage: Python 3.6x64 on Windows 10\r\n\r\nSteps to reproduce the behavior:\r\n**Followed instructions mentioned on tensorflow.org's guide on installing tensorflow on Windows\r\n(www.tensorflow.org/install/install_windows)\r\n\r\n1. Installed CUDA 9.0 (latest release supported by tf_gpu)\r\n2. Installed cuDNN v7 for (CUDA 9.0)\r\n3. Installed tensorflow_gpu using native-pip\r\n**protobuf is part of tf gpu installation \r\n4. After the installation, i wanted to test the tf, so as usual tried:\r\n  import tensorflow\r\n  but got an error:==>\r\n  Traceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\Niraj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Niraj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\Niraj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\Niraj\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nInitially i thought the issue was related to CUDA or cdnn itself, but after hours of trial & error with multiple cuda & cudnn version combination, i decided to focus on the library mentioned in the error message itself.  \r\nI'm not very new to python extensions so obvious work around for me was to try earlier version of protobuf, so i tried: \r\n\r\npip uninstall protobuf\r\npip install protobuf==3.6.0\r\n\r\nand tada!! tensorflow started working.\r\nI'm still not sure if this was actually caused by protobuf or incorrect installations etc.\r\nMy sincere apologies if this issue is unrelated.\r\nPlease let me know if you need more details on this issue.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Hi,\r\nPfb more details:\r\nHave I written custom code : n/a\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: python native pip without\u200b Anaconda\r\nTensorFlow version: 1.10\r\nBazel version: n/a \r\nCUDA/cuDNN version: Cuda: 9.0, cuDNN v7.05\r\nGPU model and memory: GTX1060 6gb\r\nExact command to reproduce: just tried to import tensorflow after pip installation\r\nMobile device: n/a\r\n\r\nI was able to fix the issue after downgrading protobuf from 3.6.1 to 3.6.0\r\nHope this helps!", "Hi,\r\nI have a few questions. What do you mean by:\r\n```\r\nInstalled tensorflow_gpu using native-pip\r\n**protobuf is part of tf gpu installation\r\n```\r\n\r\nAlso, did you upgrade your tensorflow installation from a previous version, or did you install tf from scratch on your machine?", "Same here, exactly same problem after upgrading to tensorflow 1.10, also tried clean install tensorflow 1.10.\r\n\r\nprotobuf 3.6.1 doesn't not work for CPU and GPU, but protobuf 3.6.0 works for both.", "**protobuf is part of tf gpu installation - meaning protobuf was installed as part of pip installation of tensorflow.\r\nAlso, i did the installation from scratch.", "What a nightmare getting this to work on Windows 10!\r\nI had to do this:\r\n1) Install Python3.6 (downgraded from 3.7)\r\n2) Install and create a virtualenv \r\n`pip3 install virtualenv`\r\n`virtualenv venv`\r\n3) Activate the virtualenv\r\n`./venv/Scripts/activate`\r\nNow comes the guessing game to figure out which version of protobuf plays nicely with which version of tensorflow\r\nIn the end I matched up tensorflow==1.5.0 and protobuf==3.4.0\r\n4) Install protobuf\r\n`pip3 install protobuf==3.4.0`\r\n5) Install tensorflow\r\n`pip3 install tensorflow==1.5.0`\r\n\r\nNow I can test it:\r\n`(venv) $ python`\r\n`Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.`\r\n`>>> import tensorflow as tf`\r\n\r\n\r\n\r\n", "I had exactly the same issue. I spend the entire day trying to figure out what was wrong. I installed and uninstalled python, tf, etc.. million times on my new Windows 10 machine. This morning I saw this post. I deleted protobuf 3.6.1 and installed 3.6.0. Bingo ! everything is working. Thanks a lot", "@acozzette @xfxyjwf\r\nIs there a known backwards compatibility issue between protobuf 3.6.1 and 3.6.0?", "@gunan No, I don't know of any backward compatibility issues between 3.6.0 and 3.6.1. @anandolee Do you have any ideas what it could be?", "This is https://github.com/protocolbuffers/protobuf/issues/5046 . @TeBoring what's the solution to this issue?", "upgrade python to >=3.6.1\nOn Fri, Aug 31, 2018 at 23:12 Feng Xiao <notifications@github.com> wrote:\n\n> This is protocolbuffers/protobuf#5046\n> <https://github.com/protocolbuffers/protobuf/issues/5046> . @TeBoring\n> <https://github.com/TeBoring> what's the solution to this issue?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21719#issuecomment-417836175>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE9H5Y9pGgyWmQ51UG7vzRJftmi5P8Itks5uWiVLgaJpZM4WDEPV>\n> .\n>\n", "Nagging Assignees @gunan, @poxvoculi: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looking at the comments, this actually looks like a missing backwards compatibility issue in protobuf between 3.6.0 and 3.6.1\r\nUnfortunately, solution is to upgrade protobuf to 3.6.1.\r\nI will make that change in our setup.py script, and close this issue.", "DLL LOAD ERROR SPECIFIED ERROR COUDN'T FOUND \r\nWINDOWS 7 INSTALLED TENSORFLOW 1.12 WITH CUDNN 9  THROUGH ANACONDA  PYTHON VERSION 3.6.7\r\n\r\n(SRI) C:\\Users\\SRIKANTH>python -c \"import tensorflow\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\imp.py\", line 243, in load_modu\r\nle\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\imp.py\", line 343, in load_dyna\r\nmic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\__init\r\n__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\site-packages\\tensorflow\\python\r\n\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\imp.py\", line 243, in load_modu\r\nle\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\SRIKANTH\\Anaconda3\\envs\\SRI\\lib\\imp.py\", line 343, in load_dyna\r\nmic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtimE", "To add an update, this issue is not only related to tf-gpu version but also occurs for cpu version if installed with python version 3.6.0\r\nthis can still be solved by replacing default protobuf version with 3.6.0\r\n", "I have installed tensorflow==1.5.0 and protobuf==3.6.0 and 3.4.0 alternatively but there is still an issues.\r\n\r\n/home/nkanwal/bert/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' \r\nas a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/nkanwal/bert/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' \r\nas a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/nkanwal/bert/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' \r\nas a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n\r\nplease suggest some solution", "> I have installed tensorflow==1.5.0 and protobuf==3.6.0 and 3.4.0 alternatively but there is still an issues.\r\n> \r\n> /home/nkanwal/bert/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type'\r\n> as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n> _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n> /home/nkanwal/bert/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type'\r\n> as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n> _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n> /home/nkanwal/bert/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type'\r\n> as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n> np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n> \r\n> please suggest some solution\r\n\r\nI din know the older versions of tensorflow were still being used. Anyway, i would suggest keeping only one version of protobuff. I think tf might be using the higher version which might be causing the issue.\r\nNot sure which exact version of protobuf will work with tf 1.5. U might have to do some trial n error but while doing so, pls try one at a time ( as in 1 version in ur virtual env at a time)\r\nHope this help!", "  File \"stylize.py\", line 97, in <module>\r\n    main()\r\n  File \"stylize.py\", line 57, in main\r\n    graph_def.ParseFromString(f.read())\r\n  File \"C:\\Users\\shubham\\myenv\\lib\\site-packages\\google\\protobuf\\message.py\", line 185, in ParseFromString\r\n    self.MergeFromString(serialized)\r\n  File \"C:\\Users\\shubham\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1083, in MergeFromString\r\n    if self._InternalParse(serialized, 0, length) != length:\r\n  File \"C:\\Users\\shubham\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1109, in InternalParse\r\n    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)\r\n  File \"C:\\Users\\shubham\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\decoder.py\", line 850, in SkipField\r\n    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)\r\n  File \"C:\\Users\\shubham\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\decoder.py\", line 820, in _RaiseInvalidWireType\r\n    raise _DecodeError('Tag had invalid wire type.')\r\ngoogle.protobuf.message.DecodeError: Tag had invalid wire type.\r\n\r\nAfter reading everything, Still, I am getting this error.\r\n"]}, {"number": 21718, "title": "Error importing tensorflow.  Unless you are using bazel, you should not try to import tensorflow from its source directory; please exit the tensorflow source tree, and relaunch your python interpreter from there.", "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Python Files\\csv_to_tfrecord.py\", line 16, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Ketan Ingale\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@iketan312 Is this still an issue?", "is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21717, "title": "parallel_for's jacobian and batch_jacobian fail when inputs are disjoint in the comp. graph", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: reproducible on macOS High Sierra 10.13.4 and Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: reproducible on both source and binary\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15.2-homebrew\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Mobile device**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.parallel_for import gradients\r\n\r\nx = tf.placeholder(tf.float32, (None,))\r\ny = tf.placeholder(tf.float32, (None,))\r\nprint(gradients.jacobian(x, y))\r\n```\r\n\r\n### Describe the problem\r\nWhen the example script is run, the following exception is thrown\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test-2.py\", line 5, in <module>\r\n    print(gradients.jacobian(x, 5))\r\n  File \"/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/gradients.py\", line 58, in jacobian\r\n    pfor_outputs = control_flow_ops.pfor(loop_fn, output_size)\r\n  File \"/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 122, in pfor\r\n    outputs.append(converter.convert(loop_fn_output))\r\n  File \"/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1075, in convert\r\n    output = self._convert_helper(y)\r\n  File \"/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1108, in _convert_helper\r\n    assert isinstance(y, ops.Tensor), y\r\nAssertionError: None\r\n```\r\n\r\nThis is due to the `tf.gradients(y, x)` (which sits under to hood of `jacobian`) returning `None` when `y` is independent of `x` in terms of computational graph, that is, when `y` is not achievable from `x` by any computation path, and `parallel_for` does not like working with `None`s.\r\n\r\nThis is not affecting me as I have found workarounds, but it's still a bug that needs fixing.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "Done\r\n(originally removed them since the problem is reproducible on macOS)", "A fix has been submitted and should sync out soon. Thanks for reporting."]}, {"number": 21716, "title": "parallel_for: use absl wrapper and add converter for SoftplusGrad", "body": "Consider the following snippet\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.parallel_for import gradients\r\n\r\nx = tf.placeholder(tf.float32, (None,))\r\ny = tf.nn.softplus(x)\r\n\r\nprint(gradients.jacobian(y, x))\r\n```\r\n\r\nOn the current master it fails with\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 7, in <module>\r\n    print(gradients.jacobian(y, x))\r\n  File \"/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/gradients.py\", line 58, in jacobian\r\n    pfor_outputs = control_flow_ops.pfor(loop_fn, output_size)\r\n  File \"/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 122, in pfor\r\n    outputs.append(converter.convert(loop_fn_output))\r\n  File \"/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1075, in convert\r\n    output = self._convert_helper(y)\r\n  File \"/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1214, in _convert_helper\r\n    if flags.FLAGS.op_conversion_fallback_to_while_loop:\r\n  File \"/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/absl/flags/_flagvalues.py\", line 488, in __getattr__\r\n    raise _exceptions.UnparsedFlagAccessError(error_message)\r\nabsl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --op_conversion_fallback_to_while_loop before flags were parsed.\r\n```\r\n\r\nThis is caused by the fact that no converter is registered for the `SoftplusGrad` op, and when parallel_for sees no converter defined, it tries to fall back to a while loop, if this is allowed with a special CLI flag. The problem, though, is that it uses raw `absl` call which requires one to first explicitly parse command line arguments. To mitigate that, one should use a flags wrapper from the `tensorflow.platform`, which is what I did.\r\n\r\nAlso, this pull introduces a converter for `SoftplusGrad` in the same way as, say, `SigmoidGrad`.", "comments": ["@agarwal-ashish, any news?"]}, {"number": 21715, "title": "Implement `extract_volume_patches` (issue #17843)", "body": "Implement `extract_volume_patches` (issue #17843). `rates` not supported as of now.\r\n\r\nThe code is pretty similar to that of `extract_image_patches`. I'm not quite sure as to whether adding the `pbtxt` file alone suffices for the documentation; I would appreciate it if someone could comment on this.", "comments": ["Sorry, I forgot to tag relevant people:\r\n@tatianashp @asimshankar @ezhulenev\r\nCould you have a look at this please? Thanks in advance!", "@hsgkim Thank you for this PR.", "@hsgkim I\u2019m with limited access to internet right now, will take a look at this PR on monday, but from my phone everything seems ok. Though I have one question, why did you commented all the code related to rates? If it works and it\u2019s an api consistent with image patches, why not to enable it right now?", "@ezhulenev As mentioned before in the issue, none of Eigen's `extract_volume_patches` functions let one pass `rates` as an argument; it's all fixed to `(1, 1, 1, 1, 1)`. I left them as comments in case someone finds a workaround for that and decides to add `rates`.", "Already forgot about that Eigen thing :) adding some comments should help people like me", "Will do, thanks for the feedback :D!", "Added comments.", "@ezhulenev @akshaym I'm really sorry for bothering you, but it says feedback/copybara's check failed. But the Details hyperlink is just linked to `http://cl/210001543` and I can't figure out what went wrong. I can build tf with my commits and the tests pass; can I ignore copybara or do I have to fix something?", "`tensorflow/tools/api/tests:api_compatibility_test` is failing (as expected)\r\n\r\nsee https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/tools/api/tests/README.txt for required steps", "@ezhulenev Thanks! Updated golden and I disabled the pylint check for `bad-whitespace` in `extract_volume_patches_op_test.py` just in case; it kept complaining about me adding spaces for expected output `np.array`s (hinders readability otherwise).", "@ezhulenev Sorry for bothering you, but can I get a progress update on this please? It says the check has been pending for a week and I was wondering if it's something I did wrong. Thanks!", "@hsgkim It's stuck in internal copybara import, whenever it gets through, I'll approve it and it should get merged (it will need additional review from tf-api owners, but that should be fast for this PR)", "@hsgkim I remember about this PR, will find out tomorrow why it's still not available internally for review", "@ezhulenev Thanks for your response!", "@ezhulenev I fixed all of them; sorry for such silly mistakes :(", "@ezhulenev Fixed `array_ops.cc` and `golden`.", "@ezhulenev I changed the name of the input (from `images` to `input`) for everything but the test Python file; I felt iffy about using `input` as a name of an argument as it already is a name of a Python function. I checked the committed files and it looked like I didn't have anything else to change, what do you think? Thanks again in advance :) ", "@martinwicke Could you do an API review please? Thanks!", "@ezhulenev can you check why the internal submit is stuck? Looks like a lint problem maybe.", "I'm triggering tests to check whether we can see the problem here.", "Fixed [lint issues](https://source.cloud.google.com/results/invocations/225bc691-0352-4963-9ed1-78236d4ee911/log).\r\n[GPU Python3](https://source.cloud.google.com/results/invocations/fb3ee2ec-bcd6-4777-8fda-a1adaacee97c/targets) and [XLA](https://source.cloud.google.com/results/invocations/bf32cdde-1d08-400d-86dc-3baf0bd7f2bb/targets) fails don't look like they were caused by me?", "@ezhulenev Could you approve the changes please?\r\n@martinwicke Is the API review over, and can this be merged when the CI checks pass?\r\n\r\nThanks in advance!", "Yes this can be merged. ", "@ezhulenev It looks like the test is stuck again, do I have to fix something else? Sorry for the bother.", "@martinwicke do you know what's happening? cr/211828838 seems approved, any way to force submit?", "The tool got stuck. Sometimes, removing and adding an approval can help. It's unstuck now and submitting, should come through soon."]}, {"number": 21714, "title": "Error converting custom model to .tflite using Toco", "body": "### System information\r\n- **Have I written custom code**: Yes, see neural net structure [here](https://github.com/gmalsagov/Emotion-Multiclass-CNN/blob/master/cnn-embeddings/cnn_embeddings.py)\r\n- **OS Platform and Distribution**: Mac OS X 10.13.4\r\n- **TensorFlow installed from (source or binary)**: pip (binary)\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: 0.15.2-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: v1.10.0-rc1-19-g656e7a2b34\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\ntoco \\\r\n--graph_def_file='path to frozen_model.pb' \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--inference_type=FLOAT \\\r\n--input_type=INT \\\r\n--input_arrays=input_x \\\r\n--output_arrays=output/predictions \\\r\n--input_shapes=1,78 \\\r\n--output_file='path to model.tflite'\r\n\r\n### Describe the problem\r\nI have a Tensorflow Convolutional Neural Network model for Multi-class Sentiment Analysis. I removed Dropout layer from the frozen model and it produces inference fine. When I am trying to convert it into a TfLite version using Toco I am getting an error that has something to do with dimensions of an input tensor (see error below). This tensor leads to a convolution tensor whose function takes a 4d input tensor.\r\n\r\n`2018-08-19 15:27:50.977853: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:130] Check failed: input_shape.dimensions_count() == 4 Conv ops require 4D inputs. Input array \"embedding/ExpandDims\" is 3D.`\r\n\r\nOutput from terminal:\r\n\r\n`\r\n2018-08-19 15:27:45.418932: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/tensorflow/venv/bin/toco\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 370, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/tensorflow/venv/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 366, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 143, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 374, in convert\r\n    dump_graphviz_video=self.dump_graphviz_video)\r\n  File \"/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 246, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 106, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2018-08-19 15:27:50.968669: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 40 operators, 57 arrays (0 quantized)\r\n2018-08-19 15:27:50.975773: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 40 operators, 57 arrays (0 quantized)\r\n2018-08-19 15:27:50.977853: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:130] Check failed: input_shape.dimensions_count() == 4 Conv ops require 4D inputs. Input array \"embedding/ExpandDims\" is 3D.\r\n\r\nNone\r\n`\r\n\r\n### Source code / logs\r\n\r\nI am using tf.expand_dims command only when declaring embedding layer to expand tensor to 4d prior to feeding it into a tf.nn.conv2d function:\r\n\r\n`self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\r\nconv = tf.nn.conv2d(self.embedded_chars, W, strides=[1, 1, 1, 1], padding='VALID', name='conv')\r\n`\r\n\r\nDoes this mean that tensorflow lite doesn't support expand dims yet? Would appreciate an explanation or any possible walkarounds for my issue.\r\n\r\nThanks", "comments": ["Update:\r\n\r\nI've replaced tf.nn.expand_dims() with alternative function\r\n\r\n`\r\nshape1 = self.embedded_chars.get_shape().as_list()\r\n`\r\n`print(shape1)`\r\n`dim = shape[1:]`\r\n`dim = [-1] + dim + [1]`\r\n`self.embedded_chars_expanded = tf.reshape(self.embedded_chars, dim)`\r\n`shape2 = self.embedded_chars_expanded.get_shape().as_list()`\r\n`print(shape2)`\r\n\r\nBoth shapes are now (None, 40, 300, 1) as required. Model trains and does inference fine but during conversion to .tflite new error pops up.\r\n\r\n`\r\nRuntimeError: TOCO failed see console for info.\r\n2018-08-19 22:13:20.177661: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 31 operators, 48 arrays (0 quantized)\r\n2018-08-19 22:13:20.189193: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 31 operators, 48 arrays (0 quantized)\r\n2018-08-19 22:13:20.192098: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:442] Check failed: input_flat_size >= product_non_wildcard_dims (300 vs. 12000)Array not large enough to fill the requested dimensions for Reshape op with output \"embedding/Reshape\". Are your input shapes correct?\r\n`\r\n\r\n@aselle Would you please take a look at this?", "Can you provide your model and a link to the code that's producing the model if possible? Any additional information will help in debugging.", "@gargn here is the link to my code as well as the frozen graph. \r\n\r\nhttps://github.com/gmalsagov/Emotion-Multiclass-CNN/tree/dev2/cnn-embeddings\r\n[frozen_model.pb.zip](https://github.com/tensorflow/tensorflow/files/2312354/frozen_model.pb.zip)\r\n", "To answer your original question - ExpandDims is supported in TOCO when the input tensor has a static shape.\r\n\r\nHowever, I am having trouble reproducing your issue from the frozen graph you provided. I tried running the command below on the tf-nightly build (installed via `pip install tf-nightly`) and I got an error stating that RandomUniform is not supported. When I added the `--allow_custom_ops` flag, TOCO converted the model without error.\r\n```\r\ntflite_convert \\\r\n--graph_def_file=$TENSORFLOW_FILE \\\r\n--input_arrays=input_x \\\r\n--output_arrays=output/predictions \\\r\n--input_shapes=1,40 \\\r\n--output_file=$TFLITE_FILE\r\n```\r\n\r\nA few changes I did have to make from your original command:\r\n1. I removed `--input_type=INT`. The supported flags are listed [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_reference.md). It is possible you intended on using `--inference_input_type`. `--inference_input_type=QUANTIZED_UINT8` is used for models where the input is quantized but the model is a float model.\r\n2. I changed the `input_shapes` from `1,78` to `1,40`.\r\n\r\nJust to summarize, I was able to run the following command without TOCO reporting an error:\r\n```\r\ntflite_convert \\\r\n--graph_def_file=$TENSORFLOW_FILE \\\r\n--input_arrays=input_x \\\r\n--output_arrays=output/predictions \\\r\n--input_shapes=1,40 \\\r\n--output_file=$TFLITE_FILE \\\r\n--allow_custom_ops\r\n```\r\n\r\nAlso, just as a note, `tflite_convert` is aliased to the same tool as `toco`. I use `tflite_convert` for my commands because that is what we use in our documentation.\r\n\r\nCan you try running your model with the tf-nightly build instead of 1.10 and see if you get the same results?", "Thank you @gargn the issue has been solved when used tf-nightly instead of tensorflow 1.10.0", "When running a model without the supported ops, the recommended steps are one of the following:\r\n1. Add a custom op as described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md).\r\n2. Only use the ops that are currently supported by TFLite in your model.\r\n\r\nClosing this issue. Feel free to reopen, or open a new bug, if you hit any other issues.", "> When running a model without the supported ops, the recommended steps are one of the following:\r\n> \r\n> 1. Add a custom op as described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md).\r\n> 2. Only use the ops that are currently supported by TFLite in your model.\r\n> \r\n> Closing this issue. Feel free to reopen, or open a new bug, if you hit any other issues.\r\n\r\n@gargn ,hello ,the link `here` is 404,can you resend?"]}, {"number": 21713, "title": "No module named 'tensorflow.compiler'", "body": "```\r\nimport tensorflow as tf\r\ntf.contrib.layers.conv2d(inputs=h_pool1,num_outputs=64,kernel_size=[5,5],stride=[1,1],padding='SAME', activation_fn=tf.nn.relu) \r\n```\r\nWhen  I executing above code,the errors orrurs,help me please!\r\nenv:tensorflow1.9+jupyter notebook +windows10 + python 3.6\r\n```\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\tpu\\python\\tpu\\tpu_feed.py in <module>()\r\n     26 from six.moves import xrange  # pylint: disable=redefined-builtin\r\n     27 \r\n---> 28 from tensorflow.compiler.xla.experimental.xla_sharding import xla_sharding\r\n     29 from tensorflow.compiler.xla.python_api import xla_shape\r\n     30 from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n\r\nModuleNotFoundError: No module named 'tensorflow.compiler'\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform:windows10 64bit\r\nanaconda3: anaconda Command line client (version 1.6.14)\r\npython3.6:Python 3.6.5 :: Anaconda, Inc.\r\ntensorflow1.9 for cpu:I installed it from the file tensorflow-1.9.0-cp36-cp36m-win_amd64.whl\r\nhttps://files.pythonhosted.org/packages/e7/88/417f18ca7eed5ba9bebd51650d04a4af929f96c10a10fbb3302196f8d098/tensorflow-1.9.0-cp36-cp36m-win_amd64.whl\r\n\r\nGPU model nvidia 1060 6G memory\r\nCUDA/cuDNN version    9.0/9.0\r\n\r\nall code:\r\n```\r\nimport tensorflow as tf\r\nx = tf.placeholder(tf.float32, [None, 24,24,3])\r\nx_image = tf.reshape(x, [-1,24,24,3])\r\nh_conv1 = tf.contrib.layers.conv2d(x_image, 64,5,1, \"SAME\", activation_fn=tf.nn.relu)\r\n```\r\n", "Can I run the codes above in cpu mode", "Go through the following [issue](https://stackoverflow.com/questions/51909312/modulenotfounderror-no-module-named-tensorflow-compiler) and it should help you in resolving this issue. Thanks!"]}, {"number": 21712, "title": "ci_build: Upgrade the Python 'six' compatibility module", "body": "", "comments": ["@pragyaak, @rthadur Version v1.12.0 of __six__ was released during this review cycle so I created #25510"]}, {"number": 21711, "title": "Add myself to CODEOWNERS for contrib/gan", "body": "", "comments": ["Thanks @joel-shor. I will include this PR in the batch update. Closing this."]}, {"number": 21710, "title": "GPU cannot detected using Tensor C++ Windows library that was built from sources", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["**System information**\r\n\u2022Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n\u2022OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 Professional SP 1\r\n\u2022Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n\u2022TensorFlow installed from (source or binary): source\r\n\u2022TensorFlow version (use command below):1.9.0-rc0\r\n\u2022Python version:3.6\r\n\u2022Bazel version (if compiling from source):  No\r\n\u2022GCC/Compiler version (if compiling from source): Visual C++ 14.0 (Visual Studio 2015 x64 Release)\r\n\u2022CUDA/cuDNN version:  CUDA 9.0/cuDNN 7.2.1 for WIndows 7\r\n\u2022GPU model and memory: NVIDIA Quadro K2100M\r\n\u2022Exact command to reproduce:\r\nI performed the following guide:\r\n[https://medium.com/@shiweili/building-tensorflow-c-shared-library-on-windows-e79c90e23e6e](url)\r\nAlso, I worked according to what I succeeded to get from here:\r\n[https://www.tensorflow.org/install/install_sources](url)\r\n\r\n**Some of the guide steps was adapted:**\r\n1. Configuration was changed as described above\r\n2. CMake was invoked like this:\r\nC:\\AAG\\HPC\\Apps\\build>C:\\CMake\\bin\\cmake.exe ..\\tensorflow-master\\tensorflow\\con\r\ntrib\\cmake -DCMAKE_BUILD_TYPE=Release -Dtensorflow_ENABLE_GPU=ON -Dtensorflow_BU\r\nILD_PYTHON_BINDINGS=OFF -Dtensorflow_ENABLE_GRPC_SUPPORT=ON -Deigan_PATCH_FILE=O\r\nN -Dtensorflow_BUILD_PYTHON_TESTS=OFF -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AV\r\nX -Dtensorflow_BUILD_SHARED_LIB=ON -DCUDA_TOOLKIT_ROOT_DIR=\"C:/Program Files/NVI\r\nDIA GPU Computing Toolkit/CUDA/v9.0\" -DCUDNN_HOME=\"C:/Program Files/NVIDIA GPU C\r\nomputing Toolkit/CUDA/v9.0\" -G\"Visual Studio 14 2015 Win64\"\r\n3. Build was invoked like this:\r\nC:\\AAG\\HPC\\Apps\\build>\"C:\\Program Files (x86)\\MSBuild\\14.0\\Bin\\MSbuild.exe\" /m:1\r\n /p:CL_MPCount=1 /p:Configuration=Release /p:Platform=x64 /p:PreferredToolArchit\r\necture=x64 All_BUILD.vcxproj\r\n\r\n**Problem description:**\r\nI made an application via Visual Studio 2015 which was linked with all Tensorflow libraries under the x64 platform and  Release configuration.\r\n\r\nI succeeded to load and run the lenet5_mnist_frozen.pb file while using the default device (CPU).\r\nI got the expected results as described in the TensorRT-Developer-Guide document.\r\n\r\nThis is the code for loading the pb file and Session creation:\r\n```\r\nboost::filesystem::path m_filePath;\r\ntensorflow::Session *m_session;\r\ntensorflow::GraphDef m_graphDef;\r\n\r\nReadBinaryProto(tensorflow::Env::Default(), m_filePath.string(), &m_graphDef);\t\r\ntensorflow::SessionOptions options;\r\ntensorflow::NewSession(tensorflow::SessionOptions(options), &m_session);\r\nm_session->Create(m_graphDef);\r\n\r\n```\r\n**Of course**, tensor input and tensor output were declared according to the examples provided in the  stock example script provided in TensorFlow and according to the lenet5_mnist_frozen.pb definitions.\r\n\r\n**If the entire code will be required I will provide it.**\r\n\r\nAfter that, I wanted to change the device to the GPU without any further change.\r\n So, I tried to separately add a call to any one of these options:\r\ntensorflow::graph::SetDefaultDevice(\"/gpu:0\", &graph_def);\r\ntensorflow::graph::SetDefaultDevice(\"/device:GPU:0\", &graph_def);\r\n\r\nBut then the Session run started to return **an error that the key wasn't found**.\r\n\r\nSo I added the following:\r\n`std::vector<tensorflow::DeviceAttributes> listDevices;\r\nm_session->ListDevices(&listDevices);\r\n\r\nstd::cout << \"TF identified devices list:\" << std::endl;\r\nfor (auto& tVal : listDevices)\r\n{\r\n\tstd::cout << tVal.DebugString() << std::endl;\r\n}`\r\n\r\nAnd I saw that the tensorflow::Session ListDevices function only returns this value:\r\n\"/job:localhost/replica:0/task:0/device:CPU:0\"\r\n\r\nWhile activating this Python code with Tensorflow 1.10:\r\n`import tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n# Creates a graph.\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\n# Creates a session with log_device_placement set to True.\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n# Runs the op.\r\nprint(sess.run(c))`\r\n\r\nI'm getting the following prints:\r\n1.10.0\r\n2018-08-19 10:26:06.096006: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-08-19 10:26:06.432040: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties: \r\n**name: Quadro K2100M major: 3 minor: 0 memoryClockRate(GHz): 0.6665**\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.86GiB\r\n2018-08-19 10:26:06.433040: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-19 10:26:07.615158: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-19 10:26:07.615158: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0 \r\n2018-08-19 10:26:07.615158: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N \r\n2018-08-19 10:26:07.616158: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1636 MB memory) -> physical GPU (device: 0, name: Quadro K2100M, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Quadro K2100M, pci bus id: 0000:01:00.0, compute capability: 3.0\r\n2018-08-19 10:26:07.832180: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\direct_session.cc:288] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Quadro K2100M, pci bus id: 0000:01:00.0, compute capability: 3.0\r\n\r\n2018-08-19 10:26:15.257922: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:935] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-19 10:26:15.257922: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:935] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-19 10:26:15.258922: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:935] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\na: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\nb: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n[[22. 28.]\r\n [49. 64.]]\r\n\r\n**Question:**\r\nGiven that my GPU is active and is recognized by the Python script, what should I do in order for the tensorflow::Session in C++ to run on my GPU as well?\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "No,\r\nIt can be closed.\r\nI couldn't find any solution except when I tried with Tensorflow 1.5 version and then it worked fine and I could identify the GPU.\r\nThanks", "Closing as per https://github.com/tensorflow/tensorflow/issues/21710#issuecomment-421607741."]}, {"number": 21709, "title": "Cherry-picks for Keras on Cloud TPUs", "body": "", "comments": []}, {"number": 21708, "title": "tf.contrib.estimator.InMemoryEvaluatorHook runs every step", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: same code as in #21590\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: from conda, also tried with pip - same effect\r\n- **TensorFlow version (use command below)**: conda: 1.9.0, pip: v1.10.0-0-g656e7a2b34 1.10.0 \r\n- **Python version**: conda: Python 3.6.6 :: Anaconda custom (64-bit), pip: 3.5.2\r\n- **CUDA/cuDNN version**: CUDA Version 9.0.176 \r\n- **GPU model and memory**:  GeForce GTX 1080 Ti 12GB\r\n- **Exact command to reproduce**: python3 cnn_mnist.py\r\n\r\n### Describe the problem\r\nWhen evaluating during training with tf.contrib.estimator.InMemoryEvaluatorHook, regardless of the parameter every_n_iter, evaluation takes place every single step.\r\n```\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [1/1]\r\nINFO:tensorflow:Finished evaluation at 2018-08-18-21:39:14\r\nINFO:tensorflow:Saving dict for global step 3: accuracy = 0.5, global_step = 3, loss = 0.6996169\r\nINFO:tensorflow:Starting evaluation at 2018-08-18-21:39:14\r\nINFO:tensorflow:Graph was finalized.\r\n2018-08-18 23:39:14.756353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-18 23:39:14.756388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-18 23:39:14.756407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-18 23:39:14.756425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-18 23:39:14.756536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10195 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [1/1]\r\nINFO:tensorflow:Finished evaluation at 2018-08-18-21:39:14\r\nINFO:tensorflow:Saving dict for global step 4: accuracy = 0.49, global_step = 4, loss = 0.69892335\r\nINFO:tensorflow:Starting evaluation at 2018-08-18-21:39:14\r\nINFO:tensorflow:Graph was finalized.\r\n2018-08-18 23:39:14.881081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-18 23:39:14.881101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-18 23:39:14.881106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-18 23:39:14.881111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-18 23:39:14.881199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10195 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [1/1]\r\nINFO:tensorflow:Finished evaluation at 2018-08-18-21:39:14\r\nINFO:tensorflow:Saving dict for global step 5: accuracy = 0.49, global_step = 5, loss = 0.6985285\r\nINFO:tensorflow:Starting evaluation at 2018-08-18-21:39:14\r\nINFO:tensorflow:Graph was finalized.\r\n```\r\n### Source code / logs\r\nCode is the same as in  #21590: \r\n[cnn_mnist.txt](https://github.com/tensorflow/tensorflow/files/2299878/cnn_mnist.txt)\r\n\r\nSame problem exists with my custom estimator.\r\n", "comments": ["Installing tf-nightly has fixed the problem. (Probably this PR #20822). "]}, {"number": 21707, "title": "AttributeError: 'AddLocationForm' object has no attribute 'found_location'", "body": "### Describe the problem\r\ni am building a kivy  weather app. but suddenly i got an error, that i cant solve it please help me\r\ni am using python version 2 and ubuntu 18.04\r\nmy source code and debug log are following bellow.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Source code \r\nimport json\r\nfrom kivy.app import App\r\nfrom kivy.uix.boxlayout import BoxLayout\r\nfrom kivy.properties import ObjectProperty\r\nfrom kivy.network.urlrequest import UrlRequest\r\n\r\n\r\nclass AddLocationForm(BoxLayout): \r\n     search_input = ObjectProperty()\r\n\t \r\n\t \r\n     def search_location(self):  \r\n         search_template = \"http://api.openweathermap.org/data/2.5/find?q={}&type=like\"\r\n         search_url = search_template.format(self.search_input.text)\r\n         request = UrlRequest(search_url, self.found_location)  \r\n\t\t \r\n\t def found_location(self,request,data):\r\n          data = json.loads(data.decode()) if not isinstance(data, dict) else data\r\n         cities = [\"{} ({})\".format(d['name'], d['sys']['country'])\r\n            for d in data['list']]\r\n         self.search_results.item_strings = cities \t \r\n\t\t\r\nclass WeatherApp(App):\r\n     pass\r\n\r\nif __name__ == '__main__':\r\n     WeatherApp().run()\r\n\r\n ###DEBUGGING log:\r\n   exec(__kvlang__.co_value, idmap)\r\n   File \"/home/midhun/Downloads/W2/weather.kv\", line 16, in <module>\r\n     on_press: root.search_location() \t\t\t\r\n   File \"main.py\", line 15, in search_location\r\n     request = UrlRequest(search_url, self.found_location)  \r\n AttributeError: 'AddLocationForm' object has no attribute 'found_location'\r\n\r\nweather.kv:\r\nAddLocationForm:\r\n\r\n<AddLocationForm>:\r\n    orientation: \"vertical\"\r\n\tsearch_input: search_box\r\n\tsearch_results: search_results_list \r\n    BoxLayout:\r\n        height: \"40dp\"  \r\n        size_hint_y: None\r\n        TextInput:\r\n\t\t    id: search_box\r\n            size_hint_x: 50  \r\n        Button:\r\n            text: \"Search\"\r\n            size_hint_x: 25\r\n            on_press: root.search_location() \t\t\t\r\n        Button:\r\n            text: \"Current Location\"\r\n            size_hint_x: 25  \r\n    ListView:  \r\n        id: search_results_list  \r\n        item_strings: [ ]  \r\n", "comments": ["Present error has nothing to do with Tensor, please post error in stackOver flow", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21706, "title": "ModuleNotFoundError: No module named 'official' Error with Official MNIST Model in Colaboratory Notebook", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nI am using stock example script from Official MNIST Model found here: https://github.com/tensorflow/models/tree/master/official/mnist\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nWindows 10\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nI don't know\r\n- **TensorFlow version (use command below)**:\r\n1.9\r\n- **Python version**:\r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nDon't know\r\n- **GCC/Compiler version (if compiling from source)**:\r\nDon't know\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nfrom official.mnist import dataset\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI have been unable to import the necessary libraries to run the Official MNIST model in a Colaboratory Notebook, although I have been able to run this model successfully using an Anaconda Prompt. I have tried copying the \"Official\" folder found within the Tensorflow directory into the same folder in my google drive where I save my Colaboratory notebooks (in the hopes that this would be the local directory that the notebook would attempt to access when making imports).\r\n\r\nHow can I import these libraries from the Tensorflow \"Official\" folder in a Colaboratory Notebook? Is it possible to do so? If not, could this be added as a feature?\r\n\r\nThank you!\r\n\r\n### Source code / logs\r\nMy Colaboratory Notebook can be found [here](https://colab.research.google.com/drive/13Gl9l-xnIL4OeNgd-Wqrb2bMcG10NUF1)\r\n\r\nHere are text file attachments containing the code I tried using, as well as the error trace:\r\n\r\n[MNIST Colab Code.txt](https://github.com/tensorflow/tensorflow/files/2299747/MNIST.Colab.Code.txt)\r\n[Error Trace - MNIST Colab.txt](https://github.com/tensorflow/tensorflow/files/2299748/Error.Trace.-.MNIST.Colab.txt)\r\n", "comments": ["Hello Mr. Wolf,\n\nIt is ok with me if you stop nagging the assignee - I just ended up using\nthe Keras library to make my model instead.\n\nThank you for your persistence in attempting to resolve this issue.\n\nOn Thu, Nov 1, 2018 at 8:50 AM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Assignee @robieta <https://github.com/robieta>: It has been 74\n> days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21706#issuecomment-435029911>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZZjtMlATe-Z3lndAJxyird2d4fU2Cxqks5uqu4egaJpZM4WCp2I>\n> .\n>\n\n\n-- \nMarkus Ville Tiitto, Pharm.D.\nPhD Student\nClinical & Experimental Therapeutics\nDepartment of Pharmaceutical Sciences\nUniversity of Kentucky, College of Pharmacy\nLee Todd, Jr. Building Rm 466\n789 S. Limestone St.\nLexington, KY 40536\nVilleTiitto2@gmail.com\nmarkus.tiitto@uky.edu\n(412) 296-9192\n", "Apologies for my very late response. Skimming your colab, the issue appears to be a path issue. In order to import `official` you need to add the models directory to your path, either with the `PYTHONPATH` environment variable in a shell or with `sys.path` inside python. So something like:\r\n\r\n```\r\nsys.path.append(\"/some/folders/and/then/models\")\r\nfrom official.mnist import dataset\r\n```\r\n\r\nI'm going to go ahead and close this since you found a solution."]}, {"number": 21705, "title": "tf.parallel_stack", "body": "parallel_stack doesn't support TF_BOOL type . is right?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I think bool CPU kernel has been registered for the OP, so it should work for tf.bool: https://github.com/tensorflow/tensorflow/blob/51100a8de57ef53e36a8a9f5a9829cbd33fbed04/tensorflow/core/kernels/inplace_ops.cc#L175", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21704, "title": "[cmake] deduplicate symbols in libtensorflow.so and pywrap_tensorflow_internal.so", "body": "They contained nearly the same symbols. We can build another shared object shipping common symbols, and the common shared object can be used for both libtensorflow.so and pywrap_tensorflow_internal.so .\r\n\r\nThis is an example patch, which produces `libtensorflow_.so` as the common shared object. (maybe better named `libtensorflow_internal.so`)\r\n\r\n```patch\r\ndiff --git a/tensorflow/contrib/cmake/tf_python.cmake b/tensorflow/contrib/cmake/tf_python.cmake\r\nindex 6d86daf5f1..ad180396c5 100755\r\n--- a/tensorflow/contrib/cmake/tf_python.cmake\r\n+++ b/tensorflow/contrib/cmake/tf_python.cmake\r\n@@ -577,13 +577,8 @@ if(WIN32)\r\n     )\r\n endif(WIN32)\r\n \r\n-# pywrap_tensorflow_internal is a shared library containing all of the\r\n-# TensorFlow runtime and the standard ops and kernels. These are installed into\r\n-# tf_python/tensorflow/python/.\r\n-add_library(pywrap_tensorflow_internal SHARED\r\n-    ${pywrap_tensorflow_internal_src}\r\n+add_library(tensorflow_ SHARED\r\n     $<TARGET_OBJECTS:tf_c>\r\n-    $<TARGET_OBJECTS:tf_c_python_api>\r\n     $<TARGET_OBJECTS:tf_core_lib>\r\n     $<TARGET_OBJECTS:tf_core_cpu>\r\n     $<TARGET_OBJECTS:tf_core_framework>\r\n@@ -601,6 +596,21 @@ add_library(pywrap_tensorflow_internal SHARED\r\n     $<$<BOOL:${tensorflow_ENABLE_GPU}>:$<TARGET_OBJECTS:tf_stream_executor>>\r\n     ${pywrap_tensorflow_deffile}\r\n )\r\n+target_link_libraries(tensorflow_ PRIVATE\r\n+    ${tf_core_gpu_kernels_lib}\r\n+    ${tensorflow_EXTERNAL_LIBRARIES}\r\n+    tf_protos_cc\r\n+)\r\n+set_target_properties(tensorflow_ PROPERTIES VERSION \"1.10.0\" SOVERSION \"1.10\")\r\n+\r\n+# pywrap_tensorflow_internal is a shared library containing all of the\r\n+# TensorFlow runtime and the standard ops and kernels. These are installed into\r\n+# tf_python/tensorflow/python/.\r\n+add_library(pywrap_tensorflow_internal SHARED\r\n+    ${pywrap_tensorflow_internal_src}\r\n+    $<TARGET_OBJECTS:tf_c_python_api>\r\n+    ${pywrap_tensorflow_deffile}\r\n+)\r\n \r\n # There is a bug in GCC 5 resulting in undefined reference to a __cpu_model function when\r\n # linking to the tensorflow library. Adding the following libraries fixes it.\r\n@@ -618,6 +628,7 @@ target_include_directories(pywrap_tensorflow_internal PUBLIC\r\n )\r\n \r\n target_link_libraries(pywrap_tensorflow_internal PRIVATE\r\n+    tensorflow_\r\n     ${tf_core_gpu_kernels_lib}\r\n     ${tensorflow_EXTERNAL_LIBRARIES}\r\n     tf_protos_cc\r\n```\r\n\r\nBy using the resulting shared object I can build the `libtensorflow.so` in the build directory with this command\r\n```\r\ng++ ../../../cc/framework/ops.cc ../../../cc/framework/scope.cc -I ../../../../ -I ./eigen/src/eigen -I . -o x -ltensorflow_ -L. -shared -fPIC\r\n```\r\nand the resulting shared object only takes 1mb of disk space.\r\n\r\n```\r\nls -lh x libtensorflow_.so* libpywrap_tensorflow_internal.so                 ~/tensorflow.pkg/tensorflow.orig/tensorflow/contrib/cmake/x\r\n-rwxr-xr-x 1 lumin lumin 5.5M Aug 18 11:37 libpywrap_tensorflow_internal.so\r\nlrwxrwxrwx 1 lumin lumin   22 Aug 18 11:36 libtensorflow_.so -> libtensorflow_.so.1.10\r\nlrwxrwxrwx 1 lumin lumin   24 Aug 18 11:36 libtensorflow_.so.1.10 -> libtensorflow_.so.1.10.0\r\n-rwxr-xr-x 1 lumin lumin 674M Aug 18 11:36 libtensorflow_.so.1.10.0\r\n-rwxr-xr-x 1 lumin lumin 648K Aug 18 11:48 x\r\n```\r\n\r\nIs my understanding correct? I can sumbit a PR for this symbol deduplication.\r\n\r\n---\r\n\r\n```\r\nreadelf -d x                                                                 ~/tensorflow.pkg/tensorflow.orig/tensorflow/contrib/cmake/x\r\n\r\nDynamic section at offset 0x68d58 contains 28 entries:\r\n  Tag        Type                         Name/Value\r\n 0x0000000000000001 (NEEDED)             Shared library: [libtensorflow_.so.1.10]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]\r\n```\r\n\r\n```\r\nreadelf -d libpywrap_tensorflow_internal.so                                  ~/tensorflow.pkg/tensorflow.orig/tensorflow/contrib/cmake/x\r\n\r\nDynamic section at offset 0x3a4790 contains 51 entries:\r\n  Tag        Type                         Name/Value\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libtensorflow_.so.1.10]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgif.so.7]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libpng16.so.16]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libjpeg.so.9]\r\n 0x0000000000000001 (NEEDED)             Shared library: [liblmdb.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libjsoncpp.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libfarmhash.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libhighwayhash.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libnsync.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libnsync_cpp.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libprotobuf.so.16]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libre2.so.4]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libsqlite3.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libz.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgrpc++.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgrpc.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libsnappy.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libdl.so.2]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libpython3.6m.so.1.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgomp.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]\r\n 0x000000000000000e (SONAME)             Library soname: [libpywrap_tensorflow_internal.so]\r\n```\r\n\r\n```\r\nreadelf -d libtensorflow_.so                                                 ~/tensorflow.pkg/tensorflow.orig/tensorflow/contrib/cmake/x\r\n\r\nDynamic section at offset 0x1a5a5ff8 contains 48 entries:\r\n  Tag        Type                         Name/Value\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgif.so.7]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libpng16.so.16]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libjpeg.so.9]\r\n 0x0000000000000001 (NEEDED)             Shared library: [liblmdb.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libjsoncpp.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libfarmhash.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libhighwayhash.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libnsync.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libnsync_cpp.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libprotobuf.so.16]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libre2.so.4]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libsqlite3.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libz.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgrpc++.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgrpc.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libsnappy.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libdl.so.2]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgomp.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]\r\n 0x000000000000000e (SONAME)             Library soname: [libtensorflow_.so.1.10]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@tensorflowbutler \r\nHave I written custom code N/A\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from N/A\r\nTensorFlow version N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A\r\nMobile device N/A", "Nagging Assignee @tatatodd: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21703, "title": "Print output of predictions in simple estimator example for DistributionStrategy", "body": "Resolved the TODO (cc @anj-s) that asked to print meaningful results from the Estimator's predictions.\r\nCollects all elements yielded by the generator in a list and prints that list instead of the generator object.\r\nResults in the following output: `Prediction results: [{'logits': array([1.0162734], dtype=float32)}, ..., {'logits': array([1.0162734], dtype=float32)}]`.", "comments": []}, {"number": 21702, "title": "assert_element_shape support partial shapes", "body": "Fix #16052.", "comments": ["@mrry could you take a look? Thanks.", "thanks, mrry. I think the test failure(including sanity error) is unrelated. ", "Hi, do you have any update here?"]}, {"number": 21701, "title": "Divergence between tf 1.9 macOS vs Win10 ?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@xtasqf there's no detail in the body of the bug. Would you please explain what the problem is in your own words, and give us reproduction instructions?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21700, "title": "Numpy 15.0 support", "body": "It seems numpy newest version has comes out for a long time, but as far as tensorflow 1.10, it not support numpy 15.0 yet.\r\nWhile pytorch using numpy 15.0 above, this is really awkward... I have to using 2 versions numpy", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "How does precisely numpy 1.15 break TF?", "@wookayin You can not be installed tensorflow newest version under numpy < 15.0", "This will be fixed in 1.11."]}]