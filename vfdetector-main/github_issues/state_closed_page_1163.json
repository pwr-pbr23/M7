[{"number": 18313, "title": "Fix the default value doc string of global_step in contrib.slim", "body": "This PR is to fix the default value doc string of global_step in contrib.slim.\r\n- As [contrib.slim.learning.L428](https://github.com/imsheridan/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L428) shown, If global_step was left as `_USE_GLOBAL_STEP`, then `tf.contrib.framework.global_step()` is used instead of `slim.variables.global_step()` according to [contrib.training.training.L386](https://github.com/imsheridan/tensorflow/blob/master/tensorflow/contrib/training/python/training/training.py#L386)\r\n![image](https://user-images.githubusercontent.com/1680977/38458125-e6e9aae8-3acc-11e8-9a69-077d776a8fa6.png)\r\n\r\n- As [contrib.slim.learning.L657](https://github.com/imsheridan/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L657) shown, If global_step was left as `None`, then `training_util.get_or_create_global_step()` is used instead of `slim.variables.get_or_create_global_step()`;\r\n![image](https://user-images.githubusercontent.com/1680977/38458102-91faaee2-3acc-11e8-8640-ec4ba1f6c14c.png)\r\n\r\n\r\n\r\n", "comments": []}, {"number": 18312, "title": "Support CMake options to use external package providers like zlib, GRPC, Eigen", "body": "This PR is to add the ability to use external package providers like GRPC, zlib, Eigen .etc. It is not possible to compile project using two different versions of protobuf right now.\r\n\r\nThis is a continuous work of PR #16210. As discussed [there](https://github.com/tensorflow/tensorflow/pull/13867#issuecomment-339789050), it did something similar to GRPC package whether you specify tensorflow__PROVIDER variable. It will determine if tensorflow needs to download dependency and use it as a module or use find_package() to find package.\r\n> Add ability to use external GRPC, zlib, Eigen and mb some other packages. Because right now it is impossible to compile your project with tensorflow if you're using different version of GRPC. I did something similar to GRPC package whether you specify tensorflow__PROVIDER variable. It will determine if tensorflow needs to download dependency and use it as a module or use find_package() to find package.", "comments": ["Thanks for rebasing this! It probably makes more sense to name the options systemlib_FOO to match with\r\nhttps://github.com/tensorflow/tensorflow/pull/15382\r\nalso they should be put under systemlib_ALL.\r\nalternatively doing this could be a second PR after this is merged", "Nagging Assignee @andrewharp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the delay.\r\nCould you rebase, and resolve the conflicts?", "@imsheridan could you pull rebase and push again?", "Gentle ping @imsheridan ", "Sure, I'll try to rebase tomorrow. Sorry for the delay.", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to inactivity.\r\nLet me know and I will reopen if the conflicts are resolved."]}, {"number": 18310, "title": "Feature Request: Locally Connected Conv2d", "body": "### Describe the problem\r\nNeed new features, the local conv is avaiable in Keras.\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D\r\nBut tensorflow does not have this feature. I have writen this layer with tensorlayer, it can work, but the initializing is too slow. So I want offical to release a better implementation. Thanks\r\n### Source code / logs\r\n```\r\nclass LocalConnectedConv(Layer):\r\n    def __init__(\r\n        self,\r\n        layer = None,\r\n        filters=0,\r\n        size=3,\r\n        multiplexH=1,\r\n        multiplexW=1,\r\n        stride=1,\r\n        overlap=True,\r\n        act = tf.identity,\r\n        name ='lcconv2d',\r\n    ):\r\n        Layer.__init__(self, name=name)\r\n        self.inputs = layer.outputs\r\n        channels = int(self.inputs.get_shape()[-1])\r\n        inputH=int(self.inputs.get_shape()[1])\r\n        inputW = int(self.inputs.get_shape()[2])\r\n        if inputH%multiplexH==0 and inputW%multiplexW==0:\r\n            print('ok')\r\n        else:\r\n            return 0\r\n        CellH= int(np.floor(inputH/multiplexH))\r\n        CellW=int(np.floor(inputW/multiplexW))\r\n        CellHm= int(np.floor(inputH/multiplexH))\r\n        CellWm=int(np.floor(inputW/multiplexW))\r\n        if overlap:\r\n            CellH = np.floor(inputH / multiplexH*2)\r\n            CellW = np.floor(inputW / multiplexW*2)\r\n            CellH=int(CellH)\r\n            CellW=int(CellW)\r\n        Wd=False\r\n        Hd=False\r\n        if CellH%2==0:\r\n            Hd=True\r\n        if CellW%2==0:\r\n            Wd=True\r\n        with tf.variable_scope(name) as vs:\r\n            Welist=[]\r\n            Bilist=[]\r\n            for i in range(multiplexH):\r\n                for j in range(multiplexW):\r\n\r\n                    We = tf.get_variable(name='weights%d-%d'%(i,j), shape=[size, size, channels, filters],\r\n                                               initializer=tf.truncated_normal_initializer(stddev=0.03),\r\n                                               dtype=tf.float32, trainable=True)\r\n                    bi = tf.get_variable(name='biases%d-%d'%(i,j), shape=[filters, ],\r\n                                              initializer=tf.constant_initializer(value=0.1),\r\n                                              dtype=tf.float32, trainable=True)\r\n                    Welist.append(We)\r\n                    Bilist.append(bi)\r\n        Convij=[]\r\n        for i in range(multiplexH):\r\n            for j in range(multiplexW):\r\n                ci=np.floor((i+0.5)*CellHm-0.01)+1\r\n                cj=np.floor((j+0.5)*CellWm-0.01)+1\r\n                if not overlap:\r\n                    if i==0:\r\n                        hcs=0\r\n                        hce=hcs+CellH+size-1\r\n                    elif i==multiplexH-1:\r\n                        hce=inputH\r\n                        hcs = hce-CellH-size+1\r\n                    elif Hd:\r\n                        hcs=ci-(CellH+size-1)/2\r\n                        hce=ci+(CellH+size-1)/2\r\n                    else:\r\n                        hcs=ci-np.floor((CellH+size-1)*0.5-0.01)-1\r\n                        hce=ci+np.floor((CellH+size-1)*0.5-0.01)\r\n                    if j==0:\r\n                        wcs=0\r\n                        wce=wcs+CellW+size-1\r\n                    elif j==multiplexW-1:\r\n                        wce=inputW\r\n                        wcs = wce-CellW-size+1\r\n                    elif Wd:\r\n                        wcs=cj-(CellW+size-1)/2\r\n                        wce=cj+(CellW+size-1)/2\r\n                    else:\r\n                        wcs=cj-np.floor((CellW+size-1)*0.5-0.01)-1\r\n                        wce=cj+np.floor((CellW+size-1)*0.5-0.01)\r\n                else:\r\n                    if i == 0:\r\n                        hcs = 0\r\n                        hce = hcs + CellH\r\n                    elif i == multiplexH - 1:\r\n                        hce = inputH\r\n                        hcs = hce - CellH\r\n                    elif Hd:\r\n                        hcs = ci - (CellH) / 2\r\n                        hce = ci + (CellH) / 2\r\n                    else:\r\n                        hcs = ci - np.floor((CellH ) * 0.5 - 0.01)-1\r\n                        hce = ci + np.floor((CellH ) * 0.5 - 0.01)\r\n                    if j == 0:\r\n                        wcs = 0\r\n                        wce = wcs + CellW\r\n                    elif j == multiplexW - 1:\r\n                        wce = inputW\r\n                        wcs = wce - CellW\r\n                    elif Wd:\r\n                        wcs = cj - (CellW ) / 2\r\n                        wce = cj + (CellW ) / 2\r\n                    else:\r\n                        wcs = cj - np.floor((CellW ) * 0.5 - 0.01)-1\r\n                        wce = cj + np.floor((CellW) * 0.5 - 0.01)\r\n                hcs=int(hcs)\r\n                wcs=int(wcs)\r\n                hce=int(hce)\r\n                wce=int(wce)\r\n                it=self.inputs[:,hcs:hce,wcs:wce,:]\r\n                convtemp=tf.nn.conv2d(it,Welist[multiplexW*i+j], strides=[1, stride, stride, 1], padding='VALID')\r\n                convtemp=tf.add(convtemp, Bilist[multiplexW*i+j])\r\n                Convij.append(convtemp)\r\n        convli=[]\r\n        for i in range(multiplexH):\r\n            convlii=[]\r\n            for j in range(multiplexW):\r\n                convlii.append(Convij[multiplexW * i + j])\r\n            convli.append(convlii)\r\n        convt=[]\r\n        for i in range(multiplexH):\r\n            convt.append(tf.concat(convli[i],axis=2))\r\n        convfin=tf.concat(convt,axis=1)\r\n        self.outputs =act(convfin)\r\n        self.all_layers = list(layer.all_layers)\r\n        self.all_params = list(layer.all_params)\r\n        self.all_drop = dict(layer.all_drop)\r\n        self.all_layers.extend([self.outputs])\r\n        self.all_params.extend(Welist)\r\n        self.all_params.extend(Bilist)\r\n```\r\n", "comments": ["As said by you, the feature has been supported by tf.keras. If I'm not wrong, tf.layer will be replaced by tf.keras.layer in the near future. Hence it might be better to use the implementation in tf.keras. ", "Thanks, but the implementation in keras only supports point-wise locally connected conv2d, I make the block-wise locally connected conv2d with overlap feature, a more complex reimplementation.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 49 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18309, "title": "Cannot build toco or use it to convert pretrained model", "body": "Hi, i encountered a lot of problems using tflite. I can build the demo and run it on my android phone. I am trying to convert my trained model to tflite format and cannot even build toco. I have already editted the workspace with the directory of sdk and ndk. The first problem shows up saying max/min is no -std member. I add the `#include<algorithm>` to that file and the next strange problem shows up. Is this tool(tensorflow lite) is going to be obsoleted? \r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:7.3.0\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: 1060/6G\r\n- **Exact command to reproduce**: bazel run --config=opt //tensorflow/contrib/lite/toco:toco -- --input_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb --output_file=/tmp/foo.tflite  --inference_type=FLOAT --input_shape=1,128,128,3 --input_array=input --output_array=MobilenetV1/Predictions/Reshape_1 --verbose_failures\r\n\r\n### Describe the problem\r\nThe command above is verifying my setup through official site examples, and it even cannot work. The error msg is below.\r\n`ERROR: C:/users/herrick/appdata/local/temp/_bazel_herrick/emsryngl/external/com_google_absl/absl/strings/BUILD.bazel:35:1: C++ compilation of rule '@com_google_absl//absl/strings:strings' failed (Exit 2)`\r\n\r\n### Source code / logs\r\nThe logs is too long. The latest error is above.", "comments": ["Another error while building the demo according to the official site instructions using bazel:\r\n`bazel build -c opt //tensorflow/examples/android:tensorflow_demo`\r\n`ERROR: C:/users/herrick/tensorflow/tensorflow/core/kernels/BUILD:5136:1: Linking of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit -1). Note: Remote connection/protocol failed with: execution failed\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(161): CreateProcessWithExplicitHandles(\"C:\\users\\herrick\\appdata\\local\\temp\\_bazel_herrick\\emsryngl\\execroot\\org_tensorflow\\external\\androidndk\\ndk\\toolchains\\arm-linux-androideabi-4.9\\prebuilt\\windows-x86_64\\bin\\arm-linux-androideabi-ar\" (...)): command is longer than CreateProcessW's limit (32768 characters)`\r\nIt seems like it is a known issue but I don't know how to shorten the path", "hello, how did you solve this?\r\n"]}, {"number": 18308, "title": "Refactor constant global variables as separate file in factorization", "body": "This PR is to refactor and add a common `constants.py` for global commonly used variables in contrib.factorization.", "comments": ["Hi @imsheridan ,\r\nWe are researchers working on identifying redundant development and duplicated pull requests. We have found there is a pull request: https://github.com/tensorflow/tensorflow/pull/16401 which might be a potentially duplicate to this one. We would like to build the link between developers to reduce redundant development. We would really appreciate if you could help us to validate and give us feedback.\r\nThank you very much for your time!\r\n", "@imsheridan wondering if you still need this PR , if yes can you please resolve conflicts"]}, {"number": 18307, "title": "WinML or DirectML support planned?", "body": "Windows only, but this should support AMD and Intel GPUs using D3D12 backend or even DirectML which with custom metacommands seems even able to use tensor cores on Volta GPUs..\r\nHope to hear news at least by Build conference in May\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It\u2019s a request such details aren\u2019t needed..", "I do not believe there are any plans to support thisWinML or DirectlyML at this time.", "@skye What about this time?", "Hi, I don't work on TF anymore, so I still don't know of any plans! Maybe someone else can comment... @mhong ?", "We currently have no plans to integrate WinML or DirectML. We are hoping that support for AMD and Intel can be added natively and cross-platform.", "@pfultz2 so is the focus on onnx or what?", "Hi @martinwicke,\r\ncurious with Intel Win support, will be with OneAPI Level Zero API or oneDNN?\r\nalso for AMD are you expecting Windows support for MIOpen and/or ROCM coming soon from AMD?\r\nthanks..", "@penpornk knows best. We're working with both Intel and AMD, but I don't know strict timelines.", "@oscarbg I believe it will be through oneDNN. \r\n@TensorFlow-MKL and @agramesh1: Please correct me if I'm wrong.", "@chsigg Do you know about AMD Windows support?", "I do not know whether AMD is planning to support Windows. As far as I know MIOpen is not supported on Windows. You might want to ask them directly at https://github.com/ROCmSoftwarePlatform/tensorflow-upstream."]}, {"number": 18306, "title": "[Go] SIGABRT (Unexpected type: 23) when running go test, using tensorflow > 1.4", "body": "The same error reported in #14546 is still present in every tensorflow release > 1.4 (and master branch).\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: Build label: 0.7.0- (@non-git)\r\n- **GCC/Compiler version (if compiling from source)**: gcc-6\r\n- **CUDA/cuDNN version**: CUDA 9 / cuDNN 7\r\n- **GPU model and memory**: Dual Nvidia 1080 Ti\r\n- **Exact command to reproduce**:\r\n\r\n```\r\ngo test github.com/tensorflow/tensorflow/tensorflow/go\r\n```\r\n\r\n### Describe the problem\r\n\r\nI have build tensorflow with CUDA support. Since Go requires the C library, I built `libtensorflow.so` and `libtensorflow_framework.so`, with:\r\n\r\n```\r\nbazel build //tensorflow:libtensorflow_framework.so\r\nbazel build //tensorflow:libtensorflow.so\r\n```\r\n\r\nI updated `LD_LIBRARY_PATH` accordingly:\r\n\r\n```bash\r\nTENSORFLOW_LIBLIB=\"${HOME}/sources/tensorflow/bazel-bin/tensorflow/\"\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/opt/cuda/lib64:/opt/cuda/extras/CUPTI/lib64:${TENSORFLOW_LIBLIB}\"\r\nexport LIBRARY_PATH=\"${TENSORFLOW_LIBLIB}:${LIBRARY_PATH}\"\r\n```\r\n\r\nI can now go get tensorflow without problems, I can hence compile it and use the `-ltensorflow` linker flag.\r\n\r\nWhen I run `go test` on the tensorflow package, the following error (`tensorflow/core/framework/tensor.cc:822] Unexpected type: 23 `) is thrown and causes SIGABRT.\r\n\r\n### Source code / logs\r\n\r\n**1.5**\r\n```\r\n(cv) [pgaleone@persefone go]$ git checkout r1.5\r\nSwitched to branch 'r1.5'\r\nYour branch is up to date with 'origin/r1.5'.\r\n(cv) [pgaleone@persefone go]$ go test\r\n2018-04-07 08:37:06.625482: F tensorflow/core/framework/tensor.cc:822] Unexpected type: 23\r\nSIGABRT: abort\r\nPC=0x7f82afedd860 m=8 sigcode=18446744073709551610\r\n\r\ngoroutine 0 [idle]:\r\nruntime: unknown pc 0x7f82afedd860\r\nstack: frame={sp:0x7f82937f5880, fp:0x0} stack=[0x7f8292ff61d0,0x7f82937f5dd0)\r\n00007f82937f5780:  0000000000000000  0000006e0000005b \r\n00007f82937f5790:  0000000000000000  000000770000007c \r\n00007f82937f57a0:  000000000000001b  00007f827c000020 \r\n00007f82937f57b0:  0000000000000014  fffffffffffffef8 \r\n00007f82937f57c0:  00007f827c00822d  0000000000000003 \r\n00007f82937f57d0:  00007f827c006940  00007f82aff2b47a \r\n00007f82937f57e0:  00007f827c0052b0  00007f827c0052c1 \r\n00007f82937f57f0:  000000000000000d  00007f82aff5cdc2 \r\n00007f82937f5800:  00007f82b025b350  0000000000000001 \r\n00007f82937f5810:  0000000000000000  00000000000000ac \r\n00007f82937f5820:  00007f82b025b350  00007f82aff5de29 \r\n00007f82937f5830:  0000000000000001  ffffffff00000008 \r\n00007f82937f5840:  01000000004d1d96  0000000000000011 \r\n00007f82937f5850:  0000000000000008  0000000000000008 \r\n00007f82937f5860:  00000000000006b8  00000000000006b8 \r\n00007f82937f5870:  00007f82937f5900  0000000000000000 \r\n00007f82937f5880: <0000000000000000  0000000000000000 \r\n00007f82937f5890:  0800000000000000  0000000008000000 \r\n00007f82937f58a0:  08000000ae000000  0000000011000000 \r\n00007f82937f58b0:  0000000000000802  000000000076074b \r\n00007f82937f58c0:  0000000000000003  a56bbb4077756a00 \r\n00007f82937f58d0:  0000000000000000  00007f82937f5ae0 \r\n00007f82937f58e0:  00007f82937f5a50  00007f82937f5a70 \r\n00007f82937f58f0:  0000000000098b4a  00007f82937f5a20 \r\n00007f82937f5900:  fffffffe7fffffff  ffffffffffffffff \r\n00007f82937f5910:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5920:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5930:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5940:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5950:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5960:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5970:  ffffffffffffffff  ffffffffffffffff \r\nruntime: unknown pc 0x7f82afedd860\r\nstack: frame={sp:0x7f82937f5880, fp:0x0} stack=[0x7f8292ff61d0,0x7f82937f5dd0)\r\n00007f82937f5780:  0000000000000000  0000006e0000005b \r\n00007f82937f5790:  0000000000000000  000000770000007c \r\n00007f82937f57a0:  000000000000001b  00007f827c000020 \r\n00007f82937f57b0:  0000000000000014  fffffffffffffef8 \r\n00007f82937f57c0:  00007f827c00822d  0000000000000003 \r\n00007f82937f57d0:  00007f827c006940  00007f82aff2b47a \r\n00007f82937f57e0:  00007f827c0052b0  00007f827c0052c1 \r\n00007f82937f57f0:  000000000000000d  00007f82aff5cdc2 \r\n00007f82937f5800:  00007f82b025b350  0000000000000001 \r\n00007f82937f5810:  0000000000000000  00000000000000ac \r\n00007f82937f5820:  00007f82b025b350  00007f82aff5de29 \r\n00007f82937f5830:  0000000000000001  ffffffff00000008 \r\n00007f82937f5840:  01000000004d1d96  0000000000000011 \r\n00007f82937f5850:  0000000000000008  0000000000000008 \r\n00007f82937f5860:  00000000000006b8  00000000000006b8 \r\n00007f82937f5870:  00007f82937f5900  0000000000000000 \r\n00007f82937f5880: <0000000000000000  0000000000000000 \r\n00007f82937f5890:  0800000000000000  0000000008000000 \r\n00007f82937f58a0:  08000000ae000000  0000000011000000 \r\n00007f82937f58b0:  0000000000000802  000000000076074b \r\n00007f82937f58c0:  0000000000000003  a56bbb4077756a00 \r\n00007f82937f58d0:  0000000000000000  00007f82937f5ae0 \r\n00007f82937f58e0:  00007f82937f5a50  00007f82937f5a70 \r\n00007f82937f58f0:  0000000000098b4a  00007f82937f5a20 \r\n00007f82937f5900:  fffffffe7fffffff  ffffffffffffffff \r\n00007f82937f5910:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5920:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5930:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5940:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5950:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5960:  ffffffffffffffff  ffffffffffffffff \r\n00007f82937f5970:  ffffffffffffffff  ffffffffffffffff \r\n\r\ngoroutine 30 [syscall]:\r\nruntime.cgocall(0x6524d0, 0xc42005b8d0, 0xc42005b8d8)\r\n\t/usr/lib/go/src/runtime/cgocall.go:128 +0x64 fp=0xc42005b8a0 sp=0xc42005b868 pc=0x405454\r\ngithub.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SetAttrTensor(0x7f827c0043b0, 0x7f827c005190, 0x7f827c005280, 0x7f827c006650)\r\n\t_cgo_gotypes.go:921 +0x45 fp=0xc42005b8d0 sp=0xc42005b8a0 pc=0x522ac5\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.setAttr.func18(0x7f827c0043b0, 0x7f827c005190, 0x7f827c005280, 0x7f827c006650)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0xce fp=0xc42005b908 sp=0xc42005b8d0 pc=0x52e4ae\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.setAttr(0x7f827c0043b0, 0xc42000e0c0, 0x6ed34c, 0x5, 0x6b21a0, 0xc420112500, 0x0, 0x0)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:306 +0x1006 fp=0xc42005bad8 sp=0xc42005b908 pc=0x5255f6\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Graph).AddOperation(0xc42000e080, 0x6ed1a3, 0x5, 0xc42001a7e8, 0x6, 0x0, 0x0, 0x0, 0xc42005bcb0, 0xc420047cf0, ...)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:209 +0x49d fp=0xc42005bc30 sp=0xc42005bad8 pc=0x52443d\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.Const(0xc42000e080, 0xc42001a7e8, 0x6, 0x67d160, 0xc420112340, 0xc42001a7e8, 0x6, 0x7ad090, 0x7ad0d0)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/util_test.go:38 +0x247 fp=0xc42005be40 sp=0xc42005bc30 pc=0x51fb77\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape.func1(0xc420128780)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:137 +0x111 fp=0xc42005bfa8 sp=0xc42005be40 pc=0x52bbc1\r\ntesting.tRunner(0xc420128780, 0xc4201124c0)\r\n\t/usr/lib/go/src/testing/testing.go:777 +0xd0 fp=0xc42005bfd0 sp=0xc42005bfa8 pc=0x4ce240\r\nruntime.goexit()\r\n\t/usr/lib/go/src/runtime/asm_amd64.s:2361 +0x1 fp=0xc42005bfd8 sp=0xc42005bfd0 pc=0x45cf41\r\ncreated by testing.(*T).Run\r\n\t/usr/lib/go/src/testing/testing.go:824 +0x2e0\r\n\r\ngoroutine 1 [chan receive]:\r\ntesting.(*T).Run(0xc4201283c0, 0x6f3637, 0x1a, 0x7013f0, 0x477b01)\r\n\t/usr/lib/go/src/testing/testing.go:825 +0x301\r\ntesting.runTests.func1(0xc420128000)\r\n\t/usr/lib/go/src/testing/testing.go:1063 +0x64\r\ntesting.tRunner(0xc420128000, 0xc42006bdf8)\r\n\t/usr/lib/go/src/testing/testing.go:777 +0xd0\r\ntesting.runTests(0xc420112260, 0xa49460, 0x11, 0x11, 0x413b29)\r\n\t/usr/lib/go/src/testing/testing.go:1061 +0x2c4\r\ntesting.(*M).Run(0xc420126000, 0x0)\r\n\t/usr/lib/go/src/testing/testing.go:978 +0x171\r\nmain.main()\r\n\t_testmain.go:82 +0x151\r\n\r\ngoroutine 26 [chan receive]:\r\ntesting.(*T).Run(0xc420128780, 0xc420018420, 0x13, 0xc4201124c0, 0x2)\r\n\t/usr/lib/go/src/testing/testing.go:825 +0x301\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape(0xc4201283c0)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:136 +0x552\r\ntesting.tRunner(0xc4201283c0, 0x7013f0)\r\n\t/usr/lib/go/src/testing/testing.go:777 +0xd0\r\ncreated by testing.(*T).Run\r\n\t/usr/lib/go/src/testing/testing.go:824 +0x2e0\r\n\r\nrax    0x0\r\nrbx    0x6\r\nrcx    0x7f82afedd860\r\nrdx    0x0\r\nrdi    0x2\r\nrsi    0x7f82937f5880\r\nrbp    0x7f82937f5ad0\r\nrsp    0x7f82937f5880\r\nr8     0x0\r\nr9     0x7f82937f5880\r\nr10    0x8\r\nr11    0x246\r\nr12    0x7f82937f5d10\r\nr13    0x5\r\nr14    0x7f827c005190\r\nr15    0x7f82937f5d10\r\nrip    0x7f82afedd860\r\nrflags 0x246\r\ncs     0x33\r\nfs     0x0\r\ngs     0x0\r\nexit status 2\r\nFAIL\tgithub.com/tensorflow/tensorflow/tensorflow/go\t0.081s\r\n```\r\n\r\n**1.6**\r\n```\r\n(cv) [pgaleone@persefone go]$ git checkout r1.6\r\nSwitched to branch 'r1.6'\r\nYour branch is up to date with 'origin/r1.6'.\r\n(cv) [pgaleone@persefone go]$ go test\r\n2018-04-07 08:37:33.608147: F tensorflow/core/framework/tensor.cc:822] Unexpected type: 23\r\nSIGABRT: abort\r\nPC=0x7f884c725860 m=7 sigcode=18446744073709551610\r\n\r\ngoroutine 0 [idle]:\r\nruntime: unknown pc 0x7f884c725860\r\nstack: frame={sp:0x7f88307f7880, fp:0x0} stack=[0x7f882fff81d0,0x7f88307f7dd0)\r\n00007f88307f7780:  0000000000000000  0000006e0000005b \r\n00007f88307f7790:  0000000000000000  000000770000007c \r\n00007f88307f77a0:  000000000000001b  00007f8828000020 \r\n00007f88307f77b0:  0000000000000014  fffffffffffffef8 \r\n00007f88307f77c0:  00007f882800a19d  0000000000000003 \r\n00007f88307f77d0:  00007f88280088b0  00007f884c77347a \r\n00007f88307f77e0:  00007f8828007220  00007f8828007231 \r\n00007f88307f77f0:  000000000000000d  00007f884c7a4dc2 \r\n00007f88307f7800:  00007f884caa3350  0000000000000001 \r\n00007f88307f7810:  0000000000000000  00000000000000ac \r\n00007f88307f7820:  00007f884caa3350  00007f884c7a5e29 \r\n00007f88307f7830:  0000000000000001  ffffffff00000008 \r\n00007f88307f7840:  01000000004d2166  0000000000000011 \r\n00007f88307f7850:  0000000000000008  0000000000000008 \r\n00007f88307f7860:  00000000000006b8  00000000000006b8 \r\n00007f88307f7870:  00007f88307f7900  0000000000000000 \r\n00007f88307f7880: <0000000000000000  0000000000000000 \r\n00007f88307f7890:  0800000000000000  0000000008000000 \r\n00007f88307f78a0:  08000000ae000000  0000000011000000 \r\n00007f88307f78b0:  0000000000000802  000000000076074b \r\n00007f88307f78c0:  0000000000000003  61e47ad4235a2e00 \r\n00007f88307f78d0:  0000000000000000  00007f88307f7ae0 \r\n00007f88307f78e0:  00007f88307f7a50  00007f88307f7a70 \r\n00007f88307f78f0:  0000000000094793  00007f88307f7a20 \r\n00007f88307f7900:  fffffffe7fffffff  ffffffffffffffff \r\n00007f88307f7910:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7920:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7930:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7940:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7950:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7960:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7970:  ffffffffffffffff  ffffffffffffffff \r\nruntime: unknown pc 0x7f884c725860\r\nstack: frame={sp:0x7f88307f7880, fp:0x0} stack=[0x7f882fff81d0,0x7f88307f7dd0)\r\n00007f88307f7780:  0000000000000000  0000006e0000005b \r\n00007f88307f7790:  0000000000000000  000000770000007c \r\n00007f88307f77a0:  000000000000001b  00007f8828000020 \r\n00007f88307f77b0:  0000000000000014  fffffffffffffef8 \r\n00007f88307f77c0:  00007f882800a19d  0000000000000003 \r\n00007f88307f77d0:  00007f88280088b0  00007f884c77347a \r\n00007f88307f77e0:  00007f8828007220  00007f8828007231 \r\n00007f88307f77f0:  000000000000000d  00007f884c7a4dc2 \r\n00007f88307f7800:  00007f884caa3350  0000000000000001 \r\n00007f88307f7810:  0000000000000000  00000000000000ac \r\n00007f88307f7820:  00007f884caa3350  00007f884c7a5e29 \r\n00007f88307f7830:  0000000000000001  ffffffff00000008 \r\n00007f88307f7840:  01000000004d2166  0000000000000011 \r\n00007f88307f7850:  0000000000000008  0000000000000008 \r\n00007f88307f7860:  00000000000006b8  00000000000006b8 \r\n00007f88307f7870:  00007f88307f7900  0000000000000000 \r\n00007f88307f7880: <0000000000000000  0000000000000000 \r\n00007f88307f7890:  0800000000000000  0000000008000000 \r\n00007f88307f78a0:  08000000ae000000  0000000011000000 \r\n00007f88307f78b0:  0000000000000802  000000000076074b \r\n00007f88307f78c0:  0000000000000003  61e47ad4235a2e00 \r\n00007f88307f78d0:  0000000000000000  00007f88307f7ae0 \r\n00007f88307f78e0:  00007f88307f7a50  00007f88307f7a70 \r\n00007f88307f78f0:  0000000000094793  00007f88307f7a20 \r\n00007f88307f7900:  fffffffe7fffffff  ffffffffffffffff \r\n00007f88307f7910:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7920:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7930:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7940:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7950:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7960:  ffffffffffffffff  ffffffffffffffff \r\n00007f88307f7970:  ffffffffffffffff  ffffffffffffffff \r\n\r\ngoroutine 30 [syscall]:\r\nruntime.cgocall(0x653f10, 0xc420057898, 0xc4200578a0)\r\n\t/usr/lib/go/src/runtime/cgocall.go:128 +0x64 fp=0xc420057868 sp=0xc420057830 pc=0x405824\r\ngithub.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SetAttrTensor(0x7f8828006320, 0x7f8828007100, 0x7f88280071f0, 0x7f88280085c0)\r\n\t_cgo_gotypes.go:1024 +0x45 fp=0xc420057898 sp=0xc420057868 pc=0x523695\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.setAttr.func18(0x7f8828006320, 0x7f8828007100, 0x7f88280071f0, 0x7f88280085c0)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:313 +0xce fp=0xc4200578d0 sp=0xc420057898 pc=0x52f98e\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.setAttr(0x7f8828006320, 0xc4200a60b8, 0x6ef1cc, 0x5, 0x6b3f40, 0xc4201284e0, 0x0, 0x0)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:313 +0x1006 fp=0xc420057aa0 sp=0xc4200578d0 pc=0x526226\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Graph).AddOperation(0xc4200a6080, 0x6ef023, 0x5, 0xc42001a408, 0x6, 0x0, 0x0, 0x0, 0xc420057c98, 0x0, ...)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:216 +0x4fb fp=0xc420057c00 sp=0xc420057aa0 pc=0x52506b\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.Const(0xc4200a6080, 0xc42001a408, 0x6, 0x67ee40, 0xc420128320, 0xc42001a408, 0x6, 0x7af1f0, 0x7af230)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/util_test.go:38 +0x271 fp=0xc420057e40 sp=0xc420057c00 pc=0x5201e1\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape.func1(0xc420140780)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:137 +0x111 fp=0xc420057fa8 sp=0xc420057e40 pc=0x52cff1\r\ntesting.tRunner(0xc420140780, 0xc4201284a0)\r\n\t/usr/lib/go/src/testing/testing.go:777 +0xd0 fp=0xc420057fd0 sp=0xc420057fa8 pc=0x4ce610\r\nruntime.goexit()\r\n\t/usr/lib/go/src/runtime/asm_amd64.s:2361 +0x1 fp=0xc420057fd8 sp=0xc420057fd0 pc=0x45d311\r\ncreated by testing.(*T).Run\r\n\t/usr/lib/go/src/testing/testing.go:824 +0x2e0\r\n\r\ngoroutine 1 [chan receive]:\r\ntesting.(*T).Run(0xc4201403c0, 0x6f54ea, 0x1a, 0x703348, 0x477f01)\r\n\t/usr/lib/go/src/testing/testing.go:825 +0x301\r\ntesting.runTests.func1(0xc420140000)\r\n\t/usr/lib/go/src/testing/testing.go:1063 +0x64\r\ntesting.tRunner(0xc420140000, 0xc42006bdf8)\r\n\t/usr/lib/go/src/testing/testing.go:777 +0xd0\r\ntesting.runTests(0xc420128220, 0xa4c4c0, 0x12, 0x12, 0x413ef9)\r\n\t/usr/lib/go/src/testing/testing.go:1061 +0x2c4\r\ntesting.(*M).Run(0xc42013e000, 0x0)\r\n\t/usr/lib/go/src/testing/testing.go:978 +0x171\r\nmain.main()\r\n\t_testmain.go:84 +0x151\r\n\r\ngoroutine 26 [chan receive]:\r\ntesting.(*T).Run(0xc420140780, 0xc4200d22c0, 0x13, 0xc4201284a0, 0x2)\r\n\t/usr/lib/go/src/testing/testing.go:825 +0x301\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape(0xc4201403c0)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:136 +0x552\r\ntesting.tRunner(0xc4201403c0, 0x703348)\r\n\t/usr/lib/go/src/testing/testing.go:777 +0xd0\r\ncreated by testing.(*T).Run\r\n\t/usr/lib/go/src/testing/testing.go:824 +0x2e0\r\n\r\nrax    0x0\r\nrbx    0x6\r\nrcx    0x7f884c725860\r\nrdx    0x0\r\nrdi    0x2\r\nrsi    0x7f88307f7880\r\nrbp    0x7f88307f7ad0\r\nrsp    0x7f88307f7880\r\nr8     0x0\r\nr9     0x7f88307f7880\r\nr10    0x8\r\nr11    0x246\r\nr12    0x7f88307f7d10\r\nr13    0x5\r\nr14    0x7f8828007100\r\nr15    0x7f88307f7d10\r\nrip    0x7f884c725860\r\nrflags 0x246\r\ncs     0x33\r\nfs     0x0\r\ngs     0x0\r\nexit status 2\r\nFAIL\tgithub.com/tensorflow/tensorflow/tensorflow/go\t0.082s\r\n```\r\n\r\n**1.7**\r\n```\r\n(cv) [pgaleone@persefone go]$ git checkout r1.7\r\nSwitched to branch 'r1.7'\r\nYour branch is up to date with 'origin/r1.7'.\r\n(cv) [pgaleone@persefone go]$ go test\r\n2018-04-07 08:38:02.846973: F tensorflow/core/framework/tensor.cc:822] Unexpected type: 23\r\nSIGABRT: abort\r\nPC=0x7f88599b1860 m=7 sigcode=18446744073709551610\r\n\r\ngoroutine 0 [idle]:\r\nruntime: unknown pc 0x7f88599b1860\r\nstack: frame={sp:0x7f88417f9880, fp:0x0} stack=[0x7f8840ffa1d0,0x7f88417f9dd0)\r\n00007f88417f9780:  0000000000000000  0000006e0000005b \r\n00007f88417f9790:  0000000000000000  000000770000007c \r\n00007f88417f97a0:  000000000000001b  00007f8838000020 \r\n00007f88417f97b0:  0000000000000014  fffffffffffffef8 \r\n00007f88417f97c0:  00007f8838009d4d  0000000000000003 \r\n00007f88417f97d0:  00007f8838008460  00007f88599ff47a \r\n00007f88417f97e0:  00007f8838006dd0  00007f8838006de1 \r\n00007f88417f97f0:  000000000000000d  00007f8859a30dc2 \r\n00007f88417f9800:  00007f8859d2f350  0000000000000001 \r\n00007f88417f9810:  0000000000000000  00000000000000ac \r\n00007f88417f9820:  00007f8859d2f350  00007f8859a31e29 \r\n00007f88417f9830:  0000000000000001  ffffffff00000008 \r\n00007f88417f9840:  01000000004d2166  0000000000000011 \r\n00007f88417f9850:  0000000000000008  0000000000000008 \r\n00007f88417f9860:  00000000000006b8  00000000000006b8 \r\n00007f88417f9870:  00007f88417f9900  0000000000000000 \r\n00007f88417f9880: <0000000000000000  0000000000000000 \r\n00007f88417f9890:  0800000000000000  0000000008000000 \r\n00007f88417f98a0:  08000000ae000000  0000000011000000 \r\n00007f88417f98b0:  0000000000000802  000000000076074b \r\n00007f88417f98c0:  0000000000000003  6b320cc32e502500 \r\n00007f88417f98d0:  0000000000000000  00007f88417f9ae0 \r\n00007f88417f98e0:  00007f88417f9a50  00007f88417f9a70 \r\n00007f88417f98f0:  00000000000cec7d  00007f88417f9a20 \r\n00007f88417f9900:  fffffffe7fffffff  ffffffffffffffff \r\n00007f88417f9910:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9920:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9930:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9940:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9950:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9960:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9970:  ffffffffffffffff  ffffffffffffffff \r\nruntime: unknown pc 0x7f88599b1860\r\nstack: frame={sp:0x7f88417f9880, fp:0x0} stack=[0x7f8840ffa1d0,0x7f88417f9dd0)\r\n00007f88417f9780:  0000000000000000  0000006e0000005b \r\n00007f88417f9790:  0000000000000000  000000770000007c \r\n00007f88417f97a0:  000000000000001b  00007f8838000020 \r\n00007f88417f97b0:  0000000000000014  fffffffffffffef8 \r\n00007f88417f97c0:  00007f8838009d4d  0000000000000003 \r\n00007f88417f97d0:  00007f8838008460  00007f88599ff47a \r\n00007f88417f97e0:  00007f8838006dd0  00007f8838006de1 \r\n00007f88417f97f0:  000000000000000d  00007f8859a30dc2 \r\n00007f88417f9800:  00007f8859d2f350  0000000000000001 \r\n00007f88417f9810:  0000000000000000  00000000000000ac \r\n00007f88417f9820:  00007f8859d2f350  00007f8859a31e29 \r\n00007f88417f9830:  0000000000000001  ffffffff00000008 \r\n00007f88417f9840:  01000000004d2166  0000000000000011 \r\n00007f88417f9850:  0000000000000008  0000000000000008 \r\n00007f88417f9860:  00000000000006b8  00000000000006b8 \r\n00007f88417f9870:  00007f88417f9900  0000000000000000 \r\n00007f88417f9880: <0000000000000000  0000000000000000 \r\n00007f88417f9890:  0800000000000000  0000000008000000 \r\n00007f88417f98a0:  08000000ae000000  0000000011000000 \r\n00007f88417f98b0:  0000000000000802  000000000076074b \r\n00007f88417f98c0:  0000000000000003  6b320cc32e502500 \r\n00007f88417f98d0:  0000000000000000  00007f88417f9ae0 \r\n00007f88417f98e0:  00007f88417f9a50  00007f88417f9a70 \r\n00007f88417f98f0:  00000000000cec7d  00007f88417f9a20 \r\n00007f88417f9900:  fffffffe7fffffff  ffffffffffffffff \r\n00007f88417f9910:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9920:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9930:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9940:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9950:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9960:  ffffffffffffffff  ffffffffffffffff \r\n00007f88417f9970:  ffffffffffffffff  ffffffffffffffff \r\n\r\ngoroutine 24 [syscall]:\r\nruntime.cgocall(0x653f10, 0xc42005b898, 0xc42005b8a0)\r\n\t/usr/lib/go/src/runtime/cgocall.go:128 +0x64 fp=0xc42005b868 sp=0xc42005b830 pc=0x405824\r\ngithub.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SetAttrTensor(0x7f8838005ed0, 0x7f8838006cb0, 0x7f8838006da0, 0x7f8838008170)\r\n\t_cgo_gotypes.go:1024 +0x45 fp=0xc42005b898 sp=0xc42005b868 pc=0x523695\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.setAttr.func18(0x7f8838005ed0, 0x7f8838006cb0, 0x7f8838006da0, 0x7f8838008170)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:313 +0xce fp=0xc42005b8d0 sp=0xc42005b898 pc=0x52f98e\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.setAttr(0x7f8838005ed0, 0xc42000e0c0, 0x6ef1cc, 0x5, 0x6b3f40, 0xc420104500, 0x0, 0x0)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:313 +0x1006 fp=0xc42005baa0 sp=0xc42005b8d0 pc=0x526226\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Graph).AddOperation(0xc42000e080, 0x6ef023, 0x5, 0xc42001a808, 0x6, 0x0, 0x0, 0x0, 0xc42005bc98, 0x0, ...)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:216 +0x4fb fp=0xc42005bc00 sp=0xc42005baa0 pc=0x52506b\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.Const(0xc42000e080, 0xc42001a808, 0x6, 0x67ee40, 0xc420104340, 0xc42001a808, 0x6, 0x7af1f0, 0x7af230)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/util_test.go:38 +0x271 fp=0xc42005be40 sp=0xc42005bc00 pc=0x5201e1\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape.func1(0xc42011a780)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:137 +0x111 fp=0xc42005bfa8 sp=0xc42005be40 pc=0x52cff1\r\ntesting.tRunner(0xc42011a780, 0xc4201044c0)\r\n\t/usr/lib/go/src/testing/testing.go:777 +0xd0 fp=0xc42005bfd0 sp=0xc42005bfa8 pc=0x4ce610\r\nruntime.goexit()\r\n\t/usr/lib/go/src/runtime/asm_amd64.s:2361 +0x1 fp=0xc42005bfd8 sp=0xc42005bfd0 pc=0x45d311\r\ncreated by testing.(*T).Run\r\n\t/usr/lib/go/src/testing/testing.go:824 +0x2e0\r\n\r\ngoroutine 1 [chan receive]:\r\ntesting.(*T).Run(0xc42011a3c0, 0x6f54ea, 0x1a, 0x703348, 0x477f01)\r\n\t/usr/lib/go/src/testing/testing.go:825 +0x301\r\ntesting.runTests.func1(0xc42011a000)\r\n\t/usr/lib/go/src/testing/testing.go:1063 +0x64\r\ntesting.tRunner(0xc42011a000, 0xc42006bdf8)\r\n\t/usr/lib/go/src/testing/testing.go:777 +0xd0\r\ntesting.runTests(0xc420104260, 0xa4c4c0, 0x12, 0x12, 0x413ef9)\r\n\t/usr/lib/go/src/testing/testing.go:1061 +0x2c4\r\ntesting.(*M).Run(0xc420118000, 0x0)\r\n\t/usr/lib/go/src/testing/testing.go:978 +0x171\r\nmain.main()\r\n\t_testmain.go:84 +0x151\r\n\r\ngoroutine 20 [chan receive]:\r\ntesting.(*T).Run(0xc42011a780, 0xc420018420, 0x13, 0xc4201044c0, 0x2)\r\n\t/usr/lib/go/src/testing/testing.go:825 +0x301\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.TestOutputDataTypeAndShape(0xc42011a3c0)\r\n\t/home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/operation_test.go:136 +0x552\r\ntesting.tRunner(0xc42011a3c0, 0x703348)\r\n\t/usr/lib/go/src/testing/testing.go:777 +0xd0\r\ncreated by testing.(*T).Run\r\n\t/usr/lib/go/src/testing/testing.go:824 +0x2e0\r\n\r\nrax    0x0\r\nrbx    0x6\r\nrcx    0x7f88599b1860\r\nrdx    0x0\r\nrdi    0x2\r\nrsi    0x7f88417f9880\r\nrbp    0x7f88417f9ad0\r\nrsp    0x7f88417f9880\r\nr8     0x0\r\nr9     0x7f88417f9880\r\nr10    0x8\r\nr11    0x246\r\nr12    0x7f88417f9d10\r\nr13    0x5\r\nr14    0x7f8838006cb0\r\nr15    0x7f88417f9d10\r\nrip    0x7f88599b1860\r\nrflags 0x246\r\ncs     0x33\r\nfs     0x0\r\ngs     0x0\r\nexit status 2\r\nFAIL\tgithub.com/tensorflow/tensorflow/tensorflow/go\t0.073s\r\n```", "comments": ["Is it possible that there is a conflicting (older) version of the C library that is being linked against? \r\nYou can figure that out with something like this:\r\n\r\n```sh\r\ngo test -c\r\nldd go.test\r\n```\r\n\r\nAnd ensuring that the path to `libtensorflow_framework.so` is the correct one.\r\n\r\nAlternatively, try something like this:\r\n\r\n```\r\ncat <<EOF >/tmp/foo.go\r\npackage main\r\n\r\nimport (\r\n  \"fmt\"\r\n  tf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n)\r\n\r\nfunc main() {\r\n  fmt.Println(tf.Version())\r\n}\r\nEOF\r\n\r\ngo run /tmp/foo.go\r\n```\r\n\r\nYou should see the same version as the branch you've checked out.\r\nIf not, then it would appear that you're linking against the incorrect version of the C library.\r\n\r\nHope that helps.", "> Is it possible that there is a conflicting (older) version of the C library that is being linked against?\r\n\r\nYes, you're right. I was linking to an older version of the C library. Now I updated it and everything works as expected. Thank you for your help"]}, {"number": 18305, "title": "remove the misleading n_class information", "body": "Fix #15800 \r\n\r\n`tf.estimator.DNNClassifier` expects a `[D0]` or `[D0, 1]` labels, where `expected_labels_dimension` is not equal to `n_classes`.  Hence the error information below is totally misleading.\r\n> Classifier configured with n_classes=1. Received 4.", "comments": ["@ispirmustafa Hi, could you take a look? I think it is a trivial modification, however the information is really misleading."]}, {"number": 18304, "title": "tf.enable_eager_execution must be called at program startup.", "body": "```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport os\r\nimport matplotlib.pyplot as plt\r\n\r\nimport tensorflow as tf\r\nprint(tf.VERSION)   #  => 1.7.0\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntf.enable_eager_execution()\r\n```\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-59-0c62cac7517d> in <module>()\r\n      7 import tensorflow.contrib.eager as tfe\r\n      8 \r\n----> 9 tf.enable_eager_execution()\r\n\r\nD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in enable_eager_execution(config, device_policy)\r\n   5242     if graph_mode_has_been_used:\r\n   5243       raise ValueError(\r\n-> 5244           \"tf.enable_eager_execution must be called at program startup.\")\r\n   5245   context._default_mode = context.EAGER_MODE\r\n   5246   if context._context is None:\r\n\r\nValueError: tf.enable_eager_execution must be called at program startup.\r\n```", "comments": ["`tf.enable_eager_execution()` is intended to be called once in a program, typically the first thing in a program. Libraries should not be calling `tf.enable_eager_execution()` and instead should in general work for both graph construction and eager execution (or `assert tf.executing_eagerly()` if they work only with eager execution).\r\n\r\nThat said, I can't reproduce the error you're seeing using the snippet provided. Is there more to reproducing the problem? Thanks.", "Thank you for your quick response. \r\n\r\nI tried the same codes in Windows terminal, it works. \r\nBut it doesn't work in Jupyter (Version 5.4.1, latest version).\r\n\r\n", "Sounds like the Jupyter kernel needs to be restarted.", "Cool, it works, thank you for  your time.", "Thanks a lot, also fix my issue", "Cool, solved my problem.", "Met this problem again when I run the example of **Neural Machine Translation with Attention** in [colab](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb).  What should I do to make this notebook work.", "For now, you can restart the colab runtime (\"Runtime -> Restart Runtime...\").\r\nIn future releases, I think the error will go away (thanks to e2c6ec9e86dd86e0dd56e0f11302a5bf5d9ed440)", "@asimshankar  Thanks guy,  but restarting the runtime does not work for me, even I change the notebook environment to python2 without GPU.", "I have the same issue with the notebook below.\r\nRestart runtime, Reset all runtimes neither do not solve, but I could run it once before.\r\n\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb", "Same issue here. \r\n\r\nOn the side: I am using a remote debugger in PyCharm here.\r\n\r\n```python\r\nimport os\r\nimport logging.config\r\nimport tensorflow as tf\r\nfrom tensor2tensor import problems\r\nfrom tensor2tensor.data_generators.translate_ende import TranslateEndeWmtBpe32k\r\n\r\nfrom asr.util import is_debug_mode\r\n\r\nif __name__ == '__main__':\r\n\r\n    logging.config.fileConfig('../logging.conf')\r\n    logger = logging.getLogger(__name__)\r\n    logger.setLevel(level=logging.DEBUG)\r\n\r\n    tfe = tf.contrib.eager\r\n\r\n    if is_debug_mode():\r\n        logger.info('Debug mode. Enabling tensorflow eager excetion.')\r\n        if not tf.executing_eagerly():\r\n            # Enable tensorflow Eager execution\r\n            tfe.enable_eager_execution()\r\n```\r\n\r\nThrows\r\n\r\n```\r\n19-09-2018:10:37:35,546 INFO     [example_transformer_ende.py:41] Debug mode. Enabling tensorflow eager excetion.\r\nTraceback (most recent call last):\r\n  File \"/home/everest11/.pycharm_helpers/pydev/pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"/home/everest11/.pycharm_helpers/pydev/pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/home/everest11/.pycharm_helpers/pydev/pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/home/everest11/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/everest11/tmp/pycharm_project_254/asr/example_transformer_ende.py\", line 44, in <module>\r\n    tfe.enable_eager_execution()\r\n  File \"/home/everest11/miniconda3/envs/t2t-asr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5187, in enable_eager_execution\r\n    config, device_policy, execution_mode, None)\r\n  File \"/home/everest11/miniconda3/envs/t2t-asr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5255, in enable_eager_execution_internal\r\n    \"tf.enable_eager_execution must be called at program startup.\")\r\nValueError: tf.enable_eager_execution must be called at program startup.\r\n```", "I had the same problem. My code looks something like\r\n```\r\nimport utils\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n```\r\nTurns out `utils` already imports tensorflow. The exception goes away if I run `tf.enable_eager_execution` immediately after tensorflow is imported **for the very first time** (i.e., in `utils`).", "I have a similar problem with this code:\r\n`import tensorflow as tf`\r\n`tf.enable_eager_execution()`\r\n\r\nThis is right at the top of the code. However, even `tf.enable_eager_execution()` is right after `import tensorflow as tf` and these are the first two lines of code, I still get the error. It is not a problem everywhere. It only happens in certain programs.", "Restart Kernel and clear output and run the code block again from start. Issue will be resolved", "As a tip, at least in tensorflow version 1.13.1, you can't even check if eager execution is enabled before you try to enable eager execution.\r\n\r\nThat is the following will not work:\r\n\r\n    import tensorflow as tf\r\n    tf.executing_eagerly()\r\n    tf.enable_eager_execution()\r\n\r\nHowever, with a new python kernel, the following works:\r\n        \r\n    import tensorflow as tf\r\n    tf.enable_eager_execution()\r\n    tf.executing_eagerly()\r\n\r\n"]}, {"number": 18303, "title": "Add missing TF_ATTRIBUTE_WEAK for MSVC", "body": "`TF_ATTRIBUTE_WEAK` is used in `//tensorflow/compiler/xla`, but not defined for MSVC.\r\n\r\nOther stuff:\r\n\r\n- Define `TF_ATTRIBUTE_ALWAYS_INLINE` as `__forceinline` for MSVC.\r\n- Change two instances of `COMPILER_MSVC` to `_MSC_VER` and `_WIN32` respectively as the first applies to MSVC only while the second one (`dllimport/dllexport`) applies to all Windows compilers.\r\n\r\n#15213 #15990", "comments": []}, {"number": 18302, "title": "Add sparse tensor support to Dataset.padded_batch()", "body": "@jsimsa Please help :) Thx\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.0.3.11\r\n- **GPU model and memory**: TitanX 12Gb\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n```\r\nTraceback (most recent call last):\r\n  File \"trainer.py\", line 70, in <module>\r\n    main(ARGS)\r\n  File \"trainer.py\", line 20, in main\r\n    train(source, meta, args.destination)\r\n  File \"trainer.py\", line 49, in train\r\n    dataset = dataset.padded_batch(batch_size, padded_shapes=(0, None, 0, None))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 821, in padded_batch\r\n    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1713, in __init__\r\n    \"Batching of padded sparse tensors is not currently supported\")\r\nTypeError: Batching of padded sparse tensors is not currently supported\r\n```\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nfrom argparse import ArgumentParser\r\nimport pickle\r\n\r\nimport tensorflow as tf\r\n\r\ndef main(args):\r\n\r\n    # Generate the complete source and meta paths.\r\n    source = args.source\r\n    if not source.endswith('.tfrecords'):\r\n        source = '{}.tfrecords'.format(source)\r\n\r\n    meta = args.source\r\n    if not source.endswith('.tfrecords'):\r\n        meta = '{}.meta.pkl'.format(meta)\r\n    else:\r\n        meta = '{}.meta.pkl'.format(meta.rsplit('.', 1)[0])\r\n    \r\n    # Train the model.\r\n    train(source, meta, args.destination)\r\n\r\ndef parse(example_proto):\r\n\r\n    features = {\r\n        'bucket': tf.FixedLenFeature([], tf.int64),\r\n        'coefficients': tf.FixedLenFeature([], tf.string),\r\n        'coefficients_length': tf.FixedLenFeature([], tf.int64),\r\n        'label': tf.VarLenFeature(tf.int64)\r\n    }\r\n    parsed_features = tf.parse_single_example(example_proto, features)\r\n\r\n    bucket = tf.cast(parsed_features['bucket'], tf.int32)\r\n    coefficients = tf.decode_raw(parsed_features['coefficients'], tf.float32)\r\n    coefficients_length = tf.cast(parsed_features['coefficients_length'], tf.int32)\r\n    label = tf.cast(parsed_features['label'], tf.int32)\r\n\r\n    return bucket, coefficients, coefficients_length, label\r\n\r\ndef train(source, meta, destination, batch_size=64, epochs=1):\r\n\r\n    # Load the training meta data.\r\n    file = open(meta, 'rb')\r\n    meta = pickle.load(file)\r\n    file.close()\r\n\r\n    # Create a tf.data input pipe line.\r\n    dataset = tf.data.TFRecordDataset([source])\r\n    dataset = dataset.map(parse)\r\n    dataset = dataset.padded_batch(batch_size, padded_shapes=([], [None], [], [None]))\r\n    dataset = dataset.repeat(epochs) \r\n    iterator = dataset.make_initializable_iterator()\r\n\r\n    bucket, coefficients, coefficients_length, label = iterator.get_next()\r\n\r\n    with tf.Session() as session:\r\n        \r\n        session.run(iterator.initializer)\r\n        \r\n        print(session.run([bucket, coefficients, coefficients_length, label]))\r\n\r\nif __name__ == '__main__':\r\n    \r\n    PARSER = ArgumentParser(description='Trains a model.')\r\n    PARSER.add_argument('--destination', required=True, type=str,\r\n    \t\t\t\t\thelp='The path where the trained model should be stored.')\r\n    PARSER.add_argument('--source', required=True, type=str,\r\n    \t\t\t\t\thelp='The path to the training data.')\r\n    ARGS = PARSER.parse_args()\r\n\r\n    main(ARGS)\r\n\r\n```", "comments": ["Nagging Assignee @jsimsa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jsimsa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jsimsa: It has been 49 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jsimsa: It has been 64 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jsimsa: It has been 79 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hello @thomasquintana, take a look at the recently introduced tf.data API for windowing and batching of windows (https://github.com/tensorflow/tensorflow/commit/a6471888cc9dfe9c18d121149bc0516a3f423fbb). Among other things, the newly introduced API makes it possible to use different batching logic for different components.", "Hi @jsimsa,\r\n\r\nIt seems the code added in [a647188](https://github.com/tensorflow/tensorflow/commit/a6471888cc9dfe9c18d121149bc0516a3f423fbb) has been removed with the move to tf.data.experimental in b72265dc002e712fc3d0f33434f13c7a36a484b2. Is there a new solution to batching a dataset with mixed Tensors and SparseTensors?\r\n\r\nCurrently I'm separating dense and sparse input processing into their own datasets which then get zipped into a single dataset, but this feels a bit hacky. Given that both `dense_dataset.padded_batch()` and `sparse_dataset.batch()` work on their own, why can't `padded_batch()` on a dataset with mixed classes use the `padded_batch` logic for dense tensors and the `batch` logic for sparse tensors transparently?", "The recommended approach is to use `tf.data.Dataset.window` to:\r\n\r\n1) create a structure of windows (finite datasets) -- one for each component of the input structure\r\n2) apply a map function that batches each window using the batching logic of your choice\r\n3) flatten the output\r\n\r\nHere is an example that applies padded batching to the first (dense) component and batching to the second (sparse) component:\r\n\r\n```\r\ndef batch_fn(dense, sparse):\r\n  return tf.data.Dataset.zip((dense.padded_batch(batch_size, ...), sparse.batch(batch_size)))\r\n\r\ndataset = ... # your input, let's assume each element is (dense, sparse)\r\ndataset = dataset.window(batch_size)\r\ndataset = dataset.flat_map(batch_fn)\r\n```"]}, {"number": 18301, "title": "tf.data.TFRecordDataset does not properly infer shape", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.0.3.11\r\n- **GPU model and memory**: TitanX 12Gb\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1659, in _partial_shape_to_tensor\r\n    [dim if dim is not None else -1 for dim in shape_like.as_list()],\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 820, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"trainer.py\", line 70, in <module>\r\n    main(ARGS)\r\n  File \"trainer.py\", line 20, in main\r\n    train(source, meta, args.destination)\r\n  File \"trainer.py\", line 49, in train\r\n    dataset = dataset.padded_batch(batch_size, padded_shapes=(0, None, 0, None))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 821, in padded_batch\r\n    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1721, in __init__\r\n    input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/util/nest.py\", line 538, in map_structure_up_to\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/util/nest.py\", line 538, in <listcomp>\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1665, in _partial_shape_to_tensor\r\n    return ops.convert_to_tensor(shape_like, dtype=dtypes.int64)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 950, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1040, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 256, in _tensor_shape_tensor_conversion_function\r\n    \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: <unknown>\r\n```\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nfrom argparse import ArgumentParser\r\nimport pickle\r\n\r\nimport tensorflow as tf\r\n\r\ndef main(args):\r\n\r\n    # Generate the complete source and meta paths.\r\n    source = args.source\r\n    if not source.endswith('.tfrecords'):\r\n        source = '{}.tfrecords'.format(source)\r\n\r\n    meta = args.source\r\n    if not source.endswith('.tfrecords'):\r\n        meta = '{}.meta.pkl'.format(meta)\r\n    else:\r\n        meta = '{}.meta.pkl'.format(meta.rsplit('.', 1)[0])\r\n    \r\n    # Train the model.\r\n    train(source, meta, args.destination)\r\n\r\ndef parse(example_proto):\r\n\r\n    features = {\r\n        'bucket': tf.FixedLenFeature([], tf.int64),\r\n        'coefficients': tf.FixedLenFeature([], tf.string),\r\n        'coefficients_length': tf.FixedLenFeature([], tf.int64),\r\n        'label': tf.FixedLenFeature([], tf.int64)\r\n    }\r\n    parsed_features = tf.parse_single_example(example_proto, features)\r\n\r\n    bucket = tf.cast(parsed_features['bucket'], tf.int32)\r\n    coefficients = tf.decode_raw(parsed_features['coefficients'], tf.float32)\r\n    coefficients_length = tf.cast(parsed_features['coefficients_length'], tf.int32)\r\n    label = tf.cast(parsed_features['label'], tf.int32)\r\n\r\n    return bucket, coefficients, coefficients_length, label\r\n\r\ndef train(source, meta, destination, batch_size=64, epochs=1):\r\n\r\n    # Load the training meta data.\r\n    file = open(meta, 'rb')\r\n    meta = pickle.load(file)\r\n    file.close()\r\n\r\n    # Create a tf.data input pipe line.\r\n    dataset = tf.data.TFRecordDataset([source])\r\n    dataset = dataset.map(parse)\r\n    dataset = dataset.padded_batch(batch_size, padded_shapes=(0, None, 0, None))\r\n    dataset = dataset.repeat(epochs) \r\n    iterator = dataset.make_initializable_iterator()\r\n\r\n    bucket, coefficients, coefficients_length, label = iterator.get_next()\r\n\r\n    with tf.Session() as session:\r\n        \r\n        session.run(iterator.initializer)\r\n        \r\n        print(session.run([bucket, coefficients, coefficients_length, label]))\r\n\r\nif __name__ == '__main__':\r\n    \r\n    PARSER = ArgumentParser(description='Trains a model.')\r\n    PARSER.add_argument('--destination', required=True, type=str,\r\n    \t\t\t\t\thelp='The path where the trained model should be stored.')\r\n    PARSER.add_argument('--source', required=True, type=str,\r\n    \t\t\t\t\thelp='The path to the training data.')\r\n    ARGS = PARSER.parse_args()\r\n\r\n    main(ARGS)\r\n```\r\n", "comments": ["I think the problem is that the `padded_shapes` argument to `dataset.padded_batch()` should be a tuple of four shapes (to match the tuple of four tensors returned from `parse()`). So something like `([], [None], [], [])` seems like it's probably the appropriate argument here.", "Thanks I must've misunderstood the documentation. That solved the problem. :beers: ", "Great, thanks for confirming!"]}, {"number": 18300, "title": "Thread limits are not respected in latest tf-nightly build.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes. My minimal example is as follows:\r\n```\r\nimport tensorflow as tf\r\n\r\nconfig = tf.ConfigProto(\r\n    inter_op_parallelism_threads=1,\r\n    intra_op_parallelism_threads=1,\r\n)\r\n\r\nwith tf.Session(config=config) as sess:\r\n  a = tf.random_uniform((25000, 25000))\r\n\r\n  a_sq = tf.matmul(a, a)\r\n  a_sq.eval()\r\n```\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nGoobuntu desktop.\r\n- **TensorFlow installed from (source or binary)**:\r\ntf-nightly (see below)\r\n- **Python version**: \r\n3.5.3\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen I run the minimal example provided above with tf-nightly==1.8.0.dev20180404, everything works correctly. It saturates one CPU core and leaves the rest unused. However, when run with tf-nightly==1.8.0.dev20180405 TensorFlow consumes all 8 cores.\r\n", "comments": ["While driving by, I noticed that the threads are being created when the C API's `TF_Graph` is being constructed, with a stack like this:\r\n\r\n```\r\n#0  __pthread_create_2_1 (newthread=0x5555566085d8, attr=0x0, start_routine=0x7fffd0ac0b10, arg=0x5555561f70f8) at pthread_create.c:511\r\n#1  0x00007fffd0ac0c4c in std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>, void (*)()) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#2  0x00007fffd0ac0d51 in std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#3  0x00007fffd1512720 in tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::string const&, std::function<void ()>) ()\r\n   from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fffd14e4467 in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int, bool) ()\r\n   from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fffd14e4710 in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, std::string const&, int) ()\r\n   from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#6  0x00007fffd188dfdd in tensorflow::LocalDevice::EigenThreadPoolInfo::EigenThreadPoolInfo(tensorflow::SessionOptions const&) ()\r\n   from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#7  0x00007fffd188e158 in tensorflow::LocalDevice::LocalDevice(tensorflow::SessionOptions const&, tensorflow::DeviceAttributes const&) ()\r\n   from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#8  0x00007fffd18b475b in tensorflow::ThreadPoolDevice::ThreadPoolDevice(tensorflow::SessionOptions const&, std::string const&, tensorflow::gtl::IntType<tensorflow::Bytes_tag_, long long>, tensorflow::DeviceLocality const&, tensorflow::Allocator*) ()\r\n   from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#9  0x00007fffd18b4e06 in tensorflow::ThreadPoolDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) () from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#10 0x00007fffd188bd3a in tensorflow::(anonymous namespace)::GetCPUDevice(tensorflow::Env*) ()\r\n   from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#11 0x00007fffd188bf11 in tensorflow::GraphRunner::GraphRunner(tensorflow::Env*) ()\r\n   from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#12 0x00007fffd5aa13f7 in tensorflow::ShapeRefiner::ShapeRefiner(int, tensorflow::OpRegistryInterface const*) ()\r\n   from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007fffd33d197d in TF_Graph::TF_Graph() () from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007fffd33d1a8e in TF_NewGraph () from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007fffd3036169 in _wrap_TF_NewGraph () from /usr/local/google/home/mrry/tf-nightly/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n[Python guff]\r\n```\r\n\r\nIt looks like that call to `GetCPUDevice()` (frame 10) is default initializing a SessionOptions and using it to create the CPU device's threadpool. Since the threadpools are static by default, by the time you come to create the session, it's too late. Strangely, adding `use_per_session_threads=True` to the `ConfigProto` didn't fix it for me. Setting the environment variable `TF_C_API_GRAPH_CONSTRUCTION=0` did reduce the CPU usage to the intended single core.\r\n\r\n/cc @skye ", "This should be fixed now.", "https://github.com/tensorflow/tensorflow/commit/9b18bd70b5739d646b21b7d45de0e5c96b8cc2a1", "@robieta thanks for the clear repro for this problem, it was very helpful!", "My pleasure. Thanks for getting everything sorted out."]}, {"number": 18299, "title": "Branch 191925087", "body": "", "comments": ["yes"]}, {"number": 18298, "title": "[INTEL MKLDNN]: Fixed unit test ops_shape_function_test while build with MKL", "body": "Add missing shape_fn attributes in several mkldnn operators. MklConv2DWithBiasBackpropBias is not a MKLDNN operator (it is a MKLML operator) and add a check to see if MKLML exist or not.", "comments": []}, {"number": 18297, "title": "Tensorflow-gpu performance drop", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Windows 10 64-bit \r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.6.0\r\n- **Python version**: 3.6.3\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GeForce GTX 780, 3Gb\r\n\r\nUsing Keras in Python with tensorflow-gpu backend. Worked fine for weeks until a few days ago, when I have suffered a huge performance drop.\r\n\r\nWhen Tensorflow is initialising, it all appears to work correctly and finds my GPU as normal. Output:\r\n\r\n    2018-04-05 02:08:32.791893: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1212] Found device 0 with properties: \r\n    name: GeForce GTX 780 major: 3 minor: 5 memoryClockRate(GHz): 1.0195\r\n    pciBusID: 0000:01:00.0\r\n    totalMemory: 3.00GiB freeMemory: 2.46GiB\r\n    2018-04-05 02:08:32.792360: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1312] Adding visible gpu devices: 0\r\n    2018-04-05 02:08:33.132555: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2187 MB memory) -> physical GPU (device: 0, name: GeForce GTX 780, pci bus id: 0000:01:00.0, compute capability: 3.5)\r\n\r\nBut whereas before with exactly the same model on the same data, each epoch took about 2-3 seconds, they now take ~17 seconds.\r\n\r\nI had a look in Task Manager, and my IDE shows to be using GPU Engine - \"GPU 0 - Copy\". Also at the beginning of each epoch, the GPU will go under ~70% load for about a second, but then the load switches over to my CPU and Memory for the remaining 15 seconds or so and the GPU goes back down to its idle load around 2%.", "comments": ["This is consistent with my experiments. As of commit `5d33c1e49178aedbb459da7ce58eca710102c06b` , the performance drop over 25600 images of imagenet when training densenet265 is about 17 seconds.", "The official benchmarks have not been affected as you can see [here](https://benchmarks-dot-tensorflow-testing.appspot.com/)\r\n\r\nSo the question is what you are doing different from official pipelines.\r\n\r\nOne possibility is if you are using feed_dict, which is not well tested by those benchmarks. Upgrading past TensorFlow 1.6 changes alignment requirement which will lead to slowdown if you use feed_dict, see https://github.com/tensorflow/tensorflow/issues/17233", "Not 100% sure what is being used because I'm using Keras. \r\nWould I receive a warning if feed_dict was being used?\r\nI do get this warning, however\r\n\r\n`WARNING:tensorflow:From C:\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1259: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\n`\r\n\r\nIf that might have anything to do with it.\r\n\r\n", "I guess the best way to debug would be to look at timeline of a single step (fast) of old version, and compare it to timeline of single step (slow) in new version, and see what changed", "Like I said, I'm using the same data and model as before, and I haven't updated any of my libraries.\r\nAlthough I guess _something_ must have changed that's caused this performance drop, but at the moment, I have no idea how to get my fast version back.", "@TwoRice Are you sure this is not a user error? Without a reproducible case or some kind of narrowing down of the regression, this report is not very useful since it's hard to make progress on it", "Yes I agree, I guess I was hoping there was some known issue, or idea on what the problem could be.", "@TwoRice if you're using Keras and you don't know whether you're using feed_dict or not, you're probably using feed_dict in the critical path (because it's a Keras default). \r\nfeed_dict is slow probably since the first day of tensorflow. It gets slower in 1.6 as @yaroslavvb mentioned. Maybe related to your issue.", "/CC @tatianashp, do you think this could be related to #17233?", "Don't know if it is due to the same reason, but I am also seeing some regressions in the 1.4 - 1.5 and 1.5 - 1.6 updates, as reported in #19027 (with reproducible snippet).\r\nIn my example, the slowdown happens in CPU computations, and the code becomes 100x slower. Not sure where it is due to tensorflow, to keras, or to anaconda compilation flags.", "@jlopezpena try running the following before running your script\r\n\r\n```\r\nsudo apt-get install google-perftools\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" \r\n```\r\n\r\nThis changes alignment of default numpy array and fixes the slowdown due to feed_dict introduced in 1.6", "Hi @TwoRice !\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Have you tried [latest version](https://www.tensorflow.org/install/gpu)(2.7) yet? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 18296, "title": "Building Tensorflow on Windows", "body": "At https://www.tensorflow.org/install/install_sources, you say:\r\n\r\n\"We do not support building TensorFlow on Windows.\"\r\n\r\nWhy not? Are there any plans to support this in future? I know it is technically possible to build with CMake, but official support would be nice. At the moment, I am trying to use a trained model for inference as part of a C++ application. I am constrained to Windows, and getting a trained model into the app is much more difficult than it needs to be. Surely there are many people out there also using Windows?\r\n\r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution: Windows 10 64-bit Fall Creator's Update\r\nTensorFlow installed from: pip install tensorflow via Anaconda\r\nTensorFlow version: 1.7\r\nBazel version: N/A\r\nCUDA/cuDNN version: CUDA 9.0\r\nGPU model and memory: Nvidia K2200 4GB\r\nExact command to reproduce: N/A", "Since we first released it in late 2016, we've been using CMake as a temporary build system to build TensorFlow for Windows binaries, while the necessary support in Bazel was developed. TensorFlow/Bazel/Windows support has improved greatly over that time (thanks mostly to @meteorcloudy's hard work), and we intend to switch over to Bazel builds on Windows soon. At that point, we will have standardized on a single build system, and it will be easier to support user builds on Windows as well as Linux and Mac OS X. The CMake build will likely remain, but transition to community support. (/cc @ewilderj).\r\n\r\nReassigning this to @martinwicke and @gunan for comment on when the switch to Bazel for Windows will be made, and what support will come with it.", "Please also ensure that SIG Build is informed", "@meteorcloudy to comment.\r\nI think right now, on windows building tensorflow with bazel works great when you only would like to have CPU support.\r\nFor GPU support, we are working on a few issues, but the switch will be immediately after.\r\nWe will need to prepare a separate page because there are many details to building on windows, but I am happy to say we are very close to the switch.", "We now have release build using Bazel for the pip package with CPU support. The GPU build is currently blocked by protobuf, we're waiting for the next protobuf release.\r\nWill update this issue once we make more progress.", "@meteorcloudy Is there any documentations using Bazel to build tensorflow on Windows? I encountered some issues in #19583, likely related to configurations. A documentation about building on Windows with Bazel will help greatly.", "I just notice this Github issue for Bazel/Windows stuffs. Subscribing.\r\n\r\n> We now have release build using Bazel for the pip package with CPU support.\r\n\r\nOfficial binary is still not built by Bazel on Windows I suppose?\r\n\r\nCurrently many dependencies such as zlib, jpeg and png do not use SIMD when building with Bazel on Windows (some PRs: https://github.com/tensorflow/tensorflow/pull/20538 https://github.com/tensorflow/tensorflow/pull/20537). There are also a few optimization flags added to CMake build but not yet for Bazel on Windows (will send PRs to Bazel's side).\r\n\r\nPlease let me know the rough timeline of shipping binary built by Bazel on Windows so that I can prioritize what to work on.", "@rongjiecomputer I'm working on some final issues of the Windows GPU build. I expect it will be ready in at most two weeks, so I guess the next release (1.10.0) could be built with Bazel. @gunan can confirm that.", "@meteorcloudy assigning you to reflect reality.", "@meteorcloudy There used to be a public Windows bot waterfall page (https://ci.tensorflow.org/job/tf-master-win-bzl/) that allows me to check if Windows Bazel build has gone wrong from time to time without having to rebuild Tensorflow locally on my computer (Tensorflow's heavy use of C++ template is too tasking for my computer).\r\n\r\nIs there such a page for Kokoro Windows bot that is accessible to the public?", "@rongjiecomputer Unfortunately, I don't think the Kokoro dashboard is publicly accessible. But the TF eng prod team and I are constantly monitoring the Windows build. The CPU build hasn't been broken for months, and recently the CPU build works as well. We're in the process of switching from CMake to Bazel on Windows. After that, we'll have presubmit for Windows build both internally and on github. ", "Providing the binaries would help with this problem (#21409 )", "Nagging Assignees @meteorcloudy, @gunan: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @meteorcloudy, @gunan: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "https://www.tensorflow.org/install/install_sources_windows", "See my [comment](https://github.com/tensorflow/tensorflow/issues/7258#issuecomment-450802749) which provides a way to build on Windows automatically."]}, {"number": 18295, "title": "fix build break cmake windows 32bit", "body": "fix cmake  build break for target windows 32bit", "comments": []}, {"number": 18294, "title": "Issue using/importing unsorted_segment_mean", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win 10\r\n- **TensorFlow installed from (source or binary)**: pip install ( and reinstalled after the first errors)\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**: test = tensorflow.unsorted_segment_mean\r\n\r\n### Describe the problem\r\n\r\nI cannot import the or use the function unsorted_segment_mean despite it being present in python/ops/math_ops and exported with tf_export, but I am able to use unsorted_segment_sum.\r\nMy IDE shows me the function but when trying to call it I get AttributeError.\r\nI have tried to import it directly using\r\nimport tensorflow.unsorted_segment_mean\r\nAnd this failed too.\r\nI have tried to import permutations of the path (ops, math_ops, ops.math_ops, etc) to try to access is directly and I got error that tensorflow did not have those modules (despite my IDE stating the opposite). And I could not access it that way either.\r\n\r\n### Source code / logs\r\nCode\r\n```python\r\nimport tensorflow as tf\r\ntry:\r\n    test=tf.unsorted_segment_mean\r\nexcept:\r\n    from tensorflow import unsorted_segment_mean\r\n    test=unsorted_segment_mean()\r\n```\r\nError:\r\n`\r\nTraceback (most recent call last):\r\n  File \"[redacted]\", line 25, in <module>\r\n    test=tf.unsorted_segment_mean\r\nAttributeError: module 'tensorflow' has no attribute 'unsorted_segment_mean'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"[redacted]\", line 27, in <module>\r\n    from tensorflow import unsorted_segment_mean\r\nImportError: cannot import name 'unsorted_segment_mean'\r\n`", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18293, "title": "[XLA/AOT] Build LLVM with Bazel on Windows", "body": "Update `//third_party/llvm/llvm.bzl` to support Windows.\r\n\r\nSee #15213", "comments": ["This is very cool!\r\nI just set up the regular Bazel presubmit for Windows today, I can help with setting up the XLA build after it's working.", "@meteorcloudy Hooray for the Bazel presubmit for Windows!\r\n\r\nWith this PR and #18303, `tfcompile` can be built out-of-the-box with the follow command:\r\n\r\n`bazel build --config=opt --config=monolithic --copt=/DNOGDI --host_copt=/DNOGDI //tensorflow/compiler/aot:tfcompile`\r\n\r\nWe can discuss about setting up the XLA build later in the tracking bug.", "@jhseu, is @sanjoy not available to review this PR?", "> IIUC with your change it is possible (and easy!) for a Googler to accidentally add a `cc_library`, have it work internally but break your windows build.\r\n\r\nYes, it is possible that anyone can accidentally add `cc_library` instead of `llvm_cc_library`, but it might not immediately break Windows build since `LLVM_COPTS` are mostly warning flags and C++ standard compliance flags (`-Zc:*`). However, I believe that everyone will know he/she should use `llvm_cc_library` just by looking at one small section of `llvm.BUILD` file.\r\n\r\nI argue that people are more likely to overlook `cc_library(copts = LLVM_COPTS, ...)` than `llvm_cc_library`, though for some reason Tensorflow team opposes the use of macro (#15466 wants to introduce `tf_cc_library`, that will solve the issue that many Tensorflow targets not getting Windows specific `-D` and cause compile errors, LLVM has less of such issue since `LLVM_COPTS` does not have `-D`).\r\n\r\nIt looks like the internal version Blaze's `package` rule has [`default_copts`](https://github.com/google/cpu_features/blob/439d371594c49d35b66493c8d2c33f2d16144f6d/BUILD#L2), I wonder why it is not in Bazel. This feature is so useful in open-source world.\r\n\r\n```\r\npackage(\r\n    default_copts = [\r\n        \"-DDISABLE_GOOGLE_GLOBAL_USING_DECLARATIONS\",\r\n        \"-Wno-implicit-fallthrough\",\r\n        \"-Ithird_party/cpu_features/include\",\r\n    ],\r\n    default_visibility = [\"//visibility:public\"],\r\n    features = [\r\n        \"-parse_headers\",  # disabled because tests (C++) depends on C target compiled with -std=gnu89.\r\n        \"-layering_check\",  # disabled because it depends on parse_headers.\r\n    ],\r\n)\r\n```\r\n\r\n> Did you consider using `cc_library(copts = LLVM_COPTS, ...)` instead of a `llvm_cc_library`? Defining a new function for this seems more powerful that necessary.\r\n\r\nYes I have, but it is harder to do such transformation in my code editor. Ctrl+H \"cc_library(\" to \"llvm_cc_library(\" is faster for me. \r\n\r\n>  I'm not a blaze/bazel expert, but talking to some folks internally it seems like the right thing to do here is to add a custom MSVC toolchain that has all the compiler / linker flags you need.\r\n\r\nYou mean writing Tensorflow-specific `CROSSTOOL` for MSVC toolchain? Firstly, Bazel team is trying to move away from `CROSSTOOL` to something better (because it is so hard to write, lack of documentation, and does not fit into the world outside Google). Secondly, more burden to Tensorflow team (or I should say myself, Tensorflow team probably won't want to maintain it).", "> Taking a step back -- LLVM is the easy bit here (since the LLVM project already builds on windows). Do you have a plan in place for making TensorFlow itself build and run on windows?\r\n\r\nI don't think I understand this part. Tensorflow already builds on Windows. If you mean whether I have a plan in place for Tensorflow XLA/AOT to build and run on Windows, you can see my tracking bug at #15213. My work started since last December.\r\n\r\nCurrent step of my plan is to have this PR merged, make a status update in [0] and the tracking bug that `tfcompile` can build and run out of the box with Bazel on Windows and some blocking issues on Bazel side are now fixed, then started the work to fix and enable all XLA/AOT tests on Windows. Once that is done, I will request Tensorflow team to set up a presubmit.\r\n\r\nhttps://github.com/rongjiecomputer/tensorflow-xla-aot-windows", "> You mean writing Tensorflow-specific CROSSTOOL for MSVC toolchain? Firstly, Bazel team is trying to move away from CROSSTOOL to something better (because it is so hard to write, lack of documentation, and does not fit into the world outside Google). Secondly, more burden to Tensorflow team (or I should say myself, Tensorflow team probably won't want to maintain it).\r\n\r\nHow about the suggestion @gunan made in #15466 -- \"Writing the flag we need to pass in windows build in our bazelrc file during configure. We already write windows-specific things into our bazelrc anyway.\".\r\n\r\n> I don't think I understand this part. Tensorflow already builds on Windows.\r\n\r\nI did not know that TF already builds on Windows.  Please ignore that part of my review.", "> How about the suggestion @gunan made in #15466 -- \"Writing the flag we need to pass in windows build in our bazelrc file during configure. We already write windows-specific things into our bazelrc anyway.\".\r\n\r\nThat will work, but that will affect *all* packages that Tensorflow depends on. That is more powerful than actually needed. I consider this solution as a hack.\r\n\r\nI will open an issue in Bazel to see if they are willing to bring `package(default_copts=[])` to Bazel or not. In the meantime, I do hope that this PR get merged and I am fine to keep coming back to fix Windows build errors arise from Tensorflow team updating `llvm.BUILD` before Bazel can provide a better solution.", "Nagging Assignee @andrewharp: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Currently have other commitments that are taking all my time. Will come back to continue working on this when I am free.", "Nagging Assignee @andrewharp: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Rebased and pushed some changes due to LLVM upstream change.\r\n\r\nSince the last time I built `tfcompile`, there is already one commit from Google internal that breaks MSVC build (fix in #19708). This is the reason why I want a Windows presubmit for XLA/AOT. To do that, LLVM needs to be built with Bazel on Windows.\r\n\r\nI filed a Bazel feature request (https://github.com/bazelbuild/bazel/issues/5198) for setting copts/linkopts per project, but I don't think Bazel team has the bandwidth to think about it right now (i.e. improvement from Bazel's side is not going to happen soon). /cc @meteorcloudy @mhlopko\r\n\r\nI still think that `llvm_cc_library` macro is the second best solution after https://github.com/bazelbuild/bazel/issues/5198.", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@sanjoy, @tatatodd any comments?", "> I'm sorry that I'm noticing it this late, but this llvm.BUILD file is auto-generated\r\n\r\nI am aware of that, which is why I said \"If llvm.BUILD is not to be edited manually no matter what, can the script used to generate llvm.BUILD be open-sourced?\" in my first comment back in April.\r\n\r\nI have been asking about the generator since December 2017.", "@sanjoy It is clear that `llvm.BUILD` is generated by a script, but is *updating* of this BUILD file also fully automated by the script as well (zero manual editing)?", "> @sanjoy It is clear that llvm.BUILD is generated by a script, but is updating of this BUILD file also fully automated by the script as well (zero manual editing)?\r\n\r\nThe llvm.BUILD generator does not require any manual editing -- it always generates a fully formed llvm.BUILD.  However, we have to _run_ it manually when there is some structural change in LLVM (for instance if a component is split into two).", "@sanjoy Since you landed https://github.com/tensorflow/tensorflow/commit/10091aa9a90c6733ac9b9800e0a54584e7acde2f, my PR already breaks.\r\n\r\nWill it be acceptable if I write a Python script to generate `llvm.autogenerated.BUILD` specifically for Tensorflow along with the output BUILD file to replace this generator script that I cannot see?", "> Will it be acceptable if I write a Python script to generate llvm.autogenerated.BUILD specifically for Tensorflow along with the output BUILD file to replace this generator script that I cannot see?\r\n\r\nI may have a simpler fix.  I working on a change that exports LLVM_COPTS, LLVM_DEFINES, LLVM_LINKOPTS, cmake_var_string, expand_cmake_vars and llvm_all_cmake_vars from llvm.bzl and changes all of the generated rules to obey these variables.  This will let you put the windows customization logic in llvm.bzl which isn't autogenerated.", "@Sanjoy That is brilliant! I think that will work.\r\n\r\nAlso note that LLVM Support library need to be handled separately as that directory have Unix and Windows specific code.\r\n\r\n```\r\ncc_library(\r\n    name = \"support\",\r\n    srcs = glob([\r\n        \"lib/Support/*.c\",\r\n        \"lib/Support/*.cpp\",\r\n        \"lib/Support/*.inc\",\r\n        \"include/llvm-c/*.h\",\r\n        \"include/llvm/CodeGen/MachineValueType.h\",\r\n        \"include/llvm/BinaryFormat/COFF.h\",\r\n        \"include/llvm/BinaryFormat/MachO.h\",\r\n        \"lib/Support/*.h\",\r\n    ]) + select({\r\n        \"@org_tensorflow//tensorflow:windows\": glob([\r\n            \"lib/Support/Windows/*.inc\",\r\n            \"lib/Support/Windows/*.h\"\r\n        ]),\r\n        \"//conditions:default\": glob([\r\n            \"lib/Support/Unix/*.inc\",\r\n            \"lib/Support/Unix/*.h\",\r\n        ]),\r\n    }),\r\n    copts = LLVM_COPTS,\r\n    hdrs = glob([\r\n        \"include/llvm/Support/*.h\",\r\n        \"include/llvm/Support/*.def\",\r\n        \"include/llvm/Support/*.inc\",\r\n        \"include/llvm/ADT/*.h\",\r\n        \"include/llvm/Support/ELFRelocs/*.def\",\r\n        \"include/llvm/Support/WasmRelocs/*.def\",\r\n    ]) + [\r\n        \"include/llvm/BinaryFormat/MachO.def\",\r\n        \"include/llvm/Support/VCSRevision.h\",\r\n    ],\r\n    deps = [\r\n        \":config\",\r\n        \":demangle\",\r\n        \"@zlib_archive//:zlib\",\r\n    ],\r\n)\r\n```", "I've pushed the first part at https://github.com/case540/tensorflow/commit/00d30794b98d13bd341b9da906399123009b9154\r\n\r\nStill need to do something about the support library.", "Nagging Assignee @andrewharp: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@sanjoy I am still waiting for the change I requested in https://github.com/tensorflow/tensorflow/commit/add96c8632df9596a73bf637d6b7015e9c7beaad#r29707275", "@sanjoy This PR is ready for review. Thanks for making all the necessary changes internally.", "It looks like this was applied internally.", "@drpngx Can you point me to the specific commit?", "@drpngx You can see that [the diff of this PR](https://github.com/tensorflow/tensorflow/pull/18293/files) is not reflected in master branch [`//third_party/llvm/llvm.bzl`](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.bzl#L292). Please re-open this PR.", "From the history, it looked like @sanjoy applied it in several commits -- I did not check carefully, but our tools returned an empty diff.\r\n\r\n@rongjiecomputer could you pull rebase and push again?\r\n\r\n@sanjoy maybe you can clarify?", "> could you pull rebase and push again?\r\n\r\nDone.\r\n\r\n> From the history, it looked like @sanjoy applied it in several commits -- I did not check carefully, but our tools returned an empty diff.\r\n\r\n@sanjoy created those commits *in preparation* for this PR, not to replace this PR.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commits/master/third_party/llvm", "OK, I tried again and it seems to have worked. I'll take a look at why it was empty before. Sorry about that. No need to push again.\r\n\r\n@sanjoy sent for review.", "This cl broke a lot of tests internally. I will rerun tests to see if the issues reproduce on these tests.", "> This cl broke a lot of tests internally. I will rerun tests to see if the issues reproduce on these tests.\r\n\r\nI accidentally missed adding `posix_cmake_vars` after several rebasing and resolving merge conflict. https://github.com/tensorflow/tensorflow/pull/18293/commits/f419d81a85425d51661adcbbe198ffd2b343568a should fix the XLA test for POSIX platforms.", "One XLA test `//tensorflow/compiler/tests:binary_ops_test_gpu` failed.", "I have just rebased against master. Can I get another CI test? I suspect that `//tensorflow/compiler/tests:binary_ops_test_gpu` test failure is not related to my change.", "Should I rebase again? Or is there something else that is blocking the internal review?", "Approved and kicked off a new Kokoro run.  I'm not authorized to merge this though -- @tatatodd can you PTAL?", "@sanjoy There is an XLA test failure:\r\nhttps://source.cloud.google.com/results/invocations/e5ce2909-025e-4bc0-8d95-065b76b529eb/targets/%2F%2Ftensorflow%2Fcompiler%2Fxla%2Ftests:exhaustive_f32_elementwise_op_test_cpu/tests\r\n\r\nIs this a known flake? Is it possible this change is causing the failure?"]}, {"number": 18292, "title": "Memory leak with py_func", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (see below)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.4 64bit\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GeForce 1080 Ti\r\n- **Exact command to reproduce**: (see below)\r\n\r\n### Issue\r\n`py_func` which has a python function referencing the graph (might be an unobvious reference, see below) results in the graph never being garbage collected. If you know about this issue usually it can be worked around, but in general it's a pitfall which should be fixed in the code (see below for suggestion).\r\n\r\n### Source Code Example\r\n\r\nThe following source code produces the leak. If you comment out `py_func.graph = graph` you will see that \"Deleted MyPyFunc\" will be printed.\r\n```py\r\nimport gc\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyPyFunc:\r\n    def __init__(self):\r\n        self.graph = None\r\n\r\n    def __call__(self, inputs):\r\n        return inputs\r\n\r\n    def __del__(self):\r\n        print(\"Deleted MyPyFunc\")\r\n\r\n\r\ndef run_graph():\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        inputs = tf.constant(0)\r\n        py_func = MyPyFunc()\r\n        op = tf.py_func(py_func, [inputs], [inputs.dtype])\r\n        # Comment this out to fix the leak\r\n        py_func.graph = graph\r\n\r\n    with tf.Session(graph=graph) as sess:\r\n        print(\"Result:\", sess.run(op))\r\n\r\n\r\nrun_graph()\r\ngc.collect()\r\nrun_graph()\r\ngc.collect()\r\n```\r\n\r\n### Analysis\r\nThe issue originates from [script_ops.py:181](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/script_ops.py#L181). Here, all `py_func`s get registered globally, i.e. these will only be deleted when [`CleanupFunc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/script_ops.py#L186) is garbage collected. It is instantiated at [script_ops.py:222](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/script_ops.py#L222) and stored in the graph when `py_func` is called. The idea is, that the `CleanupFunc.__del__` method is called when the graph is garbage collected, which in turn will delete the reference to the python function, so it can get garbage collected as well.\r\n\r\nIn the example, `MyPyFunc` contains a reference to the graph. By calling `py_func` the reference to `MyPyFunc` is stored globally in `_py_funcs = FuncRegistry()`. That means the graph is referenced globally, so it will never get garbage collected, which in turn means that the `CleanupFunc.__del__` will never be called, so `MyPyFunc` will never be deleted from `_py_funcs`.\r\n\r\n### Importance\r\nConsider the following:\r\n```py\r\nclass MyOp:\r\n    def __init__(self, inputs):\r\n        self.op = tf.py_func(self._py_func, [inputs], [inputs.dtype])\r\n\r\n    def _py_func(self, inputs):\r\n        return inputs\r\n```\r\nThis looks like valid code, but having `MyOp.op` in your graph will make the graph leak. The problem lies in the fact that `_py_func` has is bound to the `MyOp` instance which contains a reference to a TensorFlow op which in turn has a reference to the graph (for the remaining inference see above). This is a big issue, because `tf.estimator.train_and_evaluate` creates a lot of graphs during training (i.e. when it switches between training and evaluation a new graph is created).\r\n\r\n### Suggestion\r\nInstead of globally storing references to the functions and keys to them, they might be better stored in the graph directly. This also prevents them from being deleted while the graph exists. Thus they are only cyclic references in the graph and can be garbage collected with the graph. Instead of storing\r\n[`graph._cleanup_py_funcs_used_in_graph.append(cleanup)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/script_ops.py#L232), the function should be stored directly like `graph._py_funcs_used_in_graph.append(func)`.", "comments": ["Good question! Could weakref solve the problem? ", "@facaiy: I can easily fix my code, now that I know where the leak comes from. The problem here is rather the pitfall which creates the leak (i.e. hidden/unobvious references to the graph from your py_func, which can be really hard to find). So if the weakref was ment for my py_func, that's actually not the issue. If you suggest replacing the direct reference in the `FuncRegistry`, that is not helpful, because then I guess the functions might actually be deleted though they are being used internally by the graph. Clarified that in the description.", "/CC @zffchen78 ", "fixes are welcome, as always.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can we close this?", "Thanks!"]}, {"number": 18291, "title": "data_generator", "body": "  import data_generators\r\nModuleNotFoundError: No module named 'data_generators'", "comments": ["I just checked the version control history for TensorFlow and there has never been a module called `data_generators`, so it's possible that you're missing some external library or tutorial code that we didn't provide."]}, {"number": 18290, "title": "Tensorflow reimplementation performs significantly worse than original POC", "body": "### Intro\r\nI have asked this question several times on stackoverflow using different wording and level of details yet didn't get a single answer. Given the nature of the problem it is understandable, you cannot debug the network without training data and code. Still, maybe there is something obvious that I am missing here. We are trying to rewrite successful Torch POC binary classifier using Tensorflow to put it into production.\r\n\r\n### Torch POC\r\nTorch model is a sequential binary classifier that works on word sequences and attempts to predict if the current word belongs to class 1 or 0. The model is quite simple, we have embedding layer and a special feature layer which we sum together before feeding the result vector into LSTM / GRU cell. At the output we do linear transform with sigmoid, compute binary cross entropy loss and update our parameters. Depending on the vocabulary size the model consists of 700k - 1000k params. \r\n\r\n### Tensorflow reimplementation\r\nWe have been using standard Tensorflow language model trainable on Penn Treebank dataset as our code base. We have adapted it to the point where it looks identical to our Torch POC (same hyperparams and equal number of parameters) and started training. \r\n\r\n### Problem\r\nIt quickly became clear that Tensorflow reimplemetation does not learn anything even though the loss drops and test dataset shows error decrease: `Test loss reduced: 97690.06433105469 --> 9929.968887329102`. Loading the trained model and quering it with words showed that the model predictions are garbage. Besides the loss values are different for Torch and Tensorflow. \r\n\r\n### Tensorflow implementation (main part):\r\n```python\r\n#################### PLACEHOLDERS ####################\r\n# We use placeholders for the word, feature inputs and corresponding targets\r\nself.words_input = tf.placeholder(tf.int32, [config.batch_size, config.seq_length])\r\nself.feats_input = tf.placeholder(tf.float32, [config.batch_size, config.seq_length, config.nfeats])\r\nself.targets_input = tf.placeholder(tf.float32, [config.batch_size, config.seq_length])\r\n\r\n#################### VARIABLES ####################\r\n# We use variables for trainable network params like word embeddings, weights and biases\r\n# select params initialization\r\nif self.init_method == \"xavier\":\r\n    self.initer = tf.contrib.layers.xavier_initializer(uniform=False)\r\n\r\nelif self.init_method == \"uniform\":\r\n    self.initer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\r\n\r\nelif self.init_method == \"normal\":\r\n    self.initer = tf.random_normal_initializer()\r\n\r\nelse:\r\n    self.initer = tf.zeros_initializer()\r\n\r\n# word embeddings\r\nwith tf.variable_scope(\"input_layer\"):\r\n    self.embedding = tf.get_variable(\"embedding\", [self.vocab_size, self.input_size],\r\n                                     initializer=self.initer, trainable=True)\r\n    # feature weights for linear transform\r\n    self.feature_weight = tf.get_variable(\"feature_weigths\", [config.nfeats, config.input_size],\r\n                                          initializer=self.initer, trainable=True)\r\n    # feature biases for linear transform\r\n    self.feature_bias = tf.get_variable(\"feature_bias\", [config.input_size],\r\n                                        initializer=self.initer, trainable=True)\r\n\r\n# weights and biases of output layer (follows hidden layer(s))\r\nwith tf.variable_scope(\"output_layer\"):\r\n    # this is where we define out linear + sigmoid sizes [hidden_size x 1 or several classes]\r\n    self.output_w = tf.get_variable(\"output_w\", [self.last_layer_size, self.pay_type],\r\n                                    initializer=self.initer, trainable=True)\r\n\r\n    self.output_b = tf.get_variable(\"output_b\", [self.pay_type], initializer=self.initer,\r\n                                    trainable=True)\r\n\r\n\r\n#################### GRAPH ####################\r\n# create embedding lookup table with embedding variable and word inputs placeholder\r\nword_embeddings = tf.nn.embedding_lookup(self.embedding, self.words_input)  # [32 x 25 x 100]\r\n\r\n# create feature linear transform layer with feature inputs placeholder\r\n# first we need to swap tensor dims from [0, 1, 2] --> [1, 0, 2] to make seq_length first\r\n_feats_trans = tf.transpose(self.feats_input, [1, 0, 2])  # [25 x 32 x 4]\r\n\r\n# apply linear transform without activation in order\r\n# to expand feature vectors [batch_size x nfeats] -> [batch_size x input_size]\r\n# this is needed to sum them with word embeddings before recurrent layer\r\n#_feats_exp = tf.map_fn(self._linear, _feats_trans, dtype=tf.float32, back_prop=True)  # [25 x 32 x 100]\r\n_feats_exp = [tf.nn.xw_plus_b(s, self.feature_weight, self.feature_bias)\r\n              for s in tf.unstack(_feats_trans, axis=0)]\r\n\r\n# now stack the list of tensors and transpose back to [batch_size x seq_length x input_size]\r\nfeats_exp = tf.transpose(tf.stack(_feats_exp, axis=0), [1, 0, 2])  # [32 x 25 x 100]\r\n\r\n# sum the outputs of the embedding and linear ops\r\ninputs_sum = tf.add(word_embeddings, feats_exp)  # [32 x 25 x 100]\r\n\r\n# apply dropout here if needed\r\n#if self.training and self.input_keep_prob < 1:\r\n#    inputs_sum = tf.nn.dropout(inputs_sum, self.input_keep_prob)\r\n\r\n# split the input matrices vertically into separate tensors\r\n_inputs_spl = tf.split(inputs_sum, config.seq_length, axis=1)\r\n# remove single tensor dimensions\r\ninputs = [tf.squeeze(split, [1]) for split in _inputs_spl]  # [25 * [32 x 100]]\r\n\r\n# build recurrent cells\r\nself.cell = self._create_recurrent_cell(config)\r\n\r\n# initialize the hidden (recurrent) state to zero\r\nself.initial_state = self.cell.zero_state(config.batch_size, tf.float32)\r\n\r\n# create recurrent network using rnn cell and return outputs and final state\r\n_outputs, self.final_state = self._apply_rec_cell(inputs, self.initial_state, self.cell, config)  # [800 x 200]\r\n\r\n# apply dropout here if needed\r\nif self.training and self.input_keep_prob < 1:\r\n    cell_output = tf.nn.dropout(_outputs, self.input_keep_prob)\r\n\r\n# the hidden layer output is fed into matmul(x, weights) + biases function\r\nlogits = tf.nn.xw_plus_b(cell_output, self.output_w, self.output_b)  # [800 x 1]\r\n\r\n# transform logits into [seq_length x batch_size x 1]\r\n_logits = tf.reshape(logits, [config.seq_length, config.batch_size, config.pay_type])  # [25 x 32 x 1]\r\n\r\nself.logits = _logits\r\n\r\n# add single dim to target tensor [32 x 25] --> [32 x 25 x 1]\r\n_targets = tf.expand_dims(self.targets_input, axis=2)\r\n# transform targets tensor [32 x 25 x 1] --> [25 x 32 x 1] for loss computation\r\n_targets = tf.transpose(_targets, [1, 0, 2])\r\n\r\n#################### LOSS CALCULATION ####################\r\n# unstack logits and targets along seq_length dimension\r\n_logits_uns = tf.unstack(_logits, axis=0)\r\n_targets_uns = tf.unstack(_targets, axis=0)\r\n\r\n# apply loss function to each sequence and compute individual losses\r\nloss_fn = tf.losses.sigmoid_cross_entropy\r\n#loss_fn = tf.nn.sigmoid_cross_entropy_with_logits\r\nindividual_losses = []\r\n\r\nfor t, o in zip(_targets_uns, _logits_uns):  # [32 x 1], [32 x 1]\r\n    _loss = loss_fn(t, o, weights=1.0, label_smoothing=0, scope=\"sigmoid_cross_entropy\",\r\n                    loss_collection=tf.GraphKeys.LOSSES, reduction=tf.losses.Reduction.NONE)\r\n    #_loss = loss_fn(labels=t, logits=o, name=\"sigmoid_cross_entropy\")\r\n    individual_losses.append(_loss)\r\n\r\n# calculate the loss sum of individual losses\r\nwith tf.name_scope('loss'):\r\n    self.loss = tf.reduce_sum(individual_losses)\r\n\r\n#################### TRAINING ####################\r\nif self.training:\r\n    self.lr = tf.Variable(0.0, trainable=False)\r\n    tvars = tf.trainable_variables()\r\n\r\n    # print model shapes and total params\r\n    print(\"MODEL params:\")\r\n    for t in tvars:\r\n        print(t.shape)\r\n    print(\"TOTAL params:\", np.sum([np.prod(t.shape) for t in tvars]))\r\n\r\n    # clip the gradient by norm\r\n    if config.grad_clip > 0:\r\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), config.grad_clip)\r\n    else:\r\n        grads = tf.gradients(self.loss, tvars)\r\n\r\n    # update variables (weights, biases, embeddings...)\r\n    with tf.name_scope(\"optimizer\"):\r\n        optimizer = tf.train.AdamOptimizer(self.lr)\r\n        #optimizer = tf.train.GradientDescentOptimizer(self.lr)\r\n        # compute grads/vars for tensorboard\r\n        self.grads_and_vars = optimizer.compute_gradients(self.loss)\r\n\r\n        self.train_op = optimizer.apply_gradients(zip(grads, tvars),          global_step=tf.train.get_or_create_global_step())\r\n```\r\n### Recurrent cell creation\r\n```python\r\n    def _create_recurrent_cell(self, config):\r\n        \"\"\"\r\n        Define and return recurrent cell.\r\n        \"\"\"\r\n            cell = rnn.GRUCell(self.hidden_size,\r\n                                           kernel_initializer=self.initer,\r\n                                           bias_initializer=self.initer,\r\n                                           activation=tf.nn.tanh)\r\n\r\n            # apply dropout if required\r\n            if self.training and self.output_keep_prob < 1.0:\r\n                cell = rnn.DropoutWrapper(cell, output_keep_prob=self.output_keep_prob)\r\n\r\n        # we might use several rnn cells in future\r\n        return rnn.MultiRNNCell(cell, state_is_tuple=True)\r\n```\r\n\r\n### Recurrent cell application\r\n```python\r\n    def _apply_rec_cell(self, inputs, initial_state, cell, config):\r\n        \"\"\"\r\n        Apply recurrence cell to each input sequence.\r\n        \"\"\"\r\n\r\n        with variable_scope.variable_scope(\"recurrent_cell\"):\r\n            state = initial_state\r\n            _outputs = []\r\n            for i, inp in enumerate(inputs):  # 25 * [32 x 100]\r\n                if i > 0:\r\n                    variable_scope.get_variable_scope().reuse_variables()\r\n                output, state = cell(inp, state)  # [32 x 200]\r\n                _outputs.append(output)\r\n\r\n        # concat the outputs and reshape them into 2D tensor (for xw_plus_b)\r\n        outputs = tf.reshape(tf.concat(_outputs, 1), [-1, self.recurrent_state_size]) \r\n        return outputs, state\r\n```\r\n\r\n### Torch POC training analysis\r\nWe first take a look at our Torch model and see how it trains during first 10 epochs. We use batch_size=32, seq_length=25, word embedding and feature vectors of size 100 and GRU cell size = 200.\r\n\r\n```\r\nnn.Sequential {\r\n  [input -> (1) -> (2) -> (3) -> (4) -> output]\r\n  (1): nn.ParallelTable {\r\n    input\r\n      |`-> (1): nn.Sequential {\r\n      |      [input -> (1) -> (2) -> output]\r\n      |      (1): nn.LookupTable\r\n      |      (2): nn.SplitTable\r\n      |    }\r\n       `-> (2): nn.Sequential {\r\n             [input -> (1) -> (2) -> output]\r\n             (1): nn.SplitTable\r\n             (2): nn.MapTable {\r\n               nn.Linear(4 -> 100)\r\n             }\r\n           }\r\n       ... -> output\r\n  }\r\n  (2): nn.ZipTable\r\n  (3): nn.MapTable {\r\n    nn.CAddTable\r\n  }\r\n  (4): nn.Sequencer @ nn.Recursor @ nn.Sequential {\r\n    [input -> (1) -> (2) -> (3) -> (4) -> output]\r\n    (1): nn.RecGRU(100 -> 200)\r\n    (2): nn.Dropout(0.5, busy)\r\n    (3): nn.Linear(200 -> 1)\r\n    (4): nn.Sigmoid\r\n  }\r\n}\r\n\r\nEpoch #1\t\r\ntraining...\t\r\n8.5397211080362e-08\tembeddings grads\t\r\n0.020771607756615\tfeature weights grads\t\r\n0.00044380914187059\tfeature bias grads\t\r\n8.6396757978946e-06\tgru grads\t\r\n5.7720841141418e-05\tgru bias grads\t\r\n0.022520124912262\toutput_w grads\t\r\n0.32349386811256\toutput_b grads\t\r\nlearning rate:\t0.0497500005\t\r\nElapsed time: 3.190759\t\r\nSpeed: 0.000019 sec/batch\t\r\nTraining ERR: 959.54436812177\t\r\nvalidating...\t\r\nValidation ERR: 121.4889880009\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 1, ERR: 121.488988\t\r\n\t\r\nEpoch #2\t\r\ntraining...\t\r\n5.9534809082606e-08\tembeddings grads\t\r\n0.010755381546915\tfeature weights grads\t\r\n0.00030940235592425\tfeature bias grads\t\r\n2.632729774632e-05\tgru grads\t\r\n8.1771839177236e-05\tgru bias grads\t\r\n0.044476393610239\toutput_w grads\t\r\n0.37297031283379\toutput_b grads\t\r\nlearning rate:\t0.049500001\t\r\nElapsed time: 2.988541\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 862.21877676807\t\r\nvalidating...\t\r\nValidation ERR: 115.8837774843\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving\r\nLast best epoch: 2, ERR: 115.883777\t\r\n\t\r\nEpoch #3\t\r\ntraining...\t\r\n1.9829073494293e-08\tembeddings grads\t\r\n0.0085931243374944\tfeature weights grads\t\r\n0.00010305171599612\tfeature bias grads\t\r\n-2.4194558136514e-05\tgru grads\t\r\n-4.8788820095069e-06\tgru bias grads\t\r\n0.0073388875462115\toutput_w grads\t\r\n0.40312686562538\toutput_b grads\t\r\nlearning rate:\t0.0492500015\t\r\nElapsed time: 2.969960\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 820.93807517737\t\r\nvalidating...\t\r\nValidation ERR: 110.50921051018\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 3, ERR: 110.509211\t\r\n\t\r\nEpoch #4\t\r\ntraining...\t\r\n-7.4974275676709e-09\tembeddings grads\t\r\n0.0028696460649371\tfeature weights grads\t\r\n-3.8964077248238e-05\tfeature bias grads\t\r\n-3.4580276405904e-05\tgru grads\t\r\n-3.2265303161694e-05\tgru bias grads\t\r\n0.049573861062527\toutput_w grads\t\r\n0.4097863137722\toutput_b grads\t\r\nlearning rate:\t0.049000002\t\r\nElapsed time: 3.005452\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 783.27910768799\t\r\nvalidating...\t\r\nValidation ERR: 107.62939401716\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving\r\nLast best epoch: 4, ERR: 107.629394\t\r\n\t\r\nEpoch #5\t\r\ntraining...\t\r\n-3.623286559673e-08\tembeddings grads\t\r\n-0.01051034592092\tfeature weights grads\t\r\n-0.00018830213230103\tfeature bias grads\t\r\n4.4970302042202e-06\tgru grads\t\r\n3.8041966035962e-05\tgru bias grads\t\r\n0.0042808093130589\toutput_w grads\t\r\n0.13134820759296\toutput_b grads\t\r\nlearning rate:\t0.0487500025\t\r\nElapsed time: 2.966464\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 753.93339692801\t\r\nvalidating...\t\r\nValidation ERR: 108.99283232633\t\r\ncheck early-stopping...\t\r\nLast best epoch: 4, ERR: 107.629394\t\r\n\t\r\nEpoch #6\t\r\ntraining...\t\r\n-1.7328080303969e-08\tembeddings grads\t\r\n-0.0070463065057993\tfeature weights grads\t\r\n-9.0054127213079e-05\tfeature bias grads\t\r\n2.1397584077931e-06\tgru grads\t\r\n6.5339445427526e-05\tgru bias grads\t\r\n0.0060789352282882\toutput_w grads\t\r\n0.13912197947502\toutput_b grads\t\r\nlearning rate:\t0.048500003\t\r\nElapsed time: 2.984332\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 727.82390461024\t\r\nvalidating...\t\r\nValidation ERR: 101.84884516429\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 6, ERR: 101.848845\t\r\n\t\r\nEpoch #7\t\r\ntraining...\t\r\n-2.1446611597753e-08\tembeddings grads\t\r\n-0.0099087124690413\tfeature weights grads\t\r\n-0.000111457935418\tfeature bias grads\t\r\n4.289226126275e-06\tgru grads\t\r\n9.2848667918588e-06\tgru bias grads\t\r\n0.033868614584208\toutput_w grads\t\r\n0.36384600400925\toutput_b grads\t\r\nlearning rate:\t0.0482500035\t\r\nElapsed time: 2.971069\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 693.15547975153\t\r\nvalidating...\t\r\nValidation ERR: 89.342911911197\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving\r\nLast best epoch: 7, ERR: 89.342912\t\r\n\t\r\nEpoch #8\t\r\ntraining...\t\r\n-2.4536879195125e-08\tembeddings grads\t\r\n-0.019674839451909\tfeature weights grads\t\r\n-0.00012751790927723\tfeature bias grads\t\r\n2.5825884222286e-06\tgru grads\t\r\n2.7131845854456e-06\tgru bias grads\t\r\n0.021293396130204\toutput_w grads\t\r\n0.52193140983582\toutput_b grads\t\r\nlearning rate:\t0.048000004\t\r\nElapsed time: 2.977743\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 662.76471467828\t\r\nvalidating...\t\r\nValidation ERR: 85.326015817933\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 8, ERR: 85.326016\t\r\n\t\r\nEpoch #9\t\r\ntraining...\t\r\n-4.5795339076449e-08\tembeddings grads\t\r\n-0.019793825224042\tfeature weights grads\t\r\n-0.00023799829068594\tfeature bias grads\t\r\n-6.2910985434428e-06\tgru grads\t\r\n-2.7645726731862e-05\tgru bias grads\t\r\n-0.0087647764012218\toutput_w grads\t\r\n0.11675848066807\toutput_b grads\t\r\nlearning rate:\t0.0477500045\t\r\nElapsed time: 3.058577\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 630.08797416603\t\r\nvalidating...\t\r\nValidation ERR: 81.433456174098\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 9, ERR: 81.433456\t\r\n\t\r\nEpoch #10\t\r\ntraining...\t\r\n-1.2227498302764e-07\tembeddings grads\t\r\n-0.079100400209427\tfeature weights grads\t\r\n-0.00063546327874064\tfeature bias grads\t\r\n2.7701785256795e-06\tgru grads\t\r\n-3.344654396642e-05\tgru bias grads\t\r\n-0.0030312589369714\toutput_w grads\t\r\n0.41179794073105\toutput_b grads\t\r\nlearning rate:\t0.047500005\t\r\nElapsed time: 3.000976\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 603.50259356992\t\r\nvalidating...\t\r\nValidation ERR: 79.982634914108\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 10, ERR: 79.982635\t\r\n```\r\n\r\nYou can well see that during first 10 epochs Torch POC loss dropped significantly. Now let's take a look at idential Tensorflow implementation that uses the same dataset, same hyperparameters and has the same number of parameters as well as optimizer.\r\n\r\n### Tensorflow version training analysis\r\n```\r\nMODEL params:\r\n(5197, 100)\r\n(4, 100)\r\n(100,)\r\n(200, 1)\r\n(1,)\r\n(300, 400)\r\n(400,)\r\n(300, 200)\r\n(200,)\r\nTOTAL params: 701001\r\n2018-04-06 11:26:30.534418: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-04-06 11:26:30.592130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-04-06 11:26:30.592438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.61GiB\r\n2018-04-06 11:26:30.592470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n\r\nERROR: unable to remove saved_model dir!\r\nsaved_model does not exist or locked, remove manually\r\n\r\n>>> Start test loss: 143462.57397460938\r\n\r\nEpoch: 1\r\n> lr update: 0.0497500005\r\n#################### DEBUGGING ####################\r\n-8.42829e-09 \t Model/input_layer/embedding:0_grads\r\n-1.0331846e-05 \t Model/input_layer/feature_weigths:0_grads\r\n-6.7426217e-06 \t Model/input_layer/feature_bias:0_grads\r\n0.32880843 \t Model/output_layer/output_w:0_grads\r\n-18.802212 \t Model/output_layer/output_b:0_grads\r\n-2.583741e-07 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n2.0594268e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n1.7847007e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n-1.2758308e-05 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 41251.31823730469\r\n> Valid loss: 4779.414421081543\r\n> Best valid loss so far: 143462.57397460938\r\nStopping in (35) epochs if no new minima!\r\n!!! NEW local minima found, saving the model...\r\n\r\nEpoch: 2\r\n> lr update: 0.049500001\r\n#################### DEBUGGING ####################\r\n4.380058e-12 \t Model/input_layer/embedding:0_grads\r\n-1.5687052e-09 \t Model/input_layer/feature_weigths:0_grads\r\n3.5040453e-09 \t Model/input_layer/feature_bias:0_grads\r\n1.0858363 \t Model/output_layer/output_w:0_grads\r\n-24.059809 \t Model/output_layer/output_b:0_grads\r\n9.927889e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n-1.6885447e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n7.744753e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n-1.343922e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 36458.438621520996\r\n> Valid loss: 4771.496253967285\r\n> Best valid loss so far: 4779.414421081543\r\nStopping in (35) epochs if no new minima!\r\n!!! NEW local minima found, saving the model...\r\n\r\nEpoch: 3\r\n> lr update: 0.0492500015\r\n#################### DEBUGGING ####################\r\n-9.34557e-13 \t Model/input_layer/embedding:0_grads\r\n-2.8814911e-18 \t Model/input_layer/feature_weigths:0_grads\r\n-7.4764217e-10 \t Model/input_layer/feature_bias:0_grads\r\n1.0264161 \t Model/output_layer/output_w:0_grads\r\n-28.068089 \t Model/output_layer/output_b:0_grads\r\n-3.470961e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n3.2109366e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-3.8721065e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n3.5263255e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 36571.039489746094\r\n> Valid loss: 5612.627830505371\r\n> Best valid loss so far: 4771.496253967285\r\nStopping in (35) epochs if no new minima!\r\n\r\nEpoch: 4\r\n> lr update: 0.049000002\r\n#################### DEBUGGING ####################\r\n-1.5510707e-08 \t Model/input_layer/embedding:0_grads\r\n-1.9394596e-05 \t Model/input_layer/feature_weigths:0_grads\r\n-1.2408569e-05 \t Model/input_layer/feature_bias:0_grads\r\n0.4255897 \t Model/output_layer/output_w:0_grads\r\n-15.242744 \t Model/output_layer/output_b:0_grads\r\n-2.0200261e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n1.8209105e-08 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n2.867294e-07 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n-2.1903145e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 37006.26306152344\r\n> Valid loss: 4739.623916625977\r\n> Best valid loss so far: 4771.496253967285\r\nStopping in (34) epochs if no new minima!\r\n!!! NEW local minima found, saving the model...\r\n\r\nEpoch: 5\r\n> lr update: 0.0487500025\r\n#################### DEBUGGING ####################\r\n1.2948664e-11 \t Model/input_layer/embedding:0_grads\r\n4.020792e-08 \t Model/input_layer/feature_weigths:0_grads\r\n1.035893e-08 \t Model/input_layer/feature_bias:0_grads\r\n-0.23937465 \t Model/output_layer/output_w:0_grads\r\n-4.5712795 \t Model/output_layer/output_b:0_grads\r\n-1.2125412e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n1.2779661e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-5.819682e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n3.9421697e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35687.37390899658\r\n> Valid loss: 4758.33650970459\r\n> Best valid loss so far: 4739.623916625977\r\nStopping in (35) epochs if no new minima!\r\n\r\nEpoch: 6\r\n> lr update: 0.048500003\r\n#################### DEBUGGING ####################\r\n8.107671e-14 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n6.486137e-11 \t Model/input_layer/feature_bias:0_grads\r\n0.86348265 \t Model/output_layer/output_w:0_grads\r\n-19.398884 \t Model/output_layer/output_b:0_grads\r\n2.4077628e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n-2.4667464e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-1.07752e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n9.429133e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 34959.51641845703\r\n> Valid loss: 4720.412452697754\r\n> Best valid loss so far: 4739.623916625977\r\nStopping in (34) epochs if no new minima!\r\n!!! NEW local minima found, saving the model...\r\n\r\nEpoch: 7\r\n> lr update: 0.0482500035\r\n#################### DEBUGGING ####################\r\n1.1791363e-14 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n9.433085e-12 \t Model/input_layer/feature_bias:0_grads\r\n0.46038043 \t Model/output_layer/output_w:0_grads\r\n-18.42015 \t Model/output_layer/output_b:0_grads\r\n-5.90758e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n6.253203e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-1.6275301e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n1.5087512e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35122.075828552246\r\n> Valid loss: 4872.291694641113\r\n> Best valid loss so far: 4720.412452697754\r\nStopping in (35) epochs if no new minima!\r\n\r\nEpoch: 8\r\n> lr update: 0.048000004\r\n#################### DEBUGGING ####################\r\n1.5793947e-14 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n1.26351585e-11 \t Model/input_layer/feature_bias:0_grads\r\n-0.23440647 \t Model/output_layer/output_w:0_grads\r\n-17.745054 \t Model/output_layer/output_b:0_grads\r\n6.400091e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n-6.770721e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-7.499852e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n6.953364e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35376.37155151367\r\n> Valid loss: 4934.555885314941\r\n> Best valid loss so far: 4720.412452697754\r\nStopping in (34) epochs if no new minima!\r\n\r\nEpoch: 9\r\n> lr update: 0.0477500045\r\n#################### DEBUGGING ####################\r\n-1.2424676e-13 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n-9.939738e-11 \t Model/input_layer/feature_bias:0_grads\r\n0.5383458 \t Model/output_layer/output_w:0_grads\r\n-21.300901 \t Model/output_layer/output_b:0_grads\r\n-7.6299974e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n8.020704e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-7.250712e-16 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n6.68652e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35422.02657318115\r\n> Valid loss: 4915.4898681640625\r\n> Best valid loss so far: 4720.412452697754\r\nStopping in (33) epochs if no new minima!\r\n\r\nEpoch: 10\r\n> lr update: 0.047500005\r\n#################### DEBUGGING ####################\r\n4.3053272e-14 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n3.4442636e-11 \t Model/input_layer/feature_bias:0_grads\r\n-0.4148861 \t Model/output_layer/output_w:0_grads\r\n-16.681393 \t Model/output_layer/output_b:0_grads\r\n-3.3981418e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n3.5677305e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-1.0744528e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n9.898426e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35475.495765686035\r\n> Valid loss: 4763.377861022949\r\n> Best valid loss so far: 4720.412452697754\r\nStopping in (32) epochs if no new minima!\r\n```\r\n\r\nYou can well see the gradients are very low in comparison to Torch POC up to the point that  some of them become zero or very close. Additionally, binary cross entropy loss is much higher and basically does not drop. Using standard `tf.train.GradientDescentOptimizer(self.lr)` does not make the gradients that small and is more stable in general but produces the same garbage predictions after training. We have been trying to figure out the reason for several weeks now and are simply out of clue what could cause such a drastic difference.  We checked our data preprocessing up to the point where we printed out the input matrix values and compared them side by side with Torch. They are identical so it is highly unlikely to be the data feeding mechanism. There is something not right with backpropagation it seems but our lack of experience with Tensorflow does not allow us to proceed further. Maybe somebody here could give us an advice on what could be the reason?\r\n\r\nAddtionally attaching Tensorboard output as well. \r\n\r\n![2018-04-06-114156_793x370_scrot](https://user-images.githubusercontent.com/7676160/38414743-a9d43972-398f-11e8-87d1-3a934fc9af3e.png)\r\n\r\n![2018-04-06-114228_1534x946_scrot](https://user-images.githubusercontent.com/7676160/38414748-acc7cd60-398f-11e8-8947-645e4bc63b43.png)\r\n\r\n![2018-04-06-114236_1544x719_scrot](https://user-images.githubusercontent.com/7676160/38414750-af4aab34-398f-11e8-8d9b-d20c5bdc1f7c.png)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "- Have I written custom code - yes\r\n- OS Platform and Distribution - Linux 4.14.30-1-MANJARO\r\n- TensorFlow installed from - pip, tensorflow-gpu 1.7\r\n- Bazel version - ???\r\n- CUDA/cuDNN version - 9.0\r\n- GPU model and memory - NVIDIA GTX 1070 mobile, 8GB\r\n- Exact command to reproduce --", "closing"]}, {"number": 18289, "title": "I can't transfer .pb file to .tflite file", "body": "\r\nHi,\r\nI trained a model with keras framework. And i saved model as .pb file. This model takes input as \"String Array\" and gives out as \"String Array\" .I want use this model on Android device. Therefore, i use TOCO but i can't give true  --input_shaped and --inference_type parameters.  I use this toco command\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco --input_file=modelimtest.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=modelim.lite --inference_type=QUANTIZED_UINT8 --input_arrays=input_1 --output_arrays=activation_1/truediv input_shaped=\r\n\r\nHow can i change this command line for my problem?\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:Source\r\n- **TensorFlow version (use command below)**:1.7\r\n- **Python version**: 3.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Cuda 9.0\r\n- **GPU model and memory**: GTX 1050\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["same problem here, I tried and run this:\r\n`\r\nbazel run --config=opt tensorflow/tensorflow/contrib/lite/toco:toco -- \\\r\n --input_file=./frozen_model/froze_output.pb \\\r\n --output_file=./model.tflite \\                      \r\n --inference_type=FLOAT \\                            \r\n --input_shape=17,100,100,9 \\                        \r\n --input_array=dense_1_input \\                       \r\n --output_array=dense_3_1/Softmax                    \r\nWARNING: Config values are not defined in any .rc file: opt\r\nERROR: Skipping 'tensorflow/tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/tensorflow/contrib/lite/toco': Extension file not found. Unable to load package for '//tensorflow/core:platform/default/build_config.bzl': BUILD file not found on package path\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tensorflow/contrib/lite/toco': Extension file not found. Unable to load package for '//tensorflow/core:platform/default/build_config.bzl': BUILD file not found on package path\r\nINFO: Elapsed time: 0.103s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tensorflow/contrib/lite/toco\r\nERROR: Build failed. Not running target\r\n\r\n`\r\nand that is what I got. Not only I'm not 100% sure that the parameters are right, but I've never used bazel and have no idea on how to proceed.", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Support for string tensors is not perfect at the moment. We are working on it.", "In any case, the error you are seeing is probably related to your setup. Did you run ./configure ?", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18288, "title": "ctc_loss complains  that Labels length is zero", "body": "OS Platform and Distribution ubuntu 16.04\r\nTensorFlow installed from pip \r\nTensorFlow version 1.7\r\nBazel version N/A\r\nCUDA/cuDNN version 9.0 7.0\r\nGPU model and memory N/A\r\nExact command to reproduce N/A\r\n\r\nI want to use ctc_loss to do gesture recognition. But I keep getting error message that Labels length is zero at batch 0. I can't figure out the problem. So come here for help.\r\nThe data I feed into ctc_loss are listed in the following table\r\n\r\n```\r\ntensname                    type                      dimension\r\nlabels                  sparsetensor         batch_size(5) x label_length(1)\r\nsequence_predicts         tensor                  time_steps(10) x batch_size(5) x num_classes(26)\r\nsequence_lengths          tensor                  batch_size(5)\r\n```\r\n\r\nI create ctc_loss with the following code\r\n\r\n```python\r\nloss = tf.nn.ctc_loss(labels,sequence_predicts,sequence_lengths,time_major = True);\r\n```\r\n\r\nI print all input tensors with the following code\r\n```python\r\ndenselabels = tf.sparse_to_dense(labels.indices,labels.dense_shape,labels.values);\r\ndenselabels = tf.Print(denselabels,[denselabels],message = \"labels = \", summarize = 20, name = \"denselabels\");\r\nsequence_predicts = tf.Print(sequence_predicts,[sequence_predicts],message = \"sequence_predicts = \", summarize = 20, name = \"sequence_predicts\");\r\nsequence_lengths = tf.Print(sequence_lengths,[sequence_lengths],message = \"sequence_lengths = \", summarize = 200, name = \"sequence_lengths\");\r\n```\r\n\r\nThe output on the console is \r\n```bash\r\nsequence_lengths = [10 10 10 10 10]\r\nlabels = [[1][2][1][0][0]]\r\nsequence_predicts = [[[0.0183351934 -0.0540067367 0.0173245296 0.10416019 -0.06072326 -0.0563994423 0.00676568877 -0.046640303 -0.0107666068 0.0554158054 0.0658506081 0.0124566173 0.0133351646 0.0134876268 -0.0453479 -0.058809232 0.0160786062 -0.0285613891 -0.105710872 0.0354634598]]...]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "hello!\r\ni met the same question during the training of speech recognition.\r\n\r\n> OS Platform : ubuntu 16.04\r\n\r\n> TensorFlow installed from pip\r\n\r\n> TensorFlow version 1.6\r\n\r\nhere is my label format of the speech:\r\n\r\n> utterance_id dialect_id\r\n\r\n> 20170809_zj L\r\n\r\nwhen i ran the training code ,the errors occurred:**Labels length is zero in batch 0**(here batch = 10)\r\nwhat confused me is that the length of the label is 1 ,not 0.why the error happens?\r\nor where did i mistaken?\r\nThank you!\r\n"]}, {"number": 18287, "title": "Eager execution breaks fit_generator in tf.keras", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version**: 1.7.0\r\n- **Python version**: 3.6.3\r\n- **Numpy version**: 1.14.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: not installed\r\n- **GPU model and memory**: CPU only\r\n- **Exact command to reproduce**: Run code below\r\n\r\n### Describe the problem\r\n`tf.enable_eager_execution()` leads to a `RuntimeError: You must compile your model before using it.` when calling Keras's `model.fit_generator`, even if the model has already been compiled. Calling `model.fit` works on the other hand. \r\n\r\n### Source code / logs\r\nMinimum reproducible test case:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()  # It works without this line\r\n\r\nx, y = np.random.randn(100, 10), np.random.randn(100, 4)\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(4, input_dim=10)])\r\nmodel.compile(tf.train.RMSPropOptimizer(0.001), 'mse')\r\n\r\nmodel.fit(x, y)  # Fitting without a generator works in eager mode\r\n\r\nclass Iterator:\r\n    def __next__(self):\r\n        return x, y\r\n\r\nmodel.fit_generator(Iterator(), steps_per_epoch=10)\r\n```\r\n\r\nLog:\r\n\r\n    Epoch 1/1\r\n    100/100 [==============================] - 0s 445us/step - loss: 2.1153\r\n    Traceback (most recent call last):\r\n      File \"tmp.py\", line 16, in <module>\r\n        model.fit_generator(Iterator(), steps_per_epoch=10)\r\n      File \"/Users/kilian/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/sequential.py\", line 860, in fit_generator\r\n        initial_epoch=initial_epoch)\r\n      File \"/Users/kilian/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 1603, in fit_generator\r\n        initial_epoch=initial_epoch)\r\n      File \"/Users/kilian/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training_generator.py\", line 52, in fit_generator\r\n        model._make_train_function()\r\n      File \"/Users/kilian/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 578, in _make_train_function\r\n        raise RuntimeError('You must compile your model before using it.')\r\n    RuntimeError: You must compile your model before using it.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Updated", "@batzner Could you use `tf.keras` to make a test, instead of contrib module?", "@fchollet Sounds like a problem. I checked the keras codes and found that generator seems to have not been supported in eager mode, right? Does anyone have worked on it?", "I updated the code to use `tf.keras` instead of contrib. The output is the same as before.", "Nagging Assignee @fchollet: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I faced a similar issue. When using `fit_generator()` in eager mode, keras throws a `NotImplemented` error. ", "Thanks for the bug report. I have fixed the issue and the fix will soon be available in the TF nightly release.", "Just noticed that having `tf.enable_eager_execution()` enabled drastically changed the training part for `fit_generator()`. Here are some examples that I ran on a pre-trained VGG19 model.\r\n\r\n**Without Eager Exectution`**\r\n![image](https://user-images.githubusercontent.com/6565924/62837277-64da1280-bc8b-11e9-88a7-e7d1c8a1ae0b.png)\r\n- The loss reduces drastically after one epoch\r\n- The training accuracy goes to 67% after epoch 1 and 93% after epoch 2\r\n- The validation accuracy jumps to 80% after one epoch\r\n\r\n**With Eager Execution**\r\n![image](https://user-images.githubusercontent.com/6565924/62837357-2ee95e00-bc8c-11e9-9188-673a7ebc2383.png)\r\n- The loss is stuck at 15 and reduces by a tiny amount every epoch\r\n- The training accuracy is at 4% after epoch 1 and at 5% after epoch 2\r\n- The validation accuracy is also at 4-5%\r\n\r\n\r\nThis is quite a strong difference. I am guessing that this is not expected ? \r\n\r\nLink to Full Notebook : [here](https://github.com/DollarAkshay/Artificial-Intelligence/blob/41532b8a08c9100a131ea82da9d9c1999aeb904d/Machine%20Learning/HackerEarth_Garden_Nerd_Flower_Recognition/Prediction%20Notebook.ipynb)\r\n\r\nDataset used for Training : [HackerEarth ML Challenge](https://www.hackerearth.com/challenges/competitive/garden-nerd-data-science-competition/machine-learning/flower-recognition/)\r\n\r\n"]}, {"number": 18286, "title": "Update README.md", "body": "Made a few changes to make the documentation clear.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Please address the CLA. Thanks!", "@samuelhychan can you please take a look at the CLA comment?\r\n", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Thanks @samuelhychan, \r\n\r\nThe change looks good, but we can't merge it unless you answer the CLA bot.\r\n\r\n\r\n\r\n", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 18285, "title": "why is the tensor value of the dimension resolved when reading tfrecord files is twice as much as is required to rescontruct the dimension\uff1f", "body": "\r\n\r\n-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04)\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow\r\n- **TensorFlow version (use command below)**: tensorflow1.4\r\n- **Python version**: Python 3.6.3\r\n- **Bazel version (if compiling from source)**: --\r\n- **GCC/Compiler version (if compiling from source)**: --\r\n- **CUDA/cuDNN version**: --\r\n- **GPU model and memory**:Tesla K80 \r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n![image](https://user-images.githubusercontent.com/35683771/38402256-8349acbc-398d-11e8-8f27-a6393ddf9b7e.png)\r\nimage Segmentation:why is the tensor value of the dimension resolved when reading tfrecord files is twice as much as is required to rescontruct the dimension\uff1f\r\n\r\n\r\n\r\n### Source code / logs\r\n\r\n2018-04-06 10:39:40.367069: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-04-06 10:39:40.845083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 11.17GiB freeMemory: 6.90GiB\r\n2018-04-06 10:39:40.845182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)\r\nNo checkpoint file found\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 26136 values, but the requested shape has 13068\r\n\t [[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw/_87, Reshape/shape)]]\r\n\t [[Node: random_crop_1/_101 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_80_random_crop_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]2018-04-06 10:39:51.060248: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\n2018-04-06 10:39:51.060359: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\n2018-04-06 10:39:51.060728: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\n2018-04-06 10:39:51.060779: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\n2018-04-06 10:39:51.060868: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\n2018-04-06 10:39:51.060926: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\n\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 26136 values, but the requested shape has 13068\r\n\t [[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw/_87, Reshape/shape)]]\r\n\t [[Node: random_crop_1/_101 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_80_random_crop_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]2018-04-06 10:39:51.066487: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 26136 values, but the requested shape has 13068\r\n\t [[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw/_87, Reshape/shape)]]\r\n2018-04-06 10:39:51.066576: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 26136 values, but the requested shape has 13068\r\n\t [[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw/_87, Reshape/shape)]]\r\n\r\n2018-04-06 10:39:51.074663: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\n2018-04-06 10:39:51.074663: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\n\t [[Node: random_crop/All/_73 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_33_random_crop/All\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\n\t [[Node: random_crop_1/_101 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_80_random_crop_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 8712 values, but the requested shape has 4356\r\n\t [[Node: Reshape_1 = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw_1/_71, Reshape_1/shape)]]\r\nERROR:tensorflow:Exception in QueueRunner: Input to reshape is a tensor with 26136 values, but the requested shape has 13068\r\n\t [[Node: Reshape = Reshape[T=DT_UINT8, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](DecodeRaw/_87, Reshape/shape)]]\r\nException in thread QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue:\r\n\r\n**I am attaching below my code snippet **\r\n", "comments": ["You have not specified whether you're executing custom code or a provided model exactly as documented.  Probably your model is not getting input of the size it expects, but it's impossible to tell from this information.\r\n\r\nUsage questions are better posed on StackOverflow.  This forum is for bug reports.  If you believe you have discovered a bug in tensorflow please post a small, reproducible example.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18284, "title": "contrib/autograph: minor spelling tweaks", "body": "", "comments": []}, {"number": 18283, "title": "Document for `tf.TextLine.Dataset.shuffle` is confusing.", "body": "In the [doc](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset#shuffle) for V1.7.\r\n\r\nThe signature is:\r\n```\r\nshuffle(\r\n    buffer_size,\r\n    seed=None,\r\n    reshuffle_each_iteration=None\r\n)\r\n```\r\n\r\nand in below description:\r\n`reshuffle_each_iteration: (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)`\r\n\r\nI can't tell what is default value for `reshuffle_each_iteration` and so I can't tell the default behaviour.", "comments": ["From the docstring as quoted:\r\n\r\n>  (Defaults to True.)", "The docstring is `True` but the signature shows `None`."]}]