[{"number": 6205, "title": "Adding fast layer normalization GPU kernels and layer functions to tf.contrib.layers", "body": "from issue #6146 \r\nInstead of putting the code into a new directory under `contrib`, I took some time and merge them into `contrib/layers`, because I thought it fits quite nicely there. \r\n\r\nDue to my rudimentary knowledge of Bazel's build rules, I couldn't figure out a way to put the kernel codes in the `kernels` folder, since there's a  `BUILD` file in the folder already, it outputs ` crosses boundary of subpackage` error during `./configure` if I reference kernel codes from `contrib/layers/BUILD`. As the result, I put them into `ln_kernels` folder as a temporary hacky measure.", "comments": ["Can one of the admins verify this patch?", "@ilblackdragon Thank you for the reviews, I am quite new to C++, and I wasn't even aware that a useful tool like clang-format exists, sorry for the potential headaches I may have caused. \r\nAs for the code duplication problem, I have some rough ideas on how to improve it, but it's quite late over here, so I will work on this tomorrow.\r\n", "I have run clang-format on the c++ codes using `-style=Google`, renamed a few variables for clarity, and fixed the issues pointed out by @ilblackdragon that I didn't have questions about.\r\n\r\nI also tried to merge the 3 separate ops into one, but to do that, I would have to pass `None` into the ops from python (for the cases where gamma and beta are not needed). I searched through the core kernels but couldn't find examples doing this, I have thought of two ways of handling this:\r\n1. pass `False` when gamma or beta is `None` and add boolean as supported dtype in `REGISTER_OP`. Then I could just handle different cases in c++ codes.\r\n2. Eliminate `LayerNormCustom` and `LayerNormBiasAddCustom`, then just pass in dummy constant tensors into the op like I am currently doing when gamma is not `None` but beta is.\r\n\r\noption 1 is a bit hacky and will probably not reduce too much code, since most code duplication happens in `layer_norm_fused_op_gpu.cu.cc` and I am not sure how to merge the three kernels into one without performance hit.\r\noption 2 will reduce the code by 3-fold, but will increase the run-time during backward pass by about 1.5-2x  when beta and gamma are not needed because of the unnecessary atomic adds. However, this performance hit will be a lot less significant in the context of entire model, since it is still a lot faster than most time-consuming ops like convolution and matmul. \r\n\r\nBoth options have some drawbacks, and I would like to know if you guys have any better suggestions to tackle this problem.", "@MycChiu Sorry for late response. I want to include people with better cuda understanding.\r\n\r\n@zheng-xq Can you please take a look at Cuda part and if you have any suggestions on simplifying TF kernels.", "@MycChiu also, could you merge & resolve conflicts? Thanks.", "No problem. Done.", "Sorry for the delay. So, for the kernels, you just define a `tf_kernel_library`:\r\n```\r\ntf_kernel_library(\r\n    name = \"depthwise_conv_op\",\r\n    prefix = \"depthwise_conv_op\",\r\n    deps = [\r\n        \":conv_ops\",\r\n        \":ops_util\",\r\n        \"//third_party/tensorflow/core:core_cpu\",\r\n        \"//third_party/tensorflow/core:framework\",\r\n        \"//third_party/tensorflow/core:lib\",\r\n        \"//third_party/tensorflow/core:nn_ops_op_lib\",\r\n    ],\r\n)\r\n```\r\nThen, for the ops, you just need to define a `cc_library`. Remember to set the `copts`:\r\n```\r\ncc_library(\r\n    name = \"word2vec_ops\",\r\n    copts = tf_copts(),\r\n    srcs = [\"ops/word2vec_ops.cc\"],\r\n    linkstatic = 1,\r\n    visibility = [\"//third_party/tensorflow:internal\"],\r\n    deps = [\"//third_party/tensorflow/core:framework\"],\r\n    alwayslink = 1,\r\n)\r\n```", "@drpngx Thanks for the directions, but by looking at the example, I am still not quite sure where to declare the dependencies for the GPU codes (`layer_norm_fused_op_gpu.cu.cc`).", "Oh, the `tf_custom_op_library` accepts a `deps` section like everything else, see for instance `contrib/rnn/BUILD`.", "OK, this most recent commit is as far as I can go, with this, if I run\r\n`bazel build -c opt --config=cuda //tensorflow/contrib/layers:layer_norm_fused_op_kernel`\r\nI would get this error:\r\n```\r\nERROR: /home/dizizmaname/tensorflow/tensorflow/contrib/layers/kernels/BUILD:36:1: C++ compilation of rule '//tensorflow/contrib/layers/kernels:layer_norm_fused_kernel' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 61 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:4:0,\r\n                 from tensorflow/contrib/layers/kernels/layer_norm_fused_op_gpu.cu.cc:23:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory\r\n #include <cuda_runtime.h>\r\n                          ^\r\ncompilation terminated.\r\nTarget //tensorflow/contrib/layers:layer_norm_fused_op_kernel failed to build\r\n```\r\nI think I was not clear enough in my last comment; what I meant was I didn't know where to put `layer_norm_fused_op_gpu.cu.cc`, if I put it in cc_library's srcs,like what I am doing in this commit, it would give the above error. If I put it in tf_custom_op_library like this:\r\n```\r\ntf_custom_op_library(\r\n    name = \"python/ops/_layer_norm_fused_op.so\",\r\n    srcs = [\r\n        \"ops/layer_norm_fused_op.cc\",\r\n        ],\r\n    gpu_srcs = [\r\n        \"kernels/layer_norm_fused_op_gpu.cu.cc\",\r\n       ],\r\n    deps = [\r\n        \"//tensorflow/contrib/layers/kernels:layer_norm_fused_kernel\",\r\n    ]\r\n)\r\n```\r\nit would give me the following error:\r\n```\r\nERROR: /home/dizizmaname/tensorflow/tensorflow/contrib/layers/BUILD:77:1: Label '//tensorflow/contrib/layers:kernels/layer_norm_fused_op_gpu.cu.cc' crosses boundary of subpackage 'tensorflow/contrib/layers/kernels' (perhaps you meant to put the colon here: '//tensorflow/contrib/layers/kernels:layer_norm_fused_op_gpu.cu.cc'?).\r\n```", "Ok, the error that you're getting is because of visibility. You have to to contrib/layers/kernels:layer_norm_fused_kernel rule, and set the visibility to your rule, something like \r\n\r\n```\r\nvisibility= [\"//tensorflow/contrib/layers/python/ops/...\"],\r\n```", "I made the visibility public like this in `/layers/kernels/BUILD`:\r\n```\r\ncc_library(\r\n    name = \"layer_norm_fused_kernel\",\r\n    copts = tf_copts(),\r\n    visibility = [\"//visibility:public\"],\r\n    srcs = [\r\n        \"layer_norm_fused_op.cc\",\r\n        \"layer_norm_fused_grad_op.cc\",\r\n        \"layer_norm_fused_op_gpu.cu.cc\",\r\n        \"layer_norm_fused_op.h\",\r\n    ],\r\n    deps = [\r\n        \"//tensorflow/core:framework_headers_lib\",\r\n        \"//third_party/eigen3\",\r\n        \"@protobuf//:protobuf\",\r\n    ],\r\n    alwayslink = 1,\r\n)\r\n```\r\n but bazel still throws the same error\r\n```\r\nERROR: /home/***/tensorflow/tensorflow/contrib/layers/kernels/BUILD:36:1: C++ compilation of rule '//tensorflow/contrib/layers/kernels:layer_norm_fused_kernel' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 61 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:4:0,\r\n                 from tensorflow/contrib/layers/kernels/layer_norm_fused_op_gpu.cu.cc:23:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory\r\n #include <cuda_runtime.h>\r\n                          ^\r\ncompilation terminated.\r\nTarget //tensorflow/contrib/layers:layer_norm_fused_op_kernel failed to build\r\n```", "The GPU sources need to be compiled in a special way. You probably want to use `tf_custom_op_library`  with `gpu_srcs` to set up the include path right.", "@drpngx I am taking @zheng-xq's advice and move this code out of `tf.contrib.layers` for now, so this is temporarily resolved, thanks for your help!", "Please resolve conflicts & update when ready. Thanks!", "Any update on this?\r\nThanks for the effort.", "@MycChiu what's the status. Are you planning to move this as stated?", "Sorry for the long delay, I have resolved the conflict just now. ", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "It looks like there are a lot of failures, most notably the sanity checking failures.\r\n\r\nCan you run clang-format --style=google over these files and make sure the relevant tests are passing?  Thanks!", "Oops, sorry for the long delay, I didn't realized my chrome logged me out of github.\r\nIt seemed like the sanity check failed because of tensorflow's indentation width is 2 spaces instead of 4.\r\nI have also fixed __ldg's backward compatibility error in Linux GPU test.\r\nAll the other checks seem to be aborted by unknown. Except the buildifier error in sanity check, which I am not really sure how to fix it.", "For BUILD, I would suggest installing https://github.com/bazelbuild/buildifier and running it over the BUILD file to fix any formatting issues", "@vrv Thank you! Apparently it was just some formatting issue, they are now buildified.", "Jenkins, test this please.", "You need to update your tests. There is no more `mul` or `concat_v2`. See `tools/compatibility/tf_upgrade.py`.\r\n\r\n```\r\nERROR: testCreateConvWithWD (__main__.ConvolutionTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 397, in testCreateConvWithWD\r\n    images, 32, [3, 3], weights_regularizer=regularizer)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers.py\", line 910, in convolution\r\n    outputs = layer.apply(inputs)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/layers/base.py\", line 323, in apply\r\n    return self.__call__(inputs, **kwargs)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/layers/base.py\", line 289, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/layers/convolutional.py\", line 138, in build\r\n    dtype=self.dtype)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 1034, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 933, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 349, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/layers/base.py\", line 278, in variable_getter\r\n    variable_getter=functools.partial(getter, **kwargs))\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/layers/base.py\", line 247, in _add_variable\r\n    regularization = regularizer(variable)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/regularizers.py\", line 107, in l2\r\n    return standard_ops.mul(my_scale, nn.l2_loss(weights), name=name)\r\nAttributeError: 'module' object has no attribute 'mul'\r\n```", "@drpngx Hmmm...It seems like those are from the `contrib/layers` folder, and git didn't merge the latest changes from master to this pull request after I moved my codes out of `contrib/layers`. I ended up just replace the folder with the one I have from the latest master branch, hopefully this would not make other things break.", "Jenkins, test this please.", "No luck: `//bazel_pip/tensorflow/contrib/layer_norm:layer_norm_fused_test`.\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'LayerNormCustom' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: LN/LayerNormCustom = LayerNormCustom[T=DT_FLOAT, epsilon=1e-12, _device=\"/device:CPU:0\"](Const)]]\r\n```", "@drpngx Ah! That's because it doesn't have a CPU implementation, is there a way to by-pass this?", "Could we just register the CPU kernel instead? It looks like you're using eigen so it should work both ways?", "@drpngx Unfortunately, the kernel itself is implemented with pure CUDA. Is there no way to only register the kernel for GPU?", "How hard would it be to have a CPU implementation? I think it might be possible to skip them under CPU, but it's such an unusual pattern that I'm concerned that it might trip up people.", "I imagine a CPU kernel won't be too hard, but I have almost no knowledge in the eigen library, so it would take a lot of time for me to write a native CPU layer norm kernel in C++. Is it okay to direct the CPU cases to the current layer_norm implementation in `tf.contrib.layers` library in python?", "Got it. I guess we can `try: except:` to bypass the error there, or use `tensorflow.python.client.device_lib.list_local_devices()` to check if a GPU is available.", "I believe there is a 'is_gpu_available' in test_util.py somewhere...  and you can see its uses.", "Jenkins, test this please.", "@vrv Thank you, I have added it to all test cases. @drpngx , Sorry, I forgot to add the `cuda_only` arguments in last commit, please run the checks again, thank you.", "Jenkins, test this please.", "Hmmm...It seems like Windows Cmake Tests failed with error `ImportError: cannot import name 'layer_norm'`.", "Oh did you modify the cmakefile?\n\nOn Feb 23, 2017 7:41 PM, \"MycChiu\" <notifications@github.com> wrote:\n\n> Hmmm...It seems like Windows Cmake Tests failed with error ImportError:\n> cannot import name 'layer_norm'.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6205#issuecomment-282195628>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbc38R4Yk6XA9_cIdk9Lee-XWPkxuks5rflFzgaJpZM4LImWx>\n> .\n>\n", "I don't think so...Could it be that the core code hasn't merged with master branch?", "Ah! Do you mean I have to `add_python_module(\"tensorflow/contrib/layer_norm\")` in the [tf_python.cmake](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake) file as well? I guess I will do it now.", "Also modify other files imitating what's done for `contrib.rnn`, I hope this will do. *One question out of curiosity though, from `contrib.cmake`'s readme, it seems like they don't support custom_op libraries and most cotrib, so why are we still adding modules from contrib? Just to avoid the `ImportError` I encountered? or prepare for future support?*", "There's \"it should work\" and \"we support\". Contrib should work, at least build, but we don't support it like we support core, which has a stable API and should work well at all times. CMake itself is not technically officially supported, since it's in contrib, but we make sure it's always working.", "I see. I think I have added all the necessary codes for cmake, can we ask Jenkins to test again now?", "Jenkins, test this please.", "Woohoo! Passes. According to the thread, @zheng-xq reviewed with \"initial comments\".\r\n\r\n@zheng-xq first pass done, would you mind taking another look?", "YES! Finally! Thank you so much for your patience @drpngx. ", "Jenkins, test this please.", "Looks like the test failure was a flake? I'll try retriggering. In the mean time, @zheng-xq PTAL?", "@tensorflow-jenkins Test this please", "Jenkins, test this please.", "Jenkins, test this please.", "I re-ran the tests, @zheng-xq left comments, could you address those? ", "Since the changes are in the contribs, they look reasonable to me in general. My only remaining concern is lack of benchmarks to support the performance improvement claim. More benchmarks results will make this CL more self sufficient for future improvement. That is a common requirement for core changes. Since this is only in contribs, it is up to you.", "@MycChiu a benchmark would indeed be good. You can look at the benchmarks for the convolution or matmul functions as examples, can you post results?", "I'm very interested in this getting merged. @martinwicke why are the benchmarks in https://github.com/MycChiu/fast-LayerNorm-TF not sufficient?\r\n\r\nIf not, where are the \"convolution or matmul\" benchmarks you are talking about? I would like to help.", "Jenkins, test this please.", "@MycChiu did you get a chance to add a benchmark?", "Sorry, tensorflow/core/kernels/BUILD is a good entry point for existing benchmarks. It also has instructions on how to run them. You can look for \"benchmark\" to find benchmark targets and their sources.\r\n\r\n@annarev FYI a good use case for benchmarking instructions.", "@tensorflow-jenkins test this please\r\n\r\nI think we can accept without benchmarks for now.  ", "@MycChiu seems to have disappeared :(\r\n\r\nI'm going to close this to keep our active PRs low (otherwise things slip through the cracks); I'm confident once they are back we'll get this PR in!", "Hi, I just wonder how the layer_norm finally goes? As for now (Jan 2018) I think the layer_norm is still using the batch_norm as the backbone. And for my model training of a small mini-batch size, layer_norm is almost 2x slower than batch normalization, any luck that we can make it faster? \r\n![image](https://user-images.githubusercontent.com/9154611/35251597-c5136930-ff90-11e7-8a14-157d96088ef9.png)\r\n\r\nThanks!", "Is there any update about this issue?\r\n", "I believed we are left with slow normalization in TensorFlow, which surprises me.\r\n\r\nMy experience shared here: https://medium.com/@mansanher/normalization-in-tensorflow-speed-is-an-issue-b8ae14336685", "Thank you @manuelsh for pointing this issue out. \r\nI have one question, did you use the `tf.contrib.rnn.LayerNormBasicLSTMCell` with `layer_norm=True` for your experiments? or have you tried your custom layer normalization implementation? if yes, can you share it with us?\r\nI have tried the @MycChiu  pull request and I got fast layer_normalization in term of time but it required more steps to converge than the model without layer_normalization !!!\r\n", "Hi Aziz,\r\n\r\nYou can find my code [here](https://github.com/manuelsh/normalisation-tensorflow/blob/master/mnist_model_tensorflow.ipynb).\r\n\r\n", "Was this ever planned on being completed?  It sounded quite promising and seemed like it was almost done.", "If someone wants to take over this PR, that would be great. You are right, it is almost done, it should just have a microbenchmark to make sure it's performing as expected (and we can continuously monitor the performance)."]}, {"number": 6204, "title": "Corrected description for ExportStrategy", "body": "", "comments": []}, {"number": 6203, "title": "accelerate crf_log_norm", "body": "the base `crf_log_norm` function looks very cool, but it runs very slow.\r\nI run the experiment on cpu\r\n```\r\nmodel name  : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz\r\n```\r\n\r\nI use the configure\r\n```python\r\nbatch_size=128\r\nmax_seq_len=200\r\n```\r\nit runs `0.5s` per batch\r\n\r\nwhen I optimize the realization way\r\nit  runs `0.25s` per batch", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "i have signed google cla use email mail@wanguanglu.com and GitHub Account Name wanguanglu\r\n", "Can you git commit --amend with the correct git email config set to mail@wanguanglu.com ?\r\n\r\nhttps://patch-diff.githubusercontent.com/raw/tensorflow/tensorflow/pull/6203.patch shows you probably don't have the right commit email set in your git config.", "OK, I try again", "i tried again by change the `.gitconfig`. it's OK now, thank you."]}, {"number": 6202, "title": "Inconsistent API for ones_initializer/zeros_initializer", "body": "This [cl](https://github.com/tensorflow/tensorflow/commit/cfb2280d3f6ced298681bd1141479a59a06abde8) has removed `shape` argument from `ones_initializer`, however, this argument remains for `zeros_initializer`.\r\n\r\nThis is a breaking change so maybe it should be mentioned in RELEASE.md with instructions on how code should transition (ie `ones_initializer` should be replaced with `ones_initializer()`). Currently older code that works in 0.11 fails in 0.12 with obscure message\r\n\r\nie\r\n\r\n```\r\n    update = tf.get_variable(name=\"update\", shape=[params_size], dtype=dtype,\r\n                             initializer=tf.ones_initializer)\r\n\r\n```\r\n\r\nfails with\r\n```\r\n...\r\n    initial_value(), name=\"initial_value\", dtype=dtype)\r\n  File \"/home/yaroslav/.conda/envs/openai/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 665, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\nTypeError: ones_initializer() got multiple values for argument 'dtype'\r\n\r\n```\r\n\r\n@itsmeolivia ", "comments": ["BTW, this seems to be fixed in https://github.com/tensorflow/tensorflow/pull/6339 (ie, syntax is consistent, both of them need to be tf.ones_initializer() and tf.zeros_initializer() ) , except maybe RELEASE.md could mention that people should add \"()\" to their tf.ones_initializer", "I get\r\n  \r\nTypeError: zeros_initializer() got multiple values for keyword argument 'dtype'\r\nfor the inception model command:\r\n\r\nbazel-bin/tensorflow_serving/example/inception_export --checkpoint_dir=inception-v3 --export_dir=inception-export\r\n\r\nNOTE: I am able to successfully run the MNIST example for tensorflow serving but not the inception one.\r\nI have tried all the recommendations mentioned previously.\r\n\r\nThe trace is as follows:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/tf_serving/tensorflow_serving/example/inception_export.py\", line 169, in <module>\r\n    tf.app.run()\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/tf_serving/tensorflow_serving/example/inception_export.py\", line 165, in main\r\n    export()\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/tf_serving/tensorflow_serving/example/inception_export.py\", line 79, in export\r\n    logits, _ = inception_model.inference(images, NUM_CLASSES + 1)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/inception_model.py\", line 87, in inference\r\n    scope=scope)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/inception_model.py\", line 87, in inception_v3\r\n    scope='conv0')\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/scopes.py\", line 155, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/ops.py\", line 234, in conv2d\r\n    outputs = batch_norm(conv, **batch_norm_params)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/scopes.py\", line 155, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/ops.py\", line 90, in batch_norm\r\n    restore=restore)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/scopes.py\", line 155, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/variables.py\", line 289, in variable\r\n    trainable=trainable, collections=collections)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 1063, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 889, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 347, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 332, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 683, in _get_single_variable\r\n    validate_shape=validate_shape)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variables.py\", line 225, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variables.py\", line 322, in _init_from_args\r\n    initial_value(), name=\"initial_value\", dtype=dtype)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 672, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\nTypeError: zeros_initializer() got multiple values for keyword argument 'dtype'", "This is a reference is to a different project that includes tensorflow. Confused about what changes I need to make in the tensorflow-serving branch to get rid of this issue.\r\n\r\nAppreciate if you can spell out the exact changes I need to make.", "`tf.zeros_initializer` had multiple incompatible API changes between 0.11, 0.12, and -master.\r\nIf you need compatibility across versions, use `tf.constant_initializer(0.0)` instead.", "I am also running into this problem when following the TensorFlow serving tutorial here https://tensorflow.github.io/serving/serving_inception.html", "Still experience the same issue here. Just did a fresh clone of tf_serving to export weights which has the latest tensorflow as well", "@neuralearner The issue I described is due to API change affecting user code and can be fixed by changing tf.ones_initializer to tf.ones_initializer() in your program", "I see that the .pyc files have the zero_initializer. Has anyone tried replacing the zero_initializer function with constant_initializer and regenerated the pyc files. In that case please outline the steps you followed.", "@neuralearner I got it working by changing tf.ones_initializer to tf.ones_initializer() and tf.zeros_initializer to tf.zeros_initializer() in `/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/ops.py`.  [This issue](https://github.com/tensorflow/serving/issues/250) helped me solve the problem.", "Thanks @alexminnaar ! This solution worked for me. There were a few more hacks required.\r\n\r\nFor those trying please also note that you need to change all tf.histogram_summary and tf.scalar_summary to tf.summary.histogram and tf.summary.scalar, to successfully load and export the model as indicated in the post pointed by @alexminnaar above \r\n\r\n", "@aselle wrote a tool to try to upgrade code (at least at 0.11?) to 1.0 APIs.  See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility and send PRs / file bugs for better functionality :)"]}, {"number": 6201, "title": "Undefined symbols for architecture x86_64", "body": "I builded a sample of ios app from https://github.com/yjmade/ios_camera_object_detection, but:\r\n\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"_cblas_sgemm\", referenced from:\r\n      tensorflow::Conv2DUsingGemmOp<float, tensorflow::(anonymousnamespace)::Im2ColConvFunctor<float, float, float, FastGemmFunctor<float, float,float> > >::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(conv_ops_using_gemm.o)\r\n      tensorflow::FusedResizeConv2DUsingGemmOp<float, tensorflow::(anonymousnamespace)::FusedResizeAndPadConvFunctor<float, float, float,FastGemmFunctor<float, float, float>, (tensorflow::(anonymousnamespace)::SamplingMode)0>, true>::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(conv_ops_fused.o)\r\n      tensorflow::FusedResizeConv2DUsingGemmOp<float, tensorflow::(anonymousnamespace)::FusedResizeAndPadConvFunctor<float, float, float,FastGemmFunctor<float, float, float>, (tensorflow::(anonymousnamespace)::SamplingMode)1>, false>::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(conv_ops_fused.o)\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nIs that any problem of my tensorflow?\r\nMac pro  \r\nIntel Iris Graphics 6100 1536 MB\r\n64bit", "comments": ["@GumpCode Did you make sure you have included the Accelerate framework in the \"Link Binary with Libraries\" build phase of your project. That's where cblas_sgemm should be defined when you are building for iOS.", "@eelretep I solved it by including the Accelerate framework in the \"Link Binary with Libraries\", thanks!"]}, {"number": 6200, "title": "Fix tutorials failures.", "body": "ptb_word_lm should work, but want to see how cifar10_train will look.", "comments": ["ptb_word_lm is fixed (great!) \r\n\r\nBut looks like cifar10_train is still failing, maybe read_input.label needs to call set_shape() as well?", "Thanks a lot for the fix, @gunan !", "Hi, with the new version of read.py I get the following error message:\r\nTypeError: strided_slice() takes at least 4 arguments (3 given)    ", "@physicsai are you using compatible versions of this file and the pip package?\r\n\r\nIf you have tensorflow 0.12.0rc0 installed, you need to use the version of the file in r0.12 branch of this repo. If you have built tensorflow from source, you can use this file.\r\n\r\nAlso, please reach out to us by either filing an issue, or through stackoverflow. Pull requests, after merging, cannot really address any questions anymore.", "@gunan  It was indeed an issue with tensorflow verions. It runs all fine when I clone the r0.11 files. Thanks for your help! "]}, {"number": 6199, "title": "\"Invalid proto descriptor for file \"tensorflow/contrib/tensorboard/plugins/trace/trace_info.proto\"", "body": "[trace_info.proto](https://github.com/tensorflow/tensorflow/blob/287db3a9b0701021f302e7bb58af5cf89fdcd424/tensorflow/contrib/tensorboard/plugins/trace/trace_info.proto) seems to be missing `import tensorflow/core/protobuf/meta_graph.proto`. It works for Python protobuf implementation but fails with tracing runs on stricter cpp implementation:\r\n\r\n[scratch.py](https://gist.github.com/yaroslavvb/b0b9e51d88bb97aa0f739f8130e1c3d5)\r\n\r\n```\r\npython scratch.py\r\n\r\n...\r\n    contrib = importlib.import_module('tensorflow.contrib')\r\n  File \"/home/yaroslav/.conda/envs/openai/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 665, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/home/yaroslav/.conda/envs/openai/lib/python3.5/site-packages/tensorflow/contrib/__init__.py\", line 49, in <module>\r\n    from tensorflow.contrib import tensorboard\r\n  File \"/home/yaroslav/.conda/envs/openai/lib/python3.5/site-packages/tensorflow/contrib/tensorboard/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.tensorboard import plugins\r\n  File \"/home/yaroslav/.conda/envs/openai/lib/python3.5/site-packages/tensorflow/contrib/tensorboard/plugins/__init__.py\", line 23, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins import trace\r\n  File \"/home/yaroslav/.conda/envs/openai/lib/python3.5/site-packages/tensorflow/contrib/tensorboard/plugins/trace/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins.trace.trace import *\r\n  File \"/home/yaroslav/.conda/envs/openai/lib/python3.5/site-packages/tensorflow/contrib/tensorboard/plugins/trace/trace.py\", line 28, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins.trace.trace_info_pb2 import TraceInfo\r\n  File \"/home/yaroslav/.conda/envs/openai/lib/python3.5/site-packages/tensorflow/contrib/tensorboard/plugins/trace/trace_info_pb2.py\", line 22, in <module>\r\n    serialized_pb=_b('\\n=tensorflow/contrib/tensorboard/plugins/trace/trace_info.proto\\x12\\ntensorflow\\\"Q\\n\\tTraceInfo\\x12\\x1f\\n\\x03ops\\x18\\x01 \\x03(\\x0b\\x32\\x12.tensorflow.OpInfo\\x12#\\n\\x05\\x66iles\\x18\\x02 \\x03(\\x0b\\x32\\x14.tensorflow.FileInfo\\\"\\xb2\\x01\\n\\x06OpInfo\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07op_type\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x03 \\x01(\\t\\x12(\\n\\ttraceback\\x18\\x04 \\x03(\\x0b\\x32\\x15.tensorflow.LineTrace\\x12&\\n\\x06inputs\\x18\\x05 \\x03(\\x0b\\x32\\x16.tensorflow.TensorInfo\\x12\\'\\n\\x07outputs\\x18\\x06 \\x03(\\x0b\\x32\\x16.tensorflow.TensorInfo\\\"3\\n\\tLineTrace\\x12\\x11\\n\\tfile_path\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0bline_number\\x18\\x02 \\x01(\\r\\\"Y\\n\\nTensorInfo\\x12\\r\\n\\x05shape\\x18\\x01 \\x03(\\x05\\x12\\r\\n\\x05\\x64type\\x18\\x02 \\x01(\\t\\x12\\x1a\\n\\x12num_bytes_per_elem\\x18\\x03 \\x01(\\r\\x12\\x11\\n\\tconsumers\\x18\\x04 \\x03(\\t\\\"\\xbb\\x01\\n\\x08\\x46ileInfo\\x12\\x11\\n\\tfile_path\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0bsource_code\\x18\\x02 \\x01(\\t\\x12K\\n\\x14multiline_statements\\x18\\x03 \\x03(\\x0b\\x32-.tensorflow.FileInfo.MultilineStatementsEntry\\x1a:\\n\\x18MultilineStatementsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\r\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\r:\\x02\\x38\\x01\\x62\\x06proto3')\r\n  File \"/home/yaroslav/.conda/envs/openai/lib/python3.5/site-packages/google/protobuf/descriptor.py\", line 827, in __new__\r\n    return _message.default_pool.AddSerializedFile(serialized_pb)\r\nTypeError: Couldn't build proto file into descriptor pool!\r\nInvalid proto descriptor for file \"tensorflow/contrib/tensorboard/plugins/trace/trace_info.proto\":\r\n  tensorflow.TensorInfo.dtype: \"tensorflow.TensorInfo.dtype\" is already defined in file \"tensorflow/core/protobuf/meta_graph.proto\".\r\n  tensorflow.TensorInfo: \"tensorflow.TensorInfo\" is already defined in file \"tensorflow/core/protobuf/meta_graph.proto\".\r\n  tensorflow.OpInfo.inputs: \"tensorflow.TensorInfo\" seems to be defined in \"tensorflow/core/protobuf/meta_graph.proto\", which is not imported by \"tensorflow/contrib/tensorboard/plugins/trace/trace_info.proto\".  To use it here, please add the necessary import.\r\n  tensorflow.OpInfo.outputs: \"tensorflow.TensorInfo\" seems to be defined in \"tensorflow/core/protobuf/meta_graph.proto\", which is not imported by \"tensorflow/contrib/tensorboard/plugins/trace/trace_info.proto\".  To use it here, please add the necessary import.\r\n\r\n```\r\n\r\n`__git_version__: \"b'0.12.0-rc0-437-g6d63f67'\"` ( pulled from head Dec 6th 5pm)\r\n", "comments": ["Strange that you are seeing this error at HEAD. This was due to both `meta_graph.proto` and `trace_info.proto` having the same package `tensorflow` and defining the same message `TensorInfo`.\r\nThis got fixed 10 days ago with commit efd47f2418629245740f9c7d775162cc80d197fc\r\n\r\nDid you install tensorflow from head or did you use the 0.12 RC0 version? Let me know if the error still persists at head. Thanks!", "Aha, it seems I don't have that change. While your commit entered internal branch on Dec 2, it was [merged](https://github.com/tensorflow/tensorflow/commit/e624a5f4) into HEAD on Dec 6 at 8:10 PM, so I was using pre-patch version."]}, {"number": 6198, "title": "Fully disable all test/build targets that execute tensorflow during build.", "body": "", "comments": ["Jenkins, test this please?", "@tensorflow-jenkins test this please", "Jenkins, test this please.", "CPU test failure known issue.", "Why do the genrule targets need to be commented out? Shouldn't it be sufficient to just mark the tests manual as was done before? If that's not sufficient, adding `tags = [\r\n        \"local\",\r\n        \"manual\",\r\n    ],` will definitely have Jenkins skip them for now until a fix is ready. As is, this breaks TensorFlow Serving and we can't sync to head. Can we revert this change or uncomment the genrules?", "That makes jenkins skip \"test execution\", but in this case problem is actually building them.\r\nYou can see that prior to this change I tried exactly what you suggested. But jenkins still built them. And the problem in our case is the tensorflow library usage during build of these targets.\r\n\r\n", "The problems mentioned shouldn't be an issue if we just have the genrules though. If no test is linking it in (i.e. keep tests commented out for now), it shouldn't be built. If it's somehow being linked, it will be compiled at most once per run so there should be no path collisions. Do you think that would work?", "we can try it out, if you have a PR I can try to see if it works.\r\nAnother way I see it working is, you can create these genrules within tensorflow serving repository and reference those genrules.\r\nAny reason these specificly have to live within tensorflow repo?", "I don't have a PR. We don't do much development in git, I'm more comfortable making this change internally, but I see this is purely in Github. Could someone from the TF team undo the genrule comment change and test? (the usual tests we always run pass so I'm not sure where it failed)\r\n\r\nWe needed it in the TF repo because the other tests there were commented out here depended on it.", "It causes a flake, which is much harder to track, and thats why it triggered our response.\r\n\r\nWhat do you mean other tests commented out here, they are commented out, so they should not run. You mean tests not under third_party/tensorflow?\r\n\r\nAgain, if you have any tests in your repo, you can easily create the same genrule, and modify your tests to depend on those. This genrule is definitely going away from tensorflow, I can guarantee that, so the sooner your tests can avoid depending on them, the better."]}, {"number": 6197, "title": "Applying Changes on CMake and Setup Docs", "body": "Applying changes previously suggested by @mrry on:\r\n - CMake README \r\n - get_started common issues", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "Failures all known issues."]}, {"number": 6196, "title": "Some example .py files missing from pip-installed TensorFlow source tree", "body": "I'm working through TensorFlow's [the RNN tutorial](https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html). Navigating to the `tensorflow/models/rnn/ptb` directory, I should see `reader.py` and `ptb_word_lm.py`, but I only see `reader.py` (alongside `__init__.py` etc).\r\n\r\nI've tried both the install using `https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py3-none-any.whl`, and the default pip installation; they don't differ in this regard.\r\n\r\nI wonder if this is related to #5014, which also deals with missing files. Everything seems to _work_ correctly, as far as I can tell \u2014 it's just that some of these examples are missing from the source tree.\r\n\r\n### Environment info\r\n\r\n- Operating System: macOS 10.12.1\r\n- I'm using Homebrew'd Python 3.5 in a virtualenv.\r\n- The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`: `0.12.0-rc0`", "comments": ["@petewarden is this deliberate?", "No, I believe this is a problem that has been introduced by moving the PTB examples over to the tensorflow models repo. If you follow the tutorial, it points you to tensorflow/models/rnn/ptb, but the code is now in https://github.com/tensorflow/models/tree/master/tutorials/rnn\r\n\r\n@nealwu can you take a look, since you were involved in the move?", "Thanks for reporting this. I've already fixed this tutorial to clarify where to find those model files after the move, so it should be visible the next time we push out an update to the TensorFlow site.\r\n\r\nIn the meantime, use this folder as a reference: https://github.com/tensorflow/models/tree/master/tutorials.", "Thanks! That's helpful.", "@nealwu line 133 of \"ptb_word_lm.py\" has a comment that reads:\r\n\r\n     # The alternative version of the code below is:\r\n     #\r\n     # inputs = tf.unstack(inputs, num=num_steps, axis=1)\r\n     # outputs, state = tf.nn.rnn(cell, inputs,\r\n     #                            initial_state=self._initial_state)\r\n\r\nhttps://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L133\r\n\r\nHowever, it is unclear from the comment which code below needs to be replaced (i.e. how many lines below the comment need to be replaced). Can you speak to this?\r\n\r\nThanks\r\nTed\r\n\r\n", "@tedsandler It replaces the 7-line chunk of code right below it that generates `outputs` and `state`.\r\n\r\nAlso, this is an issue someone else opened and isn't the best place to ask such a question. In the future if you believe there is a problem with the code you can open a new issue, or if you just have a question like this, you should ask on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow)."]}, {"number": 6195, "title": "Running summary call crashes sporadically", "body": "Every so often when calling session.run on my summary graph, the whole thing crashes. The only error it says is Segmentation fault (core dumped). I don't know exactly what steps to take to reproduce it, but I do know it fails on this line:\r\n``` python\r\n_, summary_str = sess.run([train_op, summary],  \r\n    options=run_options,\r\n    run_metadata=run_metadata)\r\n```\r\nSummary is just defined as `summary = tf.summary.merge_all()`\r\n\r\nIt never crashes when only running train_op, and only crashes when running the summary every 10,000 or so iterations.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: 8.0, cudnn 5.1.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nls -l /usr/local/cuda/lib64/libcud* results in \r\n\r\n-rw-r--r-- 1 root root   558720 Nov 17 14:07 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Nov 17 14:07 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Nov 17 14:07 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Nov 17 14:07 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Nov 17 14:07 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Nov 17 17:24 /usr/local/cuda/lib64/libcudnn.so\r\nlrwxrwxrwx 1 root root       33 Nov 17 17:25 /usr/local/cuda/lib64/libcudnn.so.5 -> /usr/local/cuda/lib64/libcudnn.so\r\nlrwxrwxrwx 1 root root       33 Nov 17 17:25 /usr/local/cuda/lib64/libcudnn.so.5.1 -> /usr/local/cuda/lib64/libcudnn.so\r\nlrwxrwxrwx 1 root root       33 Nov 17 17:25 /usr/local/cuda/lib64/libcudnn.so.5.1.5 -> /usr/local/cuda/lib64/libcudnn.so\r\n\r\n1. The commit hash (`git rev-parse HEAD`) 93a91d9b782e01612175bb1f76688aa2580a968f\r\n2. The output of `bazel version`\r\nExtracting Bazel installation...\r\n..........\r\nBuild label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n", "comments": ["Could you supply a link to the log that is written leading up to the crash?", "https://dl.dropboxusercontent.com/u/23140466/events.out.tfevents.1481226412.ava\r\nThe log itself should be uncorrupted since the summary call that crashes everything never gets written to it.", "Do you have INFO logging turned on?\r\n\r\n```python\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n```\r\n\r\nCould you send the plain-text output of that?", "That didn't seem to produce any extra output.", "Can you attach gdb to the Python process and capture the stack?", "#0  0x00007ffea76664ef in ?? () from /usr/local/cuda/extras/CUPTI/lib64/libcupti.so\r\n#1  0x00007ffea762d2b3 in ?? () from /usr/local/cuda/extras/CUPTI/lib64/libcupti.so\r\n#2  0x00007ffea763531b in ?? () from /usr/local/cuda/extras/CUPTI/lib64/libcupti.so\r\n#3  0x00007ffed7d961d0 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#4  0x00007ffed7d97615 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#5  0x00007ffed7cbafca in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#6  0x00007ffed7cbd965 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#7  0x00007ffed7df87e9 in cuMemcpyHtoDAsync_v2 () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#8  0x00007ffedfb683f0 in perftools::gputools::cuda::CUDADriver::AsynchronousMemcpyH2D(perftools::gputools::cuda::CudaContext*, unsigned long long, void const*, unsigned long long, CUstream_st*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#9  0x00007ffedfafc618 in perftools::gputools::Stream::ThenMemcpy(perftools::gputools::DeviceMemoryBase*, void const*, unsigned long long) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#10 0x00007ffedf6214f4 in void tensorflow::(anonymous namespace)::ConcatGPUCall<float, int>(tensorflow::OpKernelContext*, std::vector<std::unique_ptr<tensorflow::TTypes<float, 2, long>::ConstMatrix, std::default_delete<tensorflow::TTypes<float, 2, long>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<float, 2, long>::ConstMatrix, std::default_delete<tensorflow::TTypes<float, 2, long>::ConstMatrix> > > > const&, tensorflow::TTypes<float, 2, long>::Tensor*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#11 0x00007ffede6f26cc in tensorflow::TensorArrayPackOrGatherOp<Eigen::GpuDevice, float, false>::Compute(tensorflow::OpKernelContext*) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#12 0x00007ffedf982e04 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#13 0x00007ffedfa66db9 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#14 0x00007ffedfa6747a in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#15 0x00007ffedfc7cda8 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#16 0x00007ffedfc7c570 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#17 0x00007ffff67f8a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#18 0x00007ffff7959184 in start_thread (arg=0x7ffea8d20700) at pthread_create.c:312\r\n#19 0x00007ffff768637d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\r\n", "@zheng-xq can you take a look? Seems to be crashing in a GPU copy.", "I was able to partially fix this by writing fewer full trace summaries to file. But I don't think that's a real fix.", "@danielgordon10  Did you find any concrete solution to this problem? I am also facing the same issue. \r\nTensorflow - 1.0.0\r\nCC: @zheng-xq @michaelisard ", "@danielgordon10 @ankitp94 are you still experiencing this issue?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of activity. Please reopen if there is new information."]}, {"number": 6194, "title": "Can I reshape Tensor in C++ like Caffe's Blob", "body": "Hello,\r\nI want to use tensors of dynamic shapes in C++.\r\nCan I reshape Tensor like Caffe's Blob, which means after reshape, the  total size of new tensor  can be unchanged.\r\n", "comments": ["This type of usage question is better asked on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow)"]}, {"number": 6193, "title": "gradients for assign/tf.scatter_update, etc", "body": "Hello,\r\n\r\nI was wondering whether there is a specific reason for tf.assign not having gradients.\r\n\r\nI think that it would make sense that tf.assign, i.e. a call of the form `variable.assign(value)` \"inherits\" the gradients from value, such that `grad(variable) = grad(value)`, after the assignment.\r\n\r\nThis would be especially useful within the context of the scatter-update functions, since those are one of the most convenient ways to avoid indexing and do partial updates of variables.\r\n\r\nIf there are cases where this is not required, assign could maybe take an argument that allows this, such as \"inherit_gradients\", or some such, do not break compatibility.\r\n\r\nThank you!\r\n", "comments": ["@girving can you comment?", "@cle-ros Can you give me an example of what new correct behavior it would allow?  It seems likely that it would also allow a lot of silently weird, unexpected behavior (whenever the pattern of imperative updates doesn't fit with the case you have in mind), so I'd be leery.", "Thank you for your quick response!\r\n\r\nThere are two cases where this could be useful. The might be recurrences, since this would allow to make updates explicit. (I'm not sure here, though)\r\n\r\nThe second, which I am more interested in, is that I have several update functions I only want to apply to specific parts of my tensor, like with the following numpy code:\r\n```\r\na = np.zeros((10,10))  # this could be any other tensor result from any kind of (differentiable) computation\r\nb = [(1,3), (3,5), (2,4)]\r\nfor index in b:\r\n    a[index] = some_other_function\r\n```\r\nConsidering that tf.scatter_update looks like a function designed for these cases, it not being differentiable (if some_other_function is) requires me to do quite a lot of additional work to perform the same step.\r\n", "@cle-ros I think you're asking for a much more complicated feature than you think you're asking for.  Simply declaring the gradient of assignment to be something specific won't help you; you want these gradient updates to be chained through imperative updates to variables to track the overall flow.  Unfortunately, this is out of scope for TensorFlow as is, since we never track anything across variable updates.", "@girving Haha, thanks, I can see how that may be the case. Anyways, I'll just continue using my existing code. Thanks for your quick response.\r\n\r\nPS: what about the first point on the recurrences?", "\"recurrences\" is a perfectly fine word, but I'm not sure what it refers to in this context.", ":-) I mean that you update the variable - including it's gradients - at a later point in the code. When I had this thought, I was thinking of recurrence relationships, where you initialize your variable to one value, and later update it to another that you might have wanted to differentiate.\r\n\r\nHowever, I just realized my mistake - the graph has to be static for the gradient calculation (which afaik only happens once and is not updated). So updating a variable like this, or having a separate variable, should end up being the same. Thanks again! \r\n\r\nI'll close this issue now."]}, {"number": 6192, "title": "tensorflow android camera demo: replacing inception5h with incetoption-resnetV2", "body": "I am trying to replace the provided inception5h with latest inception-resnetV2. I have downloaded the inception-resnet-V2 by entring \r\n `wget http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz -O /tmp/inception_resnet_v2_2016_08_30.tar.gz`\r\n\r\nAnd unzip it in \r\n\r\n`unzip /tmp/inception_resnet_v2_2016_08_30.tar.gz -d tensorflow/examples/android/assets/`\r\n\r\nAfter unzip I only found **inception_resnet_v2_2016_08_30.ckpt** but for Android demo requires a `MODEL_FILE` with .pb extension and a `LABEL_FILE`. How I can get these files for inception-resnetV2.\r\n", "comments": ["I can't speak to that specific model, but in the general case you have to have the actual GraphDef which defines the model topology (this may be created by a python script somewhere?). Then you can run [freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) on it to combine it with the checkpoint file, which embeds all the trained weights inside it as constant ops so that it can easily be loaded on mobile.\r\n\r\nAdditionally, you can then run [optimize_for_inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py) to strip out unused ops that may not be supported on Android.", "@andrewharp  Can you provide some python script that would do that? I am learning tensorflow, still at beginner level.", "The linked scripts are both Python scripts that contain example invocations in the comments;  you just have to provide a GraphDef in addition to the checkpoint you already have. I'm not sure where the corresponding GraphDef for your checkpoint is found, perhaps wherever you found the download link has associated code for generating it?", "Closing; please reopen if you have specific issues with the scripts."]}, {"number": 6191, "title": "Tensorflow pypi packages not found from TravisCI", "body": "I am experiencing trouble installing tensorflow from TravisCI. As tensorflow 0.12.0rc0 is on pypi a simple\r\n\r\n    pip install tensorflow\r\n\r\nshould install tensorflow (CPU). Yet, although this works locally, it does not work on Travis with the following error:\r\n\r\n    Could not find any downloads that satisfy the requirement tensorflow==0.12.0rc0 (from -r requirements.txt (line 1))\r\n      No distributions at all found for tensorflow==0.12.0rc0 (from -r requirements.txt (line 1))\r\n\r\nAccording to [this scipy issue](https://github.com/scipy/scipy/issues/6213), it could be a problem with the packages that are being uploaded to pypi. In particular, there is a `cp27mu-manylinux` version for python 2.7 and `cp34m-manylinux` versions for python 3.4 (and also 3.5). The above issue seems to suggest that the \"mu\" versions would be used for python3 and the \"m\" versions for 2.7.\r\n\r\nOther packages solve this problem by installing tensorflow from the URL. Given that tensorflow is now on pypi, the failure to import it from travis seems to point to a bug in your build system.", "comments": ["@yifeif can you take a look? I don't know why it would choke on the mu for 2.7, strange version of python in travis?", "The CPython you have might be compiled with narrow unicode. Could you check with `import sysconfig\r\nsysconfig.get_config_var('Py_UNICODE_SIZE')`? So far we are only providing package for Python2.7 built with UCS4 (`-enable-unicode=ucs4`) on Pypi as it is used by most Linux distributions. \r\n\r\nDoes install from the URL work for your use case? ", "Hi yifeif,\r\nunicode size is 4 on python 2.7 and `None` with python3.\r\n\r\nIf I use the URL to install, the installation terminates successfully, but I can't import tensorflow:\r\n\r\n    ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.16' not found (required by /home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so)", "Thanks @igordertigor for checking! \r\nCould you also check the pip version on travis?\r\n\r\nLooking at the error , our binary might be built against a higher version of glibc than the ones you have. Please refer to [here](https://github.com/tensorflow/tensorflow/issues/5033#issuecomment-263681255) for more details \r\n\r\nIs it possible to upgrade them on your system?\r\n\r\n\r\n", "Thank you @yifeif,\r\nI'm running `pip install -U pip` before doing anything else on travis. So the pip version is currently the most recent one, `9.0.1`.\r\n\r\nI agree that the issue you reference is likely to be related, but it seems as if the discussion there doesn't really include a fix that would work on travis. Or am I missing something there?\r\n\r\nAlso, I'm not quite sure what you mean by \"them\" when you ask if it is possible to upgrade them on my system. I can locally upgrade anything, but that wouldn't help on travis. Seems like upgrading glibc on travis is a bit of a hassle, but might work.", "I meant upgrading glibc on travis :). Could you give it a try?", "Ok, upgrading glibc did not really work, but choosing a more recent ubuntu distribution (trusty rather than precise) seems to do the cut and I can import tensorflow."]}, {"number": 6190, "title": "Hanging when Matching Filenames in 0.12", "body": "I have a network which uses `tf.train.shuffle_batch` with a `tf.train.string_input_produces`. The network worked in 0.11, but hangs in 0.12. \r\n\r\nDisclaimer: I don't have access to 0.11 anymore and I changed some other stuff, but I'm fairly sure that the important bit is still the same.\r\n\r\nHanging means: I don't get further than the \"Creating TensorFlow device\" message.\r\n\r\nThe reason I'm thinking it's due to the queue is because after enabling the `log_device_placement` flag, the last output I'm seeing is\r\n\r\n    I tensorflow/core/common_runtime/simple_placer.cc:827] \r\n        matching_filenames/MatchingFiles/pattern\r\n\r\nAlso, using a barebowns graph like\r\n\r\n```\r\na = tf.constant(4)\r\nb = tf.constant(2)\r\ns = tf.Session()\r\ns.run(a + b)\r\n```\r\n\r\nworks, so I don't think it is due to my TensorFlow installation.\r\n\r\nThe hang is reproducible for me. I don't know how to enable further logging but would be glad to do so.\r\n\r\n---\r\n\r\nThe code to create the queue is as follows, basically it's the same as [in the tutorial](https://www.tensorflow.org/versions/r0.12/how_tos/reading_data/index.html)\r\n\r\n```\r\nfilenames = tf.train.match_filenames_once(filenames_glob)\r\nfilename_queue = tf.train.string_input_producer(\r\n    filenames, num_epochs=num_epochs, shuffle=shuffle)\r\nimage = _read_pngs(filename_queue)\r\n\r\nmin_after_dequeue = 100\r\ncapacity = min_after_dequeue + 1 * batch_size\r\n\r\nreturn tf.train.shuffle_batch(\r\n    [image],\r\n    batch_size=batch_size, capacity=capacity,\r\n    min_after_dequeue=min_after_dequeue,\r\n    seed=666, name=name)\r\n```", "comments": []}, {"number": 6189, "title": "Inconsistent behavior for tf.variable_scope", "body": "#### Problem:\r\nTensorflow doesn't place ops (e.g. `mul`) in pre-existing variable scopes (and automatically creates a new scope instead). \r\n\r\n#### Minimal Reproducible Example\r\n```python\r\nwith tf.variable_scope('layer123'):\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\r\n    w = v * 2\r\nprint(w.name)    # Prints 'layer123/mul:0'\r\n```\r\n\r\nHowever, \r\n```python\r\nwith tf.variable_scope('layer123'):\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\r\n\r\nwith tf.variable_scope('layer123'):\r\n    w = v * 2\r\n\r\nprint(w.name)    # Prints 'layer123_1/mul:0'\r\n```\r\n\r\nObserve that for the latter, the op `w` is placed in a different variable scope, auto-named `layer123_1`.\r\n\r\nI've tried the following, to the same effect:\r\n\r\n```python\r\nwith tf.variable_scope('layer123') as scope:\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\r\n\r\nwith tf.variable_scope(scope):\r\n    w = v * 2\r\n\r\nprint(w.name)    # Prints 'layer123_1/mul:0'\r\n```\r\n\r\n```python\r\nwith tf.variable_scope('layer123'):\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\r\n\r\nwith tf.variable_scope('layer123', reuse=True):\r\n    w = v * 2\r\n\r\nprint(w.name)    # Prints 'layer123_1/mul:0'\r\n```\r\n\r\n#### VersionSpec\r\nTensorflow version: 0.11.0 (GPU)\r\nOS: Ubuntu 14.04 (w/ CUDA 8)", "comments": ["Sorry about this confusion. Your third example at least seems to contradict the[ variable_scope documentation](https://www.tensorflow.org/versions/r0.12/how_tos/variable_scope/index.html): \"When opening a variable scope using a captured object instead of a string, we do not alter the current name scope for ops.\"\r\n\r\n@ebrevdo can you give some guidance?", "No problem! But if this is purely a documentation issue, I'd really appreciate any pointers to a workaround (i.e. to guarantee that `w.name == 'layer123/mul'`). Thank you! ", "I think that [tf.name_scope](https://www.tensorflow.org/versions/r0.12/api_docs/python/framework.html#name_scope) is the right thing to use for naming ops rather than variables, but since I know this has changed over time I defer to @ebrevdo for a definitive answer.", "I believe variable and name scopes are exclusive.  variable_scope and\nname_scope change their own scope hierarchies independently.\n\nOn Thu, Dec 8, 2016 at 10:27 AM, Michael Isard <notifications@github.com>\nwrote:\n\n> I think that tf.name_scope\n> <https://www.tensorflow.org/versions/r0.12/api_docs/python/framework.html#name_scope>\n> is the right thing to use for naming ops rather than variables, but since I\n> know this has changed over time I defer to @ebrevdo\n> <https://github.com/ebrevdo> for a definitive answer.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6189#issuecomment-265815454>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3asKUKZVRqv6gHwD_W-xGm8dl6Pks5rGEv5gaJpZM4LHrZ6>\n> .\n>\n", "I ran another test (on @michaelisard 's advice):\r\n\r\n```python\r\nwith tf.name_scope('layer123') as scope:\r\n    v = tf.Variable(3., name='v')\r\n    \r\nwith tf.name_scope('layer123'):\r\n    w = v * 2\r\n\r\nprint(w.name)    # still prints 'layer123_1/mul:0'\r\n```\r\n\r\nReplacing the first `name_scope` with `variable_scope` and `Variable` with `get_variable` did not change anything. However, I've found this works: \r\n\r\n```python\r\nwith tf.name_scope('layer123') as scope:\r\n    v = tf.Variable(3., name='v')\r\n\r\nwith tf.name_scope(scope):\r\n    w = v * 2\r\n\r\nprint(w.name)    # prints 'layer123/mul:0' like it should\r\n```\r\n\r\nThis is quite unsatisfactory, for obvious reasons (e.g. get_variable functionality is rendered useless, encapsulation is broken, etc.). ", "w is not a variable, it's a Tensor.  w.name gives the Tensor's name, but\nv.name returns the variable's name as you would see in the checkpoint.\n\nindependently, you can do:\n\nwith variable_scope(...) as varscope:\n   create_variables\n\nwith variable_scope(varscope, reuse=True):\n   reuse variables via get_variable\n\n\nOn Thu, Dec 8, 2016 at 10:45 AM, Nasim Rahaman <notifications@github.com>\nwrote:\n\n> I ran another test (on @michaelisard <https://github.com/michaelisard> 's\n> advice):\n>\n> with tf.name_scope('layer123') as scope:\n>     v = tf.Variable(3., name='v')\n>     with tf.name_scope('layer123'):\n>     w = v * 2\n> print(w.name)    # still prints 'layer123_1/mul:0'\n>\n> Replacing the first name_scope with variable_scope and Variable with\n> get_variable did not change anything. However, I've found this works:\n>\n> with tf.name_scope('layer123') as scope:\n>     v = tf.Variable(3., name='v')\n> with tf.name_scope(scope):\n>     w = v * 2\n> print(w.name)    # prints 'layer123/mul:0' like it should\n>\n> This is quite unsatisfactory, for obvious reasons (e.g. get_variable\n> functionality is rendered useless, etc.).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6189#issuecomment-265820123>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxopoVQzOCx_S29w4DO_llkqZJMnks5rGFBQgaJpZM4LHrZ6>\n> .\n>\n", "@ebrevdo I understand that w is a `Tensor` resulting from the `mul` op, and that it's not possible to access it via something like `tf.get_variable('mul')`. I just need the `mul` op to reside in the same \"name-group\" as the variable `v` (such that `w.name == layer123/mul:0`), for prettier Tensorboard visualization. Certainly, this is possible when `w` is defined immediately after `v` without exiting the first `variable_scope`, as shown in the very first example. I was looking for a way for this to work after I had once exited the first `variable_scope`.\r\n\r\nAlso, I've tried the following, which also doesn't work:\r\n\r\n```python\r\nwith tf.variable_scope('layer123') as varscope:\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(3., tf.float32))\r\n\r\nwith tf.variable_scope(varscope, reuse=True):\r\n    w = v * 2\r\n\r\nprint(w.name)    # Prints 'layer123_1/mul:0'\r\n```\r\n\r\nEDIT/CLARIFICATION: I understand that your reply:\r\n> w is not a variable, it's a Tensor.  w.name gives the Tensor's name, but\r\nv.name returns the variable's name as you would see in the checkpoint.\r\n\r\nwas a reply to my statement: \r\n\r\n> This is quite unsatisfactory, for obvious reasons (e.g. get_variable functionality is rendered useless, encapsulation is broken, etc.).\r\n\r\nI was referring to having to use `tf.name_scope` instead of `tf.variable_scope` to have consistent naming behaviour, because the former is ignored by `tf.get_variable`. :)", "Here's a use-case for re-opening variable scopes without renaming.\r\nConsider a OOP design where a class declares a couple of TensorFlow variables and then provides multiple methods on those variables.\r\n\r\n```python\r\nclass Foo(object):\r\n    def __init__(self):\r\n        self.x = tf.get_variable('x', [], initializer=tf.constant(0.0))\r\n\r\n    def meth(self, foo):\r\n        z = tf.multiply(foo, 3.14, name='z')\r\n        self.x.assign(z)\r\n```\r\n\r\nThe easiest to visualize (in TensorBoard) pattern would to be have one variable scope per instance of class and call of method.\r\n```python\r\nf0 = Foo()      # Foo/__init__/x\r\nf1 = Foo()      # Foo_1/__init__/x\r\nf0.meth(1.0)    # Foo/meth/*\r\nf0.meth(2.0)    # Foo/meth_1/*\r\nf1.meth(3.0)    # Foo_1/meth/*\r\n```\r\n\r\nThe way that I thought of to implement this is to create a VariableScope on each instance, and then reopen it for each method call before creating another variable scope for the method. The reopening of the variable scope for each method call without changing the naming is currently not possible, so this scheme doesn't work.", "Thanks @ppwwyyxx . The main issue here that @nasimrahaman and I are looking for is the naming. Additionally, there's no need for the variable scope just to access the variables as the variables themselves could be saved on the instance (no ``get_variable`` call). \r\n\r\nFor my previous example, the suggested scheme would currently give the names:\r\n```python\r\nf0 = Foo()      # Foo/__init__/x\r\nf1 = Foo()      # Foo_1/__init__/x\r\nf0.meth(1.0)    # Foo_2/meth/*\r\nf0.meth(2.0)    # Foo_3/meth/*\r\nf1.meth(3.0)    # Foo_4/meth/*\r\n```\r\nwhich doesn't help at all with distinguishing calls to different instances of ``Foo``.", "@eamartin My bad earlier. I just found you can do something like this: (from #6007)\r\n```python\r\nclass Foo(object):\r\n    def __init__(self):\r\n        with tf.variable_scope(None, 'Foo'):\r\n            self.sc = tf.get_variable_scope()\r\n            self.x = tf.get_variable('x', initializer=0.0)\r\n            print(self.x)\r\n\r\n    def meth(self, foo):\r\n        with tf.name_scope(self.sc.original_name_scope):\r\n            z = tf.multiply(foo, 3.14, name='z')\r\n            print(z)\r\n\r\nf0 = Foo()      # Foo/x\r\nf1 = Foo()      # Foo_1/x\r\nf0.meth(1.0)    # Foo/z\r\nf0.meth(2.0)    # Foo/z_1\r\nf1.meth(3.0)    # Foo_1/z\r\n```\r\nMaybe this will help?\r\n", "Thanks @ppwwyyxx , this works for my OOP use-case. I wasn't aware of ``original_name_scope``, this could use some docs!\r\n\r\nOne tricky case with this pattern: If you want to re-open the scope and create new variables, how do you give the appropriate names to the created ops in the re-opened scope? Here's the workaround:\r\n\r\nFirst attempt:\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope('foo') as scope:\r\n    x = tf.get_variable('x', initializer=1.0)\r\n    x2 = tf.multiply(x, 2, name='x2')\r\n\r\nwith tf.variable_scope(scope):\r\n    y = tf.get_variable('y', initializer=2.0)\r\n    y2 = tf.multiply(y, 2, name='y2')\r\n\r\nprint(x, x2, y, y2)\r\n```\r\noutputs ``Tensor(\"foo/x/read:0\", shape=(), dtype=float32) Tensor(\"foo/x2:0\", shape=(), dtype=float32) Tensor(\"foo/y/read:0\", shape=(), dtype=float32) Tensor(\"foo_1/y2:0\", shape=(), dtype=float32)``. This is wrong for ``foo_1/y2``.\r\n\r\nNext (ugly approach): Add a ``with tf.name_scope(scope.original_name_scope):`` block inside of the ``with tf.variable_scope(scope):`` block. This leads to correct output ``Tensor(\"foo/x/read:0\", shape=(), dtype=float32) Tensor(\"foo/x2:0\", shape=(), dtype=float32) Tensor(\"foo/y/read:0\", shape=(), dtype=float32) Tensor(\"foo/y2:0\", shape=(), dtype=float32)``\r\n\r\n", "@eamartin Yes that's a bit tricky. I hope tensorflow can change the default behavior, i.e. when you reenter a variable scope, you should expect to automatically reenter its name scope as well instead of creating a new one.", "Hi everyone. I am having this problem too. Is there any method to have a consistent variable scope name and the variable names now? I am worry about that Foo/x and Foo_1/x are two independent variables (please correct me if I am wrong). Thank you. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "https://github.com/tensorflow/tensorflow/pull/14390 now supports opening a variable scope without a new name scope already. I think this issue is resolved.", "Great!", "Hi @ebrevdo @ppwwyyxx @michaelisard , I think I am facing a similar problem but not sure about it. My TensorFlow version is 1.2.1.\r\n\r\nI am trying to finetune [resnet_50](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/resnet_v1.py) from `tensorflow.slim` package on my own data. \r\n\r\nTo have the training and validation cycles together I use,\r\n\r\n```\r\n        with slim.arg_scope(resnet_arg_scope()):\r\n            self.network_logits,_     = resnet_v1_50(inputs=inputs) \r\n            self.network_logits_val,_ = resnet_v1_50(inputs=inputs, reuse=True, is_training=False) ## Define a new function for testing but reuse the same model variables\r\n```\r\n\r\nNote, that my intension is to reuse the variables in the trained model for validation. However, on my TensorBoard I get two seperate `resnet_50_v1` and  `resnet_50_v1_1` blocks.\r\n\r\n![ full graph ](https://user-images.githubusercontent.com/34502974/34751909-4907825e-f603-11e7-9282-5d9be078ee26.png)\r\n\r\nHowever, when I zoomed in to the block `resnet_50_v1_1` (The block used in the validation cycle), I only see a set of operators(i.e. all the variables are passed from `resnet_50_v1`).\r\n\r\n![zoomed_1_1](https://user-images.githubusercontent.com/34502974/34752040-17de5ecc-f604-11e7-8a17-3b29039ee4bd.jpg)\r\n\r\nThis is how a similar block looks inside the `resnet_50_v1` (The block used in the training cycle),\r\n\r\n![train zoomed cropped](https://user-images.githubusercontent.com/34502974/34752279-5a4aff58-f605-11e7-963c-d285628de0b8.jpg)\r\n\r\nI think this sperate blocks appear due to the discussed `name_scope` issue . If that is the case is my code technically safe? To me this is just a problem of easthetic appearance on tensorboard. Please correct me if I am wrong.\r\n\r\nCould you please help me clarify why I am getting two distinct blocks `resnet_50_v1` and `resnet_50_v1_1`? Is my code technically safe from what it appears? Thanks in Advance!\r\n\r\n\r\n\r\n\r\n\r\n  \r\n  "]}, {"number": 6188, "title": "NotFoundError: ..._cudnn_rnn_ops.so [0.12.0]", "body": "no luck with new build so far (running linux-64, conda 4.2.13, Python3.5)\r\n\r\n`conda install -c jjhelmus tensorflow=0.12.0rc0 `\r\n\r\ntrying to run a script I get:\r\n```\r\nNotFoundError: /home/miriam/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/_cudnn_rnn_ops.so: cannot open shared object file: No such file or directory\r\n```", "comments": ["@meereeum I also face similar issue. Thus, a quick fix is to use tensorflow version  0.11, while the issue is being fixed.", "@kmario23 yep - that's my strategy at the moment. the new embedding projector is really great though !", "@meereeum Could you provide a minimal example?\r\nMaybe a subset of the script you are running?\r\n\r\nIf I can reproduce the problem, I might be able to help.\r\nAs our tests do not show that there is a problem, I am not sure if this is due to your installation or something in TF.", "@gunan \r\n\r\n```\r\nimport tensorflow as tf\r\ntf.contrib.layers.optimize_loss\r\n```\r\n\r\n(not even creating the tensorflow op, just referring to it)\r\n\r\n```\r\n---------------------------------------------------------------------\r\nNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-2-00e1e97a2d9c> in <module>()\r\n----> 1 tf.contrib.layers.optimize_loss\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/__init__.py in __getattr__(self, item)\r\n     33     # Replace the lazy loader with the imported module itself.\r\n     34     import importlib  # pylint: disable=g-import-not-at-top\r\n---> 35     contrib = importlib.import_module('tensorflow.contrib')\r\n     36     return getattr(contrib, item)\r\n     37 \r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/importlib/__init__.py in import_module(name, package)\r\n    124                 break\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n    128 \r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/importlib/_bootstrap.py in _gcd_import(name, package, level)\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/importlib/_bootstrap.py in _find_and_load(name, import_)\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/importlib/_bootstrap.py in _load_unlocked(spec)\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/importlib/_bootstrap_external.py in exec_module(self, module)\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/__init__.py in <module>()\r\n     23 from tensorflow.contrib import copy_graph\r\n     24 from tensorflow.contrib import crf\r\n---> 25 from tensorflow.contrib import cudnn_rnn\r\n     26 from tensorflow.contrib import distributions\r\n     27 from tensorflow.contrib import factorization\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/cudnn_rnn/__init__.py in <module>()\r\n     19 from __future__ import print_function\r\n     20 \r\n---> 21 from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnGRU\r\n     22 from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnLSTM\r\n     23 from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnRNNRelu\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py in <module>()\r\n     26 \r\n     27 _cudnn_rnn_ops_so = loader.load_op_library(\r\n---> 28     resource_loader.get_path_to_datafile(\"_cudnn_rnn_ops.so\"))\r\n     29 \r\n     30 _cudnn_rnn_common_doc_string = \"\"\"\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/util/loader.py in load_op_library(path)\r\n     40   if os.name != 'nt':\r\n     41     path = resource_loader.get_path_to_datafile(path)\r\n---> 42     ret = load_library.load_op_library(path)\r\n     43     assert ret, 'Could not load %s' % path\r\n     44     return ret\r\n\r\n/home/miriam/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py in load_op_library(library_filename)\r\n     62       # pylint: disable=protected-access\r\n     63       raise errors_impl._make_specific_exception(\r\n---> 64           None, None, error_msg, error_code)\r\n     65       # pylint: enable=protected-access\r\n     66   finally:\r\n\r\nNotFoundError: /home/miriam/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/_cudnn_rnn_ops.so: cannot open shared object file: No such file or directory\r\n```", "Hmm, I still cannot reproduce the issue.\r\nWhich OS are you using? Anything outside ubuntu, you will probably need to rebuild tensorflow from source.\r\n\r\nAlso, could you print the contents of the folder `/home/miriam/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/`\r\n\r\nAnd finally, could you reinstall the rc1, and see if the issue is resolved in that version?", "Hello,\r\n  In r0.11, the contents of the folder ( ..../cudnn_rnn/python/ops/) is: \r\n __init__.py\r\ncudnn_rnn_ops.py\r\n__init__.pyc\r\ncudnn_rnn_ops.pyc\r\n_cudnn_rnn_ops.so\r\n__pycache__\r\n\r\nJust wondering, could this be caused because of using anaconda2 but Python3.x & tf 0.12? But, I also face similar issue with anaconda3, Python 3.5, & tf 0.12 as well.", "Not sure if this can be caused by anaconda vs python version mismatch. But to me more and more this looks like an anaconda bug, as with python (native) python(virtualenv) it all works fine.\r\n\r\nThe only thing I am not sure is, your OS.\r\nI can see that `_cudnn_rnn_ops.so` is there for you, so it should be accessible. The only reason it wont would be permissions or your OS not being able to read it, but I have not come accross that before.", "I'm using debian (jessie), but have had no problems with tensorflow packages built from this anaconda contributor before...\r\n\r\nin contrast to above, I see only\r\n```\r\ncudnn_rnn_ops.py\r\n__init__.py\r\n__pycache__\r\n```\r\n\r\nso am lacking the `_cudnn_rnn_ops.so` file indicated by the error", "aha, definitely something with the anaconda build (which is unfortunate), as a `pip install` (with PYTHONPATH pointing to conda python) seems to work", "Thanks for verifying.\r\nI will then close this issue as it seems to be an anaconda problem.\r\nIn the meantime, we will consider removing anaconda instructions."]}, {"number": 6187, "title": "Run GPU tests on Windows with Bazel", "body": "After this change, we can run:\r\n\r\nCPU build + python tests: `tensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh`\r\nCPU C++ tests: `tensorflow/tools/ci_build/windows/cpu/bazel/run_cc_test_windows.sh`\r\nGPU build + python tests: `tensorflow/tools/ci_build/windows/gpu/pip/build_tf_windows.sh`\r\nGPU C++ tests: `tensorflow/tools/ci_build/windows/gpu/bazel/run_cc_test_windows.sh`", "comments": ["I refactored all the scripts, please take a look again. :)"]}, {"number": 6186, "title": "tf.split_v results in unknown shape size", "body": "### Environment info\r\nOperating System:\r\nLinux Mint 17.3 3.19.0-32-generic\r\nTensorflow Version 0.12.0-rc0 (https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc0-cp34-cp34m-linux_x86_64.whl)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n`\r\ntf.split_v(tf.placeholder(tf.float32, shape=[10, 32, 32, 3]), [3,7], 0)\r\n`\r\nThe result is of shape (?, ?, ?, ?). \r\n\r\nHowever, when using split for equal length splits it shows the resulting shape:\r\n`\r\ntf.split(0, 2, tf.placeholder(tf.float32, shape=[10, 32, 32, 3]))\r\n`\r\n\r\n", "comments": ["The shape inference for SplitV is in [array_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/array_ops.cc#L421) and according to the comment it assumes that the value of size_splits is not known at shape inference time, and hence returns unknown shape as you report. I suspect that you could add logic like that in e.g. [image_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc#L39) which checks to see if the input value is known at inference time, and reports the correct shape if so.\r\n\r\nI am marking contributions welcome: please feel free to submit a PR with this change and reference this issue.", "This is a duplicate of https://github.com/tensorflow/tensorflow/issues/5977.\r\n\r\nNote that the code in question does not just _assume_ the value is not known at shape inference time, it is in fact impossible for the C++ shape inference code to inspect the contents of Tensors during shape inference.  That the Tensor was a constant in the python code is lost by the time it gets to the shape inference.  We can hack this by setting the output shapes in the python as well.", "@ekelsen: that's actually not true anymore -- if the tensor is a constant in the graph, the shape inference code can actually ask for the values in the shape function, and output a more specific shape if it is available.\r\n\r\nExample: https://github.com/tensorflow/tensorflow/blob/4172ca2cd7ac34be67bda2600944d284e8907b95/tensorflow/core/framework/common_shape_fns.cc#L598\r\n\r\n", "Well in that case we can fix it in the C++ code :) thanks for the heads up @vrv.", "I have opened a PR for this where I have fix this using C++ code. Should I reference #5977 in that PR too?", "Yes, please do.  Thanks for the contribution!"]}, {"number": 6185, "title": "Add python server stop method", "body": "Add support for graceful server stop in python.", "comments": ["Can one of the admins verify this patch?", "Unfortunately this doesn't actually add the support, since the [implementation](https://github.com/tensorflow/tensorflow/blob/a1d06460f56d7b2f5b7a8317c1aeba6a587b104f/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L224) of `GrpcServer::Stop()` is incomplete. We wouldn't be able to accept this PR until this has been implemented and tested. If I recall correctly, the current shutdown code leaks in-flight buffers, so we would need additional tracking code in the gRPC services to make this work cleanly.\r\n\r\nWould you be keen to work on that?", "@mrry Sure, if I can help. Any contexts about the in-flight buffers?", "IIRC the problem is that the [`Call`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_call.h#L134) objects used in `GrpcMasterService` and `GrpcWorkerService` have pointers to the underlying gRPC service objects in their `responder_` field. So you need some way to ensure that all of the pending requests have completed/been cancelled before deleting the server itself.\r\n\r\nAt the time I implemented this, it didn't seem worth the additional complexity to support clean shutdown (which was not a feature anybody was using at the time...). I'd be delighted if there's a simple solution here.", "IIRC the problem is that the `Call` objects in the servers have non-owned pointers to the service, both directly and through the `responder_` object. A clean shutdown would need some way to wait until all of the pending calls had completed or been cancelled. At the time I implemented this, it didn't seem worth the additional bookkeeping to support this feature (and nobody had explicitly requested it). It would be great if you could come up with a simple solution here!", "Got it. Yeah, this is not a high priority issue, it's not quite useful in production. May be we can just add an TODO tag in the source:).\r\n\r\nAny way, I will have a look about gRPC buffer stuff if no googlers will fix https://github.com/grpc/grpc/issues/8975 because it's a real blocker to us.", "So, @llhe, should I close this PR or did you want to fix the issue?", "Close it first, I can reopen it when I have free cycles.", "Is it implemented?\r\n"]}, {"number": 6184, "title": "wide_n_deep Tutorial example not working", "body": "Windows 10 - 0.12.0-rc0\r\n\r\nEncountered error \"AttributeError: 'NoneType' object has no attribute 'bucketize'\" when running the example without any modification.\r\n\r\nFull working and error log as follows:\r\n\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:From C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py:711 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py:711 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py:711 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nTraceback (most recent call last):\r\n  File \"wide_n_deep_tutorial.py\", line 207, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"wide_n_deep_tutorial.py\", line 203, in main\r\n    train_and_eval()\r\n  File \"wide_n_deep_tutorial.py\", line 196, in train_and_eval\r\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py\", line 711, in fit\r\n    max_steps=max_steps)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 191, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 355, in fit\r\n    max_steps=max_steps)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 699, in _train_model\r\n    train_ops = self._get_train_ops(features, labels)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1052, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1019, in _call_model_fn\r\n    params=self.params)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py\", line 504, in _dnn_linear_combined_model_fn\r\n    scope=scope)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\", line 526, in weighted_sum_from_feature_columns\r\n    transformed_tensor = transformer.transform(column)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\", line 869, in transform\r\n    feature_column.insert_transformed_feature(self._columns_to_tensors)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column.py\", line 1489, in insert_transformed_feature\r\n    name=\"bucketize\")\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\ops\\bucketization_op.py\", line 48, in bucketize\r\n    return _bucketization_op.bucketize(input_tensor, boundaries, name=name)\r\nAttributeError: 'NoneType' object has no attribute 'bucketize'\r\n\r\n", "comments": ["I have traced back the error to \"_bucketization_op.py\".  There is a line of code that says \r\n\"_bucketization_op = loader.load_op_library(\r\n    resource_loader.get_path_to_datafile(\"_bucketization_op.so\"))\"\r\n\r\nI could not find the file \"_bucketization_op.so\" in my TensorFlow installation for Windows 10.  Is \"_bucketization_op.so\" supposed to be in the installation directory?\r\n\r\nThank you.", "I have a similar issue. My installation is tensorflow version 0.12.0rc1, on Python 3.5.2 :: Anaconda 4.2.0 (64-bit) on Windows 10. \r\n```\r\n`File \"C:\\Users\\Prodipta\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\ops\\bucketization_op.py\", line 48, in bucketize\r\n    return _bucketization_op.bucketize(input_tensor, boundaries, name=name)\r\nAttributeError: 'NoneType' object has no attribute 'bucketize'`\r\n```\r\nThe function call (in the bucketization_op.py) get_path_to_datafile(\"_bucketization_op.so\") just returns \"_bucketization_op.so\" (object type str). The file itself does not seem to exist.\r\n\r\nThe code runs fine removing the bucketization and the column crossing. (Also, I had to make some changes on the input_fn definition from dense_shape=[df[k].size, 1] to just shape=[df[k].size, 1])\r\n", "I'm having a similar issue.  Tensorflow 0.12.0rc1, Python 3.5.2, Windows 10\r\n\r\n`WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:From C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py:711 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py:711 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py:711 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 578, in merge_with\r\n    self.assert_same_rank(other)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 624, in assert_same_rank\r\n    \"Shapes %s and %s must have the same rank\" % (self, other))\r\nValueError: Shapes (0,) and (?, ?) must have the same rank`", "On windows load_op_library doesn't have any action, just return None\r\n(https://github.com/tensorflow/tensorflow/blob/5a5a25ea3ebef623e07fb9a46419a9df377a37a5/tensorflow/contrib/util/loader.py)\r\n\r\nI found the message from this url https://github.com/tensorflow/tensorflow/commit/45c838623c0df0489e1777af494ee4da0cf4e435\r\n\r\n> In tf.contrib, only load external op libraries on **non-Windows** platforms.\r\n> This enables tf.contrib to be used on Windows, which does not currently\r\n>support the TensorFlow plugin mechanism.\r\n\r\n", "Side question: any idea what's causing the warning: \r\n**WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.**\r\n\r\nIs there a way to prevent this and/or can it be ignored?", "For someone who is searching as I was...\r\n\r\nThe [tutorial](https://www.tensorflow.org/tutorials/wide/) has two issues (at least on windows installation):\r\n1. TypeError: __init__() got an unexpected keyword argument 'density_shape'\r\n2. AttributeError: 'NoneType' object has no attribute 'bucketize'\r\n\r\n**Temporary SOLUTION as follows:**\r\n\r\n**Issue 1 - temp. solution**\r\nChange \"density_shape\" in following code:\r\n`  categorical_cols = {\r\n      k: tf.SparseTensor(\r\n          indices=[[i, 0] for i in range(df[k].size)],\r\n          values=df[k].values,\r\n          density_shape=[df[k].size, 1])\r\n      for k in CATEGORICAL_COLUMNS}`\r\nTo just \"shape\":\r\n`  categorical_cols = {\r\n      k: tf.SparseTensor(\r\n          indices=[[i, 0] for i in range(df[k].size)],\r\n          values=df[k].values,\r\n          shape=[df[k].size, 1])\r\n      for k in CATEGORICAL_COLUMNS}`\r\n\r\n**Issue 2 - temp. solution**\r\nChange model type from \"wide\" or \"wide_n_deep\" to \"deep\"\r\n\r\n\r\n@caisq \r\nWould be great if someone from responsible persons could comment the problems. \r\nIs it just tutorial issue? Is it windows installation issue? **Doe's it mean, this is general issue, therefore \"wide_n_deep\" model is unusable?** \r\n\r\nThank you for your support", "As I see it from my attempts, it is an windows version issue - they do not ship the DLLs for some shared libraries like they do the SOs for the Linux version. Better switch to the Linux version, it appears to work just fine.", "@mcnarik I experience your problem 1 as well and I'm using linux, the parameter name on linux is **dense_shape** and should be changed to **shape**. I guess they forget to update the tutorial.", "to be clear, the issues with windows is on the point 2, bucketize has no definition, the shared objects missing in windows.", "@mcnarik @shadofren I believe \"problem 1\" is caused by the v1.0.0-alpha release (https://github.com/tensorflow/tensorflow/releases/tag/v1.0.0-alpha), in which they also changed the wide and deep tutorial to match the change of `SparseTensor`'s argument from `shape` to `dense_shape` (see 056c0877ad).\r\n\r\nThe wide and deep tutorial code in branch `r0.12` (https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/examples/learn/wide_n_deep_tutorial.py) runs on my machine (with Mac OS X 10.11.5, Python 2.7.12, and Tensorflow 0.12.1) - worth having a look!\r\n\r\nWill leave problem 2 for those with better understanding.", "@liuchbryan \r\nThe **point 1** is clear but it makes sense to write it here together with solution because official tutorial is  wrong and I spend a lot of time just searching for solution. \r\n\r\nRegarding **point 2**. I know it works on different systems (Linux, Mac OS X) and I also test the example you shared. However from my point of view, installing a different system is not solution. I would like to use Windows because I know them much better and therefore can focus on neural networks and not on how to install and work with Linux. Since the tool should support Windows, I think this is valid expectation. \r\nAs mentioned by **prodipta** the issue on windows is in TensorFlow packages/classes for windows and should be therefore fixed (soon or later). But maybe I'm wrong...\r\n\r\nAnyway, I really appriciate work of the developers and love the tool!", "@mcnarik \r\nApologies if I did not made clear in my previous comment - the entire comment (bar from the last line) was solely dealing with issue/problem 1 you have mentioned. That is, you can also run the wide and deep tutorial code which lives in branch `r0.12` if you only have access to the older Tensorflow 0.12, which should run without any error. On hindsight, we are essentially suggesting the same fix, and it is absolutely the right thing to do to suggest that temporary fix on here. \r\n\r\nI am inclined not to say the official tutorial (here defined as: `wide_n_deep_tutorial.py` hosted on GitHub) is \"wrong\" - while it may be too ahead in time to be actually useful, it is consistent with Tensorflow's latest version (v1.0.0-alpha). On the other hand, I think we will agree the code shown in [this blog post](https://www.tensorflow.org/tutorials/wide_and_deep/) might need some verbose warning for the time being, and some update on the use of `shape`/`dense_shape` argument keyword in the future.\r\n\r\nI totally agree your comment on the ability to use Tensorflow on Windows - it will be a shame if half the community is denied use simply because some libraries are missing!\r\n\r\nSorry for getting off-tangent, will leave the thread for the actual working for issue/problem 2.", "I got the same problem and doing two changes \"dense_shape\" to \"shape\" and \"wide_n_deep\" to \"deep\", it worked. However I received a bunch of warnings. Is it due to windows or I need to update my system? ", "Is there an update on whether this will be addressed, so the wide & deep model can be run on Windows?\r\n\r\nThe issue still replicates on Windows 10 (64bit), Anaconda 4.3.8 virtual environment with Python 3.5.3 and TensorFlow 1.0.0.\r\n\r\nRunning with the `deep` instead of the `wide_n_deep` flag, does not actually run the model of interest based on `tf.contrib.learn.DNNLinearCombinedClassifier`.\r\n\r\nThx in advance.\r\n\r\n", "@tomwanzek,  I followed the build for   https://github.com/tensorflow/tensorflow/issues/8217   but It can not solve the issue ", "I'm also having the same problem...Any updates on this?", "I just pulled down the last stable nightly build for Windows/GPU (114) (the later builds up to 122 have failed for GPU) and the Windows/CPU build (122). I can confirm that, the original issue still persists.\r\n\r\n@mrry Thanks for all the hard work on TF \ud83d\udcaf Are you in a position to pin down, what remains to be done  assuming that the now merged  PR #8217 was one of the missing building blocks to a resolution?", "@tomwanzek I think the changes are fairly modest. There are two options:\r\n\r\n1. (More principled.) Convert the Wide&Deep-related ops to using @guschmue's newly-added `tf.load_library()` support, which will require some additions to the CMake build, similar to the ones Guenther did for `tf.contrib.rnn` in #8217.\r\n\r\n2. (Easier.) Convert the Python code in the Wide&Deep libraries to use statically generated Python wrappers, rather than getting them from the return value of `tf.load_op_library()`. We've already done this for several libraries (e.g. `tf.contrib.tensor_forest` in #6908).", "(EDIT: Interestingly, an internal change is pending to do option 2, so it might be easier to wait a couple days and the HEAD should just work.)", "@mrry sounds great! Whatever option is more appropriate in light of the TF Roadmap...your call \ud83d\udc4d \r\n\r\nOnly speaking for myself here: I'd rather wait a little longer and have a solution that is robust and consistent (beyond just wide & deep), than have a quick fix which outlives itself soon and implies different cognitive models for implementation of different models. Thx in advance.", "@tomwanzek The change I mentioned is now in, and I just sent a [pull request](https://github.com/tensorflow/tensorflow/pull/8808) to cherry-pick it into the 1.1 release. Therefore, if I'm not mistaken, you should be able to upgrade to a current nightly build or the *next* 1.1 release candidate (which should be out soon), and these ops will now work.\r\n\r\n(Coincidentally, I just posted an [answer on Stack Overflow that explains the whole sorry story](http://stackoverflow.com/a/43097772/3574081), in case you're interested!)", "@mrry Sounds great!!! I will give it a shot shortly as soon as they become available. I saw nightly 128 for Windows 10/GPU now references 1.1.0RC0. So I will give it a shot, although, based on you comment timing it might have to wait for the next stable nightly.\r\n\r\nIn any case, thanks a lot to yourself, @guschmue and everyone who chipped in on the windows load PR \ud83d\udc83 ", "Just to confirm, I created a conda environment based on Windows/GPU Stable Build129 (1.1.0rc0 nightly) and the primary errors are indeed gone.\r\n\r\nThere are some remaining warnings  related to deprecated API usage, but overall the `wide` and `wide_n_deep` models are runnable at this point!\r\n\r\nSo one more time: Cudos to @mrry and @guschmue .", "Thanks for confirming this, Tom!"]}, {"number": 6183, "title": "R0.12", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6182, "title": "SummaryWriter.add_summary() gives error AttributeError: 'FileWriter' object has no attribute 'add_sumary'", "body": "I literally copied and pasted an example from the tensor board training site and it doesn't run.\r\n\r\nsummary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True)) train_writer.add_summary(summary, i)\r\n\r\nBoth these lines give an error.\r\napparently sess.run returns a None type.", "comments": ["Session.run should return a list of tensors in this case, see the [Session.run documentation](https://www.tensorflow.org/versions/r0.12/api_docs/python/client.html#Session). I recommend posting a more detailed description of the problem you are seeing on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) which is the right venue for usage questions."]}, {"number": 6181, "title": "Quantization code in different path to where specified in Help Documentation", "body": "Following the documentation here to Quantize - https://www.tensorflow.org/versions/r0.12/how_tos/quantization/index.html\r\n\r\nseems to be wrong.\r\n\r\nI had to change  \r\n`bazel build tensorflow/contrib/quantization/tools:quantize_graph\r\nbazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \\\r\n--input=/tmp/classify_image_graph_def.pb \\\r\n--output_node_names=\"softmax\" --output=/tmp/quantized_graph.pb \\\r\n--mode=eightbit`\r\nTO\r\n`bazel build tensorflow/tools/quantization/quantize_graph\r\nbazel-bin/tensorflow/tools/quantization/quantize_graph \\\r\n--input=/tmp/classify_image_graph_def.pb \\\r\n--output_node_names=\"softmax\" --output=/tmp/quantized_graph.pb \\\r\n--mode=eightbit\r\n`\r\nie it seems like it's moved from tensorflow/contrib to tensorflow/tools/\r\n", "comments": ["Thanks! Would you submit a PR with the documentation fix and reference this issue?", "Just sent out a PR so this will be fixed shortly."]}, {"number": 6180, "title": "How to create a 5D tensor as the input of a 3DCNN", "body": "I try to create a 4D tensor as input of a 2DCNN. I am confused about how to create a 5DCNN .Is the 4D tensor as one of dimension of a 5D tensor? Is there someone who know the detials ?\r\n", "comments": ["This question is better suited to [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as it is a usage question not a bug report."]}, {"number": 6179, "title": "How to localize the position of object by using tensorflow", "body": "The ILSVRC competition held by imagenet includes Object localization and Object detection and other tasks.\r\n\r\nHow did the convolutional neural network solve the localization problem? \r\nI can only find tutorials on classification task.\r\n\r\nI think it is proper to add an example of localization in tensorflow tutorial.", "comments": ["This question is better suited to [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is a usage question not a bug report."]}, {"number": 6178, "title": "Partial Revert \"Migrate tf summary ops to use tf.contrib.deprecated endpoints.", "body": "This partially reverts commit c532a5b558a451d599190f1dbbdf68f08dfcaa88.\r\n\r\nThis should fix the pull-requests-cpu test failures.\r\n\r\n@danmane FYI, ", "comments": ["Looks like this did not work. Closing this PR."]}, {"number": 6177, "title": "[Compression]Compression issues of File isn't open for reading with r0.12", "body": "I have met a problem with the model of  Compression again.\r\n\r\n[https://github.com/tensorflow/models/tree/master/compression](url)\r\nI use the OS of Ubuntu 14.04 LTS and the version of Tensorflow is r0.12\r\n\r\nThe error print as:\r\nTraceback (most recent call last):\r\n  File \"encoder.py\", line 103, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"encoder.py\", line 99, in main\r\n    np.savez_compressed(code_file, shape=int_codes.shape, codes=export)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.py\", line 600, in savez_compressed\r\n    _savez(file, args, kwds, True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.py\", line 642, in _savez\r\n    zipf.write(tmpfile, arcname=fname)\r\n  File \"/usr/lib/python2.7/zipfile.py\", line 1139, in write\r\n    zinfo.header_offset = self.fp.tell()    # Start of header bytes\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 141, in tell\r\n    \"File isn't open for reading\")\r\ntensorflow.python.framework.errors_impl.PermissionDeniedError: File isn't open for reading\r\nException tensorflow.python.framework.errors_impl.PermissionDeniedError: PermissionDeniedError() in <bound method ZipFile.__del__ of <zipfile.ZipFile object at 0x7f368ed35a50>> ignored\r\n", "comments": ["Find the solution at [https://github.com/tensorflow/models/issues/500](url)", "Is that solution working? Can I close the issue?", "Closing due to inactivity.\r\nThe link you have seems to be broken, and issue number 500 in models repo seems to be unrelated.\r\nFor posterity, could you repaste which issue/pr you think resolves the problem?"]}, {"number": 6176, "title": "Disable all tests depending on *half_plus_two* genrules.", "body": "This is a measure to stabilize flaky GPU builds.\r\n\r\nThis is a fix for the CUDA out of memory error issues we have been seeing.", "comments": ["The CPU failures are known issues.\r\nSo this should fix the GPU issues."]}]