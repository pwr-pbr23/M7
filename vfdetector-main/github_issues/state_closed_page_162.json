[{"number": 49941, "title": "Modify tf.math.reduce_std so that it is compatible with ragged tensors", "body": "Same issue as `reduce_variance` addressed in #37000 for `reduce_std`\r\n", "comments": ["Please close this issue once related PR is merged.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49941\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49941\">No</a>\n"]}, {"number": 49939, "title": "Cannot convert a Tensor of dtype resource to a NumPy array", "body": "Having issue in converting autokeras functional model to onnx \r\nUsing tensorflow == 2.3.1 keras == 2.4.3\r\n```\r\nfrom autokeras import StructuredDataClassifier\r\nmodel = StructuredDataClassifier(max_trials=100)\r\nmodel.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), epochs=1000, verbose=1)\r\nautoKeras_model = model.export_model()\r\nautoKeras_model.summary()\r\n\r\nModel: \"functional_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 18)]              0         \r\n_________________________________________________________________\r\nmulti_category_encoding (Mul (None, 18)                0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 32)                608       \r\n_________________________________________________________________\r\nre_lu (ReLU)                 (None, 32)                0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 32)                1056      \r\n_________________________________________________________________\r\nre_lu_1 (ReLU)               (None, 32)                0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 1)                 33        \r\n_________________________________________________________________\r\nclassification_head_2 (Activ (None, 1)                 0         \r\n=================================================================\r\nTotal params: 1,697\r\nTrainable params: 1,697\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\nconverting to onnx\r\n```\r\nimport onnxruntime\r\nimport keras2onnx\r\nonnx_model = keras2onnx.convert_keras(autoKeras_model, \"autokeras\", debug_mode=1)\r\n```\r\nI get error\r\n```\r\ntf executing eager_mode: True\r\nINFO:keras2onnx:tf executing eager_mode: True\r\ntf.keras model eager_mode: False\r\nINFO:keras2onnx:tf.keras model eager_mode: False\r\nModel: \"functional_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 18)]              0         \r\n_________________________________________________________________\r\nmulti_category_encoding (Mul (None, 18)                0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 32)                608       \r\n_________________________________________________________________\r\nre_lu (ReLU)                 (None, 32)                0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 32)                1056      \r\n_________________________________________________________________\r\nre_lu_1 (ReLU)               (None, 32)                0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 1)                 33        \r\n_________________________________________________________________\r\nclassification_head_2 (Activ (None, 1)                 0         \r\n=================================================================\r\nTotal params: 1,697\r\nTrainable params: 1,697\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-76-0567e6de6858>\", line 1, in <module>\r\n    onnx_model = keras2onnx.convert_keras(ExportedautoKeras_model, model_name, debug_mode=1)\r\n\r\n  File \"C:\\Users\\Pe\\Anaconda3\\lib\\site-packages\\keras2onnx\\main.py\", line 62, in convert_keras\r\n    tf_graph = build_layer_output_from_model(model, output_dict, input_names, output_names)\r\n\r\n  File \"C:\\Users\\Pe\\Anaconda3\\lib\\site-packages\\keras2onnx\\_parser_tf.py\", line 302, in build_layer_output_from_model\r\n    return extract_outputs_from_subclassing_model(model, output_dict, input_names, output_names)\r\n\r\n  File \"C:\\Users\\Pe\\Anaconda3\\lib\\site-packages\\keras2onnx\\_parser_tf.py\", line 264, in extract_outputs_from_subclassing_model\r\n    concrete_func, lower_control_flow=True)\r\n\r\n  File \"C:\\Users\\Pe\\Anaconda3\\lib\\site-packages\\keras2onnx\\_graph_cvt.py\", line 437, in convert_variables_to_constants_v2\r\n    tensor_data = _get_tensor_data(func)\r\n\r\n  File \"C:\\Users\\Pe\\Anaconda3\\lib\\site-packages\\keras2onnx\\_graph_cvt.py\", line 209, in _get_tensor_data\r\n    data = val_tensor.numpy()\r\n\r\n  File \"C:\\Users\\Pe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1063, in numpy\r\n    maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Users\\Pe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1031, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nInvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n```\r\ntf life also gives same error\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(autoKeras_model)\r\ntflite_model = converter.convert()\r\n```\r\nAny help much appreciated, Thanks\r\n", "comments": ["Resource types are supported through only saved model converter since TF 2.5.", "So I have to save to disk then retrieve is it? In docker this will be a issue.", "Sorry for  inconvenience. Unfortunately, we currently have this only working option.", "tf==2.5 and python==3.9.5 fixed this issue, it worked, Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49939\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49939\">No</a>\n"]}, {"number": 49938, "title": "Want to save this training weight by epoch", "body": "https://github.com/lancerane/Adversarial-domain-adaptation/blob/master/Domain%20adaptation,%20TF2.0.ipynb\r\n\r\nThanks Oscar Bennett provide this code, but i don't know how to save this weight then test another data.\r\nAnd if can train supervise val loss  to learn avoid overfitting.\r\nBecause this model DANN have three part and i can't use keras save weight, Can u give me some direction?\r\n---------------\r\nEPOCHS = 100\r\n\r\nalpha = 1\r\nfor epoch in range(EPOCHS):\r\n  reset_metrics()\r\n  \r\n  for domain_data, label_data in zip(domain_train_ds, train_ds):\r\n    \r\n    try:\r\n      train_step(label_data[0], label_data[1], domain_data[0], domain_data[1], alpha=alpha)\r\n     \r\n    #End of the smaller dataset\r\n    except ValueError: \r\n      pass\r\n    \r\n  for test_data, m_test_data in zip(test_ds,mnist_m_test_ds):\r\n    test_step(test_data[0], test_data[1], m_test_data[0], m_test_data[1])\r\n  \r\n  template = 'Epoch {}, Train Accuracy: {}, Domain Accuracy: {}, Source Test Accuracy: {}, Target Test Accuracy: {}'\r\n  print (template.format(epoch+1,\r\n                         train_accuracy.result()*100,\r\n                         conf_train_accuracy.result()*100,\r\n                         test_accuracy.result()*100,\r\n                         m_test_accuracy.result()*100,))\r\n-----------------------------", "comments": ["https://core.ac.uk/download/pdf/228291159.pdf P41\r\nhave a way but is keras save weight,\r\ni don't know how to do save weight  tensorflow.", "@BeccaHuang ,\r\n\r\nPlease take a look at this links for information on keras.[Link1](https://stackoverflow.com/questions/47266383/save-and-load-weights-in-keras),[Link2](https://stackoverflow.com/questions/57152978/keras-how-to-save-models-or-weights),[Link3](https://www.tensorflow.org/guide/keras/save_and_serialize).\r\n\r\nThanks!\r\n\r\n", "@BeccaHuang ,\r\n\r\nPlease post this question in [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thank you, give me direction ,I already post this quenstion  in StackOverflow.\r\nBest regards", "You can use `tf.train.Checkpoint` to save model's weights. If you use `keras`, you can add a `ModelCheckpoint` callback.", "@BeccaHuang ,\r\n\r\nPlease look at @luozhouyang  comment, provided links and let us know if the issue got resolved.\r\n\r\nPlease feel free to close this issue and track in StackOverflow as this is not bug or feature request.Thanks!", "In keras you can use `ModelCheckpoint` :\r\n\r\nfor example-\r\n```\r\ncheckpoint_path='weights.{epoch:02d}-{loss:0.2f}.h5'\r\n\r\ncheckpoint= tf.keras.callbacks.ModelCheckpoint(\r\n    checkpoint_path,\r\n    monitor=\"val_loss\",\r\n    verbose=0,\r\n    save_best_only=False,\r\n    save_weights_only=True,\r\n    mode=\"auto\",\r\n    save_freq=\"epoch\",\r\n    options=None,)\r\n```\r\n\r\nHere you can monitor your metrics and you can resume your training at any time from your previous checkpoint.\r\n\r\nfor example-\r\n\r\nLoading checkpoint:\r\n```\r\nif checkpoint_path is not None :\r\n model.load_weights('weights.03-0.66.h5')   #Here 3 is the epoch number and 0.66 is the loss of the last checkpoint saved\r\n Finding the epoch index from which we are resuming\r\n initial_epoch = 3\r\nelse:\r\n initial_epoch = 0\r\n```\r\n\r\n\r\nWhile doing custom training you can use `tf.train.Checkpoint` as mentioned by @luozhouyang  which tracks the latest weights and you can save it in each iteration easily.", "Hi @luozhouyang and @zyberg2091 ,\r\n\r\nThank your replay,\r\nI try these advise many days, but i still confuse because my try model is combite three part,\r\nCan't use history= model.fit(...)   then i don't know how to use\u00a0checkpoint in each epoch.\r\n\r\nOften need to definition model , but if three part how can i  definition combite one model?\r\n\r\nTry code dann\u2198   Domain confusion and Adversarial loss\r\nhttps://github.com/lancerane/Adversarial-domain-adaptation/blob/master/Domain%20adaptation,%20TF2.0.ipynb\r\n\r\nThe expression may not be clear, hope you can understand! ><\"", "Here is an official tutorial [tf.train.Checkpoint](https://www.tensorflow.org/guide/checkpoint)", "Thanks @luozhouyang,\r\nThe official tutorial model is  a simple linear model   \"net = Net()\"  [just one part]\r\nbut my model have three part build a model,\r\ni don't know how to use this by dann  model.", "```python\r\nckpt = tf.train.Checkpoint(model_a=a, model_b=model_b, model_c=model_c, ...)\r\n```", "Thanks @luozhouyang,\r\nyou give me direction, i try for long time but i am not a smart person.\r\nfirst dataset and need to define optimization step but if variables have three is ok?\r\ni always get error, i don't know why how to do\r\n------------ \r\nin dep train_step\u2198\r\nvariables_feature = feature_generator.trainable_variables\r\nvariables_label = label_predictor.trainable_variables\r\nvariables_domain = domain_predictor.trainable_variables\r\n\r\nin for epoch in range(epochs)\u2198\r\nckpt = tf.train.Checkpoint(model_a=feature_generator, model_b=label_predictor, model_c=domain_predictor)\r\nmanager = tf.train.CheckpointManager(ckpt, './tf_ckpts')\r\n\r\ndef train_and_checkpoint(net, manager):\r\n  ckpt.restore(manager.latest_checkpoint)\r\n  if manager.latest_checkpoint:\r\n    print(\"Restored from {}\".format(manager.latest_checkpoint))\r\n  else:\r\n    print(\"Initializing from scratch.\")\r\n\r\nckpt.step.assign_add(1)\r\nif int(ckpt.step) % 10 == 0:\r\n  save_path = manager.save()\r\n  print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\r\n  print(\"loss {:1.2f}\".format(loss.numpy()))\r\n\r\ntrain_and_checkpoint(ckpt, manager)\r\n-------\r\nthis is train_step code\r\n![image](https://user-images.githubusercontent.com/67473475/121030445-0f65e280-c7dc-11eb-933a-5fdcd7de4fe7.png)\r\n", "Thanks @luozhouyang,\r\nyou give me direction, i try for tf.train.Checkpoint, get this error [ `Checkpoint` was expecting a trackable object (an object derived from `TrackableBase`)]\r\n\r\nI already change  [tf.compat.v2.keras.optimizers.Adam] result same get error.\r\nI don't know where get wrong\r\n\r\n![image](https://user-images.githubusercontent.com/67473475/121496702-4c64eb80-ca0d-11eb-9972-b054a2ddb1e1.png)\r\n", "@BeccaHuang We generally not support debugging long codes. GitHub is mainly for bugs and performance related issues. If you think you find a bug, can you please share a simple standalone code to reproduce the issue? Thanks!", "Thanks jvishnuvardhan , i am sorry, because  i don't know how clear expression about my problem, I tried to re-expression.\r\n\r\nIn tensorflow2.0 how save and load weight ,about build adversarial neural network  model have two different optimizer, by epoch training, It's not model = tf.keras.Model(...) framework and can't model.fit,  can't tf.saved_model.save(model),how can i do?\r\n\r\n\r\n I use this to do train step\u2192 [with tf.GradientTape(persistent=True) as tape], but i don't know how can i save weight?\r\n ", "I try but i don't know how to use simple code to show.\r\nModel Framework i try use @ofbennett DANN model code\r\nabout [with tf.GradientTape(persistent=True) as tape] train step\r\nhow can to save weight?\r\n\r\nhttps://github.com/lancerane/Adversarial-domain-adaptation/blob/master/Domain%20adaptation,%20TF2.0.ipynb", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49938\">No</a>\n"]}, {"number": 49937, "title": "outputs  from tensor flow lite doesn't meet python script to generate output from any layer if i train the model ", "body": "https://i.stack.imgur.com/qZ2rl.png\r\n\r\nimage from \"Quantization and Training of Neural Networks for Efficient\r\nInteger-Arithmetic-Only Inference\"\r\n\r\nHello , i am trying to catch the error which is simply i take input ,weight ,zeros , scales then add bias to the result and apply this equation in first image \r\nusing python comparing the result with output from tensor flow lite \r\nbut a weird thing happen which is if i untrained model just initialize it with random values the two output from python and tensor flow lite meet but if i train model and do the same thing i didn't get the same output. \r\nin details : to get output from point wise layer (conv2d ) \r\ni extract weights,zeros,inputs,scale from tensor flow lite \r\nand apply the equation in image get same result if i initialize model with random values \r\nin moment that  i train model the result is different , note that i extract the new parameters out for sure \r\ni  implemented moblie net and using normal relu not relu 6 is this can make problem ?\r\ni upload my neutron model :\r\nhttps://drive.google.com/file/d/1ENVZCf0hSOiIIh8-0X_-7I23sv3Az2_2/view?usp=sharing", "comments": ["i try to know the operation done inside tensor flow lite cause i want to implement it on fpga \r\nis there is another way to view this operation of instead equation written in paper i will be okay with it \r\ncause my main target to implement this operation on and run model on fpga ", "the soultion to \r\ntf.lite.Interpreter(\r\n    model_path=None, model_content=None, experimental_delegates=None,\r\n    num_threads=None,\r\n    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.AUTO,\r\n    experimental_preserve_all_tensors=False\r\n)\r\nmake >>   experimental_preserve_all_tensors=true to save inputs, scales cause if it false he will save outputs only and any ask to intermediate node will be garbage \r\nthanks to Eng.Karim Nosseir ", "Hi @mostafa1231 , can we close this issue because you already get the solution? Please feel free to reopen if it's not the case. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49937\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49937\">No</a>\n"]}, {"number": 49936, "title": "[RNN] Tflite's fused LSTM does not support masking", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7\r\n- TensorFlow installation (pip package or built from source): tensorflow-nightly\r\n\r\n### 2. Code\r\n\r\n```\r\n        model = LSTM_model\r\n\trun_model = tf.function(lambda x: model(x))\r\n\t## This is important, let's fix the input size.\r\n\tBATCH_SIZE = 1\r\n\tINPUT_LEN = 150\r\n\tINPUT_SIZE = 9\r\n\tconcrete_func = run_model.get_concrete_function(\r\n\t    [tf.TensorSpec([BATCH_SIZE, INPUT_LEN, INPUT_SIZE], \"float32\")])\r\n\tMODEL_DIR = \"lstm\"\r\n\tmodel.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\r\n\t# Convert the model.\r\n\tconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\n        #converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\ttflite_model = converter.convert()\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\n   I find that the performance of LSTM with masking decreases heavily if I convert it into fused version by tf.function (The Tflite model work OK if I directly convert it with function \"from_keras_model\". And I find that the masking seems not to work in the fused version.\r\n", "comments": ["Indeed we do not support masking for the fused lstm, also can you share the tflite model? thanks", "The current TFLite UndirectionalSequenceLSTM doesn't support mask input yet. So fusing to LSTM with masking to that is being disabled at the moment.\r\nCan you share the performance numbers?", "@Mrlyk423 ,\r\nCan you please provide the required details to debug the issue.Also please try to execute the code in latest tf version 2.7.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49934, "title": "need a custom implementation: BroadcastTo.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10, x86-64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DIV, LEAKY_RELU, MAX_POOL_2D, MUL, SUB, TANH, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: BroadcastTo.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\anaconda3\\envs\\pytorch1\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\anaconda3\\envs\\pytorch1\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Anaconda3\\envs\\pytorch1\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\AppData\\Roaming\\Python\\Python37\\site-packages\\absl\\app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\AppData\\Roaming\\Python\\Python37\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)", "comments": ["@mahdaneh \r\nlooking at the error log,seems like this is similar to  [issue](https://github.com/tensorflow/tensorflow/issues/33490) and let us know if it helps.Thanks", "Please use the recent TF 2.x version for TFLite conversion after creating your TF model in TF 1.x. The above BroadcastTo op is already supported in the recent versions.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49934\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49934\">No</a>\n", "Thanks for your hint @abattery. The issue is resolved by migrating my  code (in TF1.15) to TF2 via help of [this](https://www.tensorflow.org/guide/upgrade). Then using TF2 and setting `tf.compat.v1.disable_v2_behavior()`, I was able to convert my model to tflite throught `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph`.\r\n"]}, {"number": 49933, "title": "DepthwiseConv2D documentation: a different filter per channel vs the same filter per channel", "body": "The documentation for [tf.keras.layers.DepthwiseConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D) describes this operation as: split input into individual channels, convolve each with the layer's kernel and finally stack the results. This gives the impression the same kernel is applied to all channels. But I believe the actual implementation applies a different kernel per channel.\r\n\r\nRelevant code sections\r\n- Backend implementation: https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/ops/nn_impl.py#L760-L766\r\n- Keras layer: https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/layers/convolutional.py#L2260-L2269\r\n\r\nAlso see [MobileNet Paper](https://arxiv.org/pdf/1704.04861.pdf) page 3 formula 3", "comments": ["[Depthwise kernel shape](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/layers/convolutional.py#L2403-L2405) further highlights, that DepthwiseConv2D is implemened as c kernels of shape (k, k) rather than one kernel of shape (k, k) applied to all c channels.", "Yes. +1.\r\n\r\nThe description in the keras layer should be clarified. \r\n\r\nNote whenmaking the update that `channel_multiplier` in `nn_impl.py` is called `depth_multiplier` in `convolutional.py`\r\n\r\n", "I made a pull request to the keras repo, see https://github.com/keras-team/keras/pull/14817."]}, {"number": 49932, "title": "Remove IMAGE_RECOGNITION variables from third_party_downloads", "body": "@advaitjain When the image_recognition_experimental example was removed I missed to delete these lines from the third_party_downloads.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49930, "title": "Incorrect variance in normalization layer of EfficientNet", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS (Google Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-0-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nEfficientNet includes [a normalization layer within its model definition](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/applications/efficientnet.py#L321), but it seems like the variance is incorrect. The variance is `[0.229, 0.224, 0.225]`, but [those values are the standard deviations of the ImageNet dataset](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/applications/imagenet_utils.py#L196). When the normalization layer is called on new inputs, the inputs are normalized using the mean (which looks correct) and the square root of the variance. So in the current EfficientNet implementation, inputs are normalized using the square root of the standard deviation of ImageNet.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect inputs to EfficientNet to be normalized according to the standard deviation of the ImageNet dataset.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): no, because I don't know where I would make this change. The change would have to be within the saved models.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.applications.EfficientNetB0(weights=\"imagenet\")\r\nnorm_layer = model.layers[2]\r\nassert \"normalization\" in norm_layer.name\r\nprint(norm_layer.mean.numpy())  # [0.485 0.456 0.406]\r\nprint(norm_layer.variance.numpy())  # [0.229 0.224 0.225]\r\n# Generate sample inputs.\r\ntf.random.set_seed(42)\r\nx = tf.random.uniform((1, 224, 224, 3), 0, 255, dtype=\"int32\", seed=42)\r\n\r\n\r\ndef get_reference(inputs):\r\n    \"\"\"Get the reference normalized outputs.\"\"\"\r\n    x = np.asarray(inputs).astype(\"float32\")\r\n    x /= 255.0\r\n    x[..., 0] -= 0.485\r\n    x[..., 1] -= 0.456\r\n    x[..., 2] -= 0.406\r\n    x[..., 0] /= 0.229\r\n    x[..., 1] /= 0.224\r\n    x[..., 2] /= 0.225\r\n    return x\r\n\r\n\r\ndef get_current_tf_efficientnet_norm_output(inputs):\r\n    \"\"\"Get the normalized outputs from the current implementation.\"\"\"\r\n    x = np.asarray(inputs).astype(\"float32\")\r\n    x /= 255.0\r\n    x[..., 0] -= 0.485\r\n    x[..., 1] -= 0.456\r\n    x[..., 2] -= 0.406\r\n    x[..., 0] /= np.sqrt(0.229)\r\n    x[..., 1] /= np.sqrt(0.224)\r\n    x[..., 2] /= np.sqrt(0.225)\r\n    return x\r\n\r\n\r\nmodel_normalizer = tf.keras.Model(model.input, norm_layer.output)\r\n# Below is True (they are the same)\r\nnp.allclose(get_current_tf_efficientnet_norm_output(x), model_normalizer(x), atol=1e-07)\r\n# Below is False (they are different)\r\nnp.allclose(get_reference(x), model_normalizer(x), atol=1e-07)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe [Normalization layer normalizes using the square root of the variance](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/layers/preprocessing/normalization.py#L242-L243) (which equal to the standard deviation) ", "comments": ["Could replicate the issue with **`Tensorflow Version 2.5`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/9adcbfed857b3fd011c67251d11e2147/gh_49930.ipynb). Thanks!", "Thanks for reporting the issue. When I look at the original implementation from the paper in https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py#L505, I didn't find any normalization layer for the inputs. (The rescale in expected in keras.application model to mimic https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/applications/imagenet_utils.py#L190).\r\n\r\n@fchollet, since the original implementation is added by you, could u provide more context about why the normalization layer is added?", "My experiment using tf.keras.applications.EfficientNetB0 with TensorFlow 2.4.2 on ILSVRC2012 validation set only achieve ~74.6% of accuracy.\r\nAfter fixing this problem by:\r\n```\r\nmean, std, c = model.layers[2].get_weights()\r\nmodel.layers[2].set_weights([mean, std**2, c])\r\n```\r\nthe result increased to 77.2%, as in the paper.", "The Keras implemention of EfficientNet is a re-implemention of [reference code](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet). Normalization function and args can be found at [here](https://github.com/tensorflow/tpu/blob/abdfd6726cec215e9c2700d6f494f0a08077a2b8/models/official/efficientnet/main.py#L369) and [here](https://github.com/tensorflow/tpu/blob/b24729de804fdb751b06467d3dce0637fa652060/models/official/efficientnet/efficientnet_builder.py#L31).\r\nIt normalizes the data by `x = (x - mean * 255) / (std * 255)` and can be split into 2 steps: `x = x / 255` for rescale and `x = (x - mean) / std` for normalization.\r\nThe normalization layer in tf.keras normalizes the data using mean and variance, and applys `x = (x - mean) / sqrt(variance)` to inputs, but it is standard deviation in current pre-trained weights of tf.keras.applications.efficientnet, so it applys `x = (x - mean) / sqrt(std)` to inputs, which is incorrect.", "@fanstvn If there is any actionable PRs, please feel free to open them in [keras-team/keras](https://github.com/keras-team/keras/issues) repository. \r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus entirely on only keras. Thanks! ", "@jvishnuvardhan - ok -- the rescaling weights of the trained model will have to be updated. can you transfer this pr to the keras repository? this pr was made while keras was still being developed in this repository.", "I will take a closer look and fix this issue. Sorry for the long wait.", "@fanstvn, thanks for the detailed trace, and I think this is a oversight in keras code. I will send a fix very soon.", "I was able to verify the correctness of fix by following code snippet.\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.applications.EfficientNetB0(weights=\"imagenet\")\r\n\r\nBATCH = 128\r\n\r\ndef resize_and_center_crop(image: tf.Tensor, size: int) -> tf.Tensor:\r\n  h, w = tf.shape(image)[0], tf.shape(image)[1]\r\n\r\n  # Figure out the necessary h/w.\r\n  ratio = (tf.cast(size, tf.float32) / tf.cast(tf.minimum(h, w), tf.float32))\r\n  h = tf.cast(tf.round(tf.cast(h, tf.float32) * ratio), tf.int32)\r\n  w = tf.cast(tf.round(tf.cast(w, tf.float32) * ratio), tf.int32)\r\n  image = tf.image.resize(image, [h, w])\r\n  top = (h - size) // 2\r\n  left = (w - size) // 2\r\n  image = tf.image.crop_to_bounding_box(image, top, left, size, size)\r\n  return image\r\n\r\ndef eval_preprocess(features):\r\n  image = features[\"image\"]\r\n  label = features[\"label\"]\r\n  assert image.dtype == tf.uint8\r\n  image = tf.cast(image, tf.float32)\r\n  # image = tf.keras.applications.resnet50.preprocess_input(image)\r\n  image = resize_and_center_crop(image, 224)\r\n  label = tf.one_hot(label, 1000)\r\n  return image, label\r\n\r\ndataset_builder = tfds.builder(\"imagenet2012\")\r\neval_ds = dataset_builder.as_dataset(split='validation')\r\neval_ds = eval_ds.map(eval_preprocess, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH)\r\n\r\nnorm_layer = model.layers[2]\r\nassert \"normalization\" in norm_layer.name\r\nprint(norm_layer.mean.numpy())  # [0.485 0.456 0.406]\r\nprint(norm_layer.variance.numpy())  # [0.229 0.224 0.225]\r\nexisting_variance = norm_layer.variance.numpy()\r\n\r\ncorrect_variance = tf.math.square(existing_variance)\r\nnorm_layer.variance = tf.convert_to_tensor(correct_variance)\r\n\r\nmodel.compile(metrics=['categorical_accuracy'])\r\nmodel.evaluate(eval_ds, verbose=1)\r\n\r\n# validation acc is 77.0\r\n```", "This should be fixed now in keras.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49930\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49930\">No</a>\n"]}, {"number": 49929, "title": "Elu int8 quantization doesn't reduce the size needed by the model on Arduino", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution : Mac OS 10.14.8 \r\n- TensorFlow installed from source\r\n- Tensorflow version : 2.5.0\r\n- Target platform : Arduino Nano 33\r\n\r\n**Describe the problem**\r\nContext :\r\nI would like to run inferencing of a DL-model on an Arduino and, since I don't have much memory available, I need to int8-quantize my model.\r\nBut the quantization of my model doesn't seem to be working, and it seems to be linked to the Elu activations functions in the model.\r\nIndeed, I get no error during both the conversion and the quantization of the model on Python and the inferencing on Arduino, but the necessary size for the model on the Arduino remains the same than without quantization.\r\n\r\nWhat I tried :\r\n- I retrained a model in which I changed the Elu for Relu activation functions. Then quantization works : thanks to the line : tflInterpreter->arena_used_bytes() on Arduino, I can see that quantization helped me to reduce the necessary size for the model by 3.\r\n- I analysed the model (quantized, with Elu) on the Netron App and I realised that there are steps of de-quantization and re-quantization before and after each call of Elu function : model de-quantize and re-quantize. I don't understand why is this doing so, when it doesn't append with Relu functions: <img width=\"582\" alt=\"Screenshot 2021-05-31 at 15 42 59\" src=\"https://user-images.githubusercontent.com/78730433/120202463-e5824e00-c226-11eb-8ac6-fe37c5d5911e.png\">\r\n- Finally, I found this commit on Tensorflow Git, which made me believe that int8 quantization for Elu is implemented : [commit Elu quant](https://github.com/tensorflow/tensorflow/commit/918f876bf812fd744151fea29b2df4aa18acfa8f) Nevertheless, they mentioned a LUT approach, which I don't understand and might (?) be linked to the troubles I am facing.\r\n\r\nDoes anyone face the same king of troubles for quantization of model containing Elu ? Do you have any idea of how to solve this problem ?\r\n\r\nThank you really much !", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49929\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49929\">No</a>\n", "Hi @EleonoreBarges, please close this issue if the fix is verified. Thanks!", "Thanks for your help, this is working now ! :) ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49929\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49929\">No</a>\n"]}, {"number": 49928, "title": "Tensorflow1.10 CPU Exceed 10% of system memory, Segmentation fault ", "body": "**I would like to ask why the program will have segmentation fault if it exceeds 10% of the system memory. Obviously, there is still a lot of memory to use.**\r\n**100G of memory is used, and 400G  unused.**\r\n\r\nThank you for your answer\uff01\r\ncode:\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ','.join(FLAGS.gpu_id)\r\n... \r\n    with tf.train.MonitoredTrainingSession(\r\n            checkpoint_dir=model_dir,\r\n            hooks=hooks,\r\n            config=config\r\n    ) as sess:\r\n        while not sess.should_stop():\r\n            sess.run(train_op)\r\n...\r\nExceed 10% of system memory, Segmentation fault !\r\n", "comments": ["@AI-Friend \r\nWe see that you are using 1.x which is not actively supported anymore, please upgrade to 2.x and let us know.\r\nYou may use stable version tf 2.4.1, refer to similar issue  [#49233] and refer to the tested [build configurations here.](https://www.tensorflow.org/install/source_windows#gpu)", "I mean, do you have any default settings in tf1.10 to limit the use of memory? \r\nAt present, I only use 20% of the memory to report an error:\r\n\r\nW tensorflow/core/framework/allocator.cc:108] Allocation of 105062400000 exceeds 10% of system memory. \r\nSegmentation fault\r\n\r\nAt present, TF1. X is still the most used version in our industry, not 2. X.\r\nThank you for your answer\uff01\r\n", "@AI-Friend \r\nCan you please try [limiting gpu](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth),refer to [link](https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory).\r\n\r\nAs 1.x is not supported please have a look at similar issues:[link](https://github.com/tensorflow/tensorflow/issues/17783),#45310", "I am using CPU instead of GPU. \r\nI want to confirm whether TF1.10 limits the CPU usage, b\r\necause the error is: \r\nW tensorflow/core/framework/allocator.cc:108] Allocation of 105062400000 exceeds 10% of system memory.\r\nExceed 10% of system memory, Segmentation fault !\r\nIf TF1.10 does not limit the CPU usage, the exit operation may be caused by other problems such as memory leaks\uff01\r\n\r\nThank you for your answer\uff01", "@AI-Friend \r\nLike i told you earlier there is no support for 1.x anymore[to check on memory leaks], it could be that memory is used for some other process in your system thats limiting usage for tensorflow.\r\nThere are issues related to the error reported in tf 1.x in past:#7717,#9263,#11550 [you may look up extremely old issues related to your error and configuration or try to build tf again on your system]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49927, "title": "[RNN] Post-training integer quantization got low performance when increasing the size of representative dataset.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7 \r\n- TensorFlow installation (pip package or built from source): tensorflow-nightly\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\n\r\n### 2. Code\r\n\r\nMy model is a 5-layer Bidirectional LSTM. The originally trained model accuracy is about 0.90. When I increase the size of the representative dataset from 400 to 20000\uff0cthe quantitative model's acc decrease from 0.85 to 0.50. How is the representative dataset used? In my opinion, the more data I provide, the better quantization performance I will obtain. But it seems not, why?\r\n\r\n```\r\ndef representative_data_gen():\r\n  for input_value in tf.data.Dataset.from_tensor_slices(train_data).batch(1).take(400):\r\n    yield [input_value]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_model_quant = converter.convert()\r\n```\r\n\r\n\r\n", "comments": [" Representative dataset is used to determine the quantization range. It's possible to model be worse if some data points are extreme. (e.g. makes huge numerical error or quantize op has huge error for the data.)\r\n\r\n We can debug it if you provide the actual model & dataset. Can you share some more details?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49926, "title": "Improvement for tf.keras.utils.get_file", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.keras.utils.get_file only works with `GET` request. I think it can work for `POST` request too if the `data` argument is utilized in [urlretrieve](https://github.com/tensorflow/tensorflow/blob/ecc0010f151cabb0a4730777790631fd6fff8f6f/tensorflow/python/keras/utils/data_utils.py#L259). \r\n\r\n**Will this change the current api? How?**\r\nYes, get_file would take in one other parameter (don't know which name and type would be best) to pass into the retrieve function.\r\n\r\n**Who will benefit with this feature?**\r\nWhoever wants to get file that's accessed through post method, such as files that require tokens or authorization.\r\n\r\n**Side note on documentation.**\r\nIn [get_file documentation](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file), `untar` and `md5_hash` arguments have been depreciated in favor of `extract` and `file_hash`. I thought about some changes but I don't know if they would add value.\r\n* Change [example](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file#example) argument from `untar=True` to `extract=True`\r\n* Rearrange arguments in documentation and function to show `extract` and `file_hash` before `untar` and `md5_hash`.\r\n", "comments": ["@bizzyvinci,\r\nCan you please specify the use-cases  for this feature? Thanks!", "I do compete on Zindi.africa and files can be downloaded using API. But, they use post method instead of get.\r\n\r\nSo using `tf.keras.utils.get_file(filename, api)` would fail with 4\\*\\* error. \r\n\r\nHowever, using `urlretrieve(api, filename, data=b'auth_token=MY_AUTH_TOKEN')` would download the file. Because as per [urlretrieve documentation](https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve), \"the optional data argument may be given to specify a POST request (normally the request type is GET)\"\r\n\r\n`tf.keras.utils.get_file(filename, api+'?auth_token=MY_AUTH_TOKEN')` would not work either.\r\n\r\nRight now, I can't find another example where `post` is used rather than `get` method to access a file.", "I've been able to make illustration with [jsonplaceholder](https://jsonplaceholder.typicode.com/guide/) and it can be found in this [notebook](https://colab.research.google.com/drive/1f_DlOgz4DL9_6xFOmsC71kaE4axEeQxo?usp=sharing). "]}, {"number": 49925, "title": "Fix tf_tensor C-API memleak by explicitly dealloc ctstring large.ptr", "body": "When we use c api to release a tensor by dtype `string`, there occures heavy memory leak problem, expecially in batch mode. Therefore, we distinguished the problem, where the TensorInterface::Release() function should also deallocate the ctstring large.ptr.", "comments": ["Could you please add a test that would fail due to the memory leak in order to exercise the fix in the PR?", "Hi @jaingaurav , I've already added an simple unit-test for TF_Tensor C API.\r\nThe following outputs is based on the current master branch, without my patch.\r\n```\r\nvalgrind --tool=memcheck --leak-check=full  ./tf_tensor_test\r\n==18488== Memcheck, a memory error detector\r\n==18488== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\r\n==18488== Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright info\r\n==18488== Command: ./tf_tensor_test\r\n==18488== \r\nRunning main() from test_main.cc\r\n[==========] Running 1 test from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from TF_Tensor\r\n[ RUN      ] TF_Tensor.TestTF_Tensor_NonScalarBytes_AllocateDelete\r\n[       OK ] TF_Tensor.TestTF_Tensor_NonScalarBytes_AllocateDelete (38 ms)\r\n[----------] 1 test from TF_Tensor (42 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test suite ran. (63 ms total)\r\n[  PASSED  ] 1 test.\r\n==18488== \r\n==18488== HEAP SUMMARY:\r\n==18488==     in use at exit: 134,249 bytes in 1,921 blocks\r\n==18488==   total heap usage: 12,436 allocs, 10,515 frees, 798,082 bytes allocated\r\n==18488== \r\n==18488== 192 bytes in 4 blocks are definitely lost in loss record 1,095 of 1,162\r\n==18488==    at 0x4C29EC3: malloc (vg_replace_malloc.c:309)\r\n==18488==    by 0x40B13C: tensorflow::(anonymous namespace)::TF_Tensor_TestTF_Tensor_NonScalarBytes_AllocateDelete_Test::TestBody() (in /data1/home/sincereli/.cache/bazel/_bazel_sincereli/5b45794ffc23b4ae3078fb441c007042/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/tf_tensor_test)\r\n==18488==    by 0x8F0A3DD: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (in /data1/home/sincereli/.cache/bazel/_bazel_sincereli/5b45794ffc23b4ae3078fb441c007042/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/com_google_googletest/libgtest.so)\r\n==18488==    by 0x8F0A64A: testing::Test::Run() (in /data1/home/sincereli/.cache/bazel/_bazel_sincereli/5b45794ffc23b4ae3078fb441c007042/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/com_google_googletest/libgtest.so)\r\n==18488==    by 0x8F0A980: testing::TestInfo::Run() (in /data1/home/sincereli/.cache/bazel/_bazel_sincereli/5b45794ffc23b4ae3078fb441c007042/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/com_google_googletest/libgtest.so)\r\n==18488==    by 0x8F0AC44: testing::TestSuite::Run() (in /data1/home/sincereli/.cache/bazel/_bazel_sincereli/5b45794ffc23b4ae3078fb441c007042/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/com_google_googletest/libgtest.so)\r\n==18488==    by 0x8F0B13B: testing::internal::UnitTestImpl::RunAllTests() (in /data1/home/sincereli/.cache/bazel/_bazel_sincereli/5b45794ffc23b4ae3078fb441c007042/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/com_google_googletest/libgtest.so)\r\n==18488==    by 0x8F0B20D: bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (in /data1/home/sincereli/.cache/bazel/_bazel_sincereli/5b45794ffc23b4ae3078fb441c007042/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/com_google_googletest/libgtest.so)\r\n==18488==    by 0x8F0B42F: testing::UnitTest::Run() (in /data1/home/sincereli/.cache/bazel/_bazel_sincereli/5b45794ffc23b4ae3078fb441c007042/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/com_google_googletest/libgtest.so)\r\n==18488==    by 0x4215AA9: main (in /data1/home/sincereli/.cache/bazel/_bazel_sincereli/5b45794ffc23b4ae3078fb441c007042/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/platform/libtest_main.so)\r\n==18488== \r\n==18488== LEAK SUMMARY:\r\n==18488==    definitely lost: 192 bytes in 4 blocks\r\n==18488==    indirectly lost: 0 bytes in 0 blocks\r\n==18488==      possibly lost: 0 bytes in 0 blocks\r\n==18488==    still reachable: 134,057 bytes in 1,917 blocks\r\n==18488==                       of which reachable via heuristic:\r\n==18488==                         stdstring          : 49,737 bytes in 919 blocks\r\n==18488==                         newarray           : 3,416 bytes in 4 blocks\r\n==18488==         suppressed: 0 bytes in 0 blocks\r\n==18488== Reachable blocks (those to which a pointer was found) are not shown.\r\n==18488== To see them, rerun with: --leak-check=full --show-leak-kinds=all\r\n==18488== \r\n==18488== For counts of detected and suppressed errors, rerun with: -v\r\n==18488== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\r\n```\r\n\r\nAn then I re-compiled & re-run the same unit-test with this hotfix patch applied. The following output reveals it can fix the upon definitely lost problem.\r\n```\r\nvalgrind --tool=memcheck --leak-check=full  ./tf_tensor_test\r\n==19617== Memcheck, a memory error detector\r\n==19617== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\r\n==19617== Using Valgrind-3.14.0 and LibVEX; rerun with -h for copyright info\r\n==19617== Command: ./tf_tensor_test\r\n==19617== \r\nRunning main() from test_main.cc\r\n[==========] Running 1 test from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from TF_Tensor\r\n[ RUN      ] TF_Tensor.TestTF_Tensor_NonScalarBytes_AllocateDelete\r\n[       OK ] TF_Tensor.TestTF_Tensor_NonScalarBytes_AllocateDelete (37 ms)\r\n[----------] 1 test from TF_Tensor (41 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test suite ran. (62 ms total)\r\n[  PASSED  ] 1 test.\r\n==19617== \r\n==19617== HEAP SUMMARY:\r\n==19617==     in use at exit: 134,057 bytes in 1,917 blocks\r\n==19617==   total heap usage: 12,436 allocs, 10,519 frees, 798,082 bytes allocated\r\n==19617== \r\n==19617== LEAK SUMMARY:\r\n==19617==    definitely lost: 0 bytes in 0 blocks\r\n==19617==    indirectly lost: 0 bytes in 0 blocks\r\n==19617==      possibly lost: 0 bytes in 0 blocks\r\n==19617==    still reachable: 134,057 bytes in 1,917 blocks\r\n==19617==                       of which reachable via heuristic:\r\n==19617==                         stdstring          : 49,737 bytes in 919 blocks\r\n==19617==                         newarray           : 3,416 bytes in 4 blocks\r\n==19617==         suppressed: 0 bytes in 0 blocks\r\n==19617== Reachable blocks (those to which a pointer was found) are not shown.\r\n==19617== To see them, rerun with: --leak-check=full --show-leak-kinds=all\r\n==19617== \r\n==19617== For counts of detected and suppressed errors, rerun with: -v\r\n==19617== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)\r\n```\r\n", "Any other comments on this PR? Please. Thanks!", "Sorry, but I do not have permission to get any information about \"feedback/copybara \u2014 Google internal checks FAILED\", any more helps are appreciated. Please, thanks!", "@saxenasaurabh  Could you please help review and merge? Thanks~"]}, {"number": 49924, "title": "Hello", "body": "I'm requesting for the PR to be merged!", "comments": []}, {"number": 49923, "title": "build tensorflow 1.15 from source code based on cuda 11", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary):  source\r\n- TensorFlow version: 1.15\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: docker container, build using tensorflow official dockerfile\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\n- CUDA/cuDNN version: 11.2/8.1\r\n- GPU model and memory: v100/16G\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to build tensorflow 1.15 from source code, because I have added some code in contrib.\r\nfirst, I build a docker image which is dev-gpu-bazel0.26.1-py3-cuda11.2-cudnn8.1.0-ubuntu18.04\r\nthen create a container base on this image. \r\nAfter run ./configure, a error occurred,\r\n\"ValueError: dictionary update sequence element #9 has length 1; 2 is required\"\r\nThis seems that function to find cuda10 information is not work for cuda 11. So I replace \"./third_party/gpus/cuda_configure.bzl\"\r\nand \"third_party/gpus/find_cuda_config.py\" with corresponding files in tensorflow2.\r\nBut, another error occurred, seems that the code are depended by other code and depend on other code.\r\n\r\n\r\nI found a issue and get a reply,  https://github.com/tensorflow/tensorflow/issues/43629#issuecomment-702851560\r\nsaid that, I can not use tf1.15 on cuda11.\r\nBut I found another page said, they can install tensorflow 1.15 on cuda 11, but by pip not by build from source code.\r\n,  https://developer.nvidia.com/blog/accelerating-tensorflow-on-a100-gpus/\r\n\r\nSo, what I want is that, How can I build tensorflow 1.15 when cuda versio is 11.2.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@xieyi4650 \r\nTensorflow 1.15 is not actively supported, please upgrade to tensorflow 2.x, you may try tensorflow stable version 2.4.1 and cuda 11.0/cuddnn 8.0 and let us know.\r\nPlease refer to the [tested build configurations here](https://www.tensorflow.org/install/source_windows#gpu). [similar issue #49233]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49923\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49923\">No</a>\n"]}, {"number": 49922, "title": "Update xtensa HiFi5 library calls for kernels: AvgPool, MaxPool, Add,\u2026", "body": "\u2026 Mul, Relu, Relu6.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49922) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 49921, "title": "Bug fix for TfLiteTensor dims", "body": "When using the MicroInterpreter, the TfLiteEvalTensor must be the source of truth for tensor dims.\r\nIt is possible during the Prepare phase for an operator to change the location of the tensor dims to somewhere other than the Flatbuffer.  When using the MicroInterpreter, the Flatbuffer is currently used as the source of truth for TfLiteTensor dims.  This fix makes the TfLiteEvalTensor dims the source of truth when initializing a TfLiteTensor.\r\n\r\nAdditional fix for issue micro: port op L2_POOL_2D from lite #47814", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n"]}, {"number": 49919, "title": "[MLIR][DISC] pattern conversion from tf2mhlo: ConvertUnpackOpDynamic, ConvertSignOpDynamic, ConvertSigmoidGradOpDynamic", "body": "We are porting our MLIR-based dynamic shape compiler to tf community (From OP def, Patttern, to Optimization pass, etc).\r\nThis is the 5th PR about tf2mhlo pattern conversion, which including ConvertUnpackOpDynamic, ConvertSignOpDynamic, ConvertSigmoidGradOpDynamic.\r\nThe rest pattern conversions we will add:\r\n- ConvertSqueezeOpxxx\r\n- ConvertStridedSliceOpxxx\r\n- ConvertPrintOp", "comments": ["Actually I see a test failure:\r\n```\r\n/build/work/1ba5f054c6ec31340884d6d72bfb5d63b759/google3/runfiles/google3/third_party/tensorflow/compiler/mlir/xla/tests/legalize-tf.mlir:2471:12: error: CHECK: expected string not found in input\r\n // CHECK: shape.shape_of {{.*}} : tensor<?xf32> -> tensor<1xindex>\r\n           ^\r\n<stdin>:1994:28: note: scanning from here\r\n func @sigmoid_grad_dynamic(%arg0: tensor<?xf32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {\r\n                           ^\r\n<stdin>:1996:44: note: possible intended match here\r\n %1 = chlo.broadcast_multiply %arg1, %arg0 : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n                                           ^\r\n/build/work/1ba5f054c6ec31340884d6d72bfb5d63b759/google3/runfiles/google3/third_party/tensorflow/compiler/mlir/xla/tests/legalize-tf.mlir:2710:12: error: CHECK: expected string not found in input\r\n // CHECK: \"mhlo.sign\"(%arg0) : (tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32>\r\n           ^\r\n<stdin>:2132:20: note: scanning from here\r\n %2 = mhlo.constant dense<0.000000e+00> : tensor<f32>\r\n                   ^\r\n<stdin>:2135:56: note: possible intended match here\r\n %5 = \"mhlo.select\"(%1, %4, %0) : (tensor<?x2x3x?xi1>, tensor<?x2x3x?xf32>, tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32>\r\n                                                       ^\r\n\r\nInput file: <stdin>\r\nCheck file: /build/work/1ba5f054c6ec31340884d6d72bfb5d63b759/google3/runfiles/google3/third_party/tensorflow/compiler/mlir/xla/tests/legalize-tf.mlir\r\n\r\n-dump-input=help explains the following input dump.\r\n\r\nInput was:\r\n<<<<<<\r\n              .\r\n              .\r\n              .\r\n           1989:  %1 = mhlo.constant dense<(1.000000e+00,0.000000e+00)> : tensor<2xcomplex<f32>> \r\n           1990:  %2 = mhlo.subtract %1, %arg0 : tensor<2xcomplex<f32>> \r\n           1991:  %3 = mhlo.multiply %0, %2 : tensor<2xcomplex<f32>> \r\n           1992:  return %3 : tensor<2xcomplex<f32>> \r\n           1993:  } \r\n           1994:  func @sigmoid_grad_dynamic(%arg0: tensor<?xf32>, %arg1: tensor<?xf32>) -> tensor<?xf32> { \r\ncheck:2471'0                                X~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: no match found\r\n           1995:  %0 = mhlo.constant dense<1.000000e+00> : tensor<f32> \r\ncheck:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           1996:  %1 = chlo.broadcast_multiply %arg1, %arg0 : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32> \r\ncheck:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncheck:2471'1                                                ?                                                  possible intended match\r\n           1997:  %2 = chlo.broadcast_subtract %0, %arg0 {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<?xf32>) -> tensor<?xf32> \r\ncheck:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           1998:  %3 = chlo.broadcast_multiply %1, %2 : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32> \r\ncheck:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           1999:  return %3 : tensor<?xf32> \r\ncheck:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           2000:  } \r\ncheck:2471'0     ~~~\r\n           2001:  func @sin(%arg0: tensor<2xf32>) -> tensor<2xf32> { \r\ncheck:2471'0     ~~~~~~~~~~\r\n              .\r\n              .\r\n              .\r\n           2127:  return %0 : tensor<1x2x3x4xf32> \r\n           2128:  } \r\n           2129:  func @sign_dynamic(%arg0: tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32> { \r\n           2130:  %0 = \"mhlo.sign\"(%arg0) : (tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32> \r\n           2131:  %1 = \"mhlo.compare\"(%arg0, %arg0) {comparison_direction = \"NE\"} : (tensor<?x2x3x?xf32>, tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xi1> \r\n           2132:  %2 = mhlo.constant dense<0.000000e+00> : tensor<f32> \r\ncheck:2710'0                        X~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: no match found\r\n           2133:  %3 = shape.shape_of %arg0 : tensor<?x2x3x?xf32> -> tensor<4xindex> \r\ncheck:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           2134:  %4 = \"mhlo.dynamic_broadcast_in_dim\"(%2, %3) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<4xindex>) -> tensor<?x2x3x?xf32> \r\ncheck:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           2135:  %5 = \"mhlo.select\"(%1, %4, %0) : (tensor<?x2x3x?xi1>, tensor<?x2x3x?xf32>, tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32> \r\ncheck:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncheck:2710'1                                                            ?                                                                 possible intended match\r\n           2136:  return %5 : tensor<?x2x3x?xf32> \r\ncheck:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           2137:  } \r\ncheck:2710'0     ~~~\r\n           2138:  func @slice_constant_start(%arg0: tensor<4xi32>) -> tensor<2xi32> { \r\ncheck:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           2139:  %0 = mhlo.constant dense<1> : tensor<1xi64> \r\n           2140:  %1 = mhlo.constant dense<2> : tensor<1xi64> \r\n              .\r\n              .\r\n              .\r\n```\r\n\r\nI haven't debugged it, if you can take a look that'd be great! (and rebase as well)", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49919) for more info**.\n\n<!-- need_author_cla -->", "> Actually I see a test failure:\r\n> \r\n> ```\r\n> /build/work/1ba5f054c6ec31340884d6d72bfb5d63b759/google3/runfiles/google3/third_party/tensorflow/compiler/mlir/xla/tests/legalize-tf.mlir:2471:12: error: CHECK: expected string not found in input\r\n>  // CHECK: shape.shape_of {{.*}} : tensor<?xf32> -> tensor<1xindex>\r\n>            ^\r\n> <stdin>:1994:28: note: scanning from here\r\n>  func @sigmoid_grad_dynamic(%arg0: tensor<?xf32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {\r\n>                            ^\r\n> <stdin>:1996:44: note: possible intended match here\r\n>  %1 = chlo.broadcast_multiply %arg1, %arg0 : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n>                                            ^\r\n> /build/work/1ba5f054c6ec31340884d6d72bfb5d63b759/google3/runfiles/google3/third_party/tensorflow/compiler/mlir/xla/tests/legalize-tf.mlir:2710:12: error: CHECK: expected string not found in input\r\n>  // CHECK: \"mhlo.sign\"(%arg0) : (tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32>\r\n>            ^\r\n> <stdin>:2132:20: note: scanning from here\r\n>  %2 = mhlo.constant dense<0.000000e+00> : tensor<f32>\r\n>                    ^\r\n> <stdin>:2135:56: note: possible intended match here\r\n>  %5 = \"mhlo.select\"(%1, %4, %0) : (tensor<?x2x3x?xi1>, tensor<?x2x3x?xf32>, tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32>\r\n>                                                        ^\r\n> \r\n> Input file: <stdin>\r\n> Check file: /build/work/1ba5f054c6ec31340884d6d72bfb5d63b759/google3/runfiles/google3/third_party/tensorflow/compiler/mlir/xla/tests/legalize-tf.mlir\r\n> \r\n> -dump-input=help explains the following input dump.\r\n> \r\n> Input was:\r\n> <<<<<<\r\n>               .\r\n>               .\r\n>               .\r\n>            1989:  %1 = mhlo.constant dense<(1.000000e+00,0.000000e+00)> : tensor<2xcomplex<f32>> \r\n>            1990:  %2 = mhlo.subtract %1, %arg0 : tensor<2xcomplex<f32>> \r\n>            1991:  %3 = mhlo.multiply %0, %2 : tensor<2xcomplex<f32>> \r\n>            1992:  return %3 : tensor<2xcomplex<f32>> \r\n>            1993:  } \r\n>            1994:  func @sigmoid_grad_dynamic(%arg0: tensor<?xf32>, %arg1: tensor<?xf32>) -> tensor<?xf32> { \r\n> check:2471'0                                X~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: no match found\r\n>            1995:  %0 = mhlo.constant dense<1.000000e+00> : tensor<f32> \r\n> check:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>            1996:  %1 = chlo.broadcast_multiply %arg1, %arg0 : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32> \r\n> check:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> check:2471'1                                                ?                                                  possible intended match\r\n>            1997:  %2 = chlo.broadcast_subtract %0, %arg0 {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<?xf32>) -> tensor<?xf32> \r\n> check:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>            1998:  %3 = chlo.broadcast_multiply %1, %2 : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32> \r\n> check:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>            1999:  return %3 : tensor<?xf32> \r\n> check:2471'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>            2000:  } \r\n> check:2471'0     ~~~\r\n>            2001:  func @sin(%arg0: tensor<2xf32>) -> tensor<2xf32> { \r\n> check:2471'0     ~~~~~~~~~~\r\n>               .\r\n>               .\r\n>               .\r\n>            2127:  return %0 : tensor<1x2x3x4xf32> \r\n>            2128:  } \r\n>            2129:  func @sign_dynamic(%arg0: tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32> { \r\n>            2130:  %0 = \"mhlo.sign\"(%arg0) : (tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32> \r\n>            2131:  %1 = \"mhlo.compare\"(%arg0, %arg0) {comparison_direction = \"NE\"} : (tensor<?x2x3x?xf32>, tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xi1> \r\n>            2132:  %2 = mhlo.constant dense<0.000000e+00> : tensor<f32> \r\n> check:2710'0                        X~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: no match found\r\n>            2133:  %3 = shape.shape_of %arg0 : tensor<?x2x3x?xf32> -> tensor<4xindex> \r\n> check:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>            2134:  %4 = \"mhlo.dynamic_broadcast_in_dim\"(%2, %3) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<4xindex>) -> tensor<?x2x3x?xf32> \r\n> check:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>            2135:  %5 = \"mhlo.select\"(%1, %4, %0) : (tensor<?x2x3x?xi1>, tensor<?x2x3x?xf32>, tensor<?x2x3x?xf32>) -> tensor<?x2x3x?xf32> \r\n> check:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> check:2710'1                                                            ?                                                                 possible intended match\r\n>            2136:  return %5 : tensor<?x2x3x?xf32> \r\n> check:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>            2137:  } \r\n> check:2710'0     ~~~\r\n>            2138:  func @slice_constant_start(%arg0: tensor<4xi32>) -> tensor<2xi32> { \r\n> check:2710'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>            2139:  %0 = mhlo.constant dense<1> : tensor<1xi64> \r\n>            2140:  %1 = mhlo.constant dense<2> : tensor<1xi64> \r\n>               .\r\n>               .\r\n>               .\r\n> ```\r\n> \r\n> I haven't debugged it, if you can take a look that'd be great! (and rebase as well)\r\n\r\nMehdi, I rebased and fixed this failure.(The root cause is typo \"_return_  failure()\")", "> Mehdi, I rebased and fixed this failure.(The root cause is typo \"return failure()\")\r\n\r\nIt seems to still fail the same way at the moment?", "> > Mehdi, I rebased and fixed this failure.(The root cause is typo \"return failure()\")\r\n> \r\n> It seems to still fail the same way at the moment?\r\n\r\nSorry, I extended chlo. Fixed.", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 49918, "title": "Check layer by layer execution time in tensorflow object detection API model", "body": "How to calculate the execution time of each layers in tensorflow object detection API model?\r\nAlso I am able to see the node names using the below code.How to use tf.graph() to get the layer names?\r\n\r\n\r\nimport tensorflow as tf\r\n# read pb into graph_def\r\n\r\n\r\nwith tf.io.gfile.GFile(\"frozen_inference_graph.pb\", \"rb\") as f:\r\n  graph_def = tf.compat.v1.GraphDef()\r\n  graph_def.ParseFromString(f.read())\r\n\r\nwith tf.Graph().as_default() as graph:\r\n  tf.import_graph_def(graph_def)\r\n\r\n\r\nfor op in graph.get_operations(): \r\n  print(op.name, [inp for inp in op.inputs])\r\n", "comments": ["@IAM-P-LP ,\r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or completed code to reproduce the issue]  or if possible share a colab gist with the issue reported.\r\n\r\nThanks!", "Hi,\r\ntf_version : 2.3.1\r\n\r\nI have trained a mobilenet-ssd v2 tf model for face detection.So using the frozen pb file,I ran the code pasted above,to get the tensor names.How to use tf.graph(),such that I will get the layer names to get the inference time taken by each layers?.\r\n", "@IAM-P-LP ,\r\n\r\nCan you please go through the issues with similar references.It helps.[Link1](https://stackoverflow.com/questions/35336648/list-of-tensor-names-in-graph-in-tensorflow),[Link2](https://www.tensorflow.org/api_docs/python/tf/Graph).It helps.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49918\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49918\">No</a>\n"]}, {"number": 49916, "title": "Additional bug fix for CreateWritableTensorDimsWithCopy", "body": "Also check that the AllocatePersistentBuffer method is available.\r\n\r\nAdditional fix for issue micro: port op L2_POOL_2D from lite #47814", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n"]}, {"number": 49912, "title": "tensorflow", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@rumeshchakma \r\nPlease fill the issue template.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49912\">No</a>\n"]}, {"number": 49911, "title": "Issue in building tensorflow wheel using bazel", "body": "Hi\r\n\r\nI built tensorflow wheel using bazel I got wheel name: tensorflow-2.5.0-cp39-cp39-macosx_11_0_universal2.whl\r\nthis wheel is installed successfully on the machine where it is built which is MacOs Big Sur 11 but I am getting an error when installing it on other macOS big sur 11 (VMWARE7.1):\r\n\r\nbs-mac43:~ buildmachine$ python3.9 -m pip install tensorflow-2.5.0-cp39-cp39-macosx_11_0_universal2.whl \r\nERROR: tensorflow-2.5.0-cp39-cp39-macosx_11_0_universal2.whl is not a supported wheel on this platform.\r\n\r\nInstalling the released wheel tensorflow-2.5.0-cp39-cp39-macosx_10_11_x86_64.whl works fine on both OS.\r\n\r\n1. What is wrong with the wheel I built?\r\n2. How to build wheel that support Mac OS 10 and 11?\r\n3. How to build wheel that support x86 and 64?\r\n\r\nThanks", "comments": ["Hi\r\n\r\nI can provide more info that the issue isn't related to VMWARE since another user face same problem that have MacOs Big Sur Version 11.3.1\r\n\r\nThanks", "@Nizarazo \r\n \r\nCould you please elaborate your Query with clear details. What is your error and the steps followed to reproduce it. Thanks\r\n", "Hi @UsharaniPagadala \r\n\r\nHere is the error:\r\nbs-mac43:~ buildmachine$ \r\npython3.9 -m pip install tensorflow-2.5.0-cp39-cp39-macosx_11_0_universal2.whl\r\nERROR: tensorflow-2.5.0-cp39-cp39-macosx_11_0_universal2.whl is not a supported wheel on this platform.\r\n\r\nI built the wheel using the instructions in website below for MacOs:\r\nhttps://www.tensorflow.org/install/source\r\n\r\nWhat additional info needed?\r\n\r\nThanks", "Hi @UsharaniPagadala \r\n\r\nAny more info is needed or we are all set and waiting for your fix?\r\n\r\nThanks", " Since the installation fails on VMWARE you may want to reach out to https://docs.vmware.com/en/VMware-vSphere-Bitfusion/3.0/Example-Guide/GUID-1C053853-4D83-4D94-A6F3-D6958478AAB2.html for support.", "Hi @ymodak\r\n\r\nAs I mentioned in my previous comment It is not an issue with VMWARE as it is happening on another user computer that doesn't have VMWARE but he has MacOs Big Sur Version 11.3.1\r\nSo it isn't happening only on same machine that built the wheel.\r\n\r\nThanks", "Hi @ymodak\r\n\r\nWhy the bug status is in awaiting response state I already replied in my previous comment?\r\n\r\nThanks", "Hi @ymodak \r\n\r\nAny more info is needed?\r\n\r\nThanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @yomdak @UsharaniPagadala \r\n\r\nDidn't hear from you for a long time and ticket will be closed soon as there is no activity.\r\nAs I mentioned in my previous comment It is not an issue with VMWARE because it is happening on another OS that isn't VMWARE but MacOs Big Sur Version 11.3.1\r\nSo it is working only on same machine that built the wheel which is MaOs Big Sur 11.3 \r\n\r\nAny advice how to make the wheel working on different machine than the one which build it?\r\nI followed the steps in the following link:\r\nhttps://www.tensorflow.org/install/source\r\nSo what I am doing wrong?\r\n\r\nThanks ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49911\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49911\">No</a>\n"]}, {"number": 49909, "title": "converting alpha to float before use", "body": "issue fixed if user enters aplha = 1 instead of alpha = 1.0, causing error during loading weights due to model_name getting wrong and downloading fails. Also typo at the line 152 to add consistency for the user.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49909) for more info**.\n\n<!-- need_sender_cla -->", "Is there a real issue that this fixes?", "@mihaimaruseac #49904 this issue was related to this bug. Mobilenetv2 could not download imgnet weights even after all value validations done, url gets wrong when alpha is 1 instead of 1.0", "https://github.com/tensorflow/tensorflow/commit/8a4bd5f159815ff5f31ec2601d4d8a9284c6d3bb is the proper fix (as it silently converts `1` to `1.0` when serializing to string to form the URL). `1` is a perfectly valid floating point number and there should be no behavior difference between `1` and `1.0`.\r\n\r\nSince above commit landed, there is no need for this PR anymore.\r\n\r\nThank you for the PR and sorry that another PR landed first"]}, {"number": 49908, "title": "debugging issue", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Quadro RTX 6000\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n`InvalidArgumentError: condition [24], then [23], and else [] must be broadcastable \t [[{{node SelectV2_7}}]] [Op:IteratorGetNext]`\r\n\r\nWhat does this mean? I tried debugging using tf.debugging all my cases were passed. still not able to figure how how to debug this. Is is possible to print the graph and nodes of the function. Any help will be appreciated. \r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): Yes", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49908\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49908\">No</a>\n"]}, {"number": 49907, "title": "Failure when training with tf.GradientTape() for regression problems", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 (the same thing happens on Linux)\r\n- TensorFlow version: 2.1.0 (the same thing happens on 2.5.0)\r\n- Python version: 3.6.6\r\n\r\n**Describe the current behavior**\r\nFor the exactly the same model, training via `tf.GradientTape()` either: (a) does not converge, or (b) converges with worse score than model training via `tf.keras.Model.fit` method. \r\n\r\n**Describe the expected behavior**\r\nSimilar training outcome when using `tf.GradinetTape()` and `tf.keras.Model.fit`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nfrom sklearn.datasets import load_boston\r\nimport tensorflow as tf\r\n\r\ntf.random.set_seed(0)\r\n\r\n\r\ndef make_model():\r\n    input_layer = tf.keras.layers.Input(shape=(np.shape(x)[1]))\r\n    inner_layer_1 = tf.keras.layers.Dense(\r\n        units=10,\r\n        activation='selu',\r\n        kernel_initializer=tf.keras.initializers.lecun_normal()\r\n    )(input_layer)\r\n    inner_layer_2 = tf.keras.layers.Dense(\r\n        units=10,\r\n        activation='selu',\r\n        kernel_initializer=tf.keras.initializers.lecun_normal()\r\n    )(inner_layer_1)\r\n    output_layer = tf.keras.layers.Dense(\r\n        units=1,\r\n        activation='linear'\r\n    )(inner_layer_2)\r\n    return tf.keras.Model(\r\n        input_layer,\r\n        output_layer\r\n    )\r\n\r\n# Get raw data.\r\nraw_features, y = load_boston(return_X_y=True)\r\n\r\n# Standardize features.\r\nx = (raw_features-raw_features.mean(axis=0)) / raw_features.std(axis=0)\r\nprint(np.std(x, axis=0))\r\n\r\n# Reshape targets.\r\ny = np.array(np.reshape(y, newshape=(-1, 1)), dtype=np.float32)\r\n\r\n# Training parameters.\r\noptimizer = tf.keras.optimizers.Adam()\r\noptimizer_mp = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\r\n    optimizer, \"dynamic\"\r\n)\r\nobjective = tf.keras.losses.MeanSquaredError()\r\nbatch_size = 4\r\nepochs = 5\r\n\r\n# Fit via fit method.\r\ntrain_fit = make_model()\r\ntrain_fit.summary()\r\ntrain_fit.compile(optimizer, objective)\r\ntrain_fit.fit(x=x, y=y, batch_size=batch_size, epochs=epochs)\r\n\r\n# Fit via gradient tape.\r\ngradient_tape_fit = make_model()\r\ngradient_tape_fit.summary()\r\ndataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\r\nfor epoch in range(0, epochs):\r\n    for step, (x_batch, y_batch) in enumerate(dataset):\r\n        with tf.GradientTape() as tape:\r\n            predictions = gradient_tape_fit(x_batch, training=True)\r\n            loss = objective(y_batch, predictions)\r\n            scaled_loss = optimizer_mp.get_scaled_loss(loss)\r\n        scaled_grads = tape.gradient(\r\n            scaled_loss, gradient_tape_fit.trainable_weights\r\n        )\r\n        gradients = optimizer_mp.get_unscaled_gradients(\r\n            scaled_grads\r\n        )\r\n        optimizer.apply_gradients(zip(\r\n            gradients, gradient_tape_fit.trainable_weights\r\n        ))\r\n    predictions_via_tape = gradient_tape_fit.predict(x)\r\n    print(\r\n        'Tape MSE: %s' % np.mean(np.power(y-predictions_via_tape, 2))\r\n    )\r\n\r\npredictions_via_train = train_fit.predict(x)\r\nprint(\r\n    'Fit MSE: %s' % np.mean(np.power(y-predictions_via_train, 2))\r\n)\r\n```", "comments": ["@markodjordjic the reason could be with way how losses are calculated. Since losses calculated using fit() are scaled during training so it is not exactly the mse but mse with some scaling.", "@tusharvickey1999 Thank you for your reply. I have updated the implementation, so now there is an optimizer which can retrieve both scaled and unscaled loss. However the issue still remains. There is a significant decline in model quality when trained via `tf.GradientTape()` in comparison to the training wit the  `tf.keras.Model.fit()` method.", "@markodjordjic \r\n\r\nPlease refer similar [issue](https://github.com/tensorflow/tensorflow/issues/33898) and let us know if it helps.Thanks", "@UsharaniPagadala Hi. Thank you for referring to the issue #33898. The issue deals with the problem in a slightly different way, but it was still possible to serve as an insight in what is the root cause of the problem. And the final conclusion is that the `tf.keras.Model.fit()` method does the **shuffling of the data by default** and then if you wish to replicate the outcome of the training process via `tf.GradientTape()` data also needs to be shuffled before consumed by the training process. This of course refers to training processes where **shuffling of the data is needed**. If shuffling of the data is not needed it **must be disabled in the `tf.keras.Model.fit()`** as well; otherwise the trainings will still have different outcomes.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@jvishnuvardhan \r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/f03c0ae7ac4b24fd3b57ec8299ef8d87/untitled85.ipynb) here.Thanks", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49907\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49907\">No</a>\n"]}, {"number": 49905, "title": "Build for macos_arm64 on m1 fails with missing toolchain", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos big sur 11.2.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.0 tag\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: venv\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): \r\n% clang --version\r\nApple clang version 12.0.5 (clang-1205.0.22.9)\r\nTarget: arm64-apple-darwin20.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n**Describe the problem**\r\nBuilding libtensorflow fails with \r\nERROR: /private/var/tmp/_bazel_mnelson/fcd9b76ea591ce077b01d60187d6f334/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n./bazel-3.7.2 build --jobs 4 --config=macos_arm64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow:libtensorflow.dylib\r\n```\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49905\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49905\">No</a>\n", "My mistake. Xcode was not installed correctly.", "Hey @realrunner still running into this issue and currently unable to compile Tensorflow `2.7.0`. Can you detail what it means that \"XCode is not installed correctly\". Tensorflow on M1 is currently **not installable** with Python 3.9", "@Fohlen I had installed the Xcode command line tools but not the full Xcode app from the app store. Once I had that installed it compiled just fine.", "This appears to be a regression in the repository. I can build the latest master but the `2.7.0` release tag is currently broken on the M1 with the error detailed.\r\n", "I'm trying to build the latest master and hit the `local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64` error. Any tips?\r\n\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos monterey 12.2.1\r\n* TensorFlow installed from (source or binary): source\r\n* TensorFlow version: [latest master at time of writing](https://github.com/tensorflow/tensorflow/commit/1f9f7b4fc1bb4922e73cdfa123beaf76b11980ec)\r\n* Python version: 3.8.12\r\n* Installed using virtualenv? pip? conda?: venv\r\n* Bazel version (if compiling from source): 5.0.0\r\n* GCC/Compiler version (if compiling from source):\r\n   ```\r\n   % clang --version\r\n   Apple clang version 13.0.0 (clang-1300.0.29.30)\r\n   Target: arm64-apple-darwin21.3.0\r\n   Thread model: posix\r\n   InstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n   ```\r\n* CUDA/cuDNN version: n/a\r\n* GPU model and memory: n/a\r\n\r\n\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Option 'java_toolchain' is deprecated\r\nWARNING: Option 'host_java_toolchain' is deprecated\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=178\r\nINFO: Reading rc options for 'build' from /Users/dxia/src/github.com/tensorflow/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/dxia/src/github.com/tensorflow/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\nINFO: Reading rc options for 'build' from /Users/dxia/src/github.com/tensorflow/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/dxia/.pyenv/versions/tf-dev/bin/python3 --action_env PYTHON_LIB_PATH=/Users/dxia/.pyenv/versions/tf-dev/lib/python3.8/site-packages --python_path=/Users/dxia/.pyenv/versions/tf-dev/bin/python3\r\nINFO: Reading rc options for 'build' from /Users/dxia/src/github.com/tensorflow/tensorflow/.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /Users/dxia/src/github.com/tensorflow/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/dxia/src/github.com/tensorflow/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:macos in file /Users/dxia/src/github.com/tensorflow/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nERROR: /private/var/tmp/_bazel_dxia/491c492a0a78de04355938f243335330/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nERROR: /private/var/tmp/_bazel_dxia/491c492a0a78de04355938f243335330/external/local_config_cc/BUILD:48:19: Analysis of target '@local_config_cc//:toolchain' failed\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:\r\nINFO: Elapsed time: 0.139s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 1 target configured)\r\n```", "Seems like this https://github.com/bazelbuild/bazel/issues/13514#issuecomment-847917936 fixes that"]}, {"number": 49904, "title": "Fail to load weights of Keras MobilenetV2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux? (Google Colab)\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.10\r\n\r\n**Describe the current behavior**\r\ntensorflow.keras.applications.MobileNetV2() fail to return a model with Exception: URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1_224_no_top.h5: 404 -- Not Found.\r\n\r\n**Describe the expected behavior**\r\nExpect to get a model with loaded weights.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): No\r\n\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow\r\nmodel = tensorflow.keras.applications.MobileNetV2(\r\n    input_shape=(224, 224, 3),\r\n    alpha=1,\r\n    include_top=False,\r\n    weights='imagenet',\r\n    input_tensor=tensorflow.keras.layers.Input((224, 224, 3)),\r\n    pooling=None,\r\n    classes=1000,\r\n    classifier_activation='softmax'\r\n)\r\n\r\nColab to reproduce: https://colab.research.google.com/drive/1d9aaTVCFDMWyyiKrEsIHBe_BFnEEiKKD?usp=sharing\r\n\r\n**Other info / logs**\r\nHTTPError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/data_utils.py in get_file(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\r\n    257       try:\r\n--> 258         urlretrieve(origin, fpath, dl_progress)\r\n    259       except urllib.error.HTTPError as e:\r\n\r\n9 frames\r\nHTTPError: HTTP Error 404: Not Found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/data_utils.py in get_file(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\r\n    258         urlretrieve(origin, fpath, dl_progress)\r\n    259       except urllib.error.HTTPError as e:\r\n--> 260         raise Exception(error_msg.format(origin, e.code, e.msg))\r\n    261       except urllib.error.URLError as e:\r\n    262         raise Exception(error_msg.format(origin, e.errno, e.reason))\r\n\r\nException: URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1_224_no_top.h5: 404 -- Not Found\r\n", "comments": ["in the arguments instead of alpha = 1 , write alpha = 1.0. It will work", "Docs mention that it expects a float between 0 and 1. https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2#args\r\nPerhaps we can silently convert the value to float and update the docs `alpha = 1.0`", "> Docs mention that it expects a float between 0 and 1. https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2#args\r\n> Perhaps we can silently convert the value to float and update the docs `alpha = 1.0`\r\n\r\nThanks a lot, it works :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49904\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49904\">No</a>\n"]}, {"number": 49903, "title": "Added test case for Accuracy alias", "body": "Added test case for fix done as part of PR #49218\r\n\r\ncc @mihaimaruseac ", "comments": []}, {"number": 49902, "title": "How to build TfLite C API for arm-linux-gnueabihf", "body": "## URL(s) with the issue:\r\n\r\n<https://www.tensorflow.org/lite/guide/build_rpi>\r\n<https://www.tensorflow.org/lite/guide/build_cmake_arm>\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nHow to build TfLite C API for arm-linux-gnueabihf?\r\n(ARMv7 NEON enabled, using CMake)\r\nI need to build both static and dynamic libraries.", "comments": ["@terryheo fyi,\r\n\r\nFor bazel, you can follow the command line to produce arm 32bit C library.\r\n\r\n```\r\nbazel build --config= elinux_armhf -c opt //tensorflow/lite/c:libtensorflowlite_c.so\r\n```", "Please check this page.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library\r\n\r\nYou can use `tensorflow_src/tensorflow/lite/c` instead of `tensorflow_src/tensorflow/lite` directory.\r\n\r\nex)\r\n```\r\nARMCC_FLAGS=\"-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations\"\r\nARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-\r\ncmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \\\r\n  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \\\r\n  -DCMAKE_C_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n  -DCMAKE_CXX_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\r\n  -DCMAKE_SYSTEM_NAME=Linux \\\r\n  -DCMAKE_SYSTEM_PROCESSOR=armv7 \\\r\n  ../tensorflow/lite/c/\r\n```", "> Please check this page.\r\n> https://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library\r\n> \r\n> You can use `tensorflow_src/tensorflow/lite/c` instead of `tensorflow_src/tensorflow/lite` directory.\r\n> \r\n> ex)\r\n> \r\n> ```\r\n> ARMCC_FLAGS=\"-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations\"\r\n> ARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-\r\n> cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \\\r\n>   -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \\\r\n>   -DCMAKE_C_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n>   -DCMAKE_CXX_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n>   -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\r\n>   -DCMAKE_SYSTEM_NAME=Linux \\\r\n>   -DCMAKE_SYSTEM_PROCESSOR=armv7 \\\r\n>   ../tensorflow/lite/c/\r\n> ```\r\n\r\nThank you. It works.\r\n\r\nBut I can not compile with my local gcc.\r\n\r\n\r\nHere is the last part of log:\r\n```\r\ncd /root/armv7hf-gcc-latest/_deps/xnnpack-build && /usr/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/root/armv7hf-gcc-latest/xnnpack/include -I/root/armv7hf-gcc-latest/xnnpack/src -I/root/armv7hf-gcc-latest/clog-source/deps/clog/include -I/root/armv7hf-gcc-latest/cpuinfo-source/include -I/root/armv7hf-gcc-latest/pthreadpool-source/include -I/root/armv7hf-gcc-latest/FXdiv-source/include -I/root/armv7hf-gcc-latest/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8  -O2  -o CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-neondot.c.o   -c /root/armv7hf-gcc-latest/xnnpack/src/qs8-gemm/gen/1x8c4-minmax-neondot.c\r\n/tmp/cczpwu5P.s: Assembler messages:\r\n/tmp/cczpwu5P.s:71: Error: selected processor does not support `vsdot.s8 q9,q10,d7[0]' in ARM mode\r\n/tmp/cczpwu5P.s:74: Error: selected processor does not support `vsdot.s8 q9,q10,d7[1]' in ARM mode\r\n/tmp/cczpwu5P.s:76: Error: selected processor does not support `vsdot.s8 q8,q10,d7[0]' in ARM mode\r\n/tmp/cczpwu5P.s:78: Error: selected processor does not support `vsdot.s8 q8,q10,d7[1]' in ARM mode\r\n/tmp/cczpwu5P.s:152: Error: selected processor does not support `vsdot.s8 q9,q10,d7[0]' in ARM mode\r\n/tmp/cczpwu5P.s:154: Error: selected processor does not support `vsdot.s8 q8,q10,d7[0]' in ARM mode\r\nmake[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:14769: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-neondot.c.o] Error 1\r\nmake[2]: Leaving directory '/root/armv7hf-gcc-latest'\r\nmake[1]: *** [CMakeFiles/Makefile2:4065: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/all] Error 2\r\nmake[1]: Leaving directory '/root/armv7hf-gcc-latest'\r\nmake: *** [Makefile:155: all] Error 2\r\n```\r\n\r\n```\r\nroot@vultr:~# arm-linux-gnueabihf-g++ -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=arm-linux-gnueabihf-g++\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc-cross/arm-linux-gnueabihf/9/lto-wrapper\r\nTarget: arm-linux-gnueabihf\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 9.3.0-17ubuntu1~20.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libitm --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --without-target-system-zlib --enable-libpth-m2 --enable-multiarch --enable-multilib --disable-sjlj-exceptions --with-arch=armv7-a --with-fpu=vfpv3-d16 --with-float=hard --with-mode=thumb --disable-werror --enable-multilib --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=arm-linux-gnueabihf --program-prefix=arm-linux-gnueabihf- --includedir=/usr/arm-linux-gnueabihf/include\r\nThread model: posix\r\ngcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04) \r\n```", "@Nugine \r\nIs this still an issue, can you please confirm on the latest tf version.", "> @Nugine Is this still an issue, can you please confirm on the latest tf version.\r\n\r\nI can reproduce the compile error with the lastest commit (22b2692d3fef35c8c3da94d222946a71467fe630)", "The build issue should be fixed. Let me know if you still have the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49902\">No</a>\n"]}]