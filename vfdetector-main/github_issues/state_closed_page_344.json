[{"number": 43802, "title": "ValueError: You are trying to load a weight file containing 0 layers into a model with 19 layers.", "body": "hey this is my code i still have the same error \r\nfrom __future__ import absolute_import\r\nfrom __future__ import print_function\r\nimport os\r\nimport numpy as np\r\nfrom keras.layers import Input\r\nfrom keras.layers.core import Activation, Flatten, Reshape\r\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D, Conv2D\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.models import Model\r\nfrom keras.utils import np_utils\r\nfrom keras.applications import imagenet_utils\r\nfrom .Architecture import Architecture\r\n\r\n\r\n\r\nclass Ronneberger(Architecture):\r\n    @staticmethod\r\n    def get_model(config, crossval_id=None) :\r\n        assert config.arch.num_dimensions is 2\r\n        assert config.arch.patch_shape[0] % 16 == 0, \"Invalid patch shape\"\r\n\r\n        num_modalities = len(config.dataset.modalities)\r\n        input_layer_shape = (num_modalities, ) + config.arch.patch_shape[:config.arch.num_dimensions]\r\n        output_layer_shape = (config.train.num_classes, np.prod(config.arch.patch_shape[:config.arch.num_dimensions]))\r\n        model = generate_unet_model(\r\n            config.arch.num_dimensions,\r\n            config.train.num_classes,\r\n            input_layer_shape,\r\n            output_layer_shape,\r\n            config.train.activation,\r\n            downsize_factor=2)\r\n\r\n        return model\r\n\r\ndef generate_net_model(\r\n    dimension, num_classes, input_shape, output_shape, activation, downsize_factor=2):\r\n    img_input = Input(shape=input_shape)\r\n    x = img_input\r\n    # Encoder\r\n    x = Conv2D(64, 3, 3, border_mode=\"same\")(img_input)\r\n    x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    x = MaxPooling2D(pool_size=(2, 2))(x)\r\n    \r\n    x = Convolution2D(128, 3, 3, border_mode=\"same\")(x)\r\n    x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    x = MaxPooling2D(pool_size=(2, 2))(x)\r\n    \r\n    x = Convolution2D(256, 3, 3, border_mode=\"same\")(x)\r\n    x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    x = MaxPooling2D(pool_size=(2, 2))(x)\r\n    \r\n    x = Convolution2D(512, 3, 3, border_mode=\"same\")(x)\r\n    x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    \r\n    x = Convolution2D(1024, 3, 3, border_mode=\"same\")(x)\r\n    x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    \r\n    # Decoder\r\n    x = Convolution2D(512, 2, 2, border_mode=\"same\")(x)\r\n    x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    \r\n    x = UpSampling2D(size=(2, 2))(x)\r\n    x = Convolution2D(256, 2, 2, border_mode=\"same\")(x)\r\n    x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    \r\n    x = UpSampling2D(size=(2, 2))(x)\r\n    x = Convolution2D(128, 2, 2, border_mode=\"same\")(x)\r\n    x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    \r\n    x = UpSampling2D(size=(2, 2))(x)\r\n    x = Convolution2D(64, 2, 2, border_mode=\"same\")(x)\r\n    x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    \r\n    x = Convolution2D(num_classes, 1, 1, border_mode=\"valid\")(x)\r\n    print(img_input.shape )\r\n    x = Reshape((48*48, num_classes))(x)\r\n    x = Activation(\"softmax\")(x)\r\n    model = Model(img_input, x)\r\n\r\n    return model\r\n\r\n\r\nand the error is \r\n\r\nload_weights_from_hdf5_group\r\n    str(len(filtered_layers)) + ' layers.')\r\n\r\nValueError: You are trying to load a weight file containing 0 layers into a model with 19 layers.", "comments": ["@elmahdy27 \r\n\r\nPlease, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know which TF version you are using.\r\n\r\nRequest you to share colab link or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "tensorboard==1.8.0\r\ntensorflow==1.8.0\r\ntensorflow-gpu==1.8.0\r\ntensorflow-tensorboard==1.5.0", "@elmahdy27 \r\n\r\nTF 1.x is not actively supported. Can you please switch to TF 2.x latest versions for better performance.Please, try with TF 2.3 latest released version and let us know if the problem still persists. Thanks!", "same problem ", "@elmahdy27 \r\n\r\nCan you share colab link or standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "https://github.com/NIC-VICOROB/SUNet-architecture this is the code i just changed the Ronneberger file, sorry upload internet in my country sucks i can't upload the dataset nor the h5 files", "it is related to the h5 file ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43802\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43802\">No</a>\n"]}, {"number": 43801, "title": "Adding some initial organization to the docs and readme.", "body": "Addresses b/165627576", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43799, "title": "cuda_configure.bzl: wrong cuda_config.cudnn_version cause wrong cudnn_headers list", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: master\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 3.1.5\r\n- GCC/Compiler version (if compiling from source): MSVC 2019\r\n- CUDA/cuDNN version: 11.1 8.0.4\r\n- GPU model and memory: RTX 2080 TI\r\n\r\n**Describe the problem**\r\n\r\nUnable to build.\r\n\r\nBut build fine if I comment out this line\r\nin file tensorflow/third_party/gpus/cuda_configure.bzl \r\n```\r\n    cudnn_headers = [\"cudnn.h\"]\r\n###    if cuda_config.cudnn_version.rsplit(\"_\", 1)[0] >= \"8\":\r\n    cudnn_headers += [\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nI added debug line to print information in  `cuda_config.cudnn_version`\r\n\r\n```\r\ncudnn_headers = [\"cudnn.h\"]\r\n\r\nfail(\"cuda_config.cudnn_version = %s\" % cuda_config.cudnn_version )\r\n    \r\nif cuda_config.cudnn_version.rsplit(\"_\", 1)[0] >= \"8\":\r\n```\r\n\r\nRESULT:\r\n```\r\ncuda_config.cudnn_version = 64_8\r\n```", "comments": ["Can you provide failing and passing build logs, please?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43799\">No</a>\n"]}, {"number": 43798, "title": "[INTEL MKL] Fix control_flow_ops_test unit test failure", "body": "This PR fixes UT failure control_flow_ops_test, which is related to MaxPool3DGrad.\r\n\r\nMaxPool3DGrad should only be rewritten when workspace is available.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43798) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "@penpornk Thanks for reviewing. I have made some changes."]}, {"number": 43797, "title": "Updated documentation of TimeDistributed", "body": "It is not clear that the weights are shared at the current documentation. I saw this question repeats at stack overflow (https://stackoverflow.com/questions/43265084/keras-timedistributed-are-weights-shared), and I thought it is worth writing that explicitly.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43797) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43797) for more info**.\n\n<!-- ok -->", "@qlzh727 Done :) You can stash all commits when merging", "@YoavRamon can you please check ubuntu sanity errors ?", "@rthadur  Done", "@rthadur looks like import/copybara fails and I can't find any logs. Do you know what happens?"]}, {"number": 43796, "title": "TFlu: Remove Docker dependency for stm32f4 target", "body": "This isn't necessarily ready for merging but is an approach that might be useful to adopt to make it possible to run unit tests via renode as part of the CI system.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43796) for more info**.\n\n<!-- need_author_consent -->", "@mansnils, I created a PR from your branch so that its easier for us to see the changes needed. This is a good reference for us and we'll pick this up once we have some more bandwidth.", "@advaitjain Can you please sign CLA. Thanks!", "Closing the current PR in favor of https://github.com/tensorflow/tensorflow/pull/44080"]}, {"number": 43795, "title": "[INTEL MKL] Removed unnecessary omp_set_num() calls", "body": "Removed extra omp_set_num() calls and added call_once to ensure one time setting for OMP_NUM_THREADS.", "comments": []}, {"number": 43794, "title": "decimal number to binary", "body": " Python program to convert decimal number to binary using bin() function", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43794) for more info**.\n\n<!-- need_sender_cla -->", "Please don't spam with invalid Hacktoberfest PRs. TensorFlow is a library, the program you are giving does not even use it."]}, {"number": 43792, "title": "update", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43792) for more info**.\n\n<!-- need_sender_cla -->", "This looks like bad Hacktoberfest PR. Can you please document what is this trying to solve, link to an issue? Can you explain why you did the changes you're doing? I see you're changing a few constants to other values, changing code around.\r\n\r\nI'll close this PR for now but if we get explanation for the changes and more context we can reopen."]}, {"number": 43791, "title": "Include support for the Constant-Q transform.", "body": "<em>A feature request for providing support for the Constant-Q transform.</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): v2.1.0\r\n- Are you willing to contribute : Yes, I have already worked on a sample code.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe constant Q transform is very closely related to the Fourier transform. Like the Fourier transform, a constant Q transform\r\nis a bank of filters, but in contrast to the former it has geometrically spaced center frequencies. Currently, we have _tf.signal.fft_  for Fourier transforms, but no such functionality for the Constant Q transforms.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nA nice feature of the constant Q transform is its increasing time resolution towards higher frequencies.\r\n\r\n", "comments": ["@ashwinpn,\r\nThis issue is the duplicate of #42343. Can you please let us know if we can close this issue as it is already being tracked there? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43789, "title": "Google Cloud Storage cannot be used in tf.keras.callbacks.experimental.BackupAndRestore as a back-up dir", "body": "\r\n**System information**\r\n- Run in official container: tensorflow/tensorflow:2.3.1\r\n- Host OS:  Debian GNU/Linux 10 (Cloud shell)\r\n\r\n**Describe the current behavior**\r\nOn successful training, the checkpoints produced by `tf.keras.callbacks.experimental.BackupAndRestore` should be removed. However, if one provided a Cloud Storage location, the cleanup fails. This is not the case for specifying a local backup location.\r\n\r\n**Describe the expected behavior**\r\n`tf.keras.callbacks.experimental.BackupAndRestore` should be able to remove the checkpoint upon successful training. \r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab Notebook](https://colab.research.google.com/drive/1XocuDL8I8tivEa-DCgH6iWBcpV5WiJRJ?usp=sharing)\r\n*Please change the Cloud Storage location to a bucket owned by you.*\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/trainig/trainer/task.py\", line 69, in <module>\r\n    main(args)\r\n  File \"/trainig/trainer/task.py\", line 41, in main\r\n    callbacks=callbacks\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1145, in fit\r\n    callbacks.on_train_end(logs=training_logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 514, in on_train_end\r\n    callback.on_train_end(logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 1547, in on_train_end\r\n    self._training_state.delete_backup()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/worker_training_state.py\", line 124, in delete_backup\r\n    file_io.delete_recursively(pathname)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 586, in delete_recursively\r\n    delete_recursively_v2(dirname)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 599, in delete_recursively_v2\r\n    _pywrap_file_io.DeleteRecursively(compat.as_bytes(path))\r\ntensorflow.python.framework.errors_impl.NotFoundError: gs://[bucket]/training/backup/ckpt-2.data-00000-of-00001 doesn't exist or not a directory.\r\n```", "comments": ["I have also looked in the codebase and believe the culprit is that even though `file_io.delete_recursively_v2` is used, it is used for all the files found the the looked up directories, [code snippet](https://github.com/tensorflow/tensorflow/blob/ac4e209f8cd33aab482d5414b38043e03fc72ef5/tensorflow/python/keras/distribute/worker_training_state.py#L115-L120). `file_io.delete_recursively_v2`, however, can remove the content recursively. This recursive call is not compatible with Cloud Storage when calling on files and not directories.\r\n\r\nThe preceding checkpoints are deleted on epoch ends and that is implemented using `file_io.delete_file`. This works well with Cloud Storage. \r\n\r\nA possible solution is to not to call `file_io.delete_recursively_v2` on looked up files, but on the directories itself. For instance,\r\n```python\r\nfile_io.delete_recursively_v2(self.write_checkpoint_manager._prefix)\r\nfile_io.delete_recursively_v2(\r\n  os.path.join(self.write_checkpoint_manager.directory, 'checkpoint'))\r\n```", "@kalosisz Closing the issue since the necessary changes are implemented.Please feel free to re-open the issue if you have any concern.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43789\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43789\">No</a>\n", "@saikumarchalla Thank you for the fix!"]}, {"number": 43788, "title": "TFLite C++ API outputs the same results all the time", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **HDK865 from QC**\r\n- TensorFlow installed from (source or binary): **Python binary, C++ API from source**\r\n- TensorFlow version (use command below): **2.3**\r\n- Python version: **3.6**\r\n- Bazel version (if compiling from source): **Build label: 3.5.0**\r\n- GCC/Compiler version (if compiling from source): **7.5.0** \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nI have an MLP model trained with **Keras** and **Tensorflow 2.3**, I train and predict correctly with that model as .tflite using Python API, but when I try to predict using C++ API output is always the same 1/model_classes, all the time.\r\n\r\nI feed the model: \r\n```c++ \r\nvoid fillInputTenso(std::vector<float> input) {\r\n        auto inputTensor = this->mInterpreter->typed_tensor<float>(this->mInterpreter->inputs()[0]);\r\n        for(int i = 0; i < input.size(); i++) {\r\n            inputTensor[i] = input[i];\r\n        }\r\n```\r\nAfter Invoke I read the output: \r\n```c++ \r\n void proceedOutput(void) {\r\n        auto output = this->mInterpreter->typed_tensor<float>(this->mInterpreter->outputs()[0]);\r\n        for(int i=0; i < this->mConfig.labels.size(); i++) {\r\n            LOGI(\" confidence = %f.\", output[i]);\r\n        }\r\n    }\r\n```\r\nI also tried this from the C++ API but the results are the same: \r\n```c++\r\nauto input = mInterpreter->typed_input_tensor<float>(0) \r\n```\r\n**It doesn't matter what input I provide to model, its completely the same output always.\r\nThis model works completely fine with Python API.**\r\n\r\nModel architecture:\r\n```python\r\nkeras.Sequential([\r\n            keras.layers.Flatten(input_shape=(self.dataset.features_num(),)),\r\n            keras.layers.Dense(128, activation='relu'),\r\n            keras.layers.Dense(nr_classes, activation='softmax')\r\n        ])\r\n```\r\nNote: I added this node to this model since the accuracy was dropped drastically when converted to tflite:\r\n```python\r\nimport tensorflow_model_optimization as tfmot\r\n            quantize_model = tfmot.quantization.keras.quantize_model\r\n            # q_aware stands for for quantization aware.\r\n            q_aware_model = quantize_model(self.__model)\r\n            # `quantize_model` requires a recompile.\r\n            q_aware_model.compile(optimizer='adam',\r\n                                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                                  metrics=['accuracy'])\r\n            self.__model = q_aware_model\r\n```", "comments": ["Can you provide some more details about the process you are employing to BUILD & run with the C++ API?\r\n\r\nAlso, a snippet of the code where you allocate tensors & invoke the C++ interpreter would help.", "This is my BUILD file.\r\n\r\n![image](https://user-images.githubusercontent.com/30228544/95462317-a2adf600-0977-11eb-82ca-a416d11b6417.png)\r\n\r\nThis is the part when I init Interpreter and AllocateTensors:\r\n\r\n```c++\r\n tflite::StderrReporter error_reporter;\r\n        auto model = tflite::FlatBufferModel::BuildFromFile(model_path, &error_reporter);\r\n        tflite::ops::builtin::BuiltinOpResolver resolver;\r\n        \r\n        if(kTfLiteOk != tflite::InterpreterBuilder(*(model), resolver)(&this->mInterpreter)) {\r\n            LOGE(\"InterpreterBuilder failed. Exiting!\");\r\n            exit(-1);\r\n        }\r\n        \r\n        this->mInterpreter->SetNumThreads(threads);\r\n        // this->mInterpreter->UseNNAPI(true);\r\n\r\n\r\n        if(kTfLiteOk != this->mInterpreter->AllocateTensors()) {\r\n            LOGE(\"interpreter->AllocateTensor failed. Exiting!\");\r\n            exit(-1);\r\n        }\r\n```\r\n\r\nI invoke:\r\n\r\n```c++\r\nif(kTfLiteOk != mInterpreter->Invoke()) {\r\n            LOGE(\"mInterpreter->Invoke() failed. Exiting!\");\r\n            exit(-1);\r\n        }\r\n\r\n```", "Hmm. At first glance, I can't find anything wrong with your code. I assume you are persisting the `model` data returned here, until the lifetime of the interpreter:\r\n```\r\nauto model = tflite::FlatBufferModel::BuildFromFile(model_path, &error_reporter);\r\n```\r\n\r\nApart from that, it's difficult to say what could be going wrong without looking into your model.\r\n\r\nI would suggest using a float model first to check everything works in that case, but it shouldn't happen that the Python bindings work but the native runtime fails.", "Hey dude, in fact `model` it had its lifetime only inside that function scope, I changed it as you mentioned now exists for the whole interpreter lifetime and all works smoothly. You sir, are a life saver :). \r\nMay I ask why its needed for the model data to exists for the whole time of interpreter, and why isn't any segmentation fault or smh if it is needed! ", "So TFLite, to save memory, does not copy constants into a different memory location. They are read from a memory-mapped version of the model data (`model` in this case). So if you destroy the model file, the Interpreter looks at garbage and does its computations. Sometimes this leads to segfaults (if the data for say `shape` is garbage integers), but that may not always happen in (un)fortunate cases like yours :-)."]}, {"number": 43786, "title": "adding CMSIS DSP and NN include path into cmsis.inc.", "body": "HEAD of master is lacking the include paths\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include\r\nin the TFLM Makefiles.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43785, "title": "Tensorflow 2.3.0 stopped detecting GPU suddenly while it used to detect in past", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10 Pro\r\n- TensorFlow installed from : `pip install tensorflow`\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.6.8\r\n- Installed using : `pip`\r\n- CUDA version : `Cuda compilation tools, release 10.1, V10.1.243`\r\n- CUdnn version : V8.0.3.33\r\n- GPU model and memory: Nvidia GTX 1050Ti, 4GB \r\n\r\n\r\n_I used to train my models locally and it's been a while(~1month) since I've trained some models and when I opened my jupyter notebook and wrote some code and tried to train my model, I noticed that the model is training really slow and so I wrote some code to check_ **if tensorflow is detecting my GPU and to my surprise it's not detecting.** \r\n\r\n![Capture](https://user-images.githubusercontent.com/30192967/95064765-d42b8500-071d-11eb-9d90-8f96e38f5029.JPG)\r\n\r\n**After lots of brainstorming over the internet,** \r\n1) I re-installed tensorflow\r\n2) I re-installed entire CUDA according to the version it supports for my gpu\r\n3) I re-installed Python\r\n\r\nI even checked if older versions of TF(1.15.4, 1.15,3, 2.0.3) could detect but no luck.\r\nBut still I tend to see the same problem. Please let me know down below if there are any solutions which I can follow.\r\n\r\n**Things to note,**\r\n1) I don't use anaconda.\r\n2) I directly installed python and set up cuda support which used to work in past and suddenly stopped working.\r\n\r\n**Logs**\r\n**Note : Please don't get carried away with ETA, since this is a very lite model**\r\n```\r\n2020-10-05 16:39:40.085550: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-05 16:39:40.133725: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2135ad1a990 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-05 16:39:40.144525: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n[INFO] compiling model...\r\n[INFO] training head...\r\nEpoch 1/20\r\n34/34 [==============================] - 32s 940ms/step - loss: 0.4717 - accuracy: 0.7959 - val_loss: 0.1328 - val_accuracy: 0.9746\r\nEpoch 2/20\r\n34/34 [==============================] - 42s 1s/step - loss: 0.1325 - accuracy: 0.9672 - val_loss: 0.0594 - val_accuracy: 0.9891\r\nEpoch 3/20\r\n18/34 [==============>...............] - ETA: 15s - loss: 0.0815 - accuracy: 0.9826\r\n```", "comments": ["Can you post something from the log?", "Hey @bhack ,\r\nPlease check the image. \r\nActually tensorflow doesn't raise any errors if it doesn't detect gpu right. So there's nothing much to post anything from the log. ", "I meant the runtime logs like e.g: \r\n`I  tensorflow/stream_executor/platform/default/dso_loader.cc:48]  Successfully opened dynamic library libcudart.so.10.1`", "@bhack  I have updated the post, attached the logs, please check it out.", "Is this complete? I don't see the cuda entries like the one that I mentioned in the previous post.", "@bhack  Exactly, am afraid why it's not mentioning the cuda entries. ", "Can you try to run it outside Jupyter?", "@bhack  The logs which I have pasted have been run from command prompt. ", "I have the same issue, you can see my [script](https://pastebin.pl/view/0fe39e7d) and the [output](https://pastebin.pl/view/db1db62c). Personally, I run on Arch Linux and I have the latest python-tensorflow-cuda package installed and all of its dependencies updated. I see that this issue also reported at the [Arch's bugs website](https://bugs.archlinux.org/task/68078).\r\n\r\nEDIT:\r\nYou can also see the result of `locate libcudart.so`:\r\n``` \r\n$ locate libcudart.so     \r\n/opt/cuda/targets/x86_64-linux/lib/libcudart.so\r\n/opt/cuda/targets/x86_64-linux/lib/libcudart.so.11.0\r\n/opt/cuda/targets/x86_64-linux/lib/libcudart.so.11.1\r\n/opt/cuda/targets/x86_64-linux/lib/libcudart.so.11.1.74\r\n```", "@alexkorpas You log is clear and I see the cuda entires. You need to have `/opt/cuda/targets/x86_64-linux/lib/` in  your `LD_LIBRARY_PATH`", "@bhack @alexkorpas \r\nOne thing to see here is, tensorflow 2.3.0 supports CUDA V10.1 and CUDNN 7.4.  (According to https://www.tensorflow.org/install/source_windows#gpu)\r\n\r\nSo i downloaded CUDA V10.1 but coming to CUDNN, **I couldn't find 7.4 version for CUDA 10.1**(checkout the below img).\r\nLink to cuDNN archive : https://developer.nvidia.com/rdp/cudnn-archive\r\n\r\n![Capture](https://user-images.githubusercontent.com/30192967/95076248-29bc5d80-072f-11eb-815c-88834eeda663.JPG)\r\n\r\n\r\nHence I downloaded **V8.0.3.33 version of cuDNN.** ", "@AdityaNikhil Are you using tensorflow with `pip install tensorflow`?", "> @AdityaNikhil Are you using tensorflow with `pip install tensorflow`?\r\n\r\n@bhack  Yes", "> @alexkorpas You log is clear and I see the cuda entires. You need to have `/opt/cuda/targets/x86_64-linux/lib/` in your `LD_LIBRARY_PATH`\r\n\r\n@bhack My problem is solved indeed, I can't be sure about the original poster's problem. I am curious, however, why it is able to find the other library files, like `libcublas.so` but not the `libcudart.so`.", "@alexkorpas Cause the other one was already in the `LD_LIBRARY_PATH` (`/usr/local/cuda/lib64`)", "@bhack  @alexkorpas \r\nUmm... Guys, any solutions for my issue?", "@AdityaNikhil Can you share the full log starting from the command run?", "@bhack Here's the full log,\r\n\r\n```\r\nD:\\PyImageSearch\\FaceMask_Detection>py train_mask_detector.py --dataset data\r\n[INFO] loading images...\r\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\r\n2020-10-06 08:57:08.795045: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-06 08:57:08.840222: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1bb885e9d30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-06 08:57:08.849748: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n[INFO] compiling model...\r\n[INFO] training head...\r\nEpoch 1/5\r\n34/34 [==============================] - 35s 1s/step - loss: 0.4115 - accuracy: 0.8165 - val_loss: 0.1454 - val_accuracy: 0.9710\r\nEpoch 2/5\r\n34/34 [==============================] - 36s 1s/step - loss: 0.1368 - accuracy: 0.9607 - val_loss: 0.0715 - val_accuracy: 0.9855\r\nEpoch 3/5\r\n34/34 [==============================] - 40s 1s/step - loss: 0.0789 - accuracy: 0.9813 - val_loss: 0.0465 - val_accuracy: 0.9891\r\nEpoch 4/5\r\n34/34 [==============================] - 38s 1s/step - loss: 0.0486 - accuracy: 0.9888 - val_loss: 0.0353 - val_accuracy: 0.9928\r\nEpoch 5/5\r\n34/34 [==============================] - 46s 1s/step - loss: 0.0381 - accuracy: 0.9878 - val_loss: 0.0306 - val_accuracy: 0.9928\r\n[INFO] evaluating network...\r\n              precision    recall  f1-score   support\r\n\r\n   with_mask       1.00      0.99      0.99       138\r\nwithout_mask       0.99      1.00      0.99       138\r\n\r\n    accuracy                           0.99       276\r\n   macro avg       0.99      0.99      0.99       276\r\nweighted avg       0.99      0.99      0.99       276\r\n\r\n[INFO] saving mask detector model...\r\n```", "What is the output of:\n`py -c 'import tensorflow as tf; print(tf.__version__)'`\n\nAnd `pip list | grep tensorflow`", "@bhack showing some errors.....\r\n\r\n```\r\nC:\\Users\\adity>py -c 'import tensorflow as tf; print(tf.__version__)'\r\n  File \"<string>\", line 1\r\n    'import\r\n          ^\r\nSyntaxError: EOL while scanning string literal\r\n```\r\n\r\nAnyway my tensorflow version is 2.3..0 \r\n\r\n`pip list `\r\n\r\n```\r\nC:\\Users\\adity>pip list\r\nPackage                  Version\r\n------------------------ ---------\r\n-rotobuf                 3.6.1\r\nabsl-py                  0.10.0\r\nargon2-cffi              20.1.0\r\nastor                    0.8.1\r\nastunparse               1.6.3\r\nasync-generator          1.10\r\nattrs                    20.2.0\r\nbackcall                 0.2.0\r\nbleach                   3.2.1\r\ncachetools               4.1.1\r\ncertifi                  2020.6.20\r\ncffi                     1.14.3\r\nchardet                  3.0.4\r\ncolorama                 0.4.3\r\ncycler                   0.10.0\r\ndecorator                4.4.1\r\ndefusedxml               0.6.0\r\nentrypoints              0.3\r\ngast                     0.3.3\r\ngoogle-auth              1.21.2\r\ngoogle-auth-oauthlib     0.4.1\r\ngoogle-pasta             0.2.0\r\ngraphviz                 0.8.4\r\ngrpcio                   1.32.0\r\nh5py                     2.10.0\r\nidna                     2.10\r\nimportlib-metadata       2.0.0\r\nimutils                  0.5.3\r\nipykernel                5.3.4\r\nipython                  7.16.1\r\nipython-genutils         0.2.0\r\nipywidgets               7.5.1\r\njedi                     0.17.2\r\nJinja2                   2.11.2\r\njoblib                   0.16.0\r\njsonschema               3.2.0\r\njupyter                  1.0.0\r\njupyter-client           6.1.7\r\njupyter-console          6.2.0\r\njupyter-core             4.6.3\r\njupyterlab-pygments      0.1.1\r\nKeras-Applications       1.0.8\r\nKeras-Preprocessing      1.1.2\r\nkiwisolver               1.2.0\r\nMarkdown                 3.2.2\r\nMarkupSafe               1.1.1\r\nmatplotlib               3.3.2\r\nmistune                  0.8.4\r\nmxnet                    1.3.1\r\nnbclient                 0.5.0\r\nnbconvert                6.0.5\r\nnbformat                 5.0.7\r\nnest-asyncio             1.4.0\r\nnetworkx                 2.3\r\nnotebook                 6.1.4\r\nnumpy                    1.18.5\r\noauthlib                 3.1.0\r\nonnx                     1.6.0\r\nopencv-python            4.4.0.44\r\nopt-einsum               3.3.0\r\npackaging                20.4\r\npandocfilters            1.4.2\r\nparso                    0.7.1\r\npickleshare              0.7.5\r\nPillow                   7.2.0\r\npip                      18.1\r\nprometheus-client        0.8.0\r\nprompt-toolkit           3.0.7\r\nprotobuf                 3.13.0\r\npyasn1                   0.4.8\r\npyasn1-modules           0.2.8\r\npycparser                2.20\r\nPygments                 2.7.1\r\npyparsing                2.4.7\r\npyrsistent               0.17.3\r\npython-dateutil          2.8.1\r\npywin32                  228\r\npywinpty                 0.5.7\r\nPyYAML                   5.3\r\npyzmq                    19.0.2\r\nqtconsole                4.7.7\r\nQtPy                     1.9.0\r\nrequests                 2.24.0\r\nrequests-oauthlib        1.3.0\r\nrsa                      4.6\r\nscikit-learn             0.23.2\r\nscipy                    1.4.1\r\nSend2Trash               1.5.0\r\nsetuptools               50.3.0\r\nsix                      1.15.0\r\nsklearn                  0.0\r\ntensorboard              2.2.2\r\ntensorboard-plugin-wit   1.7.0\r\ntensorflow               2.2.1\r\ntensorflow-cpu           2.3.0\r\ntensorflow-estimator     2.2.0\r\ntensorflow-gpu-estimator 2.3.0\r\ntensorflowjs             2.3.0\r\ntermcolor                1.1.0\r\nterminado                0.9.1\r\ntestpath                 0.4.4\r\nthreadpoolctl            2.1.0\r\ntornado                  6.0.4\r\ntraitlets                4.3.3\r\ntyping-extensions        3.7.4.1\r\nurllib3                  1.25.10\r\nwcwidth                  0.2.5\r\nwebencodings             0.5.1\r\nWerkzeug                 1.0.1\r\nwheel                    0.35.1\r\nwidgetsnbextension       3.5.1\r\nwrapt                    1.12.1\r\nzipp                     3.2.0\r\n```", "@AdityaNikhil I see multiple versions of tensorflowin the system..Can you uninstall install tensorflow-gpu and let me know if you face the same error. Also try to use cudnn version 7.6", "@gowthamkpr Like you said, i reinstalled tensorflow....\r\n\r\nThese are the logs i got after running jupyter notebook, look at the ending lines of the logs,\r\n\r\n```\r\nC:\\Users\\adity>jupyter notebook\r\n[I 16:57:19.992 NotebookApp] Serving notebooks from local directory: C:\\Users\\adity\r\n[I 16:57:19.993 NotebookApp] Jupyter Notebook 6.1.4 is running at:\r\n[I 16:57:19.993 NotebookApp] http://localhost:8888/?token=7fe06a373ecb9f2097b748de8859708c2e5ab390f1d4a3ed\r\n[I 16:57:19.994 NotebookApp]  or http://127.0.0.1:8888/?token=7fe06a373ecb9f2097b748de8859708c2e5ab390f1d4a3ed\r\n[I 16:57:19.994 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[C 16:57:20.057 NotebookApp]\r\n\r\n    To access the notebook, open this file in a browser:\r\n        file:///C:/Users/adity/AppData/Roaming/jupyter/runtime/nbserver-5768-open.html\r\n    Or copy and paste one of these URLs:\r\n        http://localhost:8888/?token=7fe06a373ecb9f2097b748de8859708c2e5ab390f1d4a3ed\r\n     or http://127.0.0.1:8888/?token=7fe06a373ecb9f2097b748de8859708c2e5ab390f1d4a3ed\r\n[W 16:57:32.115 NotebookApp] 404 GET /nbextensions/pydeck/extension.js?v=20201013165719 (::1) 35.92ms referer=http://localhost:8888/notebooks/GPU_test.ipynb\r\n[W 16:57:32.522 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20201013165719 (::1) 2.46ms referer=http://localhost:8888/notebooks/GPU_test.ipynb\r\n[I 16:57:32.718 NotebookApp] Kernel started: 426ccc10-8f30-4ddd-afe8-52de224c6493, name: python3\r\n2020-10-13 16:57:38.011301: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-10-13 16:57:48.656762: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-10-13 16:57:49.558492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-10-13 16:57:49.593286: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-10-13 16:57:50.982769: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-10-13 16:57:51.100584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-13 16:57:51.211304: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-10-13 16:57:52.000000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-10-13 16:57:52.709896: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-10-13 16:57:52.728970: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\r\n2020-10-13 16:57:52.754276: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n", "The problem is `dlerror:\u00a0cudnn64_7.dll\u00a0not\u00a0found` che if this library Is in your `%PATH%` dirs", "@bhack I checked in `Environment variables > system variables > Path` ,\r\nI couldn't find it there. Where can i get the path of this file so i can add it in the **PATH**", "https://www.tensorflow.org/install/gpu?hl=en#windows_setup", "@bhack  @gowthamkpr \r\n\ud83c\udf8a\ud83c\udf8a\ud83c\udf87\ud83c\udf87\ud83c\udf89\ud83c\udf89\ud83c\udf89\ud83c\udf89Thanks a lot, I finally fixed this issue. \ud83c\udf8a\ud83c\udf8a\ud83c\udf87\ud83c\udf87\ud83c\udf89\ud83c\udf89\ud83c\udf89\ud83c\udf89\r\n\r\nFor anyone who's troubled with the same issue, here are few fixes that you can try,\r\n**Remember**\r\nBelow approaches will only work if you correctly install tensorflow gpu as per the instructions followed worldwide and still you're unable to use the GPU. \r\n\r\n**Level 1 approaches**\r\n1) Try restarting computer.\r\n2) Try various versions of tensorflow that might work.\r\n\r\n**Level 2 approaches**\r\nRun below code to check status of your GPU after trying each fix below.\r\n```\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\r\n```\r\n1) Check your PATH in environment variables, if these paths are correctly set,\r\n```\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include\r\n```\r\nhttps://www.tensorflow.org/install/gpu?hl=en#windows_setup\r\nNow check again, if **tensorflow** is able to detect your GPU by running above code. If not follow along.\r\n\r\n2) Type `pip list` in your command prompt and uninstall all tensorflow related packages one by one and just reinstall \r\n`tensorflow-gpu` by typing `pip install tensorflow-gpu` .\r\n3) After above step, try running the above code again, if still not able to detect, follow along.\r\n\r\nNow after following above steps, if you try importing tensorflow in your code, you most probably \r\nwill get this error in your command line. ( https://github.com/tensorflow/tensorflow/issues/43785#issuecomment-707677554)\r\nWhich says,\r\n`Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found`\r\n\r\nSo now simply go to this path in your system,\r\n**C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\<version_of_your_CUDA>\\bin** ,\r\n\r\nThere you'll find this file **cudnn64_8.dll** (Or a similar file). \r\nJust change it from **cudnn64_8.dll** to **cudnn64_7.dll** \r\n\r\nSo these are the fixes which I followed and I hope they work for you as well. Good Luck!\r\n\r\nAnd also I'll keep this issue open for some more time. So to discuss, if someone is facing even more issues though following above steps.", "> So now simply go to this path in your system,\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA<version_of_your_CUDA>\\bin ,\r\nThere you'll find this file cudnn64_8.dll (Or a similar file).\r\nJust change it from cudnn64_8.dll to cudnn64_7.dll\r\n\r\nThis Is totally unreccomended! \r\nPlease install TF compatibile Cudnn for the specific versions and don't rename dlls manually.", "@bhack Well, I did install TF compatible CUDNN and I still faced issues.\r\nSo I had to rename the file and that's what fixed this issue and I followed it from here,\r\nhttps://github.com/tensorflow/tensorflow/issues/28223#issuecomment-487874009", "@AdityaNikhil where do you have found the compatible list of Cudnn for TF as It seems you have Cudnn 8.x?\r\nThis Is the requested version and the well tested compatibility Matrix:\r\nhttps://www.tensorflow.org/install/gpu?hl=en#software_requirements\r\nhttps://www.tensorflow.org/install/source?hl=en#gpu", "@bhack \r\nI explained why I had to download Cudnn 8.x version here,\r\nhttps://github.com/tensorflow/tensorflow/issues/43785#issuecomment-703582177", "You could use 7.6.5", "At least you have better hope to have ABI compatibility between 7.4.x and 7.6.x that with the Hazard of renaming Cudnn 8 as It was Cudnn 7.\r\n\r\nSee cuDNN API Compatibility:\r\n> Beginning in cuDNN 7, the binary compatibility of a patch and minor releases is maintained as follows:\r\nAny patch release x.y.z is forward or backward-compatible with applications built against another cuDNN patch release x.y.w (meaning, of the same major and minor version number, but having w!=z).\r\ncuDNN minor releases beginning with cuDNN 7 are binary backward-compatible with applications built against the same or earlier patch release (meaning, an application built against cuDNN 7.x is binary compatible with cuDNN library 7.y, where y>=x).\r\nApplications compiled with a cuDNN version 7.y are not guaranteed to work with 7.x release when y > x.", "@bhack Okay, now since you're saying that renaming dll files is not good, am thinking to download and configure Cudnn 7.6 version but am afraid that the Cudnn 8.x files are already placed in the CUDA folder. \r\nDo you still want me to install Cudnn 7.6 files and place those files in CUDA folder or do you want me to totally reinstall CUDA and Cudnn and place the files properly?", "Renaming dlls file between versions It Is not good in general not only for TF and we cannot reccomend this expecially without ABI compatibility.\n\nHonestly this is now just a machine configuration support request.\n\nPlease close this ticket and ask support at https://stackoverflow.com/questions/tagged/tensorflow\n\nOr at Nvidia support forums  for this type of machine configuration topics.", "Okay thanks @bhack!"]}, {"number": 43784, "title": "embedding_lookup cause ran out of memory ", "body": "I am running the following code to test embedding_lookup.\r\n```python\r\n\r\n# command:\r\n# python3 -m pdb embtest.py --features=1000 --nnz=30 --batch=128\r\n#\r\n# error:\r\n# *** tensorflow.python.framework.errors_impl.ResourceExhaustedError: \r\n#     Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.\r\n#\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nimport os\r\nimport time\r\n\r\ndef measure(params, sp_ids, steps, thr):\r\n  res = tf.nn.embedding_lookup([params[0:thr],params[thr:]], sp_ids, None, name=\"TEST1\")\r\n  print(\"Finished test\")\r\n  return res\r\n\r\nif __name__ == \"__main__\":\r\n\r\n  import sys \r\n  import argparse\r\n\r\n  parser = argparse.ArgumentParser(\r\n     description=\"Measure the performance of tensorflow embeddingbag using tf.nn.embedding\" )\r\n  parser.add_argument(\"--features\", type=int, default=10)\r\n  parser.add_argument(\"--em\", type=int, default=2)\r\n  parser.add_argument(\"--nnz\", type=int, default=2)\r\n  parser.add_argument(\"--batch\", type=int, default=4)\r\n  parser.add_argument(\"--steps\", type=int, default=1)\r\n  parser.add_argument(\"--warmups\", type=int, default=0)\r\n\r\n  args = parser.parse_args()\r\n\r\n  features     = args.features\r\n  em           = args.em\r\n  nnz          = args.nnz\r\n  batch        = args.batch\r\n  steps        = args.steps\r\n  warmups      = args.warmups\r\n\r\n  sp_ids = np.random.randint(0, features, (batch * nnz,))\r\n  res = tf.zeros([batch, em])\r\n\r\n  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"grpc://\"+os.environ[\"TPU_IP\"])\r\n  tf.config.experimental_connect_to_cluster(resolver)\r\n  tf.tpu.experimental.initialize_tpu_system(resolver)\r\n  print(\"   \")\r\n  tpus = tf.config.list_logical_devices('TPU')\r\n  print(\"There are {} tpu logical devices\".format(len(tpus)))\r\n  print(tpus[0])\r\n\r\n  with tf.device('TPU:0'):\r\n    params = tf.random.uniform([features, em])\r\n    res = measure(params, sp_ids, tf.constant(steps), features//2)\r\n \r\n  print(res)\r\n```\r\nBut got the following error:\r\n\r\n```bash\r\nhongzhang@shan-tf1:~$ python  embtest.py --features=1000 --nnz=30 --batch=128\r\nEager execution :  True\r\n2020-10-05 08:23:42.244623: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-05 08:23:42.250601: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\r\n2020-10-05 08:23:42.251595: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c1dde0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-05 08:23:42.251631: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-05 08:23:42.263068: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.178.175.58:8470}\r\n2020-10-05 08:23:42.263113: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:38651}\r\n2020-10-05 08:23:42.279709: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.178.175.58:8470}\r\n2020-10-05 08:23:42.279743: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:38651}\r\n2020-10-05 08:23:42.280176: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:38651\r\n   \r\nThere are 8 tpu logical devices\r\nLogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')\r\nTraceback (most recent call last):\r\n  File \"embtest.py\", line 84, in <module>\r\n    t1 = measure(params, sp_ids, tf.constant(steps), features//2)\r\n  File \"embtest.py\", line 15, in measure\r\n    res = tf.nn.embedding_lookup([params[0:thr],params[thr:]], sp_ids, None, name=\"TEST1\")\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 394, in embedding_lookup_v2\r\n    return embedding_lookup(params, ids, \"div\", name, max_norm=max_norm)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 328, in embedding_lookup\r\n    transform_fn=None)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 246, in _embedding_lookup_and_transform\r\n    ret.set_shape(ids.get_shape().concatenate(element_shape_s))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 1206, in set_shape\r\n    if not self.shape.is_compatible_with(shape):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 1167, in shape\r\n    self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.\r\n\r\nLargest program allocations in vmem:\r\n\r\n  XLA label: register allocator spill slots\r\n  Allocation type: scoped\r\n\r\n  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}\r\n  Allocation type: scoped\r\n\r\n  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}\r\n  Allocation type: scoped\r\n\r\n  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}\r\n  Allocation type: scoped\r\n\r\n  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}\r\n  Allocation type: scoped\r\n\r\n2020-10-05 08:23:59.826142: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.\r\n\r\nLargest program allocations in vmem:\r\n\r\n  XLA label: register allocator spill slots\r\n  Allocation type: scoped\r\n\r\n  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}\r\n  Allocation type: scoped\r\n\r\n  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}\r\n  Allocation type: scoped\r\n\r\n  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}\r\n  Allocation type: scoped\r\n\r\n  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}\r\n  Allocation type: scoped\r\n```\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nos: Linux\r\nos kernel version: #1 SMP Debian 4.19.146-1 (2020-09-17)\r\nos release version: 4.19.0-11-cloud-amd64\r\nos platform: Linux-4.19.0-11-cloud-amd64-x86_64-with-debian-10.6\r\nlinux distribution: ('debian', '10.6', '')\r\nlinux os distribution: ('debian', '10.6', '')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='shan-tf1', release='4.19.0-11-cloud-amd64', version='#1 SMP Debian 4.19.146-1 (2020-09-17)', machine='x86_64', processor='')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n\r\ntf.version.VERSION = 2.3.0-dev20200620\r\ntf.version.GIT_VERSION = v1.12.1-34769-gfd2d4cdb70\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\n\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1731551c4ea1a508bc1be3f50da9bbb7/43784.ipynb). Thanks!", "@shz0116 Can you try with:\r\n```\r\nargs, unknown = parser.parse_known_args()\r\n```\r\nUpdate:\r\nThis was for @amahendrakar Colab reproducibility", "> args, unknown = parser.parse_known_args()\r\n\r\nSame errors.\r\n\r\n", "> > args, unknown = parser.parse_known_args()\n> \n> Same errors.\n> \n> \n\nYes this was only about the Colab example", "As you are testing EmbeddingBag without fusion see:\r\n\r\nhttps://github.com/google/jax/issues/3206\r\nhttps://github.com/tensorflow/addons/issues/2201\r\nhttps://github.com/tensorflow/tensorflow/issues/32675", "@nikitamaia  Can you add again the `awaiting tensorflower` label?", "@shz0116,\r\n\r\nI am able to run your code successfully with **`TF 2.5`** in Google colab with **`TPU runtime`** enabled. Please take a look at [the gist](https://colab.research.google.com/gist/sanatmpa1/b1f0709eb922e090dd9e6c6018043709/43784_san.ipynb) of the working code and let me know if it resolves your issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43784\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43784\">No</a>\n"]}, {"number": 43782, "title": "can't compile tensorflow from source", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14 and 1.13\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip \r\n- Bazel version (if compiling from source): 0.24.1 \r\n- GCC/Compiler version (if compiling from source): gcc-4.8.5 for bazel and gcc-8 for cuda libary\r\n- CUDA/cuDNN version: 10.1 / 7\r\n- GPU model and memory: 1070 ti 8gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nI can't compile from tensorflow 1.14-1.13 source code (haven't tried other 1 tensorflow branches) and 2.4 has been successfully compiled with and without GPU. Originally the error was with newer versions of basel and gcc-8, but after reading the https://www.tensorflow.org/install/source table, I installed gcc-4.8, but the error persisted.\r\n\r\n\r\nI have read the solutions\r\nhttps://github.com/tensorflow/serving/issues/928 but adding the --cxxopt = -std = c ++ 11 flag didn't work for me\r\nI also read that installing libc-ares-dev library might help, but that didn't help either\r\n\r\nI thought the problem might be related to the fact that I am using cuda-10.1 and not 10.0 as in the table. I tried to set up the file without cuda, but I got exactly the same error, so I think this is not the problem\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n**writte config file**\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.8/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-8\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: -march=native -mno-avx\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n\r\n**build command**\r\nbazel build --cxxopt=-std=c++11 --config=cuda  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**output error**\r\nINFO: From ProtoCompile external/com_github_googleapis_googleapis/google/api/http.pb.h:\r\nbazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile external/com_github_googleapis_googleapis/google/rpc/status.pb.h:\r\nbazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From Compiling external/mkl_dnn/src/cpu/simple_concat.cpp:\r\nexternal/mkl_dnn/src/cpu/simple_concat.cpp:98: warning: ignoring #pragma omp simd [-Wunknown-pragmas]\r\n             PRAGMA_OMP_SIMD()\r\n \r\nINFO: From Compiling external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:\r\nexternal/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc: In function 'grpc_error* try_http_parsing(grpc_chttp2_transport*)':\r\nexternal/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:2466:40: warning: 'void* memset(void*, int, size_t)' clearing an object of non-trivial type 'grpc_http_response' {aka 'struct grpc_http_response'}; use assignment or value-initialization instead [-Wclass-memaccess]\r\n   memset(&response, 0, sizeof(response));\r\n                                        ^\r\nIn file included from external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:44:\r\nexternal/grpc/src/core/lib/http/parser.h:71:16: note: 'grpc_http_response' {aka 'struct grpc_http_response'} declared here\r\n typedef struct grpc_http_response {\r\n                ^~~~~~~~~~~~~~~~~~\r\nERROR: /home/dmitry/.cache/bazel/_bazel_dmitry/1319b3f3ad6252a51422db144992f79d/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'\r\n static long gettid(void) { return syscall(__NR_gettid); }\r\n             ^~~~~~\r\nIn file included from /usr/include/unistd.h:1170,\r\n                 from external/grpc/src/core/lib/gpr/log_linux.cc:41:\r\n/usr/include/x86_64-linux-gnu/bits/unistd_ext.h:34:16: note: old declaration '__pid_t gettid()'\r\n extern __pid_t gettid (void) __THROW;\r\n                ^~~~~~\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: warning: 'long int gettid()' defined but not used [-Wunused-function]\r\n static long gettid(void) { return syscall(__NR_gettid); }\r\n             ^~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 281.090s, Critical Path: 57.54s\r\nINFO: 1539 processes: 1539 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n\r\n(there was even more INFO output, but I did not copy everything, there really is a lot)\r\n", "comments": ["@Int37 \r\n\r\nCan you please refer the tested build configurations from [here](https://www.tensorflow.org/install/source#gpu) and see if the problem still persists with tested build configurations. \r\n\r\nCan you upgrade to TF 2.x more recent versions for better performance. Thanks!", "@ravikyram, thanks for your answer, i installed python 3.7 but the problem persists. I already have everything suitable for tested build configuration, except perhaps except for cuda and cudnn, but as I wrote above, the problem remains even if i try to compile without cuda. I also tried to do what is written here https://github.com/tensorflow/tensorflow/issues/33758#issuecomment-547143723, but this did not fix my error.\r\n  I already have tensorflow 2.4 compiled, but I would also like to compile version 1.14 in order to use projects that ask for tensorflow 1.14 or less", "@Int37 \r\n\r\nTF 1.x is not actively supported.In this case please post the issue in [stack overflow.](https://stackoverflow.com/questions/tagged/tensorflow) . Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43782\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43782\">No</a>\n"]}, {"number": 43781, "title": "[MLIR] TFlite contains unused file", "body": "**tensorflow/compiler/mlir/lite/transforms/load_quantization_recipe.cc** is called via \r\n`tf-opt -allow-unregistered-dialect -tfl-load-recipe %s | FileCheck %s`\r\n\r\nHowever, according to [this commit](https://github.com/tensorflow/tensorflow/commit/36167cb04c890f55f8abf1df1d79bf4bf6361d08), \"-allow-unregistered-dialect\" is disabled (the only way to call the file is deprecated).\r\nHence, in **tensorflow/compiler/mlir/lite/transforms/load_quantization_recipe.cc** can be removed as it is not involved in any mlir::tfl passes.", "comments": ["Thanks for reporting this! Will remove the unused dependency.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43781\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43781\">No</a>\n"]}, {"number": 43780, "title": "[INTEL MKL] MKL DNN 0.x code cleanup - Relu ops", "body": "DNN 0.x cleanup of Relu (Elu, Tanh, etc) ops:\r\n\r\n(1) Remove all DNN 0.x related code \r\n\r\n(2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 43779, "title": "Update mobilenet_v3 performance table to correct markdown format", "body": "Updated mobilenet_v3 performance table to correct markdown format which is shown at the official tensorflow api documents\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Large\r\n\r\nThis is current documentation\r\n![image](https://user-images.githubusercontent.com/21135812/95040390-7b082500-070e-11eb-82d8-c7362289d0d1.png)\r\n\r\nAnd this is expected table form written in correct markdown format\r\n![image](https://user-images.githubusercontent.com/21135812/95040423-94a96c80-070e-11eb-83fb-f24d19876165.png)\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43779) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43779) for more info**.\n\n<!-- ok -->"]}, {"number": 43778, "title": "Tensorflow does not work because of Cudnn library issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 and 7.4\r\n- GPU model and memory: RTX 2060 6GB VRAM\r\n\r\n**Describe the problem**\r\nI first installed tf 2.3.0 with CUDA version: 10.1 and CUDnn version: 7.6. I had to work with some old code of mine so I uninstalled the previous CUDA, deleted all the files, and did a fresh install of the CUDA and CUDnn versions mentioned above along with tf 1.15.0. When I try to run the TensorFlow python program I keep getting the same error. It has something to do with source compilation of the previous CUDnn version (7.6.0), I cannot seem to find how to undo this, I tried reinstalling anaconda as well and checked all the environment variable paths of my system, all of them seem to point to the right directories.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI ran the file via command line using python main.py. \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nEPOCH 1 ...\r\n2020-10-04 16:15:46.208813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-10-04 16:15:47.780566: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2020-10-04 16:15:47.793953: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node conv1_1/Conv2D}}]]\r\n         [[Mean/_73]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node conv1_1/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main_test.py\", line 202, in <module>\r\n    run()\r\n  File \"main_test.py\", line 193, in run\r\n    correct_label, keep_prob, learning_rate)\r\n  File \"main_test.py\", line 146, in train_nn\r\n    feed_dict={input_image: image, correct_label: label,                                keep_prob: 0.5, learning_rate: 0.0009})\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node conv1_1/Conv2D (defined at C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\r\n         [[Mean/_73]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node conv1_1/Conv2D (defined at C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'conv1_1/Conv2D':\r\n  File \"main_test.py\", line 202, in <module>\r\n    run()\r\n  File \"main_test.py\", line 184, in run\r\n    input_image, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)\r\n  File \"main_test.py\", line 36, in load_vgg\r\n    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\loader_impl.py\", line 269, in load\r\n    return loader.load(sess, tags, import_scope, **saver_kwargs)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\loader_impl.py\", line 422, in load\r\n    **saver_kwargs)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\loader_impl.py\", line 352, in load_graph\r\n    meta_graph_def, import_scope=import_scope, **saver_kwargs)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1477, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\meta_graph.py\", line 809, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\importer.py\", line 405, in import_graph_def\r\n    producer_op_list=producer_op_list)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\importer.py\", line 517, in _import_graph_def_internal\r\n    _ProcessNewOps(graph)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\importer.py\", line 243, in _ProcessNewOps\r\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3561, in _add_new_tf_operations\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3561, in <listcomp>\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3451, in _create_op_from_tf_operation\r\n    ret = Operation(c_op, self)\r\n  File \"C:\\Users\\manas\\anaconda3\\envs\\tensorflow1-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n", "comments": ["@manasmacherla \r\nWe do not support anaconda build/install issues officially.\r\nPlease refer to these issues and let us know if it helps, else please create issue in respective repo:\r\n#25160 [link](https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in), [link1](https://medium.com/@adwin596/solved-error-failed-to-get-convolution-algorithm-4396982082a7),[link2](https://superuser.com/questions/1397250/cudnn-error-failed-to-get-convolution-algorithm).", "These links did not provide any solution but downgrading to tf 1.14.0 worked. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43778\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43778\">No</a>\n", "@manasmacherla \r\nThanks for the update, the issue #25160 is directed towards downgrading as done,glad that the issue is resolved."]}, {"number": 43777, "title": "Trivial example does not save/restore weights, prints warnings from TensorFlow/Keras implementation", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7, colab\r\n- TensorFlow installed from (source or binary): pip, colab\r\n- TensorFlow version (use command below): 2.3.1 (MacOS), v2.3.0-0-gb36436b087 2.3.0 (colab)\r\n- Python version: 3.8.5 (MacOS) 3.6.9 (colab)\r\n\r\n**Describe the current behavior**\r\nTrained weights are not restored, two warnings about the TensorFlow implementation are printed:\r\n`WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.`\r\n\r\n**Describe the expected behavior**\r\nWeights are restored (test accuracy is consistent with saved model), no warnings printed.\r\nI find several *closed* bug reports from others over that look like the same problems  (lack of saving and the bogus warnings), going back to the 2.0.0 days. It has clearly not been fixed.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_train = x_train.astype('float32') / 255\r\nx_test = x_test.astype('float32') / 255\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(512, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')])\r\nmodel.compile(optimizer='rmsprop',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(\r\n    x_train,\r\n    y_train,\r\n    epochs=8,\r\n    batch_size=128,\r\n    validation_split=0.2)\r\n\r\ntest_loss, test_acc = model.evaluate(x_test, y_test)\r\nprint('model_test_acc:', test_acc)\r\n\r\nmodel.save(\"my_saved_path\")\r\n\r\nsaved_model = tf.keras.models.load_model(\"my_saved_path\")\r\n\r\ntest_loss, test_acc = saved_model.evaluate(x_test, y_test)\r\nprint('saved_model_test_acc:', test_acc)\r\n```", "comments": ["@diyessi \r\n\r\nThis issue has been resolved in TF nightly version. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/db47a93deb4f8ba866bfefe2bd2ccd35/untitled411.ipynb).Please, verify once and close the issue. Thanks!", "Weights are properly restored, and those two warnings are gone, although two new deprecation warnings about the keras implementation have appeared that I hope you will have fixed/muted for 2.4.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43777\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43777\">No</a>\n"]}, {"number": 43776, "title": "OSError: [WinError 126] The specified module could not be found even when following documentation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- windows 10 home single language \r\n- TensorFlow installed from :cmd\r\n- TensorFlow version:1.14\r\n- Python version:3.7.9\r\n- Installed using virtualenv? pip? conda?:pip\r\n\r\n\r\n\r\n**Describe the problem**\r\ni was following the guide to install the imageai documentation  and did this \r\npip3 install tensorflow==1.14\r\npip3 install opencv-python\r\npip3 install keras==2.2.4\r\npip3 install numpy==1.18.5\r\npip3 install imageai --upgrade \r\ni changed from 1.13.1 to 1.14 because it wasnt working before, same thing with numpy from 1.16.1 to 1.18.5(the lasted accepted according to the cmd) \r\nbut i cant run the image ai because every time i try to do something it says \r\nOSError: [WinError 126] The specified module could not be found \r\nheres the full error \r\n**\r\nC:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 83, in <module>\r\n    from tensorflow.python import keras\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\keras\\__init__.py\", line 26, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 24, in <module>\r\n    from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\keras\\utils\\__init__.py\", line 39, in <module>\r\n    from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\keras\\utils\\multi_gpu_utils.py\", line 22, in <module>\r\n    from tensorflow.python.keras.engine.training import Model\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 41, in <module>\r\n    from tensorflow.python.keras.engine import training_arrays\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 40, in <module>\r\n    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\scipy\\__init__.py\", line 136, in <module>\r\n    from . import _distributor_init\r\n  File \"C:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\scipy\\_distributor_init.py\", line 61, in <module>\r\n    WinDLL(os.path.abspath(filename))\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.2544.0_x64__qbz5n2kfra8p0\\lib\\ctypes\\__init__.py\", line 364, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found \r\n** \r\nive put \r\nC:\\Users\\Leoga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\Scripts \r\nto path as well and it still does not work \r\neverytime i import tensorflow or imageai or even keras this error happens\r\nand ive tried following some people with similar problems but none of them worked for me, like downloading a wheel a user found and so on \r\nive been trying to slove this for 2 days please help me\r\n", "comments": ["@leofrio,\r\nTo resolve the `FutureWarning`, please run the below commands\r\n```\r\n!pip install numpy==1.16.4\r\n!pip install tensorflow==1.14\r\n```\r\n\r\nRegarding the `OSError`, please take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have all the compatible software installed. Also, make sure that you have installed Python from [python.org](https://www.python.org/downloads/windows/) and not from the Microsoft Store. Thanks!", "i tried downloading using !pip commands but it said its not recognizable as a command in the cmd, also ive tried using a python download from python.org but everytime i type python on the cmd even if i have a python.org version installed it takes me to the windows store version\r\n", "!pip install numpy 1.16.4\r\n'!pip' is not recognized as an internal or external command,\r\noperable program or batch file.", "> @leofrio,\r\n> To resolve the `FutureWarning`, please run the below commands\r\n> \r\n> ```\r\n> !pip install numpy==1.16.4\r\n> !pip install tensorflow==1.14\r\n> ```\r\n> \r\n> Regarding the `OSError`, please take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have all the compatible software installed. Also, make sure that you have installed Python from [python.org](https://www.python.org/downloads/windows/) and not from the Microsoft Store. Thanks!\r\n\r\nalso,relating to system requirements i do fullfil them", "@leofrio,\r\nSorry, my bad. The exclamation mark is not required while running the command. Please run the below commands and let us know if it helps. \r\n```\r\npip install numpy==1.16.4\r\npip install tensorflow==1.14\r\n```\r\nThanks!", "thank you so much, i realized i was using the windows store version of python , so i just uninstalled that, when i downloaded the the one from python.org i checked the option to put python to path, and it appeared on the cmd, now everything is working as it should even the tensorflow-gpu, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43776\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43776\">No</a>\n"]}, {"number": 43775, "title": "Add metadata in tensorflow lite model", "body": "How to add metadata in object detection model? In documentation only about image classifier model is given.", "comments": ["@krn-sharma \r\nPlease refer to [this link](https://www.tensorflow.org/lite/convert/metadata).\r\nyou may also refer to [link](https://www.tensorflow.org/lite/inference_with_metadata/codegen).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@krn-sharma Write metadata to the object detection model using the following script:\r\n```\r\nfrom tflite_support.metadata_writers import object_detector\r\nfrom tflite_support.metadata_writers import writer_utils\r\nfrom tflite_support import metadata\r\n\r\nObjectDetectorWriter = object_detector.MetadataWriter\r\n_MODEL_PATH = \"detect.tflite\"\r\n_LABEL_FILE = \"labelmap.txt\"\r\n_SAVE_TO_PATH = \"detect_metadata.tflite\"\r\n\r\nwriter = ObjectDetectorWriter.create_for_inference(\r\n    writer_utils.load_file(_MODEL_PATH), [127.5], [127.5], [_LABEL_FILE])\r\nwriter_utils.save_file(writer.populate(), _SAVE_TO_PATH)\r\n\r\n# Verify the populated metadata and associated files.\r\ndisplayer = metadata.MetadataDisplayer.with_model_file(_SAVE_TO_PATH)\r\nprint(\"Metadata populated:\")\r\nprint(displayer.get_metadata_json())\r\nprint(\"Associated file(s) populated:\")\r\nprint(displayer.get_packed_associated_file_list())\r\n```", "error: unpack_from requires a buffer of at least 4 bytes. Please help!!"]}, {"number": 43774, "title": "Fixed inputs not being cast to tuple", "body": "Fix for #43773 \r\n\r\nRNN's __call__  when deserialized from H5 and reconstructed from config will have the layers multiple inputs contained within a list instead of a tuple. This is due to the JSON decoder always creating lists when deserializing array. This change makes sure that its always typecast back to a tuple.", "comments": ["This serializing-deserializing to JSON might also be an issue for other layers, so a better fix might be writing a custom JSONEncoder that has custom logic for handling tuples", "@juulie Can you please check @qlzh727's comments and keep us posted ? Thanks!", "@juulie Any update on this PR? Please. Thanks!", "Hum, it seems that the unit tests are failing for this change. Can u please take a look?\r\n\r\nhttps://source.cloud.google.com/results/invocations/7edd6a34-af6c-4067-91ba-e9b521f8e3f4/targets", "@juulie  Can you please check @qlzh727's comments and keep us posted ? Thanks!", "@juulie  Any update on this PR? Please. Thanks!", "Sorry ive been working on other things and havent had the chance to look into the unit tests, ill get back to it later this month", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 43772, "title": "third_party/gpus/find_cuda_config.py is unable to detect cudnn version and causing `./configure` to fail", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.1.2 (checked out at this release tag)\r\n- Python version: Python 3.8.3\r\n- Installed using virtualenv? pip? conda?: Conda (conda 4.8.5)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0\r\n- CUDA/cuDNN version: CUDA 11.1, cuDNN 8\r\n- GPU model and memory: RTX 3080, 10G\r\n\r\n\r\n**Describe the problem**\r\nWhen I was trying to configure TensorFlow to build from source, I got the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1554, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1437, in main\r\n    if validate_cuda_config(environ_cp):\r\n  File \"./configure.py\", line 1318, in validate_cuda_config\r\n    config = dict(\r\nValueError: dictionary update sequence element #9 has length 1; 2 is required\r\n```\r\n\r\nand after debugging I noticed that the issue is that the cuDNN version isn't detected, and thus `./configure` fails.\r\n\r\n\r\nthird_party/gpus/find_cuda_config.py\r\ncudnn_version.h\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nMy `./configure` sequence:\r\n\r\n```\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.29.1 installed.\r\nPlease specify the location of python. [Default is /home/madz/anaconda3/envs/tensorflow/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/madz/anaconda3/envs/tensorflow/lib/python3.8/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/madz/anaconda3/envs/tensorflow/lib/python3.8/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1554, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1437, in main\r\n    if validate_cuda_config(environ_cp):\r\n  File \"./configure.py\", line 1318, in validate_cuda_config\r\n    config = dict(\r\nValueError: dictionary update sequence element #9 has length 1; 2 is required\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nThen when I tried to run `python third_party/gpus/find_cuda_config.py cuda cudnn`, I got the following output:\r\n\r\n```\r\ncublas_include_dir: /usr/local/cuda/include\r\ncublas_library_dir: /usr/local/cuda/lib64\r\ncuda_binary_dir: /usr/local/cuda/bin\r\ncuda_include_dir: /usr/local/cuda/include\r\ncuda_library_dir: /usr/local/cuda/lib64\r\ncuda_toolkit_path: /usr/local/cuda\r\ncuda_version: 11.1\r\ncudnn_include_dir: /usr/local/cuda/include\r\ncudnn_library_dir: /usr/local/cuda/lib64\r\ncudnn_version: \r\ncupti_include_dir: /usr/local/cuda/include\r\ncupti_library_dir: /usr/local/cuda/lib64\r\nnvvm_library_dir: /usr/local/cuda/nvvm/libdevice\r\n```\r\n\r\nIt turns out for my cuDNN installation, the header file that should be checked is `cudnn_version.h` and not `cudnn.h`. I wanted to make a pull request, but I'm not really sure how to adjust this so that it can use either header file.\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["For whomever is following/fixing this. The issue is at this line: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/cdf2c541c3dd3fb6d03cce4d23fc6c548bc9017c/third_party/gpus/find_cuda_config.py#L347\r\n\r\n\"cudnn.h\" needs to be changed to \"cudnn_version.h\" as @emados says\r\n\r\nHowever, simply doing this as a blank change is likely to break things for other versions. So, more might need to be done. A bit beyond my scope, but figured I'd help out so maybe this can be closed. \r\n\r\n", "This is fixed at head.  We typically don't backport bugfixes to older releases.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43772\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43772\">No</a>\n"]}, {"number": 43771, "title": "Defined layers but received error:  __init__() missing 1 required positional argument: 'layer'", "body": "print(tf.__version__) 2.2.0\r\nprint(keras.__version__) 2.3.0-tf\r\n\r\nMessage:\r\n __init__() missing 1 required positional argument: 'layer'\r\n\r\nI would like to better understand why I get this message with the following definition used in the example:\r\n\r\nhttps://aihub.cloud.google.com/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba\r\n\r\nModel function definition:\r\n\r\ndef model_fn(features, labels, mode, params = params):\r\n    # import the data and unpack the features\r\n    # serving input_fn returns a dict, convert to multivalue obj\r\n    if isinstance(features, dict):\r\n        features = features['words'], features['length']\r\n   \r\n    words, length = features\r\n   \r\n    # Embedding\r\n    embedding = tf.Variable(tf.random.normal([params['max_words'], params['dim']]))\r\n    embedding_lookup_for_x = tf.nn.embedding_lookup(embedding, words)\r\n   \r\n    # LSTM\r\n    lstm_cell_fw = tf.keras.layers.LSTMCell(params['lstm_size'])\r\n    lstm_cell_bw = tf.keras.layers.LSTMCell(params['lstm_size'])\r\n    states, final_state = tf.keras.layers.Bidirectional(\r\n                                        cell_fw = lstm_cell_fw,\r\n                                        cell_bw = lstm_cell_bw,\r\n                                        inputs = embedding_lookup_for_x,\r\n                                        dtype = tf.float32,\r\n                                        time_major = False,\r\n                                        sequence_length = length)\r\n    lstm_out = tf.keras.layers.concat([states[0], states[1]], axis = 2)\r\n\r\n\r\nThe original definition was (no longer supported):\r\n\r\n    # LSTM\r\n#    lstm_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)\r\n#    lstm_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)\r\n#    states, final_state = tf.nn.bidirectional_dynamic_rnn(\r\n#                                        cell_fw = lstm_cell_fw,\r\n#                                        cell_bw = lstm_cell_bw,\r\n#                                        inputs = embedding_lookup_for_x,\r\n#                                        dtype = tf.float32,\r\n#                                        time_major = False,\r\n#                                        sequence_length = length)\r\n#    lstm_out = tf.concat([states[0], states[1]], axis = 2)\r\n\r\nPlease advise.", "comments": ["@bavanhassel \r\nPlease share complete stand alone indented code for us to replicate the issue faced or if possible share a colab gist with the error.\r\nWith respect tot he error faced please refer to these issues and let us know:\r\n[link](https://stackoverflow.com/questions/56106546/typeerror-init-missing-1-required-positional-argument-units), [link1](https://github.com/prodo56/Time-series-analysis-using-RNN/issues/1#issuecomment-325418816)", "\r\n\r\n\r\nimport tensorflow as tf\r\nfrom functools import partial\r\nfrom keras.preprocessing.text import Tokenizer\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nfrom keras.utils import to_categorical\r\nimport pickle\r\n\r\nparams = {\r\n    'dim' : 300,            # dimension of embeddings\r\n    'maximum_steps' : 1000, # number of training steps        \r\n    'lstm_size' : 150,      # dimension of LSTM\r\n    'batch_size' : 25,      # batch size\r\n    'max_words' : 10000,    # maximum number of words to embed\r\n    'padding_size' : 20,    # maximum sentence size\r\n    'num_classes' : 14,     # number of unique classes\r\n    'save_dir' : 'models/' # directory to save hash tables, model weights, etc.\r\n}\r\n\r\n\r\n# In[238]:\r\n\r\n\r\ndef save_obj(directory, obj, name):\r\n    '''Helper function using pickle to save and load objects'''\r\n    with open(directory + name + '.pkl', 'wb+') as f:\r\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\r\n\r\ndef load_obj(directory, name):\r\n    '''Helper function using pickle to save and load objects'''\r\n    with open(directory + name + \".pkl\", \"rb\") as f:\r\n        return pickle.load(f)\r\n   \r\ndef load_data(file = \"data/train.txt\"):\r\n    '''Helper function to load and transform inputs and labels\r\n    included as a separate function due to NER-specific evaluation needs:\r\n        tensorflow does not have multi-class precision/accuracy as a metric\r\n        so data_y is needed to manually calculate evaluations'''\r\n    file = open(file, 'r')\r\n    sentence, labels = [], []\r\n    data_x, data_y = [], []\r\n    for line in file:\r\n        line = line.strip(\"\\n\").split(\"\\t\")\r\n       \r\n        # lines with len > 1 are words\r\n        if len(line) > 1:\r\n            sentence.append(line[1])\r\n            labels.append(line[0][2:]) if len(line[0]) > 1 else labels.append(line[0])\r\n       \r\n        # lins with len == 1 are sentence breaks\r\n        if len(line) == 1:\r\n            data_x.append(' '.join(sentence))\r\n            data_y.append(labels)\r\n            sentence, labels = [], []\r\n    return data_x, data_y\r\n\r\ndef make_tokenizer(file = \"data/train.txt\", params = params):\r\n    ''' In order for one hot encoding of words and labels to work,\r\n    every word and label has to be seen at least once to make a hashing table.\r\n    This function outputs hash tables for the words and the labels\r\n    that can be used to one-hot-encode them in the generator\r\n    '''\r\n    # Load parameters and data\r\n    max_words = params['max_words']\r\n    padding_size = params['padding_size']\r\n    save_dir = params['save_dir']\r\n    data_x, data_y = load_data(file)\r\n           \r\n    # Use the Keras tokenizer API to generate hashing table for data_x\r\n    tokenizer = Tokenizer(num_words = max_words)\r\n   \r\n    tokenizer.fit_on_texts(data_x)\r\n    word_index = tokenizer.word_index\r\n   \r\n    # Flatten data_y and create hashing table using set logic\r\n    data_y_flattened = [item for sublist in data_y for item in sublist]\r\n    data_x_flattened = [item for sublist in data_x for item in sublist]\r\n   \r\n    labels_index = dict([(y, x + 1) for x, y in enumerate(sorted(set(data_y_flattened)))])\r\n    labels = []\r\n    for item in data_y:\r\n        labels.append([labels_index.get(i) for i in item])\r\n    labels_lookup = {v : k for k, v in labels_index.items()} # reverse dictionary for lookup\r\n    # save hash tables to disk for model serving\r\n    for item, name in zip([word_index, labels_index, labels_lookup],\r\n                          [\"word_index\", \"labels_index\", \"labels_lookup\"]):\r\n        save_obj(save_dir, item, name)\r\n    return word_index, labels_index, labels_lookup\r\n\r\nword_index, labels_index, labels_lookup = make_tokenizer()\r\n\r\n\r\n# In[239]:\r\n\r\n\r\nword_index\r\n\r\n\r\n# In[240]:\r\n\r\n\r\nlabels_lookup\r\n\r\n\r\n# In[241]:\r\n\r\n\r\nlabels_index\r\n\r\n\r\n# In[242]:\r\n\r\n\r\ndef generate_batches(file = \"data/train.txt\", params = params, train = True):\r\n    ''' Generate minibatch with dimensions:\r\n    batch_x : (batch_size, max_len)\r\n    lengths : (batch_size,)\r\n    batch_y : (batch_size, num_classes)\r\n   \r\n    file : path to .txt containing training data in BIO format\r\n    '''\r\n   \r\n    batch_size = params['batch_size']\r\n    max_len = params['padding_size']\r\n    save_dir = params['save_dir']\r\n   \r\n    # load hash tables for tokenization\r\n    for item, name in zip([word_index, labels_index, labels_lookup],\r\n                          [\"word_index\", \"labels_index\", \"labels_lookup\"]):\r\n        item = load_obj(save_dir, name)\r\n   \r\n    while True:\r\n        with open(file, 'r') as f:\r\n            batch_x, lengths, batch_y = [], [], []\r\n            words, labels = [], []\r\n            for line in f:\r\n                line = line.strip(\"\\n\").split(\"\\t\")\r\n                # lines with len > 1 are words\r\n                if len(line) > 1:\r\n                    labels.append(line[0][2:]) if len(line[0]) > 1 else labels.append(line[0])\r\n                    words.append(line[1])\r\n\r\n                # lines with len == 1 are breaks between sentences\r\n                if len(line) == 1:\r\n                    words = [word_index.get(x) if x in word_index.keys() else 0 for x in words]\r\n                    labels = [labels_index.get(y) for y in labels]\r\n                    batch_x.append(words)\r\n                    batch_y.append(labels)\r\n                    lengths.append(min(len(words), max_len))\r\n                    words, labels = [], []\r\n\r\n                if len(batch_x) == batch_size:\r\n                    batch_x = pad_sequences(batch_x, maxlen = max_len, value = 0, padding = \"post\")\r\n                    batch_y = pad_sequences(batch_y, maxlen = max_len, value = 0, padding = \"post\")\r\n                    yield (batch_x, lengths), batch_y\r\n                    batch_x, lengths, batch_y = [], [], []\r\n            if train == False:\r\n                break\r\n\r\n\r\n# In[243]:\r\n\r\n\r\n# For model training, we need an input function that will feed a tf.Dataset\r\ndef input_fn(file, params = None, train = True):\r\n    params = params if params is not None else {}\r\n    shapes = (([None, None], [None]), [None, None]) # batch_x, lengths, batch_y shapes\r\n    types = ((tf.int32, tf.int32), tf.int32)        # batch_x, lengths, batch_y data types\r\n   \r\n    generator = partial(generate_batches, file, train = train)\r\n    dataset = tf.data.Dataset.from_generator(generator, types, shapes)\r\n    return dataset\r\n\r\n\r\n# For model serving, we need a serving function that will feed tf.placeholders\r\ndef serving_input_fn():\r\n    words = tf.placeholder(dtype=tf.int32, shape=[None, None], name='words')\r\n    length = tf.placeholder(dtype=tf.int32, shape=[None], name='length')\r\n    receiver_tensors = {'words': words, 'length': length}\r\n    features = {'words': words, 'length': length}\r\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n\r\n\r\n\r\n# In[244]:\r\n\r\n\r\n#import keras\r\n#import keras.utils\r\n#from keras import utils as np_utils\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nprint(tf.__version__)\r\nprint(keras.__version__)\r\n\r\n\r\n# In[245]:\r\n\r\n\r\n\r\n\r\ndef model_fn(features, labels, mode, params = params):\r\n    # import the data and unpack the features\r\n    # serving input_fn returns a dict, convert to multivalue obj\r\n    if isinstance(features, dict):\r\n        features = features['words'], features['length']\r\n   \r\n    words, length = features\r\n   \r\n    # Embedding\r\n    embedding = tf.Variable(tf.random.normal([params['max_words'], params['dim']]))\r\n    embedding_lookup_for_x = tf.nn.embedding_lookup(embedding, words)\r\n   \r\n    # LSTM\r\n    lstm_cell_fw = tf.keras.layers.LSTMCell(params['lstm_size'])\r\n    lstm_cell_bw = tf.keras.layers.LSTMCell(params['lstm_size'])\r\n    states, final_state = tf.keras.layers.Bidirectional(\r\n                                        cell_fw = lstm_cell_fw,\r\n                                        cell_bw = lstm_cell_bw,\r\n                                        inputs = embedding_lookup_for_x,\r\n                                        dtype = tf.float32,\r\n                                        time_major = False,\r\n                                        sequence_length = length)\r\n    lstm_out = tf.keras.layers.concat([states[0], states[1]], axis = 2)\r\n\r\n    \r\n\r\n#tf.compat.v1.keras.layers.LSTMCell\r\n#keras.layers.Bidirectional(keras.layers.RNN(cell))\r\n    \r\n    \r\n    # LSTM\r\n#    lstm_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)\r\n#    lstm_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)\r\n#    states, final_state = tf.nn.bidirectional_dynamic_rnn(\r\n#                                        cell_fw = lstm_cell_fw,\r\n#                                        cell_bw = lstm_cell_bw,\r\n#                                        inputs = embedding_lookup_for_x,\r\n#                                        dtype = tf.float32,\r\n#                                        time_major = False,\r\n#                                        sequence_length = length)\r\n#    lstm_out = tf.concat([states[0], states[1]], axis = 2)\r\n    \r\n       \r\n    # Conditional random fields\r\n    logits = tf.layers.dense(lstm_out, params['num_classes'])\r\n    crf_params = tf.get_variable(\"crf\", [params['num_classes'], params['num_classes']],\r\n                                 dtype=tf.float32)\r\n    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, length)\r\n    training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n   \r\n    # Prediction\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\r\n            'pred_ids': pred_ids,\r\n            'tags': words,\r\n            'length' : length,\r\n        }\r\n        export_outputs = {\r\n          'prediction': tf.estimator.export.PredictOutput(predictions)\r\n      }\r\n       \r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions,\r\n                                          export_outputs=export_outputs)\r\n   \r\n    # Loss functions and optimizers\r\n    log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\r\n        logits, labels, length, crf_params)\r\n   \r\n    loss = tf.reduce_mean(-log_likelihood)\r\n    train_op = tf.train.AdamOptimizer().minimize(\r\n        loss, global_step = tf.train.get_or_create_global_step())\r\n       \r\n    # Training\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        return tf.estimator.EstimatorSpec(mode = mode,\r\n                                           loss = loss,\r\n                                           train_op = train_op)\r\n\r\n\r\n# In[246]:\r\n\r\n\r\n# Spin up the estimator\r\nconfig = tf.estimator.RunConfig()\r\nestimator = tf.estimator.Estimator(model_fn, 'models/model', config, params)\r\n\r\n# Create train spec\r\ntrain_input_fn = partial(input_fn, \"data/train.txt\", params = params)\r\ntrain_spec = tf.estimator.TrainSpec(train_input_fn)\r\n\r\n# Create evaluation spec\r\neval_input_fn = partial(input_fn, \"data/test.txt\", params = params, train = False)\r\neval_spec = tf.estimator.EvalSpec(eval_input_fn)\r\n\r\n\r\n# In[247]:\r\n\r\n\r\nimport time\r\nts = time.time()\r\nestimator.train(input_fn = train_input_fn, max_steps = 1000)\r\nte = time.time()\r\nprint(\"Completed in {} seconds\".format(int(te - ts)))\r\nestimator.export_savedmodel('models/saved_model/', serving_input_fn)\r\n\r\n\r\n# In[130]:\r\n\r\n\r\n# Generate predictions\r\npredictions = estimator.predict(eval_input_fn)\r\n\r\n# Load hash tables and true labels\r\nlabels_index = load_obj(params['save_dir'], \"labels_index\")\r\n_, true = load_data(\"data/test.txt\")\r\n\r\n# Specify which label_index is non-entity\r\ndummy_label = labels_index.get(\"O\")\r\n\r\n# Convert [[string, string], [string, string] ...] to [[int, int], [int, int]]\r\n# with hashing table for label indexes\r\nlabels = []\r\nfor row in true:\r\n    labels.append([labels_index.get(y) for y in row])\r\n   \r\n# Loop through preds, labels and calculate metrics\r\nprecisions, recalls, f1s = [], [], []\r\nfor pred, true in zip(predictions, labels):\r\n    pred = pred['pred_ids'][:pred['length']] # undo pad_sequences\r\n    pred = [x for x in pred if x != dummy_label] # remove preds that aren't entities\r\n    true = np.asarray([x for x in true if x != dummy_label])\r\n    recall = calc_recall(true, pred)\r\n    recalls.append(recall)\r\n    precision = calc_precision(true, pred)\r\n    precisions.append(precision)\r\n    f1s.append(calc_f1(precision, recall))\r\n   \r\nprint(\"Precision: {} \\nRecall: {} \\nF1-score: {}\".format(np.around(np.mean(precisions), 3),\r\n                                                         np.around(np.mean(recalls), 3),\r\n                                                         np.around(np.mean(f1s), 3)))\r\n\r\n\r\n# In[211]:\r\n\r\n\r\nfrom pathlib import Path\r\nfrom tensorflow.contrib import predictor\r\n\r\nLINE = 'did george clooney make a science fiction movie in the 1980s'\r\n\r\n\r\ndef predict(line, export_dir = 'models/saved_model/', params = params):\r\n    # Load hash tables\r\n    word_index = load_obj(params['save_dir'], \"word_index\")\r\n    labels_lookup = load_obj(params['save_dir'], \"labels_lookup\")\r\n   \r\n    # Identify and load model weights\r\n    subdirs = [x for x in Path(export_dir).iterdir()\r\n                   if x.is_dir() and 'temp' not in str(x)]\r\n    latest_model = str(sorted(subdirs)[-1])\r\n    predict_fn = predictor.from_saved_model(latest_model)\r\n               \r\n    # Preprocess sentence input\r\n    line = line.strip().split()\r\n    vector = [word_index.get(x) if x in word_index.keys() else 0 for x in line] # tokenize\r\n    vector[len(vector):20] = [0] * (20 - len(vector)) # pad prediction\r\n       \r\n    # Calculate precision and transform for display\r\n    predictions = predict_fn({'words': [vector], 'length': [len(line)]})\r\n    tags = predictions.get('tags')\r\n    preds = predictions.get('pred_ids')\r\n    for tag, pred in zip(tags, preds):\r\n        tag = [word for word in tag if word != 0] # unpad\r\n        pred = pred[:len(tag)]\r\n        pred = [labels_lookup.get(num) for num in pred] #untokenize\r\n        print(line, \"\\n\", pred)\r\n   \r\npredict(LINE)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n\r\n\r\n\r\n", "@bavanhassel \r\nPlease share the code as a colab gist [google colab gist], indentation errors in this code will delay the replication of issue faced.", "https://colab.research.google.com/gist/bavanhassel/9efd30c15d02f64a5454e45ae0dfc665/try_keras_ner_github.ipynb", "@bavanhassel \r\nPlease share \"ner_dataset.csv\" for us to replicate the issue on our local.", "https://www.kaggle.com/abhinavwalia95/loading-named-entity-recognition-training-dataset\r\n", "@bavanhassel \r\nPlease provide the csv directly, i cannot access kaggle and download it.", "\r\nThis application does not support uploading csv files", "[ner_dataset.txt](https://github.com/tensorflow/tensorflow/files/5333738/ner_dataset.txt)\r\nYou can change the file extension back to csv", "@bavanhassel \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4174830821f613f3c07332258f5069a4/untitled425.ipynb).", "[test.txt](https://github.com/tensorflow/tensorflow/files/5335251/test.txt)\r\n[train.txt](https://github.com/tensorflow/tensorflow/files/5335252/train.txt)\r\n", "@bavanhassel\r\nI ran the code shared and face a different error, please find the [gist here.](https://colab.research.google.com/gist/Saduf2019/cad9490be5a452e5bfbf77b6a28906dc/untitled429.ipynb).\r\nIt is not possible to replicate the issue faced unless all dependencies are shared.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43770, "title": "Wrong paragraph ordering in tutorial", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/classification\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWhen you open the above tutorial, the description of the dataset (Fashion MNIST) is placed at the bottom of the page.\r\nHowever, in [the original .ipynb file](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/classification.ipynb), it is in the first section \"Import the Fashion MNIST dataset\".\r\nIt seems that the order of the paragraph is wrongly rearranged when building the web page.", "comments": ["Thanks for reporting. I'll fix it.", "This is now fixed.", "Thanks!"]}, {"number": 43769, "title": "tensorflow is not running on my miniconda", "body": "I have installed tensorflow using pip install, but it does not run.\r\n\r\nWhen trying to run it, I receive the following error message:\r\n\r\n(base) PS C:\\WINDOWS\\system32> python\r\nPython 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)] :: Ana\r\nconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywra\r\np_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywra\r\np_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywra\r\np_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\__init__.py\", lin\r\ne 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\"\r\n, line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\__init__.py\", lin\r\ne 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\__init__.py\", lin\r\ne 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\importlib\\__init__.py\", line 127, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\__ini\r\nt__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywra\r\np_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywra\r\np_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywra\r\np_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\pywra\r\np_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nPlease help to fix it.\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "Thank you, tensorflow-butler."]}, {"number": 43767, "title": "Build error: concat_lib_gpu.cc(123): error C2908: explicit specialization", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): MSVC 2019\r\n- CUDA/cuDNN version: 10.1 / 7.6.0\r\n- Building for CC: 3.5\r\n- GPU model and memory: RTX 2080 TI\r\n\r\n**Describe the problem**\r\n\r\nBuild error: concat_lib_gpu.cc(123): error C2908: explicit specialization \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\npre set env vars\r\n```\r\nenv['TEST_TMPDIR'] = env['USERPROFILE']\r\nenv['PATH'] = ';'.join ( [str( cwd_path / 'tools' / ('bazel_'+version) ), str(msys_path), str(msys_usr_bin_path), \r\n                                  env['PATH']] )\r\nenv['BAZEL_SH'] = str ( msys_usr_bin_path / 'bash.exe' )\r\nenv['BAZEL_VC'] = env['VCINSTALLDIR']\r\nenv['BAZEL_POWERSHELL'] = r'C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe'\r\nenv['MSYS_NO_PATHCONV']  = '1'\r\nenv['MSYS2_ARG_CONV_EXCL']  = '\"*\"'\r\n\r\nenv['TF_NEED_CUDA']='1'\r\n\r\nenv['CUDA_TOOLKIT_PATH'] = str(cuda_toolkit_path).replace ('\\\\','\\\\\\\\' )\r\nenv['TF_CUDA_PATHS'] = env['CUDA_TOOLKIT_PATH']\r\nenv['CUDNN_INSTALL_PATH'] = str (cudnn_install_path).replace ('\\\\','\\\\\\\\' )\r\nenv['TF_CUDA_VERSION'] = cuda_ver\r\nenv['TF_CUDNN_VERSION'] = cudnn_ver[0]\r\nenv['TF_CUDA_COMPUTE_CAPABILITIES'] = cc\r\nenv['CUDA_BIN_PATH'] = str ( cuda_toolkit_path / 'bin' )\r\nenv['CUDNN_INCLUDE'] = str ( cudnn_install_path / 'include' )\r\nenv['CUDNN_LIBRARY'] = str ( cudnn_install_path / 'lib' / 'x64' )\r\nenv['CUDNN_ROOT'] = str ( cudnn_install_path )\r\n\r\nenv['PATH'] = ';'.join ( [ str(cudnn_install_path / 'bin'), env['CUDA_BIN_PATH'], \r\n       str(cuda_toolkit_path / 'extras' / 'CUPTI' / 'libx64'), env['PATH'] ] )\r\n\r\nenv['USE_DEFAULT_PYTHON_LIB_PATH']  = '1'\r\nenv['TF_NEED_TENSORRT']  = '0'\r\nenv['TF_NEED_IGNITE']  = '0'\r\nenv['TF_NEED_ROCM']  = '0'\r\nenv['TF_ENABLE_XLA']  = '0'\r\nenv['TF_OVERRIDE_EIGEN_STRONG_INLINE']  = '0'\r\nenv['TF_SET_ANDROID_WORKSPACE']  = '0'\r\nenv['CC_OPT_FLAGS'] = \"/W0 /DTHRUST_IGNORE_CUB_VERSION_CHECK\"\r\n```\r\n\r\nrun commands\r\n```\r\nbazel clean\r\npython configure.py\r\nbazel build --config=cuda --config=v2 --copt=-nvcc_options=disable-warnings \r\n      --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nerror log:\r\n\r\n```\r\nERROR: D:/developpython/tools/tensorflow/tensorflow-master/tensorflow/core/kerne\r\nls/BUILD:258:1: C++ compilation of rule '//tensorflow/core/kernels:concat_lib' f\r\nailed (Exit 2): python.exe failed: error executing command\r\n  cd D:/developpython/_internal/__z/u/_bazel_administrator/wtpap475/execroot/org\r\n_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=D:/DevelopPython/tools/CUDA/v10.1\r\n    SET CUDNN_INSTALL_PATH=D:\\DevelopPython\\tools\\CUDNN\\10.1-7.6.5\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\r\n\\Tools\\MSVC\\14.27.29110\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6\r\n.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C\r\n:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Fil\r\nes (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows\r\nKits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\includ\r\ne\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Too\r\nls\\MSVC\\14.27.29110\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\l\r\nib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Pr\r\nogram Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Commo\r\nn7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Vi\r\nsual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64;C:\\Program\r\nFiles (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\\r\nProgram Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExt\r\nensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\r\n\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C\r\n:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\r\n\\Roslyn;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Too\r\nls\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program F\r\niles (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studi\r\no\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.\r\n30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\r\n;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C\r\n:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=D:/DevelopPython/python/3.6.8/python.exe\r\n    SET PYTHON_LIB_PATH=D:/DevelopPython/python/3.6.8/Lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=D:\\DevelopPython\\_internal\\__z\\t\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5\r\n    SET TF_CUDA_PATHS=D:\\DevelopPython\\tools\\CUDA\\v10.1\r\n    SET TF_CUDA_VERSION=10.1\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=D:\\DevelopPython\\_internal\\__z\\t\r\n  D:/DevelopPython/python/3.6.8/python.exe -B external/local_config_cuda/crossto\r\nol/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_\r\nWINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STD\r\nEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /w\r\nd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibaz\r\nel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ib\r\nazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-o\r\nut/x64_windows-opt/bin/external/nsync /Iexternal/gif /Ibazel-out/x64_windows-opt\r\n/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/exter\r\nnal/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin\r\n/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x\r\n64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archiv\r\ne /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ib\r\nazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x\r\n64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-\r\nopt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/b\r\nin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/\r\nexternal/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/ext\r\nernal/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows\r\n-opt/bin/external/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external\r\n/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_w\r\nindows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers\r\n /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includ\r\nes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_\r\ncuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/externa\r\nl/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64\r\n_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_header\r\ns_virtual /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eige\r\nn_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync\r\n/public /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/g\r\nif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_g\r\noogle_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/\r\nsrc /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/far\r\nmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib\r\n/Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_con\r\nversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/extern\r\nal/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel\r\n-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal\r\n/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/\r\nlocal_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cufft/in\r\nclude /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/incl\r\nude /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt\r\n/bin/external/local_config_cuda/cuda/curand/include /DTF_USE_SNAPPY /DEIGEN_MPL2\r\n_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN\r\n_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN\r\n_AND_MEAN -DNOGDI /experimental:preprocessor -nvcc_options=disable-warnings /std\r\n:c++14 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_MONOLITHIC_BUILD /DP\r\nLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_A\r\nVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY\r\n-DGOOGLE_CUDA=1 /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/c\r\noncat_lib/concat_lib_gpu.obj /c tensorflow/core/kernels/concat_lib_gpu.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been de\r\nprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental\r\n:preprocessor'\r\ntensorflow/core/kernels/concat_lib_gpu.cc(123): error C2908: explicit specializa\r\ntion; 'void tensorflow::ConcatGPU<tensorflow::uint16>(tensorflow::OpKernelContex\r\nt *,const std::vector<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1\r\n,IndexType>,16,Eigen::MakePointer>,std::default_delete<Eigen::TensorMap<Eigen::T\r\nensor<const T,2,1,IndexType>,16,Eigen::MakePointer>>>,std::allocator<std::unique\r\n_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer\r\n>,std::default_delete<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,E\r\nigen::MakePointer>>>>> &,tensorflow::Tensor *,Eigen::TensorMap<Eigen::Tensor<T,2\r\n,1,IndexType>,16,Eigen::MakePointer> *)' has already been instantiated\r\n        with\r\n        [\r\n            T=tensorflow::uint16,\r\n            IndexType=Eigen::DenseIndex\r\n        ]\r\ntensorflow/core/kernels/concat_lib_gpu.cc(124): error C2908: explicit specializa\r\ntion; 'void tensorflow::ConcatGPU<tensorflow::int8>(tensorflow::OpKernelContext\r\n*,const std::vector<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,I\r\nndexType>,16,Eigen::MakePointer>,std::default_delete<Eigen::TensorMap<Eigen::Ten\r\nsor<const T,2,1,IndexType>,16,Eigen::MakePointer>>>,std::allocator<std::unique_p\r\ntr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer>,\r\nstd::default_delete<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eig\r\nen::MakePointer>>>>> &,tensorflow::Tensor *,Eigen::TensorMap<Eigen::Tensor<T,2,1\r\n,IndexType>,16,Eigen::MakePointer> *)' has already been instantiated\r\n        with\r\n        [\r\n            T=tensorflow::int8,\r\n            IndexType=Eigen::DenseIndex\r\n        ]\r\ntensorflow/core/kernels/concat_lib_gpu.cc(131): error C2908: explicit specializa\r\ntion; 'void tensorflow::ConcatGPU<tensorflow::uint32>(tensorflow::OpKernelContex\r\nt *,const std::vector<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1\r\n,IndexType>,16,Eigen::MakePointer>,std::default_delete<Eigen::TensorMap<Eigen::T\r\nensor<const T,2,1,IndexType>,16,Eigen::MakePointer>>>,std::allocator<std::unique\r\n_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer\r\n>,std::default_delete<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,E\r\nigen::MakePointer>>>>> &,tensorflow::Tensor *,Eigen::TensorMap<Eigen::Tensor<T,2\r\n,1,IndexType>,16,Eigen::MakePointer> *)' has already been instantiated\r\n        with\r\n        [\r\n            T=tensorflow::uint32,\r\n            IndexType=Eigen::DenseIndex\r\n        ]\r\ntensorflow/core/kernels/concat_lib_gpu.cc(132): error C2908: explicit specializa\r\ntion; 'void tensorflow::ConcatGPU<tensorflow::uint64>(tensorflow::OpKernelContex\r\nt *,const std::vector<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1\r\n,IndexType>,16,Eigen::MakePointer>,std::default_delete<Eigen::TensorMap<Eigen::T\r\nensor<const T,2,1,IndexType>,16,Eigen::MakePointer>>>,std::allocator<std::unique\r\n_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer\r\n>,std::default_delete<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,E\r\nigen::MakePointer>>>>> &,tensorflow::Tensor *,Eigen::TensorMap<Eigen::Tensor<T,2\r\n,1,IndexType>,16,Eigen::MakePointer> *)' has already been instantiated\r\n        with\r\n        [\r\n            T=tensorflow::uint64,\r\n            IndexType=Eigen::DenseIndex\r\n        ]\r\n```\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43767\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43767\">No</a>\n"]}, {"number": 43766, "title": "Error when creating model with LSTM layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 1909 OS build 18363.1016\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI am trying to run the example from here:\r\nhttps://www.tensorflow.org/guide/ragged_tensor\r\n\r\nAfter adding LSTM layer to the model I get an error:\r\n\r\n> NotImplementedError: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n\r\n**Describe the expected behavior**\r\n\r\nLSTM layer should be added with no error.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nkeras_model = tf.keras.Sequential([\r\n    tf.keras.layers.Embedding(1000, 16),\r\n    tf.keras.layers.LSTM(32),\r\n])\r\n\r\n```\r\n\r\n**Other info / logs**\r\nCall stack:\r\n\r\n>\tTensor.__array__ in ops line 848\tPython\r\n \t_wrapreduction in fromnumeric line 90\tPython\r\n \tprod in fromnumeric line 2962\tPython\r\n \tprod in C:\\Users\\FA\\Google Drive\\Colab Notebooks\\PoetryTransformer\\Accentizer\\<__array_function__ internals> line 6\tPython\r\n \t_constant_if_small in array_ops line 2732\tPython\r\n \tzeros in array_ops line 2794\tPython\r\n \twrapped in _tag_zeros_tensor in array_ops line 2747\tPython\r\n \twrapper in add_dispatch_support in dispatch line 201\tPython\r\n \tcreate_zeros in _generate_zero_filled_state in recurrent line 2981\tPython\r\n \t<listcomp> in nest line 635\tPython\r\n \tmap_structure in nest line 635\tPython\r\n \t_generate_zero_filled_state in recurrent line 2984\tPython\r\n \t_generate_zero_filled_state_for_cell in recurrent line 2968\tPython\r\n \tLSTMCell.get_initial_state in recurrent line 2524\tPython\r\n \tRNN.get_initial_state in recurrent line 646\tPython\r\n \tRNN._process_inputs in recurrent line 862\tPython\r\n \tLSTM.call in recurrent_v2 line 1108\tPython\r\n \tLayer._functional_construction_call in base_layer line 1117\tPython\r\n \tLayer.__call__ in base_layer line 926\tPython\r\n \tRNN.__call__ in recurrent line 663\tPython\r\n \tSequential.add in sequential line 221\tPython\r\n \t_method_wrapper in no_automatic_dependency_tracking in base line 457\tPython\r\n \tSequential.__init__ in sequential line 142\tPython\r\n \t_method_wrapper in no_automatic_dependency_tracking in base line 457\tPython\r\n \tragged module line 5\tPython\r\n", "comments": ["I have found out that the error happens only when I run in Visual Studio. So I think that this issue relates to Microsoft.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43766\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43766\">No</a>\n"]}]