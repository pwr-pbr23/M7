[{"number": 23086, "title": "Float16 support for log_uniform_candidate_sampler/uniform_candidate_sampler", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.11.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nFloat16 architectures are promising for significantly increase the model size / training speed using the same amount of memory. So I am requesting Float16 support for `log_uniform_candidate_sampler` and `uniform_candidate_sampler`. Currently, in the returned tuple,  `true_expected_count` and `sampled_expected_count` as returned as float32. This causes issues when using `sampled_softmax_loss` since everything needs to be in the same datatype. \r\n\r\nI was able to find a hack work around\r\n\r\nhttps://stackoverflow.com/questions/52711895/how-to-run-define-tensorflow-graph-were-all-variables-are-in-float16-instead-ins\r\n\r\nHowever, this actually increases the training speed; I am not sure why (more details below)\r\n\r\nI have tried to develop a replacement function, however, I was unable to interpret the machine generated code where `log_uniform_candidate_sampler` and `uniform_candidate_sampler` are defined. I describe my attempt to interpret the code here\r\n\r\nhttps://stackoverflow.com/questions/52734709/how-to-interpret-this-machine-generated-python-code\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, most likely the best way is to have an optional arg for what datatype the user would like use, ie datatype=tf.float16. Otherwise defaults to float32\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThose who are developing embedding items (ie Word2Vec) using float16 embeddings. \r\n\r\n**Any Other info.**\r\n\r\n>Have I written custom code\r\n\r\nHere is the code I used, for convenience it's in a Google Colab notebook so you can just run the code in the notebook. \r\n\r\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\r\n\r\nI wrote a custom function to replace `true_expected_count` and `sampled_expected_count` generated from `uniform_candidate_sampler` because I wanted to isolate the issue if the slow-down is due to using float16 instead of float32. The specific code to replace those variables is  \r\n\r\n```\r\n    LogUniformCandidateSampler = namedtuple(\"namedtuple\", [\"sampled_candidates\", \"true_expected_count\", \"sampled_expected_count\"]) \r\n    sampled_values = tf.nn.uniform_candidate_sampler(\r\n          true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,\r\n          num_true=1,\r\n          unique=True,\r\n          range_max=vocabulary_size,\r\n          seed=None)\r\n    \r\n    ray= tf.fill( dims =[num_sampled] ,  value= (num_sampled/vocabulary_size)   ) \r\n    fillvalue2 = tf.cast(ray, testDataType)\r\n\r\n    jay= tf.fill( dims =[ batch_size  ,1] ,  value= (num_sampled/vocabulary_size)   ) \r\n    fillvalue = tf.cast(jay, testDataType)\r\n\r\n    true_expected_count2 = tf.get_variable( 'this1' ,dtype=testDataType, initializer = fillvalue  )\r\n    sampled_expected_count2 = tf.get_variable( 'this2' , dtype=testDataType, initializer = fillvalue2   )\r\n\r\n    sampled_value_16 = LogUniformCandidateSampler(\r\n        sampled_values.sampled_candidates,\r\n        true_expected_count2,\r\n        sampled_expected_count2 )\r\n```\r\n\r\nYou can switch between float16 and float32 by changing the `testDataType` variable at the top. You can also change the embedding size at the top by changing the `embedding_size` variable, which I recommend for switching between TPU mode and GPU mode in Google Colab. \r\n\r\nAfter you select which datatype and embedding size you want to use, select 'Runtime' -> 'Run all', and you can check the time it takes between steps at the training code at the bottom. \r\n\r\nIn GPU mode, 100 steps takes about 11 seconds for float32, but takes 15 seconds for float16. I imagine float16 would have been faster. For GPU mode I used an embedding size of 52 (highest embedding size I could use before the system would crash during training for float32).\r\n\r\nIn TPU mode, 100 steps takes about 10 seconds for float32, but takes 6 minutes 5 seconds for float16. So 36x slower for float16 than float32. For TPU mode I used an embedding size of 154 (highest embedding size I could use before the system would crash during training for float32).\r\n\r\n>OS Platform and Distribution\r\n\r\nPython 3. Not sure about the rest, whatever Google Colab uses. \r\n\r\n>TensorFlow installed from\r\n\r\nNot sure, whatever Google Colab uses. \r\n\r\n>CUDA/cuDNN version\r\n\r\nNot sure, whatever Google Colab uses. \r\n\r\n>GPU model and memory\r\n\r\nI believe Colab for GPU uses a Tesla k80, and for TPU they are using V2. \r\n\r\n>Exact command to reproduce\r\n\r\nSame code as linked above \r\n\r\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\r\n\r\n>Mobile device\r\n\r\nN/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Here it is. I'll also edit this into the original post. \r\n\r\n>Have I written custom code\r\n\r\nHere is the code I used, for convenience it's in a Google Colab notebook so you can just run the code in the notebook. \r\n\r\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\r\n\r\nI wrote a custom function to replace `true_expected_count` and `sampled_expected_count` generated from `uniform_candidate_sampler` because I wanted to isolate the issue if the slow down is due to using float16 instead of float32. The specific code to replace those variables is\r\n\r\n```\r\n    LogUniformCandidateSampler = namedtuple(\"namedtuple\", [\"sampled_candidates\", \"true_expected_count\", \"sampled_expected_count\"]) \r\n    sampled_values = tf.nn.uniform_candidate_sampler(\r\n          true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,\r\n          num_true=1,\r\n          unique=True,\r\n          range_max=vocabulary_size,\r\n          seed=None)\r\n    \r\n    ray= tf.fill( dims =[num_sampled] ,  value= (num_sampled/vocabulary_size)   ) \r\n    fillvalue2 = tf.cast(ray, testDataType)\r\n\r\n    jay= tf.fill( dims =[ batch_size  ,1] ,  value= (num_sampled/vocabulary_size)   ) \r\n    fillvalue = tf.cast(jay, testDataType)\r\n\r\n    true_expected_count2 = tf.get_variable( 'this1' ,dtype=testDataType, initializer = fillvalue  )\r\n    sampled_expected_count2 = tf.get_variable( 'this2' , dtype=testDataType, initializer = fillvalue2   )\r\n\r\n    sampled_value_16 = LogUniformCandidateSampler(\r\n        sampled_values.sampled_candidates,\r\n        true_expected_count2,\r\n        sampled_expected_count2 )\r\n```\r\n\r\nYou can switch between float16 and float32 by changing the `testDataType` variable at the top. You can also change the embedding size at the top by changing the `embedding_size` variable, which I recommend for switching between TPU mode and GPU mode in Google Colab. \r\n\r\nAfter you select which datatype and embedding size you want to use, select 'Runtime' -> 'Run all', and you can check the time it takes between steps at the training code at the bottom. \r\n\r\nIn GPU mode, 100 steps takes about 11 seconds for float32, but takes 15 seconds for float16. I imagine float16 would have been faster. For GPU mode I used an embedding size of 52 (highest embedding size I could use before the system would crash during training for float32).\r\n\r\nIn TPU mode, 100 steps takes about 10 seconds for float32, but takes 6 minutes 5 seconds for float16. So 36x slower for float16 than float32. For TPU mode I used an embedding size of 154 (highest embedding size I could use before the system would crash during training for float32).\r\n\r\n>OS Platform and Distribution\r\n\r\nPython 3. Not sure about the rest, whatever Google Colab uses. \r\n\r\n>TensorFlow installed from\r\n\r\nNot sure, whatever Google Colab uses. \r\n\r\n>CUDA/cuDNN version\r\n\r\nNot sure, whatever Google Colab uses. \r\n\r\n>GPU model and memory\r\n\r\nI believe Colab for GPU uses a Tesla k80, and for TPU they are using V2. \r\n\r\n>Exact command to reproduce\r\n\r\nSame code as linked above \r\n\r\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\r\n\r\n>Mobile device\r\n\r\nN/A", "@Santosh-Gupta,\r\nSorry for the delayed response. Can you please look into [Mixed Precision Guide](https://www.tensorflow.org/guide/mixed_precision) and check if it helps? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23084, "title": "Remove TF_NCCL_VERSION from Dockerfile.gpu.ppc64le", "body": "Now NCCL2 is available in open source and TensorFlow is building\r\nwith it as the default there is no need to force the version to 1", "comments": []}, {"number": 23083, "title": "Documentation for learning_rate_power in the FTRL optimizer", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n**System information**\r\n- TensorFlow version: 1.6.0\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer\r\n\r\n**Describe the documentation issue**\r\nThis is probably a silly, small issue, but the docs for the FTRL optimizer do not describe what `learning_rate_power` and how it interacts with `learning_rate`.  I tried to figure it out based on the source, but couldn't actually find [where learning_rate_power is used in the code.](https://github.com/tensorflow/tensorflow/blob/1c7bc899dbb86cec70a2c11207a9ce8acf30c13b/tensorflow/python/training/ftrl.py#L41)\r\n\r\nThe `l2_shrinkage_regularization_strength` has a very detailed explanation and an equation.  Something similar for `learning_rate_power` would be nice.", "comments": ["There's a fix now in review. \r\n\r\nSee section 3.1 in the [paper](https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf). ", "Thanks!", "Thank you @MarkDaoust. Please post here once the fix is completed."]}, {"number": 23082, "title": "Error while calling TFLite interpreter (Similar to #21574)", "body": "**Tensorflow Version: 1.11.0\r\nPython Version: 3.4 (Also tried with 2.7)**\r\n\r\nI tried to install Tensorflow **using PIP install** on my Raspberry Pi3 B+ (Raspbian Stretch - June 2018 Version) and when I tried to run the sample [label_image.py ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/python/label_image.py)example with TFLite model file I am getting this error -\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"label_image.py\", line 37, in <module>\r\n    interpreter = tf.contrib.lite.Interpreter(model_path=args.model_file)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/lite/python/interpreter.py\", line 52, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.4/importlib/__init__.py\", line 109, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 2254, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\r\n  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n  File \"/usr/lib/python3.4/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\nImportError: /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf\r\n\r\n```\r\n\r\nI also tried another way and build the **Cross Compile Package** using latest Tensorflow code from master branch and install the package on pi. After running the [same example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/python/label_image.py) I am facing this error -\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"label_image.py\", line 37, in <module>\r\n    interpreter = tf.contrib.lite.Interpreter(model_path=args.model_file)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.4/importlib/__init__.py\", line 109, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 2254, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\r\n  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py\", line 48, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/distribute/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/tpu/__init__.py\", line 69, in <module>\r\n    from tensorflow.contrib.tpu.python.ops.tpu_ops import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py\", line 39, in <module>\r\n    resource_loader.get_path_to_datafile(\"_tpu_ops.so\"))\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid name: \r\n\r\nAn op that loads optimization parameters into HBM for embedding. Must be\r\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\r\nembedding table configuration. For example, this op is used to install\r\nparameters that are loaded from a checkpoint before a training loop is\r\nexecuted.\r\n```\r\nSo none of the provided methods are working for me.", "comments": ["I think it will be better if you create a new issue expressing the problem you are facing. The reason for that is the previous issue was created by different user which may be addressed by TensorFlow team or else user must have found a workaround himself. By opening new issue we can focus on your specific problem and will help us to keep track of issues we are committing to. Thanks!  ", "@ymodak Thanks for your quick response. I have updated the issue with the details as per your suggestion.", "@saurabh-kachhia \r\nI resolved it by native build on RaspberryPi (Raspbian Stretch).\r\nThe official pip package is broken.\r\n\r\n**For Python2.7 or Python3.5**\r\nhttps://github.com/PINTO0309/Tensorflow-bin.git\r\n\r\nI tried implementing \"Tenforflow Lite UNet\" to RaspberryPi.\r\nhttps://github.com/PINTO0309/TensorflowLite-UNet.git\r\n", "@saurabh-kachhia  Please post your findings after trying PINTO0309's suggestion. Thanks!\r\n\r\n> @saurabh-kachhia\r\n> I resolved it by native build on RaspberryPi (Raspbian Stretch).\r\n> The official pip package is broken.\r\n> \r\n> **For Python2.7 or Python3.5**\r\n> https://github.com/PINTO0309/Tensorflow-bin.git\r\n> \r\n> I tried implementing \"Tenforflow Lite UNet\" to RaspberryPi.\r\n> https://github.com/PINTO0309/TensorflowLite-UNet.git\r\n\r\n", "@PINTO0309 Thanks a lot for sharing .whl files. It fixed my issue. \r\n\r\n@ymodak Yes, it resolved the issue, but the official build and cross compiling of the package are still broken. Is there any update on it?", "Just ran into this error as well, @PINTO0309 solution fixes it... thanks!", "Cross compiling to the raspberry pi tends to be pretty tricky to get right and is somewhat fragile. Unfortunately, it is quite difficult to build Tensorflow natively in a reasonable amount of time. We'll see if we can work on getting this resolved.", "@saurabh-kachhia \r\n\r\nPrebuilt binary for TensorflowLite's standalone installer. For RaspberryPi.\r\nEven without cross-compiling, it was completed in about 30 minutes by building with RaspberryPi alone.\r\nIt seems to be still improving, but I am looking forward to it.\r\n\r\n**Tensorflow Lite v1.12.0 rc0, Binary Size=1.1MB, Build Time=30min**\r\n**https://github.com/PINTO0309/TensorflowLite-bin.git**\r\n\r\n**Official tutorial**\r\n**https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package**\r\n\r\n**Official Commits**\r\n**[Changed Files](https://github.com/tensorflow/tensorflow/commit/5847293aeb9ab45a02c4231c40569a15bd4541c6)**\r\n\r\n@aselle \r\nPython2.7 ---> Unfortunately, It becomes error if Tensorflow is not installed.\r\n```\r\npi@raspberrypi:~/TensorflowLite-UNet $ python tflite_test.py \r\nTraceback (most recent call last):\r\n  File \"tflite_test.py\", line 4, in <module>\r\n    import tflite_runtime as tflr\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflite_runtime/__init__.py\", line 1, in <module>\r\n    import tflite_runtime.lite.interpreter\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflite_runtime/lite/__init__.py\", line 1, in <module>\r\n    from interpreter import Interpreter as Interpreter\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflite_runtime/lite/interpreter.py\", line 22, in <module>\r\n    from tensorflow.python.util.lazy_loader import LazyLoader\r\nImportError: No module named tensorflow.python.util.lazy_loader\r\n```\r\nPython3.5 ---> The `__init__.py` description is inappropriate and results in an `import error`.\r\n```\r\npi@raspberrypi:~/TensorflowLite-UNet $ python3 tflite_test.py \r\nTraceback (most recent call last):\r\n  File \"tflite_test.py\", line 4, in <module>\r\n    import tflite_runtime as tflr\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/__init__.py\", line 1, in <module>\r\n    import tflite_runtime.lite.interpreter\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/lite/__init__.py\", line 1, in <module>\r\n    from interpreter import Interpreter as Interpreter\r\nImportError: No module named 'interpreter'\r\n```", "@PINTO0309, when build the newest version of tf-lite runtime python package, I have the same problem like you. \r\nSo the tf lite runtime whl of your repo: https://github.com/PINTO0309/TensorflowLite-bin.git can work or not ? ", "@brunolinux \r\n\r\nNo. I am waiting for the aselle adjustment work to end.\r\nSince my repository is incomplete, please do not use.", "Same issue as OP on my custom tflite model.\r\n\r\nPi 1, BCM2835 without NEON, tf-1.11 form [piwheels](https://www.piwheels.hostedpi.com/), works.\r\n\r\nPi 2, BCM2836 with NEON, tf-1.11 from [piwheels](https://www.piwheels.hostedpi.com/), not work. tf-1.11 from [PINTO0309's Tensorflow-bin](https://github.com/PINTO0309/Tensorflow-bin) works.", "I'm the maintainer of the piwheels.org project which hosts the Raspberry Pi TF wheels (built by the TF team).\r\n\r\nPlease note that the neon instructions differ between 2836 and 2837, therefore the armv7l wheel won't work on 2836. The original Pi 2 was 2836 but after the Pi 3 came out, any Pi 2s made had the 2837 on them. So there's only a limited supply of 2836 in the world. However, the armv6l wheels will work on 2836 (and 2837).\r\n\r\nTo install the armv6l wheels on a Pi 2/3:\r\n\r\n```bash\r\nwget https://www.piwheels.org/simple/tensorflow/tensorflow-1.11.0-cp35-none-linux_armv6l.whl\r\nmv tensorflow-1.11.0-cp35-none-linux_armv6l.whl tensorflow-1.11.0-cp35-none-linux_armv7l.whl\r\nsudo pip3 install tensorflow-1.11.0-cp35-none-linux_armv7l.whl\r\n```", "@PINTO0309 @aselle This still doesn't seem to work, latest pip packages still broken on RPi 3b+. Any updates?", "@Mark84 \r\nInformally, I generated the wheel package for Tensorflow v2.0.0-alpha.\r\n**https://github.com/PINTO0309/Tensorflow-bin**\r\n**tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl**\r\n\r\nI have not built v1.12.0 and v1.13.1.", "@PINTO0309 Didn\u2019t try the alpha, but did try the 1.11 version, but I need the tensorflow.lite package which isn\u2019t included as it\u2019s broken still?", "@Mark84 \r\nThe official package is broken, but the package I created works fine.\r\n```console\r\n$ sudo apt-get install -y libhdf5-dev libc-ares-dev libeigen3-dev\r\n$ sudo pip3 install keras_applications==1.0.7 --no-deps\r\n$ sudo pip3 install keras_preprocessing==1.0.9 --no-deps\r\n$ sudo pip3 install h5py==2.9.0\r\n$ sudo apt-get install -y openmpi-bin libopenmpi-dev\r\n$ sudo pip3 uninstall tensorflow\r\n$ wget -O tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl https://github.com/PINTO0309/Tensorflow-bin/raw/master/tensorflow-1.11.0-cp35-cp35m-linux_armv7l_jemalloc_multithread.whl\r\n$ sudo pip3 install tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl\r\n\r\n\u3010Required\u3011 Restart the terminal.\r\n```", "@PINTO0309 \r\nFollowing those steps exactly and then\r\nUsing example label_image.py : from : tensorflow/tensorflow/lite/examples/python/label_image.py\r\n      \r\n(.venv):~/tensor_example $ python3 label_image.py \r\nTraceback (most recent call last):\r\n  File \"label_image.py\", line 26, in <module>\r\n    from tensorflow.lite.python import interpreter as interpreter_wrapper\r\nImportError: No module named 'tensorflow.lite'\r\n\r\n\r\nEDIT: I've just tried the 2.0.0 alpha wheel and that one works with no .lite module import errors \ud83d\udc4d \r\nCan't make the 1.11 wheels work however, error is consistent for me", "@PINTO0309 \r\nThanks for providing the package! For me your package worked after having run into problems with the installation stalling during numpy and h5py. Ended up solving that by downloading the wheels and installing them manually, in case it helps anyone else:\r\n\r\n```\r\nwget -O numpy-1.16.1-cp35-cp35m-linux_armv7l.whl https://www.piwheels.hostedpi.com/simple/numpy/numpy-1.16.1-cp35-cp35m-linux_armv7l.whl\r\npip3 install numpy-1.16-1.whl\r\n\r\nwget -O h5py-2.9.0-cp35-cp35m-linux_armv7l.whl https://www.piwheels.org/simple/h5py/h5py-2.9.0-cp35-cp35m-linux_armv7l.whl\r\npip3 install h5py-2.9.0-cp35-cp35m-linux_armv7l.whl --no-deps\r\n\r\npip3 install tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl --no-deps\r\n```\r\n\r\n", "@Mark84\r\nIn the case of Tensorflow 1.11.0, it is necessary to describe as follows.\r\n\"contrib\" is required for the path.\r\nIt is not necessary to specify \"contrib\" after Tensorflow 1.12.0.\r\n```python\r\nimport tensorflow.contrib.lite\r\n```\r\n\r\n@zaichang \r\nThanks for the feedback!\r\nI will update the installation procedure.", "@PINTO0309 \r\nI'm not sure if that my amendments should be part of the official installation procedure as I'm not sure if the problem I faced is isolated to my setup (I did not start from a fresh image). But maybe it'd help someone out there", "Hi\uff0c@PINTO0309 @aselle \r\n\r\nHow to solve the problem:\r\n\r\nPython3.5 ---> The __init__.py description is inappropriate and results in an import error.\r\n\r\npi@raspberrypi:~/TensorflowLite-UNet $ python3 tflite_test.py \r\nTraceback (most recent call last):\r\n  File \"tflite_test.py\", line 4, in <module>\r\n    import tflite_runtime as tflr\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/__init__.py\", line 1, in <module>\r\n    import tflite_runtime.lite.interpreter\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/lite/__init__.py\", line 1, in <module>\r\n    from interpreter import Interpreter as Interpreter\r\nImportError: No module named 'interpreter'\r\n\r\nBest Regards\r\n", "@wangzhihua520 \r\n```python\r\n# Tensorflow v1.12.0\r\nfrom tensorflow.contrib.lite.python import interpreter as ip\r\n```\r\nor\r\n```python\r\n# Tensorflow v1.13.0\r\nfrom tensorflow.lite.python import interpreter as ip\r\n```\r\n```python\r\ninterpreter = ip.Interpreter(model_path=<model_file_path>)\r\n```", "Dear @PINTO0309 @aselle \r\n\r\nThank you very much for your help.\r\n\r\nI just follow the instruction: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package\r\nto deploy Tensorflow Lite interpreter and use it from python without requiring the rest of Tensorflow. As your suggestion, I should install the total version of tensorflow, is it right?\r\n\r\nIf so, where I can download the total version of tensorflow?  To do as \r\n\r\n```\r\nwget -O numpy-1.16.1-cp35-cp35m-linux_armv7l.whl https://www.piwheels.hostedpi.com/simple/numpy/numpy-1.16.1-cp35-cp35m-linux_armv7l.whl\r\npip3 install numpy-1.16-1.whl\r\nwget -O h5py-2.9.0-cp35-cp35m-linux_armv7l.whl https://www.piwheels.org/simple/h5py/h5py-2.9.0-cp35-cp35m-linux_armv7l.whl\r\npip3 install h5py-2.9.0-cp35-cp35m-linux_armv7l.whl --no-deps\r\npip3 install tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl --no-deps\r\n\r\n```\r\nIn the previous answer this page, the reason for this problem is \"Python3.5 ---> The __init__.py description is inappropriate and results in an import error.\" how to modify the __init__.py?\r\n\r\nMy platform is Nanopi, armv7l, Ubuntu 16.04 LTS. (I have tested that this platform can use  tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl from piwheel.org)\r\n\r\nBest regards\r\n\r\n", "@wangzhihua520\r\nAlthough it is an unofficial package, please try the following.\r\nIt's a full build package, but its introduction is very easy.\r\n**https://github.com/PINTO0309/Tensorflow-bin**\r\n\r\nFor armv7l.\r\n\r\n**`Tensorflow v1.11.0`**\r\nor\r\n**`Tensorflow v1.13.1`**\r\nor\r\n**`Tensorflow v2.0.0 alpha`**\r\n\r\nIf that doesn't work, try rebuilding Tensorflow yourself, referring to the working sequence in my repository.\r\nI built a full Tensorflow using RaspberryPi3. (Native Build)\r\n**https://github.com/PINTO0309/Tensorflow-bin#build-parameter**\r\nYou may need to adjust the parameters depending on your device environment.\r\n\r\nI can not try because I do not own \"NanoPi\".", "> @wangzhihua520\r\n> Although it is an unofficial package, please try the following.\r\n> It's a full build package, but its introduction is very easy.\r\n> **https://github.com/PINTO0309/Tensorflow-bin**\r\n> \r\n> For armv7l.\r\n> \r\n> **`Tensorflow v1.11.0`**\r\n> or\r\n> **`Tensorflow v1.13.1`**\r\n> or\r\n> **`Tensorflow v2.0.0 alpha`**\r\n> \r\n> If that doesn't work, try rebuilding Tensorflow yourself, referring to the working sequence in my repository.\r\n> I built a full Tensorflow using RaspberryPi3. (Native Build)\r\n> **https://github.com/PINTO0309/Tensorflow-bin#build-parameter**\r\n> You may need to adjust the parameters depending on your device environment.\r\n> \r\n> I can not try because I do not own \"NanoPi\".\r\n\r\nHello,\r\n\r\nI am trying to deploy TFlite standalone (without TF) on RPi zero and struggling.  I tried https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package, however it is slow on RPi0 so I tried on RPi4 and getting errors. Goal was to build a whl on RPi4 so that the process is validated and then try on RPi0 (with different architecture).", "Hi @saurabh-kachhia ,  \r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23082\">No</a>\n"]}, {"number": 23081, "title": "1.12.0-rc2 cherry-pick request: Include .inc files for absl headers", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 23080, "title": "Fix the issues in API compatibility test", "body": "When running `$ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True`, the compatibility test failed due to some files in the estimator module as below. This PR tries to fix this issue. \r\n```\r\nsisue 1\t: Object tensorflow.estimator.Estimator expected but not found (removed).\r\nIssue 2\t: Object tensorflow.estimator.BaselineClassifier expected but not found (removed).\r\nIssue 3\t: Object tensorflow.estimator.export.ExportOutput expected but not found (removed).\r\nIssue 4\t: Object tensorflow.estimator.inputs expected but not found (removed).\r\nIssue 5\t: Object tensorflow.estimator.export.RegressionOutput.__metaclass__ expected but not found (removed).\r\nIssue 6\t: Object tensorflow.estimator.export.PredictOutput.__metaclass__ expected but not found (removed).\r\nIssue 7\t: Object tensorflow.estimator.EvalSpec expected but not found (removed).\r\nIssue 8\t: Object tensorflow.estimator.BaselineRegressor expected but not found (removed).\r\nIssue 9\t: Object tensorflow.estimator expected but not found (removed).\r\nIssue 10\t: Object tensorflow.estimator.FinalExporter expected but not found (removed).\r\nIssue 11\t: Object tensorflow.estimator.export.PredictOutput expected but not found (removed).\r\nIssue 12\t: Object tensorflow.estimator.WarmStartSettings expected but not found (removed).\r\nIssue 13\t: Object tensorflow.estimator.export.ClassificationOutput expected but not found (removed).\r\nIssue 14\t: Object tensorflow.estimator.experimental expected but not found (removed).\r\nIssue 15\t: Object tensorflow.estimator.export.RegressionOutput expected but not found (removed).\r\nIssue 16\t: Object tensorflow.estimator.BoostedTreesRegressor expected but not found (removed).\r\nIssue 17\t: Object tensorflow.estimator.export expected but not found (removed).\r\nIssue 18\t: Object tensorflow.estimator.LatestExporter expected but not found (removed).\r\nIssue 19\t: Object tensorflow.estimator.EstimatorSpec expected but not found (removed).\r\nIssue 20\t: Object tensorflow.estimator.export.ClassificationOutput.__metaclass__ expected but not found (removed).\r\nIssue 21\t: Object tensorflow.estimator.RunConfig expected but not found (removed).\r\nIssue 22\t: Object tensorflow.estimator.LinearClassifier expected but not found (removed).\r\nIssue 23\t: Object tensorflow.estimator.DNNLinearCombinedClassifier expected but not found (removed).\r\nIssue 24\t: Object tensorflow.estimator.BoostedTreesClassifier expected but not found (removed).\r\nIssue 25\t: Object tensorflow.estimator.DNNClassifier expected but not found (removed).\r\nIssue 26\t: Object tensorflow.estimator.export.ExportOutput.__metaclass__ expected but not found (removed).\r\nIssue 27\t: Object tensorflow.estimator.ModeKeys expected but not found (removed).\r\nIssue 28\t: Object tensorflow.estimator.BestExporter expected but not found (removed).\r\nIssue 29\t: Object tensorflow.estimator.Exporter expected but not found (removed).\r\nIssue 30\t: Object tensorflow.estimator.VocabInfo expected but not found (removed).\r\nIssue 31\t: Object tensorflow.estimator.DNNLinearCombinedRegressor expected but not found (removed).\r\nIssue 32\t: Object tensorflow.estimator.DNNRegressor expected but not found (removed).\r\nIssue 33\t: Object tensorflow.estimator.export.ServingInputReceiver expected but not found (removed).\r\n```", "comments": ["@case540 @gunan Could you have a look at this PR? It seems like there are big changes in the estimator module. Is my understanding right?", "Does...\r\n\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True --only_test_core_api True\r\n\r\n...work? Should maybe make --only_test_core_api default to True I suppose. But yeah, estimator code is moving out of core TF into its own repo https://github.com/tensorflow/estimator", "@case540 Yes, your command works. Should we keep the estimator pbtxt files or just delete them? ", "Keep them (for now at least). We run internal tests against them. Also, I think if you \"pip install tensorflow_estimator\" you will be able to run without the core_only flag yourself.", "@case540 Sounds good. We could close this PR?", "Closing! Thanks and if you see any potential issues with these tests due to the Estimator mode migration please let me know!"]}, {"number": 23079, "title": "Enhance CUDA detection.", "body": "This proposal enhance cmake CUDA related dependency on all cmake compatible platforms.\r\n\r\n1. Use cmake internals (only) to **find cuda libraries** and **detect cuda arch**\r\n2. Libraries are picked up as **shared** in case of tensorflow_BUILD_SHARED_LIB else **static**\r\n3. **-DCUDA_ARCH_NAME** can be selected as **Auto** or **All** or **custom** as per cmake docs\r\n4. Get rid of any hardcoded versions and strings, all details are automatically detected and generated\r\n\r\nThe cmake's internal CUDA module is mature enough and well supported on all platforms, desired and preferable instead of hardcoded or custom local functions.\r\n\r\nAdditionally **contrib/data** OPS is gone, so for updated compilation flow is removed from cmake too. OPS and library submodules as **contrib** will be addressed in separate PR hopefully with better way than is now.\r\n\r\nAdded @perfinion to this.\r\n\r\n", "comments": []}, {"number": 23078, "title": "Building the pip package on Windows", "body": "Hi,\r\n\r\nI'm trying to build tensorflow from source. I managed to successfully finish the bazel build step (CPU-only), but when I run the command \r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nwhat happens is that the build_pip_package process spawns itself and the bash process multiple times (like a 1000 times) and the whole process ends with an out of memory error. Here's the output of the command:\r\n\r\nbazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package .\\tfbuild\r\n/usr/bin/bash: warning: shell level (1000) too high, resetting to 1\r\nLAUNCHER ERROR: Cannot launch process: \"C:/msys64/usr/bin/bash.exe\r\nReason: (error: 8): Not enough memory resources are available to process this command.\r\n\r\nIs there a way to limit the number of spawned processes? I cannot find any help online.\r\n\r\nUpdate:\r\n**Have I written custom code: NO\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: N/A\r\nTensorFlow version: latest sources\r\nBazel version: 0.18.0 x64 (2018-10-15)\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: Intel HD graphics\r\nExact command to reproduce: bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package .\\tfbuild\r\nMobile device: NO**\r\n\r\nThank You", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "From your logs, I am pretty certain this is an msys issue on your system.\r\nGoogling your problem gets me to this:\r\nhttps://github.com/mintty/wsltty/issues/6\r\n\r\nHave to tried looking through msys issues?", "Is this still an issue? Did you a chance to look at ?\r\n\r\n> From your logs, I am pretty certain this is an msys issue on your system.\r\n> Googling your problem gets me to this:\r\n> [mintty/wsltty#6](https://github.com/mintty/wsltty/issues/6)\r\n> \r\n> Have to tried looking through msys issues?\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23077, "title": "Slow training speed and incompatible ops issue in tf.keras.utils.multi_gpu_model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. See https://github.com/kami93/PredRNN\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (use command below):v1.11.0\r\n- Python version: Anaconda Python 3.6.6\r\n- Bazel version (if compiling from source):0.17.2\r\n- GCC/Compiler version (if compiling from source): GCC 7.3.0\r\n- CUDA/cuDNN version: CUDA 9.2, cuDNN 7.3\r\n- GPU model and memory: Gefroce 1080Ti * 4 (4-way GPU), 11,178 MiB each.\r\n\r\n**Describe the current behavior**\r\nHi. I have built a TensorFlow Keras Sequential model (See [predRNN.py](https://github.com/kami93/PredRNN/blob/master/predRNN.py)) which consists of several Keras layers including my custom layer (See [keras_custom/layers/STLSTM.py](https://github.com/kami93/PredRNN/blob/master/keras_custom/layers/STLSTM.py)).\r\n\r\nThe model by itself runs very well when tf.keras.utils.multi_gpu_model is not applied. The model can be created, compiled, and perform the training (by Model.fit method) without any warning or error.\r\n\r\nHowever, if tf.keras.utils.multi_gpu_model is applied to replicate the model on several GPUs for multi gpu training, warnings like the followings are raised. \r\n\r\n```\r\nNo node-device colocations were active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation.\r\nDevice assignments active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation:\r\n  with tf.device(/gpu:2): </home/simon/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py:221>\r\n```\r\n```\r\nWARNING:tensorflow:Tried to colocate op 'training/Adam/gradients/replica_3/sequential/stlst_m2d/while/convolution_14_grad/ShapeN/Const' (defined at predRNN.py:142) having device '/device:CPU:1' with op 'replica_3/sequential/stlst_m2d/while/convolution_14' (defined at /home/simon/Desktop/git/PredRNN/keras_custom/layers/STLSTM.py:969) which had an incompatible device '/device:GPU:3'.\r\n```\r\nI think here \"tf.keras.utils.multi_gpu_model\" is not being able to properly assign the convolution ops to other devices which they are initially created for the \"model_creation_device\" (See [line 97 in predRNN.py](https://github.com/kami93/PredRNN/blob/master/predRNN.py#L97)).\r\n\r\nActually, the moodel behaves different with the different selection of \"model_creation_device\".\r\n\r\n**Case 1**\r\nIf model_creation_device == '/cpu:0', the CPU usage is almost 100% in every core. For the GPU usages, gpu:0, gpu:1, gpu:2 and gpu:3 are all underutilized below 30%. The training is extremely slow compared to other cases. I think some convolution ops are performed by 'CPU:0' here.\r\n\r\n![cpu_usage](https://user-images.githubusercontent.com/20102/47162030-33740d00-d32e-11e8-8dc4-b5fd40be6c09.png)\r\n![nvidia-smi_cpu0](https://user-images.githubusercontent.com/20102/47163769-a3d05d80-d331-11e8-8e39-d8fa27ec9542.png)\r\n\r\n##################################################################################\r\n\r\n**Case 2**\r\nIf model_creation_device == '/cpu:1', the CPU usage is normal below 30% in every core. However memory shortage warnings such as the followings are raised.\r\n```\r\n2018-10-18 23:14:56.079586: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-10-18 23:14:56.082410: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n```\r\nFor the GPUs usage, only the 'gpu:0' is highly utilized around 100% all the time. Others are mostly underutilized around 30%. However, the training speed is fastest amongst the cases. \r\n\r\n![cpu_usage_cpu1](https://user-images.githubusercontent.com/20102/47166363-fc562980-d336-11e8-88d8-1c4b7708a135.png)\r\n![cpu1](https://user-images.githubusercontent.com/20102/47166330-e7799600-d336-11e8-862d-de21e67fc211.png)\r\n\r\n##################################################################################\r\n\r\n**Case 3**\r\nIf model_creation_device == '/gpu:0', '/gpu:1', '/gpu:2', or '/gpu:3', the CPU usage is normal below 30% in every core. However, memory shortage warnings are raised.\r\n\r\nThe GPUs usage is somewhat strange. Only the \"model_creation_device\" is highly utilized around 100% all the time, and others are mostly underutilized around 30%. \r\n\r\n![cpu_usage_gpus](https://user-images.githubusercontent.com/20102/47165523-1989f880-d335-11e8-9728-424b649c967c.png)\r\n![gpu1](https://user-images.githubusercontent.com/20102/47165150-64efd700-d334-11e8-83e1-abc2cf41b02f.png)\r\nThe training speed is way faster than model_creation_device == '/cpu:0' case. However, slightly slower than model_creation_device == '/cpu:1' case.\r\n\r\n**Describe the expected behavior**\r\nMulti GPU training of \"PredRNN\" keras model using \"tf.keras.utils.multi_gpu_model\" works without any warning related ops compatibility with devices. All CPU core's usage is below 30% and GPUs usage are around 100% equally for all GPUs while training.\r\n\r\n**Code to reproduce the issue**\r\nRun predRNN.py in https://github.com/kami93/PredRNN.\r\n\r\nModify the \"model_creation_device\" to reproduce issues.\r\n\r\n**Other info / logs**\r\nModel source code: https://github.com/kami93/PredRNN\r\nlogs when model_creation_device = '/cpu:0'\r\nhttps://pastebin.com/esjt8ZLa\r\nlogs when model_creation_device = '/cpu:1'\r\nhttps://pastebin.com/yRfm9yW8\r\nlogs when model_creation_device = '/gpu:0'\r\nhttps://pastebin.com/CB6Wbzn7\r\nlogs when model_creation_device = '/gpu:1'\r\nhttps://pastebin.com/c917nU53\r\nlogs when model_creation_device = '/gpu:2'\r\nhttps://pastebin.com/rbMSPaWN\r\nlogs when model_creation_device = '/gpu:3'\r\nhttps://pastebin.com/j3xGDvrw", "comments": ["I tried tf.contrib.distribute.MirroredStrategy too.\r\n\r\nFirstly, the model raised the following ValueError with tf.contrib.distribute.MirroredStrategy being declared with the default arguments.\r\n```\r\nValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/replica:0/task:0/device:GPU:0', '/replica:0/task:0/device:GPU:1', '/replica:0/task:0/device:GPU:2', '/replica:0/task:0/device:GPU:3']) (current device )\r\n```\r\n\r\nFrom #22550, The ValueError was resolved by specifying device list.\r\n```\r\ntf.contrib.distribute.MirroredStrategy(['/device:CPU:0', '/device:CPU:1', '/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'])\r\n```\r\n\r\n##################################################################################\r\n\r\nPredRNN.py gist\r\nhttps://gist.github.com/kami93/4d4dc9a220e342c9a7710de26dfc34c5\r\n\r\nSetting model_creation_device == '/cpu:1'  raises the following errors.\r\n```\r\nInvalidArgumentError (see above for traceback): Could not colocate node with its resource and reference inputs; devices /replica:0/task:0/device:CPU:1 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.\r\n\t [[{{node training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam}} = ResourceApplyAdam[T=DT_FLOAT, use_locking=false, use_nesterov=false, _device=\"/replica:0/task:0/device:CPU:1\"](stlst_m2d_1/kernel_x/replica_1, stlst_m2d_1/kernel_x/Adam/replica_1, stlst_m2d_1/kernel_x/Adam_1/replica_1, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp_1, training/TFOptimizer/Adam/learning_rate, training/TFOptimizer/Adam/beta1, training/TFOptimizer/Adam/beta2, training/TFOptimizer/Adam/epsilon, training/TFOptimizer/Identity_1)]]\r\n\r\n```\r\nTraceback: https://pastebin.com/RzRdUMk1\r\n\r\n##################################################################################\r\n\r\nSetting the model_creation_device == '/gpu:1'  raises the following errors.\r\n```\r\nInvalidArgumentError (see above for traceback): Could not colocate node with its resource and reference inputs; devices /replica:0/task:0/device:CPU:1 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.\r\n\t [[{{node training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam}} = ResourceApplyAdam[T=DT_FLOAT, use_locking=false, use_nesterov=false, _device=\"/replica:0/task:0/device:CPU:1\"](stlst_m2d_1/kernel_x/replica_1, stlst_m2d_1/kernel_x/Adam/replica_1, stlst_m2d_1/kernel_x/Adam_1/replica_1, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp_1, training/TFOptimizer/Adam/learning_rate, training/TFOptimizer/Adam/beta1, training/TFOptimizer/Adam/beta2, training/TFOptimizer/Adam/epsilon, training/TFOptimizer/Identity_1)]]\r\n```\r\nTraceback: https://pastebin.com/bAkzrt31\r\n\r\n##################################################################################\r\nNot specifying the device context with model_creation_device raises the following errors.\r\ngist: https://gist.github.com/kami93/4283471067c2e36fc48cf0f5d8604e8a\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Could not colocate node with its resource and reference inputs; devices /replica:0/task:0/device:CPU:1 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.\r\n\t [[{{node training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam}} = ResourceApplyAdam[T=DT_FLOAT, use_locking=false, use_nesterov=false, _device=\"/replica:0/task:0/device:CPU:1\"](stlst_m2d_1/kernel_x/replica_1, stlst_m2d_1/kernel_x/Adam/replica_1, stlst_m2d_1/kernel_x/Adam_1/replica_1, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp_1, training/TFOptimizer/Adam/learning_rate, training/TFOptimizer/Adam/beta1, training/TFOptimizer/Adam/beta2, training/TFOptimizer/Adam/epsilon, training/TFOptimizer/Identity_1)]]\r\n```\r\nTraceback: https://pastebin.com/bF8DByeG", "Is there a fix to this: \r\n```\r\nNo node-device colocations were active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation.\r\nDevice assignments active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation:\r\n  with tf.device(/gpu:2): </home/simon/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py:221>\r\n```\r\n I am having the same problem. \r\n\r\nThanks a lot.", "DistributionStrategies will be the new API for distributing Keras models. Can you try using [the new API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md#multi-gpu-training)?", "I have been trying the new api using `distribution = tf.contrib.distribute.MirroredStrategy()`. The code ran through with the following warning: \r\n`WARNING:tensorflow:Not all devices in DistributionStrategy are visible to TensorFlow session.                                                                                                  \r\nWARNING:tensorflow:You are accessing attribute optimizerof the DistributedCallbackModel that may not have been set correctly.\r\nWARNING:tensorflow:You are accessing attribute _unconditional_checkpoint_dependenciesof the DistributedCallbackModel that may not have been set correctly.`\r\n\r\nI also did not see any GPU involved via `gpustats`. \r\n\r\nIs this normal? \r\n\r\n\r\n\r\n", "No, but it is difficult to help you debug without further information. Can you provide a minimal code snippet that will allow us to reproduce what you are seeing?", "**System information**\r\n- Have I written custom code \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: TITAN Xp (12196MiB)\r\n--\r\n\r\n\r\n**Describe the current behavior**\r\n- WARNINGS SHOWN ABOVE\r\n- While the script is running, I also did not see any GPU involved via `gpustats`.\r\n\r\nThe script without ```distribution = tf.contrib.distribute.MirroredStrategy()``` ran well. Since I need to populate the network with a huge set, I plan to parallelize the work on various GPUs. \r\n\r\n**Describe the expected behavior**\r\n- Please kindly let me know how should I interpret the WARNINGS above and get rid of them.\r\n- Multiple GPUs are expected to be in usage while the script is called.\r\n\r\n**Code to reproduce the issue**\r\nMy network architecture looks like this (a hierarchical one): \r\n```\r\nconfig = tf.ConfigProto(allow_soft_placement = True, log_device_placement= False)\r\nos.environ['CUDA_VISIBLE_DEVICES'] = options.GPU  # specify the GPU used by parser.add_option()\r\n\r\n# load in data\r\n#######################DNN Level 1########################\r\nif options.L1_model == 0:\r\n\tmodel = BuildModel()\r\n        tf.keras.backend.get_session().run(tf.global_variables_initializer())\r\n\tmodel.fit(X_train, y_train[:, 0],\r\n\t\t\tvalidation_data=(X_test, y_test[:, 0]),\r\n\t\t\tepochs=options.epochs,\r\n\t\t\tverbose=2,\r\n\t\t\tbatch_size=options.batch_size_L1)\r\n          #######################DNN Level 1########################\r\nif options.L2_model == 0:\r\n \r\n\tmodel = BuildModel()\r\n        tf.keras.backend.get_session().run(tf.global_variables_initializer())\r\n\tmodel.fit(..., batch_size=options.batch_size_L2)\r\n```\r\n\r\nI don't think the warnings have something to do with a hierarchical structure. In a flat model, the same warnings showed up.\r\n\r\n`BuildModel()` is defined this way:\r\n```\r\ndef buildModel(word_index, embeddings_index, nClasses, \r\nMAX_SEQUENCE_LENGTH, EMBEDDING_DIM, gpusno, nLayers=3,Number_Node=100, dropout=0.5):\r\n\t\r\n\tembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\r\n\tfor word, i in word_index.items():\r\n\t\tembedding_vector = embeddings_index.get(word)\r\n\t\tif embedding_vector is not None:\r\n\t\t\tembedding_matrix[i] = embedding_vector\r\n\t\t\t\r\n\tmodel = tf.keras.models.Sequential()\r\n\t\r\n\tmodel.add(tf.keras.layers.Embedding(len(word_index) + 1,\r\n\t\t\t\t\t\t\t\tEMBEDDING_DIM,\r\n\t\t\t\t\t\t\t\tweights=[embedding_matrix],\r\n\t\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\r\n\t\t\t\t\t\t\t\ttrainable=True))\r\n\tmodel.add(tf.keras.layers.Flatten())\r\n\t\r\n\tfor i in range(0,nLayers):\r\n\t\tmodel.add(tf.keras.layers.Dense(Number_Node, activation='relu'))\r\n\t\tmodel.add(tf.keras.layers.Dropout(dropout))\r\n\t\r\n\tmodel.add(tf.keras.layers.Dense(nClasses, activation='softmax'))\r\n\r\n\t\r\n\tdistribution = tf.contrib.distribute.MirroredStrategy()\r\n        # i cannot add argument num_gpus, as tf will say it cannot find the devices.\r\n\t\r\n\tmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n\t\t\t\t  optimizer=tf.train.AdamOptimizer(),\r\n\t\t\t\t  metrics=['accuracy'], \r\n\t\t\t\t  distribute=distribution)\r\n\t\t\t\t  \r\n\tprint('model summary:') \r\n\tmodel.summary()\r\n\r\n\treturn model\r\n```\r\n\r\nBTW, I compared the `multi_gpu` mode and `single gpu` mode on a smaller sample after 1 epoch. The training and validation accuracies vary largely. In most of the submodels, `single gpu` model produces much higher accuracy numbers. I will run the scripts for more epochs and see about results. Any suggestions on this?\r\n", "> It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n\r\nYes, it is still an issue, please advise accordingly. Thank you.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 23076, "title": "prediction from tensorflow differs on broadwell and on SkyLake", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.18.0\r\n- GCC/Compiler version (if compiling from source): 7.3\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have experimented a bit with CNNs for semantic segmentation and it's predictions and I have noticed that for exactly the same network predictions are different regarding the architecture where I deploy the network. I currently see the following state:\r\n\r\n`pip install tensorflow` - correct result\r\n`pip install tensorflow-gpu` - correct result (P100)\r\n`bazel build -c opt --copt=-march=broadwell --copt=-mfpmath=both -k //tensorflow/tools/pip_package:build_pip_package` - correct result\r\n`bazel build -c opt --copt=-march=native --copt=-mfpmath=both -k //tensorflow/tools/pip_package:build_pip_package` - different (1-2% mismatch on my tested images) result on SkyLake (but faster prediction than with `--copt=-march=broadwell`)\r\n\r\n**NOTE:**\r\nI'm using 32core Google Cloud machines in the us-central1-f zone with CPU option SkyLake.\r\n\r\n**Question:**\r\nIs expected the prediction on SkyLake to be different from the prediction on broadwell and GPU on purpose, or should it be exactly the same and I'm just very unlucky here regarding rounding in some libraries underneath the tensorflow?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "You shouldn't see a big difference in semantic segmentation and its predictions between different architectures. Please provide more details or post your code snippet here."]}, {"number": 23075, "title": "bazel-bin problem.help,help,help~~~", "body": "Hello,I don`t know what the problem it is? it can`t generate a new pb file.thank you.\r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/media/long/data/android/PoseEstimationForMobile/release/cpm_model/model.pb --out_graph=/media/long/data/android/PoseEstimationForMobile/release/cpm_model/graph_opt1.pb --inputs=inputs/X --outputs=output/predict --transforms='strip_unused_nodes(type=float, shape=\"256*64\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms'\r\n2018-10-18 21:49:33.990823: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes\r\n2018-10-18 21:49:33.991322: E tensorflow/tools/graph_transforms/transform_graph.cc:263] Input node output/predict not found in graph\r\n2018-10-18 21:49:33.991366: E tensorflow/tools/graph_transforms/transform_graph.cc:264] usage: bazel-bin/tensorflow/tools/graph_transforms/transform_graph\r\nFlags:\r\n\t--in_graph=\"\"                    \tstring\tinput graph file name\r\n\t--out_graph=\"\"                   \tstring\toutput graph file name\r\n\t--inputs=\"\"                      \tstring\tinputs\r\n\t--outputs=\"\"                     \tstring\toutputs\r\n\t--transforms=\"\"                  \tstring\tlist of transforms\r\n\t--output_as_text=false           \tbool\twhether to write the graph in text protobuf format\r\n\r\nTransforms are:\r\nadd_default_attributes\r\nbackport_concatv2\r\nbackport_tensor_array_v3\r\nflatten_atrous_conv\r\nfold_batch_norms\r\nfold_constants\r\nfold_old_batch_norms\r\nfreeze_requantization_ranges\r\nfuse_pad_and_conv\r\nfuse_remote_graph\r\nfuse_resize_and_conv\r\nfuse_resize_pad_and_conv\r\ninsert_logging\r\nmerge_duplicate_nodes\r\nobfuscate_names\r\nplace_remote_graph_arguments\r\nquantize_nodes\r\nquantize_weights\r\nremove_attribute\r\nremove_control_dependencies\r\nremove_device\r\nremove_nodes\r\nrename_attribute\r\nrename_op\r\nrewrite_quantized_stripped_model_for_hexagon\r\nround_weights\r\nset_device\r\nsort_by_execution_order\r\nsparsify_gather\r\nstrip_unused_nodes", "comments": ["@chenloveheimei Hi, request you to provide a code snippet which helps to reproduce the issue from our end. Also please fill this [template](https://github.com/tensorflow/tensorflow/issues/new?template=bug-performance-issue.md) as it also helps us to understand the problem clearly. Thank you !", "PB FIle:\r\n[graph.pb.zip](https://github.com/tensorflow/tensorflow/files/2494701/graph.pb.zip)\r\n\r\nthe pic of model:\r\n![model](https://user-images.githubusercontent.com/30176962/47214098-84880d80-d3cf-11e8-83ae-006838e8d3b0.jpeg)\r\n\r\ncommand:\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=model.pb \\\r\n--out_graph=graph_opt1.pb \\\r\n--inputs='image' \\\r\n--outputs='Convolutional_Pose_Machine/stage_5_out' \\\r\n--transforms='strip_unused_nodes(type=float, shape=\"256*64\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms'\r\n\r\nthe log of Error:\r\n2018-10-19 17:27:50.254984: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes\r\n2018-10-19 17:27:50.286576: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying remove_nodes\r\n2018-10-19 17:27:50.335090: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fold_constants\r\n2018-10-19 17:27:50.359307: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node MobilenetV2/MobilenetV2_part_0/inverted_bottleneck_MobilenetV2_part_0_1/MobilenetV2_part_0_1_up_pointwise/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Conv2d_0/Relu6, MobilenetV2/MobilenetV2_part_0/inverted_bottleneck_MobilenetV2_part_0_1/MobilenetV2_part_0_1_up_pointwise/weights)\r\n2018-10-19 17:27:50.359760: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node MobilenetV2/MobilenetV2_part_0/inverted_bottleneck_MobilenetV2_part_0_1/MobilenetV2_part_0_1_pointwise/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](MobilenetV2/MobilenetV2_part_0/inverted_bottleneck_MobilenetV2_part_0_1/MobilenetV2_part_0_1_depthwise/Relu, MobilenetV2/MobilenetV2_part_0/inverted_bottleneck_MobilenetV2_part_0_1/MobilenetV2_part_0_1_pointwise/weights)\r\n2018-10-19 17:27:50.360079: E tensorflow/core/framework/node_def_util.cc:110] Error in the node: {{node MobilenetV2/MobilenetV2_part_0/inverted_bottleneck_MobilenetV2_part_0_2/MobilenetV2_part_0_2_up_pointwise/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](MobilenetV2/MobilenetV2_part_0/inverted_bottleneck_MobilenetV2_part_0_1/MobilenetV2_part_0_1_pointwise/BatchNorm/Relu6, MobilenetV2/MobilenetV2_part_0/inverted_bottleneck_MobilenetV2_part_0_2/MobilenetV2_part_0_2_up_pointwise/weights)\r\n.....\r\n2018-10-19 17:27:50.397343: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fold_batch_norms\r\n2018-10-19 17:27:50.406738: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying fold_old_batch_norms\r\n\r\nQuestion:\r\nwhether the comand I wrote is right or not?\r\nBut it can generate new pb file.the size of origin file is 2.7M,the new one is 2.4M.", "This is an issue about graph_transform tool, not really bazel as far as I can tell.\r\nI am not familiar with this tool.", "@drpngx  Any suggestions ? ", "This looks more like a Stack Overflow question than a bug unfortunately. Could you post full details there? Thanks!"]}, {"number": 23074, "title": "Gradients with respect to a TensorArray", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n1.10.0\r\n- Are you willing to contribute it (Yes/No):\r\nNo (not enough expertise)\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIt appears that one cannot use the `tf.gradients` function for taking derivatives with respect to TensorArrays. I would like there to be the feature to return a TensorArray with the relevant gradients\r\n\r\n**Will this change the current api? How?**\r\nSlightly but in a non-breaking manner (hopefully). It would be an addition to the capabilities of `tf.gradients`\r\n\r\n**Who will benefit with this feature?**\r\nThis is a feature that is crucial if we wish to take derivatives with respect to intermediate states in a recurrent neural network.\r\n\r\nAlso related: issue #5412 (currently closed due to no response)\r\n\r\n**Any Other info.**\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Can you please try and put one example that can explain the expected functionality you wish to see?\r\nIt can be a small script that shows current behavior and you can comment what is your desired behavior from the function. Thanks! ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23073, "title": "tensorflow-gpu install from source. Bazel tests failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nUbuntu 16.04 x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\nno\r\n- TensorFlow installed from (source or binary): \r\nsource\r\n- TensorFlow version:\r\n1.11\r\n- Python version:\r\n2.7.12\r\n- Installed using virtualenv? pip? conda?:\r\nvirtualenv, all dependency packages using pip\r\n- Bazel version (if compiling from source):\r\n0.18.0\r\n- GCC/Compiler version (if compiling from source):\r\ngcc 5.4.0 \r\n- CUDA/cuDNN version: \r\n9.0 / 7.3.1\r\n- GPU model and memory: \r\nNvidia Tesla P100 12Gb\r\n\r\n**Describe the problem**\r\nHello! This is my first post on github, and I apologize in advance if my English is not clear.\r\nI am new to Tensorflow and I try to install it (with gpu support) from source according to the [instructions](https://www.tensorflow.org/install/source#common_installation_problems)\r\nNvidia 384.145 drivers were pre-installed, and other dependencies too (cuda, cudnn). LD_LIBRARY_PATH and CUDA_HOME variables also set.\r\nThe whole installation takes place in virtualenv\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nstuck at this stage:\r\n```\r\ngit checkout r1.11\r\nbazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...\r\n```\r\n\r\n**Any other info / logs**\r\nsome output after \r\n> bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...\r\n\r\n```\r\n//tensorflow/python/kernel_tests:rnn_test                  (9/10 cached) FAILED in 1 out of 10 in 5.3s\r\n  Stats over 10 runs: max = 5.3s, min = 1.8s, avg = 3.2s, dev = 1.2s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/rnn_test/shard_5_of_10/test.log\r\n//tensorflow/contrib/lookup:lookup_ops_test                              FAILED in 4.6s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/contrib/lookup/lookup_ops_test/test.log\r\n//tensorflow/contrib/rpc/python/kernel_tests:rpc_op_test                 FAILED in 0.2s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/contrib/rpc/python/kernel_tests/rpc_op_test/test.log\r\n//tensorflow/core:util_tensor_slice_set_test                             FAILED in 0.1s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/core/util_tensor_slice_set_test/test.log\r\n//tensorflow/go:test                                                     FAILED in 0.1s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/go/test/test.log\r\n//tensorflow/python:build_info_test                                      FAILED in 0.8s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/build_info_test/test.log\r\n//tensorflow/python:flags_test                                           FAILED in 0.8s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/flags_test/test.log\r\n//tensorflow/python:stacktrace_handler_test                              FAILED in 0.8s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/stacktrace_handler_test/test.log\r\n//tensorflow/python/debug:dist_session_debug_grpc_test                   FAILED in 0.7s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/debug/dist_session_debug_grpc_test/test.log\r\n//tensorflow/python/debug:grpc_large_data_test                           FAILED in 0.7s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/debug/grpc_large_data_test/test.log\r\n//tensorflow/python/debug:session_debug_grpc_test                        FAILED in 0.8s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/debug/session_debug_grpc_test/test.log\r\n//tensorflow/python/debug:source_remote_test                             FAILED in 0.8s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/debug/source_remote_test/test.log\r\n//tensorflow/python/keras:image_test                                     FAILED in 5.8s\r\n  /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/keras/image_test/test.log\r\n```\r\nlog details of failed tests:\r\n[1_rnn_test.log](https://github.com/tensorflow/tensorflow/files/2491474/1_rnn_test.log)\r\n[2_lookup_ops_test.log](https://github.com/tensorflow/tensorflow/files/2491475/2_lookup_ops_test.log)\r\n[3_rpc_op_test.log](https://github.com/tensorflow/tensorflow/files/2491477/3_rpc_op_test.log)\r\n[4_util_tensor_slice_set.log](https://github.com/tensorflow/tensorflow/files/2491478/4_util_tensor_slice_set.log)\r\n[5_go_test.log](https://github.com/tensorflow/tensorflow/files/2491479/5_go_test.log)\r\n[6_build_info_test.log](https://github.com/tensorflow/tensorflow/files/2491480/6_build_info_test.log)\r\n[7_flags_test.log](https://github.com/tensorflow/tensorflow/files/2491481/7_flags_test.log)\r\n[8_stacktrace_handler_trace.log](https://github.com/tensorflow/tensorflow/files/2491482/8_stacktrace_handler_trace.log)\r\n[9_dist_sess_debug_grpc_test.log](https://github.com/tensorflow/tensorflow/files/2491483/9_dist_sess_debug_grpc_test.log)\r\n[10_grpc_large_data_test.log](https://github.com/tensorflow/tensorflow/files/2491484/10_grpc_large_data_test.log)\r\n[11_sess_debug_grpc_test.log](https://github.com/tensorflow/tensorflow/files/2491485/11_sess_debug_grpc_test.log)\r\n[12_source_remote_test.log](https://github.com/tensorflow/tensorflow/files/2491486/12_source_remote_test.log)\r\n[13_image_test.log](https://github.com/tensorflow/tensorflow/files/2491487/13_image_test.log)\r\n\r\nPS: some of theese logs contain error about:\r\n\r\n> Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMAF\r\n\r\nI hope for your help in solving this issue. Thanks!\r\n\r\n", "comments": ["@ruff21 Hi, can you try with Bazel 0.15.0 and GCC 4.8 and see if you are running into the same error. ", "@harshini-gadige Hi, did as suggested - the error is not the same. Now bazel swears at the lack of packages '@mobile_ssd'\r\n\r\ntried to test on r1.10 and r1.11\r\n\r\n```\r\nERROR: /home/iict01/tensorflow/tensorflow/examples/android/BUILD:93:1: no such package '@mobile_ssd//': Error downloading [http://storage.googleapis.com/download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip, http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip] to /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/external/mobile_ssd/ssd_mobilenet_v1_android_export.zip: All mirrors are down: [] and referenced by '//tensorflow/examples/android:external_assets'\r\nERROR: Analysis of target '//tensorflow/examples/android:external_assets' failed; build aborted: no such package '@mobile_ssd//': Error downloading [http://storage.googleapis.com/download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip, http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip] to /home/iict01/.cache/bazel/_bazel_iict01/d9bbc19c8526907df6f203d5223f36ca/external/mobile_ssd/ssd_mobilenet_v1_android_export.zip: All mirrors are down: []\r\n\r\n```\r\n\r\nupd:\r\nevery time I run a test, basel cannot find another package, '@speech_commands', then '@stylize', then '@mobile_ssd' again.\r\n\r\n", "@gunan Can you look into this ?", "The above does not mean the installation has failed.\r\nYou have not run configure, so failure of compilation is expected.\r\nAlso, TF has many tests, not all of which are passing all the time for every single configuration.\r\nPlease see these scripts for the test scripts we are running:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/gpu/run_py3_core.sh\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/gpu/run_cc_core.sh\r\n", "I can confrm following the instruction on how to build from source verbatim resulted in about 430 errors, but running the script mentioned by @gunan did not result in errors, perhaps the install instructions should be updated to repeat this script rather than worrying users building from source , because I need to to build from source, becaue I have an older CPU without AVX2 instructions"]}, {"number": 23072, "title": "Runmetadata and graph definitions are not mached", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\n1.6\r\n- Python version:\r\nPython 2.7.12\r\n- Bazel version (if compiling from source):\r\n0.13.0\r\n- GCC/Compiler version (if compiling from source):\r\n5.4.0 2\r\n- CUDA/cuDNN version:\r\n9.0, 7.0\r\n- GPU model and memory:\r\nTitanXP, 12G\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI was trying to match information between graph definition and timeline but they didn't match.\r\nFor example, below is a node in graph definition. \r\n`node {\r\n    name: \"AutoParallel-Replica-0-Accum-Apply/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape\"\r\n    op: \"SparseAccumulatorApplyGradient\"\r\n    input: \"AutoParallel-Accum/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape\"\r\n    input: \"Const_35\"\r\n    input: \"AutoParallel-Replica-0/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape_1\"\r\n    input: \"AutoParallel-Replica-0/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape\"\r\n    input: \"ToInt64_107\"\r\n    device: \"/job:worker/task:0/device:CPU:0\"\r\n    attr {\r\n      key: \"_class\"\r\n      value {\r\n        list {\r\n          s: \"loc:@AutoParallel-Accum/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape\"\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"dtype\"\r\n      value {\r\n        type: DT_FLOAT\r\n      }\r\n    }\r\n    attr {\r\n      key: \"has_known_shape\"\r\n      value {\r\n        b: true\r\n      }\r\n    }\r\n  }\r\n`\r\nHowever, Runmetadata contains below node stat for the node. The inputs in timeline_label are different with information in the graph node. I want to know the policy or rules to match two different information.\r\n` node_stats {\r\n      node_name: \"AutoParallel-Replica-0-Accum-Apply/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape\"\r\n      all_start_micros: 1539680850138092\r\n      op_start_rel_micros: 10\r\n      op_end_rel_micros: 12405\r\n      all_end_rel_micros: 12418\r\n      memory {\r\n        allocator_name: \"cpu_rdma_bfc\"\r\n        total_bytes: 2574336\r\n        peak_bytes: 2574336\r\n        allocator_bytes_in_use: 260048896\r\n        allocation_records {\r\n          alloc_micros: 1539680850142050\r\n          alloc_bytes: 2574336\r\n        }\r\n        allocation_records {\r\n          alloc_micros: 1539680850150477\r\n          alloc_bytes: -2574336\r\n        }\r\n      }\r\n      timeline_label: \"[cpu_rdma_bfc 2.5MB 2.5MB] AutoParallel-Replica-0-Accum-Apply/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape = SparseAccumulatorApplyGradient(AutoParallel-Accum/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup/Gather_3_grad/Reshape, Const, AutoParallel-Replica-0/model/lm/sampled_softmax_loss/embedding_lookup/DynamicPartition:3, AutoParallel-Replica-0/model/gradients/model/lm/sampled_softmax_loss/embedding_lookup_grad/Gather_3_G818, ToInt64_2)\"\r\n      scheduled_micros: 1539680850138009\r\n      memory_stats {\r\n      }\r\n    }\r\n`\r\n**Describe the expected behavior**\r\nInput names in graph definition and timeline label in metadata have the same names\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Some of the fields are missing. You can use the latest TensorFlow version then compare graph definition with timeline again. ", "Closing as this is resolved, free to reopen if problem persists."]}, {"number": 23071, "title": "Error while importing tensorflow after installation ", "body": "Please help me to solve this problem i am trying from several days. i get this error when i import tensorflow after installation , i am using windows 10 OS. \r\n\r\n>>> import tensorflow as tf\r\nRuntimeError: module compiled against API version 0xc but this version of numpy is 0xa\r\nImportError: numpy.core.multiarray failed to import\r\nImportError: numpy.core.umath failed to import\r\nImportError: numpy.core.umath failed to import\r\n2018-10-18 18:24:15.573678: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr", "comments": ["I'll need more info. Please update your numpy and tf versions.", "@sam081  thank you dear it is working now , i have uninstall Numpy and install it again . now it is working. "]}, {"number": 23070, "title": "Syntax and bug fix for Slurm cluster resolver on issues raised by CI test", "body": "@frankchn This fixes some bugs and issues rasied by the Ubuntu Python3 PIP CI test #22423 , thanks!\r\n\r\nhttps://source.cloud.google.com/results/invocations/31d6152f-cd75-4ac6-b132-8639e7166b77/targets/%2F%2Fbazel_pip%2Ftensorflow%2Fcontrib%2Fcluster_resolver:slurm_cluster_resolver_py_test/log", "comments": []}, {"number": 23069, "title": "TensorFlow Docker image contains old Jupyter release", "body": "**System information**\r\n- TensorFlow version (you are using): 1.11 (tensorflow/tensorflow:1.11.0-gpu-py3), amd64\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nLatest release of the Docker TensorFlow image contains Jupyter 4.4.0 which is pretty old (latest release is 5.5.0)\r\n\r\n**Will this change the current api? How?**\r\nNo changes to the API.\r\n\r\n**Who will benefit with this feature?**\r\nUsers that use Jupyter latest features (e.g. \"delete not empty folder from web interface\") and bug fixes.\r\n", "comments": ["My bad, I typed` jupyter --version` (4.4.0) instead of `jupyter notebook --version` (which gives 5.7.0, i.e. latest)\r\n\r\nI close the issue."]}, {"number": 23068, "title": "TFLite benchmark_model cannot be compiled successfully", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.6.6\r\n- Bazel version: 0.18.0\r\n- GCC/Compiler version: Android Clang NDK 18\r\n\r\n**Describe the problem**\r\n\r\nWhen use command in README for benchmark, the compiling failed.\r\n\r\nbazel build -c opt \\\r\n  --config=android_arm \\\r\n  --cxxopt='--std=c++11' \\\r\n  tensorflow/contrib/lite/tools/benchmark:benchmark_model\r\n\r\n**Any other info / logs**\r\n\r\nERROR: /data/tensorflow/contrib/lite/tools/benchmark/BUILD:19:1: Linking of rule '//tensorflow/contrib/lite/tools/benchmark:benchmark_model' failed (Exit 1)\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__locale:51: error: undefined reference to 'uselocale'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__locale:56: error: undefined reference to 'uselocale'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__locale:56: error: undefined reference to 'uselocale'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/src/locale.cpp:76: error: undefined reference to 'newlocale'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__locale:51: error: undefined reference to 'uselocale'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/src/locale.cpp:5439: error: undefined reference to 'freelocale'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__bsd_locale_fallbacks.h:53: error: undefined reference to 'wcsnrtombs'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__bsd_locale_fallbacks.h:68: error: undefined reference to 'mbsnrtowcs'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/__bsd_locale_fallbacks.h:83: error: undefined reference to 'mbtowc'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/src/locale.cpp:1531: error: undefined reference to 'freelocale'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:739: error: undefined reference to 'strtoll_l'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:739: error: undefined reference to 'strtoll_l'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:779: error: undefined reference to 'strtoull_l'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:779: error: undefined reference to 'strtoull_l'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:779: error: undefined reference to 'strtoull_l'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:779: error: undefined reference to 'strtoull_l'\r\n/usr/local/google/buildbot/src/android/ndk-release-r18/external/libcxx/include/locale:820: error: undefined reference to 'strtold_l'\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)", "comments": ["@leo-nullptr : I compiled with android-ndk-r16b and it worked without any issues. Does it work with android-ndk-r16b?", "> @leo-nullptr : I compiled with android-ndk-r16b and it worked without any issues. Does it work with android-ndk-r16b?\r\n\r\nWhen I compiled with android-ndk-r16b, it succeed with arm-v7a but failed with arm64-v8a.\r\nSUCCEED: `bazel build --config android_arm --cxxopt=-std=c++11 //tensorflow/contrib/lite/tools/benchmark:benchmark_model`\r\nFAILED: `bazel build --config android_arm64 --cxxopt=-std=c++11 //tensorflow/contrib/lite/tools/benchmark:benchmark_model`\r\n\r\nLogs:\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:164:11: error: no member named 'vfwscanf' in the global namespace\r\n  using ::vfwscanf;\r\n        ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:170:11: error: no member named 'vswscanf' in the global namespace\r\n  using ::vswscanf;\r\n        ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:174:11: error: no member named 'vwscanf' in the global namespace\r\n  using ::vwscanf;\r\n        ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:191:11: error: no member named 'wcstof' in the global namespace\r\n  using ::wcstof;\r\n        ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:280:14: error: no member named 'wcstof' in namespace 'std'\r\n  using std::wcstof;\r\n        ~~~~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:283:14: error: no member named 'vfwscanf' in namespace 'std'; did you mean 'fwscanf'?\r\n  using std::vfwscanf;\r\n        ~~~~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:148:11: note: 'fwscanf' declared here\r\n  using ::fwscanf;\r\n          ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:286:14: error: no member named 'vswscanf' in namespace 'std'; did you mean 'swscanf'?\r\n  using std::vswscanf;\r\n        ~~~~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:160:11: note: 'swscanf' declared here\r\n  using ::swscanf;\r\n          ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:289:14: error: no member named 'vwscanf' in namespace 'std'\r\n  using std::vwscanf;\r\n        ~~~~~^", "@shashishekhar Is it caused by libc++ in new ndk forced on android P?", "@leo-nullptr : Does this [workaround](https://github.com/tensorflow/tensorflow/issues/20192#issuecomment-404971539) work for you?", "@shashishekhar I have exported ANDROID_NDK_API_LEVEL as 24, and benchmark_model cannot be compiled with NDK r18. Thanks!"]}, {"number": 23067, "title": "QueueRunner going towards deprecation, but tf.data does not replace all usecases?", "body": "In my code I get QueueRunner deprecation warning:\r\n\r\n```\r\nQueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\n```\r\n\r\nDid a bit of investigation [here](https://www.tensorflow.org/guide/datasets) and it seems that tf.data is a new thing that is faster and better than the old queue stuff. But, as I understand, it it not replacing queues because data is expected to be static, (as far as I understood and of all the examples I saw), modification / appending new training data to the dataset is not supported, so I don't know how this change applies to the reinforcement learning, which learns on new data environment returns all the time. So, as far as I see, deprecation of queue runner is coming and I don't see tf.data replacing it for this usecase?\r\n\r\nAny kind of explanation and example is welcome! Thanks!\r\n\r\nPS. Started out this question [on stack overflow](https://stackoverflow.com/questions/52855870/queuerunner-going-towards-deprecation-but-tf-data-does-not-replace-all-usecases) first, but didn't get any solutions.", "comments": ["Would love to hear a response for this one, having the same problem", "For reinforcement learning, `tf.data` may have limited ability here. It is much more convenient to use Gym from Open AI. You can leverage a suite of methods and libraries and integrate them into training. \r\n\r\n\r\n\r\n", "Closing as this is resolved, free to reopen if problem persists.", "@wt-huang @mrry I would like to revisit this issue as it's getting closer towards TF 2.0, in which all Queue APIs are going to be removed.\r\n\r\nI agree that when it comes to reinforcement learning, queue APIs could be more powerful as I don't find any way to implement the similar behavior using `tf.data`. For example, replay buffers (used in IMPALA, Ape-X, or even in DQN, etc.) cannot be implemented easily using `tf.data`.\r\n\r\nOne particular scenario I ran into recently is bothered by an **\"reentrancy\"** issue, when using `Dataset.from_generator`. For example, let's consider a case where we need to **generate batches on-line** through running an environment simulator (Atari, DeepMind Lab, etc.) --- perhaps primarily because it's too expensive to store all image observations as dataset. To deal with this using `tf.data` API, we may use `Dataset.from_generator` to execute an arbitrary python code as a part of TF operation, but the problem here is that non-reentrant generators will be broken. One example: some RL environments that use OpenGL, EGL hardware render, etc. should be running in the **same thread**, not on thread pools.\r\n\r\nPerhaps the most straightforward and classic way (queue-based) is to have a **writer-reader** or **pub-sub** structure, say\r\n\r\n* K parallel actors generates trajectory and put into the buffer/queue\r\n* one or parallel reader thread reads from the queue and construct the batch as usual (prefetch, interleave, map, batch, etc.)\r\n\r\nwhich would be quite a common pattern in RL. As @wt-huang mentioned one can use custom (parallel) utilities that generates batches, but it is desirable that they should run **in parallel** (like prefetch, etc.). \r\n\r\nI am still trying to figure out a good way of this particular use-cases (though I believe it's quite *general* in RL and online learning settings) to generate data on-line. However if we are doubtful `tf.data` fully supports this use cases, it might be a good idea to keep queue stuffs as V1-compat APIs, or implement a new `tf.data` API that can implement such a pub-sub pattern as well.", "Hi @wookayin! We aren't removing *all* queue APIs in TF 2.0. In particular `FIFOQueue` and `RandomShuffleQueue` will remain in the 2.0 API (as `tf.queue.FIFOQueue` and `tf.queue.RandomShuffleQueue`), so you can continue to build arbitrarily sophisticated queuing networks in your code.\r\n\r\nWe have removed `tf.train.QueueRunner` from the 2.0 API (although it will continue to be available in `tf.compat.v1`), because it was only needed to support \"transparent\" thread creation in simple queue-based pipelines, and this is now more robustly handled inside `tf.data`. However, there is no magic inside `QueueRunner`, and you are free to create your own Python threads to move data between queues if that suits your workflow. Indeed, with eager execution in TF 2.0, it might be easier to build more of the simulation pipeline in pure Python code, rather than in graph-based data structures like `FIFOQueue`.\r\n\r\nIf you do have suggestions for a better future API for RL workloads, please do suggest it, either via the [TensorFlow RFC](https://github.com/tensorflow/community/tree/master/rfcs) repository, or the [SIG IO](https://github.com/tensorflow/io/) repository.", "Queues are not only used for buffering input datas. And if queues are reserved, why queuerunners need to be removed. which makes users create their own runners? \r\nFor tf.data, how to make it work like \"shared queue\"? (queue with shared_name) @mrry ", "The existing QueueRunner API is tied to several TensorFlow 1.x concepts, like having a single global graph, collections, and the `tf.Session` interface. All of these are deprecated, and have been removed from the TensorFlow 2.0 API, except via the `tf.compat.v1` compatibility interface. As a result, QueueRunners don't make sense in TensorFlow 2.0, but you can still access them via `tf.compat.v1.train.QueueRunner` etc. if you're using the compatibility interface for other code.\r\n\r\nLikewise, `shared_name` relies on TensorFlow 1.x `tf.Session` semantics, and there is no current equivalent in the 2.0 API. Using the compatibility interface, `tf.compat.v1.data.make_initializable_iterator(dataset, shared_name=...)` will create an iterator that will be shared between sessions on the same server.", "> The existing QueueRunner API is tied to several TensorFlow 1.x concepts, like having a single global graph, collections, and the `tf.Session` interface. All of these are deprecated, and have been removed from the TensorFlow 2.0 API, except via the `tf.compat.v1` compatibility interface. As a result, QueueRunners don't make sense in TensorFlow 2.0, but you can still access them via `tf.compat.v1.train.QueueRunner` etc. if you're using the compatibility interface for other code.\r\n> \r\n> Likewise, `shared_name` relies on TensorFlow 1.x `tf.Session` semantics, and there is no current equivalent in the 2.0 API. Using the compatibility interface, `tf.compat.v1.data.make_initializable_iterator(dataset, shared_name=...)` will create an iterator that will be shared between sessions on the same server.\r\n\r\nhow to initialize this shared iterator on two separated sessions? not found any reference. could you please show me an example(in the 1.0 API). @mrry \r\nposted a question on stackoverflow [link](https://stackoverflow.com/questions/55612210/how-to-use-shared-name-on-initializable-iterator)"]}, {"number": 23066, "title": "[Docker] Upgraded images to Ubuntu 18.04 LTS.", "body": "Upgraded all Docker base images to `18.04`.\r\n\r\nI haven't upgraded the GPU ones as those are still marked experimental: https://hub.docker.com/r/nvidia/cuda/", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLA signed.", "CLAs look good, thanks!\n\n<!-- ok -->", "Looks good to me, but @gunan, is this safe to do? I don't know if there are any problems keeping us at 16.04.", "I think ubuntu 18.04 has python 3.7?\r\nIf yes, i think this will break all our python 3 docker images.", "Yes it has python 3.7 default but what are the python 3 docker images that are you talking about?", "Check also https://github.com/tensorflow/tensorflow/issues/17022", "All the images that have \"py3\" suffix here:\r\nhttps://hub.docker.com/r/tensorflow/tensorflow/tags/", "I will close this PR for now.", "@gunan Actually, 18.04 LTS has Python 3.6.6:\r\n\r\n```\r\n$ apt-get install python3\r\n$ python3\r\nPython 3.6.6 (default, Sep 12 2018, 18:26:19)\r\n```", "Seems different https://packages.ubuntu.com/bionic/python\nhttps://packages.ubuntu.com/bionic-updates/python3", "https://packages.ubuntu.com/bionic-updates/python3.7", "@bhack `python3` ultimately installs `python3.6` which pulls in 3.6.6: https://packages.ubuntu.com/bionic-updates/python3.6", "Mhh it is strange cause https://packages.ubuntu.com/bionic-updates/python3 gives 3.6.5 ... But apt wins ", "So can we reopen this? ", "@gunan Friendly ping. Can you please take a look at this PR? Thanks!", "Our Docker images are updated nightly as long as that nightly package builds (for the `nightly-` images) and per-release (for `latest-` and `XX.XX-`), so if your deployment is time-critical and you only want to use stable releases, I'd recommend using your own image until these changes are deployed.", "Note that the GPU development images were not changed in this PR. nvidia/cuda doesn't provide Ubuntu 18 / cuda 9.0 base images.\r\n\r\n", "@angersson Is TF still default on 9.0? Any plan for 9.2? There was a bug in [cuda 9.0 series](https://devtalk.nvidia.com/default/topic/1028112/cuda-setup-and-installation/nvcc-bug-related-to-gcc-6-lt-tuple-gt-header-/) with \"modern\" version of GCC>=6. Some [Asan flags for cuda](https://github.com/google/sanitizers/issues/629) like `protect_shadow_gap=0` are not avaialbe on gcc 5.x.", "@bhack As far as I know, the TF team is working on upgrading directly to CUDA 10. @tfboyd, can you weigh in?", "https://github.com/tensorflow/tensorflow/issues/22706"]}, {"number": 23065, "title": "DLL load failed for tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.10\r\n- Python version: 3.6\r\n- Installed using virtualenv: Canopy pip: 1.18 \r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n```\r\nImportError                               Traceback (most recent call last)\r\nC:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nC:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nC:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nC:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-7-6b0f4483b0d5> in <module>()\r\n----> 1 import tensorflow as tf\r\n      2 \r\n      3 a = tf.Variable(1, name=\"a\")\r\n      4 b = tf.Variable(2, name=\"b\")\r\n      5 f = a + b\r\n\r\nC:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     20 \r\n     21 # pylint: disable=g-bad-import-order\r\n---> 22 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     23 \r\n     24 try:\r\n\r\nC:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nC:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Dell\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(1, name=\"a\")\r\nb = tf.Variable(2, name=\"b\")\r\nf = a + b\r\n\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as s:\r\n    init.run()\r\n    print( f.eval() )\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n``", "comments": ["@vinit13792 Hi, this could be due to missing msvc redistributable library. Can you confirm if you installed this?\r\nAlso please refer this issue #19584 and see if the suggestions provided there helps. Thank you !", "> @vinit13792 Hi, this could be due to missing msvc redistributable library. Can you confirm if you installed this?\r\n> Also please refer this issue #19584 and see if the suggestions provided there helps. Thank you !\r\n\r\nHi, i haven't installed msvc redistributable library, let me check by installing that.", "> @vinit13792 Hi, this could be due to missing msvc redistributable library. Can you confirm if you installed this?\r\n> Also please refer this issue #19584 and see if the suggestions provided there helps. Thank you !\r\n\r\n", "> @vinit13792 Hi, this could be due to missing msvc redistributable library. Can you confirm if you installed this?\r\n> Also please refer this issue #19584 and see if the suggestions provided there helps. Thank you !\r\n\r\nyeah i have msvc redist lib and still i'm getting the same error.", "Hi, I have the exact same error, any clue on how to solve it?\r\nMany thanks!!\r\n", "Hello vinit13792; I have followed this steps and I have finally managed to execute on jupyter notebook:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(1, name=\"a\")\r\nb = tf.Variable(2, name=\"b\")\r\nf = a + b\r\n\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as s:\r\n    init.run()\r\n    print( f.eval() )\r\n```\r\n\r\nStep 1: install canopy (version 2.1.6)\r\nStep 2: on the editor (NOT the command editor type and execute:    ` !pip install pydotplus`\r\nStep 3: on the command editor I typed and executed:      `pip install tensorflow==1.10.0`\r\n   however with this I got a very strange error of Permissions\r\nStep 4: back on the Canopy editor type and execute: `!pip install tensorflow`\r\nand also; `!pip install keras`\r\n\r\nAnd with this I managed to finally get results!!\r\n\r\nHope it helps", "> Hello vinit13792; I have followed this steps and I have finally managed to execute on jupyter notebook:\r\n> import tensorflow as tf\r\n> \r\n> a = tf.Variable(1, name=\"a\")\r\n> b = tf.Variable(2, name=\"b\")\r\n> f = a + b\r\n> \r\n> init = tf.global_variables_initializer()\r\n> with tf.Session() as s:\r\n> init.run()\r\n> print( f.eval() )\r\n> \r\n> Step 1: install canopy (version 2.1.6)\r\n> Step 2: on the editor (NOT the command editor type and execute: !pip install pydotplus\r\n> Step 3: on the command editor I typed and executed: pip install tensorflow==1.10.0\r\n> however with this I got a very strange error of Permissions\r\n> Step 4: back on the Canopy editor type and execute: !pip install tensorflow\r\n> and also; !pip install keras\r\n> \r\n> And with this I managed to finally get results!!\r\n> \r\n> Hope it helps\r\n\r\nHi AlvaroSSM, i tried the exact same steps which you mentioned above, and i still ended up having the same error. Any pointers as to what to do next? I have cnopy version 2.1.9", "I tried to debug this or a related issue a little more: https://github.com/tensorflow/tensorflow/issues/27035#issuecomment-479392652", "Apologies for the delay in response.\r\n*TensorFlow release binaries (CPU/GPU) version 1.6 and higher are prebuilt with AVX instruction sets.*  \r\nSee [hardware requirements][1] to know more.  \r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:  \r\n\r\n* Try Google Colab to use TensorFlow.    \r\n * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install``` to install any other preferred TF version.  \r\n    * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.  \r\n* Try to build TF from sources by changing CPU optimization flags.\r\n\r\n\r\n  [1]: https://www.tensorflow.org/install/pip#hardware-requirements", "Closing as duplicate of https://github.com/tensorflow/tensorflow/issues/19584", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23065\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23065\">No</a>\n"]}, {"number": 23064, "title": "Is there something wrong with AttentionWrapper when use BahdanauAttention or LuongAttention? ", "body": "When using attention model, we need to get a AttentionWrapper object,  which defined in tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py. When focused on the func call, i find something wrong. \r\n\r\ntensorflow/contrib/seq2seq/python/ops/attention_wrapper.py: In class AttentionWrapper:\r\n  def call(self, inputs, state):\r\n    \"\"\"Perform a step of attention-wrapped RNN.\r\n    - Step 1: Mix the `inputs` and previous step's `attention` output via\r\n      `cell_input_fn`.\r\n    - Step 2: Call the wrapped `cell` with this input and its previous state.\r\n    - Step 3: Score the cell's output with `attention_mechanism`.\r\n    - Step 4: Calculate the alignments by passing the score through the\r\n      `normalizer`.\r\n    - Step 5: Calculate the context vector as the inner product between the\r\n      alignments and the attention_mechanism's values (memory).\r\n    - Step 6: Calculate the attention output by concatenating the cell output\r\n      and context through the attention layer (a linear layer with\r\n      `attention_layer_size` outputs).\r\n....\r\n    # Step 1: Calculate the true inputs to the cell based on the\r\n    # previous attention value.\r\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\r\n    cell_state = state.cell_state\r\n    cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\r\n.....\r\n    for i, attention_mechanism in enumerate(self._attention_mechanisms):\r\n      attention, alignments, next_attention_state = _compute_attention(\r\n          attention_mechanism, cell_output, previous_attention_state[i],\r\n          self._attention_layers[i] if self._attention_layers else None)\r\n      alignment_history = previous_alignment_history[i].write(\r\n          state.time, alignments) if self._alignment_history else ()\r\n\r\n      all_attention_states.append(next_attention_state)\r\n      all_alignments.append(alignments)\r\n      all_attentions.append(attention)\r\n      maybe_all_histories.append(alignment_history)\r\n.....\r\n\r\nThe code is just copyed.  In _compute_attention(...),  we get alignment and attention, but here the second parameter  we used is  cell_ouput , which was calculted from self._cell(cell_inputs, cell_state). \r\n\r\nBut In paper: neuaral machine translation by  jonintly learning to align and trainslate(https://arxiv.org/pdf/1409.0473.pdf), the BahdanauAttention, which should be calculated by cell_state,  not cell_output.\r\n\r\nIn paper: effective Approaches to Attention-based Neural Machine Translation(https://arxiv.org/pdf/1508.04025.pdf), i cannot  find the LuongAttention used to calculate current cell state and output, so, how should step 1 occured? The default self._cell_input_fn(inputs, state.attention) just cancat input and stattion.\r\n\r\nIs there someting wrong?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@qzfnihao  -  Hi, could you please provide the information asked by the tensorflowbutler. Thanks !", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23063, "title": "Adding determiner to increase clarity", "body": "The current phrasing leaves it ambiguous whether the third party is not involved in the conflict at hand, or conflicts in general.", "comments": []}, {"number": 23062, "title": "Train the Model in android app", "body": "I just finished three courses in CodeLab about Tensorflow.\r\nI'm finding the way ***how can I train the Model on Android device instead of training from the computer and use that Model on Android***\r\n", "comments": ["that means, I want to build, train, and update the *.lite model and *label.txt in android ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23060, "title": "Attributes values not inferred by TFE C API (eager mode)", "body": "I'm trying to enable eager execution mode in Java based on the`TFE_*` methods exposed by TensorFlow C API and I have noticed that those method do not infer attribute values the same way that graph execution does.\r\n\r\nFor example, the `NodeDefBuilder` class (used in graph execution mode) will infer type attribute values automatically based on the type of the tensors the node receives in input (see [`NodeDefBuilder::SingleInput`](https://github.com/tensorflow/tensorflow/blob/7e0257d953401288bc10dc11d07b418371bbc56d/tensorflow/core/framework/node_def_builder.cc#L123). Eager classes do not. Also, in graph mode, default values are assigned automatically when some attributes are not provided (see [`NodeDefBuilder::Finalize`](https://github.com/tensorflow/tensorflow/blob/7e0257d953401288bc10dc11d07b418371bbc56d/tensorflow/core/framework/node_def_builder.cc#L245)). Again, eager classes do not.\r\n\r\nMy question is are those features missing by design or should they be part of the eager C API as well? \r\n\r\nI can do a work around in the Java client, as @eaplatanios did for Scala, but I'm tempted to think that this should be implemented in the C API and that the graph and eager methods should be more symmetric to avoid moving this complexity in all the clients.\r\n\r\n-----------\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.11\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\nMobile device: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.11\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\nMobile device: N/A", "Created a PR to address this issue: https://github.com/tensorflow/tensorflow/pull/23468, closing this one."]}, {"number": 23059, "title": "tf.argmax docs don't say which argument is deprecated", "body": "**System information**\r\n- TensorFlow version: 1.11\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/argmax\r\n\r\n**Describe the documentation issue**\r\n\r\nThe doc says: \"SOME ARGUMENTS ARE DEPRECATED. They will be removed in a future version. Instructions for updating: Use the axis argument instead\"\r\n\r\nI believe both the dimension and output_type args are deprecated? The update instructions only mention the to, but not the from. This may be a problem with the `deprecated_args` decorator.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 23058, "title": "dll load failed in tensorflow install using pip on windows 10", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 1.11\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip and venv\r\n- CUDA/cuDNN version: 8 | 5.1/6\r\n- GPU model and memory: nvidia geforce gtx 960m\r\n- Have I written custom code: N/A\r\n- Bazel version: N/A\r\n- Exact command to reproduce: N/A\r\n- Mobile device: N/A\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI did look at these issues and applied the fixes but still not working.\r\nhttps://github.com/tensorflow/tensorflow/issues/10033\r\nhttps://github.com/tensorflow/tensorflow/issues/5949\r\n\r\nThen i found this https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c\r\nturns out there were some version issues like it needed cuda 8 instead of 9. so I fixed all those things and its still not working please help\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\luthr\\rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\luthr\\rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\luthr\\rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\luthr\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\luthr\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\luthr\\rl\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\luthr\\rl\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\luthr\\rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\luthr\\rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\luthr\\rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\luthr\\rl\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\luthr\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\luthr\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nExact command to reproduce\nMobile device", "@Ridhwanluthra Hello, thanks for the post. Please make sure you are using cuDNN 7 and CUDA 9 when using tensorflow versions 1.5 or above. Also please refer [this](https://www.tensorflow.org/install/source_windows#tested_build_configurations) for the compatible Tensorflow and CUDA versions if you haven't checked already. Thank you !", "@harshini-gadige That is what I had initially done but it gave me the same error. I will try once again and get back to you", "My graphics drivers were outdated and did not update on its own had to uninstall and reinstall them and it worked. I am assuming it was because of that as I rebuilt everything and it worked and this was the only difference between before and now", "I have CUDA 9.2, cuDNN 7.4.1.5, tensorflow-gpu 2.8.0, and python 3.6.7; and I added ...\\cuda\\bin to my path but it does not work.", "Have you solved this problem? The problem I encountered is exactly the same as you. Can you help me?I tried many ways\u3002"]}, {"number": 23057, "title": "tf.arg_max docs suggest nonexistent page in 1.12", "body": "**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/arg_max\r\n\r\n**Describe the documentation issue**\r\n\r\nthe 1.12 page for arg_max mentions this function is deprecated in favor of argmax: \"THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use argmax instead\"\r\n\r\nHowever, argmax is not documented for 1.12 (it is fine in 1.11).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device"]}, {"number": 23055, "title": "Can't build the demo app, run this bazel command from the tensorflow directory:", "body": "Can't figure out how to build the demo app. Tried different ways doesn't work. \r\nI have tried:\r\n\r\n**`bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=armeabi-v7a //tensorflow/contrib/l\r\nite/examples/android:tflite_demo`**\r\n\r\n\r\n**`bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \r\n//tensorflow/contrib/lite/examples/android:tflite_demo`**\r\n\r\n\r\n**`bazel build -c opt --config=android_arm{,64} --cxxopt='--std=c++11' \\\r\n//tensorflow/contrib/lite/examples/android:tflite_demo`\r\n`**\r\n\r\n**System information**\r\n Ubuntu 16.04 on GCP \r\nrun  Docker\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n`kaisenaiko@tpu-cv-app:~$ **sudo docker run --rm -it --privileged -p 6006:6006 detect-tf**\r\nroot@c567d9e05859:/tensorflow# \r\nroot@c567d9e05859:/tensorflow# bazel build -c opt --config=android_arm{,64} --cxxopt='--std=c++11' \\\r\n> //tensorflow/contrib/lite/examples/android:tflite_demo\r\nWARNING: The following configs were expanded more than once: [android]. For repeatable flags, repeats are counted twice and\r\n may lead to unexpected behavior.\r\nWARNING: option '--crosstool_top' was expanded to from both option '--config=download_clang' (source /tensorflow/.tf_config\r\nure.bazelrc) and option '--config=android_arm' (source command line options)\r\nWARNING: option '--cpu' was expanded to from both option '--config=android_arm' (source command line options) and option '-\r\n-config=android_arm64' (source command line options)\r\nWARNING: option '--fat_apk_cpu' was expanded to from both option '--config=android_arm' (source command line options) and o\r\nption '--config=android_arm64' (source command line options)\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdow\r\nn\".\r\nERROR: No default_toolchain found for cpu 'arm64-v8a'. Valid cpus are: [\r\n  k8,\r\n  local,\r\n  armeabi-v7a,\r\n  x64_windows,\r\n  x64_windows_msvc,\r\n  x64_windows_msys,\r\n  s390x,\r\n  ios_x86_64,\r\n]\r\nINFO: Elapsed time: 5.567s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (3 packages loaded)\r\nroot@c567d9e05859:/tensorflow# **bazel build -c opt --config=android_arm{,32} --cxxopt='--std=c++11' //tensorflow/contrib/lit\r\ne/examples/android:tflite_demo**\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=123\r\nINFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n  'build' options: --spawn_strategy=standalone --genrule_strategy=standalone\r\nINFO: Reading rc options for 'build' from /tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=t\r\nrue --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt --define=grpc_no_ares=true \r\n--define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python2.7/dist-packag\r\nes --python_path=/usr/bin/python --config=xla --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env T\r\nF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=1 --config=download_clang\r\nINFO: Found applicable config definition build:xla in file /tensorflow/.tf_configure.bazelrc: --define with_xla_support=tru\r\ne\r\nINFO: Found applicable config definition build:download_clang in file /tensorflow/.bazelrc: --crosstool_top=@local_config_d\r\nownload_clang//:toolchain --define=using_clang=true\r\nINFO: Found applicable config definition build:android_arm in file /tensorflow/.bazelrc: --config=android --cpu=armeabi-v7a\r\n --fat_apk_cpu=armeabi-v7a\r\nINFO: Found applicable config definition build:android in file /tensorflow/.bazelrc: --crosstool_top=//external:android/cro\r\n`\r\n`\r\nroot@c567d9e05859:/tensorflow# **bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n> //tensorflow/contrib/lite/examples/android:tflite_demo**\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdow\r\nn\".\r\nUnhandled exception thrown during build; message: //tensorflow/contrib/lite/examples/android:tflite_demo BuildConfiguration\r\nValue.Key[347ce3beb85cc56b9abdec126d2f85bd] false -> ErrorInfo{exception=com.google.devtools.build.lib.analysis.config.Inva\r\nlidConfigurationException: No default_toolchain found for cpu 'arm64-v8a'. Valid cpus are: [\r\n  k8,\r\n  local,\r\n  armeabi-v7a,\r\n  x64_windows,\r\n  x64_windows_msvc,\r\n  x64_windows_msys,\r\n  s390x,\r\n  ios_x86_64,\r\n], rootCauses={BuildConfigurationValue.Key[785f82d034ad1259081f051c32361d64]}, cycles=[], isCatastrophic=false, rootCauseOf\r\nException=BuildConfigurationValue.Key[785f82d034ad1259081f051c32361d64], isDirectlyTransient=false, isTransitivelyTransient\r\n=false}\r\nINFO: Elapsed time: 5.027s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (13 packages loaded)\r\n    currently loading: tensorflow/contrib/lite/java\r\njava.lang.IllegalStateException: //tensorflow/contrib/lite/examples/android:tflite_demo BuildConfigurationValue.Key[347ce3b\r\neb85cc56b9abdec126d2f85bd] false -> ErrorInfo{exception=com.google.devtools.build.lib.analysis.config.InvalidConfigurationE\r\nxception: No default_toolchain found for cpu 'arm64-v8a'. Valid cpus are: [\r\n  k8,\r\n  local,\r\n  armeabi-v7a,\r\n  x64_windows,\r\n  x64_windows_msvc,\r\n  x64_windows_msys,\r\n  s390x,\r\n  ios_x86_64,\r\n], rootCauses={BuildConfigurationValue.Key[785f82d034ad1259081f051c32361d64]}, cycles=[], isCatastrophic=false, rootCauseOf\r\n`\r\n`root@c567d9e05859:/tensorflow# **bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=armeabi-v7a   //tensorflow/contrib/l\r\nite/examples/android:tflite_demo**\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nERROR: /tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:666:1: no such package '@androidndk//': The repository could not be resolved and referenced by '//tensorflow/contrib/lite/kernels/internal:cpu_check'\r\nERROR: Analysis of target '//tensorflow/contrib/lite/examples/android:tflite_demo' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 10.508s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (59 packages loaded)\r\n    Fetching https://mirror.bazel.build/github.com/google/gemmlowp/archive/38ebac7b059e84692f53e5938f97a9943c120d98.zip\r\n    Fetching https://mirror.bazel.build/bitbucket.org/eigen/eigen/get/fd6845384b86.tar.gz\r\n    Fetching https://mirror.bazel.build/.../ARM_NEON_2_x86_SSE/archive/0f77d9d182265259b135dad949230ecbf1a2633d.tar.gz\r\nroot@c567d9e05859:/tensorflow# `", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "You should config NDK and SDK tool path first.", "Did you try leo's suggestion?\r\n\r\n> You should config NDK and SDK tool path first.\r\n\r\nPlease refer this link [to build NDK and SDK](https://www.tensorflow.org/lite/demo_android#install_android_ndk_and_sdk) if haven't already.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23054, "title": "tanh on CPU exceeds range (and is inconsistent)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.1 LTS\r\n- **TensorFlow installed from (source or binary)**: binary (or source)\r\n- **TensorFlow version (use command below)**: ('v1.10.0-0-g656e7a2b34', '1.10.0')\r\n- **Python version**: Python 2.7.15rc1\r\n- **CUDA/cuDNN version**: N/A, CPU problem\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n```\r\nimport numpy\r\nimport tensorflow\r\nsess = tensorflow.Session()\r\nsess.run(tensorflow.tanh(numpy.arange(8.8,9.0,0.01,dtype=numpy.float32)))\r\n```\r\n\r\n### Describe the problem\r\ntanh on float32s can return values outside of the range [-1,1] when run on a CPU, which suggests a bug in its implementation.  This occurs across multiple machines and builds (but does not seem to occur for me when running on a GPU, or when using numpy instead of tensorflow).  On the system specs given above, the commands produce two instances of 1.0000001:\r\n```\r\n>>> sess.run(tensorflow.tanh(numpy.arange(8.8,9.0,0.01,dtype=numpy.float32)))\r\narray([1.        , 1.        , 0.99999994, 1.0000001 , 1.        ,\r\n       0.99999994, 0.9999998 , 0.99999994, 0.9999998 , 1.        ,\r\n       1.0000001 , 0.9999998 , 1.        , 0.99999994, 0.9999998 ,\r\n       0.99999994, 1.        , 1.        , 0.9999998 , 1.        ],\r\n      dtype=float32)\r\n```\r\n\r\nOf lesser concern, return values also differ on the same system when specifying overlapping ranges (here 8.8 changed to 8.9 does not match second half of above):\r\n```\r\n>>> sess.run(tensorflow.tanh(numpy.arange(8.9,9.0,0.01,dtype=numpy.float32)))\r\narray([1.        , 0.99999994, 1.        , 0.99999994, 1.0000001 ,\r\n       0.9999998 , 1.        , 1.0000001 , 1.        , 1.        ],\r\n      dtype=float32)\r\n```", "comments": ["@CamdenCU  -  Hi, thanks for bringing this to our notice. I tried running the code on gpu and it is throwing values outside the range [-1,1] in gpu too.\r\n\r\n@drpngx Hi, could you please look into this ? I suspect this could be a bug(correct me if I'm wrong)\r\n\r\n", "I tend to think that this is not really bug, however if you call `atanh` then it might break. @rmlarsen or @aselle WDYT?", "My best guess is that the bug lies with the generic_fast_tanh_float implementation in MathFunctionsImpl.h (or something like it).  The polynomial it uses is not appropriate for single precision input.\r\n\r\nI don't know why a result is not determined exclusively by its corresponding input.", "Seen this as well. I got a 1.0000001 from tf.tanh() two days ago.", "I think this was resolved in TF2.x. I am not able to reproduce the issue with TF2.7. [Here](https://colab.research.google.com/gist/jvishnuvardhan/a4d7ad53e6bdadb6f6683b20f5255988/untitled1118.ipynb) is a gist for reference.\r\n\r\nOutput\r\n`tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(20,), dtype=float32)`\r\n\r\nI am closing this issue as this was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23054\">No</a>\n"]}]