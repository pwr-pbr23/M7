[{"number": 5273, "title": "Update classifier.fit()", "body": "Needed to change input parameters as followed by tutorial too.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tegg89, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @MrQianJinSi to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Can one of the admins verify this patch?\n"]}, {"number": 5272, "title": "Tensorflow stop running while running \"sess=tf.Session()\"", "body": "I am running tensorflow on ubuntu14.04 server using GPU and I had successuflly run the demo in the tutorial about mnist a few days ago.\nToday, I run the same demo but to find it get stucked. I find out that I can import tensorflow successfully, but every time I run \"sess=tf.Session()\" the program will stop. It is strange that **neither error information sent back nor the python quit**. The cursor just stop there, enen \"ctrl+c\" does not work. \nAs the server I installed tensorflow is public, I suspect that someone had changed something on the server. But I do not know. How can I fix it?\n", "comments": ["OK, please gdb attach to the process and check the stack trace. Try to install nmon (\"n\", \"d\", \"c\") and see if anything is happening.\n", "also, `nvidia-smi -l`.\n", "Thank you very much, the problem has been solved. As **drpngx** said, when I run \"nvidia-smi\", the server also stoped working. As a result, I re-install the CUDA and Cudnn, then all the problem has been solved. Not intentionally, maybe someone who share the same machine or even I myself had changed some configuration.  All in all, thank you very much. \n", "Glad to hear you're good to go!\n", "I am having the same problem on windows .I am getting the following error\r\nc:\\tf_jenkins\\home\\workspave\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_excutor\\cuda\\duda_driver.cc:94] check failed: s.ok() could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerror: cuDevicePrimaryCtxSetFlags not found\r\n\r\nand python has stopped working whenever I run tf.Session, \r\nCould you please help I am struggling with this problem"]}, {"number": 5271, "title": "Fix deprecated usage of variable_op_scope", "body": "`variable_scope.variable_op_scope` was deprecated in commit 3d1ee95, but some of codes or documentations are not updated. This patch fixes them in favor of `variable_scope`.\n", "comments": ["Can one of the admins verify this patch?\n", "@wookayin, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @philstahlfeld and @zhangyaobit to be potential reviewers.\n", "@DjangoPeng who has been working on a related PR.\n", "I rebased the branch on top of current master and dropped the change for auto-generated docs.\n", "Great, thanks!  @tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Looks like unrelated test failures.  Merging.\n", "@wookayin @drpngx Yep, this PR is LGTM~\n", "Yay!\n", "Thanks all!\n"]}, {"number": 5270, "title": "Branch 137581645", "body": "", "comments": ["@zheng-xq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @nsthorat and @sguada to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5269, "title": "Fix minor error in os_setup.md.", "body": "As title.\n", "comments": ["Can one of the admins verify this patch?\n", "@raix852, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @keveman and @vrv to be potential reviewers.\n", "@tensorflow-jenkins test this please.\n", "Merged. Thanks, @raix852 \n"]}, {"number": 5268, "title": "Can I use multiple CPU but none GPU on inception v3 code?", "body": "I have access to GPUs on the server, but the sysadmin has trouble on setting up CuDNN and CUDA, and I found when I use multi CPU on CIFAR-10 code, the speed was similar with GPUs. \nSo I wonder whether there is a way to use the inception code to train the ImageNet in a environment with multiple CPUs like 32 or 64 CPU cores but none GPU?\n", "comments": ["Yes, in theory it's possible, but inception going to be very, very slow so that's probably not something that you'd want to do. Setting up cuda and cudnn can be tricky, but worth the trouble IMO.\n"]}, {"number": 5267, "title": "Initial support for OpenCL", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @girving and @martinwicke to be potential reviewers.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "I signed it !\n", "@lukeiwanski Can you comment here to let CLAbot know you're fine with contributing these commits?\n", "@jart This change has a lot of bazel (mostly parallel to the GPU case), which I thought you'd be most qualified to review.\n", "I signed it !\n", "CLA bot still reporting failure with \"CLA signed, but unable to verify author consent\".\n@martinwicke how do we convince clabot?\n", "Luke, have you signed the CLA?  Can you:\n- Sign it under the email address you used to author the commits\n- Make sure that email is added to your github acct?\n", "I have 2 emails connected with my github .. both are added to google group for my company. Not quite sure what is happening with CLA.\n", "@willnorris for additional help with corp CLA stuff.\n\n(Will, let me know if there's anything we can do on your behalf to validate -- the corp agreement side of things is something I have no visibility into)\n", "@vrv: http://go/cla#github-consent.  googlebot is never going to mark this PR as okay.  And that's fine, it's not supposed to.  It's up to you as the merger to confirm that commit authors have consented.\n", "I've updated the pull request to pick up the latest TensorFlow changes: this should take care of the MacOS failures.\n", "Jenkins, test this please.\n", "Approving the C++ changes.  Don't know about build changes.\n", "I'm not sure if I'm qualified to review this. But I would like to have a better understanding of whether or not we considered alternatives to adding more stuff to the configure script. I'd also like to know how we plan to regression test this functionality.\n", "At the moment, we are running \"bazel test tensorflow/...\" with config=sycl.\nWe have a few machines that can have opencl support, we will utilize those to run tests continuously.\n", "Thank you for supporting OpenCL. How can I test it out? I have a triple AMD R9 290X setup.\n", "This is just the bare minimum changes we need for opencl.\nFull support is still not there. Once we have full support, we will have documentation , and a few toy examples you can use.\n"]}, {"number": 5266, "title": "Revert \"TF Checkpoint V2: add \"write_version\" arg to tf.train.Saver.\"", "body": "This reverts commit a1d0c8e59bec28643688852271859fb750fab1fc.\n", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @concretevitamin, @tensorflower-gardener and @keveman to be potential reviewers.\n", "I saw\nWindows Cmake Tests \u2014 FAILURE\n", "No windows support on 0.11, so expected failure.\n", "That's expected. 0.11 does not have windows support yet, but the test was added universally for PRs on any branch. \n", "I see.  LGTM (I don't see an \"approve\" button).  Thanks for helping on this Yifei!\n"]}, {"number": 5265, "title": "adding android studio 2.2.0+ build support ", "body": "Enable following features inside IDE:\n\n  native navigation\n  native code tracing\n\n@andrewharp PLAT, thx!\n", "comments": ["Can one of the admins verify this patch?\n", "@ggfan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp and @martinwicke to be potential reviewers.\n"]}, {"number": 5264, "title": "Error for souce code installation for Ubuntu14.04", "body": "Hello, TF experts:\nI want to install TF from source code**(I have already installed TF from virtualenv, so I dont know if this is a problem)**, but when I run `./configure`, I always get the error:\n\n> WARNING: Output base '/aramis/home/wen/.cache/bazel/_bazel_wen/0e4e3a4a81f88b598a8f8eff37db10f2' is on NFS. This may lead to surprising failures and undetermined behavior.\n> ........\n> INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n> ERROR: /aramis/home/wen/.cache/bazel/_bazel_wen/0e4e3a4a81f88b598a8f8eff37db10f2/server (Directory not empty).\n\nI tried the command below:\n`bazel clean --expunge\n, bazel clean --expunge_async`\n\nIt does not work.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttp://stackoverflow.com/questions/40144776/tensorflow-installation-error-directory-not-empty/40309084#40309084\n### Environment info\n\nOperating System: Ubuntu14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nlow_source/tensorflow$ ls /usr/local/cuda/lib64/libcud*\n/usr/local/cuda/lib64/libcudadevrt.a\n/usr/local/cuda/lib64/libcudart.so\n/usr/local/cuda/lib64/libcudart.so.7.5\n/usr/local/cuda/lib64/libcudart.so.7.5.18\n/usr/local/cuda/lib64/libcudart_static.a\n/usr/local/cuda/lib64/libcudnn.so\n/usr/local/cuda/lib64/libcudnn.so.4\n/usr/local/cuda/lib64/libcudnn.so.4.0.7\n/usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`) f794cd393b1e7821fcc3cdcee9b6a4400f2540bf\n2. The output of `bazel version`\n\n> WARNING: Output base '/aramis/home/wen/.cache/bazel/_bazel_wen/0e4e3a4a81f88b598a8f8eff37db10f2' is on NFS. This may lead to surprising failures and undetermined behavior.\n> ........\n> Build label: 0.3.2\n> Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n> Build time: Fri Oct 7 17:25:10 2016 (1475861110)\n> Build timestamp: 1475861110\n> Build timestamp as int: 1475861110\n> [1]+  Interrupt               /aramis/home/wen/.dropbox-dist/dropboxd  (wd: ~)\n> (wd now: /aramis/dataARAMIS/users/junhao.wen/DeepLearning/TensorFlow/tensorflow_source/tensorflow)\n\nCan you help me?????\nThanks in advance\n", "comments": ["Bazel has a warning on NFS. Could you try on a local drive?\n", "Thanks for your rapid response, I will try it tomorrow with Ubuntu14, cuz It is true i put the source code in NFS, thanks for the point, I will response you tomorrow:)\nBTW, to install the virtualenv version and source code version TF on one machine, it is OK if we deal with the PATH correctly, right?\nThanks \n", "TF works under virtualenv, and in fact we recommend this.\n", "Yes, it works fine for me under virtualenv, but when I follow your tutorial to retrain the inception, i can not run bazel build tensorflow/examples/image_retraining:retrain, this command need to install TF from source code, right? I know that I can just run the retrain.py script like python retrain.py, but sometimes I think it is more flexible to run it with bazel(from source code).\n\nSo all your tutorial with bazel command, we can just use python command to follow???\n\nIf I misunderstood it, please correct me:)\nThanks\n", "I usually build a whl file from the source code then pip install, so it would be a similar procedure. I don't think you can run in-place from the source tree.\n", "So you mean with the **virtualenv**  version, I can not run bazel build from the source code, tha is ture(**You have to run configure before running bazel build. Otherwise, the build will fail with a clear error message**.), that is why I wanna install TF from source on the same machine, and I dont know if it will be some conflict for the two versions on the same machine.\n\nFrom the installing from sources of TF installation page, you said that to run the configure file in the TF root, is `'This creates a canonical set of symbolic links to the Cuda libraries on your system'`\nBut when I installed TF from **virtualenv**, this links have been created, so basically, if I run the configure file from the source, I will just do the same thing???\n\nSorry for so many questions:)\n", "Yes, you have to run configure. Basically follow the instructions https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#installing-from-sources\n\nIf you actually want to work in-place, you can do so (see \"Setting up TensorFlow for Development\"). Otherwise follow the instructions from the section above. Make sure you enter the virualenv before pip install.\n", "Closing due to lack of activity.\n"]}, {"number": 5263, "title": "OOM Error Message Should Show Which GPU is Out of Memory", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI've created a few overflow threads in regards to balancing seq2seq memory loads over multiple gpus here:\n\nhttp://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow\n### Environment info\n\nOperating System:\n\nUbuntu 14.04 CUDA 7.5 -- tensorflow 0.11\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nSimply load two matrices. One incredibly larger one on a gpu0 and a smaller one on gpu1. It would be great if the OOM message could tell you which gpu ran out of memory. This way you can redistribute resources from one gpu to another.\n### What other attempted solutions have you tried?\n\nCurrently I look at all tensors assigned per device and how large each tensor it is. I then sum the size of these tensors using the following code:\n\n``` python\ndef calculate_variable_sizes_per_device(batch_size = 64):\n  dev_list = get_available_gpus()\n  dev_size_list = [[dev_list[i], 0] for i in xrange(len(dev_list))]\n\n  tf.logging.info('calculating variable sizes on respective devices: %d' % len(dev_list))\n  print('dev_list', dev_list)\n  for eachvar in tf.all_variables():\n    device = eachvar.device\n    print('device', device)\n    var_shape = find_replace_list(eachvar.get_shape().as_list(), '?', batch_size)\n    print('varshape',var_shape)\n    var_size = np.prod(np.array(var_shape))\n    print('varsize', var_size)  \n\n    for i,dev in enumerate(dev_list):\n      if dev.replace('/','') in device.lower():\n        dev_size_list[i][1] += var_size\n\n  return dev_size_list\n\ndef find_replace_list(list_, find, replace):\n  for n,i in enumerate(list_):\n    if i==find:\n      list_[n]=replace\n  return list_\n\n\ndef get_available_gpus():\n  local_device_protos = device_lib.list_local_devices()\n  return [x.name for x in local_device_protos] #if x.device_type == 'GPU']\n```\n", "comments": ["What does the OOM error message currently say for you?  (Including the entire stack trace, not just the error string) ?\n", "Sorry, forgot to add that. The error message seems to tell you which tensors have not been initialized. However, this does not tell you which gpu ran out of memory, because tensors can not be initialized on both gpus. You are still left with not knowing which gpu ran out of memory. \n\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 11.27GiB          [1020/1823]\nI tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats: \nLimit:                 12104795546\nInUse:                 12104599040\nMaxInUse:              12104599040\nNumAllocs:                   50546\nMaxAllocSize:            164069376\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] *********************************************************\n\n---\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 384.0KiB.  See logs \nfor memory state.\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 74370 get requests, put_count=2\n3731 evicted_count=1000 eviction_rate=0.042139 and unsatisfied allocation rate=0.695697\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=10010\n evicted_count=10000 eviction_rate=0.999001 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=20010\n evicted_count=20000 eviction_rate=0.9995 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=30010\n evicted_count=30000 eviction_rate=0.999667 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=40010\n evicted_count=40000 eviction_rate=0.99975 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=50010\n evicted_count=50000 eviction_rate=0.9998 and unsatisfied allocation rate=0\nW tensorflow/core/framework/op_kernel.cc:968] Internal: Dst tensor is not initialized.```\n[[Node: vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/embedding_atte\nntion_decoder/Attention_0_60/attn_stack_layer0/Reshape_2/_91437 = _Recv[client_terminated=false, recv_device=\"/j\nob:localhost/replica:0/task:0/gpu:1\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnati\non=1, tensor_name=\"edge_356892_vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/\nembedding_attention_decoder/Attention_0_60/attn_stack_layer0/Reshape_2\", tensor_type=DT_FLOAT, _device=\"/job:loc\nalhost/replica:0/task:0/gpu:1\"]()]]\n         [[Node: vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/embedding_atte\nntion_decoder/MultiRNNCell_69/Cell1/LayerNormBasicLSTMCell/forget/forget/moments/sufficient_statistics/Shape/_89\n165 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:loca\nlhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_525863_vanilla_generator_seq2seq_mod\nel/model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/MultiRNNCell_69/Cell1/LayerNormB\nasicLSTMCell/forget/forget/moments/sufficient_statistics/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/r\neplica:0/task:0/cpu:0\"]()]]\n", "Cool, thanks.\n\n1) Extending the BFCAllocator to be passed in the 'identity' of the device would be a reasonable feature request that someone from the community could add.\n\n2) The stack trace does mention that this is \"gpu:1\" -- if you look at the error message for the node that failed:\n\n``````\nW tensorflow/core/framework/op_kernel.cc:968] Internal: Dst tensor is not initialized.```\n[[Node: vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/embedding_atte\nntion_decoder/Attention_0_60/attn_stack_layer0/Reshape_2/_91437 = _Recvclient_terminated=false, recv_device=\"/j\nob:localhost/replica:0/task:0/gpu:1\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnati\non=1, tensor_name=\"edge_356892_vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/\nembedding_attention_decoder/Attention_0_60/attn_stack_layer0/Reshape_2\", tensor_type=DT_FLOAT, _device=\"/job:loc\nalhost/replica:0/task:0/gpu:1\"]]\n\n``````\n\nThe combination of \"Dst tensor\" and the \"recv_device\" being \"gpu:1\" should at least give you some information in the short term.\n", "Thanks @vrv -- The stack trace does also mention gpu0 which is what led to my confusion. I will try re-appropriating more tensors to gpu0 and see what happens and report back. \n\n```\n[[Node: vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/embedding_atte\nntion_decoder/Attention_0_60/attn_stack_layer0/Reshape_2/_91437 = _Recvclient_terminated=false, recv_device=\"/j\nob:localhost/replica:0/task:0/gpu:1\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnati\non=1, tensor_name=\"edge_356892_vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/\nembedding_attention_decoder/Attention_0_60/attn_stack_layer0/Reshape_2\", tensor_type=DT_FLOAT, _device=\"/job:loc\nalhost/replica:0/task:0/gpu:1\"]]\n```\n", "Indeed, this error message is telling you that when copying a tensor (Send/Recv) from GPU:0 to GPU:1, that there was a failure to allocate memory on GPU:1.\n", "When I run this network on one titan x, it runs with a batch size of 64. I'm attempting to raise the batch size to 96 with two titan x's on board. I feel that this should be easily done as there is double the GPU memory. Even a batch size of 128 should be doable.\n\nI just placed more of the forward pass on GPU0 in response to your previous message. I'm still getting a memory error. Currently I have:\n\nGPU0 --> stores all vars and handles forward pass\nGPU1 --> stores all gradients and handles gradient computations\n\nI will keep trying to change things around, but perhaps I'm approaching this the wrong way. This is turning more into a stack overflow question. Any help is greatly appreciated and I posted the overflow question in regards to this here:\n\nhttp://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow\n"]}, {"number": 5262, "title": "Edited demo/index.html", "body": "I have edited index.html as per issue 5247\n", "comments": ["@SriramRamesh, thanks for your PR! By analyzing the history of the files in this pull request, we identified @danmane and @jart to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for the fix.\n", "Thanks for merging\n"]}, {"number": 5261, "title": "Running a computation graph in C++ with custom ops", "body": "The current C API does not seem to support loading custom op library, which is tf.load_op_library for python.\nBecause of this, I cannot run a computation graph with custom ops.\nIs there any way I can register custom ops in C?\n", "comments": ["@jhseu checking what the correct route is\n", "It does support it:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h#L973\n\nPython's load_op_library calls it underneath:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/load_library.py#L55\n", "Closing, but feel free to reopen if I misinterpreted the issue.\n", "What is the usage for C API?"]}, {"number": 5260, "title": "Why the embedding_lookup() returns zeros when the index exceed embedding matrix size?", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nI'm now using TF version 0.10.0 installed from source.\nI'm not sure its a bug or intended implementation but the `embedding_lookup()` returns zeros when the index exceed embedding matrix size. For example,\n\n```\n import tensorflow as tf\n import numpy as np\n\n embd_mat = np.linspace(1,10,10).reshape([10,1])*np.array([1,2,3]).reshape([1,3])\n idx = np.linspace(0,19,20)\n\n embd_in = tf.placeholder(tf.float32,[10,3])\n idx_in = tf.placeholder(tf.int32,[20])\n\n output = tf.nn.embedding_lookup(embd_in,idx_in)\n\n with tf.Session() as sess:\n     sess.run(tf.initialize_all_variables())\n\n     embd_out = sess.run(output,feed_dict={embd_in:embd_mat, idx_in:idx})\n\n     print embd_out\n```\n\nThe output of above code is,\n\n```\n[[  1.   2.   3.]\n [  2.   4.   6.]\n [  3.   6.   9.]\n [  4.   8.  12.]\n [  5.  10.  15.]\n [  6.  12.  18.]\n [  7.  14.  21.]\n [  8.  16.  24.]\n [  9.  18.  27.]\n [ 10.  20.  30.]\n [  0.   0.   0.]\n [  0.   0.   0.]\n [  0.   0.   0.]\n [  0.   0.   0.]\n [  0.   0.   0.]\n [  0.   0.   0.]\n [  0.   0.   0.]\n [  0.   0.   0.]\n [  0.   0.   0.]\n [  0.   0.   0.]]\n```\n\nIs there any reason it returns zeros not raises an error?\nI think that case happens only when programmers make a mistake.\nUnless there is any specific reason which I don't know of, I think it should raise a value error that the index is exceeding the matrix size.    \n", "comments": ["@strategist333 not sure what the expected behavior is\n", "I expected that it would return an error similar to when two matrices with unmatched sizes are multiplied (e.g. `tf.matmul(tf.ones([3,10]),tf.ones([20,1]))`). \n", "I'm not able to reproduce this on the 0.10.0 TF binary release. Could it be possible that you've made local source modifications?\n\nThe intended behavior is indeed to error:\n`tensorflow.python.framework.errors.InvalidArgumentError: indices[10] = 10 is not in [0, 10)\n     [[Node: embedding_lookup_1 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Placeholder_2\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Placeholder_2_0, _recv_Placeholder_3_0)]]\n`\n", "It's weird... I downloaded TF from master branch about a month ago and installed ( I checked the version in the python as stated in the instruction).  I haven't changed the code except some BUILD files because of errors while configuring (related to issue #4312). \n\nI also tested with another machine and had the same issue. I remember that I haven't changed any source for this one. \n", "OK, please continue debugging side-by-side and re-open if you find that the actual problem is in our bits.\n", "@drpngx Today, I re-installed latest Tensorflow from source without any modification and it showed the same output as in my first question. According to @strategist333 , it seems it should raise an error though... \r\nPrinted version from Tensorflow is `0.11.head` and commit hash is 55dbc54192a378c5e685c52595f42503f037320e", "Marking this as duplicate of #5847 which contains further discussion."]}, {"number": 5259, "title": "Update the misleading comment for cifar10.py's softmax_linear layer", "body": "A fix for the issue #5251, make the comment more meaningful.\n", "comments": ["@a7744hsc, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @benoitsteiner and @vrv to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can you sign the CLA?  If not, can you close this PR?\n", " I'v signed it, but forgot to moment here ,sorry.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "(Made the change myself, thanks!)\n\nSkipping tests since this is just a doc change.\n", "@tensorflow-jenkins test this please (just for sanity check) \n"]}, {"number": 5258, "title": "access list input in self-defined op in tensorflow", "body": "I want to define an op myself according to documents in tensorflow provided in the link below:\nhttps://www.tensorflow.org/versions/r0.11/how_tos/adding_an_op/index.html#naming\n\nThe simplest example is as below:\n1. You register the variables\n`\nREGISTER_OP(\"ZeroOut\")\n    .Input(\"to_zero: int32\")\n    .Output(\"zeroed: int32\")\n`\n1. You covert input to a C++ reference as shown here:\n\n`\nvoid Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat<int32>();\n....\n}\n`\n\nSo my question is: if the input is a list of tensors, then should we convert it into vector<Tensor> in C++, like the example below?\n`\nconst vector<Tensor>& input_tensor = context->input(0);\n`\n", "comments": ["Not sure, what do you want to do with it?\n", "Closing due to lack of activity.\n"]}, {"number": 5257, "title": "tf.while_loop seg faults upon setting parallel_iterations=0", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nHaven't found segmentation error related issue with tf.while_loop yet.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\ncuda-7.5\n-rw-r--r-- 1 root root 189170 Jun 14 14:21 libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jun 14 14:21 libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jun 14 14:21 libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jun 14 14:21 libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jun 14 14:21 libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n``` python\nimport tensorflow as tf                                                                                                                                                                        \nimport numpy as np                                                                                                                                                                             \n\ni = tf.constant(0)                                                                                                                                                                             \ns = tf.constant(0)                                                                                                                                                                             \nx = tf.random_uniform([], 0, 761-64, dtype=tf.int32)                                                                                                                                           \np = tf.Print(x, [x], message=\"This is random x: \")                                                                                                                                             \nc = lambda i,s,x,p: tf.less(i, 50)                                                                                                                                                             \nb = lambda i,s,x,p: (tf.add(i, 1), tf.add(s, x),tf.random_uniform([], 0,                                                                                                                       \n    761-64, dtype=tf.int32),tf.Print(x, [x], message=\"This is random x: \"))                                                                                                                    \nr = tf.while_loop(c, b, [i,s,x,p], parallel_iterations=1)\n\nsess = tf.Session()                                                                                                                                                                            \ninit = tf.initialize_all_variables().run                                                                                                                                                       \nprint sess.run(r)\n```\n### What other attempted solutions have you tried?\n\nonly ways which do not use the while loop.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\nerror output:\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:04:00.0\nTotal memory: 11.92GiB\nFree memory: 11.81GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:04:00.0)\nSegmentation fault (core dumped)\n", "comments": ["Are you running on Mac?\n", "I'm running on Ubuntu 14.04.5 LTS\n", "Does it SEGV on CPU as well? Which version of tensorflow?\n", "@cdjkim - Might be a copy-paste error but it doesn't look like you're actually running the init op. Rather than:\n\n```\nsess = tf.Session()\ninit = tf.initialize_all_variables().run\nprint sess.run(r)\n```\n\ntry:\n\n```\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nprint sess.run(r)\n```\n", "Thanks @darrengarvey , nice find.\n\nIt should not crash and that's definitely a bug.\n", "`parallel_iterations` is the number of iterations allowed to run in parallel. So setting it to 1 means running the iterations sequentially. Setting it to 0 makes no sense. We will add a check and print a proper error message.\n"]}, {"number": 5256, "title": "Fix: replace deprecated nvcc flag with its replacement.", "body": "Remedy for the warning:\nINFO: From Compiling *.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @jart to be potential reviewers.\n"]}, {"number": 5255, "title": "Error compiling Raspberry Pi label_image", "body": "Hi there,\nI'm trying to build Temsorflow on a Raspberry pi 3 running Raspbian Jessie and kernel 4.3.\nI followed the instructions on: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile \nWhen I compile the label_image example, as described in:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples\nI receive the following compilation errors with:  make -f tensorflow/contrib/pi_examples/label_image/Makefile\n\ngcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads -I/home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads/eigen-latest/ -I/home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto/ -I/home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto_text/ -c tensorflow/contrib/pi_examples/label_image/label_image.cc -o /home/pi/utensor/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o\nIn file included from ./tensorflow/core/framework/tensor.h:19:0,\n                 from tensorflow/contrib/pi_examples/label_image/label_image.cc:32:\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:4:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such ffile or directory\n #include \"unsupported/Eigen/CXX11/Tensor\"\n                                                                    ^\ncompilation terminated.\n\nIs there anyway to fix it?\nThanks.\n", "comments": ["Could you paste the exact error log? That \"ffile\" looks suspicious.\n", "To fix the errors, I have followed the steps in:\nhttps://github.com/tensorflow/tensorflow/issues/4680\n"]}, {"number": 5254, "title": "Error when using tf.contrib.crf and tflearn to build sequential labeling tasks", "body": "I'm using tf.contrib.crf and tflearn to build sequential labeling tasks,\n\nBuild when testing the implementation, the InvalidArgumentError occurs in sess.run\n\nHere's the snippet code.\n\n``` python\nimport numpy as np\nimport tensorflow as tf\nimport tflearn\n\ndef retrieve_sequence_length(data):\n    with tf.name_scope('sequence_length'):\n        used = tf.sign(tf.abs(data))\n        length = tf.cast(tf.reduce_sum(used, reduction_indices=[1, 2]), tf.int32)\n    return length \n\ndef loss(x, y, sequence_length):\n    with tf.name_scope('loss'):\n        n_states = x.get_shape()[-1].value\n        transition_params = tflearn.variable(name='transition_params', shape=[n_states, n_states], \n                                             initializer='xavier', dtype=tf.float32)\n        loss = tf.contrib.crf.crf_log_norm(x, sequence_length, transition_params)\n        # loss, _ = tf.contrib.crf.crf_log_likelihood(x, y, sequence_length, transition_params)\n    return tf.reduce_mean(loss)\n\n# Data settings.\nnum_examples = 10000\nnum_words = 20\nnum_features = 100\nnum_tags = 5\n\n# Random features.\nx = np.random.uniform(low=0.1, high=1.1, size=(num_examples, num_words, num_features)).astype(np.float32)\n\n# Random tag indices representing the gold sequence.\ny = np.random.randint(num_tags, size=[num_examples, num_words]).astype(np.int32)\n\n# All sequences in this example have the same length, but they can be variable in a real model.\n# sequence_lengths = np.full(num_examples, num_words - 1, dtype=np.int32)\n\n\ninput0 = tflearn.input_data(shape=[None, num_words, num_features], dtype=tf.float32)\nsequence_length = retrieve_sequence_length(input0)\nprint 'sequence_length: ', sequence_length\n\nunary_scores = tflearn.time_distributed(input0, tflearn.fully_connected, [num_tags, 'softmax'])\nprint 'unary_score: ', unary_scores\n\nnet = tflearn.regression(unary_scores,\n                         placeholder=tf.placeholder(shape=[None, num_words], dtype=tf.int32),\n                         learning_rate=0.001,\n                         optimizer='sgd',\n                         loss=lambda x, y: loss(x, y, sequence_length),\n                         metric=None,\n                         n_classes=num_tags,\n                         to_one_hot=False)\n\nmodel = tflearn.DNN(net)\n\nmodel.fit(x, y, batch_size=32)\n```\n\nThe stacktrace is :\n\n```\nTraceback (most recent call last):\n  File \"c.py\", line 54, in <module>\n    model.fit(x, y, batch_size=32)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tflearn/models/dnn.py\", line 214, in fit\n    callbacks=callbacks)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 304, in fit\n    show_metric)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 762, in _train\n    feed_batch)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\n    run_metadata_ptr)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: All inputs to node SGD/gradients/loss/RNN/while/expand_dims_state_grad/Reshape/StackPush must be from the same frame.\n```\n", "comments": ["After upgrade tensorflow from 0.11.0rc0 to 0.11.0rc1, the stacktrace is:\n\n```\nTraceback (most recent call last):\n  File \"c.py\", line 61, in <module>\n    model.fit(x, y, batch_size=32)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tflearn/models/dnn.py\", line 214, in fit\n    callbacks=callbacks)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 304, in fit\n    show_metric)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tflearn/helpers/trainer.py\", line 762, in _train\n    feed_batch)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\n    run_metadata_ptr)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/weiguo.fwg/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: The node 'SGD/gradients/loss/RNN/while/ExpandDims_grad/Reshape/StackPush' has inputs from different frames. The input 'SGD/gradients/loss/RNN/while/ExpandDims_grad/Shape' is in frame ''. The input 'SGD/gradients/loss/RNN/while/ExpandDims_grad/Reshape/RefEnter' is in frame 'loss/RNN/while/loss/RNN/while/'.\n```\n", "The error is consistent. I'm not sure what the problem is. Could you try on stackoverflow (tag: \"tensorflow\")?\n", "I don't think we want to spend time on issues for outside libraries. Please file a bug report in their repo. \n", "Problem solved. The problem seems to be caused by `tf.control_dependencies`.\n\n``` python\n# Compute gradients operations\nwith tf.control_dependencies([loss_avg_op, acc_avg_op]):\n    self.grad = tf.gradients(total_loss, self.train_vars)\n    if clip_gradients > 0.0: \n        self.grad, self.grad_norm = \\\n            tf.clip_by_global_norm(self.grad, clip_gradients)\n```\n\nwhen `loss_avg_op, acc_avg_op` is removed from `control_dependencies`, the above problem can be solved. \n"]}, {"number": 5253, "title": "Let user to choice whether to do bazel clean during configure", "body": "Currently running the configure will do 'bazel clean --expunge' by default.  It is OK for the first time to run configure.  But when I need to rerun the configure (because some errors occur, I will retry), I have to spend much time to download files from internet again each time during to the execution of 'bazel clean --expunge'. \n\nSo, should we let user to choice whether to do bazel clean during configure?\n", "comments": ["@jart \n\nI have also been puzzled by that. It prevents using the cache, and doesn't really belong in a configure script. You can use sed s/bazel.clean/true/ <configure|bash as a workaround.\n", "The `bazel clean --expunge` and `bazel fetch //tensorflow/...` need to happen unfortunately to work around bugs in Bazel. There's issues relating to cache invalidation on remote repositories. There's also issues relating to passing environment variables to repository rules. CC: @davidzchen\n\nIt'd probably be better if we didn't have an option to disable the clean and fetch. We want to guarantee that builds work, even if that means things go a little slower. We're going to be improving the state of the build real soon though. We're actually planning to make the configure script optional. I think the Bazel team is doing amazing work helping to make that happen.\n\nThanks for bringing this to our attention. Feel free to post any other questions you might have and I'll be happy to answer them.\n", "One other note, if you're 100% certain you want to roll the dice by disabling the clean and fetch, it shouldn't be too difficult to comment out those lines in the configure script.\n", "If you have a link to the issue, please post here and/or in the code.\n", "It's possible that the related issues are https://github.com/bazelbuild/bazel/issues/1022 and https://github.com/bazelbuild/bazel/issues/1595 and https://github.com/bazelbuild/bazel/issues/974. There was another one, but I'm having trouble digging it up.\n\nAlso here is an explanation that @davidzchen gave me a week ago regarding the necessity of fetch.\n\n> The reason why we fetch in the configure script is because the way the script\n> passes values to the cuda_configure workspace rule is via environment variables\n> (see comment at the top of [cuda_configure.bzl](https://github.com/tensorflow/tensorflow/blob/d95a5b7cc6dca2d3603d73da1078274b29639ec7/third_party/gpus/cuda_configure.bzl)).\n> \n> The configure script exports these environment variables before calling bazel\n> fetch, which will set up external repositories, such as @local_config_cuda,\n> according to the values provided by the user. However, if we do not do this,\n> when the user runs bazel fetch/build after running ./configure, then\n> cuda_configure will use the environment variables in the user's shell, which\n> would not contain any of the values set by ./configure.\n> \n> That is, with this change, even if the user runs ./configure to enabling\n> building with CUDA, when the user subsequently runs bazel build, the build will\n> fail because @local_config_cuda would have been configured without GPU support.\n> \n> It might be more ideal to be able to pass values to Skylark repository rules\n> via command line flags. For example, something like the following would build\n> with CUDA support enabled, and cuda_configure would look for the cuda\n> installation in a set of standard install locations:\n> \n> ```\n> bazel build --enable_cuda //tensorflow/...\n> ```\n> \n> If the user would like to specify a custom cuda install location, then she\n> would be able to do so with another flag:\n> \n> ```\n> bazel build --enable_cuda --cuda_toolkit_path=/opt/local //tensorflow/...\n> ```\n> \n> If adding arbitrary flags is not feasible, then perhaps we can use an approach\n> similar to the kubernetes vars:\n> \n> ```\n> bazel build --vars=enable_cuda=true,cuda_toolkit_path=/opt/local //tensorflow/...\n> ```\n> \n> These are hypothetical examples, and I haven't scoped out the work that would\n> be required to implement this feature. However, with the way things are set up\n> currently, removing the fetch from the configure script would cause the build\n> to fail.\n\nWe're also working hard to pay off the technical debt in TensorFlow that is contributing to this broader problem. For example, we recently committed https://github.com/tensorflow/tensorflow/commit/1dba78b997dc49df9df663a838e0bacd6602f5b8 which removes SWIG from the configure script. So we're one step closer to having a hermetically sealed build.\n\nOnce the Bazel bugs are fixed, and once TensorFlow is fully hermetically sealed, `bazel clean` will become a command that no one will ever need to run again.\n", "Nice, thanks!\n", "https://github.com/bazelbuild/bazel/issues/2033\nIf I try to build tensorflow on NFS, it's necessary to provide `--output_user_root` for `clean` to work properly\n", "FYI @damienmg is working on [improvements](https://bazel-review.googlesource.com/c/6697/3/site/designs/_posts/2016-10-18-repository-invalidation.md) to Bazel's Skylark remote repositories to both improve its caching and invalidation and make it possible to pass environment variables to workspace rules, which will enable us to implement the kind of interface I suggested in [@jart's comment](https://github.com/tensorflow/tensorflow/issues/5253#issuecomment-257000911).\n"]}, {"number": 5252, "title": "multigpu gradient averaging if grad_op is IndexedSlices; race condition in apply_gradients?", "body": "If one follows the [`cifar10_multi_gpu_train.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py) example (build two towers that share weights, feed data independently, collect gradients, average them, apply then) and passes gradients to the [`average_gradients()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L110) function, but graph uses `tf.gather` consequently, the resulting gradient op will be `IndexedSlices` that has many `-1` values in `indices` field (from my experience, not sure about why this happens), so when tensorflow will attempt to apply `tensorflow.python.ops.gradients._IndexedSlicesToTensor` to convert it to Tensor (to apply `tf.expand_dims` on it), it will fail on [`unsorted_segment_sum`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients.py#L92) , because `-1`s are indeed not in the indexes range. I have explicitly printed `w_grad_op.indices` and they looked like: \n\n```\n[0 1 2 3 6 4 5 6 7 8 9 13 12 11 15 16 ... 230 231 -1 -1 ... -1 280 -1 -1 265 -1 -1 ... -1]\n```\n\n\\- so the `average_gradients()` indeed fails on these examples (people have been reporting similar problems [[1]](http://stackoverflow.com/questions/37074077/fail-to-average-indexedslices-on-porting-ptb-word-lm-py-to-multi-gpu-towers), [[2]](http://stackoverflow.com/questions/39017896/tensorflow-how-to-average-several-indexedslicesvalue) .. 2-3 more around the web).\n\nOne way of dealing with (solution 1) it is just applying gradients consequentially (with lower learning rate) instead of averaging them first:\n\n```\n train_op = tf.group(*[gd.apply_gradients(grad) for grad in tower_grads])\n```\n\n\\-  this works great and scales nicely, but (in my case) led to some weird convergence issues (eventually leading to `nan`s popping here and there), possibly because of some sort of race conditions (?), while applying gradients (`gd.apply_gradients()` is probably not very atomic). One possible workaround (solution 2) might be to use `tf.control_dependencies`:\n\n```\ndef rigid_op_sequence(op_lambdas):\n    ## equivalent to:\n    ## op = op1()\n    ## with.control_dependencies([op]):\n    ##    op = op2()\n    ##    with.control_dependencies([op]):\n    ##        op = op3()\n    ##        ...\n    ## return op\n\n    with ExitStack() as stack:\n        for op_func in op_lambdas:\n            op = op_func()\n            context = tf.control_dependencies([op])\n            stack.enter_context(context)\n    return op\n\ngrad_update_lambdas = [(lambda: gd.apply_gradients(grad)) for grad in tower_grads]\ntrain_op = rigid_op_sequence(grad_update_lambdas)\n```\n\nhowever this solution seems to somewhat significantly degenerate performance, because of that \"blocking\" (GPU load dropped from ~90% to ~60%).\n\nSeveral fundamental questions:\n1. What is officially recommended way of dealing with this \"multigpu IndexedSlices\" case? (could not find any; people keep asking)\n2. Does (solution 1) indeed might experience race condition or tensorflow handles it somehow and my convergences issues were not related to that? (and we all can just use solution 1)\n", "comments": ["@mrry who might have better insight\n", "@ebrevdo, could you comment on this?\n", "UPD: after little more testing on relatively large language model, it turned out that even though `nvidia-smi` shows lower gpu utilization when I'm using \"blocking\" solution (2), it scales great (on 2 gpus the speedup is 1.83x) and performs actually even better (10% faster) then non-blocking one (1). \n", "@MInner having -1s in the IndexedSlices object output of gather sounds like a bug.  do you know if gather() is being called with -1s for the indices input?\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I have the same problem when trying to implement a multigpu version model with embedding layer.\r\n\r\nWhat is officially recommended way of dealing with this \"multigpu IndexedSlices\" case? (could not find any; people keep asking)", "@TomorrowIsAnOtherDay ,hi do you have any idea of average multigpu indexedslices?", "I have the same problem, I am wondering if this is solved in new versions of tensorflow? or is there any solution for averaging IndexedSlices.", "@yuefengz can maybe shed some light on this.", "Any ideas? want to know the solutions", "tf.distribute.MirroredStrategy handles aggregating IndexedSlices already afaik. How are you doing multi GPU training?"]}, {"number": 5251, "title": "Incorrect comment on cifar10.py", "body": "Hi there,\nOn line 259 of cifar10.py, the comment is `# softmax, i.e. softmax(WX + b)`, but actually this layer only did a liner calculation(wx+b). So I believe this is a little bit misunderstanding for a newbie like me. \n", "comments": ["Feel free to submit a pull request\n", "Thanks @drpngx , I've submitted a pull request based on what I know.  \n", "Thanks!\n"]}, {"number": 5250, "title": "NaN gradient after py_func call", "body": "Hello,\n\nI did implemented faster rcnn with py_func op. \n- py-func is placed in the middle of graph. And there are the other path in parallel with py_func node.\n- I did not add any gradient function for it, So it should ignore gradient path through py_func. And gradient calculation should done with connection from the other path.\n\nIt works fine in tensorflow r0.9 version. \nBut, in the tensorflow version > r0.9, the node before py_func get NaN error when writing histogram summary. .\nI did check several version: r0.10, r0.11, all of them does not worked.\n\nIs there any change in gradient calculation for the node which have connection with py_func node ?\nOr should I use py-func in different way?\n\nAny advice will be helpful for me.\nThanks in advance.\n", "comments": ["Do you happen to have a small repro case?\n", "It is not the pyfunc which makes NaN value.\nSoftmax_with_cross_entropy funcution was changed in version 0.10 and it return NaN if label gt is not in valid label value range. I did you \"-1\" label as ignore-label in version 0.9 and just use valid index value to calculate loss. But migrating it to upper version, it makes NaN loss.\n\nSorry to make mistake.\n", "Good to know. Glad you figured it out!\n"]}, {"number": 5249, "title": "AttributeError: 'module' object has no attribute 'save_v2'", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nNone\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n/usr/local/cuda/lib64/libcudadevrt.a    /usr/local/cuda/lib64/libcudart.so.8.0.44  /usr/local/cuda/lib64/libcudnn.so.5\n/usr/local/cuda/lib64/libcudart.so  /usr/local/cuda/lib64/libcudart_static.a   /usr/local/cuda/lib64/libcudnn.so.5.1.5\n/usr/local/cuda/lib64/libcudart.so.8.0  /usr/local/cuda/lib64/libcudnn.so      /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc1-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   `0.11.0rc1`\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n`tf.train.Saver(..., write_version=tf.train.SaverDef.V2)`\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n```\n File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1077, in __init__\n   self.build()\n File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1106, in build\n   restore_sequentially=self._restore_sequentially)\n File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 702, in build\n   save_tensor = self._AddSaveOps(filename_tensor, saveables)\n File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 309, in _AddSaveOps\n   save = self.save_op(filename_tensor, saveables)\n File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 252, in save_op\n   return io_ops.save_v2(filename_tensor, tensor_names, tensor_slices,\nAttributeError: 'module' object has no attribute 'save_v2'\n```\n", "comments": ["@concretevitamin This is straight from our pip package. I don't see how this could have passed the tests.\n", "@yifeif that is the rc1 package. I looked at the `training.py` and it's not like `HEAD`. Not sure if v2 saving is supposed to work there. Since the code is there, I assume it's supposed to work.\n", "`saver.py` does indeed have `save_v2`, but `ops/io_ops` doesn't have `Register.*V2`.\n", "@concretevitamin could you take a look?\n", "I'll take a look tomorrow.\n", "Sorry for the troubles -- rc2 will likely not have this issue (we either will completely release the V2 functionalities, or none of it will go in; TBD).\n\nAre you eager to use V2 right now to save peak memory?\n", "No, using v2 is not hugely important. I just tried it out because I heard v1 was going to be deprecated soon.\n", "OK, closing. rc2 will do it, and you can use v1 in rc1.\n", "I am getting this same issue using rc2 on Mac OSX el capitan.  Python 2.7, CPU only in Anaconda.  It works fine when I build from source, but using the pip package it does not work.  I downloaded the rc2 pip package today to check and installed using `pip install --ignore-installed --upgrade $TF_BINARY_URL` while my environment is activated.  $TF_BINARY_URL was set using `export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc2-py2-none-any.whl`\n\nIf it matters, I had a previous version of tensorflow installed in this environment (0.11, rc1) before installing the pip package.  Also, when running in my built-from-source environment, I ran my code by copying it into the source (tensorflow/example/) directory, and building it with Bazel similar to the \"image_retraining\" example.\n", "@jmoney4769 can you try installing that pip package using a fresh environ?\n", "@concretevitamin Tried a couple of things:\nFirst, same environment, I ran `pip install --ignore-installed $TF_BINARY_URL` (not no upgrade here).  Same results.\n\nSecond, (still the same environment) I ran `pip uninstall tensorflow` then `pip install --ignore-installed $TF_BINARY_URL`.  Same results.\n\nThird, I created a new environment (`conda create -n test python=2.7 -y`), then `source activate test` then `pip install --ignore-installed $TF_BINARY_URL`.  Same results as before.  \n\nThe pip package created when building from source works when installing to a clean environment.\n", "@jmoney4769, thanks for your feedback. Let me take a look at our pip packages and get back to you.\n", "@jmoney4769, could you give \"pip install\" with $TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc2-py2-none-any.whl another shot? \n", "That is working now.  Thank you for the quick responses.\n"]}, {"number": 5248, "title": "Branch 137452948", "body": "Merging internal changes.\n", "comments": ["@zheng-xq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @dsmilkov and @nsthorat to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please. \n"]}, {"number": 5247, "title": "tensorboard demos don't work because of wrong paths", "body": "Checked on commit 32d1dcc10e1fdf33dc6742337c6e0869f7b3c557.\n\nIssue:\nWhile going through tensorboard/DEVELOPMENT.MD it is not possible to have tensorboard's demos working as demo's html as well as components htmls import other html with wrong file names.\n\nFor example in:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/demo/index.html\n\nthere is an import of `../components/tf-tensorboard/tf-tensorboard.html` and in fact this file doesn't exist. It exist under slightly different file name with replaced \"-\" with \"_\" (../components/tf_tensorboard/tf-tensorboard.html)\n\nThe difference is in directory names.\nThe same occurs if I want to load particular component's demo.\n", "comments": ["Nice find! Do you have a PR you could send?\n\n@danmane to double-check\n", "https://github.com/tensorflow/tensorflow/pull/5262\n", "This issue can be closed. \nChanges have been merged.\n", "Thanks for reporting back!\n", "Hey again.\nThe merged PR is not the real fix as directory names should be changed because more html files depend on old names (names with \"-\" not with \"_\").\nBasically currently any single html file inside https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tensorboard/components and subdirectories has wrong paths.\n\n@drpngx ^\n", "Ouch, right. Feel free to send a general PR.\n"]}, {"number": 5246, "title": "Fix graph_io_test flakiness.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @tensorflower-gardener and @rohan100jain to be potential reviewers.\n"]}, {"number": 5245, "title": "mean_squared_error gives warning about sum_of_squared_error", "body": "I used tf.contrib.losses.mean_squared_error, but during training stage, tensorflow give me warning `WARNING:tensorflow:sum_of_squares (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-10-01.`\n\nBut I already used mean_squared_error, so this warning should not show. \n\nAlso the mean_squared_error [definition link](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.losses.html#mean_squared_error) has the same warning, which I guess should not be there?\n", "comments": ["What version are you using? In the current version it's been removed (as threatened).\n", "`tf.__version__` gives me '0.10.0'. Yeah, I guess that's the reason. I may need update my TF version. (would be good to see the document update, though)\n", "I don't know that we cherry-pick for errors in documentation, but if so, you're welcome to submit a PR.\n\n@yifeif @gunan \n", "We probably wont update 0.10 docs, as we are working on finalizing 0.11 now.\n", "Wait...when I upgrade TF to 0.11.0rc1, I still get the same warning when using mean_squared_error. Also, more surprisingly, `sum_of_squares` and `mean_squared_error` gives me same results, and it looks the result is the mean squared error. \n\nI hope it's a small issue, because `sum_of_squares` is deprecated anyway. \n", "If we have a doc update for 0.11, we are accepting them until EOD today.\n", "It's been removed at `HEAD`, so I think it's OK. It's not easy to fix the docs so I think it's fine to have that little quirk.\n\nFTR, we had\n\n``` python\n@deprecated(\"2016-10-01\", \"Use mean_squared_error.\")\ndef sum_of_squares(predictions, targets, weight=1.0, scope=None):\n ...\n\nmean_squared_error = sum_of_squares\n```\n"]}, {"number": 5244, "title": "Allow setting session level options and run_metadata in SKFlow Estimator.fit()", "body": "Feature request: Allow specifying `Session.run()` parameters `options` and `run_metadata` when calling `Estimator.fit()`.  \n\nUse case: The [Chrome Timeline profiling method](http://stackoverflow.com/questions/37751739/tensorflow-code-optimization-strategy/37774430#37774430) requires setting these options.\n\nPossible implementation:  add an additional fields to the `RunConfig` class that can be set when when the `Estimator` is initialized. Then pass these options to `graph_actions._monitored_train()`, which then added them to the `MonitoredSession.run()` function call.\n", "comments": ["@martinwicke does that look like something we'd want to have?\n", "We're actually working on that. It will likely be in the form of exactly what you have outlined: the ability to give a RunOptions proto as part of RunConfig.\n", "fantastic, I look forward to it.\n\nOn Thu, Oct 27, 2016 at 7:42 PM, Martin Wicke notifications@github.com\nwrote:\n\n> We're actually working on that. It will likely be in the form of exactly\n> what you have outlined: the ability to give a RunOptions proto as part of\n> RunConfig.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5244#issuecomment-256800099,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAH4MI2HDw6WrrmiQ7lJzwkCHGh3_ThNks5q4TbMgaJpZM4Kio2y\n> .\n", "hello,does Estimator has this feature yet? @drpngx ", "We got sidetracked, so no, I am sorry.", "Is this feature ready yet?\r\n\r\nThanks.", "I thought it would be a simple change by adding 2 more fields in RunConfig proto, is it?\r\n\r\nThanks."]}]