[{"number": 34176, "title": "tf.Identity messes determinism (initialization order changed?)", "body": "### System information\r\n- **OS Platform and Distribution** : Linux Ubuntu 16.04.6 \r\n- **TensorFlow installed from** : PIP (binary)\r\n- **TensorFlow version**: tensorflow-gpu==1.14.0\r\n- **Python version**: 2.7.12\r\n- **GPU model and memory**: No relevant : the issue occurs both on GPU and CPU\r\n- **Exact command to reproduce**: `python test_script.py` (test_script is provided in the source code section below)\r\n\r\n### Describe the problem\r\nI am expecting the function below to be deterministic no matter the value of the `use_identity_op` parameter, though it is not :  \r\n```python\r\ndef my_graph(use_identity_op) :\r\n   \"\"\"Dummy function for testing\r\n   \r\n   :param use_identity_op: whether to apply the identity operator to the \r\n   a variable or not\r\n   :type use_identity_op: bool\r\n   :return: a dummy vector\r\n   :rtype: list(float)\r\n   \"\"\"\r\n   tf.compat.v1.reset_default_graph()\r\n   tf.compat.v1.set_random_seed(32)\r\n   a = tf.compat.v1.get_variable('my_a', [5], initializer=None)\r\n   if use_identity_op : \r\n        a = tf.identity(a)\r\n   b = tf.compat.v1.get_variable('my_b', [5], initializer=None)\r\n   c = a + b\r\n   session = tf.compat.v1.Session()\r\n   session.run(tf.compat.v1.global_variables_initializer())\r\n   return session.run(c).tolist()\r\n```\r\n\r\n### Source code / logs\r\nHere is the complete test_case (i.e `test_script.py`) : \r\n```python\r\nimport tensorflow as tf\r\nimport unittest\r\n\r\ndef my_graph(use_identity_op) :\r\n   \"\"\"Dummy function for testing\r\n   \r\n   :param use_identity_op: whether to apply the identity operator to the \r\n   a variable or not\r\n   :type use_identity_op: bool\r\n   :return: a dummy vector\r\n   :rtype: list(float)\r\n   \"\"\"\r\n   tf.compat.v1.reset_default_graph()\r\n   tf.compat.v1.set_random_seed(32)\r\n   a = tf.compat.v1.get_variable('my_a', [5], initializer=None)\r\n   if use_identity_op : \r\n        a = tf.identity(a)\r\n   b = tf.compat.v1.get_variable('my_b', [5], initializer=None)\r\n   c = a + b\r\n   session = tf.compat.v1.Session()\r\n   session.run(tf.compat.v1.global_variables_initializer())\r\n   return session.run(c).tolist()\r\n\r\nclass TestTFIdentity(unittest.TestCase): \r\n   def test_my_graph_is_deterministic_CPU(self):\r\n      \r\n      with tf.device('/device:CPU:0'):\r\n         #The 3 calls below should be equivalent.\r\n\r\n         run1 = my_graph(use_identity_op=False)\r\n         run2 = my_graph(use_identity_op=False)\r\n         self.assertEqual(run1, run2)\r\n         \r\n         run3 = my_graph(use_identity_op=True)\r\n         #This fails though I feel it shouldn't \r\n         self.assertEqual(run1, run3)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\nAnd here is the result log : \r\n```bash\r\n======================================================================\r\nFAIL: test_my_graph_is_deterministic_GPU (__main__.TestTFIdentity)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_TF.py\", line 36, in test_my_graph_is_deterministic_CPU\r\n    self.assertEqual(run1, run3)\r\nAssertionError: Lists differ: [-0.948743462562561, -0.324153... != [-0.10568153858184814, 0.36630...\r\n\r\nFirst differing element 0:\r\n-0.948743462562561\r\n-0.10568153858184814\r\n\r\n- [-0.948743462562561,\r\n-  -0.324153870344162,\r\n-  -0.7790395617485046,\r\n-  -1.0308094024658203,\r\n-  1.3954466581344604]\r\n+ [-0.10568153858184814,\r\n+  0.36630767583847046,\r\n+  -1.0092220306396484,\r\n+  -0.11402678489685059,\r\n+  0.6194109916687012]\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.699s\r\n```\r\n", "comments": ["@mathisc ,\r\nIssue replicating for TF 1.14 and TF 1.15. kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/8e4f0c1b45cac5b5cd15a203cffa2515/untitled30.ipynb) of colab. Thanks!", "Hi @oanush : in order to get the correct error in the notebook you'll need to change this line `unittest.main()` to `unittest.main(argv=['first-arg-is-ignored'], exit=False)` : [Here](https://colab.research.google.com/gist/mathisc/c57514d4931d68e3f48e0cea30d6bbb6/untitled30.ipynb#scrollTo=E-dIlChiWyUr)\r\n", "this is a known issue with random seeds in the tf1 API. Either pass a\r\nseed argument to every random op or switch to using\r\ntf.random.experimental.Generator for deterministically seedable random\r\nnumbers.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34176\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34176\">No</a>\n"]}, {"number": 34175, "title": "[r2.1:Cherrypick] Fix flaky doctest.", "body": "The doctest was using dictionary output to exhibit structured\nelement usage. This caused a problem since dictionary key order\nis undefined. This changes the example to use a tuple instead.\n\nPiperOrigin-RevId: 279359482\nChange-Id: Ideb9970935079b156f88f48672dea2d375d497ae", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34175) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34175) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 34174, "title": "Performance regression with obj. det model from 1.14 to 1.15", "body": "2.7x slowdown on a nearly-off-the-shelf tensorflow object detection API model (differences and pipeline config below) when going from 1.14 to 1.15.  The regression is _not_ present in 2.0.\r\n\r\nThis is a fine-tuned SSD mobilenets FPN coco config, with the only changes being:\r\n\r\nFixed size input of 1024x1024\r\nSingle class detection\r\n model draws from layers 2-5 instead of 3-6 and so is a fair bit larger than one might otherwise expect.\r\n\r\nI've attached my pipeline.config to reproduce the bug.\r\n\r\nI'm just running natively in TF with my own inference pipeline, doing the obvious feed_dict to get data in:\r\n\r\n```\r\n            out = sess.run(\r\n                [\r\n                    sess.graph.get_tensor_by_name(node) for node in [\r\n                        'num_detections:0', 'detection_scores:0',\r\n                        'detection_boxes:0', 'detection_classes:0'\r\n                    ]\r\n                ],\r\n                feed_dict={'image_tensor:0': img_batch})\r\n```\r\n[pipeline.config.txt](https://github.com/tensorflow/tensorflow/files/3833314/pipeline.config.txt)\r\n\r\n**System information**\r\nRunning on Ubuntu 18.04, Xeon, Titan V GPU, 12GB\r\n\r\nComparing between TF 1.14-GPU and 1.15 with conda/pip.\r\n```\r\npython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)\r\n[GCC 7.3.0] on linux\r\n```\r\n\r\nCUDA 10.0\r\nCuDNN 7.6\r\n\r\n**Describe the current behavior**\r\n\r\n2.7x slower in 1.15. :-)\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee attached pipeline.config (would need to specify some training/testing tfrecord)\r\n\r\n\r\n**Other info / logs**\r\nThis issue is related to an issue on tensorflow/serving:  https://github.com/tensorflow/serving/issues/1469\r\n\r\n", "comments": ["I can confirm this. All I did was re-export the SavedModel from the official pretrained checkpoint with a fixed `[None, 640, 640, 3]` input size and performance with batch size 4 went from 5.67 FPS to 0.22 FPS on a GTX 1060. So 25x slower for me. My setup:\r\n\r\n* Tensorflow 1.14.0 (CPU version) for exporting the SavedModel\r\n* Pipeline config: https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config\r\n* `checkpoint` and `model.ckpt.*` used in the exporting operation: http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz\r\n\r\nFor inference I used the official libtensorflow-gpu from here: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.15.0.tar.gz\r\n\r\nInterestingly, the performance is fine when running the SavedModel from the official `ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz` tarball directly.", "@dave-andersen, can you try @ilaripih's workaround and let us know if that helps. Thanks!", "@gadagashwini I don't think I presented any workaround. Yes, the performance is fine when using the official saved_model.pb from download.tensorflow.org. But as soon as I export the model (which I have to do for any custom model) the performance with TF 1.15.0-gpu drops.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 34173, "title": "[r2.1:Cherrypick] Run Model.*_on_batch using tf.function where appropriate.", "body": "PiperOrigin-RevId: 279412139\nChange-Id: I3c55a5388b0cfd144d63f5e9f444cf3e96471ec4", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34173) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34173) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 34172, "title": "[r2.1:Cherrypick] Export Keras preprocessing layers in tf.keras.layers.experimental.", "body": "PiperOrigin-RevId: 279470036\nChange-Id: Ib2b363d21baa14ac49088dd700f769c933164340", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34172) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34172) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 34170, "title": "[r2.1:Cherrypick] Add a missing line for compute in toolchain and Re-add the supported CUDA compute capabilities into the toolchain", "body": null, "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34170) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34170) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 34169, "title": "unable to run hello_world on stm32f7 (tflite micro)", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- GCC/Compiler version (if compiling from source): arm-none-eabi-gcc.exe (GNU Tools for ARM Embedded Processors 6-2017-q2-update) 6.3.1 20170620 (release) [ARM/embedded-6-branch revision 249437]\r\n\r\n**Describe the problem**\r\nI successfully build the mbed.bin file and can transfer it to the STM devkit via ST-LINK V2. However, nothing shows up on D1 (rx pin of adafruit-ft232h-breakout) that is connected to pin PA2 (tx pin of devkit). \r\n\r\nIn the profiles/develop.json, I am using the \"-std=gnu++14\" cxx flag for GCC_ARM. I am unable to compile using \"-std=c++11\" nor \"-std=c++14\".\r\n\r\nI also don't have an lcd screen attached to the devkit.\r\n\r\nAny idea why I'm not seeing any output?\r\n\r\nThanks.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Sorry I realiszed I was running on the wrong devkit"]}, {"number": 34168, "title": "Added reference kernel fallback for the fully_connected kernel", "body": "I added the reference kernel fallback for the fully_connected kernel's EvalQuantizedInt8 routine.", "comments": ["gentle ping @petewarden for review ?"]}, {"number": 34167, "title": "Added reference kernel fallback for depthwise_conv", "body": "I added the reference kernel fallback for the depthwise_conv EvalQuantizedPerChannel routine", "comments": ["gentle ping @petewarden for review ?"]}, {"number": 34166, "title": "py_function documentation example gives error", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/py_function\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe following is the code example on the documentation page does not execute:\r\n\r\n```python\r\ndef log_huber(x, m):\r\n  if tf.abs(x) <= m:\r\n    return x**2\r\n  else:\r\n    return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))\r\n\r\nx = tf.compat.v1.placeholder(tf.float32)\r\nm = tf.compat.v1.placeholder(tf.float32)\r\n\r\ny = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)\r\ndy_dx = tf.gradients(y, x)[0]\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n  # The session executes `log_huber` eagerly. Given the feed values below,\r\n  # it will take the first branch, so `y` evaluates to 1.0 and\r\n  # `dy_dx` evaluates to 2.0.\r\n  y, dy_dx = sess.run([y, dy_dx], feed_dict={x: 1.0, m: 2.0})\r\n```\r\n\r\nRunning this code returns the following error:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-174-af2c7dfc03d9> in <module>()\r\n      5         return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))\r\n      6 \r\n----> 7 x = tf.compat.v1.placeholder(tf.float32)\r\n      8 m = tf.compat.v1.placeholder(tf.float32)\r\n      9 \r\n\r\n~/anaconda2/envs/berttf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py in placeholder(dtype, shape, name)\r\n   2625   \"\"\"\r\n   2626   if context.executing_eagerly():\r\n-> 2627     raise RuntimeError(\"tf.placeholder() is not compatible with \"\r\n   2628                        \"eager execution.\")\r\n   2629 \r\n\r\nRuntimeError: tf.placeholder() is not compatible with eager execution.\r\n```", "comments": ["@ibarshai ,\r\nTensorflow 2.0 by default uses Eager-Execution. Hence Placeholders are not getting executed.\r\nJust put this line to deactivate the eager execution :\r\n**`tf.compat.v1.disable_eager_execution()`**\r\nkindly find the [gist](https://colab.sandbox.google.com/gist/oanush/67b6cb3d582e97284e75b06c2b657927/34166.ipynb) of colab for your reference.Thanks", "@ibarshai ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34165, "title": "tf.keras.backend.sqrt(tf.constant(-1.0)) is 0 which is misleading and tf.sqrt(tf.constant(-1.0)) is 'nan' which is the way it should be.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: Run in CPU\r\n**Describe the current behavior**\r\ntf.keras.backend.sqrt(tf.constant(-1.0)) returns 0 as clip_by_value is being done in the source code (Which is highly misleading, as can be seen only in the source and not in the function document) whereas tf.sqrt(tf.constant(-1.0)) returns 'nan' which is the expected behavior of any sqrt function. This causes some bugs which are very difficult to track. \r\n\r\n**Describe the expected behavior**\r\nMake sqrt functions return only the expected behavior and remove the clip_by_value.\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\ntf.keras.backend.sqrt(tf.constant(-1.0)).numpy()\r\ntf.sqrt(tf.constant(-1.0)).numpy()\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["As you can see from the [source code](https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/keras/backend.py#L2104) tf.keras.backend.sqrt clips the input value to zero if its less than 0 and to infinity if its equal to infinity as shown by the function [here](https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/keras/backend.py#L2104). This is the reason why the output is zero\r\n\r\nBut in tf.sqrt the sqrt function is from gen_math_ops.py which has a different behaviour i.e., behaviour similar to numpy and thats the reason why its value is defaulted to NaN", "But developers usually expect the tensorflow to return nan when tf.sqrt( negative numbers) is done, as thats the norm unlike returning 0, which defies the name of the function. Or, I would request you to update the documentation just to avoid the confusion as tensorflow is moving towards keras. ", "Sure @saikumarGadde ", "@MarkDaoust Is this issue is solved? Thanks. ", "@saikumarGadde: Yes, you have a point but: we need to be very conservative about changing the behavior of existing functions. Also and what is the goal of using `tf.keras.backend` at all?\r\n\r\nAt this point `tf.keras.backend` should be considered an implementation detail. It was a tools for writing keras to be backend-agnostic. TensorFlow is effectively the only backend now.\r\n\r\nThat's why fchollet removed the documentation for these in: \r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/91e5ad0fad9bbf8462a797ddd7183df1c15f6832#diff-e329ed6b8d30dca9a441689005047035\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/backend?version=nightly \r\n\r\nThe functions will still be available in the package but they are undocumented to communicate that they should not be used."]}, {"number": 34164, "title": "Added reference kernel fallback for conv", "body": "I added the reference kernel fallback for the conv's EvalQuantizedPerChannel routine, as suggested by @freddan80", "comments": ["Gentle ping @petewarden for review ?"]}, {"number": 34163, "title": "Do not get performance improvement on tf.matmul when building with AVX2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow version (use command below): v1.14\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 8.3.1\r\n\r\n**Describe the current behavior**\r\nRun the benchmarking tests for `tf.matmul` operation under different TensorFlow packages: \r\n1. **the pip package v1.14 released by Tensorflow.** \r\n   This package is not compiled with AVX2 as indicated by this log info `tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA`\r\n2. **built the TensorFlow package with AVX2 from source** using the command `bazel build -c opt --copt=-march=native //tensorflow/tools/pip_package:build_pip_package`.\r\n\r\nHowever, there is no big performance difference between the above two packages:\r\n1. tensorflow-v1.14-cpu\r\n    8192 x 8192 matmul took: **1.54 sec, 713.30 G ops/sec**\r\n\r\n2. build from source with AVX2\r\n    8192 x 8192 matmul took: **1.60 sec, 687.73 G ops/sec**\r\n\r\n**Describe the expected behavior**\r\nThe benchmark with the second package should be faster than the first one.\r\n\r\n**Code to reproduce the issue**\r\nBorrow the benchmark code from [here](https://stackoverflow.com/a/41810634/3471050).\r\n```Python\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\nimport time\r\n\r\nn = 8192\r\ndtype = tf.float32\r\n\r\nmatrix1 = tf.Variable(tf.ones((n, n), dtype=dtype))\r\nmatrix2 = tf.Variable(tf.ones((n, n), dtype=dtype))\r\nproduct = tf.matmul(matrix1, matrix2)\r\n\r\n\r\n# avoid optimizing away redundant nodes\r\nconfig = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\nsess = tf.Session(config=config)\r\n\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\nimport time\r\n\r\nn = 8192\r\ndtype = tf.float32\r\n\r\nmatrix1 = tf.Variable(tf.ones((n, n), dtype=dtype))\r\nmatrix2 = tf.Variable(tf.ones((n, n), dtype=dtype))\r\nproduct = tf.matmul(matrix1, matrix2)\r\n\r\n\r\n# avoid optimizing away redundant nodes\r\nconfig = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\nsess = tf.Session(config=config)\r\n\r\n\r\nsess.run(tf.global_variables_initializer())\r\niters = 10\r\n\r\n# pre-warming\r\nsess.run(product.op)\r\n\r\nstart = time.time()\r\nfor i in range(iters):\r\n  sess.run(product.op)\r\nend = time.time()\r\nops = n**3 + (n-1)*n**2 # n^2*(n-1) additions, n^3 multiplications\r\nelapsed = (end - start)\r\nrate = iters*ops/elapsed/10**9\r\nprint('\\n %d x %d matmul took: %.2f sec, %.2f G ops/sec' % (n, n,\r\n                                                            elapsed/iters,\r\n                                                            rate,))\r\n```\r\n", "comments": ["cc: @angerson @yifeif ", "Sorry for the delay, had a quick chat with Penporn and she mentioned that MKL is already using JIT to run the most optimized code for your cpu architecture, which might explained why you are not seeing a performance boost. cc: @penpornk in case she has more to add. Thanks!", "Yes to what Yifei said. That message is meant for non-matmul/convolution ops in TensorFlow. \r\n\r\nTF has been using [oneDNN](https://github.com/oneapi-src/oneDNN) (new name of MKL-DNN) matrix multiplication routine ([sgemm](https://cs.opensource.google/tensorflow/tensorflow/+/eb2df6e1addf161595950ec7f280f95596854cb8:tensorflow/core/kernels/eigen_contraction_kernel.h;l=166)) as a building block for matmul and convolution operations since release 1.13. sgemm detects the CPU architecture at runtime and generates the most appropriate vector instructions the CPU is capable of, e.g., AVX-512. So, the generic TF wheel can already use up to AVX-512 instructions in matmul/convolution ops even when they are built with just AVX. That's why you didn't see the performance improvement.\r\n\r\nSorry for the confusing message. It was outdated. Fixed that in https://github.com/tensorflow/tensorflow/commit/95f92b6d33b7b405c5c327a68c216a5a5b5d9f93", "@yifeif @penpornk Thanks very much for your detailed answers! Good to know these information. Close this issue as it has been resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34163\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34163\">No</a>\n"]}, {"number": 34162, "title": "Updated DeviceSpec docstring and fixed typos", "body": "Updated docstring (as per #34124), adding instructions for using `device_spec.to_string()` method with eager execution enabled. Fixed missing parentheses in example.", "comments": ["@mihaimaruseac Please review\r\n", "Done. \r\n", "@nikochiko can you please fix the build failures ?", "```\r\nFAIL: Found 1 non-whitelisted pylint errors:\r\ntensorflow/python/framework/device_spec.py:88: [C0301(line-too-long), ] Line too long (88/80)\r\n```", "Done. Looks good?\r\n", "Any idea why the Android build is failing?"]}, {"number": 34161, "title": "Assert shapes", "body": "Reapply #34076 onto master.\r\n\r\ngit rebase -i --onto master r2.0 assert-shapes", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34161) for more info**.\n\n<!-- need_author_cla -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34161) for more info**.\n\n<!-- need_author_consent -->", "I rewrote the problematic commits to use the right email address. That should make CLA bot happy.\r\n\r\nhttps://stackoverflow.com/questions/2919878/git-rewrite-previous-commit-usernames-and-emails\r\n\r\n@googlebot: I'm fixing the PR submitted by @dubesar, they submitted this, they're okay with it.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34161) for more info**.\n\n<!-- cla_yes -->", "@googlebot I consent"]}, {"number": 34160, "title": "[r2.1-CherryPick]: Fixing compilation error in MKL eager test", "body": "", "comments": []}, {"number": 34159, "title": "Unresolved external symbol, C++, Windows 7, TensorFlow 2.0", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Windows 7 Enterprise\r\n- TensorFlow installed from: Source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6.8\r\n- Bazel version: 0.26.1\r\n- GCC/Compiler version: VS2017 15.7.5\r\n- GPU model and memory: Intel HD 530\r\n\r\n[edit]\r\nThe build was based on commit 1cf0898dd4331baf93fe77205550f2c2e6c90ee5 from the r2.0 branch.\r\n[/edit]\r\n\r\nI tried to build the shared C++ library on Windows 7 for TensorFlow 2 with the recommended version of bazel and I was quite happy that it seems to be a lot more straightforward now (previously I was only able to build 1.6 with cmake).\r\n\r\nHowever, the problem that now occurs when I try to use TensorFlow in a program. I have custom written code but for the time being I only tried to run a few simple examples.\r\n\r\nThis small program\r\n\r\n```\r\n#define NOMINMAX\r\n#define COMPILER_MSVC\r\n#include \"tensorflow/core/platform/logging.h\"\r\n\r\nint main(int argc, char * argv[])\r\n{\r\n  LOG(ERROR) << \"test\";\r\n}\r\n```\r\n\r\ncompiles and works without problems but when I try to run the [example trainer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/tutorials/example_trainer.cc) the program compiles but fails to link with the following errors:\r\n```\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: __cdecl tensorflow::Operation::Operation(class tensorflow::Node *)\" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@Z)\" in function \"\"public: __cdecl tensorflow::Input::Input<int,void>(int const &)\" (??$?0HX@Input@tensorflow@@QEAA@AEBH@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::Scope::GetUniqueNameForOp(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?GetUniqueNameForOp@Scope@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV34@@Z)\" in function \"\"class tensorflow::Output __cdecl tensorflow::ops::Const<float>(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (??$Const@M@ops@tensorflow@@YA?AVOutput@1@AEBVScope@1@AEBUInitializer@Input@1@@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: void __cdecl tensorflow::Scope::UpdateStatus(class tensorflow::Status)const \" (?UpdateStatus@Scope@tensorflow@@QEBAXVStatus@2@@Z)\" in function \"\"class tensorflow::Output __cdecl tensorflow::ops::Const<float>(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (??$Const@M@ops@tensorflow@@YA?AVOutput@1@AEBVScope@1@AEBUInitializer@Input@1@@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: void __cdecl tensorflow::Scope::UpdateBuilder(class tensorflow::NodeBuilder *)const \" (?UpdateBuilder@Scope@tensorflow@@QEBAXPEAVNodeBuilder@2@@Z)\" in function \"\"class tensorflow::Output __cdecl tensorflow::ops::Const<float>(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (??$Const@M@ops@tensorflow@@YA?AVOutput@1@AEBVScope@1@AEBUInitializer@Input@1@@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: bool __cdecl tensorflow::Scope::ok(void)const \" (?ok@Scope@tensorflow@@QEBA_NXZ)\" in function \"\"class tensorflow::Output __cdecl tensorflow::ops::Const<float>(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (??$Const@M@ops@tensorflow@@YA?AVOutput@1@AEBVScope@1@AEBUInitializer@Input@1@@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: class tensorflow::Graph * __cdecl tensorflow::Scope::graph(void)const \" (?graph@Scope@tensorflow@@QEBAPEAVGraph@2@XZ)\" in function \"\"class tensorflow::Output __cdecl tensorflow::ops::Const<float>(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (??$Const@M@ops@tensorflow@@YA?AVOutput@1@AEBVScope@1@AEBUInitializer@Input@1@@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: class tensorflow::Status __cdecl tensorflow::Scope::ToGraphDef(class tensorflow::GraphDef *)const \" (?ToGraphDef@Scope@tensorflow@@QEBA?AVStatus@2@PEAVGraphDef@2@@Z)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: class tensorflow::Status __cdecl tensorflow::Scope::DoShapeInference(class tensorflow::Node *)const \" (?DoShapeInference@Scope@tensorflow@@QEBA?AVStatus@2@PEAVNode@2@@Z)\" in function \"\"class tensorflow::Output __cdecl tensorflow::ops::Const<float>(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (??$Const@M@ops@tensorflow@@YA?AVOutput@1@AEBVScope@1@AEBUInitializer@Input@1@@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"private: class tensorflow::Scope __cdecl tensorflow::Scope::WithOpNameImpl(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?WithOpNameImpl@Scope@tensorflow@@AEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"class tensorflow::Output __cdecl tensorflow::ops::Const(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z)\" in function \"\"class tensorflow::Output __cdecl tensorflow::ops::Const<float>(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (??$Const@M@ops@tensorflow@@YA?AVOutput@1@AEBVScope@1@AEBUInitializer@Input@1@@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"struct tensorflow::NodeBuilder::NodeOut __cdecl tensorflow::ops::AsNodeOut(class tensorflow::Scope const &,class tensorflow::Input const &)\" (?AsNodeOut@ops@tensorflow@@YA?AUNodeOut@NodeBuilder@2@AEBVScope@2@AEBVInput@2@@Z)\" in function \"\"class tensorflow::Output __cdecl tensorflow::ops::Const<float>(class tensorflow::Scope const &,struct tensorflow::Input::Initializer const &)\" (??$Const@M@ops@tensorflow@@YA?AVOutput@1@AEBVScope@1@AEBUInitializer@Input@1@@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: __cdecl tensorflow::ops::Div::Div(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Div@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: __cdecl tensorflow::ops::MatMul::MatMul(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0MatMul@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: __cdecl tensorflow::ops::Sqrt::Sqrt(class tensorflow::Scope const &,class tensorflow::Input)\" (??0Sqrt@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: __cdecl tensorflow::ops::Square::Square(class tensorflow::Scope const &,class tensorflow::Input)\" (??0Square@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@@Z)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: __cdecl tensorflow::ops::Sum::Sum(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Sum@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z)\" in function \"\"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"public: __cdecl tensorflow::SessionOptions::SessionOptions(void)\" (??0SessionOptions@tensorflow@@QEAA@XZ)\" in function \"\"void __cdecl tensorflow::example::ConcurrentSteps(struct tensorflow::example::Options const *,int)\" (?ConcurrentSteps@example@tensorflow@@YAXPEBUOptions@12@H@Z)\".\r\n1>  main.obj : error LNK2019: unresolved external symbol \"\"class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)\" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z)\" in function \"\"void __cdecl tensorflow::example::ConcurrentSteps(struct tensorflow::example::Options const *,int)\" (?ConcurrentSteps@example@tensorflow@@YAXPEBUOptions@12@H@Z)\".\r\n1>  main.obj : error LNK2001: unresolved external symbol \"\"const tensorflow::GraphDef::`vftable'\" (??_7GraphDef@tensorflow@@6B@)\".\r\n```\r\n\r\nNow I read [here](https://github.com/tensorflow/tensorflow/issues/30647) in a post by [gunan](https://github.com/gunan) that this has to do with the fact that not all symbols are exported, however, I am not sure whether this is the case here since I find it strange that compiling a tensorflow .lib and .dll without any custom changes results in a library that is not able to compile/work with official example code.\r\nAm I missing something?\r\n\r\nCheers", "comments": ["I'm hitting the same issue when trying to compile and link the 'example_trainer' executable with the standard tensorflow C++ libs:\r\n\r\n`cl tensorflow/tensorflow/cc/tutorials/example_trainer.cc -DNOMINMAX /I tensorflow/bazel-genfiles/tensorflow/include /I eigen /I abseil-cpp /I protobuf/src /EHsc protobuf\\cmake\\build\\release\\libprotobuf.lib tensorflow\\bazel-genfiles\\tensorflow\\tensorflow_cc.lib`\r\n\r\nReturns an error like:\r\n\r\n`example_trainer.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z) referenced in function \"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\r\nexample_trainer.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ) referenced in function \"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\r\nexample_trainer.obj : error LNK2019: unresolved external symbol \"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ) referenced in function \"class tensorflow::GraphDef __cdecl tensorflow::example::CreateGraphDef(void)\" (?CreateGraphDef@example@tensorflow@@YA?AVGraphDef@2@XZ)\r\nexample_trainer.exe : fatal error LNK1120: 21 unresolved externals`\r\n\r\nCan someone from the dev team please give an example of how to compile and link the 'example_trainer' code with the default tensorflow c++ libs on Windows, without using Bazel?\r\n\r\nI'm aware of the \"add TF_EXPORT to each function\" workaround from [here](https://github.com/tensorflow/tensorflow/issues/30647) and [here](https://github.com/tensorflow/tensorflow/issues/24885), however I want to build TF libs once and don't know what functions I'll need ahead of time.\r\n\r\nI'm also aware of the 'def_file_filter.py.tpl' trick, but as @gunan has mentioned [here](https://github.com/tensorflow/tensorflow/issues/30647#issuecomment-529315280), it seems that not all symbols can be linked against.\r\n\r\nThanks in advance for any help or suggestions!\r\n", "I made the following changes to get my app to link against the Tensorflow dll\r\n\r\nI feel potentially the TF_EXPORT is not the best route, it looks like a few people are having these kinds of issues.. I'm quite happy to submit a PR but I feel the TF_EXPORT is a bit of a hack, \r\n\r\nhttps://github.com/conradjones/tensorflow/commit/ffdbaa25411340cf49b2eba8e180733340af61cd", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34159\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34159\">No</a>\n", "Same issue here,  I  think  tf-export macro can not be a final solution.", "I have the same issue.\r\nIf I didn't link with `tensorflow_cc.lib`, then there will 8 unresolved symbols. If linked with it, there are only 5 unresolved symbols. So I assume the problem is indeed not all symbols were exported.\r\nBut the building process takes 18 hours in my laptop. I would prefer a more releastic solution than using `TF_EXPORT` and recompile."]}, {"number": 34158, "title": "Loss and metrics differ with masking or sample weights", "body": "Update on Jan 8, 2021: I updated the title from \"Metrics incorrect for RNN with mask\" as I discovered more information that widens the scope of this issue. See comment on that date.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**\r\n- TensorFlow installed from (source or binary): **binary (conda)**\r\n- TensorFlow version (use command below): **2.0.0**\r\n- Python version: **3.6.8**\r\n- Bazel version (if compiling from source): **n/a**\r\n- GCC/Compiler version (if compiling from source): **n/a**\r\n- CUDA/cuDNN version: **10.0.130 / 7.6.0**\r\n- GPU model and memory: **1080 Ti**\r\n\r\n**Describe the current behavior**\r\nI am training an RNN (GRU) where my varying-length sequences are right-padded with 0s and a mask is applied. _Many_ sequences are more than half 0s (padding). I compile the model with a loss of `'mean_squared_error'` and a metric of `'mean_squared_error'`, but the output is different when the mask is in effect.\r\n```\r\nmodel.compile(optimizer=keras.optimizers.RMSprop(),\r\n              loss='mean_squared_error',\r\n              metrics=['mean_squared_error'])\r\n```\r\nOr equivalently:\r\n```\r\nmodel.compile(optimizer=keras.optimizers.RMSprop(),\r\n              loss=keras.losses.MeanSquaredError(),\r\n              metrics=[keras.metrics.MeanSquaredError()])\r\n```\r\n\r\nExample output (note the different values for `loss` vs. `mean_squared_error` for both training and validation):\r\n```\r\nEpoch 1/50\r\n210328/210328 [==============================] - 610s 3ms/sample - loss: 4.5338e-06 - mean_squared_error: 1.1923e-05 - val_loss: 2.5456e-06 - val_mean_squared_error: 6.7928e-06\r\nEpoch 2/50\r\n210328/210328 [==============================] - 525s 2ms/sample - loss: 2.1835e-06 - mean_squared_error: 5.7421e-06 - val_loss: 2.2920e-06 - val_mean_squared_error: 6.1160e-06\r\nEpoch 3/50\r\n210328/210328 [==============================] - 513s 2ms/sample - loss: 1.9939e-06 - mean_squared_error: 5.2437e-06 - val_loss: 2.2535e-06 - val_mean_squared_error: 6.0133e-06\r\n...\r\nEpoch 50/50\r\n210328/210328 [==============================] - 527s 3ms/sample - loss: 1.5595e-06 - mean_squared_error: 4.1011e-06 - val_loss: 1.7867e-06 - val_mean_squared_error: 4.7677e-06\r\n```\r\n\r\nWhen I disable the masking, I get the following output:\r\n```\r\nEpoch 1/3\r\n210328/210328 [==============================] - 516s 2ms/sample - loss: 7.1682e-06 - mean_squared_error: 7.1682e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06\r\nEpoch 2/3\r\n210328/210328 [==============================] - 434s 2ms/sample - loss: 5.9133e-06 - mean_squared_error: 5.9133e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06\r\nEpoch 3/3\r\n210328/210328 [==============================] - 442s 2ms/sample - loss: 5.9085e-06 - mean_squared_error: 5.9085e-06 - val_loss: 6.7073e-06 - val_mean_squared_error: 6.7073e-06\r\n```\r\nWithout the mask, the values for `loss` and `mean_squared_error` agree. For the validation set, the values are not really improving and the value of 6.7e-06 seems to be what you get when you evaluate on the 0s that would otherwise be ignored by the masking. Comparing the values between the runs suggests that the `mean_squared_error` calculations are not using the mask when it is in effect, but the `loss` calculations do use the mask. (We'd expect lower values when we correctly ignore irrelevant time steps.)\r\n\r\n**Describe the expected behavior**\r\nThe values for `loss` and `mean_squared_error` should agree and both use the masking.\r\n\r\n**Code to reproduce the issue**\r\nI don't have full code and data to share since my model and data are proprietary.\r\n\r\n**Other info / logs**\r\nI can't think of any relevant logs.\r\n", "comments": ["@seandaug, Please provide the complete code to reproduce the reported issue. Thanks!", "Here's a small example that illustrates the issue.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\n# %% Make some sine wave data\r\nnum_time_series = 100\r\nmax_length = 200\r\nnum_points = 1\r\ntraining_data = np.zeros((num_time_series, max_length, num_points))\r\nnp.random.seed(123)\r\ntf.random.set_seed(123)\r\nfor i in range(num_time_series):\r\n    # Make a sequence that doesn't fill the array so there's padding at the end\r\n    length = np.random.randint(0.25 * max_length, .75 * max_length + 1)\r\n    period = np.random.random() * 20 + 5\r\n    shift = np.random.random()\r\n    training_data[i, 0:length, 0] = np.sin(2 * np.pi / period * np.linspace(0, length - 1, length) + shift)\r\n\r\n\r\n# %% Define the model\r\ndef make_model(use_mask):\r\n    input_seq = keras.layers.Input(shape=(None, num_points))\r\n    masked_input_seq = keras.layers.Masking(mask_value=0.0)(input_seq) if use_mask else input_seq\r\n    gru = keras.layers.GRU(units=3, return_sequences=True)(masked_input_seq)\r\n    output = keras.layers.Dense(units=1)(gru)\r\n    model = keras.Model(input_seq, output)\r\n    model.compile(optimizer=keras.optimizers.RMSprop(),\r\n                  loss='mean_squared_error',\r\n                  metrics=['mean_squared_error'])\r\n    return model\r\n\r\n\r\n# %% Train model with mask. Note that reported 'loss' and 'mean_squared_error' differ\r\nmake_model(True).fit(training_data[:, :-1, :],\r\n                     training_data[:, 1:, :],\r\n                     batch_size=10,\r\n                     epochs=3,\r\n                     verbose=1)\r\n\r\n\r\n# %% Train model without mask. Note that reported 'loss' and 'mean_squared_error' match\r\nmake_model(False).fit(training_data[:, :-1, :],\r\n                      training_data[:, 1:, :],\r\n                      batch_size=10,\r\n                      epochs=3,\r\n                      verbose=1)\r\n```\r\n\r\nExample output when mask is used (note that `loss` and `mean_squared_error` differ):\r\n```\r\nEpoch 1/3\r\n100/100 [==============================] - 10s 98ms/sample - loss: 0.2741 - mean_squared_error: 0.5393\r\nEpoch 2/3\r\n100/100 [==============================] - 2s 20ms/sample - loss: 0.2690 - mean_squared_error: 0.5292\r\nEpoch 3/3\r\n100/100 [==============================] - 2s 20ms/sample - loss: 0.2650 - mean_squared_error: 0.5214\r\n```\r\n\r\nExample output when no mask is used (note that `loss` and `mean_squared_error` match):\r\n```\r\nEpoch 1/3\r\n100/100 [==============================] - 4s 37ms/sample - loss: 0.2475 - mean_squared_error: 0.2475\r\nEpoch 2/3\r\n100/100 [==============================] - 1s 13ms/sample - loss: 0.2305 - mean_squared_error: 0.2305\r\nEpoch 3/3\r\n100/100 [==============================] - 1s 13ms/sample - loss: 0.2174 - mean_squared_error: 0.2174\r\n```", "I could replicate the issue with Tf 2.0. \r\nPlease take a look at the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/740c0fd036e17fe505996957b79d5cbb/untitled255.ipynb). Thanks!", "Adding @pavithrasv who is the owner of metric and loss. I think the loss will take into account of the masks and exclude the masked value, but I don't think metric will do that, which is why you see the value difference here. I will let @pavithrasv  to confirm.", "@seandaug I think this was resolved in the `tf-nightly`. I am not able to reproduce the error with `tf-nightly`. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/6e844af327510d58c27d59f78593825d/untitled255.ipynb). Thanks!\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34158\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34158\">No</a>\n", "@jvishnuvardhan The gist you posted using `tf-nightly` does not appear to work anymore. The issue seems to have reappeared.", "It appears that the difference is in how the \"mean\" is computed over the values for a \"loss\" vs. a \"metric\". Consider this very simple example that just outputs the input after applying a mask:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ny_true = np.array([[[0.], [1.], [1.]]])\r\nx = np.array([[[1.], [0.], [1.]]])\r\nmodel = tf.keras.Sequential([tf.keras.layers.Masking(mask_value=0., input_shape=(3, 1))])\r\nmodel.compile(loss='mae', metrics=['mae'])\r\nmodel.fit(x, y_true)  # Would intuitively expect mask to give loss of 0.5, but get 0.3333\r\n```\r\nThis generates the following output:\r\n```\r\n1/1 [==============================] - 0s 432us/step - loss: 0.3333 - mae: 0.5000\r\n```\r\nHere we have a sequence of length 3, the second value of which is masked out. Therefore, the MAE should be 0.5 because the total absolute error is 1.0 and it is divided among 2 entries (after applying the mask). However, as a \"loss\", the MAE is reported as 0.3333 instead of 0.5.\r\n\r\nWe get similar intuitively unexpected results when using `sample_weight` as well (no masking involved). Consider this simple example that just outputs the input and allows sample weights to be specified:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ny_true = np.array([[0.], [1.]])\r\nx = np.array([[1.], [0.]])\r\nweights = np.array([1., 0.])\r\nmodel = tf.keras.Sequential([tf.keras.layers.InputLayer(input_shape=(1,))])\r\nmodel.compile(loss='mae', metrics=['mae'])\r\nmodel.fit(x, y_true, sample_weight=weights)  # Might expect to see 1.0, but get 0.5\r\n```\r\nThis generates the following output:\r\n```\r\n1/1 [==============================] - 0s 427us/step - loss: 0.5000 - mae: 1.0000\r\n```\r\nHere we have two samples, each with a loss of 1.0, but due to the provided sample weights, one of them should be ignored. Thus, we would expect to see an MAE of 1.0, which is correctly reported using the \"metric\". However, the \"loss\" does not match intuition because it is dividing by the batch size, not the total sample weight.\r\n\r\nI suspect these observations are because of the use of `reduction=losses_utils.ReductionV2.AUTO` in `tf.keras.losses.Loss` that relies on `losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE`. Then the denominator of the MAE calculation is the batch size (including sequence length) that does not consider the mask nor the sample weights.\r\n\r\nIt appears that for the metric calculation, MAE is computed using `Mean` in `tensorflow.python.keras.metrics` that specifies `reduction=metrics_utils.Reduction.WEIGHTED_MEAN`. However, there's no analogous `WEIGHTED_MEAN` reduction option for loss functions.\r\n\r\nSo to me, the big question here is: **Why do loss functions use `SUM_OVER_BATCH_SIZE` that ignores masking and sample weights?**", "I agree with @seandaug. Loss and metrics differ if I enable masking on an LSTM model. I am trying to find a way to solve this problem as well.", "Hi. I've encountered this problem as well. Is there any update on this issue?", "@Recepcan @Taoup See https://github.com/tensorflow/tensorflow/issues/34491#issuecomment-869086784 for a workaround."]}, {"number": 34157, "title": "Keras fails to feed ragged/sparse inputs with correct input placeholder", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, custom subclassed model\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-17764-gae26958 2.1.0-dev20191111\r\n- Python version: Google Colab py3\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: Google Colab\r\n- GPU model and memory: Google Colab\r\n\r\n**Describe the current behavior**\r\nError raised when trying to feed ragged/sparse tensors to model with tf.data.Dataset\r\n\r\n**Describe the expected behavior**\r\nThere should be no error\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1Ysbe9uxWLn1eFWNMax9SAHewHMrXwG-R\r\n\r\n**Other info / logs**\r\n```python\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-ed795b275880> in <module>()\r\n----> 1 sparse_model = MyModel(False)\r\n      2 sparse_input = my_dataset(False)\r\n      3 \r\n      4 sparse_model.compile(optimizer='Adam', loss='mse')\r\n      5 sparse_model.fit(sparse_input)\r\n\r\n13 frames\r\n<ipython-input-2-45855ec3bda4> in __init__(self, ragged_on, **kwargs)\r\n     13     outputs = tf.keras.layers.GlobalAveragePooling1D()(dense)\r\n     14 \r\n---> 15     super(MyModel, self).__init__(inputs=inputs, outputs=outputs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in __init__(self, *args, **kwargs)\r\n    144 \r\n    145   def __init__(self, *args, **kwargs):\r\n--> 146     super(Model, self).__init__(*args, **kwargs)\r\n    147     _keras_api_gauge.get_cell('model').set(True)\r\n    148     # initializing _distribution_strategy here since it is possible to call\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py in __init__(self, *args, **kwargs)\r\n    167         'inputs' in kwargs and 'outputs' in kwargs):\r\n    168       # Graph network\r\n--> 169       self._init_graph_network(*args, **kwargs)\r\n    170     else:\r\n    171       # Subclassed network\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py in _init_graph_network(self, inputs, outputs, name, **kwargs)\r\n    270 \r\n    271     if any(not hasattr(tensor, '_keras_history') for tensor in self.outputs):\r\n--> 272       base_layer_utils.create_keras_history(self._nested_outputs)\r\n    273 \r\n    274     self._base_init(name=name, **kwargs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in create_keras_history(tensors)\r\n    185     keras_tensors: The Tensors found that came from a Keras Layer.\r\n    186   \"\"\"\r\n--> 187   _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n    188   return created_layers\r\n    189 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)\r\n    245           else:\r\n    246             with ops.init_scope():\r\n--> 247               constants[i] = backend.function([], op_input)([])\r\n    248       processed_ops, created_layers = _create_keras_history_helper(\r\n    249           layer_inputs, processed_ops, created_layers)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in __call__(self, inputs)\r\n   3725         value = math_ops.cast(value, tensor.dtype)\r\n   3726       converted_inputs.append(value)\r\n-> 3727     outputs = self._graph_fn(*converted_inputs)\r\n   3728 \r\n   3729     # EagerTensor.numpy() will often make a copy to ensure memory safety.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1527       TypeError: For invalid positional/keyword argument combinations.\r\n   1528     \"\"\"\r\n-> 1529     return self._call_impl(args, kwargs)\r\n   1530 \r\n   1531   def _call_impl(self, args, kwargs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _call_impl(self, args, kwargs, cancellation_manager)\r\n   1567       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n   1568           list(kwargs.keys()), list(self._arg_keywords)))\r\n-> 1569     return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n   1570 \r\n   1571   def _filtered_call(self, args, kwargs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1668       # No tape is watching; skip to running the function.\r\n   1669       return self._build_call_outputs(self._inference_function.call(\r\n-> 1670           ctx, args, cancellation_manager=cancellation_manager))\r\n   1671     forward_backward = self._select_forward_and_backward_functions(\r\n   1672         args,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    521               inputs=args,\r\n    522               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 523               ctx=ctx)\r\n    524         else:\r\n    525           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError:  You must feed a value for placeholder tensor 'data/values' with dtype float and shape [?]\r\n\t [[node data/values (defined at <ipython-input-2-45855ec3bda4>:15) ]] [Op:__inference_keras_scratch_graph_22]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph\r\n```", "comments": ["Issue replicating for the given code. Thanks!", "Original issue is still here with tf-nightly, but this workaround works https://github.com/tensorflow/tensorflow/issues/27170#issuecomment-589546804", "Does it still fail? Looks like it's fixed in nightly -- even the colab suggests so\r\n", "@shkarupa-alex \r\n\r\nI have tried in colab with TF version 2.3 and i am not seeing any issue. Please,find the gist [here](https://colab.research.google.com/gist/ravikyram/e3bb77fa0ae38cb42291528cd091a85e/untitled303.ipynb).Please, verify once and close the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34157\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34157\">No</a>\n"]}, {"number": 34156, "title": "Distributed training with TensorFlow2.0 not working: \"Aborting RingReduce with Invalid argument: Shape mismatch in the collective instance 106.\"", "body": "We trying to build a multi-worker strategy based on tutorials: [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) and [distributed training with TF2](https://github.com/lambdal/TensorFlow2-tutorial/blob/master/05-distributed-training/worker.py). The code works with this configurations: \r\n- first node:\r\n`os.environ['TF_CONFIG'] = json.dumps({ 'cluster': {  'worker': [\"localhost:5006\",\"localhost:5007\"] },  'task': {'type': 'worker', 'index': 0}})`\r\n- second node:\r\n`os.environ['TF_CONFIG'] = json.dumps({ 'cluster': {  'worker': [\"localhost:5006\",\"localhost:5007\"] },  'task': {'type': 'worker', 'index': 1}})`.\r\n\r\nThen we tried to run the code from two devices with the following system information:\r\n- OS: Linux Mint 19.1 Cinnamon, Cinnamon Version: 4.0.10\r\n- python version: 3.6.8\r\n- tensorflow version: 2.1.0\r\n\r\nwith the following configurations:\r\n- on the first device:\r\n`os.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"10.0.0.109:6006\",\"10.0.0.148:6006\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})`\r\n- on the second device:\r\n`os.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"10.0.0.109:6006\",\"10.0.0.148:6006\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 1}\r\n})`.\r\n\r\nThe devices can navigate into each other through ssh without password. We report the used script and the [link](https://docs.aws.amazon.com/en_us/forecast/latest/dg/getting-started.html#gs-upload-data-to-s3) to the source data set.\r\nScript:\r\n```\r\ndef create_dataframe(data, time_field):\r\n    data['time'] = pd.to_datetime(data[time_field])\r\n    data = data.drop(columns=[time_field])\r\n    data.sort_values('time')\r\n    data = data.reset_index(drop=True)\r\n    data.set_index('time', inplace=True)\r\n    return data\r\n\r\ndef aggregate(df, freq , field, aggregation):\r\n    df_count = df.groupby(pd.Grouper(level='time', freq=freq))[field].agg(aggregation)\r\n    df_count.dropna(inplace=True)\r\n    df_count = df_count.to_frame().reset_index()\r\n    df = df.reset_index()\r\n    df_count.set_index('time', inplace=True)\r\n    return df_count\r\n\r\ndef split_sequence(sequence, n_steps):\r\n    X, y = list(), list()\r\n    for i in range(len(sequence)):\r\n        end_ix = i + n_steps\r\n        if end_ix > len(sequence)-1:\r\n            break\r\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\r\n        X.append(seq_x)\r\n        y.append(seq_y)\r\n    return array(X), array(y)\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\ndf=pd.read_csv(\"/path/to/electricityusagedata.csv\")\r\n\r\ndf = create_dataframe(df, 'dt')\r\ndfy = aggregate(df,freq = '1h', field = 'value', aggregation = 'mean')\r\nn_steps = 10\r\nn_features=1\r\nvalue=df['value']\r\n\r\nX, y = split_sequence(dfy.value, n_steps)\r\n\r\nXtrain = X.reshape(X.shape[0], X.shape[1], n_features)\r\n\r\nwith strategy.scope():\r\n\r\n\tinputs = tf.keras.layers.Input(shape=(n_steps, n_features))\r\n\tlayer = tf.keras.layers.Dense(5, activation='relu')(inputs)\r\n\tlayer = tf.keras.layers.Dense(1)(layer)\r\n\r\n\tlstm_stacked = tf.keras.Model(inputs=inputs, outputs=layer)\r\n\tlstm_stacked.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.MeanSquaredError())\r\n\tlstm_stacked.summary()\r\n\r\nlstm_stacked.fit(Xtrain,y,epochs=10)\r\n```\r\n\r\nThe complete output from the command line is\r\n```\r\n2019-11-11 11:44:33.962245: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n2019-11-11 11:44:33.962282: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2019-11-11 11:44:34.962335: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-11-11 11:44:34.962374: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-11-11 11:44:34.962394: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (keypartner): /proc/driver/nvidia/version does not exist\r\n2019-11-11 11:44:34.962620: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-11 11:44:34.991840: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294830000 Hz\r\n2019-11-11 11:44:34.992692: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57231a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-11 11:44:34.992737: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-11-11 11:44:35.000579: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> localhost:6006, 1 -> 10.0.0.148:6006}\r\n2019-11-11 11:44:35.001075: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:6006\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 10, 1)]           0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 10, 5)             10        \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10, 1)             6         \r\n=================================================================\r\nTotal params: 16\r\nTrainable params: 16\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\n2019-11-11 11:45:04.330943: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:428] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\r\nop: \"FlatMapDataset\"\r\ninput: \"PrefetchDataset/_8\"\r\nattr {\r\n  key: \"Targuments\"\r\n  value {\r\n    list {\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"f\"\r\n  value {\r\n    func {\r\n      name: \"__inference_Dataset_flat_map_slice_batch_indices_173\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_types\"\r\n  value {\r\n    list {\r\n      type: DT_INT64\r\n    }\r\n  }\r\n}\r\n. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\r\nTrain on 8750 samples\r\nEpoch 1/10\r\n2019-11-11 11:45:07.181787: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Invalid argument: Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\r\n\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.181701406\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\\nAdditional GRPC error information:\\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\\n\\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\",\"grpc_status\":3}\r\n2019-11-11 11:45:07.181854: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\r\n\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.181701406\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\\nAdditional GRPC error information:\\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\\n\\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\",\"grpc_status\":3}\r\n2019-11-11 11:45:07.181990: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.181911798\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-11-11 11:45:07.182013: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.181911798\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-11-11 11:45:07.182077: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Invalid argument: Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\r\n\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.181701406\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\\nAdditional GRPC error information:\\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\\n\\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\",\"grpc_status\":3}\r\n2019-11-11 11:45:07.182133: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\r\n\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.181701406\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\\nAdditional GRPC error information:\\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\\n\\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\",\"grpc_status\":3}\r\n\t [[scoped_allocator_1_1_CollectiveReduce]]\r\n2019-11-11 11:45:07.182123: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.181911798\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n  32/8750 [..............................] - ETA: 12:56Traceback (most recent call last):\r\n  File \"worker1.py\", line 91, in <module>\r\n    lstm_stacked.fit(Xtrain,y,epochs=10)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 792, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 340, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 127, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2341, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1589, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1670, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 523, in call\r\n    ctx=ctx)\r\n  File \"/home/keypartner/my_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\r\n\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573469107.181701406\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\\nAdditional GRPC error information:\\n{\"created\":\"@1573469107.160620334\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Shape mismatch in the collective instance 106. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [49] but another member in the group expected shape [64]. This is likely due to different input shapes at different members of the collective op.\",\"grpc_status\":3}\\n\\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\",\"grpc_status\":3}\r\n\t [[scoped_allocator_1_1_CollectiveReduce]] [Op:__inference_distributed_function_843]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n\r\n2019-11-11 11:45:07.262512: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n\r\n```\r\nWe are wondering why the multi devices example doesn't work meanwhile it works on multi threads at localhost.\r\n", "comments": ["@jessicadef, Please provide the complete standalone code replicate the reported issue. Thanks!", "@jessicadef, Please paste the code snippet to localize the reported issue. Thanks!", "@gadagashwini Thanks for your reply! I figured out where the issue was: one of the two machines was running the code in a virtualenv environment which did not allow the machines to communicate.", "@jessicadef,\r\nCan you please let us know if you are happy to close if no issue persists. Thanks!", "I met this problem with tf 2.1, but after I upgraded it to tf 2.2, the problem was solved."]}, {"number": 34155, "title": "BERT-joint TF1 baseline fails on TPU training", "body": "BERT-joint baseline script [https://github.com/google-research/language/tree/master/language/question_answering/bert_joint#training-our-model](url)\r\n fails on TPU training with:\r\n\r\nE1111 11:25:56.709489 140509220771584 error_handling.py:81] Closing session due to error From /job:worker/replica:0/task:0:\r\nEnd of sequence\r\n\t [[node input_pipeline_task0/while/IteratorGetNext (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n2019-11-11 11:26:03.693223: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"Unavailable: Socket closed\" and grpc_error_string = \"{\"created\":\"@1573471563.693073061\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\r\n2019-11-11 11:26:03.693329: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"Unavailable: Socket closed\" and grpc_error_string = \"{\"created\":\"@1573471563.693032484\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\r\nERROR:tensorflow:Error recorded from outfeed: Step was cancelled by an explicit call to `Session::Close()`.\r\nE1111 11:26:17.120502 140509233358592 error_handling.py:75] Error recorded from outfeed: Step was cancelled by an explicit call to `Session::Close()`.\r\nINFO:tensorflow:An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: Session c3d5a74e41ee70e5 is not found.\r\nI1111 11:26:17.120874 140510448183168 monitored_session.py:1269] An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: Session c3d5a74e41ee70e5 is not found.\r\nERROR:tensorflow:\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo custom code - this is google provided example\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nGoogle Colab default\r\n- TensorFlow version (use command below):\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\nv1.15.0-0-g590d6eef7e 1.15.0\r\n- Python version: 3.6.8 (default, Oct  7 2019, 12:59:55) \r\n[GCC 8.3.0]\r\nUsing TPU\r\n\r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py\r\n", "comments": ["@mosicr Sorry for missing this issue. Is this still an issue? Can you please run with `TF1.15.3` and see whether the issue persists with a newer version. I ran the code and notice some file missing error as shown below. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/5036b9b468071cf62f41e4d9f76d4960/untitled944.ipynb). Thanks\r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nValidationError                           Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py in _assert_validators(self, validators)\r\n    527       try:\r\n--> 528         validator.verify(self)\r\n    529       except _exceptions.ValidationError as e:\r\n\r\n10 frames\r\nValidationError: Flag --vocab_file must have a value other than None.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nIllegalFlagValueError                     Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py in _assert_validators(self, validators)\r\n    529       except _exceptions.ValidationError as e:\r\n    530         message = validator.print_flags_with_values(self)\r\n--> 531         raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))\r\n    532 \r\n    533   def __delattr__(self, flag_name):\r\n\r\nIllegalFlagValueError: flag --vocab_file=None: Flag --vocab_file must have a value other than None.\r\n```\r\n\r\nPlease verify and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34154, "title": "tf.nn.moments with keras causes Invalid tape state", "body": "### System information\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from : binary\r\n- TensorFlow version: v1.12.1-8709-gddde447 2.0.0-dev20190813\r\n- Python version: 3.6\r\n\r\nAlso reproducable with cuda 10.0 cudnn 7.something / GTX-1070 but I doubt it's a GPU issue...\r\n\r\n### Current Behaviour\r\nUsing output of `tf.nn.moments` causes `Invalid tape state` when training keras models in manual loops (i.e. not using `fit`).\r\n\r\n### Expected Behaviour\r\n`mean, var = tf.nn.moments(x, axes, keepdims=True)` (and use thereafter) should be equivalent to \r\n```\r\nmean = tf.reduce_mean(x, axis=axes, keepdims=True)\r\nxs = x - mean\r\nvar = tf.reduce_mean(xs**2, axis=axes, keepdims=True)\r\n```\r\n\r\n# Code\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef normalize_manual(x):\r\n    mean = tf.reduce_mean(x, axis=-1, keepdims=True)\r\n    x = x - mean\r\n    var = tf.reduce_mean(tf.square(x), axis=-1, keepdims=True)\r\n    return x / (tf.sqrt(var + 1e-6))\r\n\r\n\r\ndef normalize_with_moments(x):\r\n    mean, var = tf.nn.moments(x, [-1], keepdims=True)\r\n    x = x - mean\r\n    return x / tf.sqrt(var + 1e-6)\r\n\r\n\r\ndef run_model_custom(normalize=normalize_with_moments):\r\n    inp = tf.keras.layers.Input(shape=(8,))\r\n    x = inp\r\n    x = tf.keras.layers.Dense(8)(x)\r\n    x = normalize(x)\r\n    out = tf.squeeze(tf.keras.layers.Dense(1)(x), axis=-1)\r\n    model = tf.keras.Model(inputs=inp, outputs=out)\r\n\r\n    x = tf.random.normal(shape=((100, 8)))\r\n    y = tf.random.normal(shape=((100,)))\r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(5)\r\n\r\n    for x, y in dataset.take(1):\r\n        with tf.GradientTape() as tape:\r\n            out = model(x)\r\n            tape.gradient(out, model.trainable_weights)\r\n        break\r\n    print('passed run_model_custom with {}'.format(normalize.__name__))\r\n\r\n\r\ndef train_model_fit(normalize=normalize_with_moments):\r\n    inp = tf.keras.layers.Input(shape=(8,))\r\n    x = inp\r\n    x = tf.keras.layers.Dense(8)(x)\r\n    x = normalize(x)\r\n    out = tf.squeeze(tf.keras.layers.Dense(1)(x), axis=-1)\r\n    model = tf.keras.Model(inputs=inp, outputs=out)\r\n\r\n    x = tf.random.normal(shape=((100, 8)))\r\n    y = tf.random.normal(shape=((100,)))\r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(5)\r\n    model.compile(optimizer=tf.keras.optimizers.SGD(), loss='mse')\r\n    model.fit(dataset, epochs=1, steps_per_epoch=2)\r\n    print('passed train_model_fit with {}'.format(normalize.__name__))\r\n\r\n\r\ndef compute_graph_tf(normalize=normalize_with_moments):\r\n    x = tf.random.normal((10, 5))\r\n    with tf.GradientTape() as tape:\r\n        layer = tf.keras.layers.Dense(2)\r\n        y = layer(x)\r\n        y = tf.reduce_sum(normalize(y))\r\n        tape.gradient(y, layer.trainable_weights)\r\n    print('passed compute_graph_tf with {}'.format(normalize.__name__))\r\n\r\n\r\nrun_model_custom(normalize_with_moments)  # <----- fails\r\n\r\n###########\r\n# the below work\r\n###########\r\ntrain_model_fit(normalize_with_moments)\r\ncompute_graph_tf(normalize_with_moments)\r\nrun_model_custom(normalize_manual)\r\ntrain_model_fit(normalize_manual)\r\ncompute_graph_tf(normalize_manual)\r\n```\r\n\r\n# Logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"norm_grad.py\", line 67, in <module>\r\n    run_model_custom(normalize_with_moments)  # <----- fails\r\n  File \"norm_grad.py\", line 36, in run_model_custom\r\n    tape.gradient(out, model.trainable_weights)\r\n  File \".../python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 1008, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \".../python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 76, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\ntensorflow.python.framework.errors_impl.InternalError: Invalid tape state.\r\n```", "comments": ["Wrapping `normalize_with_moments` in  `tf.keras.layers.Lambda` makes this work, though it's still very surprising and difficult to identify the issue. I'm guessing that's due to `tf.nn.moments` being a single op with multiple outputs? How does `keras` handle this in `fit` differently?", "The problem here is that if you don't wrap `normalize_with_moments` in `tf.keras.layers.Lambda`  here could be no differentiable path in the graph connecting input to output and thus the resulting error. ", "@gowthamkpr one of the nicest things about `tf2` is that most ops don't require a `tf.keras.layers.Lambda` wrapper. If a path cannot be found connecting inputs to outputs an informative error should be raised in model construction, not during gradient computation. It also doesn't answer the question of what `Model.fit` is doing.", "@jackd Is this still an issue? When I ran your code with `tf-nightly`, I didn't see any error.  For the line that was throwing error for you earlier, it didn't show any error with `tf-nightly`. Please add a line `!pip install tf-nightly-gpu` at the top to install `tf-nightly`.\r\n\r\nFor the following line \r\n`run_model_custom(normalize_with_moments)  # <----- fails`\r\n\r\nIt outputs\r\n\r\n`passed run_model_custom with normalize_with_moments`\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "@jackd I am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!"]}, {"number": 34153, "title": "tf.data.experimental.choose_from_datasets freezes if one of the datasets is empty", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n`tf.data.experimental.choose_from_datasets` freezes if one of the datasets is empty. This can happen for instance, when splitting the dataset into classes, recombining them using this function and one of the classes is not present in the input data. \r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone using complicated input pipelines. \r\n\r\n**Any Other info.**\r\n", "comments": ["@thijskooi \r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "@thijskooi thank you for reporting the issue. Could you please provide working example that can be used to readily reproduce the issue to aide with debugging? Thank you.", "Hi @thijskooi, I'm not able to reproduce your error with TF 1.14.0 (and at HEAD). Can you provide a repro?", "Sorry for the delay in reply. Unfortunately, we can not create a minimal sample where this happens without sharing too much code, the issue may be further down our pipeline. I will close the issue for now, if this comes up in the future again with a clearer example I will re-open. "]}, {"number": 34152, "title": "Can't train a model using Java in Windows", "body": "I know training a model is currently not supported for Java.\r\n\r\nI was just wondering when this is likely to be available?", "comments": ["Examples found in the [tensorflow/models](https://github.com/tensorflow/models/tree/master/samples/languages/java#tensorflow-for-java-examples) GitHub repository can be good starting point.\r\nSee https://github.com/tensorflow/models/tree/master/samples/languages/java/training", "Thank you but this is a very simple example.\r\nFor example if I follow\r\nhttps://github.com/tensorflow/hub/blob/580847f6613fdf0b0d7911d8811c136df3e9404a/examples/image_retraining/retrain.py#L721\r\n\r\nas an example for retraining an image recognition model, I should take the bottleneck and create a new training layer on top, I can't seem to do this in Java, perhaps I'm wrong but from what I've read elsewhere there just isn't java support for this.\r\n\r\nIf there is a way I can retrain Inception V3 in Java and I'm missing it then I would be happily guided.", "@jedakiah13 You can try referring to this [deeplab example](https://stackoverflow.com/questions/61104096/tensorflow-inference-using-java-api-extremely-slow) and try converting the code to java on your own. Thanks!.\r\n\r\nI'm closing this issue as this can be better asked in stackoverflow as this is a support question if you have any more questions. Thanks!"]}, {"number": 34151, "title": "Cannot import 'GradientDescentOptimizer'", "body": "\r\n![image](https://user-images.githubusercontent.com/3198940/68577281-bb353800-04aa-11ea-8a50-0a48d2ad0974.png)\r\n\r\nI use train.GradientDescentOptimizer and tf.optimizers.GradientDescentOptimizer, which not work. So I read the doc but only search GradientDescentOptimizer  in tf.compat.v1.train.GradientDescentOptimizer", "comments": ["See the documentation for [tf.train](https://www.tensorflow.org/api_docs/python/tf/train) and [tf.optimizers](https://www.tensorflow.org/api_docs/python/tf/optimizers). According to these, we do not have a batch gradient descent optimizer in either. There is Stochastic Gradient Descent optimizer in `tf.optimizers.SGD`. For gradient descent optimizer, so the way to apply Gradient Descent is `tf.compat.v1.train.GradientDescentOptimizer`.", "It works for me.Thanks."]}, {"number": 34150, "title": "TensorFlow Lite Op Request", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.14.6\r\n- TensorFlow installed from (source or binary): 2.0\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```bash\r\nTensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EQUAL, FLOOR, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MAX_POOL_2D, MUL, RESHAPE, REVERSE_V2, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, LoopCond, RandomUniform, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArrayV3, TensorArrayWriteV3.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nThe model comes from this repository : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The model contains v1 control flow ops and tensorlist, which is not supported by our old converter.\r\n\r\nCould you try to export the model in TF 2.0, and then try with our new converter (install the latest tf-nightly) and then add this:\r\nconverter.experimental_new_converter = True.", "Thank's @haozha111 I'll try that !", "is the issue fixed?", "@sachaarbonel \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34150\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34150\">No</a>\n"]}, {"number": 34149, "title": "Adding processing to FinddBatchNormV2 and FinddBatchNormV3 in the opt\u2026", "body": "\u2026imize method", "comments": ["@jhseu Can you please take a look on this PR? Thanks!", "@SmokerX Can you please resolve conflicts? Thanks!", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 34148, "title": "issues in importing keras ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win 7 64 bit\r\n- TensorFlow installed from (source or binary): installed using anaconda prompt\r\n- TensorFlow version (or github SHA if from source): 1:13;1\r\n\r\nDESCRIPTION : I run it using CPU , I have installed using the this command : conda install -c conda-forge keras in conda prompt\r\n\r\nPython 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]\r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nIPython 7.6.1 -- An enhanced Interactive Python.\r\nCODE:\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Convolution2D\r\nfrom keras.layers import MaxPooling2D\r\nfrom keras.layers import Flatten\r\nfrom keras.layers import Dense\r\n\r\nLOG:\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Convolution2D\r\nfrom keras.layers import MaxPooling2D\r\nfrom keras.layers import Flatten\r\nfrom keras.layers import Dense\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-d43c180dcd07>\", line 1, in <module>\r\n    from keras.models import Sequential\r\n\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n\r\n", "comments": ["I think your issue is same as https://github.com/tensorflow/tensorflow/issues/27984\r\nPlease go through this issue and ping if not solved.", "sir I saw the issue but i dont know what to do next?\n\nOn Mon, Nov 11, 2019 at 2:37 PM Sarvesh Dubey <notifications@github.com>\nwrote:\n\n> I think your issue is same as #27984\n> <https://github.com/tensorflow/tensorflow/issues/27984>\n> Please go through this issue and ping if not solved.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34148?email_source=notifications&email_token=ANXR7E35DFCWZCUQB7AYH53QTEODNA5CNFSM4JLRNXW2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDWDTDY#issuecomment-552352143>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANXR7E4MSG5I3NU5NBVYRSTQTEODNANCNFSM4JLRNXWQ>\n> .\n>\n", "> I think your issue is same as #27984\r\n> Please go through this issue and ping if not solved.\r\n\r\nsir, could you explain me what to do next , i saw the issuse but i dont knw what to do , im a beginner", "https://github.com/fo40225/tensorflow-windows-wheel\r\n\r\nDownload TensorFlow binaries from the link above. Since you are using CPU", "> https://github.com/fo40225/tensorflow-windows-wheel\r\n> \r\n> Download TensorFlow binaries from the link above. Since you are using CPU\r\n\r\nsir i click that green clone or download button its downloading more than 500mb,i dont know whether its right or wrong since im using cpu , i might have downloaded the wrong file  , is there any specific library file or a link. after downloading should i need to directly install or i should paste it somewhere?? please mention  the link of cpu tensorflow binaries i can directly downlaod. i dont know wht it is .. im new to coding sir ", "@karthikeyan-DS-BOT read the readme you will find the solution. Specifically your concern is also mentioned there", "*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\r\n\r\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n\r\n* Try Google Colab to use TensorFlow.\r\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\r\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\r\n  * All you need is a good internet connection and you are all set.\r\n*  If unable to find prebuilt binaries without avx instructions sets then you may ; Try to build TF from sources by changing CPU optimization flags.", "Sir ,thanks for the information,  i have this problem i installed TF using\nanaconda prompt it's updated to the latest version 2.0.0 in anaconda prompt\n, but when i see the installed packages in conda environment base root\npackages i could see TF version as 1.13.0 but i couldn't update or delete\nit there it's loading then it's getting stuck. What's the problem?\n\nOn Mon, Nov 11, 2019, 23:52 Yasir Modak <notifications@github.com> wrote:\n\n> *TensorFlow release binaries version 1.6 and higher are prebuilt with AVX\n> instruction sets.*\n>\n> Therefore on any CPU that does not have these instruction sets, either CPU\n> or GPU version of TF will fail to load.\n> Apparently, your CPU model does not support AVX instruction sets. You can\n> still use TensorFlow with the alternatives given below:\n>\n>    - Try Google Colab to use TensorFlow.\n>       - The easiest way to use TF will be to switch to google colab\n>       <https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true>.You\n>       get pre-installed latest stable TF version. Also you can usepip\n>       install to install any other preferred TF version.\n>       - It has an added advantage since you can you easily switch to\n>       different hardware accelerators (cpu, gpu, tpu) as per the task.\n>       - All you need is a good internet connection and you are all set.\n>    - If unable to find prebuilt binaries without avx instructions sets\n>    then you may ; Try to build TF from sources by changing CPU optimization\n>    flags.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34148?email_source=notifications&email_token=ANXR7EYK7GSFLFSPKHJRNOLQTGPH7A5CNFSM4JLRNXW2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDXU7EQ#issuecomment-552554386>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANXR7EZ2MR6MSKV3END7XXLQTGPH7ANCNFSM4JLRNXWQ>\n> .\n>\n", "@karthikeyan-DS-BOT,\r\nPlease follow the steps mentioned below to install Tenosrflow using conda\r\nSet Up Anaconda Environments\r\n`conda create --name tf_2.0.0 `\r\nActivate the new Environment\r\n`source activate tf_2.0.0`\r\nInstall Tf 2.0\r\n`conda install tensorflow=2.0 python=3.7`\r\n\r\nSteps to install Tensorflow using PIP:\r\n`$pip install virtualenv`\r\n`$virtualenv tf_2.0.0 `  # tf_2.0.0 is virtual env name\r\n`$source tf_2.0.0/bin/activate`\r\n`tf_2.0.0 $ pip install tensorflow==2.0.0`\r\n\r\nLet us know how it progresses. Thanks!", "> @karthikeyan-DS-BOT,\r\n> Please follow the steps mentioned below to install Tenosrflow using conda\r\n> Set Up Anaconda Environments\r\n> `conda create --name tf_2.0.0 `\r\n> Activate the new Environment\r\n> `source activate tf_2.0.0`\r\n> Install Tf 2.0\r\n> `conda install tensorflow=2.0 python=3.7`\r\n> \r\n> Steps to install Tensorflow using PIP:\r\n> `$pip install virtualenv`\r\n> `$virtualenv tf_2.0.0 ` # tf_2.0.0 is virtual env name\r\n> `$source tf_2.0.0/bin/activate`\r\n> `tf_2.0.0 $ pip install tensorflow==2.0.0`\r\n> \r\n> Let us know how it progresses. Thanks!\r\n\r\nThnks!!! ill try and let you know if it works ", "> > @karthikeyan-DS-BOT,\r\n> > Please follow the steps mentioned below to install Tenosrflow using conda\r\n> > Set Up Anaconda Environments\r\n> > `conda create --name tf_2.0.0 `\r\n> > Activate the new Environment\r\n> > `source activate tf_2.0.0`\r\n> > Install Tf 2.0\r\n> > `conda install tensorflow=2.0 python=3.7`\r\n> > Steps to install Tensorflow using PIP:\r\n> > `$pip install virtualenv`\r\n> > `$virtualenv tf_2.0.0 ` # tf_2.0.0 is virtual env name\r\n> > `$source tf_2.0.0/bin/activate`\r\n> > `tf_2.0.0 $ pip install tensorflow==2.0.0`\r\n> > Let us know how it progresses. Thanks!\r\n> \r\n> Thnks!!! ill try and let you know if it works\r\n\r\nthanks buddy , it really worked.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34148\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34148\">No</a>\n", "@karthikeyan-DS-BOT, Glad to know that it worked. "]}, {"number": 34147, "title": "How to get training time per epoch using TPUEstimator?", "body": "I know with Keras you can set verbose levels (0,1,2) to see training time per each epoch; however, I can't seem to figure out how to get a \"verbose\" feature working when trying to use the TPUEstimator. I'm using v1.14\r\n", "comments": ["```model.fit``` and ```model.evaluate```methods offer verbose functionality. You may use them with a variety of accelerators cpu, gpu and tpu.\r\nYou may set the log level in the program by;\r\n```python\r\nimport tensorflow as tf\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n```", "Hi Yasir, thanks for answering! \r\n\r\nDoesn't model.fit and model.evaluate require a model that takes numpy arrays as a dataset only?\r\n\r\n", "That's correct. It will come in handy if using keras sequential api. For estimators you may try setting log level info for training status."]}, {"number": 34146, "title": "Have install future module, but still can not find builtins when Building from source", "body": "<em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 16.04, kerenl: Linux dell 4.15.0-54-generic):\r\n- Mobile device (not)\r\n- TensorFlow installed from: source\r\n- TensorFlow version: master branch, git clone https://github.com/tensorflow/tensorflow.git\r\n- Python version: 2.7.12\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version: 0.29.1\r\n- GCC/Compiler version:  5.4.0\r\n- CUDA/cuDNN version:  CUDA 9.0, cuDNN 7.3.1\r\n- GPU model and memory: GeForce GT 730\r\n\r\n**Configure info**\r\n\r\n> \r\n\tjxl@dell:~/third_softwares/tensorflow$ ./configure\r\n\tWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n\tYou have bazel 0.29.1 installed.\r\n\tPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\n\tFound possible Python library paths:\r\n\t  /opt/ros/kinetic/lib/python2.7/dist-packages\r\n\t  /usr/lib/python2.7/dist-packages\r\n\t  /usr/local/lib/python2.7/dist-packages\r\n\t  /home/jxl/kalibr_ws/devel/lib/python2.7/dist-packages\r\n\t  /home/jxl/third_softwares/Autoware/ros/install/rslidar_pointcloud/lib/python2.7/dist-packages\r\n\tPlease input the desired Python library path to use.  Default is [/opt/ros/kinetic/lib/python2.7/dist-packages]\r\n\r\n\tDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\n\tNo XLA JIT support will be enabled for TensorFlow.\r\n\r\n\tDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\n\tNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\n\tDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\n\tNo ROCm support will be enabled for TensorFlow.\r\n\r\n\tDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\n\tCUDA support will be enabled for TensorFlow.\r\n\r\n\tDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\n\tNo TensorRT support will be enabled for TensorFlow.\r\n\r\n\tFound CUDA 9.0 in:\r\n\t    /usr/local/cuda/lib64\r\n\t    /usr/local/cuda/include\r\n\tFound cuDNN 7 in:\r\n\t    /usr/lib/x86_64-linux-gnu\r\n\t    /usr/include\r\n\r\n\tPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n\tYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\n\tPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5]: \r\n\r\n\tDo you want to use clang as CUDA compiler? [y/N]: n\r\n\tnvcc will be used as CUDA compiler.\r\n\r\n\tPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\tPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\tWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\n\tNot configuring the WORKSPACE for Android builds.\r\n\r\n\tPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t\t--config=mkl         \t# Build with MKL support.\r\n\t\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t\t--config=numa        \t# Build with NUMA support.\r\n\t\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\n\tPreconfigured Bazel build configs to DISABLE default on features:\r\n\t\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t\t--config=nogcp       \t# Disable GCP support.\r\n\t\t--config=nohdfs      \t# Disable HDFS support.\r\n\t\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\n\tConfiguration finished\r\n\tjxl@dell:~/third_softwares/tensorflow$ \r\n\r\n\r\n**Describe the problem**\r\n> \r\n\tjxl@dell:~/third_softwares/tensorflow$ sudo pip  install future\r\n\t[sudo] jxl \u7684\u5bc6\u7801\uff1a \r\n\t/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\r\n\t  warnings.warn(warning, RequestsDependencyWarning)\r\n\tDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\r\n\tRequirement already satisfied: future in /home/jxl/.local/lib/python2.7/site-packages (0.18.2)\r\n\tjxl@dell:~/third_softwares/tensorflow$ \r\n\r\n\tjxl@dell:~/third_softwares/tensorflow$ bazel build --config=opt --config=cuda  --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"   //tensorflow/tools/pip_package:build_pip_package\r\n\tStarting local Bazel server and connecting to it...\r\n\tWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\n\tINFO: Options provided by the client:\r\n\t  Inherited 'common' options: --isatty=1 --terminal_columns=231\r\n\tINFO: Reading rc options for 'build' from /home/jxl/third_softwares/tensorflow/.bazelrc:\r\n\t  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --incompatible_remove_legacy_whole_archive --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\n\tINFO: Reading rc options for 'build' from /home/jxl/third_softwares/tensorflow/.tf_configure.bazelrc:\r\n\t  'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/opt/ros/kinetic/lib/python2.7/dist-packages --python_path=/usr/bin/python --action_env PYTHONPATH=/home/jxl/kalibr_ws/devel/lib/python2.7/dist-packages:/opt/ros/kinetic/lib/python2.7/dist-packages:/home/jxl/third_softwares/Autoware/ros/install/rslidar_pointcloud/lib/python2.7/dist-packages --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5 --action_env LD_LIBRARY_PATH=/home/jxl/kalibr_ws/devel/lib:/opt/ros/kinetic/lib:/opt/ros/kinetic/lib/x86_64-linux-gnu:/home/jxl/third_softwares/ssdcaffe/distribute/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc-5 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\n\tINFO: Found applicable config definition build:v2 in file /home/jxl/third_softwares/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n\tINFO: Found applicable config definition build:cuda in file /home/jxl/third_softwares/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\n\tINFO: Found applicable config definition build:using_cuda in file /home/jxl/third_softwares/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\n\tINFO: Found applicable config definition build:opt in file /home/jxl/third_softwares/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\n\tINFO: Found applicable config definition build:cuda in file /home/jxl/third_softwares/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\n\tINFO: Found applicable config definition build:using_cuda in file /home/jxl/third_softwares/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\n\tDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\n\tDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/jxl/.cache/bazel/_bazel_jxl/630c8d4015ad0c22430bd92a914b2e86/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n\t - /home/jxl/.cache/bazel/_bazel_jxl/630c8d4015ad0c22430bd92a914b2e86/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n\t - /home/jxl/third_softwares/tensorflow/WORKSPACE:37:1\r\n\tDEBUG: /home/jxl/.cache/bazel/_bazel_jxl/630c8d4015ad0c22430bd92a914b2e86/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5: \r\n\tAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n\tTraceback (most recent call last):\r\n\t  File \"/home/jxl/.cache/bazel/_bazel_jxl/630c8d4015ad0c22430bd92a914b2e86/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n\t    from builtins import bytes  # pylint: disable=redefined-builtin\r\n\tImportError: No module named builtins\r\n\tINFO: Call stack for the definition of repository 'local_config_git' which is a git_configure (rule definition at /home/jxl/third_softwares/tensorflow/third_party/git/git_configure.bzl:66:17):\r\n\t - /home/jxl/third_softwares/tensorflow/tensorflow/workspace.bzl:74:5\r\n\t - /home/jxl/third_softwares/tensorflow/WORKSPACE:19:1\r\n\tERROR: An error occurred during the fetch of repository 'local_config_git':\r\n\t   Traceback (most recent call last):\r\n\t\tFile \"/home/jxl/third_softwares/tensorflow/third_party/git/git_configure.bzl\", line 64\r\n\t\t\t_fail(result.stderr)\r\n\t\tFile \"/home/jxl/third_softwares/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\t\tfail((\"%sGit Configuration Error:%s %...)))\r\n\tGit Configuration Error: Traceback (most recent call last):\r\n\t  File \"/home/jxl/.cache/bazel/_bazel_jxl/630c8d4015ad0c22430bd92a914b2e86/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n\t    from builtins import bytes  # pylint: disable=redefined-builtin\r\n\tImportError: No module named builtins\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Uninstall pip installed with get-pip.py, install pip with apt.\r\nAnd then remove ~/.local/lib/python2.7/site-packages/  dir.\r\nThen pip install future module."]}]