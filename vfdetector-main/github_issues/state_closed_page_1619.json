[{"number": 4341, "title": "DO NOT MERGE: Change GPU test to use cuda8. Update docker ubuntu to 16.04.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the annotation information on this pull request, we identified @caisq, @vrv and @jendap to be potential reviewers\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins Test this please.\n", "Just wanted to understand the status of this PR... are you waiting for someone to review this? \n", "Thanks for the ping Rohan. We were waiting for cuda 8.0 to come out of RC, but it looks like a bunch of  things changed in the nvidia docker image, and some of changes in this PR no longer applies. I will close this PR.\n"]}, {"number": 4340, "title": "[DO NOT MERGE]R0.10", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Closing this in favor of #4344 \n"]}, {"number": 4339, "title": "Add \"Gamma Correction\" Image Adjustment", "body": "This appears to be different from but similar to the [existing brightness adjustment](https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#adjust_brightness).\n\nSee: http://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.adjust_gamma\n", "comments": ["WIP\n"]}, {"number": 4338, "title": "Add Substr Op", "body": "Adds the `tf.substr` Op, as proposed in #4232. Once merged, work can continue on the `tf.decode_image` Op (#4009).\n", "comments": ["Can one of the admins verify this patch?\n", "OOO, please reassign. \n", "@tensorflow-jenkins Test this please.\n", "Still working on this; I've been exploring the TensorFlow core to see what I could pick up. I've added in the element-wise version of this Op and have been working at making broadcasting work.\n\nI'm trying to use the functionality in `core/util/bcast.h`, but it's not quite there yet. I've been able to get the appropriate Tensor shape calculations, but I don't know how to convert the `TensorBroadcastingOp` back into a `Eigen::TensorMap` (or similar) so that I can actually access the values associated with the broadcasted Tensor. I've commented out my latest attempt on [line 99 of substr_op.cc](https://github.com/tensorflow/tensorflow/pull/4338/files#diff-8ce346dacb5764a51c596536a50de93eR99)\\- is this close to how I should attempt this, or am I barking up the wrong tree?\n\nI'll need to update the shape function to match the broadcast result, as well.\n", "I admit that my brain shuts off when I see Eigen compilation failures, bringing in @benoitsteiner to help. :P\n", "Glad to hear it's not just me :P Looking forward to seeing how to get this implemented.\n", "Pinging @benoitsteiner if you have time. Any recommendations for accessing data from a `Eigen::TensorBroadcastingOp`? I don't have a huge amount of Eigen experience, so I'm not sure where I'm going wrong in my code above (or if I'm barking up the wrong tree entirely)\n", "I think this is ready for a re-review. Here's the highlights of the changes:\n- Moved check to see if pos/len are the same shape inside of shape function\n- Shape function uses `BroadcastBinaryOpShapeFn`\n- `substr_op.cc` checks for scalar pos/len or pos/len with the same shape as input tensor at the beginning of the Op. If it is neither, sets an Unimplemented error status\n- Cleaned up `substr_op.cc` to remove most of duplicated code\n- Added tests to make sure pos/len shape mismatches throw appropriate errors\n\nI have a few questions about some judgement calls:\n- Since pos/len shapes are checked by the shape function, I removed checking again inside of the kernel implementation. Is that fine, or should we double check?\n- I left a second Unimplemented error in the final `else` block toward the bottom of the kernel implementation. It's identical to the one at the top of the code, and shouldn't be reachable. I left it in to make it extra-extra explicit that this is where broadcasting implementations would go. I can take this out and just leave the comments explaining the situation if that's preferable.\n", "Thanks @benoitsteiner for your notes- from them I was able to get broadcasting to work. I've pushed the latest changes along with updated tests which pass on my machine. I attempted to do some clean-up with a macro (as different dimensionality broadcasts have to be defined separately and similarly), but was hitting some cross-initialization hiccups. Here are the main notes for @vrv:\n- Added 1 and 2 dimension broadcasts in `substr.cc`. They still end up iterating through the tensors sequentially, however\n- Added/updated `_testBroadcast` and `_testBadBroadcast` in `substr_op_test.py`\n- I left in the base cases of pos/len with either scalar or matching shape to the input strings. This way, we avoid allocating the additional memory used in broadcasting\n\nAssuming things look good, the last thing I need to do is update the documentation to indicate that 1/2 dimensional broadcasting is supported.\n", "@tensorflow-jenkins test this please\n", "I forgot to add the `from __future__ import print_function` statement to the kernel tests. Fixed.\n", "@tensorflow-jenkins test this please\n", "Absolutely- thanks for reviewing! I'll have these changes made later today.\n", "Alright! I've added the calls to `SubtleMustCopy` and `FastBoundsCheck`. Additionally, I modified the loops to avoid multiple accessor calls to the string tensor (not sure if it will make much of a difference) and added examples to the docstring in string_ops.cc. If this works, then I believe it just needs the LGTM from @benoitsteiner \n\nLet me know when I should squash.\n", "(No need to squash, we'll squash when we merge).  Just one suggestion above and I'll kick off tests and merge.\n", "Fixed!\n", "Great!  @tensorflow-jenkins test this please\n", "Not sure what's up with the `tensorflow/contrib/learn/python/learn/monitors_test.py` failure. I'm not able to replicate the test failure on my machine with the PR branch (I'm not sure how this code would affect that test, regardless).\n", "@tensorflow-jenkins test this please\n", "Rebased to master to see if that helps.\n", "@tensorflow-jenkins test this please\n", "Awesome- thanks for your help and guidance @vrv and @benoitsteiner! \n"]}, {"number": 4337, "title": "Multi-CPU kernel for sparse_tensor_dense_matmul", "body": "Are there plans to implement a multi-cpu kernel for tf.sparse_tensor_dense_matmul? The current version seems to be single core only and is not performing well. \n\nI am working with a very large sparse tensor - about 8GB sparse and 200GB dense that I cannot work with as a dense tensor for memory reasons. Any suggestions for achieving fast matrix multiplication? \n", "comments": ["@concretevitamin Could you take a look at this? Thanks.\n", "@ebrevdo is a more suitable person to look at this, I believe.\n", "Is this still current?", "Ping!\r\nIs this still an issue?\r\n@concretevitamin @ebrevdo could you comment and/or close this issue?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@rueberger is this still an issue for you?", "I am no longer working on the particular problem which prompted me to open the issue. The workaround I used at the time as to compute the matrix multiplication outside of Tensorflow using an MKL function.\r\n\r\nHowever, in my opinion, a multi-cpu kernel for sparse matmul is a highly desirable feature.\r\n\r\nThe need for such a feature frequently arises in advanced imaging applications (essentially large inverse problems). For instance, one can sharpen images produced by a microscope if they know the shape of the scope's point spread function, but this requires large matmuls. Classical techniques for doing this don't require the full power of Tensorflow, but there is a lot of interesting things one might do with machine learning to solve such problems, that would not be feasible without an efficient sparse matmul kernel.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Agreed it would be nice to optimize these methods.  I'm not sure the COO representation is great for this, but we would review performance improvements to this op.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Hi @rueberger! we are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in newer versions.   Feel free to  look up on [this thread ](https://stackoverflow.com/questions/38836269/does-tensorflow-view-all-cpus-of-one-machine-as-one-device) too.Thanks!", "Same answer as last time. No longer working on this and have not tried latest", "Ok! @rueberger! Please create a new issue if you require further assistance  .Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 4336, "title": "different initial states for different data", "body": "i'm working with bidirectional rnn and it would be nice that it had an option to have two different initial states for different data to be trained.\nthnx\n", "comments": ["Do you mean two different initial states, one for each direction?\n", "This already exists; see the arguments `initial_state_fw` and `initial_state_bw` for both `bidirectional_rnn` and `bidirectional_dynamic_rnn`.\n", "no i dont mean two initial states for different directions i mean two initial states for each direction so in total 4 initial state\nthnx\n", "You can pass tuples for the initial states for, e.g., lstm cells.\n\nOn Sep 21, 2016 11:30 PM, \"soshiant1992\" notifications@github.com wrote:\n\n> no i dont mean two initial states for different directions i mean two\n> initial states for each direction so in total 4 initial state\n> thnx\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4336#issuecomment-248823180,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim9AiFh_tNw_OsgBcyqfbhl4FjQ9bks5qsiCQgaJpZM4J6uXc\n> .\n"]}, {"number": 4335, "title": "configure issues", "body": "```\nERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/wenjian/pkgs/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/wenjian/pkgs/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n\n```\n", "comments": ["I am having the same issue (fresh pull of master); it seems to reference a different package each time I try to run `./configure` (presumably because of bazel is running more than one job at a time.), e.g.\n\n> ERROR: /opt/local/src/tensorflow/tensorflow/core/BUILD:689:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core:ios_tensorflow_test_lib'.\n> ERROR: /opt/local/src/tensorflow/tensorflow/core/BUILD:689:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core:ios_tensorflow_test_lib'.\n> ERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n\nand\n\n> ERROR: /opt/local/src/tensorflow/tensorflow/contrib/session_bundle/BUILD:237:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by /opt/local/src/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature_lite'.\n> ERROR: /opt/local/src/tensorflow/tensorflow/contrib/session_bundle/BUILD:237:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /opt/local/src/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature_lite'.\n> ERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n\nEDIT: looks like this is a possible duplicate of #4312 \n", "Yes, it's duplicate of #4312.\n"]}, {"number": 4334, "title": "[mnist]:Delete the unused domain names", "body": "Delete the unused domain names of `ps` and `worker`.\n", "comments": ["Can one of the admins verify this patch?\n", "@DjangoPeng, thanks for your PR! By analyzing the annotation information on this pull request, we identified @Mistobaan to be a potential reviewer\n", "@tensorflow-jenkins test this please.\n", "LGTM.\n", "Merged. Thanks.\n"]}, {"number": 4333, "title": "dynamic_rnn() cannot receive an input of dynamic shape during training", "body": "Hi all, I met some problem when using dynamic_rnn() and wonder if there is any solution.\n\nFirstly I sort all examples according to its length(variable length input). Then I use \"batch padding\" in my code. So in each step during training, data generator generates a padded minibatch, whose shape differ from the last batch. And I feed the minibatch data to the graph. Since the placeholder \"inputs\" in graph has an uncertain shape, it raised an error when tensor flows to `dynamic_rnn()` node. If I specify the placeholder's shape, everything is ok.\n\nNow `dynamic_rnn()` can only receive inputs with a fixed shape. How can I combine it with \"batch padding\"?\n\nThanks!\n", "comments": ["The batch and time dimensions in your placeholder may be None, but the last dimension must be statically known.\n", "@ebrevdo  Thanks for your clear answer! It works as you said.\n"]}, {"number": 4332, "title": "Tensor with inconsistent dimension size? ", "body": "I've been implementing a convolutional neural network for object detection and I met the issue below:\n\nFor object detection task, usually, one input image is associated with an undetermined number of object bounding boxes. Each bounding box can be represented by 4 coordinates. Thus, to represent bounding boxes as a tensor, the shape will be: \n    _[**batch_size, variable_num_bbox(?), 4**]_. \nNote that here, it's not just that _**variable_num_bbox**_ can't be determined before the graph construction, but also, even within one batch input, different images can have different numbers of bounding boxes. \n\nAs an illustrative example, I would like to convert the following array into a tensor: \n    _[[[1, 2, 3, 4], [2, 3, 4, 5]], [[3, 4, 5, 6]]]_\nHere, _**variable_num_bbox**_ is 2 for the first image, but it's 1 for the second image. \n\nI've tried and failed several ways to convert the above nested list to a tensor, which leads me to the question whether tensorflow support tensors with inconsistent dimension size? If no, is there any plan to support it to give developers such flexibility? And if it's not going to be supported, is there a way to by-pass this issue for object detection task? One solution would be to set _**batch_size=1**_ , and a bounding box can be represented as _**[variable_num_bbox(?), 4]**_, so yes, the dimension inconsistency is gone, but that will hurt the efficiencyy significantly.\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 4331, "title": "Problem with using tf.contrib.metrics.streaming_mean_iou", "body": "I am trying to use `tf.contrib.metrics.streaming_mean_iou` in my network and I am getting following Error.\n\n```\nW tensorflow/core/framework/op_kernel.cc:940] Failed precondition: Attempting to use uninitialized value mean_iou/total_confusion_matrix\n     [[Node: mean_iou/total_confusion_matrix/read = Identity[T=DT_INT64, _class=[\"loc:@mean_iou/total_confusion_matrix\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](mean_iou/total_confusion_matrix)]]\nTraceback (most recent call last):\n....\n....\nFile \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value mean_iou/total_confusion_matrix\n     [[Node: mean_iou/total_confusion_matrix/read = Identity[T=DT_INT64, _class=[\"loc:@mean_iou/total_confusion_matrix\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](mean_iou/total_confusion_matrix)]]\n....\n....\nFile \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 1805, in streaming_mean_iou\n    dtype=dtypes.int64)\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 79, in _create_local\n    collections=collections)\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 211, in __init__\n    dtype=dtype)\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 323, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1106, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/janzikes/anaconda2/envs/research/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n```\n\nIt seems to me that there might be some bug in the `tf.contrib.metrics.streaming_mean_iou`since all the variables are already initialised.\n\nI am using it as follows:\n\n```\naccuracy_increment, iou_increment = self.session.run(\n                [self.accuracy, self.mean_iou], feed_dict={\n                self.X: eval_patches[\n                        q * self.microbatch_size:\n                        q * self.microbatch_size +\n                        self.microbatch_size],\n                self.Y: eval_labels[\n                        q * self.microbatch_size:\n                        q * self.microbatch_size +\n                        self.microbatch_size],\n                self.p_keep_conv: 1})\n```\n\nAnd the mean iou is computed in the object as:\n\n```\nself.mean_iou, _ = tf.contrib.metrics.streaming_mean_iou(\n            self.predictions, tf.argmax(self.Y, dimension=3), num_classes=2)\n```\n\nI am using from source build tensorflow (branch r10.0) and it's happening to me on both CPU and GPU.\n\nI am not 100% sure that I am using the `tf.contrib.metrics.streaming_mean_iou` correctly, but I have played with that a lot, but there is quite lack of the documentation, so I am currently stuck assuming that there is either some bug in the `tf.contrib.metrics.streaming_mean_iou` or not enough documentation for me to make it work.\n\nI thank in advance anyone for any comments.\n", "comments": ["iou is using a local variable, have you called initialize_local_variables()?\n", "(sorry our initialization functions are wrongly named -- we're working on it)\n", "Than you, this was the problem, I did not call `initialize_local_variables()`.\n", "@martinwicke @ziky90 Hi I am currently using streaming_mean_iou function too. I want to know if there is a way to get the confusion matrix for precision of each class, not only the average.\n", "That would be a better question for StackOverflow. \n"]}, {"number": 4330, "title": "overparametrized convolution error in tf.nn.separable_conv2d ", "body": "I'm trying to build a network of layers with channel-wise separable convolution. (Just like the paper [Factorized CNN](http://128.84.21.199/abs/1608.04337))\n\nI've found that separable_conv2d and depthwise_conv2d is the two options, and I'm trying out these two.\n\nWhat I am trying to build is as below\n\n```\ndepthwise_filter = tf.get_variable(\"depth_conv_w\", [3,3,64,3], initialize=tf.random_normal_initializer(stddev=np.sqrt(2.0/9/32)))\npointwise_filter = tf.get_variable(\"point_conv_w\", [1,1,192,64], initializer=tf.random_normal_initializer(stddev=np.sqrt(2.0/9/128)))\nconv_tensor = tf.nn.depthwise_conv2d(tensor, depthwise_filter, [1,1,1,1], padding='SAME')\nconv_tensor = tf.nn.conv2d(conv_tensor, pointwise_filter, [1,1,1,1], padding='VALID')\n```\n\nand it works fine.\n\nHowever, if I switch the last 2 lines with\n\n```\nconv_tensor=tf.nn.separable_conv2d(tensor,depthwise_filter,pointwise_filter,[1,1,1,1],padding='SAME')\n```\n\nthen tensorflow gives me 'overparamatrized convolution error' as specified in [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn.py#L705)\n\nI think **channel_multiplier \\* in_channel** is usually larger than or equal to the **out_channel**.\nSo I believe this is an error.\n\nPS) I can get the same result as the **separable_conv2d** with **depthwise_conv2d and 1x1 convolution**. Is there any advantage of using **separable_conv2d**?\n", "comments": ["From a previous discussion of this topic:\n\n\"If this inequality isn't satisfied, you're expanding the number of activations and then reducing it, which is usually not a good idea. In particular, it often means you're using more parameters and FLOPS than you would with a regular convolution, which defeats the purpose of using a separable conv.\"\n", "I thought that the purpose of using separable conv is to share the intermediate activations before the linear channel projection (in a normal convolution, they are not shared), which is far efficient.\nA [diagram](http://i.stack.imgur.com/qaSdE.png) from '[Factorized Convolutional Neural Networks](https://arxiv.org/abs/1608.04337)' explains this concept well.\n\nMaybe I am misunderstanding the whole idea of separable_conv2d. Do you have the link for the previous discussion? @andydavis1 \n", "Unfortunately, I don't have a link to discussion. My understanding is that separable convolutions are used to reduce flop count (exploiting redundancy across the channels), which is why we raise errors for those parameter combinations.\n\nYou may get more information by posting this on Stackoverflow with the  `tensorflow` tag, so I'm going to close this out for now.\n", "Are you sure the math checking this isn't just being done wrong?  In a standard convolution the number of parameters is height x width x in_channels x out_channels.  By using a filter with height and width of one for the projection to out_channels in separable convolution it saves parameters by a factor of nearly height x width.\n\nSo a proper check for over parameterization is closer to in_channels x channel_multiplier > out_channels x height x width. \n", "Yes, I understand that it can reduce flop count significantly, and that when channel_multiplier is large, the operation becomes very complex.\nHowever, I think it should not be raised as an error for 2 reasons:\n1. separable_conv2d is not equivalent to conventional convolution.\n2. the amount of flops may be less even when in_channel \\* channel_multiplier > out_channel. Will elaborate more below.\n\nI will assume in_channel = out_channel since many modules (layers) in deep architecture has such config.\n\nFor a single patch, the number of multiplications in separable_conv2d is **in_channel \\* filter_w \\* filter_h \\* channel_multiplier + 1 \\* 1 \\* (in_channel \\* channel_multiplier) \\* out_channel**.\nThe number of multiplications in conventional conv is **in_channel \\* filter_w \\* filter_h \\* out_channel**.\nWhen we compare two computations, where out_channel >> channel_multiplier in most cases, the amount of computations may be less for separable_conv2d. \n\nIs it really necessary to add such a restriction that in_channel \\* channel_multiplier <= out_channel ?\n(in my case, for a single patch, channel_multiplier=3, the number of multiplications is 64 \\* 3 \\* 3 \\* 3+1 \\* 1 \\* 192 \\* 64 = 18,624, where the conventional conv yields 64 \\* 3 \\* 3 \\* 64=36,864)\n", "cc @vincentvanhoucke in case he wants to comment.\n", "Yes. I worked out a back-of-the-envelope flop cost between separable and conventional convolutions:\n\nOR = out_rows\nOC = out_cols\nID = in_depth\nDM = depth_multiplier\nOD = out_depth\nFR = filter_rows\nFC = filter_cols\n\nseparable_conv_cost = OR \\* OC \\* ID \\* DM \\* (FR \\* FC + OD)\nconventional_conv_cost = OR \\* OC \\* ID \\* FR \\* FC \\* OD\n\n// So to save on flops we want:\n\nDM < (FR \\* FC \\* OD) / (FR \\* FC + OD)\n\n// So plugging in your numbers from above (FR=FC=3, OD=192):\n\nDM < 8.5\n\nSo to save flops, we want DM < 8.5, yet your DM = 3.  So perhaps (at least from a flops perspective), this check is too restrictive.  There may be other reasons for this restriction (perhaps expansion and reduction of parameters leads to training issues), but I'll let Vincent comment on that... \n", "If you have valid uses for a separable convolution which blows up the number of activations and then shrinks them back, then I'm ok accepting a PR that removes this check. Separable convolutions predate this paper by a couple of years, so it's entirely possible that people have found productive uses of this regime. In general, introducing an activation bottleneck anywhere in the network tends to be a terrible idea from an optimization standpoint, and unless your dimensions are widely pathological, it also implies that you're introducing more free parameters than there was in the convolution in the first place. But one can argue there is nothing broken about doing so.\n", "Thank you for your comments, I'm sorry I am 1 week late with my response.\nI understand your concerns regarding computational optimization. Productive uses of such kind of operation should be verified. However, the researchers/developers should be able to choose it freely. Since the computation/free params may blow up, a warning can be added to the documentation.\nI will request a PR soon, but I am very new to Github, need to search how to do it...\n", "@vincentvanhoucke : Suppose I am doing style transfer on phone. That will require depthwise separable convolution in place of convolution. The thing is, this involves a decoder that typically reduces number of channels from 128 to 64, then 64 to 32, then 32 to 3. In all 3 cases, you will see the number of channels being decreased. Do you believe that is a valid use case? I wanted to use separable convolution, but cannot because of this reason in style transfer.", "Like I said, I'd gladly and swiftly accept a PR that removes that check.", "I'll send a change for it.", "Thank you! Using depthwise convolution followed by pointwise convolution is very slow. Like it takes almost double the time.", "@singlasahil14  Could you use separable conv in your model? I also want to use separable conv but in keras, or does it only work in tensorflow? . Can you share your code ?.", "@arnoldaclf : I wasn't trying to use it in keras. \r\nhttps://github.com/singlasahil14/style-transfer/blob/master/style_network_factory.py\r\nHere is my code."]}, {"number": 4329, "title": "typo in tutorials/mnist/beginners/index.html", "body": "https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html\n\n\"East entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.\"\n\n should probably be \"Each entry...\".\n\nSorry if this is an inappropriate forum to draw attention to this.  There was not corrections contact.\n", "comments": ["I see this typo was fixed 24 days ago [[commit](https://github.com/tensorflow/tensorflow/commit/e144b79a4e740c550f4866bfa33b1b4ff8790e8b)]. I wonder why the change is not reflected yet on the concerned webpage?\n", "@andydavis1 You can close the issus.\nI have checked the `master branch`, and the typo has been fixed by the commit which @ivmarkp referenced. The reason why it still exists is that the `tensorflow.org` show you the tutorial of `r0.10 branch` . So, when the next version(maybe v0.11) comes, the problem will be fixed automatically. \n"]}, {"number": 4328, "title": "In a distribute model, if I run 10 worker jobs, how many ps jobs should run?", "body": "In a distribute model, 10 nodes each have one GPU and run a worker job, how many ps jobs should run?\n", "comments": ["sorry, I should ask this problem in at right place.\n"]}, {"number": 4327, "title": "Updated the documentation for Pi building to address malloc bug", "body": "This is a docs-only change to address #3442 \n", "comments": ["@petewarden, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke, @naturegirl and @vrv to be potential reviewers\n"]}, {"number": 4326, "title": "ImportError: No module named _pywrap_tensorflow", "body": "Traceback (most recent call last):\n  File \"./train.py\", line 3, in <module>\n    import tensorflow as tf\n  File \"/Users/thomassonderman/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Users/thomassonderman/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 48, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/Users/thomassonderman/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 21, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/Users/thomassonderman/tensorflow_gpu/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 20, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow')\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    **import**(name)\nImportError: No module named _pywrap_tensorflow\n\nRelated Problem:\nSyntaxNet ImportError: No module named _pywrap_tensorflow #97\nFix did not work\n\nEnvironment:\nMacbook retina Nvidia Os: El Capitan \n\nInstalled version of CUDA and cuDNN: \ncuda 7.5\n\n(tensorflow_gpu) Thomass-MacBook-Pro:DIS thomassonderman$ python -c \"import tensorflow; print(tensorflow.**version**)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.7.5.dylib locally\n0.10.0rc0\n\nIf installed from source, provide \n\nThe commit hash (`git rev-parse HEAD`)\n\n3cb39956e622b322e43547cf2b6e337020643f21\n\nBuild label: 0.3.1-homebrew\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Aug 4 09:58:27 2016 (1470304707)\nBuild timestamp: 1470304707\nBuild timestamp as int: 1470304707\n\nimport tensorflow works, just not any examples\n### What other attempted solutions have you tried?\n\nredownloading the source and recompiling, reinstalling pip package.\nsetting up bazel-completion\n", "comments": ["I was running it as ./train.py instead of python train.py\n"]}, {"number": 4325, "title": "Remove autoencoder estimator", "body": "Reasons for removal:\n- I created this one long time ago but it's not working anymore since the API has been evolved quite a bit. It causes confusion for users. \n- Better and more flexible API needed to be figured out in order to support different types of autoencoders, e.g. RNN/LSTM, etc. Or perhaps users can just create their own custom models.\n", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @mrry, @tensorflower-gardener and @vrv to be potential reviewers\n", "@tensorflow-jenkins Test this please\n", "@terrytangyuan Ha! I had just started using `TensorFlowDNNAutoencoder` successfully. Can you point me at a good alternative?\n", "You can provide your own `model_fn` and pass it to `Estimator`. You can take a look at what I have removed. Feel free to submit a PR for more flexible APIs in your mind. \n", "@terrytangyuan I'm struggling in particular to figure out how to get the [`generate()`](https://github.com/tensorflow/tensorflow/blob/37256f4857cdadefa09e0505f4acc91ffbf626e2/tensorflow/contrib/learn/python/learn/estimators/autoencoder.py#L92-L100) method to work, since the estimator now lacks a `._session`.\n\nWhere do I find an appropriate session? I guess I want to use `Estimator._infer_model` or `Estimator.evaluate`, but I'm a bit tangled here. I can't figure it out what I need to pass from the tf.learn docs.\n", "Perhaps my blog post could help you understand a bit more: http://terrytangyuan.github.io/2016/07/08/understand-and-build-tensorflow-estimator/\n\nWe switched to use [SessionRunHook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/session_run_hook.py) in Estimator which is why you cannot find the `._session`. \nThis does require some reading of the source code instead of just reading from the docs. Hope this helps. \n", "The blog post doesn't mention SessionRunHook, can you point me at an example which uses it?\n\nIn particular, it's not clear how (before it was removed) TensorFlowDNNAutoencoder maintained state between `fit` and `transform`, and how to hook this up with SessionRunHook.\n", "You can find some usages in tests. The blog post is pretty old to the speed\nof development of TF.\n\nOn Friday, September 16, 2016, Peter Waller notifications@github.com\nwrote:\n\n> The blog post doesn't mention SessionRunHook, can you point me at an\n> example which uses it?\n> \n> In particular, it's not clear how (before it was removed)\n> TensorFlowDNNAutoencoder maintained state between fit and transform, and\n> how to hook this up with SessionRunHook.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4325#issuecomment-247709697,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AEEnSj9cLVuXi0v3_KC5TlPFlNlTPMEsks5qqwW3gaJpZM4J6Efp\n> .\n"]}, {"number": 4324, "title": "can't find module tensorflow after building 'quantize_graph'", "body": "Hi,\n\nI followed this [blog post](https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/) in order build the tool for quantizing graph (tensorflow/contrib/quantization/tools/quantize_graph).\nI had a clean working version of tensorflow installed properly.\n\nI used this command in order to build the quantization tool :\n**bazel build tensorflow/contrib/quantization/tools:quantize_graph**\n\nIt seems that the build went ok :\n**INFO: Found 1 target...\nTarget //tensorflow/contrib/quantization/tools:quantize_graph up-to-date:\n  bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph\nINFO: Elapsed time: 20.105s, Critical Path: 16.60s**\n\nHowever after that finished when i open python and try to import tensorflow i get an error message :\n**ImportError: No module named tensorflow**\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled from source, without GPU support :\n1. commit hash = 37256f4857cdadefa09e0505f4acc91ffbf626e2\n2. bazel version = \nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n", "comments": ["i get the same behaviour when trying to build the freeze_graph tool :\nbazel build tensorflow/python/tools:freeze_graph\n", "It looks like you need to follow the install steps again. Feel free to reopen the issue if you still have the problem."]}, {"number": 4323, "title": "add an `is` to make it a sentence", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@vra, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers\n", "@tensorflow-jenkins test this please\n", "@vra, can you please sign the CLA? \n", "@caisq sorry for that. I update my CLA and hope the check will begin automatically.\n", "LGTM.\n", "Merged. Thanks.\n"]}, {"number": 4322, "title": "Ubuntu16.04+cuda8+cudnn5.1, meets error", "body": "> INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n> .\n> ERROR: /home/ei/github/test/tensorflow/tensorflow/contrib/session_bundle/BUILD:237:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /home/ei/github/test/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature_lite'.\n> ERROR: /home/ei/github/test/tensorflow/tensorflow/core/BUILD:689:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core:ios_tensorflow_test_lib'.\n> ERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n> Configuration finished\n\nI'm trying to do as http://textminingonline.com/dive-into-tensorflow-part-iii-gtx-1080-ubuntu16-04-cuda8-0-cudnn5-0-tensorflow. when it comes to './configure', terminal show errors.\n\nWho can help me with this issue\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/4312\n\nI got an answer here,but no use.\n\nthe true answer is:# **DONOT USE git clone**,DOWNLOAD tensorflow10rc0.zip from github instead!\n"]}, {"number": 4321, "title": "model.saver.save() freeze and get killed", "body": "Below is my flow for training an encoder-decoder model.\n\nThe training runs alright. But the program froze and eventually got killed during the call to Saver.export_meta_graph(). \n\nHowever, I was able to get the saved checkpoint and continue training.\n\nDoes anyone know what may be the cause for this problem?\n\n```\n    for t in xrange(FLAGS.num_epochs):\n        print(\"Epoch %d\" % t)\n\n        start_time = time.time()\n\n        # shuffling training examples\n        # random.shuffle(train_set)\n\n        # progress bar\n        for _ in tqdm(xrange(FLAGS.steps_per_checkpoint)):\n            time.sleep(0.01)\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                             if train_buckets_scale[i] > random_number_01])\n            formatted_example = model.get_batch(train_set, bucket_id)\n            _, step_loss, _ = model.step(sess, formatted_example, bucket_id, \n                                         forward_only=False)\n            loss += step_loss\n            current_step += 1\n\n        epoch_time = time.time() - start_time\n\n        # Once in a while, we save checkpoint, print statistics, and run evals.\n        if t % FLAGS.epochs_per_checkpoint == 0:\n\n            # Print statistics for the previous epoch.\n            loss /= FLAGS.steps_per_checkpoint\n            ppx = math.exp(loss) if loss < 300 else float('inf')\n            print(\"learning rate %.4f epoch-time %.2f perplexity %.2f\" % (\n                model.learning_rate.eval(), epoch_time, ppx))\n\n            # Decrease learning rate if no improvement of loss was seen over last 3 times.\n            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n                sess.run(model.learning_rate_decay_op)\n            previous_losses.append(loss)\n\n            checkpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")\n```\n", "comments": ["Could you try to provide more information to narrow this down a bit?  \n\n1) I don't see `tf.train.Saver` being used in your snippet.\n2) It'd be good to see the stack trace when the program got killed.\n3) How large are your MetaGraph, GraphDef, and checkpoint?\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 4320, "title": "Minor corrections on comments in census_widendeep test", "body": "", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @caisq and @theweiho to be potential reviewers\n", "LGTM. Thanks.\n"]}, {"number": 4319, "title": "Error when building tensorflow from source", "body": "I am trying to build and install tensorflow from source. However when building with bazel it returns an error stating Extension file 'tensorflow/tensorflow.bzl' has errors.\n\nHere are the commands that I ran and the logs are included.\n\n``` bash\n\nmkvirtualenv tensorflow_dev\nbrew install bazel swig\nworkon tensorflow_dev\npip install six numpy wheel ipython\nexport TF_DIR=/Users/shashank/Documents/repositories/tensorflow\ncd /Users/shashank/Documents/repositories/\ngit clone git@github.com:tensorflow/tensorflow.git\ncd $TF_DIR && ./configure\n\n~/Documents/repositories/tensorflow ~/Documents/repositories/tensorflow\nPlease specify the location of python. [Default is /Users/shashank/.virtualenvs/tensorflow_dev/bin/python]:\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /Users/shashank/Documents/py_config/\n  /Users/shashank/.virtualenvs/tensorflow_dev/lib/python2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/Users/shashank/Documents/py_config/]\n\n/Users/shashank/Documents/py_config/\nDo you wish to build TensorFlow with GPU support? [y/N]\nNo GPU support will be enabled for TensorFlow\nConfiguration finished\n\ncd $TF_DIR && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\nERROR: /Users/shashank/Documents/repositories/tensorflow/tensorflow/tensorflow.bzl:568:26: Traceback (most recent call last):\n        File \"/Users/shashank/Documents/repositories/tensorflow/tensorflow/tensorflow.bzl\", line 562\n            rule(attrs = {\"srcs\": attr.label_list...\"), <3 more arguments>)}, <2 more arguments>)\n        File \"/Users/shashank/Documents/repositories/tensorflow/tensorflow/tensorflow.bzl\", line 568, in rule\n            attr.label_list(cfg = \"data\", allow_files = True)\nexpected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.\nINFO: Elapsed time: 0.063s\n```\n\nThanks for your help\n", "comments": ["Try to update your bazel installation. I had this problem a few minutes ago, and this seems to have solved it on my side.\n", "I reinstalled, as you have prescribed. Same error. I think there is a configuration step that I am missing with the tensorflow.bzl build file\n", "I have the same issue\n", "I had the same issue with master master, but I could build after switching to the r0.10 branch.\n", "I'm having the same error. It points out to an invalid configuration. A recent commit has changed the lines pointed by the error: https://github.com/tensorflow/tensorflow/commit/7bcdcbbf60fc08346fd8016270a0563f4b51362b\n", "Update: upgrading bazel, as pointed out by @CalaveraLoco , seems to fix the problem.\nThe issue I had is that I have two bazel installations and only upgraded the 'second' one.\n", "Which version of bazel do I need to upgrade to ?\n\nI installed bazel with homebrew.\n\nHere are my build details\n\n```\nBuild label: 0.2.1-homebrew\nBuild target: bazel-out/local_darwin-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Apr 1 00:35:17 2016 (1459470917)\nBuild timestamp: 1459470917\nBuild timestamp as int: 1459470917\n```\n", "I used 0.3.0\n", "I updated my bazel version to 0.3.1\n\n```\nBuild label: 0.3.1-homebrew\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Aug 4 09:58:27 2016 (1470304707)\nBuild timestamp: 1470304707\nBuild timestamp as int: 1470304707\n```\n\nNow when I execute this command in the root dir of tensorflow. I receive a different error.\n\n```\nexport TF_DIR=/Users/shashank/Documents/repositories/tensorflow\ncd /Users/shashank/Documents/repositories/\ngit clone git@github.com:tensorflow/tensorflow.git\ncd $TF_DIR && ./configure\n\n~/Documents/repositories/tensorflow ~/Documents/repositories/tensorflow\nPlease specify the location of python. [Default is /Users/shashank/.virtualenvs/tensorflow_dev/bin/python]:\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /Users/shashank/Documents/py_config/\n  /Users/shashank/.virtualenvs/tensorflow_dev/lib/python2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/Users/shashank/Documents/py_config/]\n\n/Users/shashank/Documents/py_config/\nDo you wish to build TensorFlow with GPU support? [y/N]\nNo GPU support will be enabled for TensorFlow\nConfiguration finished\n\ncd $TF_DIR && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n..\nERROR: /Users/shashank/Documents/repositories/tensorflow/tensorflow/BUILD:69:1: //tensorflow:all_files: invalid label '@' in element 0 of attribute 'srcs' in 'filegroup' rule: invalid fully-qualified label: @.\nERROR: /Users/shashank/Documents/repositories/tensorflow/tensorflow/BUILD:84:12: Target '//tensorflow:internal' contains an error and its package is in error (this is usually caused by a missing package group in the package-level visibility declaration).\nERROR: /Users/shashank/Documents/repositories/tensorflow/tensorflow/BUILD:82:1: Target '//tensorflow:__init__.py' contains an error and its package is in error and referenced by '//tensorflow:tensorflow_py'.\nERROR: /Users/shashank/Documents/repositories/tensorflow/tensorflow/tools/pip_package/BUILD:16:1: Target '//tensorflow:tensorflow_py' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:simple_console'.\nERROR: /Users/shashank/Documents/repositories/tensorflow/tensorflow/tools/pip_package/BUILD:23:1: Target '//tensorflow:tensorflow_py' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 2.489s\n```\n", "Hi , My environment is Ubuntu16.04 +gtx1080+CUDA8.0 +CUDNNV5.1+Python2.7+Bazel 0.3.\n0\nWhen I execute ./configure command in tensorflow-master directory, I experienced the same  error with  @shkr \n\nHow should this problem be fixed?\n\nThanks\n", "@szsongyj Easiest fix is to as people have said to upgrade bazel to 0.3.1. Another quick fix is to revert the offending commit:\n\n```\ngit show -R 7bcdcbbf | git apply\n```\n", "@gustavla Thanks for your kindly help.\nI will try this solution and  feedback here this evening.\n\nRegards,\n", "I cloned the git repo again, then also installed `brew install coreutils` (i do not have a GPU I did it anyways). After that the source build is working with bazel 0.3.1. Thanks @gustavla \n\nI will investigate why in my other build I have an error with bazel 0.3.1.\n", "@gustavla  @shkr  \nI updated the bazel and confirmed the current version is  0.3.1(using command: \"which bazel\" and   \"bazel version\"respectively )\n\nand I run  the configure process as @gustavla suggested:\n\ngit clone https://github.com/tensorflow/tensorflow\ncd tensorflow\ngit show -R 7bcdcbbf | git apply\n./configure\nBut the following errors appeared and configured process failed\n\n/home/syj/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1no such target '//tensorflow/core:android_lib_lite':target 'android_lib_lite'not declared in package.....\n\n/home/syj/tensorflow/tensorflow/core/BUILD:899:1no such package '@zlib_archive//':Error downloading from http://zlib.net/zlib-1.2.8.tar.gz to ......\n\nconfiguration of query \"deps((//...union @bazel_tools//tools/jdk:toolchain))\"failed:errors were encountered while computing transitive closure.\n\nI am appreciated if anyone can tell me how to solve this problem.\n\nThank you\n", "@szsongyj Is this an actual copy-paste?\n\n//tensorflow/core:android_lib_lite does not exist, as the actual name is //tensorflow/core:android_tensorflow_lib_lite.\n\nEven then, it would not be referenced from tensorflow/contrib/session_bundle/BUILD. Can you check to make sure the file itself has not been altered somehow?\n", "@szsongyj The problem you are seeing is an unrelated issue that has been resolved (see #4312). You should make sure you have the latest master branch. You can make sure your checked out copy has incorporated this commit by running `git branch --contains f66b491` and make sure you see a line that starts with `*` (e.g. `* master`).\n\n@andewharp The error I got in #4312 also said `android_lib_lite`. However, the whole premise of that issue was that those packages only exist in Google's internal branches, so I have no idea if that's a typo or not.\n", "@gustavla Right, I probably should have remembered merging that commit :P\n", "@andrewharp\n1)The file itself has not been altered. Because i just issue the following commands:\ngit clone https://github.com/tensorflow/tensorflow\ncd tensorflow\n./configure\n\n2)Is this an actual copy-paste? \nThis is not the actual copy-paste ,because I run the configure last night  at home with my home computer,Then I got to bed before the config process finished ,and I  found the error results when I got up early in this morning , then I took some screen photos before I  left home for work. (now i am in office)\n\nSo  the error information is exactly like what I have posted althrough this is only part of error information\n@gustavla \nThank you. Could you please tell me the exact commands assuming my current location is \"/home/syj\", then what commands one by one should I type?\n\nJust like i have said what i have done is just typing the following commands,but failed . \n\ngit clone https://github.com/tensorflow/tensorflow\ncd tensorflow\n./configure\n\nby the way. Concerning my environment , CUDA8,CuDNN5.1 ,Caffe are all normal.  \n", "@shkr The internal fix https://github.com/tensorflow/tensorflow/commit/f66b491a06627510c1cf751fc11db2caf5aa1f25 was pushed today in https://github.com/tensorflow/tensorflow/pull/4360, so if you sync now I assume the issue will go away. I had just forgotten it was part of the push when I asked you about the copy-paste.\n", "@gustavla\n\nPlease refer the following ,does that indicate my cloned tensorflow is the latest master branch or not? What is the reason of the error? How can i fix it ? Thank you&Regards\n\nsyj@syj-dl:~/tensorflow$ git branch\n- master\n  syj@syj-dl:~/tensorflow$ git branch --contains f66b491\n  error: malformed object name f66b491\n", "syj@syj-dl:~/tensorflow$ git branch\n- master     (this is a star before master)\n\nsyj@syj-dl:~/tensorflow$ git branch --contains f66b491 \nerror: malformed object name f66b491\n", "syj@syj-dl:~$ git clone https://github.com/tensorflow/tensorflow\n\u6b63\u514b\u9686\u5230 'tensorflow'...\nremote: Counting objects: 86413, done.\nremote: Compressing objects: 100% (6/6), done.\nremote: Total 86413 (delta 0), reused 0 (delta 0), pack-reused 86407\n\u63a5\u6536\u5bf9\u8c61\u4e2d: 100% (86413/86413), 56.90 MiB | 1.30 MiB/s, \u5b8c\u6210.\n\u5904\u7406 delta \u4e2d: 100% (63995/63995), \u5b8c\u6210.\n\u68c0\u67e5\u8fde\u63a5... \u5b8c\u6210\u3002\nsyj@syj-dl:~$ cd tensorflow\nsyj@syj-dl:~/tensorflow$ git branch\n- master\n  syj@syj-dl:~/tensorflow$ git branch -av\n- master                                  02a8d07 added threadpool.h to framework_headers in BUILD (#4108)\n  remotes/origin/0.6.0                    4be56f3 Merge pull request #1085 from tensorflow/vrv-fix-stale-doc\n  remotes/origin/HEAD                     -> origin/master\n  remotes/origin/go                       5b11eb9 remove non idiomatic NewGraphFromReader(reader, bool) and add NewGraphFromBuffer and New Graph\n  remotes/origin/issue_template_update    9d32095 Add instructions how to get commit hash\n  remotes/origin/master                   02a8d07 added threadpool.h to framework_headers in BUILD (#4108)\n  remotes/origin/mrry-patch-1             56fc276 Delete unused boringssl.BUILD\n  remotes/origin/mrry-patch-2             61542a5 Add -O2 to the adding an op HOWTO\n  remotes/origin/r0.10                    c715c31 Merge pull request #4284 from caisq/r0.10-fixes\n  remotes/origin/r0.7                     657f1d9 Python3 word2vec compatibility issue #1760 fixed\n  remotes/origin/r0.8                     9b69ec3 Fix broken link to Anaconda installation (#2679)\n  remotes/origin/r0.9                     9454b90 Merge pull request #3734 from RenatoUtsch/r0.9\n  remotes/origin/revert-3010-issue-2977   69e9316 Revert \"Added complex type support to ops for GPU\"\n  remotes/origin/revert-3614-satok0       70846ec Revert \"Fix prototype mismatch of ByteCount in env.cc\"\n  remotes/origin/tflearn-tutorial-updates 6e42687 New tutorial on tf.contrib.learn monitors. Change: 128589916\n  remotes/origin/tutorial-url-fix         51b4ea4 As of 2016-08-01, feature_columns argument is now required by DNNClassifier; updating tutorials/code accordingly. Change: 130785833\n  remotes/origin/vincentvanhoucke-patch-1 1862eb9 Remove confusing section\n  remotes/origin/vincentvanhoucke-patch-2 1cf5af0 Make the default command use a named container\n  remotes/origin/vrv-patch-1              35d2eea Update tf_op_files.txt to add tile_6.cc\n  syj@syj-dl:~/tensorflow$ git branch --contains f66b491\n- master\n  syj@syj-dl:~/tensorflow$ ./configure\n  ~/tensorflow ~/tensorflow\n  Please specify the location of python. [Default is /home/syj/anaconda2/bin/python]: \n  Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\n  Google Cloud Platform support will be enabled for TensorFlow\n  Found possible Python library paths:\n  /home/syj/anaconda2/lib/python2.7/site-packages\n  Please input the desired Python library path to use.  Default is [/home/syj/anaconda2/lib/python2.7/site-packages]\n\n/home/syj/anaconda2/lib/python2.7/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n\n.\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nERROR: /home/syj/tensorflow/tensorflow/core/BUILD:692:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core:ios_tensorflow_test_lib\n", ".\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nERROR: /home/syj/tensorflow/tensorflow/core/BUILD:692:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core:ios_tensorflow_test_lib'.\nERROR: /home/syj/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/syj/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/syj/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/syj/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/syj/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n", "The above is the whole steps and  output? what's wrong?  \n", "@szsongyj If you're getting \"malformed object name\" it means your repo doesn't contain that commit. However your most recent clone seems to contain it.\n\nCan you try patching in the https://github.com/okuchaiev/tensorflow-float16/commit/c1b6623154e14799419f91bd197b7900a78dbc2b commit referenced by https://github.com/tensorflow/tensorflow/issues/4312? I think that's what you're running into now.\n\nBTW, you can make your console output more readable by using the \"insert code\" button in the editor when pasting.\n", "Thank you @andrewharp \n\nin [https://github.com/tensorflow/tensorflow/issues/4312](url), @ibab said  \n\n> You can fix this by commenting out the if_android and if_ios calls in contrib/session_bundle/BUILD and the android_tensorflow_lib_lite_no_rtti_lite_runtime target in the core BUILD file.These seem to be based on Google internal targets.\n\nDoes that means I should  edit BUILD file under /home/syj/tensorflow , or some other file? Thank you\n`syj@syj-dl:~/tensorflow$ vi BUILD`\n", "`syj@syj-dl:~/tensorflow$ ls\nACKNOWLEDGMENTS  bzip2.BUILD      grpc.BUILD         png.BUILD    util\nADOPTERS.md      configure        ISSUE_TEMPLATE.md  README.md    WORKSPACE\nAUTHORS          CONTRIBUTING.md  jpeg.BUILD         RELEASE.md   zlib.BUILD\navro.BUILD       eigen.BUILD      jsoncpp.BUILD      six.BUILD\nboost.BUILD      farmhash.BUILD   LICENSE            tensorflow\nbower.BUILD      gif.BUILD        nanopb.BUILD       third_party\nBUILD            gmock.BUILD      navbar.md          tools\n`\nIt seems that in my side , thers is no \n\n> tensorflow/core/kernels/BUILD\n", "@szsongyj You are listing only the contents of the local directory. Use `ls -R` to see the entire tree recursively, or `ls tensorflow/core/kernels` to see the contents of the relevant directory.\n\nTo edit the actual file from your current location try `vi tensorflow/core/kernels/BUILD`.\n", "@andrewharp  sorry I got it . it is a foolish question\n\nsyj@syj-dl:~/tensorflow/tensorflow/core/kernels$ ls BUILD\n\nBUILD\n", "@szsongyj If you select the content after pasting and then press \"Insert code\", github will pick the right escape chars (it needs three quotes for multi-line chunks).\n\nRegarding the question, you may need to apply the entire patch if you continue to run into problems after commenting out only that one lib. I would just try it and see what happens.\n", "@andrewharp Thank you for your  guidance. NowI have commented out \u201candroid_tensorflow_kernels_no_rtti_lite_runtime\u201d section\uff0cand  the configure is running . I  will paste the errors here if problem still exists. Thanks again.\n", "ERROR: /home/syj/tensorflow/tensorflow/core/BUILD:692:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core:ios_tensorflow_test_lib'.\n", "@szsongyj  ios_tensorflow_test_lib is also commented out in the patch. I would just go ahead apply the entire thing.\n", "@ andrewharp   OK I have commented \" ios_tensorflow_test_lib\" section\n", "ERROR: /home/syj/tensorflow/tensorflow/workspace.bzl:85:3: no such package '@six_archive//': Error downloading from https://pypi.python.org/packages/source/s/six/six-1.10.0.tar.gz#md5=34eed507548117b2ab523ab14b2f8b55 to /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/six_archive: Error downloading https://pypi.python.org/packages/source/s/six/six-1.10.0.tar.gz#md5=34eed507548117b2ab523ab14b2f8b55 to /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/six_archive/six-1.10.0.tar.gz: Connection timed out and referenced by '//external:six'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n", "Can you try `git pull --recurse-submodules`?\n", "At this moment, I have totally made modification as guidance of [https://github.com/okuchaiev/tensorflow-float16/commit/c1b6623154e14799419f91bd197b7900a78dbc2b](url)\n\nso I am reconfiguring right now. Let's wait to see the result.\n", "@andrewharp  you mean \ni execute the command of \n\n> git pull --recurse-submodules\n\nunder  /home/syj/tensorflow    ?\n", "Yes. I usually just add the `--recurse-submodules` flag when I clone to make things easier.\n", "@ andrewharp  \nsyj@syj-dl:~/tensorflow$ git pull --recurse-submodules\nremote: Counting objects: 767, done.\nremote: Total 767 (delta 633), reused 633 (delta 633), pack-reused 133\n\u63a5\u6536\u5bf9\u8c61\u4e2d: 100% (767/767), 121.42 KiB | 16.00 KiB/s, \u5b8c\u6210.\n\u5904\u7406 delta \u4e2d: 100% (641/641), \u5b8c\u6210 234 \u4e2a\u672c\u5730\u5bf9\u8c61.\n\u6765\u81ea https://github.com/tensorflow/tensorflow\n   8f91632..4addf4b  master     -> origin/master\n\u66f4\u65b0 8f91632..4addf4b\nerror: Your local changes to the following files would be overwritten by merge:\n    tensorflow/core/BUILD\nPlease, commit your changes or stash them before you can merge.\nAborting\n", "Just as [https://github.com/okuchaiev/tensorflow-float16/commit/c1b6623154e14799419f91bd197b7900a78dbc2b](url) , I modified the following three files \n\ntensorflow/contrib/session_bundle/BUILD\ntensorflow/core/BUILD\n tensorflow/core/kernels/BUILD \n\nSo when i execute \n`syj@syj-dl:~/tensorflow$ git pull --recurse-submodules`  \n\nIt prompts me \n\n> error: Your local changes to the following files would be overwritten by merge:\n> tensorflow/core/BUILD\n>  Aborting\n\nWhat should I do?\n", "@ andrewharp  I need  leave  and be back tomorrow. Hope to still get your help regarding the pending issues. I will try when i am back ,and give you feedback\n I have been stucked in this issue for several days. Hope to get it solved ASAP.\nThank you & Best Regards.\n", "@szsongyj Perhaps try one of the methods suggested here: http://stackoverflow.com/questions/14318234/how-to-ignore-error-on-git-pull-about-my-local-changes-would-be-overwritten-by-m\n\nAlternatively follow the [xkcd method](https://xkcd.com/1597/) and start over, this time with --recurse-submodules :)\n", "Just to make sure we don't lose sight of this issue, there should be some discussion of what the best solution to this is. As I see it, the two easiest fixes is either to revert 7bcdcbbf or simply bump the officially supported version of bazel to 0.3.1.\n", "If someone could also provide explanation for why this commit is necessary https://github.com/tensorflow/tensorflow/commit/7bcdcbbf60fc08346fd8016270a0563f4b51362b, that will also be useful.\n", "`syj@syj-dl:~$ cd tensorflow/\nsyj@syj-dl:~/tensorflow$ git reset --hard\nHEAD \u73b0\u5728\u4f4d\u4e8e 8f91632 fix comment grammar (#4381)\n\nsyj@syj-dl:~/tensorflow$ git pull --recurse-submodules\nremote: Counting objects: 12, done.\nremote: Total 12 (delta 6), reused 6 (delta 6), pack-reused 6\n\u5c55\u5f00\u5bf9\u8c61\u4e2d: 100% (12/12), \u5b8c\u6210.\n\u6765\u81ea https://github.com/tensorflow/tensorflow\n   4addf4b..503a202  master     -> origin/master\n\u66f4\u65b0 8f91632..503a202\nFast-forward\n README.md                                          |    8 +-\n grpc.BUILD                                         |    3 +\n tensorflow/cc/framework/cc_op_gen.cc               |    5 +\n tensorflow/contrib/factorization/BUILD             |    1 -\n tensorflow/contrib/factorization/**init**.py       |    6 +-\n tensorflow/contrib/factorization/examples/mnist.py |    7 +-\n .../python/kernel_tests/clustering_ops_test.py     |   15 +-\n .../python/kernel_tests/wals_solver_ops_test.py    |    4 +-\n .../python/ops/factorization_ops_test.py           |   19 +-\n .../factorization/python/ops/gmm_ops_test.py       |   19 +-\n .../contrib/factorization/python/ops/gmm_test.py   |   74 +-\n .../factorization/python/ops/kmeans_test.py        |   93 +-\n tensorflow/contrib/framework/BUILD                 |    1 -\n tensorflow/contrib/framework/**init**.py           |    2 +-\n .../contrib/framework/python/framework/**init**.py |    1 +\n .../framework/python/framework/deprecation.py      |   80 ++\n .../framework/python/framework/deprecation_test.py |  207 ++++\n .../contrib/framework/python/ops/**init**.py       |    1 -\n .../contrib/framework/python/ops/embedding_ops.py  |   80 --\n tensorflow/contrib/layers/BUILD                    |    1 -\n tensorflow/contrib/layers/**init**.py              |    1 +\n .../contrib/layers/python/layers/embedding_ops.py  |   14 +-\n .../contrib/layers/python/layers/feature_column.py |   52 +-\n .../contrib/layers/python/layers/optimizers.py     |    7 +-\n .../layers/python/layers/optimizers_test.py        |   11 +-\n tensorflow/contrib/learn/BUILD                     |   13 +\n .../estimators/dnn_sampled_softmax_classifier.py   |  513 +++++++++\n .../dnn_sampled_softmax_classifier_test.py         |  382 +++++++\n .../learn/python/learn/estimators/estimator.py     |    4 +-\n .../learn/python/learn/estimators/linear.py        |   16 +-\n .../learn/python/learn/estimators/random_forest.py |   55 +-\n .../learn/python/learn/learn_io/graph_io.py        |    7 +-\n tensorflow/contrib/learn/python/learn/monitors.py  |    4 +-\n .../learn/python/learn/tests/estimators_test.py    |   26 +-\n tensorflow/contrib/learn/python/learn/trainable.py |    5 +-\n tensorflow/contrib/makefile/Dockerfile             |   17 +\n tensorflow/contrib/makefile/build_with_docker.sh   |   54 +\n tensorflow/contrib/rnn/BUILD                       |   44 +-\n tensorflow/contrib/rnn/ops/gru_ops.cc              |   35 +\n tensorflow/contrib/rnn/ops/gru_ops_test.cc         |   63 ++\n tensorflow/contrib/rnn/ops/lstm_ops.cc             |   84 ++\n tensorflow/contrib/rnn/ops/lstm_ops_test.cc        |  196 ++++\n tensorflow/contrib/rnn/python/ops/gru_ops.py       |   24 +-\n tensorflow/contrib/rnn/python/ops/lstm_ops.py      |   67 +-\n tensorflow/contrib/slim/python/slim/evaluation.py  |   50 +-\n .../contrib/slim/python/slim/evaluation_test.py    |    5 +\n tensorflow/contrib/tensor_forest/data/data_ops.py  |   37 +-\n tensorflow/core/BUILD                              |    2 +\n tensorflow/core/common_runtime/constant_folding.cc |    6 +\n tensorflow/core/debug/BUILD                        |   11 +\n tensorflow/core/debug/debug_service.proto          |   35 +\n tensorflow/core/distributed_runtime/BUILD          |   12 +\n .../core/distributed_runtime/master_session.cc     |  214 +++-\n tensorflow/core/distributed_runtime/scheduler.cc   |  319 ++++++\n tensorflow/core/distributed_runtime/scheduler.h    |  118 ++\n tensorflow/core/framework/cost_graph.proto         |    7 +-\n .../core/framework/shape_inference_testutil.cc     |    2 +-\n tensorflow/core/kernels/constant_op.cc             |    2 +\n tensorflow/core/kernels/cwise_op_abs.cc            |    1 +\n tensorflow/core/kernels/cwise_op_div.cc            |    7 +-\n tensorflow/core/kernels/cwise_op_gpu_abs.cu.cc     |    2 +-\n tensorflow/core/kernels/cwise_op_gpu_div.cu.cc     |    3 +-\n tensorflow/core/kernels/cwise_op_gpu_mul.cu.cc     |    4 +-\n tensorflow/core/kernels/cwise_op_gpu_sign.cu.cc    |    2 +-\n tensorflow/core/kernels/cwise_op_mul.cc            |    8 +-\n tensorflow/core/kernels/cwise_op_sign.cc           |    4 +-\n tensorflow/core/kernels/cwise_ops_gpu_common.cu.h  |    3 +\n tensorflow/core/kernels/fft_ops.cc                 |   53 +-\n tensorflow/core/kernels/reverse_op.cc              |   18 +-\n tensorflow/core/kernels/reverse_op_gpu.cu.cc       |    1 -\n tensorflow/core/kernels/shape_ops.cc               |    4 +-\n tensorflow/core/kernels/slice_op.cc                |    4 +\n tensorflow/core/kernels/slice_op_cpu_impl.h        |    2 +-\n tensorflow/core/kernels/slice_op_gpu.cu.cc         |    2 +\n tensorflow/core/kernels/stack_ops.cc               |   10 +-\n tensorflow/core/kernels/tensor_array_ops.cc        |    2 +\n tensorflow/core/lib/io/match_test.cc               |    5 +-\n tensorflow/core/lib/io/path.cc                     |   81 ++\n tensorflow/core/lib/io/path.h                      |    9 +\n tensorflow/core/lib/io/path_test.cc                |   20 +\n .../core/lib/io/snappy/snappy_buffers_test.cc      |    2 +-\n tensorflow/core/lib/io/table_builder.cc            |    2 +-\n tensorflow/core/lib/io/table_builder.h             |    3 +-\n tensorflow/core/lib/io/zlib_buffers_test.cc        |    2 +-\n tensorflow/core/lib/io/zlib_outputbuffer.cc        |    5 +-\n tensorflow/core/lib/png/png_io.cc                  |    2 +-\n tensorflow/core/lib/strings/str_util.h             |    1 +\n tensorflow/core/ops/compat/ops_history.v0.pbtxt    |  215 ++--\n tensorflow/core/ops/data_flow_ops.cc               |  197 ++--\n tensorflow/core/ops/data_flow_ops_test.cc          |   63 +-\n tensorflow/core/ops/io_ops.cc                      |   53 +-\n tensorflow/core/ops/io_ops_test.cc                 |   49 +-\n tensorflow/core/ops/math_ops.cc                    |  117 +-\n tensorflow/core/ops/math_ops_test.cc               |   28 +-\n tensorflow/core/ops/ops.pbtxt                      |  113 +-\n tensorflow/core/platform/cloud/gcs_file_system.cc  |  243 ++++-\n tensorflow/core/platform/cloud/gcs_file_system.h   |    7 +-\n .../core/platform/cloud/gcs_file_system_test.cc    |  394 +++++--\n .../platform/cloud/google_auth_provider_test.cc    |    2 +-\n tensorflow/core/platform/cloud/http_request.cc     |   82 +-\n tensorflow/core/platform/cloud/http_request.h      |   33 +-\n tensorflow/core/platform/cloud/http_request_fake.h |   56 +-\n .../core/platform/cloud/http_request_test.cc       |   75 +-\n tensorflow/core/platform/cloud/oauth_client.cc     |    4 +-\n tensorflow/core/platform/env.cc                    |    3 +-\n tensorflow/core/platform/env_test.cc               |    9 +-\n tensorflow/core/platform/file_system.cc            |    7 +-\n tensorflow/core/platform/file_system.h             |    3 +-\n tensorflow/core/protobuf/config.proto              |    3 +\n tensorflow/core/public/version.h                   |    3 +-\n tensorflow/core/util/example_proto_fast_parsing.cc |    6 +-\n .../g3doc/api_docs/python/contrib.framework.md     |   86 +-\n tensorflow/g3doc/api_docs/python/contrib.layers.md |   51 +-\n tensorflow/g3doc/api_docs/python/framework.md      |    4 +-\n .../shard0/tf.contrib.framework.deprecated_args.md |   36 +\n ...ntrib.framework.safe_embedding_lookup_sparse.md |   44 -\n .../python/functions_and_classes/shard0/tf.fft.md  |   10 +-\n .../functions_and_classes/shard0/tf.fft2d.md       |   10 +-\n .../functions_and_classes/shard0/tf.ifft3d.md      |    9 +-\n .../python/functions_and_classes/shard0/tf.mul.md  |    2 +-\n .../functions_and_classes/shard0/tf.nn.ctc_loss.md |   17 +-\n .../functions_and_classes/shard1/tf.Tensor.md      |    4 +-\n .../functions_and_classes/shard1/tf.batch_fft.md   |   18 -\n .../shard1/tf.nn.embedding_lookup_sparse.md        |    2 +-\n .../shard2/tf.contrib.layers.optimize_loss.md      |    3 +-\n ....contrib.layers.safe_embedding_lookup_sparse.md |   45 +\n .../shard3/tf.batch_ifft3d.md                      |   18 -\n .../functions_and_classes/shard3/tf.ifft2d.md      |    9 +-\n .../functions_and_classes/shard4/tf.batch_ifft.md  |   18 -\n .../python/functions_and_classes/shard5/tf.div.md  |    2 +-\n .../functions_and_classes/shard6/tf.batch_fft3d.md |   18 -\n .../shard6/tf.batch_ifft2d.md                      |   18 -\n .../functions_and_classes/shard6/tf.nn.elu.md      |    2 +-\n .../shard6/tf.self_adjoint_eig.md                  |    8 +-\n .../functions_and_classes/shard7/tf.fft3d.md       |   10 +-\n .../python/functions_and_classes/shard7/tf.ifft.md |    9 +-\n .../functions_and_classes/shard8/tf.Variable.md    |    4 +-\n .../functions_and_classes/shard9/tf.batch_fft2d.md |   18 -\n tensorflow/g3doc/api_docs/python/index.md          |    9 +-\n tensorflow/g3doc/api_docs/python/math_ops.md       |  129 +--\n tensorflow/g3doc/api_docs/python/nn.md             |   21 +-\n tensorflow/g3doc/api_docs/python/state_ops.md      |    4 +-\n tensorflow/python/BUILD                            | 1141 ++++++++++++++++----\n tensorflow/python/build_defs.bzl                   |   27 +\n tensorflow/python/debug/BUILD                      |    2 +\n tensorflow/python/framework/function.py            |  192 +++-\n tensorflow/python/framework/function_test.py       |  109 +-\n tensorflow/python/framework/importer.py            |   30 +-\n tensorflow/python/framework/importer_test.py       |   32 +\n tensorflow/python/kernel_tests/BUILD               |    2 +-\n tensorflow/python/kernel_tests/cwise_ops_test.py   |   10 +\n tensorflow/python/kernel_tests/fft_ops_test.py     |  149 +--\n tensorflow/python/kernel_tests/reader_ops_test.py  |  199 +++-\n .../kernel_tests/self_adjoint_eig_op_test.py       |   45 +\n tensorflow/python/kernel_tests/stack_ops_test.py   |   11 +-\n tensorflow/python/lib/io/file_io.py                |    7 +-\n tensorflow/python/lib/io/py_record_reader.cc       |    5 +\n tensorflow/python/lib/io/py_record_writer.cc       |    5 +\n tensorflow/python/lib/io/tf_record.py              |   21 +-\n tensorflow/python/ops/data_flow_ops.py             |  134 +--\n tensorflow/python/ops/embedding_ops.py             |    7 +-\n tensorflow/python/ops/image_ops_test.py            |   19 +\n tensorflow/python/ops/linalg_grad.py               |   44 +\n tensorflow/python/ops/linalg_ops.py                |    8 +-\n tensorflow/python/ops/math_grad.py                 |   60 +-\n tensorflow/python/ops/math_ops.py                  |   13 +-\n tensorflow/python/training/adam.py                 |    3 +-\n tensorflow/python/training/adam_test.py            |   19 +\n tensorflow/tensorboard/README.md                   |    9 -\n .../components/vz-projector/scatterWebGL.ts        |   84 +-\n .../vz-projector/scatterWebGLPointsCanvasLabels.ts |  194 ++--\n tensorflow/tensorflow.bzl                          |    7 +-\n tensorflow/tools/docker/Dockerfile.devel           |    2 +-\n tensorflow/tools/docker/Dockerfile.devel-gpu       |    2 +-\n .../bin/crosstool_wrapper_driver_is_not_gcc.tpl    |    3 +\n 175 files changed, 6287 insertions(+), 2114 deletions(-)\n delete mode 100644 tensorflow/contrib/framework/python/ops/embedding_ops.py\n create mode 100644 tensorflow/contrib/learn/python/learn/estimators/dnn_sampled_softmax_classifier.py\n create mode 100644 tensorflow/contrib/learn/python/learn/estimators/dnn_sampled_softmax_classifier_test.py\n create mode 100644 tensorflow/contrib/makefile/Dockerfile\n create mode 100755 tensorflow/contrib/makefile/build_with_docker.sh\n create mode 100644 tensorflow/contrib/rnn/ops/gru_ops_test.cc\n create mode 100644 tensorflow/contrib/rnn/ops/lstm_ops_test.cc\n create mode 100644 tensorflow/core/debug/debug_service.proto\n create mode 100644 tensorflow/core/distributed_runtime/scheduler.cc\n create mode 100644 tensorflow/core/distributed_runtime/scheduler.h\n create mode 100644 tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.contrib.framework.deprecated_args.md\n delete mode 100644 tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.contrib.framework.safe_embedding_lookup_sparse.md\n delete mode 100644 tensorflow/g3doc/api_docs/python/functions_and_classes/shard1/tf.batch_fft.md\n create mode 100644 tensorflow/g3doc/api_docs/python/functions_and_classes/shard2/tf.contrib.layers.safe_embedding_lookup_sparse.md\n delete mode 100644 tensorflow/g3doc/api_docs/python/functions_and_classes/shard3/tf.batch_ifft3d.md\n delete mode 100644 tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.batch_ifft.md\n delete mode 100644 tensorflow/g3doc/api_docs/python/functions_and_classes/shard6/tf.batch_fft3d.md\n delete mode 100644 tensorflow/g3doc/api_docs/python/functions_and_classes/shard6/tf.batch_ifft2d.md\n delete mode 100644 tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.batch_fft2d.md\n create mode 100644 tensorflow/python/build_defs.bzl\n\nsyj@syj-dl:~/tensorflow$ vi  tensorflow/contrib/session_bundle/BUILD \nsyj@syj-dl:~/tensorflow$ vi tensorflow/core/BUILD\nsyj@syj-dl:~/tensorflow$ vi  tensorflow/core/kernels/BUILD`\n\nERROR: /home/syj/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_autogrow_textarea//': Error downloading from https://github.com/polymerelements/iron-autogrow-textarea/archive/v1.0.12.tar.gz to /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/iron_autogrow_textarea: Error downloading https://github.com/polymerelements/iron-autogrow-textarea/archive/v1.0.12.tar.gz to /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/iron_autogrow_textarea/v1.0.12.tar.gz: Connection timed out and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: /home/syj/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_autogrow_textarea//': Error downloading from https://github.com/polymerelements/iron-autogrow-textarea/archive/v1.0.12.tar.gz to /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/iron_autogrow_textarea: Error downloading https://github.com/polymerelements/iron-autogrow-textarea/archive/v1.0.12.tar.gz to /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/iron_autogrow_textarea/v1.0.12.tar.gz: Connection timed out and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n'.\n\n`BUT,Using wget I can download successfully.\n\nsyj@syj-dl:~$ wget https://github.com/polymerelements/iron-autogrow-textarea/archive/v1.0.12.tar.gz\nwget: /usr/local/lib/libcrypto.so.1.0.0: no version information available (required by wget)\nwget: /usr/local/lib/libssl.so.1.0.0: no version information available (required by wget)\nwget: /usr/local/lib/libssl.so.1.0.0: no version information available (required by wget)\n--2016-09-16 23:53:20--  https://github.com/polymerelements/iron-autogrow-textarea/archive/v1.0.12.tar.gz\n\u6b63\u5728\u89e3\u6790\u4e3b\u673a github.com (github.com)... 192.30.253.112\n\u6b63\u5728\u8fde\u63a5 github.com (github.com)|192.30.253.112|:443... \u5df2\u8fde\u63a5\u3002\n\u5df2\u53d1\u51fa HTTP \u8bf7\u6c42\uff0c\u6b63\u5728\u7b49\u5f85\u56de\u5e94... 302 Found\n\u4f4d\u7f6e\uff1ahttps://codeload.github.com/PolymerElements/iron-autogrow-textarea/tar.gz/v1.0.12 [\u8ddf\u968f\u81f3\u65b0\u7684 URL]\n--2016-09-16 23:53:21--  https://codeload.github.com/PolymerElements/iron-autogrow-textarea/tar.gz/v1.0.12\n\u6b63\u5728\u89e3\u6790\u4e3b\u673a codeload.github.com (codeload.github.com)... 192.30.253.121\n\u6b63\u5728\u8fde\u63a5 codeload.github.com (codeload.github.com)|192.30.253.121|:443... \u5df2\u8fde\u63a5\u3002\n\u5df2\u53d1\u51fa HTTP \u8bf7\u6c42\uff0c\u6b63\u5728\u7b49\u5f85\u56de\u5e94... 200 OK\n\u957f\u5ea6\uff1a 8743 (8.5K) [application/x-gzip]\n\u6b63\u5728\u4fdd\u5b58\u81f3: \u201cv1.0.12.tar.gz\u201d\n\nv1.0.12.tar.gz      100%[===================>]   8.54K  --.-KB/s    in 0.001s  \n\n2016-09-16 23:53:23 (12.8 MB/s) - \u5df2\u4fdd\u5b58 \u201cv1.0.12.tar.gz\u201d [8743/8743]) `\n", "@jart could you please take a look at this issue?\n", "`syj@syj-dl:~$mv tensorflow  tensorflow.bak0917\n\nsyj@syj-dl:~$ git clone https://github.com/tensorflow/tensorflow\n\u6b63\u514b\u9686\u5230 'tensorflow'...\nremote: Counting objects: 87905, done.\nremote: Compressing objects: 100% (685/685), done.\nremote: Total 87905 (delta 384), reused 0 (delta 0), pack-reused 87215\n\u63a5\u6536\u5bf9\u8c61\u4e2d: 100% (87905/87905), 57.97 MiB | 1.28 MiB/s, \u5b8c\u6210.\n\u5904\u7406 delta \u4e2d: 100% (65026/65026), \u5b8c\u6210.\n\u68c0\u67e5\u8fde\u63a5... \u5b8c\u6210\u3002\n\nsyj@syj-dl:~$ cd tensorflow\nsyj@syj-dl:~/tensorflow$ git pull --recurse-submodules\nAlready up-to-date.\n\nsyj@syj-dl:~/tensorflow$ vi tensorflow/contrib/session_bundle/BUILD\nsyj@syj-dl:~/tensorflow$ vi  tensorflow/core/BUILD\nsyj@syj-dl:~/tensorflow$ vi tensorflow/core/kernels/BUILD\n\nsyj@syj-dl:~/tensorflow$ ./configure\n~/tensorflow ~/tensorflow\nPlease specify the location of python. [Default is /home/syj/anaconda2/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\nGoogle Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /home/syj/anaconda2/lib/python2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/home/syj/anaconda2/lib/python2.7/site-packages]\n/home/syj/anaconda2/lib/python2.7/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n`\n", "Thank you all @andrewharp,  @gustavla,  @aselle, @shkr , with your kind help  after over 10 days attempt  the configuration passed.  :)\n", "Thank you @aselle , @jart \n", "`syj@syj-dl:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\nINFO: Elapsed time: 789.711s, Critical Path: 749.46s\n\nsyj@syj-dl:~/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n2016\u5e74 09\u6708 17\u65e5 \u661f\u671f\u516d 11:12:39 CST : === Using tmpdir: /tmp/tmp.DoXXfMKIU3\n/tmp/tmp.DoXXfMKIU3 ~/tensorflow\n2016\u5e74 09\u6708 17\u65e5 \u661f\u671f\u516d 11:12:39 CST : === Building wheel\n~/tensorflow\n2016\u5e74 09\u6708 17\u65e5 \u661f\u671f\u516d 11:12:51 CST : === Output wheel file is in: /tmp/tensorflow_pkg\n\nsyj@syj-dl:~/tensorflow$ pip install /tmp/tensorflow_pkg/tensorflow-0.10.0-py2-none-any.whl\nProcessing /tmp/tensorflow_pkg/tensorflow-0.10.0-py2-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /home/syj/anaconda2/lib/python2.7/site-packages (from tensorflow==0.10.0)\nCollecting mock>=2.0.0 (from tensorflow==0.10.0)\n  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 15kB/s \nCollecting protobuf==3.0.0b2 (from tensorflow==0.10.0)\n  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 10kB/s \nRequirement already satisfied (use --upgrade to upgrade): wheel in /home/syj/anaconda2/lib/python2.7/site-packages (from tensorflow==0.10.0)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /home/syj/anaconda2/lib/python2.7/site-packages (from tensorflow==0.10.0)\nRequirement already satisfied (use --upgrade to upgrade): funcsigs>=1; python_version < \"3.3\" in /home/syj/anaconda2/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==0.10.0)\nCollecting pbr>=0.11 (from mock>=2.0.0->tensorflow==0.10.0)\n  Downloading pbr-1.10.0-py2.py3-none-any.whl (96kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 12kB/s \nRequirement already satisfied (use --upgrade to upgrade): setuptools in /home/syj/anaconda2/lib/python2.7/site-packages/setuptools-23.0.0-py2.7.egg (from protobuf==3.0.0b2->tensorflow==0.10.0)\nInstalling collected packages: pbr, mock, protobuf, tensorflow\n  Found existing installation: protobuf 3.0.0\n    Uninstalling protobuf-3.0.0:\n      Successfully uninstalled protobuf-3.0.0\nSuccessfully installed mock-2.0.0 pbr-1.10.0 protobuf-3.0.0b2 tensorflow-0.10.0\n\nsyj@syj-dl:~/tensorflow/tensorflow/models/image/mnist$ python convolutional.py\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.8225\npciBusID 0000:01:00.0\nTotal memory: 7.92GiB\nFree memory: 7.53GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nInitialized!\nStep 0 (epoch 0.00), 7.2 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\n\n......................................\nStep 8500 (epoch 9.89), 5.1 ms\nMinibatch loss: 1.619, learning rate: 0.006302\nMinibatch error: 1.6%\nValidation error: 0.8%\nTest error: 0.8%\n\n`\n", "Unless using Bazel 0.3.1 and applying [a temporary fix](https://github.com/okuchaiev/tensorflow-float16/commit/c1b6623154e14799419f91bd197b7900a78dbc2b), running `configure` still produces errors.\n\n```\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n...\nERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core'\n defined by /home/ubuntu/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core'\n defined by /home/ubuntu/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such package 'base': BUILD file not found on package path and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:2211:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in packa\nge 'tensorflow/core' defined by /home/ubuntu/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n```\n", "@szsongyj \ngit pull --recurse-submodules don't work   my system is CentOS 6.7 \n", "I am getting the same error.  I checked bazel version and its 3.1:\n`\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n`\nI am on commit 56a8e1.  CUDA 8.0 with CUDNN 5.0 with Nvidia GTX 1080 (driver version 367.44), and Ubuntu 16.04.1 (only combination that works with GTX 1080 for me). \n\nThis should address the problem: https://github.com/tensorflow/tensorflow/commit/ea0771512af531c2a3fa1f60d3e6dd2b47384e99\n\nBut at this moment, installation attempts to download the file below but the link is broken: \nhttp://www.ijg.org/files/jpegsrc.v9a.tar.gz\nI'll remove if the link outage is temporary or file under new issue. ?\n", "Missing RTTI target issue fixed in ea0771512af531c2a3fa1f60d3e6dd2b47384e99\n\n@peymanr If you're still experiencing the jpeg problem please file a new issue.\n", "just add  \n$ git reset --hard 70de76e \nbefore you configure\nhttp://www.nvidia.com/object/gpu-accelerated-applications-tensorflow-installation.html\n", "@shkabko Thank you! It works for me. But why the latest version can't get build?\n", "I just worked it out by updating the bazel to 0.4.1.", "Fixed with an upgrade of bazel\r\n```\r\n$ dpkg -l | grep bazel\r\nii  bazel            0.4.5             amd64        Bazel is a tool that automates software builds and tests.\r\n```", "Solved by installing a fresh, updated version of Bazel. Download the right version for your/your docker's OS (To check docker OS: `uname -a`) from here https://github.com/bazelbuild/bazel/releases. Then: \r\n```\r\nchmod + x bazel-0.13.0-installer-linux-x86_64.sh\r\n./bazel-0.13.0-installer-linux-x86_64.sh\r\n```\r\nNow, build should work just fine."]}, {"number": 4318, "title": "Increase the maximum number of images per class", "body": "In the transfer learning example the maximum number of images per class is limited to 65535. There is not even a warning if the number of images exceeds it.\n\nThis PR increases it to 2 ^ 27 - 1 and adds a warning message.\n", "comments": ["Can one of the admins verify this patch?\n", "@danieljl, thanks for your PR! By analyzing the annotation information on this pull request, we identified @petewarden, @ebrevdo and @tensorflower-gardener to be potential reviewers\n", "Thanks @danieljl, this looks like a great fix. I've reviewed it, and it makes sense to me.\n", "Jenkins, test this please.\n", "Jenkins, test this please. This looks good to me.\n"]}, {"number": 4317, "title": "unable to install tensorflow ", "body": "i am using ubuntu 12.04. i was following instructions from tensorflow.org for installing the tensor flow but getting the following error after downloading the the tensor flow-\nownloading tensorflow-0.10.0-cp27-none-linux_x86_64.whl (36.6Mb): 36.6Mb downloaded\n  Running setup.py egg_info for package from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n    Traceback (most recent call last):\n      File \"<string>\", line 14, in <module>\n    IOError: [Errno 2] No such file or directory: '/tmp/pip-uIJFE4-build/setup.py'\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n\n  File \"<string>\", line 14, in <module>\n\nIOError: [Errno 2] No such file or directory: '/tmp/pip-uIJFE4-build/setup.py'\n\n---\n\nCommand python setup.py egg_info failed with error code 1\nStoring complete log in /home/dhanpal/.pip/pip.log\n", "comments": ["I don't know if TensorFlow works with Ubuntu 12. Anybody have experience with that?\n\nWhat is your version of setuptools and pip?\n", "#56 looks related.", "In fact, looks like a duplicate. Thanks @ppwwyyxx! Closing. \r\n\r\n@hunk3749, try upgrading pip.", "I tried each and every upgrading packages but didn't help.\r\nI tried \r\n```\r\npip install --upgrade pip\r\npip install --upgrade setuptools\r\npip install unroll\r\neasy_install -U setuptools\r\npip install ez_setup\r\n```\r\nAny updates on that so far?\r\n\r\n"]}, {"number": 4316, "title": "WMT en-fr training corpus filename change", "body": "`giga-fren.release2.fr.gz` file has been renamed to `giga-fren.release2.fixed.fr.gz`\n", "comments": ["@gokceneraslan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @keveman and @ilblackdragon to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Ah, reported already. Closing.\n"]}, {"number": 4315, "title": "Pass through -fno-canonical-system-headers", "body": "This fixes the critical build issue #3743. After I figured out what was causing the issue, I couldn't really get a discussion of possible solutions going. I figured I would just go ahead and create a PR with the simplest and most immediate fix to get the ball rolling.\n\nThe fix is simple, gcc needs to be passed `-fno-canonical-system-headers`, and this PR makes it recognize this flag if it's passed to `crosstool_wrapper_driver_is_not_gcc` and pass it along to the compiler command.\n\nHowever, one could argue that there are more general and better solutions, such as passing through all `-f...` flags (there are actually currently many flags that are passed into the Python script that are not passed along). If anyone feels strongly about such an alternative solution, I would like to discuss the details of it before putting together a replacement PR.\n\nI might need to rebase this build before it can be merged. I cloned an older commit since the master head is broken for me (#4312).\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for breaking out the PR!\n\n@davidzchen @damienmg what's the \"proper\" thing to do here? Should all -f options be passed through? All options? Is there a reason not to pass through certain options?\n", "If `crosstool_wrapper_driver_is_not_gcc` is meant to provide a command line interface that is more compatible with gcc, my intuition is that all -f options should be passed through.\n\n+@lberki for his thoughts on this.\n", "I would pull people with good C++ knowledge for a general answer that but for this specific flag it is correct to pass it through.\n\nSome option might not be supported by nvcc and you might not want to pass them through.\n", "Jenkins, test this please\n", "@gustavla can you rebase? \n", "@martinwicke Done.\n", "Jenkins, test this please\n"]}, {"number": 4314, "title": "Add float16 support for Elu, EluGrad ops", "body": "Support for `tf.float16` dtype was recently added (#1300) to a bunch of ops, starting with matmul, conv2D and then many more. Can we add it for elu also please? \n\nLooking at `tensorflow/nn_ops.cc`, I see that the `Elu` and `EluGrad` ops are registered for types `{float, double}` whereas for a whole bunch of other activation functions I see the type as `T: realnumbertype` or `{half, float, double}`.\n\nTo help with triaging, here's the paper that motivates elu activation: [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](http://arxiv.org/abs/1511.07289)\n\nAnd FWIW @alexatknit in #1300 commented Re: looking forward to float16 support for elu too.\n", "comments": ["A fix is on its way. I'll update this issue once the fix is merged.\n", "My fix has been committed as https://github.com/tensorflow/tensorflow/commit/d92ba3066e79258a3096ee72c105b4816f196425, closing this issue.\n"]}, {"number": 4313, "title": "changed file name in WMT translation demo #4122", "body": "Will resolve No such file or directory error.\n", "comments": ["@andrewt3000, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @keveman and @ilblackdragon to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it.\n\nOn Fri, Sep 9, 2016 at 10:46 PM, googlebot notifications@github.com wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4313#issuecomment-246086386,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAzMVb00xB_ZlBK2gO4hSHmXk4TtK_17ks5qoigcgaJpZM4J5oCZ\n> .\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins Test this please.\n"]}, {"number": 4312, "title": "Can't configure due to undeclared packages inside if_android/if_ios", "body": "I am having trouble configuring the latest master branch (dbe7ee0dfa9e5ab26284522379f2747510fc267b). When I run `./configure`, I get:\n\n```\nERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\nERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.\nERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\nERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.\nERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.\nERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\nERROR: [...]/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by [...]/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\nConfiguration finished\n```\n\nTo summarize, the dependencies that are included inside the `if_android` and `if_ios` are not found. They don't exist in the repository, so that is not that surprising. What is more surprising though is that my vanilla installation is not returning empty lists when `if_android` is called. I haven't looked into how those functions work, so not sure why that is happening.\n### Environment info\n\nSetup: CentOS, Bazel 0.3.1, CUDA 7.5, CuDNN 5.1, Tensorflow master (dbe7ee0dfa9e5ab26284522379f2747510fc267b)\n\nI run `configure` and set it up for GPU support. Actually, I don't think this is criticial, but first I had to open up `configure` and add `--output_base=...` on the two calls to `bazel`, since my setup requires a custom cache directory.\n### Fix\n\nThe `if_...` lines were added in ed87884e50e1a50f7dc7b36dc7a7ff225442bee0, so a fix that I know works is to use its parent commit 7705791619f5e851687e9a63b4315087e189f8be.\n", "comments": ["I can confirm this issue. In my environment `configure` fails on the latest commit (7705791619f5e851687e9a63b4315087e189f8be) with the following errors:\n\n```\nERROR: /home/sarroff/repo/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such target '//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime': target 'android_tensorflow_lib_lite_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/sarroff/repo/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\nERROR: /home/sarroff/repo/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by /home/sarroff/repo/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n```\n\nAs suggested by @gustavla, I checked out https://github.com/tensorflow/tensorflow/commit/ed87884e50e1a50f7dc7b36dc7a7ff225442bee0 and `configure` ran without error.\n\n**Setup**: Ubuntu 16.04, Bazel 0.3.1, CUDA 8.0rc, CuDNN 5.1.5, gcc 5.4\n", "You can fix this by commenting out the `if_android` and `if_ios` calls in `contrib/session_bundle/BUILD` and the `android_tensorflow_lib_lite_no_rtti_lite_runtime` target in the core `BUILD` file.\nThese seem to be based on Google internal targets.\n\n`if_android` doesn't return an empty list directly, but gives you a `select` object, which is resolved to an empty list by bazel.\n", "Confirmed that if I make these changes to the following three files `configure` runs without error:\n- Remove all `if_mobile` and `if_android` conditions from `tensorflow/contrib/session_bundle/BUILD`\n- Remove the `ios_tensorflow_test_lib` target from `tensorflow/core/BUILD`\n- Remove the `android_tensorflow_kernels_no_rtti_lite_runtime` target from `tensorflow/core/kernels/BUILD`\n", "> Remove all if_mobile and if_android conditions from tensorflow/contrib/session_bundle/BUILD\n\nDo you mean that I comment out everything related to mobile devices,  not just if_mobile and if_android? Or are these 2 components sufficient?\n\n```\n#    \"if_android\",\n#   \"if_ios\",\n#    \"if_mobile\",\n#   \"if_not_mobile\",\n```\n", "You don't need to remove them from the call to `load` at the top of the file, but instead each time they are used, like this:\n\n``` python\ncc_library(\n    name = \"session_bundle\",\n    srcs = [\"session_bundle.cc\"],\n    hdrs = [\"session_bundle.h\"],\n    copts = if_ios([\"-DGOOGLE_LOGGING\"]),\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":signature\",\n    ] + if_not_mobile([\n        \":manifest_proto_cc\",\n        \"//tensorflow/core:core_cpu\",\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:lib\",\n        \"//tensorflow/core:protos_all_cc\",\n    ]) #+ if_mobile([\n    #    \":manifest_portable_proto\",\n    #    \"//tensorflow/core:meta_graph_portable_proto\",\n    #]) + if_android([\n    #    \"//tensorflow/core:android_lib_lite\",\n    #]) + if_ios([\n    #    \"//tensorflow/core:ios_tensorflow_lib\",\n    #]),\n)\n```\n", "Having this issue as well\n", "I had the same issue too. Following @ibab @woodshop suggestions solved this problem.\nI think `configure` need an option to decide target environment.\n", "I fixed the issue according to @ibab and @woodshop advice as well. \n\nHowever, I also commented out `copts = if_ios([\"-DGOOGLE_LOGGING\"]),` and any if_ios conditions, which was seen invalid when I was configuring. Thank you!\n", "if you download tensorflow from github instead of 'git clone', you will not meet this.\n", "Just to clarify @Eidosper's comment: the suggestion is to use an official TF release (e.g. tagged v0.10.0rc0). This is not a solution for anyone that needs the current state of TF development (i.e. the newest commit of the master branch).\n", "@petewarden This seems to be a sync issue maybe? Do the `is_*` macros only work on the inside and should be rewritten? \n\nThe rules work fine in our CI, which makes it a little more confusing. What's used to determine `if_mobile` etc.?\n", "This is surprising because these rules have been in for several months without causing this issue, so something must have changed recently. The is_\\* macros rely on config_settings in the top-level BUILD file that test against cross-tool usage, so I wouldn't expect them to return non-empty results.\n\nThe quickest way to debug this is probably to reproduce it at head, and then do a binary search on github checkins to narrow it down to a particular commit. This shouldn't be too hard since it happens very quickly, but I'm traveling for the next couple of days and won't be able to work on it until I'm done.\n", "@petewarden: I've narrowed the problem down to commit https://github.com/tensorflow/tensorflow/commit/ed87884e50e1a50f7dc7b36dc7a7ff225442bee0 using `git bisect`.\nAn easy way to reproduce it is to run `bazel fetch //tensorflow/contrib/session_bundle/...`.\n", "Thanks for doing that @ibab! I've notified the author of that change, hopefully we should be able to figure out a fix.\n", "@ibab I hope it didn't take you long, because I already did this and reported ed87884e50e1a50f7dc7b36dc7a7ff225442bee0 as the offending commit in my original post :smile:\n\nNow, the question is, did that commit trigger a latent bug that was introduced at some other point in time or has it always been there. This is what I thought @petewarden was implying by saying they had been around for \"several months\", referring to the is_\\* macros. This would involve running a git bisect and in each step applying ed87884e50e1a50f7dc7b36dc7a7ff225442bee0 as a patch.\n\nI was thinking it might be a bazel version thing, but I tried both 0.3.1 and 0.3.0 (after a quick fix: #4343), and I have the same problem.\n", "@msevrens https://hackathonprojects.wordpress.com/2016/09/13/installing-10-series-gpus-cuda-8-0rc-and-tensorflow-on-ubuntu-14-04/ here is a post describing the comments if you're still having trouble\n", "@tmulc18 you should fix that link :)\n", "This seems to have auto-closed when I merged f66b491, but it does not seem to be completely fixed yet, does it?\n", "No, it doesn't seem entirely solved. The configuration fails in my environment due to the following two targets:\n- The `ios_tensorflow_test_lib` target from `tensorflow/core/BUILD`\n- The `android_tensorflow_kernels_no_rtti_lite_runtime` target from `tensorflow/core/kernels/BUILD`\n\nHowever I am now able to build the project _despite_ configuration errors, whereas I was not able to do so before commit f66b491.\n\nCommenting out the targets enables the configuration to complete without errors.\n", "I fresh cloned tensorflow for latest master commit, and I confirm I'm experiencing the same issues as @woodshop, so not fixed yet.\nNote that I didn't have those issues configuring a few days ago.\n\nHowever, you can actually build tensorflow despite configuration errors now because f66b491 removes the `if_mobile`, `if_android`, and `if_ios` conditions(which had to be removed manually beforehand).\n\nGoing ahead and removing the next 2 targets enables the configuration to complete without errors for me, using CUDA 8.\n\nI'll build the example trainer with bazel along GPU support and test if it works now.\n", "meet same issue.\n", "Just got told my #4476 is a duplicate of this ticket, so coming here with: Is there anything I can do to get this fixed up? It seems to be hitting a number of external contributors and it's been 10 days since it was first reported. A time suck for all of us to be rediscovering it and working out the same patches!\n", "If you remove the reference to //base and the references to the lite_no_rtti_lite_runtime and android_tensorflow_lib_lite_no_rtti_lite_runtime targets, does everything work?  If so I'll prepare a change to strip these out.\n\nI don't know why our CI build isn't catching these obvious errors.\n", "Yeah, if you delete all mentions of \"//base\", \"//tensorflow/core:android_proto_lib_no_rtti_lite_runtime\", and \"//tensorflow/core:android_tensorflow_lib_lite_no_rtti_lite_runtime\" it seems to build okay.\n\nI only get these errors when I build with CUDA. Could that be the deal? (I'm also on OS X.)\n", "@vrv I'm halfway done with a CL to fix the export right now\n", "@andrewharp woohoo, thanks! send to me :)\n\n@jmhodges thanks for confirming.  We also have CUDA builds, so perhaps we're only building what's necessary for tests / pip installation, rather than the entire repo (e.g., we don't try to build  tensorflow/..., though maybe we should.  At least we should validate the BUILD files, which I thought we used to do).  cc @gunan \n", "Hunh, weird. It happens as soon as I run `./configure` on a fresh build!\n", "we do not presubmit with cuda+mac. We only have nightlies.\nThose builds are broken at the moment, but at first glance, it looks like a different issue (the nightly failures and the reports here)\n", "ea0771512af531c2a3fa1f60d3e6dd2b47384e99 is commited, which should fix the remaining configure issues regarding `//base` and `android_tensorflow_lib_lite_no_rtti_lite_runtime`.\n"]}]