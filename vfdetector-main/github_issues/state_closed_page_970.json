[{"number": 24322, "title": "java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: Samsung S5\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 1.12\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\nI have created my own architecture, on tensorflow and I have used Toco to convert to .tflite, However the android app still throws the following error: \"java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model.\"\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n'''\r\n       AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_PATH);\r\n        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n        FileChannel fileChannel = inputStream.getChannel();\r\n        long startOffset = fileDescriptor.getStartOffset();\r\n        long declaredLength = fileDescriptor.getDeclaredLength();\r\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n'''\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Apologies for the delay in response. Is this still an issue for you?\r\nCan you please take a look at duplicate #23628? Thanks!", "Hi there, I'm trying to convert a Tensorflow 1.12 / Keras model with TFLiteConverter.from_keras_model_file. I've got no issue doing this on Colaboratory (tf 1.12), but when I run my model I encounter the dreaded error : \r\n\"Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\" error. with firebase-ml-model-interpreter:16.2.4\r\n\r\nThe model structure is the following : \r\n\r\nInput(884)\r\nBatchNormalization()\r\nReshape(52,17)\r\nLSTM(256, unroll=True, return_sequences=True)\r\nLSTM(256, unroll=True)\r\nBatchNormalization()\r\nDense(260)\r\n\r\nI previously successfully converted and used similar models, but without Reshape and (unrolled) LSTM layers.\r\n\r\n(EDIT) For information, doing the Batchnormalization layer after the Reshape prevent the conversion from happening.\r\n\r\nAny advice on version or changes I should make ?\r\n\r\nThank you very much in advance.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n@NicolasVidal Can you please post a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose) explaining your problem if it still persists? thanks", "I'm having the same issue, converted the model with from_keras_model_file, tried to load it in Android but gives me this error:\r\n\r\n`java.lang.IllegalArgumentException: Contents of /conv.tflite does not encode a valid TensorFlowLite model: Could not open '/conv.tflite'.The model is not a valid Flatbuffer file`"]}, {"number": 24321, "title": "Nightly development containers still come with `1.12.0-rc0` pre-installed.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 18.04`\r\n- TensorFlow installed from (source or binary): binary inside docker\r\n- TensorFlow version: `1.12.0`\r\n- Python version: `both 2.7 and 3.6` within development containers\r\n- Installed using virtualenv? pip? conda?: `pip`\r\n- Bazel version (if compiling from source): `0.15.0`\r\n- GCC/Compiler version (if compiling from source): `7.3.0`\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nAfter downloading nightly development `cpu` images for both Python2 and Python 3, I realized the version for per-install TensorFlow still reads `1.12.0-rc0`.\r\n\r\nHere is the console logs for Python3 for example:\r\n```\r\n$ docker run -it tensorflow/tensorflow:nightly-devel-py3 bash\r\nUnable to find image 'tensorflow/tensorflow:nightly-devel-py3' locally\r\nnightly-devel-py3: Pulling from tensorflow/tensorflow\r\n32802c0cfa4d: Already exists \r\nda1315cffa03: Already exists \r\nfa83472a3562: Already exists \r\nf85999a86bef: Already exists \r\n0f910117152a: Pull complete \r\na65c5ef5ec56: Pull complete \r\ncf4bb4e1826b: Pull complete \r\n66ef88223fb9: Pull complete \r\n8aeb73a82dfe: Pull complete \r\nbc8f7b434f2b: Pull complete \r\nd265fbe2714c: Pull complete \r\nddec192687d1: Pull complete \r\n78df3767776d: Pull complete \r\nb96b75a73e63: Pull complete \r\na93b1cef267b: Pull complete \r\nafc439a6b9ad: Pull complete \r\nDigest: sha256:3f9e30100331fb4199aced4501c94d2718b2e20678fee4c1d398a756420ecab7\r\nStatus: Downloaded newer image for tensorflow/tensorflow:nightly-devel-py3\r\nroot@4f201309883c:~# ipython\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf                                                                                                                                                                                                                                                            \r\ntf\r\nIn [2]: tf.VERSION                                                                                                                                                                                                                                                                         \r\nOut[2]: '1.12.0-rc0'\r\n```\r\n\r\nLooks like user still needs to manually install the nightly TensorFlow wheel inside the container to get the latest and greatest:\r\n```\r\n$ pip install -U tf-nightly\r\nCollecting tf-nightly\r\n  Downloading https://files.pythonhosted.org/packages/59/51/821e5c26631142d97e0ea832eff848e5d1e88db5346692584768ff8afc69/tf_nightly-1.13.0.dev20181212-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\r\n    100% |################################| 92.5MB 519kB/s \r\nRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.16.1)\r\nCollecting tf-estimator-nightly (from tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/02/71/0d1c7adae69ac5a2c77e5ba0c3ac3fd2de5ce2aeb1467aabfe8701a723ea/tf_estimator_nightly-1.12.0.dev20181208-py2.py3-none-any.whl (300kB)\r\n    100% |################################| 307kB 8.7MB/s \r\nRequirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.7.1)\r\nRequirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.11.0)\r\nRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.32.3)\r\nRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.6.1)\r\nCollecting tb-nightly<1.14.0a0,>=1.13.0a0 (from tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/80/55/164d6a25d03cecfafa5c88a015564a56b5ee7e951e210fb66e17d935e104/tb_nightly-1.13.0a20181126-py3-none-any.whl (3.2MB)\r\n    100% |################################| 3.2MB 9.1MB/s \r\nRequirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.6.1)\r\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.4)\r\nRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\r\nRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.0.6)\r\nRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.0.5)\r\nRequirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\r\nRequirement already satisfied, skipping upgrade: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tf-estimator-nightly->tf-nightly) (2.0.0)\r\nRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly) (40.6.2)\r\nRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly) (3.0.1)\r\nRequirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly) (0.14.1)\r\nRequirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly) (2.8.0)\r\nRequirement already satisfied, skipping upgrade: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tf-estimator-nightly->tf-nightly) (5.1.1)\r\nInstalling collected packages: tf-estimator-nightly, tb-nightly, tf-nightly\r\nSuccessfully installed tb-nightly-1.13.0a20181126 tf-estimator-nightly-1.12.0.dev20181208 tf-nightly-1.13.0.dev20181212\r\nroot@4f201309883c:~# ipython\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf                                                                                                                                                                                                                                                            \r\n\r\nIn [2]: tf.VERSION                                                                                                                                                                                                                                                                         \r\nOut[2]: '1.13.0-dev20181212'\r\n```\r\n", "comments": ["Our Docker images changed recently. For working on TensorFlow, you'd want a `devel-` image, which has no TensorFlow installed (it has Bazel and the source code instead). For working with the TF Nightly images, it's just `nightly-` (there is no `nightly-devel`). We're leaving the old tags in place so as to not break anyone. You can see the full list of tags on [Docker Hub](https://hub.docker.com/r/tensorflow/tensorflow)."]}, {"number": 24320, "title": "BatchNormalization produces NaN weights without NaN loss", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n  - Custom code.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  - Ubuntu 17.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n  - N/A\r\n- TensorFlow installed from (source or binary):\r\n  - pip package `tensorflow-gpu`\r\n- TensorFlow version (use command below):\r\n  - v1.8.0-0-g93bc2e2072 1.8.0\r\n- Python version:\r\n  - 3.6.3 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n  - V9.0.176\r\n- GPU model and memory:\r\n  - GeForce GTX 1060 6GB\r\n\r\n**Describe the current behavior**\r\n\r\nCopying from [keras issue 11858](https://github.com/keras-team/keras/issues/11858), as requested by @ymodak.\r\n\r\nHi,\r\n\r\nNot sure this can be labelled as a bug, but it's problematic. BatchNormalization seems to silently produce NaN weights when training a `multi_gpu_model` if the training dataset size is not a multiple of `batch_size`.\r\n\r\nFor example, with a training dataset (1825, 401, 401, 3), validation dataset (140, 401, 401, 3), `epochs=1`, `batch_size=16`, `gpu_number=2`\r\n\r\n```python\r\n    # instantiate model\r\n    with tf.device('/cpu:0'):\r\n        # DenseNet121: blocks=[6, 12, 24, 16]\r\n        base_model = densenet.DenseNet121(include_top=False, weights=None,\r\n                                       input_shape=(401, 401, 3), pooling='avg')\r\n        x = Dense(units=1, activation='sigmoid', name='fc1')(base_model.output)\r\n        model = Model(inputs=base_model.input, outputs=x)\r\n\r\n    # compile model\r\n    parallel_model = multi_gpu_model(model, gpus=gpu_number)\r\n    parallel_model.compile(loss={'fc1': 'binary_crossentropy'},\r\n                           optimizer='Adadelta',\r\n                           metrics={'fc1': ['acc']})\r\n\r\n    # train model\r\n    tic = datetime.datetime.now()\r\n    parallel_model.fit(train_onecell_im,\r\n                       {'fc1': (train_onecell_dice >= quality_threshold).astype(np.float32)},\r\n                       validation_data=(test_onecell_im,\r\n                                        {'fc1': (test_onecell_dice >= quality_threshold).astype(np.float32)}),\r\n                       batch_size=batch_size, epochs=epochs, initial_epoch=0)\r\n    toc = datetime.datetime.now()\r\n    print('Training duration: ' + str(toc - tic))\r\n```\r\n\r\nThe training apparently goes fine\r\n\r\n```\r\nTrain on 1825 samples, validate on 140 samples\r\nEpoch 1/1\r\n1825/1825 [==============================] - 108s 59ms/step - loss: 0.6604 - acc: 0.6323 - val_loss: 0.6932 - val_acc: 0.4643\r\nTraining duration: 0:02:08.115762\r\n\r\n```\r\n\r\nbut the weights have NaNs, e.g.\r\n\r\n```\r\nmodel.get_layer('conv1/bn').get_weights()\r\n[array([1.0001292 , 1.        , 0.9996672 , 0.9999442 , 1.000509  ,\r\n       1.0001016 , 1.0002009 , 1.0004678 , 0.9999988 , 0.999962  ,\r\n       1.0003603 , 1.0001667 , 0.9999296 , 0.9999381 , 1.00001   ,\r\n       0.99967813, 0.9999821 , 0.99981546, 0.9999899 , 1.0002408 ,\r\n       0.9999446 , 0.9999995 , 0.99989605, 1.0000395 , 1.0000094 ,\r\n       0.9999432 , 0.999968  , 0.99994946, 0.9997129 , 1.0000957 ,\r\n       0.99997395, 1.000016  , 0.99995   , 0.99981534, 0.99984217,\r\n       0.9999743 , 0.99999624, 1.0005921 , 1.0001019 , 1.000008  ,\r\n       0.99993116, 0.99998087, 0.9999631 , 0.9999878 , 0.9999804 ,\r\n       1.0003394 , 0.999895  , 0.9997747 , 0.9999677 , 0.99998355,\r\n       1.000003  , 0.9998863 , 0.9999338 , 0.9998308 , 1.0000825 ,\r\n       1.000022  , 0.9999998 , 0.9997648 , 1.0000801 , 1.000631  ,\r\n       1.0000259 , 0.9996165 , 1.0001084 , 0.9996289 ], dtype=float32), array([ 0.00369794, -0.00307915, -0.0148356 ,  0.01176912, -0.00456085,\r\n        0.00461122,  0.00392016, -0.00510793, -0.00388927,  0.00678776,\r\n       -0.0033672 ,  0.0020039 ,  0.00688829,  0.00877651,  0.00838199,\r\n       -0.0217527 , -0.00673187, -0.01623467,  0.00523926, -0.0005527 ,\r\n        0.00700372, -0.00372984, -0.01347521, -0.00636716,  0.00206494,\r\n        0.00884918, -0.00814271, -0.00801541, -0.02038615,  0.00171547,\r\n        0.00709944, -0.00221861,  0.00538696, -0.01515745, -0.01330438,\r\n        0.00306095,  0.00399868, -0.0049634 , -0.00725381,  0.00373429,\r\n       -0.01107734, -0.00610222, -0.00854702, -0.00504343, -0.0080514 ,\r\n       -0.00920443,  0.00863727, -0.01750346,  0.00656873, -0.00534429,\r\n        0.00434025, -0.01352841, -0.00819136, -0.01453205, -0.00043049,\r\n       -0.00257635, -0.00448346, -0.01370949,  0.00355583, -0.00480247,\r\n        0.00179911, -0.01858746, -0.00059417, -0.0123234 ], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\r\n      dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\r\n      dtype=float32)]\r\n```\r\n\r\nI think this happens because the training dataset of 1825 gets split into sets of `batch_size=16`. So there's going to be a set of 1 training image, and maybe that doesn't work with BatchNormalization.\r\n\r\nInference with the trained model gives\r\n\r\n```\r\nfoo = model.predict(test_onecell_im)\r\n\r\nfoo\r\narray([[nan],\r\n       [nan],\r\n       [nan],\r\n...\r\n       [nan],\r\n       [nan]], dtype=float32)\r\n```\r\n\r\nA solution is to make sure that the number of training images is a multiple of `batch_size`.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["The example given in [multi_gpu_model](https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model) states that the samples are divided equally across multiple gpus. Thus your solution to make training images as multiples of batch_size is correct. However its not mentioned explicitly if this should be the case. Adding @fchollet he may know more.", "I think you're right about the cause. Recommend to use the new distribution strategies API:\r\n\r\n```python\r\nmodel = ...\r\nmodel.compile(optimizer, loss, distribute=tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\nmodel.fit(...)\r\n```", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Also, please check the tutorial[`tf.distribute.Strategy with training loops`](https://www.tensorflow.org/beta/tutorials/distribute/training_loops) for more details on `MirroredStrategy`. Thanks!\r\n"]}, {"number": 24319, "title": "[tf.keras.layers.LSTM] reset_state performance issue", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): modified `tensorflow/python/keras/layers/recurrent.py` to get timings of LSTM reset (see code change below)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): 1.12.0, binary from PyPi\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nTime required to execute `tensorflow.keras.Model` `reset_states()` increases non-linearly with the number of stateful `tensorflow.keras.layers.LSTM` units, but only the first time `reset_states()` is called. The second time `reset_states()` it basically doesn't take any time at all.\r\nA few numbers to highlight this - all numbers refer to the reset of a single LSTM (one model layer), not the whole model:\r\n- 2 LSTMs: reset of the first LSTM takes ~20ms and of the last takes ~30ms (could be just a random variation)\r\n- 20 LSTMs: reset of the first LSTM takes ~300ms and of the last ~700ms\r\n- 200 LSTMs: reset of the first LSTM takes ~3500ms and of the last ~7000ms\r\nI traced the delay down to `tensorflow.keras.backend.set_value`, so it obviously is really setting of the state that takes extremely long.\r\n\r\n**Describe the expected behavior**\r\nI would expect that the reset of a single LSTM (one model layer) stays constant independent of the number of LSTMs the model consists of. More importantly, I would expect that the time it takes to reset a single LSTM doesn't increase, but is constant for all of the LSTMs of a model..\r\n\r\n**Code to reproduce the issue**\r\nImport module `datetime` in `tensorflow/python/keras/layers/recurrent.py` and replace method of `reset_states` of class `RNN` by\r\n```\r\n  def reset_states(self, states=None):\r\n    import datetime\r\n    if not self.stateful:\r\n      raise AttributeError('Layer must be stateful.')\r\n    batch_size = self.input_spec[0].shape[0]\r\n    if not batch_size:\r\n      raise ValueError('If a RNN is stateful, it needs to know '\r\n                       'its batch size. Specify the batch size '\r\n                       'of your input tensors: \\n'\r\n                       '- If using a Sequential model, '\r\n                       'specify the batch size by passing '\r\n                       'a `batch_input_shape` '\r\n                       'argument to your first layer.\\n'\r\n                       '- If using the functional API, specify '\r\n                       'the batch size by passing a '\r\n                       '`batch_shape` argument to your Input layer.')\r\n    # initialize state if None\r\n    if self.states[0] is None:\r\n      if _is_multiple_state(self.cell.state_size):\r\n        self.states = [\r\n            K.zeros([batch_size] + tensor_shape.as_shape(dim).as_list())\r\n            for dim in self.cell.state_size\r\n        ]\r\n      else:\r\n        self.states = [\r\n            K.zeros([batch_size] +\r\n                    tensor_shape.as_shape(self.cell.state_size).as_list())\r\n        ]\r\n    elif states is None:\r\n      if _is_multiple_state(self.cell.state_size):\r\n        now = datetime.datetime.now()\r\n        for state, dim in zip(self.states, self.cell.state_size):\r\n          K.set_value(state,\r\n                      np.zeros([batch_size] +\r\n                               tensor_shape.as_shape(dim).as_list()))\r\n        print(f\"LSTM reset time: {datetime.datetime.now() - now}\")  # TODO\r\n      else:\r\n        K.set_value(self.states[0], np.zeros(\r\n            [batch_size] +\r\n            tensor_shape.as_shape(self.cell.state_size).as_list()))\r\n    else:\r\n      if not isinstance(states, (list, tuple)):\r\n        states = [states]\r\n      if len(states) != len(self.states):\r\n        raise ValueError('Layer ' + self.name + ' expects ' +\r\n                         str(len(self.states)) + ' states, '\r\n                         'but it received ' + str(len(states)) +\r\n                         ' state values. Input received: ' + str(states))\r\n      for index, (value, state) in enumerate(zip(states, self.states)):\r\n        if _is_multiple_state(self.cell.state_size):\r\n          dim = self.cell.state_size[index]\r\n        else:\r\n          dim = self.cell.state_size\r\n        if value.shape != tuple([batch_size] +\r\n                                tensor_shape.as_shape(dim).as_list()):\r\n          raise ValueError(\r\n              'State ' + str(index) + ' is incompatible with layer ' +\r\n              self.name + ': expected shape=' + str(\r\n                  (batch_size, dim)) + ', found shape=' + str(value.shape))\r\n        # TODO(fchollet): consider batch calls to `set_value`.\r\n        K.set_value(state, value)\r\n```\r\n\r\nThen set `COUNT_LSTMS` to the number of LSTMs that should be used in the following script and execute it:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nCOUNT_LSTMS = 200\r\n\r\nBATCH_SIZE = 100\r\nUNITS_INPUT_OUTPUT = 5\r\nUNITS_LSTMS = 20\r\nBATCHES_TO_GENERATE = 2\r\nSEQUENCE_LENGTH = 20\r\n\r\n# build model\r\nmy_input = tf.keras.layers.Input(batch_shape=(BATCH_SIZE,\r\n                                              None,\r\n                                              UNITS_INPUT_OUTPUT))\r\nmy_lstm_layers = [tf.keras.layers.LSTM(units=UNITS_LSTMS,\r\n                                       stateful=True,\r\n                                       return_sequences=True)(my_input)\r\n                  for _ in range(COUNT_LSTMS)]\r\nmy_output_layer = tf.keras.layers.Dense(UNITS_INPUT_OUTPUT)\r\nmy_output = tf.keras.layers.TimeDistributed(my_output_layer)(\r\n    tf.keras.layers.concatenate(my_lstm_layers, axis=-1))\r\nmy_model = tf.keras.Model(my_input, my_output)\r\n\r\n\r\n# generation\r\npred_input = np.random.randn(BATCH_SIZE, 1, UNITS_INPUT_OUTPUT)\r\n\r\nfor batch in range(BATCHES_TO_GENERATE):\r\n    print('resetting states')\r\n    my_model.reset_states()\r\n    print(f\"start generation of batch {batch}\")\r\n    for _ in range(SEQUENCE_LENGTH):\r\n        pred_input = my_model.predict(pred_input, batch_size=BATCH_SIZE)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@eega,\r\nSorry for the delayed response. Can you please confirm if the problem still persists? Thanks!", "Sorry, no, I'm not in the position anymore to verify that issue. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24318, "title": "Build Fail gcc: internal compiler error: Killed ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Ubuntu 18.04\r\n- TensorFlow installed from source\r\n- TensorFlow version: git master\r\n- Python version:\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: CUDA 10.0/ cuDNN 7.3\r\n- GPU model and memory: GTX980Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuit fail\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ntensorflow/core/common_runtime/eager/execute.cc: In function 'tensorflow::Status tensorflow::{anonymous}::ValidateInputTypeAndPlacement(tensorflow::EagerContext*, tensorflow::Device*, tensorflow::EagerOperation*, const tensorflow::OpKernel*, tensorflow::RunMetadata*)':\r\ntensorflow/core/common_runtime/eager/execute.cc:179:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < op->Inputs().size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/eager/execute.cc: In function 'tensorflow::Status tensorflow::{anonymous}::EagerRemoteExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*)':\r\ntensorflow/core/common_runtime/eager/execute.cc:490:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < op->Inputs().size(); i++) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/eager/execute.cc:524:20: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (*num_retvals != output_dtypes.size()) {\r\n       ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/eager/execute.cc: In lambda function:\r\ntensorflow/core/common_runtime/eager/execute.cc:565:33: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n               for (int i = 0; i < retvals.size(); i++) {\r\n                               ~~^~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/eager/execute.cc: In function 'tensorflow::Status tensorflow::{anonymous}::MaybeUpdateOpDevice(tensorflow::EagerOperation*)':\r\ntensorflow/core/common_runtime/eager/execute.cc:626:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < op->Inputs().size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/eager/execute.cc:622:8: warning: variable 'device_set_for_resource_variable' set but not used [-Wunused-but-set-variable]\r\n   bool device_set_for_resource_variable = false;\r\n        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/eager/execute.cc: In function 'tensorflow::Status tensorflow::EagerExecute(tensorflow::EagerContext*, tensorflow::Device*, const absl::InlinedVector<tensorflow::TensorHandle*, 4>&, tensorflow::KernelAndDevice*, tensorflow::NodeExecStats*, tensorflow::StepStats*, tensorflow::GraphCollector*, tensorflow::TensorHandle**, int)':\r\ntensorflow/core/common_runtime/eager/execute.cc:716:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < op_inputs.size(); ++i) {\r\n                   ~~^~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/eager/execute.cc:753:43: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       while (step_stats->dev_stats_size() < ctx->devices()->size()) {\r\n              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/eager/execute.cc:758:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       for (int i = 0; i < ctx->devices()->size(); ++i) {\r\n                       ~~^~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/eager/execute.cc:713:27: warning: variable 'output_memory_types' set but not used [-Wunused-but-set-variable]\r\n   const MemoryTypeVector* output_memory_types = nullptr;\r\n                           ^~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/refcount.h:22,\r\n                 from ./tensorflow/core/platform/tensor_coding.h:21,\r\n                 from ./tensorflow/core/framework/resource_handle.h:19,\r\n                 from ./tensorflow/core/framework/allocator.h:24,\r\n                 from ./tensorflow/core/common_runtime/device.h:35,\r\n                 from ./tensorflow/core/common_runtime/eager/execute.h:18,\r\n                 from tensorflow/core/common_runtime/eager/execute.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:452:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:461:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nERROR: /home/xx/tensorflow/tensorflow/core/kernels/BUILD:762:1: C++ compilation of rule '//tensorflow/core/kernels:broadcast_to_op' failed (Exit 4)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1255.664s, Critical Path: 397.27s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 5547 processes: 5547 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n", "comments": ["@paradox56: See this issue: https://github.com/tensorflow/tensorflow/issues/7723\r\n\r\nIn short you need to use --local_resources on the bazel command to limit the amount of memory used by the build. ", "Thanks! That makes sense as my computer becomes extremely laggy during build process"]}, {"number": 24317, "title": "Build Tensorflow / Tensorflow Lite for Linux on ARM 64 bit", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nSources\r\n- TensorFlow version:\r\n- Python version:\r\n2.7\r\n- Installed using virtualenv? pip? conda?:\r\nNo\r\n- Bazel version (if compiling from source):\r\n15.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nHi , I am looking for a way to BUILD   Tensorflow/Tensorflow  Lite shared c++ library to run on ARM 64 bit with Ubuntu OS.\r\nThanks. \r\n\r\n", "comments": ["Please find instructions in [this](https://developer.codeplay.com/computecppce/latest/tensorflow-arm-setup-guide) link which help you to build Tensorflow on ARM.\r\n[This](https://gist.github.com/Brainiarc7/6d6c3f23ea057775b72c52817759b25c) is another resource which helps you for the same.\r\nAlso please be informed that we strictly encourage users to submit a bug/feature request in this Github repo. For your question, request you to post it on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). Thank you !", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 24316, "title": "[Feature Request] Nested HParams", "body": "// -\r\n\r\n", "comments": []}, {"number": 24315, "title": "[TFLite] Minor improvement of static build", "body": "- Include the experimental API in the static library. It is already included when building via Bazel.\r\n- Build static library with position independent code to allow its integration into dynamic libraries.", "comments": ["Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 59 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24314, "title": "Fix bug that made CMake to failed generating project for VS2017.", "body": "If version_info.cc is only available when not in Win32 then tf_version_srcs should indeed only exist when not in Win32 otherwise we receive a message error from CMake configuration saying it cannot find the file.\r\n\r\nThe error received was\r\n\r\n```\r\nCMake Error at tf_core_framework.cmake:332 (add_library):\r\n  Cannot find source file:\r\n\r\n    C:/Git/tensorflow/tensorflow/core/util/version_info.cc\r\n\r\n  Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm\r\n  .hpp .hxx .in .txx\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:587 (include)\r\n\r\n\r\nCMake Error at tf_core_framework.cmake:332 (add_library):\r\n  No SOURCES given to target: tf_core_framework\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:587 (include)\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@BRabbit27 Can you please sign the CLA to move forward with this PR?", "> @BRabbit27 Can you please sign the CLA to move forward with this PR?\r\n\r\nI have already signed it. Although, I just checked and this tiny change in the CMake file does not solve the whole compilation problem. I found a lot more problems compiling this via CMake, I think it has been completely deprecated and I better try to compile with Bazel, hopefully it'll be easier.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F24314).\n\n<!-- cla_yes -->", "@av8ramit Could you PTAL and approve."]}, {"number": 24313, "title": "Issue building Tensorflow 1.8.0 from sources", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OEL 7.4 (Oracle Linux)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.4.1\r\n- **Bazel version (if compiling from source)**: 0.19.2\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: Not using Cuda\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nWhile trying to build tensorflow from source, it failed stating:\r\n\"gcc: error: pywrap_tensorflow_internal_versionscript.lds: No such file or directory\"\r\n\r\n### Source code / logs\r\nERROR: /scratch/pransen/tensorflow-1.8.0/tensorflow/python/BUILD:3315:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1) gcc failed: error executing command /bin/gcc -shared -o bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Wl,--version-script pywrap_tensorflow_internal_versionscript.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' ... (remaining 90 argument(s) skipped)\r\n", "comments": ["@pransen  Request you to try bazel clean and build again.\r\nbazel clean --expunge", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 24312, "title": "@nccl_archive//:nccl: missing input file '@nccl_archive//:src/nccl.h'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 9585202ed095ec63c1a6f947a0197fce852e9036\r\n- Python version: 2.7.12\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): Ubuntu 5.4.0-6ubuntu1~16.04.10\r\n- CUDA/cuDNN version: 9.0/7.2.1.38\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the problem**\r\n\r\nd6a46850353acfe26625c5ab1ffe7bd5c5a4aaf0 breaks the build. Reverting it fix the problem. Ping @chsigg.\r\n\r\n```\r\nERROR: missing input file '@nccl_archive//:src/nccl.h'\r\nERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/nccl_archive/BUILD.bazel:104:1: @nccl_archive//:nccl: missing input file '@nccl_archive//:src/nccl.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/nccl_archive/BUILD.bazel:104:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 194.156s, Critical Path: 14.83s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 634 processes: 634 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Seems resolved by now. Thanks!", "Thanks, and sorry. That was a temporary hiccup.\n\nOn Fri, Dec 14, 2018 at 4:31 AM Bairen Yi <notifications@github.com> wrote:\n\n> Closed #24312 <https://github.com/tensorflow/tensorflow/issues/24312>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24312#event-2025897236>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHLOjq7QXKLBdFMTjuB5pkX3oK5yOY05ks5u4xuTgaJpZM4ZO-Pe>\n> .\n>\n"]}, {"number": 24311, "title": "TensorFlow Lite - Interpreter.resizeInput method fails after using a square shape during conversion", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX / Ubuntu 16.04 for creation of the model\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.0-dev20181129\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI created a custom keras model (cnn network with Resnet layers).  I defined the input to accept variable sized grey scale images (None, None, None, 1).  When I created TensorFlow lite model I was forced to provide input shape parameters (eg., 1,48,48,1).  Per documentation I should be able to change the input size using Interpreter.resizeInput() method.  I have to do this to accept variable sized images.  However this is not working ... I keep getting this error in Android studio.\r\n\r\njava.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/kernel_util.cc:129 d1 == d2 || d1 == 1 || d2 == 1 was not true.Node number 21 (ADD) failed to prepare.\r\n  \r\n\r\n**Describe the expected behavior**\r\n\r\nI should be able to pass a variable size image to the model.  The model (*pb file) works fine in Ubuntu.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you able to run inference with the shape used during conversion (1,48,48,1)? Or does the issue only occur if you resize to a different shape than the one used during conversion?", "Jared,\n\nYes.  I am able to inference with the shape used during conversion.  The\nissue occurs only when I resize to a different shape than the one used\nduring conversion.\n\nravi\n\nOn Thu, Dec 20, 2018 at 1:25 PM Jared Duke <notifications@github.com> wrote:\n\n> Are you able to run inference with the shape used during conversion\n> (1,48,48,1)? Or does the issue only occur if you resize to a different\n> shape than the one used during conversion?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24311#issuecomment-449139274>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AjgTBaXGiE-nF1W5tN9Nlxuqrwv3FDCYks5u7AArgaJpZM4ZO-Jm>\n> .\n>\n", "I see, and what shape are you providing at runtime to resizeInput? And is there only 1 input tensor in your graph?", "If you're comfortable sharing your (.tflite) model directly (either attaching here or direct messaging me), I'd be happy to troubleshoot directly.", "Let me create a simpler version of the TensorFlow Lite model for you.  I\nwill send it to you shortly.\n\n\n\nOn Fri, Dec 21, 2018 at 8:26 AM Jared Duke <notifications@github.com> wrote:\n\n> If you're comfortable sharing your (.tflite) model directly (either\n> attaching here or direct messaging me), I'd be happy to troubleshoot\n> directly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24311#issuecomment-449432374>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AjgTBdE2diGYU9-M8FDxBBy96ToEb6Kuks5u7QuqgaJpZM4ZO-Jm>\n> .\n>\n", "I have attached the tensorflow lite model which was created with shape (1,48,48,1).  It works in Android Studio when the input matches (1,48,48,1).  When I use input with different size and use Interpreter.resizeInput(), it does not work.\r\n\r\n\r\n\r\n[sample_model_48_48.tflite.zip](https://github.com/tensorflow/tensorflow/files/2704171/sample_model_48_48.tflite.zip)\r\n", "Thanks! And just for the sake of reproducibility, can you provide the exact shape you provided to resizeInput() when it failed?", "I tried (1, 230, 150, 1) ,   (1, 200, 120, 1).\r\n", "Please note that When I create the tflite model with (1, 230, 150, 1) it works!\r\n", "It's probably that, when you convert with a square shape, certain transformations are made which require that the graph maintain a square shape.\r\n\r\nI'm curious, if you convert with (1, 48, 48, 1), can you resize to, say, (1, 200, 200, 1) at runtime?", "I tried that too.  It did not work.", "Tried to reshape to (1,200,200,1) .  Got the same error...\r\n\r\njava.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/kernel_util.cc:129 d1 == d2 || d1 == 1 || d2 == 1 was not true.Node number 9 (ADD) failed to prepare.\r\n\r\n", "Hi,    Thanks for investigating this issue.    Would like to know when a resolution is possible.  I am working on a project deadline which needs a solution for variable size image as input.    Thanks again for everything so far.", "Still working on a more general solution. The converter makes some (questionable) optimizations for square shapes, which is preventing the subsequent resize. \r\n\r\nIn the meantime, for this case I would use the workaround you already discovered, that is, feed it a non-square shape during conversion (e.g., (1, 230, 150, 1)), which should allow resizing at runtime.", "@jdduke could you help to look at my issue here #23399? I convert the model with none square input shapes, but it still not allow resizing at runtime. Thank you.", "Thanks for flagging, @shawn-tian, this appears to be the same issue that we're tracking internally, I'll consolidate the bugs.", "As mentioned in the related issue #22377, not all graphs can be arbitrarily resized after conversion (e.g., when there's a reshape op that expects compatibility with a particular fixed shape).", "@jdduke Is it possible to locate which op cannot be resized after conversion? In my case, the error occurs in the add op as reported by @raviilango, even I freeze the graph with a non-square input.", "OK, the problem in this specific issue is related to the ResizeNearestNeighbor op.\r\n\r\nIn particular, it looks like the Add op that fails takes two inputs: one that has the new proper shape from the explicit resize, and one from the ResizeNearestNeighbor op. However, the ResizeNearestNeighbor op is using a fixed size, which produces a tensor (1, 48, 48, 96) that is incompatible with the resized tensor(s).\r\n\r\nWhat you'll need to do is make the size input for resize_nearest_neighbor a proper tensor that is also an input to the graph. When you resize the usual input tensor, you'll also need to update the *values* of the size input tensor for the resize_nearest_neighbor op.\r\n\r\nI'll look at the model from issue #23399 to see if it's the same underlying cause.", "Happy to offer more guidance, but in general we cannot always guarantee that a reshape will \"just work\", if there are downstream ops that have hardcoded inputs which dictate the shape of their outputs.", "@shawn-tian I've added some notes on issue #23399 about your specific model. In this case, the dimension being square was a red herring.", "Hi Jared,\r\n\r\nI have attached the results from my test.  The issue is still open.  Is there any timeline for a fix.  This is really critical for a project I am working on.  The current solution having multiple models (one each for a specific size) is not scalable.  Please note that the protobuf model works fine with variable sized inputs.   Greatly appreciate your assistance in prioritizing this issue for a fix.\r\n\r\n[IssueWithVariableSizedImage.xlsx](https://github.com/tensorflow/tensorflow/files/2757386/IssueWithVariableSizedImage.xlsx)\r\n\r\n\r\n\r\n\r\n\r\n", "I think the only way I can debug this further is with the original graph def that you used to create the .tflite model, as well as the commands you used during conversion.", "Thanks for the followup.  Looking forward to finding a resolution for this issue.", "Hi Jared,\r\n\r\nI have sent the scripts to create the .pb and .tflite files directly to you.\r\n\r\nthanks,\r\nravi\r\n", "Hi Jared,\r\n\r\nappreciate any update on this issue. \r\n\r\nthanks,\r\nravi", "tensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (4 != 2)\r\nSame problem\r\nIn other model squares it is possible to use dynamic size\r\nError using rectangle for current model"]}, {"number": 24310, "title": "[nGraph] Upgraded to v0.9.0 and fixed the broken MacOS build", "body": "This PR consists of the following:\r\n1. Updated to nGraph v0.11.0 and ngraph-tf v0.9.0\r\n2. Fixed the build issue in MacOS (as reported here: https://github.com/tensorflow/tensorflow/issues/22902", "comments": ["The ngraph changes LGTM. @gunan for the fix to the linker error mentioned in #22902.", "Hi @gunan - please let me know if yo have any question about MacOS build fix in this PR. We are trying to get this PR merged to master before the r1.13 is branched off tomorrow. Thanks"]}, {"number": 24309, "title": "[Tensorflow tutorial: speech commands] svdf model doesn't work in the demo app ", "body": "**Describe the current behavior**\r\nHi all,\r\nI'm trying the speech commands tutorial of Tensorflow.\r\nBut when I run the low latency svdf model in the demo app, it doesn't work.\r\nIt showed the error as **Other info / logs** below.\r\nI saw the same issue here: https://github.com/tensorflow/tensorflow/issues/13868\r\nBut it seems there's no specific and detailed solution.\r\nCould anyone help me with this?\r\nThanks! \r\nDavid \r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands\r\nhttps://www.tensorflow.org/versions/master/tutorials/audio_recognition\r\n\r\n**Other info / logs**\r\nE/zygote64: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n                  Process: org.tensorflow.demo, PID: 13070\r\n                  java.lang.RuntimeException: Unable to start activity ComponentInfo{org.tensorflow.demo/org.tensorflow.demo.SpeechActivity}: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/low_latency_svdf-135000.pb'\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2904)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2979)\r\n                      at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1643)\r\n                      at android.os.Handler.dispatchMessage(Handler.java:105)\r\n                      at android.os.Looper.loop(Looper.java:180)\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6944)\r\n                      at java.lang.reflect.Method.invoke(Native Method)\r\n                      at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:853)\r\n                   Caused by: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/low_latency_svdf-135000.pb'\r\n                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)\r\n                      at org.tensorflow.demo.SpeechActivity.onCreate(SpeechActivity.java:152)\r\n                      at android.app.Activity.performCreate(Activity.java:6986)\r\n                      at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1232)\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2857)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2979)\u00a0\r\n                      at android.app.ActivityThread.-wrap11(Unknown Source:0)\u00a0\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1643)\u00a0\r\n                      at android.os.Handler.dispatchMessage(Handler.java:105)\u00a0\r\n                      at android.os.Looper.loop(Looper.java:180)\u00a0\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6944)\u00a0\r\n                      at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n                      at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\u00a0\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:853)\u00a0\r\n                   Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32, qint8, quint8, qint32\r\n                  \t; NodeDef: {{node count_nonzero/Sum}} = Sum[T=DT_INT64, Tidx=DT_INT32, keep_dims=false](count_nonzero/ToInt64, count_nonzero/Const); Op<name=Sum; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_INT32, DT_QINT8, DT_QUINT8, DT_QINT32]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:561)\r\n                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)\r\n                      \t... 13 more", "comments": ["Passing to Rocky, since he's been looking at this.", "Just checked in a small change to the svdf model that fixes this."]}, {"number": 24308, "title": "enable_select_tf_ops=true cannot work?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux 4.15.0-42-generic Ubuntu 18.04 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):b'v1.12.0-rc0-4289-g68834966da' 1.12.0-rc0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source):7.3.0\r\n- CUDA/cuDNN version:None\r\n- GPU model and memory:None\r\n\r\nI know some ops are not supporting yet by tflite.So I use **TensorFlow Lite with select TensorFlow ops** as:\r\n\"\r\nbazel run --define=with_select_tf_ops=true //tensorflow/lite/toco:toco -- \\\r\n  --output_file=/tmp/out.tflite \\\r\n  --input_file=/models/inf.pb \\\r\n  --input_arrays=image_tensor \\\r\n  --output_arrays=boxes,scores,num_boxes \\\r\n  --input_shapes=1,1024,1024,3 \\\r\n  --inference_type=FLOAT \\\r\n  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS \\\r\n  --enable_select_tf_ops=true\r\n\"\r\nBUT **it still crashed** when converting:\r\n2018-12-11 23:14:32.493249: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2018-12-11 23:14:32.493271: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2018-12-11 23:14:32.493309: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3\r\n2018-12-11 23:14:32.493355: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit\r\n2018-12-11 23:14:32.493380: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit\r\n2018-12-11 23:14:32.493415: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit\r\n2018-12-11 23:14:32.493421: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3\r\n2018-12-11 23:14:32.493437: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3\r\n2018-12-11 23:14:32.493454: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3\r\n2018-12-11 23:14:32.493498: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3\r\n2018-12-11 23:14:32.493520: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3\r\n2018-12-11 23:14:32.493549: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3\r\n2018-12-11 23:14:32.503265: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 795 operators, 1359 arrays (0 quantized)\r\n2018-12-11 23:14:32.516861: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 787 operators, 1344 arrays (0 quantized)\r\n2018-12-11 23:14:32.535269: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 787 operators, 1344 arrays (0 quantized)\r\n2018-12-11 23:14:32.554909: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 547 operators, 936 arrays (0 quantized)\r\n2018-12-11 23:14:32.571142: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 547 operators, 936 arrays (0 quantized)\r\n2018-12-11 23:14:32.578976: F tensorflow/lite/toco/tooling_util.cc:627] Check failed: dim >= 1 (0 vs. 1)\r\n\r\nWhat should I do to make `select_tf_ops` mechanism work?\r\n\r\n\r\n", "comments": ["@MaeThird  Just to  make sure, did you get a chance to go through the below link which has information on how to select TF ops(which are not in TF Lite).\r\n[Reference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md) ", "@MaeThird Just to make sure, did you get a chance to go through the below link which has information on how to select TF ops(which are not in TF Lite).\r\n[Reference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md)", "Also, please check if the missing ops are whitelisted in the reference link we provide above.", "OK,I'm working on it.", "@MaeThird When I run the code on the given reference on 1.12 tf, i get the error in the line tf.lite.OpsSet.TFLITE_BUILTINS\r\nError:\r\nAttributeError: module 'tensorflow' has no attribute 'lite'\r\n\r\nI tried changing it to:-\r\ntf.contrib.lite.OpsSet.TFLITE_BUILTINS as well.\r\nBut it is unable to find OpsSet atrribute for lite. Any idea about how to solve this?", "@astu9880, can you try using the nightly TF build instead?", "In my case, if the  input is shape [1,112,112,6] and then feed it into the network everything works as expected.\r\nBut actually,the input image is shape [1,112,112,3] and I convert the RGB to HSV with shape [1,112,112,3],then concatenate the input image with the hsv to get a new 'input' tensor with shape [1,112,112,6] now I feed it into the network.**Unfortunately the toco converter parsed the depthwise_conv wrong.Some depth_multipliers are parsed as 0 and some are 1**.\r\n![00](https://user-images.githubusercontent.com/14851411/51227436-547ce600-198f-11e9-88c7-b9e26804ade5.JPG)\r\n![01](https://user-images.githubusercontent.com/14851411/51227437-547ce600-198f-11e9-8bc1-35512ba077ef.JPG)\r\nI converted the tflite file with  `--allow_custom_ops=true` for the unsupported op RGBToHSV and \r\nimplement it as a customer ops in tflite.\r\n**The shape of the tensor returned by tf.concat cannot be fixed in toco converting time in this case.Is this caused the wrong parsing of  successive depthwise_conv** ?\r\n The parameter `depth_multiplier` is redundant in tflite so I just force it to 1 which saved my time(^_^).\r\n@astu9880 your error is really weird.Try it again?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Are you able to share the output .tflite model? or the source model?"]}, {"number": 24307, "title": "some bug in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): liunx ubuntun 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: huawei mate 9\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):no\r\n- GCC/Compiler version (if compiling from source):6\r\n- CUDA/cuDNN version:9.0\r\n- GPU model and memory:2080\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n I need to transplant my net to mobilephone, first i run the demo in office,it works well,when i change the DetectiorMode to YOLO or MULTIBOX which provide in office code it didn't works\r\n**Describe the current behavior** \r\nwhen i change the mode to MULTIBOX there is no predictions\r\nwhen i change the mode to YOLO, the app is  crash\r\nI wonder if there is no implement in source code,\r\nwondering for you replay\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@shenyingying : I tried today and didn't see any crash on my Pixel2 .\r\nCan you include the adb logcat for the crash, also if possible your modifications to the code.", "> @shenyingying : I tried today and didn't see any crash on my Pixel2 .\r\n> Can you include the adb logcat for the crash, also if possible your modifications to the code.\r\n\r\n@shashishekhar  hi ,what yolo version you tried,in my application,the yolov2-tiny is ok,the yolov3-tiny isn't work,have you find this issue?", "@shenyingying : Can you attach the model to the bug or the log for the crash, will be easier to debug.", "@shashishekhar HI guys\r\n`2019-01-03 11:09:26.049 18193-18561/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[input], outputs:[output]\r\n    \r\n    --------- beginning of crash\r\n2019-01-03 11:09:26.055 18193-18561/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.demo, PID: 18193\r\n    java.lang.IllegalArgumentException: ConcatOp : Dimensions of inputs should match: shape[0] = [1,1,1,256] vs. shape[13] = [0,1,1,256]\r\n    \t [[{{node concat_13}} = ConcatV2[N=19, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Reshape_228, Reshape_229, Reshape_230, Reshape_231, Reshape_232, Reshape_233, Reshape_234, Reshape_235, Reshape_236, Reshape_237, Reshape_238, Reshape_239, Reshape_240, Reshape_241, Reshape_242, Reshape_243, Reshape_244, Reshape_245, Reshape_246, concat_13/axis)]]\r\n        at org.tensorflow.Session.run(Native Method)\r\n        at org.tensorflow.Session.access$100(Session.java:48)\r\n        at org.tensorflow.Session$Runner.runHelper(Session.java:314)\r\n        at org.tensorflow.Session$Runner.run(Session.java:264)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n        at org.tensorflow.demo.TensorFlowYoloDetector.recognizeImage(TensorFlowYoloDetector.java:173)\r\n        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:292)\r\n        at android.os.Handler.handleCallback(Handler.java:808)\r\n        at android.os.Handler.dispatchMessage(Handler.java:101)\r\n        at android.os.Looper.loop(Looper.java:166)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n2019-01-03 11:09:26.080 701-2668/? E/iMonitor: FaultDetect: DUMPTOOL_PRINTF return.`\r\nthis use yolov2.pb not yolov2-tiny throw this exception,can you solve it\uff0cths a lot \r\n", "@shashishekhar  there is another question , i can't convert yolov3-tiny.weight to yolov3-tiny.pb in CPU mode ,can you ever try it.", "@shenyingying : This looks like you are using TensorflowMobile instead of TfLite. From the error it looks like the Concat in your graph is receiving one input with dims [1,1,1,256] and another as [0,1,1,256] which is causing the error.\r\n\r\nCan you provide the model and code you are using, I suspect it is an error in feeding inputs. \r\n\r\nFor the second question can you attach: yolov3-tiny.weight to the bug.", "@shenyingying : Please reopen if you are still encountering this issue."]}, {"number": 24306, "title": "contrib\\cmake: version_info.cc not found", "body": "\r\n**System information**\r\n- OS Windows 10\r\n- TensorFlow version: lastest on master branch\r\n- Python version: 3.5\r\n- GCC/Compiler version (if compiling from source): Visual-studio 14 2015 win64\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 6.1/8Gb\r\n\r\n**Describe the problem**\r\n\r\nI followed the instruction in contrib/cmake to build tensorflow on windows, in generation step: \r\n\r\n`          Configuring done\r\n          CMake Error at tf_core_framework.cmake:332 (add_library):\r\n          Cannot find source file:\r\n\r\n    E:/work_space/projectcpp/tensorflow/tensorflow/tensorflow/core/util/version_info.cc\r\n\r\n    Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm\r\n           .hpp .hxx .in .txx\r\n          Call Stack (most recent call first):\r\n           CMakeLists.txt:587 (include)\r\n`\r\nIt seems like a known-bug but it is still there, please help.\r\nThank", "comments": ["We encourage you to use Bazel instead of Cmake as we no longer support cmake for Windows.\r\nAlso please refer this [link](https://www.tensorflow.org/install/source_windows) for more information on Building Tensorflow on Windows.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "version_info.cc\r\n\r\n```c++\r\n/*  Generated by gen_git_source.py  */\r\n#include <string>\r\nconst char* tf_git_version() {return \"b'unknown'\";}\r\nconst char* tf_compiler_version() {return __VERSION__;}\r\nconst int tf_cxx11_abi_flag() {\r\n#ifdef _GLIBCXX_USE_CXX11_ABI\r\n  return _GLIBCXX_USE_CXX11_ABI;\r\n#else\r\n  return 0;\r\n#endif\r\n}\r\nconst int tf_monolithic_build() {\r\n#ifdef TENSORFLOW_MONOLITHIC_BUILD\r\n  return 1;\r\n#else\r\n  return 0;\r\n#endif\r\n}\r\n\r\n```"]}, {"number": 24305, "title": "TensorFlow docker image JupyterNotebook does not start", "body": "1. I built the docker image using Dockerfile present in `tensorflow/tensorflow/tools/docker/Dockerfile`, When I run the image it does gives me url for notebook\r\n\r\n```\r\nCopy/paste this URL into your browser when you connect for the first time,\r\n    to login with a token:\r\n        http://(e01f14b69e8e or 127.0.0.1):8888/?token=da541XXXXXXXXXXXXXXXXXXXXXX\r\n```\r\n\r\nHowever opening the url fails with error \r\n`127.0.0.1 didn\u2019t send any data.\r\n`\r\n\r\nFollowing is the port-mapping output \r\n\r\n```\r\ndebug1: client_input_global_request: rtype hostkeys-00@openssh.com want_reply 0\r\ndebug1: Connection to port 8888 forwarding to localhost port 8888 requested.\r\ndebug1: channel 2: new [direct-tcpip]\r\ndebug1: Connection to port 8888 forwarding to localhost port 8888 requested.\r\ndebug1: channel 3: new [direct-tcpip]\r\nchannel 2: open failed: connect failed: Connection refused\r\nchannel 3: open failed: connect failed: Connection refused\r\ndebug1: channel 2: free: direct-tcpip: listening port 8888 for localhost port 8888, connect from 127.0.0.1 port 51331 to 127.0.0.1 port 8888, nchannels 4\r\n\r\n```", "comments": ["The images in tensorflow/tools/docker/ are deprecated. Can you please try using [tensorflow/tools/dockerfiles](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles) instead?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24304, "title": "Tensorflow Lite: ResNet example model gave VERY poor result during validation with ImageNet", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): The version I download from master branch\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.15\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI am studying tensorflow lite. I downloaded the ResNet frozen graph ResNet_V2_101 from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models.md#image-classification-float-models .\r\n\r\nAnd then I followed https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb to convert this frozen graph to both Lite model and quantized lite model.\r\n```\r\nimport tensorflow as tf\r\nimport pathlib\r\nimport sys\r\nimport tensorflow as tf\r\nfrom tensorflow.python.saved_model import tag_constants\r\nimport time\r\ngraph_def_file = \"resnet_saved_model/resnet_v2_101_299_frozen.pb\"\r\ninput_arrays = [\"input\"]\r\noutput_arrays = [\"output\"]\r\nconverter = tf.lite.TocoConverter.from_frozen_graph(str(graph_def_file),input_arrays,output_arrays,input_shapes = {\"input\":[1,299,299,3]})\r\ntflite_model = converter.convert()\r\nopen(\"saved_model/resnet_v2_101_299_frozen.tflite\", \"wb\").write(tflite_model) \r\n\r\nconverter.post_training_quantize = True\r\ntflite_quantized_model = converter.convert()\r\nopen(\"saved_model/resnet_v2_101_299_frozen_quantize.tflite\", \"wb\").write(tflite_quantized_model) \r\n```\r\nThen I followed https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/accuracy/ilsvrc to evalute its accuracy using ImageNet Validation Dataset (50000 images) on my desktop.\r\n\r\nHowever, when I run \r\n`bazel run -c opt   --cxxopt='--std=c++11'   --   //tensorflow/lite/tools/accuracy/ilsvrc:imagenet_accuracy_eval   --model_file=\"/home/kathy/saved_model/ResNet_V2_101.tflite\"   --ground_truth_images_path=\"/media/kathy/Documents/val_imgs\"   --ground_truth_labels=\"/home/kathy/workspace/tensorflow/tensorflow/lite/tools/accuracy/ilsvrc/VALIDATION_LABELS.txt\"   --model_output_labels=\"/home/kathy/workspace/tensorflow/tensorflow/lite/tools/accuracy/ilsvrc/resnet_output_labels.txt\"   --output_file_path=\"/tmp/accuracy_output.txt\" --num_images=0`\r\nand checked the output accuracy_output.txt. The accuracy is very poor. I can capture some results among the 50000 images. \r\n```\r\nTop 1, Top 2, Top 3, Top 4, Top 5, Top 6, Top 7, Top 8, Top 9, Top 10\r\n0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000\r\n0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000\r\n0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000\r\n0.000, 0.000, 0.000, 25.000, 25.000, 25.000, 25.000, 25.000, 25.000, 25.000\r\n0.000, 0.000, 0.000, 20.000, 20.000, 20.000, 20.000, 20.000, 20.000, 20.000\r\n0.000, 0.000, 0.000, 16.667, 16.667, 16.667, 16.667, 16.667, 16.667, 16.667\r\n0.000, 0.000, 0.000, 14.286, 14.286, 14.286, 14.286, 14.286, 14.286, 14.286\r\n0.000, 0.000, 0.000, 12.500, 12.500, 12.500, 12.500, 12.500, 12.500, 12.500\r\n0.000, 0.000, 0.000, 11.111, 11.111, 11.111, 11.111, 11.111, 11.111, 11.111\r\n0.000, 0.000, 0.000, 10.000, 10.000, 10.000, 10.000, 10.000, 10.000, 10.000\r\n0.000, 0.000, 0.000, 9.091, 9.091, 9.091, 9.091, 9.091, 9.091, 9.091\r\n0.000, 0.000, 0.000, 8.333, 8.333, 8.333, 8.333, 8.333, 8.333, 8.333\r\n\r\n```\r\n\r\nAfter running 50000 validation images, the top-1 to top-10 results are\r\n`0.080, 0.146, 0.230, 0.324, 0.408, 0.518, 0.608, 0.678, 0.770, 0.888 `\r\nHowever, according to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb, the top-1 accuracy can reach 76.8 but my attempt even cannot reach 1% in the end. Why this happens? Where I did wrong? Thanks!", "comments": ["I use the the wrong resnet_output_labels. ", "@WeiyiLi So can you reach the same accuracy as published?", "@chenlh14 Not exactly same but very similar.", "Just curious, what was the latency of running one inference, roughly speaking? Also, was it on Android? Thank you!"]}, {"number": 24303, "title": "Prevent stream executor construction for XLA devices", "body": "This PR prevents stream executor construction for the devices that are not in the visible_gpu_devices list given by the user. Construction of executors was causing allocations on other devices, reducing available memory.", "comments": ["At a high level, I don't think `std::set<int>` where `{-1}` means \"all devices\" is the cleanest abstraction for representing this constraint.\r\n\r\nI think preferable would be `absl::optional<std::set<Int>>` in the `Options` struct, and then as soon as we can enumerate all the device ordinals, we convert this into a plain `std::set<int>` without the special case for `{-1}` and just fill the set with all devices.\r\n\r\nWDYT?", "I didn't thought of optional. I will convert it and address other comment and update tomorrow. Thanks.", "@jlebar, please let me know if you have any further comments and I will try to get them in before the branching tomorrow.\r\n\r\nThanks,\r\nSami\r\n", "@jlebar I tried to address your comments. Please bear with me until I get used to the XLA style. Let me know if more changes are needed.", "Will get this merged asap so @tfboyd can consider a cherry-pick.", "Seems like it solves:\r\n- https://github.com/uber/horovod/issues/602\r\n- https://github.com/tensorflow/tensorflow/issues/23458\r\n\r\n I will do a final check once merged and close the issues if I can confirm.", "I believe it is a bug/feature in Grappler. At\r\nhttps://github.com/tensorflow/tensorflow/blob/6dbe2cd22fa8d6b48ef9bb913c00de74886be07f/tensorflow/core/grappler/grappler_item_builder.cc#L106\r\ngrappler_item_builder is initializing a new set of devices from the device factory without passing session config.\r\nAccording to https://github.com/tensorflow/tensorflow/issues/18861 initializing devices with different session config is an error condition. I am trying to understand the expected behavior. Perhaps we should make initialization sticky.\r\n"]}, {"number": 24302, "title": "TFTRT: LOG(ERROR) --> VLOG(1) when use_calibration=True with fp32/fp16", "body": "This is just to keep the default LOG cleaner.\r\n\r\nThe API mentions that `use_calibration` is ignored in case of precisions fp32/fp16.\r\n\r\nMy preference would be to change the default to `use_calibration = False` but that's a change in API behavior, and @aaroey asked to postpone that to TF2.0.", "comments": ["Can we use `LOG(WARNING)`?", "> Can we use `LOG(WARNING)`?\r\n\r\n@aaroey WARNING is definitely better than ERROR. But I still wouldn't print a warning just because it would confuse users when they see the message. My guess is that your concern is about users who actually want to do calibration with fp32/fp16, but I don't think anyone would be interested in that functionality. Please let me know if you still want to change it to WARNING.", "@pooyadavoodi fair enough, thanks!"]}, {"number": 24301, "title": "Add cuDNN deterministic env variable (only for convolution)", "body": "This change is a major component of the recipe for making TensorFlow training reproducible on GPUs.\r\n\r\nSetting the environment variable TF_CUDNN_DETERMINISTIC=1 (or true) will ensure that both forward and  backwards convolution algorithms are both fixed and deterministic. It overrides autotune and selects deterministic back-prop algorithms.", "comments": ["I will to close this pull request for now and open another one with master as base. However, at the time of writing this comment, master is not building for me.", "See [follow-up pull request](https://github.com/tensorflow/tensorflow/pull/24355) based on master."]}, {"number": 24300, "title": "Failed to compile a graph using XLA AOT? Executing genrule @org_tensorflow//:gen_graph failed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tf-nightly  -> b'v1.12.0-rc0-3642-g2ce0bec9da' 1.13.0-dev20181203\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.20.0      libprotoc 3.3.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GTX 1080 Ti - 11GB\r\n\r\n### Issue \r\nI am new to XLA compilation and run into some issue while compiling our model. Any help would be appreciated!  I was trying to use tfcompile to compile a pre-trained [ssd_resnet_50_fpn_coco model](http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz) unsuccessfully. I was running into Error message shown below. \r\n\r\n### Code to reproduce the issue:\r\nThe `BUILD` file looks like this\r\n```C\r\nload('@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl', 'tf_library')\r\n\r\ntf_library(\r\n    name = 'graph',\r\n    config = 'graph.config.pbtxt',\r\n    cpp_class = 'Graph',\r\n    graph = 'graph.pb',\r\n)\r\n```\r\n`graph.config.pbtxt` looks like this\r\n```JavaScript\r\nfeed {\r\n      id {\r\n        node_name: \"image_tensor\"\r\n      }\r\n      shape {\r\n        dim {\r\n          size: 1\r\n        }\r\n        dim {\r\n          size: 640\r\n        }\r\n        dim {\r\n          size: 640\r\n        }\r\n        dim {\r\n          size: 3\r\n        }\r\n      }\r\n    }\r\n    fetch {\r\n      id {\r\n        node_name: \"detection_boxes\"\r\n      }\r\n    }\r\n    fetch {\r\n      id {\r\n        node_name: \"detection_scores\"\r\n      }\r\n    }\r\n    fetch {\r\n      id {\r\n        node_name: \"detection_classes\"\r\n      }\r\n    }\r\n    fetch {\r\n      id {\r\n        node_name: \"num_detections\"\r\n      }\r\n    }\r\n```\r\nThen I run `bazel build --show_progress_rate_limit=600 @org_tensorflow//:graph`\r\n\r\n> ERROR: /home/syntech/.cache/bazel/_bazel_syntech/186a880d737f29aade7a2d700e840c56/external/org_tensorflow/BUILD:4:1: Executing genrule @org_tensorflow//:gen_graph failed (Exit 1)\r\nINVALID ARGUMENTS: Merge of two inputs that differ on more than one predicate {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,then), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,then)} and {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,else)}\r\n\tfor node {{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/Merge}}\r\n tfcompile performs ahead-of-time compilation of a TensorFlow graph,\r\nresulting in an object file compiled for your target architecture, and a\r\nheader file that gives access to the functionality in the object file.\r\nA typical invocation looks like this:\r\n  $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt --cpp_class=\"mynamespace::MyComputation\"\r\nusage: bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/aot/tfcompile\r\nFlags:\r\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t... ( I have omitted some of the irrelevant output here) \r\n        ...\r\n        ...\r\nTarget @org_tensorflow//:graph failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1373.473s, Critical Path: 57.61s\r\nINFO: 3686 processes: 3686 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nRunning with `--verbose_failures` gives \r\n>INFO: Invocation ID: 7d6f9793-2517-4ea7-afab-faf4fa30eaf9\r\nINFO: Analysed target @org_tensorflow//:graph (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/syntech/.cache/bazel/_bazel_syntech/186a880d737f29aade7a2d700e840c56/external/org_tensorflow/BUILD:4:1: Executing genrule @org_tensorflow//:gen_graph failed (Exit 1): bash failed: error executing command\r\n  (cd /home/syntech/.cache/bazel/_bazel_syntech/186a880d737f29aade7a2d700e840c56/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/home/syntech/TensorRT-4.0.1.6/lib/ \\\r\n    PATH=/home/syntech/tx_dev/venv/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/syntech/bin \\\r\n    PYTHON_BIN_PATH=/home/syntech/tx_dev/venv/bin/python \\\r\n    PYTHON_LIB_PATH=/home/syntech/tx_dev/venv/lib/python3.5/site-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; CUDA_VISIBLE_DEVICES='\\'''\\'' bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/aot/tfcompile --graph=external/org_tensorflow/models/frozen_inference_graph.pb --config=external/org_tensorflow/ssdresnet.config.pbtxt --entry_point=____graph --cpp_class=Model --target_triple=x86_64-pc-linux --out_header=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph.h --out_metadata_object=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph_tfcompile_metadata.o --out_function_object=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph_tfcompile_function.o  ')\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nINVALID ARGUMENTS: Merge of two inputs that differ on more than one predicate {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,then), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,then)} and {s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Greater/_303__cf__306:0,else), s(Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/pred_id/_304__cf__307:0,else)}\r\n\tfor node {{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/cond/Merge}}\r\n\r\nAnd when I try to compile our own customized SSD-ResNet (with additional preprocessing operation to decode an encoded image string).  The following error occur:\r\n>INFO: Invocation ID: 63eccf9a-fa37-427f-ac89-e746f28c80b5\r\nINFO: Analysed target @org_tensorflow//:graph (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/syntech/.cache/bazel/_bazel_syntech/186a880d737f29aade7a2d700e840c56/external/org_tensorflow/BUILD:4:1: Executing genrule @org_tensorflow//:gen_graph failed (Exit 1)\r\n2018-12-07 15:25:48.709118: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nINVALID ARGUMENTS: Detected unsupported operations when trying to compile graph tfcompile on XLA_CPU_JIT: Shape (No registered 'Shape' OpKernel for XLA_CPU_JIT devices compatible with node {{node map/Shape}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, out_type=DT_INT32\r\n\t.  Registered:  device='XLA_CPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64]\r\n){{node map/Shape}}\r\n\r\n\r\n", "comments": ["I tried using Graph Transformation Tool to strip the unused/training nodes and fused operations but run into new issues with unsupported operations such as `tf.where()`.  Are there any plans on supporting those in the near future? and are there any work arounds? Thank you!", "wrt `tf.where()`, see https://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473  We are actively working on making it possible to use this op, but it's a very big change, and at the moment there's really no workaround other than rewriting your model not to use `tf.where`.\r\n\r\nI'm going to close this because I'm not sure there's much that we can do immediately, but if you have other questions feel free to ask, I'm not trying to end the conversation or anything.\r\n\r\ncc @sanjoy "]}, {"number": 24299, "title": "Keras + Estimator training step evaluated twice in tensorflow 1.10 but not in >=1.11", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pipenv\r\n- TensorFlow version (use command below): (v1.10.0-rc1-19-g656e7a2b34 1.10.0) and (v1.11.0-rc2-4-gc19e29306c 1.11.0)\r\n- Python version: 3.5.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nIn 1.10 (**latest ml-engine supported tensorflow**), using Keras + Estimator API results in the training step seemingly being evaluated twice.\r\n\r\nI've also tried writing my own custom estimator and the problem goes away, which indicates this is an interaction between Keras and the Estimator API.\r\n\r\nThe initial reason why I found this is because my model was getting worse results in gcloud than in local, which led me down this path.\r\n\r\ncc: @fchollet \r\n\r\nhttps://stackoverflow.com/questions/53699535/getting-worse-results-on-gcloud-vs-local-training/\r\n\r\n**Describe the expected behavior**\r\n\r\nIn >=1.11, the training step is evaluated once (correctly). However, 1.11 is not supported in gcloud ml-engine.\r\n\r\n**Code to reproduce the issue**\r\n1. Download `https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/imdb`\r\n2. Edit `./tensorflow/keras/requirements.txt` to use `tensorflow==1.10`, install dependencies with your favorite tool (I use pipenv)\r\n3. Edit `sample.sh`, comment out the `rm ...` lines\r\n4. Run `./sample.sh`\r\n5. Edit `./tensorflow/keras/requirements.txt` to use `tensorflow==1.11`, install dependencies with your favorite tool (I use pipenv)\r\n6. Run `./sample.sh`\r\n\r\n**Tensorboard evidence**\r\n\r\nHere's a sample tensorboard training loss:\r\n![image](https://user-images.githubusercontent.com/692818/49831612-27626580-fd49-11e8-8532-6aacb7719714.png)\r\nOrange is tensorflow 1.10, Red is tensorflow 1.11\r\n\r\n#### Output log for 1.10\r\n\r\nCalling attention on the doubled training lines, e.g.:\r\n```\r\nINFO:tensorflow:loss = 0.33765173, step = 1202 (0.980 sec)\r\nINFO:tensorflow:global_step/sec: 205.573\r\nINFO:tensorflow:global_step/sec: 196.979\r\n```\r\n\r\n```\r\n(keras) bash-3.2$ ./sample.sh\r\nRunning 'tensorflow/keras' code sample.\r\nCopying gs://cloud-samples-data/ml-engine/imdb/imdb.npz...\r\n\\ [1 files][ 16.7 MiB/ 16.7 MiB]\r\nOperation completed over 1 objects/16.7 MiB.\r\nCopying gs://cloud-samples-data/ml-engine/imdb/imdb_word_index.json...\r\n- [1 files][  1.6 MiB/  1.6 MiB]\r\nOperation completed over 1 objects/1.6 MiB.\r\n/Users/davidma/.local/share/virtualenvs/keras-jsYBpDFJ/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\r\n  return f(*args, **kwds)\r\nINFO:tensorflow:Using the Keras model provided.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_save_checkpoints_steps': 500, '_num_worker_replicas': 1, '_evaluation_master': '', '_session_config': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x116fee9b0>, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_is_chief': True, '_master': '', '_tf_random_seed': None, '_save_summary_steps': 100, '_train_distribute': None, '_task_type': 'worker', '_keep_checkpoint_every_n_hours': 10000, '_device_fn': None, '_log_step_count_steps': 100, '_model_dir': 'iris_20181211_132929', '_global_id_in_cluster': 0, '_service': None, '_task_id': 0}\r\n2018-12-11 13:29:38.241599: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 500 or save_checkpoints_secs None.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from iris_20181211_132929/keras_model.ckpt\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into iris_20181211_132929/model.ckpt.\r\nINFO:tensorflow:loss = 0.70380175, step = 2\r\nINFO:tensorflow:global_step/sec: 79.0904\r\nINFO:tensorflow:global_step/sec: 52.5143\r\nINFO:tensorflow:loss = 0.6854124, step = 202 (3.168 sec)\r\nINFO:tensorflow:global_step/sec: 200.878\r\nINFO:tensorflow:loss = 0.6641856, step = 402 (1.008 sec)\r\nINFO:tensorflow:global_step/sec: 196.043\r\nINFO:tensorflow:Saving checkpoints for 500 into iris_20181211_132929/model.ckpt.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-12-11-21:29:58\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from iris_20181211_132929/model.ckpt-500\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-12-11-21:29:59\r\nINFO:tensorflow:Saving dict for global step 500: accuracy = 0.7399388, global_step = 500, loss = 0.6379081\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: iris_20181211_132929/model.ckpt-500\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\nINFO:tensorflow:Restoring parameters from iris_20181211_132929/model.ckpt-500\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:SavedModel written to: iris_20181211_132929/export/exporter/temp-b'1544563800'/saved_model.pb\r\nINFO:tensorflow:global_step/sec: 27.1525\r\nINFO:tensorflow:loss = 0.5980946, step = 602 (4.209 sec)\r\nINFO:tensorflow:global_step/sec: 189.933\r\nINFO:tensorflow:global_step/sec: 200.593\r\nINFO:tensorflow:loss = 0.4988567, step = 802 (1.005 sec)\r\nINFO:tensorflow:global_step/sec: 197.686\r\nINFO:tensorflow:global_step/sec: 204.341\r\nINFO:tensorflow:Saving checkpoints for 1000 into iris_20181211_132929/model.ckpt.\r\nINFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\r\nINFO:tensorflow:loss = 0.40151688, step = 1002 (1.476 sec)\r\nINFO:tensorflow:global_step/sec: 100.741\r\nINFO:tensorflow:global_step/sec: 201.636\r\nINFO:tensorflow:loss = 0.33765173, step = 1202 (0.980 sec)\r\nINFO:tensorflow:global_step/sec: 205.573\r\nINFO:tensorflow:global_step/sec: 196.979\r\nINFO:tensorflow:loss = 0.31514037, step = 1402 (0.998 sec)\r\nINFO:tensorflow:global_step/sec: 207.539\r\nINFO:tensorflow:Saving checkpoints for 1500 into iris_20181211_132929/model.ckpt.\r\nINFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\r\nINFO:tensorflow:global_step/sec: 102.045\r\nINFO:tensorflow:loss = 0.24642324, step = 1602 (1.475 sec)\r\nINFO:tensorflow:global_step/sec: 202.219\r\nINFO:tensorflow:global_step/sec: 200.376\r\nINFO:tensorflow:loss = 0.23321539, step = 1802 (0.993 sec)\r\nINFO:tensorflow:global_step/sec: 200.815\r\nINFO:tensorflow:global_step/sec: 206.724\r\nINFO:tensorflow:Saving checkpoints for 1954 into iris_20181211_132929/model.ckpt.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-12-11-21:30:09\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from iris_20181211_132929/model.ckpt-1954\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-12-11-21:30:10\r\nINFO:tensorflow:Saving dict for global step 1954: accuracy = 0.875825, global_step = 1954, loss = 0.3054119\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 1954: iris_20181211_132929/model.ckpt-1954\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\nINFO:tensorflow:Restoring parameters from iris_20181211_132929/model.ckpt-1954\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:SavedModel written to: iris_20181211_132929/export/exporter/temp-b'1544563810'/saved_model.pb\r\nINFO:tensorflow:Loss for final step: 0.2535662.\r\nPython script succeeded\r\n```\r\n\r\n#### Output log for 1.11\r\n\r\nNote expected behavior one one training line per loss line:\r\n```\r\nINFO:tensorflow:loss = 0.10183072, step = 1601 (0.973 sec)\r\nINFO:tensorflow:global_step/sec: 100.407\r\n```\r\n\r\n```\r\n(keras) bash-3.2$ ./sample.sh\r\nRunning 'tensorflow/keras' code sample.\r\nCopying gs://cloud-samples-data/ml-engine/imdb/imdb.npz...\r\n- [1 files][ 16.7 MiB/ 16.7 MiB]\r\nOperation completed over 1 objects/16.7 MiB.\r\nCopying gs://cloud-samples-data/ml-engine/imdb/imdb_word_index.json...\r\n- [1 files][  1.6 MiB/  1.6 MiB]\r\nOperation completed over 1 objects/1.6 MiB.\r\nINFO:tensorflow:Using the Keras model provided.\r\n2018-12-11 13:10:57.544772: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nINFO:tensorflow:Using config: {'_train_distribute': None, '_service': None, '_is_chief': True, '_num_worker_replicas': 1, '_eval_distribute': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11998c0b8>, '_keep_checkpoint_every_n_hours': 10000, '_device_fn': None, '_tf_random_seed': None, '_global_id_in_cluster': 0, '_log_step_count_steps': 100, '_task_id': 0, '_save_checkpoints_steps': 500, '_master': '', '_experimental_distribute': None, '_save_checkpoints_secs': None, '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_evaluation_master': '', '_num_ps_replicas': 0, '_protocol': None, '_model_dir': 'iris_20181211_131049', '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_task_type': 'worker'}\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 500 or save_checkpoints_secs None.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='iris_20181211_131049/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: ('iris_20181211_131049/keras/keras_model.ckpt',)\r\nINFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: embedding/embeddings; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into iris_20181211_131049/model.ckpt.\r\nINFO:tensorflow:loss = 0.69291425, step = 1\r\nINFO:tensorflow:global_step/sec: 58.3576\r\nINFO:tensorflow:loss = 0.6530326, step = 101 (1.713 sec)\r\nINFO:tensorflow:global_step/sec: 103.954\r\nINFO:tensorflow:loss = 0.52952975, step = 201 (0.961 sec)\r\nINFO:tensorflow:global_step/sec: 98.6123\r\nINFO:tensorflow:loss = 0.38874263, step = 301 (1.014 sec)\r\nINFO:tensorflow:global_step/sec: 104.086\r\nINFO:tensorflow:loss = 0.29528034, step = 401 (0.961 sec)\r\nINFO:tensorflow:Saving checkpoints for 500 into iris_20181211_131049/model.ckpt.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-12-11-21:11:17\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-500\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-12-11-21:11:19\r\nINFO:tensorflow:Saving dict for global step 500: accuracy = 0.8717262, global_step = 500, loss = 0.31936663\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: iris_20181211_131049/model.ckpt-500\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\nINFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-500\r\nWARNING:tensorflow:From /Users/davidma/.local/share/virtualenvs/keras-jsYBpDFJ/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py:1018: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPass your op to the equivalent parameter main_op instead.\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:SavedModel written to: iris_20181211_131049/export/exporter/temp-b'1544562680'/saved_model.pb\r\nINFO:tensorflow:global_step/sec: 23.9337\r\nINFO:tensorflow:loss = 0.2431845, step = 501 (4.178 sec)\r\nINFO:tensorflow:global_step/sec: 100.357\r\nINFO:tensorflow:loss = 0.23459233, step = 601 (0.997 sec)\r\nINFO:tensorflow:global_step/sec: 101.69\r\nINFO:tensorflow:loss = 0.19313249, step = 701 (0.983 sec)\r\nINFO:tensorflow:global_step/sec: 102.563\r\nINFO:tensorflow:loss = 0.16869232, step = 801 (0.975 sec)\r\nINFO:tensorflow:global_step/sec: 103.525\r\nINFO:tensorflow:loss = 0.18477763, step = 901 (0.966 sec)\r\nINFO:tensorflow:Saving checkpoints for 1000 into iris_20181211_131049/model.ckpt.\r\nINFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\r\nINFO:tensorflow:global_step/sec: 68.294\r\nINFO:tensorflow:loss = 0.17220864, step = 1001 (1.464 sec)\r\nINFO:tensorflow:global_step/sec: 99.2842\r\nINFO:tensorflow:loss = 0.1683223, step = 1101 (1.007 sec)\r\nINFO:tensorflow:global_step/sec: 101.347\r\nINFO:tensorflow:loss = 0.16812363, step = 1201 (0.986 sec)\r\nINFO:tensorflow:global_step/sec: 101.03\r\nINFO:tensorflow:loss = 0.13969234, step = 1301 (0.990 sec)\r\nINFO:tensorflow:global_step/sec: 103.339\r\nINFO:tensorflow:loss = 0.14550006, step = 1401 (0.968 sec)\r\nINFO:tensorflow:Saving checkpoints for 1500 into iris_20181211_131049/model.ckpt.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-12-11-21:11:31\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-1500\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-12-11-21:11:32\r\nINFO:tensorflow:Saving dict for global step 1500: accuracy = 0.8762469, global_step = 1500, loss = 0.32633907\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 1500: iris_20181211_131049/model.ckpt-1500\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\nINFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-1500\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:SavedModel written to: iris_20181211_131049/export/exporter/temp-b'1544562692'/saved_model.pb\r\nINFO:tensorflow:global_step/sec: 31.6362\r\nINFO:tensorflow:loss = 0.11526476, step = 1501 (3.161 sec)\r\nINFO:tensorflow:global_step/sec: 102.767\r\nINFO:tensorflow:loss = 0.10183072, step = 1601 (0.973 sec)\r\nINFO:tensorflow:global_step/sec: 100.407\r\nINFO:tensorflow:loss = 0.092377335, step = 1701 (0.996 sec)\r\nINFO:tensorflow:global_step/sec: 104.123\r\nINFO:tensorflow:loss = 0.077209204, step = 1801 (0.960 sec)\r\nINFO:tensorflow:global_step/sec: 102.508\r\nINFO:tensorflow:loss = 0.1030057, step = 1901 (0.976 sec)\r\nINFO:tensorflow:Saving checkpoints for 1954 into iris_20181211_131049/model.ckpt.\r\nINFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-12-11-21:11:38\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-1954\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Finished evaluation at 2018-12-11-21:11:39\r\nINFO:tensorflow:Saving dict for global step 1954: accuracy = 0.86955726, global_step = 1954, loss = 0.38054743\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 1954: iris_20181211_131049/model.ckpt-1954\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\nINFO:tensorflow:Restoring parameters from iris_20181211_131049/model.ckpt-1954\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:SavedModel written to: iris_20181211_131049/export/exporter/temp-b'1544562699'/saved_model.pb\r\nINFO:tensorflow:Loss for final step: 0.05532141.\r\nPython script succeeded\r\n```", "comments": ["Looks like ml cloud engine supports tf > 1.10 now so the workaround for whoever sees this would be to upgrade!"]}, {"number": 24298, "title": "[ROCm] Introduce TENSORFLOW_USE_ROCM macro to support operators on ROCm", "body": "Enable supported TensorFlow operators on ROCm. Use TENSORFLOW_USE_ROCM macro\r\nwhich was introduced in PR#20277.\r\n\r\nOnly trivial ones which doesn't inlcude custom CUDA / HIP GPU kernels are\r\nincluded this commit. Subsequent ones would cover other operators.", "comments": ["@ymodak, I work primarily on XLA, I'm not familiar with TF core, so I'm not comfortable reviewing this.", "Nagging Reviewer @benbarsdell, @rmlarsen: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied.", "Will break this PR into smaller ones."]}, {"number": 24297, "title": "Fix Bazel version check", "body": "Make sure that native.bazel_version is not empty before parsing. Before this PR, an empty bazel version was parsed to a tuple which caused a crash at the beginning of build procedure if user built bazel instead of installing it from distribution packages.", "comments": ["@Gunan, @martinwicke Is this already applied internally. Conflict seems to be just formatting and a comment.", "Odd. Maybe the internal merge went wrong and failed to mark the PR as merged. I'll close this then, sorry about that."]}, {"number": 24296, "title": "Compilation failed when building keras model with CTC on TPU", "body": "**System information**\r\n- Google Colab with TPU\r\n\r\n\r\n**Describe the current behavior**\r\nKeras model with CTC on TPU gives error\r\n\r\n**Describe the expected behavior**\r\nshould work as it does on GPU and CPU backends\r\n\r\n**Code to reproduce the issue**\r\nI made a small modification to the MNIST TPU example (https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/keras_mnist.ipynb)\r\njust to test whether TPUs currently support CTC\r\n```\r\ndef ctc_batch_cost(args):\r\n    y_pred,y_true = args\r\n    input_length = y_true[:,1:2]+5\r\n    label_length = y_true[:,1:2]+5\r\n    return K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\r\n  \r\ninp = tf.keras.Input(shape=(28*28,))\r\nx = l.Reshape(input_shape=(28*28,), target_shape=(28, 28))(inp)\r\nx = l.Dense(10, activation='softmax')(x)\r\nloss = l.Lambda(ctc_batch_cost, output_shape=(1,), name='ctc')([x, inp ])\r\n\r\nmodel = tf.keras.Model(inputs=[inp], outputs=loss)\r\n\r\nmodel.compile(optimizer='adam', \r\n              loss={'ctc': lambda y_true, y_pred: y_pred},\r\n              metrics=['accuracy'])\r\n```\r\nunfortunately, while this code works with the GPU backend, for the TPU backend, it gives\r\n\r\n> RuntimeError: Compilation failed: Compilation failure: Detected unsupported operations when trying to compile graph cluster_1_6470206916041614137_f15n_1[] on XLA_TPU_JIT: Where (No registered 'Where' OpKernel for XLA_TPU_JIT devices compatible with node {{node tpu_139921521852368/ctc/boolean_mask/Where}} = Where[T=DT_BOOL, _device=\"/device:TPU_REPLICATED_CORE\"](tpu_139921521852368/ctc/boolean_mask/Reshape_1)\r\n> \t.  Registered:  device='CPU'; T in [DT_BOOL]\r\n>   device='CPU'; T in [DT_COMPLEX128]\r\n>   device='CPU'; T in [DT_COMPLEX64]\r\n>   device='CPU'; T in [DT_DOUBLE]\r\n>   device='CPU'; T in [DT_FLOAT]\r\n>   device='CPU'; T in [DT_BFLOAT16]\r\n>   device='CPU'; T in [DT_HALF]\r\n>   device='CPU'; T in [DT_INT8]\r\n>   device='CPU'; T in [DT_UINT8]\r\n>   device='CPU'; T in [DT_INT16]\r\n>   device='CPU'; T in [DT_UINT16]\r\n>   device='CPU'; T in [DT_INT32]\r\n>   device='CPU'; T in [DT_INT64]\r\n> ){{node tpu_139921521852368/ctc/boolean_mask/Where}}\r\n\r\nis there any workaround for this ? \r\nCTC has only a CPU implementation, and the GPU backed offloads it to the CPU, why isn't this also the case for TPUs ?\r\n\r\nThanks,", "comments": ["any thoughts ?\r\n\r\nRegards,,", "Same issue as this: https://github.com/tensorflow/tpu/issues/236\r\nBecause` tf.where` is called with both `x` and `y` as \"None\".", "@ASDen is it solved?", "@ASDen,\r\nCan you please refer to [this comment](https://github.com/tensorflow/tpu/issues/236#issuecomment-437128153) where the similar issue has been resolved? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24296\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24296\">No</a>\n"]}, {"number": 24295, "title": "[ROCm] Rename GPU device array helpers from cuda_ to gpu_", "body": "Common utility helpers between CUDA and ROCm are renamed:\r\n\r\n- core/kernels/cuda_device_array.h -> core/kernels/gpu_device_array.h\r\n- core/kernels/cuda_device_array_gpu.h -> core/kernels/gpu_device_array_gpu.h\r\n\r\nIn subsequent PRs, ROCm-specific terminology changes would be introduced.", "comments": ["@yifeif May I ask you to take a look at this PR? It's a precursor to upstream ROCm-specific logic for TensorFlow operators.\r\n\r\nFailed CI targets are:\r\n- MacOS Contrib\r\n- MacOS Python2 and CC\r\n- Windows Bazel\r\n- Windows Bazel GPU\r\n\r\nFrom the logs they seem to have nothing to do with this PR, but a configuration issue in CI test infrastructure. I'm wondering if it's possible to manually apply `ready-to-pull` label, after your review and approval?", "ping?", "Nagging Reviewer @yifeif: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Sorry for the delay @whchung. This change looks pretty benign, and dont think the errors are related.", "@whchung Can you please resolve the merge conflicts? Thanks!", "@ymodak sorry for being late. resolved all conflicts with tip.", "@yifeif Can you please approve this PR again after the latest merge conflicts were resolved?", "For sure! Looks good other than one more instance needs to be renamed:\r\n'./tensorflow/core/kernels/split_lib_gpu.h:27:59: fatal error: tensorflow/core/kernels/cuda_device_array_gpu.h: No such file or directory'\r\n\r\nMind making the change and we can import again? Thank you!\r\n", "@yifeif sorry I made some mistakes in the rebase process. it should have been fixed by now.", "@yifeif could you take another look now? there is only one failed target `Ubuntu contrib` and looking from the logs it shouldn't have anything to do with this PR", "Thanks @whchung! Re-importing now.", "@yifeif thanks! #26678 is a subsequent PR after this. Wondering could you help also take a look at it?", "Ofc. Taking a look now!"]}, {"number": 24294, "title": "Rename CudnnScratchAllocator to DnnScratchAllocator", "body": "Rename CudnnScratchAllocator as the logic is applicable for not only Cudnn, but\r\nalso other DNN algorithm libraries such as MIOpen.", "comments": ["@timshen91 May I ask you to take a look at this PR? Now we have StreamExecutor PRs under your review, and this PR is a precursor to upstream ROCm-specific logic for TensorFlow operators.\r\n\r\nFailed CI targets are:\r\n- MacOS Contrib\r\n- MacOS Python2 and CC\r\n- Windows Bazel\r\n- Windows Bazel GPU\r\n\r\nFrom the logs they seem to have nothing to do with this PR, but a configuration issue in CI test infrastructure. I'm wondering if it's possible to manually apply `ready-to-pull` label, after your review and approval?"]}, {"number": 24293, "title": "[ROCm] ROCm-specific GPU utility functions.", "body": "Common utility helpers between CUDa and ROCm are renamed:\r\n- core/util/cuda_device_functions.h -> core/util/gpu_device_functions.h\r\n- core/util/cuda_kernel_helper.h -> core/util/gpu_kernel_helper.h\r\n- core/util/cuda_kernel_helper_test.cu.cc -> core/util/gpu_kernel_helper_test.cu.cc\r\n- core/util/cuda_launch_config.h -> core/util/gpu_launch_cnofig.h\r\n\r\nROCm-specific changes for GPU device functions are also introduced in this PR.\r\n\r\nGPU kernel invocation logic would be introduced in the next PR. All custom CUDA kernels are still launched as `CudaLaunchKernel` for now.", "comments": ["@smit-hinsu Thanks for approving this PR. Failed CI targets are:\r\n- MacOS Contrib\r\n- MacOS Python2 and CC\r\n- Windows Bazel\r\n- Windows Bazel GPU\r\n\r\nFrom the logs they seem to have nothing to do with this PR, but a configuration issue in CI test infrastructure. I'm wondering if it's possible to manually apply `ready-to-pull` label?", "@whchung Can you please resolve the conflict? Thanks!", "@whchung Can you please resolve the conflict? Thanks!", "> @whchung Can you please resolve the conflict? Thanks!\r\n\r\nGentle ping", "quite a few stuffs have changed since the early form of the PR. let me examine the failed logs more closely and revise the PR.", "@rthadur could you give another go for this PR? thanks. the failures on \"Ubuntu Python3 PIP\" and \"Windows Bazel\" don't seem to be related to this PR.", "ping?", "@whchung request to re base your branch", "@rthadur sorry for being late. I've rebased the PR.", "@smit-hinsu , @qqfish ping?", "invite @yifef to this PR as it's closely related to #24295 which was just merged", "Correctly tagging per @whchung comment. /cc @yifeif :)", "Assigning the PR to @tatianashp and @thirupalanisamy as this PR is now blocking upstreaming some TensorFlow operators for ROCm.", "> In subsequent PRs, ROCm-specific changes for GPU device functions and GPU kernel invocation logic / macro would be introduced.\r\n\r\nWen-Heng, would you mind pointing me to the PRs/code that is going to come after this one. It doesn't need to be complete, just give me an idea what's going to come. As it stands, renaming the files in this PR looks like unnecessary churn because they still provide CUDA-only functionality. I assume it makes more sense once everything is merged, and I would like to get idea what that will look like.", "@chsigg thanks. I\u2019m working on a PR to demonstrate this. It\u2019s taking a little more that I wanted as recently TF tip has migrated from <<<>>> to CudaLaunchKernel so our downstream fork needs to be changed a bit. I\u2019ll manage to get a PR submitted today. In fact I think I might just amend this PR.", "@chsigg I've revised this PR to put (many) more meats in. Macros / functions names were changed to be more vendor-neutral, and some ROCm-specific device function implementation were added.\r\n\r\nI haven't changed kernel launching logic `CudaLaunchKernel` in this PR yet. I'd like to submit it as another PR, as this PR has already become pretty big.", "@chsigg I've fixed all the issues from CI tests. Some failures in `Linux GPU` test target don't seem to be related to this PR. Would you mind take another look at this PR? Thanks!", "@chsigg ping?", "I'm still not sold that renaming these files and function names provides real value. For example \r\n> `data += GpuShuffleXorSync(kCudaWarpAll, data, delta);`\r\n\r\nis not HIP-ready, so it might be better to keep the original CUDA name.\r\n\r\nWould you be ok with getting the _functionality_ in first, i.e. the #ifdef blocks to compute launch configs? The fact that the file and function names contain CUDA seems a cosmetic detail that we can overlook for now.", "@chsigg all the implementation details to get ROCm path functional have already been added inside this PR:\r\n\r\n- use device intrinsics on AMDGPU to compute shuffles\r\n- introduce GPU_LAUNCH_KERNEL to cope with differences in kernel launch syntax between CUDA and ROCm\r\n- use #if blocks to distinguish differences between ROCm and CUDA runtime API calls, especially mark those places yet supported by HIP runtime APIs.\r\n\r\n- and finally terminology changes (Cuda -> GPU)\r\n\r\nI\u2019m wondering could you do another round of review, especially pay attention to those files in the bottommost of this PR where most of these functionality changes take place?", "@chsigg I should also mention that the purpose for this PR to is lay infrastructure for subsequent PRs for other TF operators which invokes custom HIP GPU kernels. Therefore the focus of this PR is not on the operators, but on the utility functions.\r\n\r\nTake `GpuShuffleXorSync` as an example, you can find ROCm-specific logic in `gpu_device_functions.h`, but you won't (yet) find ROCm-specific logic in TF operators which depend on this utility function. I took this route to avoid breaking upstream implementation that much.\r\n\r\nShall this PR be merged, we'll then file additional PRs to enable those operators supported on ROCm incrementally like other PRs submitted and merged recently.", "@chsigg On existing AMD hardware warp (wavefront in AMD terminology) is 64.", "> @chsigg On existing AMD hardware warp (wavefront in AMD terminology) is 64.\r\n\r\nI'm aware of that. My question was:\r\n> Can you run with warp size of 32 (@ half the occupancy), or do we need to handle warp size 64 in the kernel implementations?\r\n", "@chsigg We'll need to handle warp size of 64 in the kernel implementations. Therefore for some operators we do have to change the implementation of some kernels and they will be reflected in subsequent PRs.", "@chsigg I've addressed your review comments.\r\n\r\nAlso a ticket was raised on HIP repo some time ago to request the feature to properly estimate required launch config for HIP kernels: https://github.com/ROCm-Developer-Tools/HIP/issues/924 .", "@chsigg rebased the PR to resolve conflicts.\r\n\r\nAlso it seems now all `<<< >>>` has been removed. I'll change `GPU_LAUNCH_KERNEL` macro to adopt `CudaLaunchKernel`.", "I've been trying to merge this PR, but the file and symbol renames are tricky, because we have a lot of internal references to them as well. It will be easier if I do the rename using internal tools. Then this PR will essentially add the macros and ROCm specific bits. Does that sound acceptable to you?", "@chsigg sure thing", "@chsigg Just want to understand where we are getting this PR merged?", "I'm in the process of renaming the files still. This is not trivial because we have many dependencies on these internally, some of which I don't have access to, so I need to ask others to make the change.\r\n\r\nTo unblock you, I've submitted a change so that both set of files exist, one forwarding to the other. I've not started renaming any symbols yet, but we will be in a similar situation that this is best done from our side. My recommendation is to update this PR to use the old symbols but #include the new headers, and trim it down to the functional changes that you would like to add. ", "> I'm in the process of renaming the files still. This is not trivial because we have many dependencies on these internally, some of which I don't have access to, so I need to ask others to make the change.\r\n> \r\n> To unblock you, I've submitted a change so that both set of files exist, one forwarding to the other. I've not started renaming any symbols yet, but we will be in a similar situation that this is best done from our side. My recommendation is to update this PR to use the old symbols but #include the new headers, and trim it down to the functional changes that you would like to add.\r\n\r\ngot it. let me revise the PR and follow your suggestion.\r\n\r\n/cc @jerryyin for awareness", "@whchung Thanks, next weeks merging should be fun ...", "@chsigg revised this PR so preserve existing symbols, while introducing ROCm-specific logic.", "@chsigg a gentle ping. could you advise steps remaining to get the PR merged", "The PR description looks outdated. Would you mind to update it?", "Thanks for the feedback. Let me try revise the PR according to your comments."]}]