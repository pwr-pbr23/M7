[{"number": 45011, "title": "Cannot run GPU on tensorflow Docker Image with Tensorflow 2.3.1", "body": "**System information**\r\n- OS Platform and Distribution: **[Tensorflow Docker Image](https://hub.docker.com/r/tensorflow/tensorflow/)**:\r\n- TensorFlow installed from (source or binary): **`python3 -m pip install tensorflow-gpu`**\r\n- TensorFlow version: **2.3.1**\r\n- Python version: **3.7.5**\r\n- Installed using virtualenv? pip? conda?: **pip (but it comes by default on docker image as far as I know)**\r\n- GCC/Compiler version (if compiling from source): `7.5.0`\r\n- CUDA/cuDNN version: ```nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243```\r\n- GPU model and memory: **NVIDIA Tesla T4, 16GB. Created using `Compute Engine Deep Learning VM API` on Google Cloud Platform.**\r\n\r\n\r\n\r\nAfter deploying a GCP Compute Engine Deep Learning VM, I am trying to use docker-compose to train a model using GPU.\r\n\r\nInside docker, I execute:\r\n```\r\npython3 -c \"import tensorflow as tf; tf.test.is_gpu_available()\"\r\n```\r\n\r\nWhich in output (`False`), there is this line: (Full output available at the end)\r\n\r\n`Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64`\r\n\r\nSomething is happening that TensorFlow GPU 2.3.1 cannot lead this `libcublas.so.10` library.\r\n\r\nOutside docker, this command returns `True`.\r\n\r\nHere's my docker-compose and Dockerfile:\r\n\r\n\r\n**docker-compose**, `version: 2.3`:\r\n```\r\n    analyses-gpu:\r\n        container_name: analyses-gpu\r\n        runtime: nvidia\r\n        build:\r\n            context: .\r\n            dockerfile: .dockerfiles/DockerfileGPU\r\n        working_dir: /app\r\n        volumes:\r\n            - ./:/app\r\n        environment:\r\n            - NVIDIA_VISIBLE_DEVICES=all\r\n        command: \"bash -c 'cd analyses/; make run'\"\r\n```\r\n\r\n**Dockerfile**:\r\n```\r\nFROM tensorflow/tensorflow:latest-gpu\r\n\r\nWORKDIR /app\r\n\r\nRUN apt-get update && apt-get upgrade -y\r\nRUN apt-get install -y git g++ && \\\r\n    apt-get install -y build-essential\r\n\r\n# Update Python\r\nRUN apt-get install -y python3.7 python3-dev\r\nRUN update-alternatives --install /usr/local/bin/python python /usr/bin/python3.7 1\r\nRUN update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.7 1\r\n```\r\n\r\nFull output mentioned:\r\n```\r\n2020-11-19 15:04:46.263566: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nWARNING:tensorflow:From <string>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-11-19 15:04:47.399117: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-19 15:04:47.409370: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\r\n2020-11-19 15:04:47.411095: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56755c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-11-19 15:04:47.411130: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-11-19 15:04:47.414750: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-11-19 15:04:48.394486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-19 15:04:48.395282: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56e3690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-11-19 15:04:48.395322: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\r\n2020-11-19 15:04:48.395600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-19 15:04:48.396263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-11-19 15:04:48.396307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-19 15:04:48.396762: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2020-11-19 15:04:48.398793: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-11-19 15:04:48.399183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-11-19 15:04:48.401310: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-19 15:04:48.402616: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-11-19 15:04:48.407285: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-11-19 15:04:48.407319: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-11-19 15:04:48.407350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-19 15:04:48.407369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-11-19 15:04:48.407384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\nFalse\r\n```\r\n", "comments": ["I had to add \"**capabilities=compute,utility**\" to the **gpus** flag while running the image. I had the same issue for tensorflow-gpu==1.15 custom installed in a docker image. Just like you said, tensorflow-gpu was able to find libcuda outside a docker container, but unable to do so from inside the container.\r\n\r\nchange the following commad\r\n`docker run --gpus all image_name bash`\r\n\r\nto \r\n`docker run --gpus 'all,\"capabilities=compute,utility\"' image_name bash`\r\n\r\n[This](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html) is the reference I followed.\r\n", "Closing this issue since its resolved. Feel free to update the thread if still have problems. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45011\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45011\">No</a>\n"]}, {"number": 45010, "title": "Fix for TF Lite verifier, arg_min_max op and lsh_projection_test on Big Endian machine", "body": "This PR consists of 2 fixes.\r\nThe first fix is in `verifier.cc`, as discussed in https://github.com/tensorflow/tensorflow/issues/44852.\r\nThe second fix is for retrieving `axis_value` data in `arg_min_max.cc`. Since the `axis` could be of type `int64`, it will cause an issue when retrieving it as int32 on Big Endian machines - It only retrieves the first 4 bytes which are the most significant bytes on Big Endian machines. Retrieve it as int64 at first and then convert it to int32 will be safer and more consistent.\r\n\r\nEdit:\r\nPush another fix for `lsh_projection_test`, as discussed in https://github.com/tensorflow/tensorflow/issues/44982.", "comments": ["Could you please review this PR @wangtz ? Thank you.", "Hi @talumbau , I just pushed the fix for test case `lsh_projection_test` as you suggested. I use the macro `defined(__s390x__)` instead of `__BYTE_ORDER__ == __ORDER_BIG_ENDIAN__` just to be safe, as I only tested it on s390x so I am not sure whether other Big Endian machines will have the same hash output. This could be generalized once tested on other architectures. Please let me know if you have other suggestions or questions, thank you.", "Make a new commit for `nnapi_delegate_test.cc` as it has part of the exact same test content as `lsh_prejoection_test`, fix it with the same approach."]}, {"number": 45008, "title": "Chinese docs on .org site should not link to .cn site", "body": "https://github.com/tensorflow/docs/blob/master/tools/tensorflow_docs/tools/nblint/style/tensorflow_docs_l10n.py sets up a rule that in Chinese docs, replace links from .org to .cn. (the change is from https://github.com/tensorflow/docs/commit/07608d0e273157c104920286ebd596f1b766909f)\r\n\r\nI always browse Chinese docs on https://www.tensorflow.org , there are so many traps in the docs that extradites me to the .cn site.\r\n\r\nThough I'm a Chinese reader, I may or may not have associations with China. Remember Malaysian, Singapore people use Simplified Chinese, as well as American (or wherever) born Chinese. I hope to use the international site.\r\n\r\nI think the establishment of the .cn site is a call from the company strategies, but this should be mainly for readers who are in China. For those who are visiting the international site, they can keep visiting the international site, you don't need to take them back to China.\r\n\r\nIf Google wants to keep letting people in China visiting the .cn site, can you use relative URL?", "comments": ["Thanks, @gqqnbig. This is WAI but will consider for future planning.\r\n\r\n> can you use relative URL?\r\n\r\nWe use relative URLS but this is not always possible because of the way the site is assembled from multiple GitHub repos. And we've heard from contributors that they prefer links that also work in GitHub, as well as Colab. It depends if the link is \"crossing the subsite boundary.\" But the site navigation is relative links so you can always use that to move around on the same domain.\r\n\r\nIt's more of a useability issue: the `tensorflow.google.cn` proxy works everywhere but `www.tensorflow.org` doesn't work in China. Since most site requests for Chinese language pages come from China, we've decided to make sure those users aren't hitting 404s. You may prefer not to use the `tensorflow.google.cn` proxy, but at least the page works.\r\n\r\nIf you are able to locally replace `tensorflow.google.cn` => `www.tensorflow.org`, that would work, too.\r\n\r\n", "I'm sorry. While I did find bugs in Tensorflow Chinese docs, due to the current rule, I won't contribute to it."]}, {"number": 45007, "title": "Mistakes in the documentation of UniqueWithCountsV2 API", "body": "## URL(s) with the issue: \r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/UniqueWithCountsV2?hl=ca#for_example\r\n\r\n## Description of issue (what needs changing): \r\n1. The Argument, `Axis` is provided directly as a `Scalar` but it should be a `Tensor`. \r\n`y, idx, count = unique_with_counts(x, axis=0)`\r\n2. The argument, `Axis` is missing in the **`Example`** but it is Mandatory\r\n`y, idx, count = unique_with_counts(x)`\r\n3. Name of the API, `unique_with_counts` is confusing. It can be changed to `UniqueWithCountsV2`\r\ny, idx, count = unique_with_counts(x)\r\n\r\nPlease find the [Colab Gist](https://colab.research.google.com/gist/rmothukuru/1e10fb3b9006d3ea8735b950f87e5667/44788.ipynb) which demonstrates all the errors.\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? : Link is not mentioned\r\n \r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? : Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : Yes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises : No\r\n\r\n### Usage example\r\n\r\nIs there a usage example? : Yes\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": []}, {"number": 45006, "title": "Building custom op failed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, I'm building custom op\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu server 20.04 ( connected by SSH from windows 10 )\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: Python 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen I compile, I have error ( it's tensorflow error ). Command I use: \r\n`\r\ng++ -std=c++11 -shared ops.cc -o ops.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2\r\n`\r\n**Describe the expected behavior**\r\nJust compile ops.cc ( ops_kernel.h ) to ops.so and use it in my custom layer\r\n**Other info / logs** \r\nError:\r\n![image](https://user-images.githubusercontent.com/27022263/99662085-75239480-2a6d-11eb-915a-71840cfed526.png)\r\nMy code:\r\nops.cc\r\n```cpp\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"ops_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ConvBuild\")\r\n\t.Input(\"input: float32\")\r\n\t.Input(\"shape: int32\")\r\n\t.Output(\"output: int32\")\r\n\t.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n    \tauto input = c->input(0);\r\n    \tauto output = c->MakeShape({\r\n    \t\tc->Dim(input, 1), \r\n    \t\tc->kUnknownDim, \r\n    \t\tc->kUnknownDim, \r\n    \t\tc->Dim(input, 4)\r\n    \t});\r\n\t\tc->set_output(0, output);\r\n\t\treturn Status::OK();\r\n\t})\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ConvBuild\").Device(DEVICE_CPU), ConvBuildOp);\r\n```\r\nops_kernel.h\r\n```cpp\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nclass ConvBuildOp : public OpKernel {\r\n public:\r\n  explicit ConvBuildOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the inputs\r\n    const Tensor& input_tensor = context->input(0);\r\n    const auto input = input_tensor.flat<float>();\r\n    const Tensor& shape_tensor = context->input(1);\r\n    const auto shape = shape_tensor.flat<int32>();\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, {input_tensor.dim_size(1), input_tensor.dim_size(2)*shape(0), input_tensor.dim_size(3)*shape(1), input_tensor.dim_size(4)},\r\n                                                     &output_tensor));\r\n\r\n    auto output = output_tensor -> flat<float>();\r\n\r\n    // Compute\r\n    for (int batch = 0; batch < input_tensor.dim_size(1); batch++) {\r\n      for (int yMain = 0; yMain < input_tensor.dim_size(2); yMain++) {\r\n        for (int xMain = 0; xMain < input_tensor.dim_size(3); xMain++) {\r\n          for (int ySub = 0; ySub < shape(0); ySub++) {\r\n            for (int xSub = 0; xSub < shape(1); xSub++) {\r\n              for (int channel = 0; channel < input_tensor.dim_size(4); channel++){\r\n                int offset_in = (ySub*shape(0) + xSub) * input_tensor.dim_size(1) * input_tensor.dim_size(2) * input_tensor.dim_size(3) * input_tensor.dim_size(4) +\r\n                    batch * input_tensor.dim_size(2) * input_tensor.dim_size(3) * input_tensor.dim_size(4) +\r\n                    yMain * input_tensor.dim_size(3) * input_tensor.dim_size(4) +\r\n                    xMain * input_tensor.dim_size(4) +\r\n                    channel;\r\n                int offset_out = batch * (input_tensor.dim_size(2) * shape(0)) * (input_tensor.dim_size(3) * shape(1)) * input_tensor.dim_size(4) +\r\n                       (yMain*shape(0)+ySub) * (input_tensor.dim_size(3) * shape(1)) * input_tensor.dim_size(4) +\r\n                       (xMain*shape(1)+xSub) * input_tensor.dim_size(4) +\r\n                       channel;\r\n                output(offset_out) = input(offset_in);\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n};\r\n```", "comments": ["So I made some changes, and build works now. I've added `-D_GLIBCXX_USE_CXX11_ABI=0` flag, renamed files and created directories ( below ). Also I've moved `REGISTER_KERNEL_BUILDER` from `builder.cc` to `cpu.h`. And it works!\r\nFiles structure:\r\n```\r\nmain/\r\n\u2514\u2500\u2500 builder/\r\n    \u251c\u2500\u2500 kernels/\r\n    \u2502   \u2514\u2500\u2500 cpu.h\r\n    \u2514\u2500\u2500 builder.cc\r\n```\r\nCommand, I use:\r\n```sudo g++ -std=c++11 -shared builder/builder.cc -o builder.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2 -D_GLIBCXX_USE_CXX11_ABI=0```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45006\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45006\">No</a>\n"]}, {"number": 45004, "title": "2.4.0rc1 not supporting RTX 3090? #44969   Update efficientnet_weight_update_util.py", "body": "updated the tensorflow command ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45004) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@googlebot I signed it!", "@googlebot I signed it!", "Looks like a local environment issue."]}, {"number": 45003, "title": "ResourceExhaustedError", "body": "Can somebody help me with this code\r\n![Screenshot 2020-11-19 045638](https://user-images.githubusercontent.com/74709388/99650677-b3946180-2a23-11eb-8015-b40159cf5064.png)\r\n ", "comments": ["@Danishzulfquar \r\nIn order to expedite the trouble-shooting process, could you please provide the TensorFlow version, the complete code to reproduce the issue and the dataset you are using.\r\n\r\nAlso if you're using a GPU, try limiting GPU memory growth as per [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if it helps.\r\nYou may also refer to these links and et us know, [link](https://github.com/tensorflow/tensorflow/issues/44118#issuecomment-714287471), [link1](https://github.com/tensorflow/tensorflow/issues/42257), [link2](https://github.com/tensorflow/tensorflow/issues/41888)\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45003\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45003\">No</a>\n"]}, {"number": 45002, "title": "Leak memory on ios", "body": "I need virtual background on video call and I use image segmentation on frame by frame. \r\nI use Twilio to create video call and get frame video, opencv to edit Image and Tensorflow-lite to use model deeplabv3\r\n\r\nBut memory leak on some iPhone . To free up memory, I destroy the model and interpreter  after I add background to frame and \u00a0allocate a new Interpreter each time when I see a new frame. it\u2019s bad ideal. How can I release memory??\r\n*sorry about my english, it's bad*\r\n", "comments": ["@ravikyram please, help me\r\n", "@NALanhnt2 \r\n\r\nPlease, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45002\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45002\">No</a>\n"]}, {"number": 45001, "title": "Error when trying to build from source on on CUDA 11 & Cudnn 8", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubunutu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1, 8.0.5\r\n- GPU model and memory: RTX 3070 8gb\r\n\r\n**Describe the problem**\r\nCannot build tensorflow 2.3 from source on with CUDA 11.1 and cudnn 8\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nfollowed the instructions from: https://www.tensorflow.org/install/source\r\n\r\n./config options are all default except cuda which was enabled \r\n\r\n**Any other info / logs**\r\nbazel build --config=cuda --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda, v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/michael/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/michael/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /home/michael/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --action_env LD_LIBRARY_PATH=/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64: --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/michael/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/michael/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file /home/michael/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/michael/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:cuda in file /home/michael/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/michael/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:v2 in file /home/michael/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /home/michael/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/michael/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Repository local_config_cuda instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule cuda_configure defined at:\r\n  /home/michael/tensorflow/third_party/gpus/cuda_configure.bzl:1399:18: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1051, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 598, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 500, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nNo library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1\r\nINFO: Repository rules_cc instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule tf_http_archive defined at:\r\n  /home/michael/tensorflow/third_party/repo.bzl:134:19: in <toplevel>\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1051, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 598, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 500, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nNo library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1051, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 598, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/gpus/cuda_configure.bzl\", line 500, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/home/michael/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nNo library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1\r\nINFO: Elapsed time: 0.456s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n\r\n\r\n", "comments": ["I don't think there is support for CUDA 11 on version 2.3. Try using 2.4.0rc1", "@Macaque2077,\r\nEvery TensorFlow release is compatible with certain CUDA and cuDNN version. TensorFlow v2.3 is compatible with CUDA 10.1 and cuDNN 7.6.\r\n\r\nAs @Harsh188 said you'll have to use any of the TF v2.4 rc version or TF-nightly with CUDA 11 and cuDNN 8. Thanks!\r\n", "you have to use cuda 10.1", "> I don't think there is support for CUDA 11 on version 2.3. Try using 2.4.0rc1\r\n\r\nThanks, I installed TensorFlow==2.4.0rc2 and it appears to be recognising the GPU and opening all the CUDA and cuDNN libraries without error.\r\n\r\nUsing:\r\ntf.test.is_gpu_available(\r\n    cuda_only=False, min_cuda_compute_capability=None\r\n)\r\n", "@Macaque2077 if it's working for you could you kindly close this issue? Also if you have the time could you look into #45044. Thanks.", "> Thanks, I installed TensorFlow==2.4.0rc2 and it appears to be recognising the GPU and opening all the CUDA and cuDNN libraries without error.\r\n\r\n@Macaque2077,\r\nThank you for the update. Marking this issue as closed, as it is resolved. Please feel free to re-open if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45001\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45001\">No</a>\n"]}, {"number": 45000, "title": "Softnms", "body": "Align soft NMS implementation with paper.\r\ncc @mingxingtan ", "comments": ["This is getting rolled back as it breaks an internal test with a very large delta\r\n\r\n```\r\nMismatched value: a is different from b. \r\nnot close where = (array([3, 3]), array([1, 3]))\r\nnot close lhs = [100. 101.]\r\nnot close rhs = [0.1 1.1]\r\nnot close dif = [99.9 99.9]\r\nnot close tol = [0.011 0.021]\r\ndtype = float32, shape = (4, 4)\r\nMismatch: 12.5%\r\nMax absolute difference: 99.9\r\nMax relative difference: nan\r\n x: array([[0.000e+00, 1.000e+01, 1.000e+00, 1.100e+01],\r\n       [0.000e+00, 0.000e+00, 1.000e+00, 1.000e+00],\r\n       [0.000e+00, 1.000e+03, 1.000e+00, 1.002e+03],\r\n       [0.000e+00, 1.000e+02, 1.000e+00, 1.010e+02]], dtype=float32)\r\n y: array([[0.000e+00, 1.000e+01, 1.000e+00, 1.100e+01],\r\n       [0.000e+00, 0.000e+00, 1.000e+00, 1.000e+00],\r\n       [0.000e+00, 1.000e+03, 1.000e+00, 1.002e+03],\r\n       [0.000e+00, 1.000e-01, 1.000e+00, 1.100e+00]])\r\n```"]}, {"number": 44999, "title": "I tried installing tensor flow 2 on new MacBook Air (M1), but getting this error \"zsh: illegal hardware instruction  python\" when I import it", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["TF hasn't offically supported M1 yet. However, you could install TensorFlow from https://github.com/apple/tensorflow_macos", "@sumeshthakr \r\nThe Apple TF on M1 chips is a private fork of TF owned by Apple. We don't have access to fixing code issues there, as per comment in#44751, hence please move this to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44999\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44999\">No</a>\n", "Although the tensorflow for arm64 is only available at apple's private fork,\r\nI think we need x86_64 package that works on top of the mac's rosetta2 emulation.", "Same issue happened even with using \r\n\r\n`pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.3.0-cp38-cp38-macosx_10_14_x86_64.whl`\r\n\r\nas mentioned in https://www.tensorflow.org/install/pip#package-location\r\n\r\ntensorflow is shown on listing packages using pip list or conda list but you can't import in Python", "none of the above worked for me. I tried uninstalling the versions of python on my system (not the native ones), uninstalling x-code command line tools and reinstalling, but I continued to get the \"illegal hardware instruction\" error.\r\n\r\nhowever, I temporarily downgraded to tensorflow 1.5.0 (and used matching versions of keras, etc.) and the errors went away.\r\n\r\n### Edit: found a solution\r\n\r\nMake sure you do _not_ open your terminal with rosetta. create a new virtual env, and run the install script from the downloaded tar https://github.com/apple/tensorflow_macos/releases"]}, {"number": 44998, "title": "[CherryPick:r2.4] add per_replica support for keras", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44998) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44997, "title": "TensorFlow Lite error on iOS: \"Make sure you apply/link the Flex delegate before inference.\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15, iPhone Simulator 12\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone Simulator 12\r\n- TensorFlow installed from (source or binary): Installed from Cocoa Pod using the following specification:\r\n\r\n  ```\r\n  pod 'TensorFlowLiteSwift', '0.0.1-nightly.20200916', :subspecs => ['CoreML', 'Metal'] \r\n  pod 'TensorFlowLiteSelectTfOps', '0.0.1-nightly.20200916'\r\n  ```\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I load my model using TensorFlow Lite and attempt to invoke it (`interpreter.invoke()`), I get the following error:\r\n\r\n```\r\n2020-11-18 16:08:54.145490-0800 OpenRDTCV[13328:3534503] Initialized TensorFlow Lite runtime.\r\n2020-11-18 16:08:56.458094-0800 OpenRDTCV[13328:3534503] Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n2020-11-18 16:08:56.458232-0800 OpenRDTCV[13328:3534503] Node number 312 (FlexSize) failed to prepare.\r\n```\r\n\r\nI have read and followed [Select TensorFlow operators](https://www.tensorflow.org/lite/guide/ops_select) and added the `-force_load` flag (see screenshot below).\r\n\r\n![Screen Shot 2020-11-18 at 4 19 39 PM](https://user-images.githubusercontent.com/926408/99604269-eb66bf00-29b9-11eb-9d51-5664451759a6.png)\r\n\r\nI have also tried a variety of versions of TensorFlowLiteSwift/TensorFlowLiteSelectTfOps. However, the error persists. Any help appreciated!\r\n\r\nHere is the relevant Swift source up until the failing `invoke()` line:\r\n\r\n```swift\r\n    guard let pixelBuffer = CVPixelBuffer.buffer(from: image) else {\r\n      print(\"Could not convert image to CV buffer\")\r\n      return []\r\n    }\r\n    \r\n    let sourcePixelFormat = CVPixelBufferGetPixelFormatType(pixelBuffer)\r\n    assert(sourcePixelFormat == kCVPixelFormatType_32ARGB ||\r\n             sourcePixelFormat == kCVPixelFormatType_32BGRA ||\r\n               sourcePixelFormat == kCVPixelFormatType_32RGBA)\r\n\r\n    let imageChannels = 4\r\n    assert(imageChannels >= 3)\r\n\r\n    let scaledSize = CGSize(width: self.inputSize.width, height: self.inputSize.height)\r\n    guard let thumbnailPixelBuffer = pixelBuffer.centerThumbnail(ofSize: scaledSize) else {\r\n      print(\"Error: could not crop image\")\r\n      return []\r\n    }\r\n\r\n    let interval: TimeInterval\r\n    let outputTensor: Tensor\r\n    do {\r\n      let inputTensor = try interpreter.input(at: 0)\r\n\r\n      // Remove the alpha component from the image buffer to get the RGB data.\r\n      guard let rgbData = rgbDataFromBuffer(\r\n        thumbnailPixelBuffer,\r\n        isModelQuantized: isModelQuantized\r\n      ) else {\r\n        print(\"Failed to convert the image buffer to RGB data.\")\r\n        return []\r\n      }\r\n\r\n      // Copy the RGB data to the input `Tensor`.\r\n      try interpreter.copy(rgbData, toInputAt: 0)\r\n\r\n      // Run inference by invoking the `Interpreter`.\r\n      let startDate = Date()\r\n      try interpreter.invoke()\r\n```\r\n\r\ncc @jdduke who I believe wrote [this commit](https://github.com/tensorflow/tensorflow/commit/05ed82f0f4c00982cd274a2ebc5d0aeb759eebcb) that seems to be relevant.", "comments": ["I tried compiling the select ops framework myself following the [instructions here](https://www.tensorflow.org/lite/guide/ops_select). After including the framework in Xcode, I'm seeing the same error.\r\n\r\nHere are screen shots of my Xcode configuration:\r\n\r\n<img width=\"899\" alt=\"Screen Shot 2020-11-20 at 7 01 04 AM\" src=\"https://user-images.githubusercontent.com/926408/99814914-836dc100-2afe-11eb-8f57-821bde391d0d.png\">\r\n<img width=\"739\" alt=\"Screen Shot 2020-11-20 at 7 00 26 AM\" src=\"https://user-images.githubusercontent.com/926408/99814922-85d01b00-2afe-11eb-9a07-61cb4d2f6fcf.png\">\r\n", "I noticed in the instructions [here](https://www.tensorflow.org/lite/guide/build_ios?hl=fi#modify_xcode_project_settings_directly) that it states: \"...you need to add it as an embedded binary to your app target\". I tried embedding the `TensorFlowLiteSelectTfOps.framework`, like so:\r\n\r\n<img width=\"747\" alt=\"Screen Shot 2020-11-20 at 9 18 04 AM\" src=\"https://user-images.githubusercontent.com/926408/99829866-b79ead00-2b11-11eb-88dc-227b6277b45b.png\">\r\n\r\nI then get an error about a missing `Info.plist` when running the app. I added a dummy `Info.plist` to `TensorFlowLiteSelectTfOps.framework`, which makes this error go away. However, the original error (\"Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\") still happens when I try to run inference.", "@teijeong Any suggestions for how to resolve this issue? Thanks!", "Update: [This section of the Select operators documentation](https://www.tensorflow.org/lite/guide/ops_select#c) suggests that you can build the tensorflow lite library yourself with the select ops included. I attempted to build tensorflow lite myself and include my custom build using the instructions [here](https://www.tensorflow.org/lite/guide/build_ios). However, I am struggling with these instructions; I believe they are incorrect or incomplete. I filed a separate issue [here](https://github.com/tensorflow/tensorflow/issues/45113).", "@MarkDaoust you were helpful solving [my other problem with the documentation](https://github.com/tensorflow/tensorflow/issues/45113). We haven't heard anything on this issue yet, maybe you have some insight? Thanks!", "@yyoon, you did a lot of work on that doc, do you know how to fix this?", "Hi @alexdmiller, is there any chance that you're experiencing that behavior only in release mode?\r\nI wonder if this is the same issue as #44891.\r\ncc/ @thaink ", "@yyoon thanks for the suggestion.\r\n\r\nI am not very familiar with Xcode and I'm not sure how to tell what mode I'm running the app in. I tried the various options under the \"play\" button, but I get the same error:\r\n\r\n<img width=\"221\" alt=\"Screen Shot 2020-11-25 at 12 06 00 PM\" src=\"https://user-images.githubusercontent.com/926408/100276618-ad681e80-2f16-11eb-8d30-056c3595eef3.png\">\r\n\r\nAlso, I should add that I made sure to include the linker flags for both \"debug\" and \"release\" under build settings.", "I noticed that the documentation for using CocoaPods has changed slightly ([link](https://www.tensorflow.org/lite/guide/ops_select#using_cocoapods)). It now says to add two flags to Xcode's `Other Linker Flag`s:\r\n\r\n```\r\n-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps\r\n-u _TF_AcquireFlexDelegate\r\n```\r\n\r\nI changed my Podfile to use the nightly TensorFlow build:\r\n\r\n```\r\npod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'\r\npod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'\r\n```\r\n\r\nNow I'm seeing this error:\r\n\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"_TF_AcquireFlexDelegate\", referenced from:\r\n     -u command line option\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```", "Okay, I just noticed something else in the build logs I hadn't noticed before:\r\n\r\n```\r\nIgnoring file Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps, missing required architecture x86_64 in file Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps (2 slices)\r\n```\r\n\r\nThis definitely seems to be the culprit.\r\n\r\nI'm attempting to run my app in the Simulator on my Macbook \u2014\u00a0is TensorFlowLiteSelectTfOps only built for the phone itself?", "Yes, unfortunately we had to remove the `x86_64` architecture support from TensorFlowLiteSelectTfOps framework, because of an issue with CocoaPods: https://github.com/CocoaPods/CocoaPods/issues/8786.\r\nSorry for this information not being reflected correctly on the official documentation. Will update the doc soon.\r\n\r\nIf you need to test it with your x86_64 simulator, you would need to [build the framework yourself](https://www.tensorflow.org/lite/guide/ops_select#using_bazel_xcode). The following build command should work:\r\n```\r\nbazel build -c opt --config=ios_x86_64 \\\r\n  //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework\r\n```", "Thanks!\r\n\r\nI was able to get unblocked by running things on an iOS device.\r\n\r\nI tried building the framework myself, but I ran into other issues. However, I think those are separate problems and we can close this issue. An update to documentation would be great! Thanks again!", "Thanks. FYI, I've already added a note in the doc saying that the prebuilt Select TF ops for iOS doesn't work on x86_64 simulator.\r\nhttps://www.tensorflow.org/lite/guide/ops_select#ios", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44997\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44997\">No</a>\n", "@yyoon Sorry to revive this issue, but I'm running into it again.\r\n\r\nEven though my app works correctly when I build it within Xcode, if I build the app using `xcodebuild` on the command line, I'm seeing the same error:\r\n\r\n```\r\nRegular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nNode number 312 (FlexSize) failed to prepare.\r\n```\r\n\r\nHere is the `xcodebuild` command I'm using:\r\n\r\n```\r\n$ xcodebuild -workspace ./MyApp.xcworkspace -scheme MyApp -destination 'generic/platform=iOS' archive -archivePath test-build\r\n```\r\n\r\nBy running the following command, I double checked that the compiler flags I set within Xcode are being respected by `xcodebuild`. (Note that I've redacted much of this command, but the important part is that we're seeing both the `-force-load` and the `-u _TF_AcquireFlexDelegate` flags.)\r\n\r\n```\r\n$ xcodebuild -showBuildSettings -workspace ./MyApp.xcworkspace -scheme MyApp\r\n...\r\nOTHER_LDFLAGS = -force_load Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps -u _TF_AcquireFlexDelegate\r\n...\r\n```\r\n\r\nI'm not sure what else could cause the `xcodebuild` app not to load the select op delegates. Any tips would be appreciated.", "I found [this document](https://gist.github.com/mtauraso/95a49eeb8e5b4335dd2b) that states:\r\n\r\n> ACTION: build, test, archive\r\n> \r\n> What these do is set up in \"Edit Scheme\" inside of Xcode.app. Archive is special in that it uses stricter header + library search paths. Sometimes the library search paths are sloppy because a sub-project has SKIP_INSTALL set to \"false\". When \"SKIP_INSTALL\" is false, libraries are copied during the build action. This copying doesn't happen on archive action resulting in a failure to find the library.\r\n\r\nIf you notice in the `xcodebuild` command I ran above, I am doing an archive build. Sure enough, the archive build generated from Xcode also fails in the same way (with the `Regular TensorFlow ops are not supported by this interpreter.` error).\r\n\r\nSo it seems that building an archive does not include the select ops.\r\n\r\nI'll continue to investigate but I'm not sure how to fix this. Again, any help appreciated! Thank you!", "Possibly related: https://github.com/tensorflow/tensorflow/issues/44879.\r\nWhich exact version of TFLite and Select Tf Ops framework are you currently using?\r\n\r\ncc: @thaink ", "#44879 seems directly related, that is my exact situation (using TestFlight). I will look at that thread tomorrow morning and see if the fix works for me.", "@yyoon I followed the instructions in #44879. What ended up ultimately working for me was (a) setting \"Strip Linked Product\" in Xcode to \"No\" and (b) updating to the latest version of the library.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44997\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44997\">No</a>\n"]}, {"number": 44994, "title": "Doc py_function unknown shape", "body": "Try to lower the rate of some recurrent tickets.\r\nSee https://github.com/tensorflow/tensorflow/issues/44970#issuecomment-729959298", "comments": []}, {"number": 44993, "title": "Include C logging API into part of the libtensorflow_framework.so", "body": "This PR includes C logging API into part of the libtensorflow_framework.so\r\nas otherwise the file system plugins will not be able to link to TF_Log\r\nto send logs to tensorflow logging system.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 44992, "title": "[Cherrypick:r2.4] add per_replica support for keras", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44992) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44991, "title": "[INTEL MKL] MKL DNN 0.x code clean -- pooling ops", "body": "DNN 0.x cleanup of MKL Avg/Max pooling ops and related base classes/utilities:\r\n\r\n(1) Remove all DNN 0.x related code;\r\n\r\n(2) Replace all DNN 1.x macro usages", "comments": ["@gzmkl  Can you please resolve conflicts? Thanks!", "@gbaned merge conflicts should have been addressed. thanks"]}, {"number": 44990, "title": "tf-gpu 2.3.1 get initial low accuracy compare to tf-gpu 2.1", "body": "I installed tf-gpu 2.3.1 and ran with gpu 1080ti , it get much lower accuracy 0.35 to begin with comparing 0.7 on my previous tf-gpu 2.1,   the loss value are similar to begin with around 1.4\r\nin 2.3.1 reporting \r\n1670/1670 - 312s - loss: 1.3193 - accuracy: 0.3429\r\nEpoch 2/12\r\n1670/1670 - 295s - loss: 0.9801 - accuracy: 0.4468\r\nEpoch 3/12\r\n1670/1670 - 295s - loss: 0.9100 - accuracy: 0.4869\r\nEpoch 4/12\r\n1670/1670 - 295s - loss: 0.8578 - accuracy: 0.5113\r\n\r\nin 2.1 reporting,\r\n418830/418830 - 271s - loss: 1.4756 - accuracy: 0.6926\r\nEpoch 2/12\r\n418830/418830 - 229s - loss: 1.0229 - accuracy: 0.7290\r\nEpoch 3/12\r\n418830/418830 - 230s - loss: 0.9297 - accuracy: 0.7491\r\nEpoch 4/12\r\n418830/418830 - 229s - loss: 0.8621 - accuracy: 0.7627\r\nEpoch 5/12\r\n\r\nSo anything like that was noticed? and anything is wrong ? or any explanation?\r\n\r\nThank you for any clues!\r\n \r\n\r\n\r\n", "comments": ["@zzhulm \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Thanks for asking but right now it is hard to forward this real project, I can configure some simply one later.\r\nhere is some update,\r\nI tried to run without GPU, the results is similar to 2.3.1 version, which means version 2.1 have some problem to count the accuracy? and in 2.3 it was fixed? the loss value seems similar, so the question is,\r\neven if in tf-gpu 2.1 the accuracy may mis-calculated , the whole learning algorithm with the GPU is still  working  right?\r\nI can conform in my case they are learning and the results make sense.\r\nThanks\r\n", "@zzhulm \r\n\r\nCould you please share sample code to investigate further to find the root cause of the issue.Thanks!\r\n", "As i said, my real project code can't be released, but i am trying to get some other code , but so far the simply example did'r show the problem yet, will try others. Thank you. ", "me too.\r\nSo I have changed tf-version to 2.2.0.\r\nThen, the performance is improved. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44989, "title": "ValueError: Cannot feed value of shape (1, 384, 513, 3) for Tensor", "body": "I am new to tensorflow,\r\nI have generated graph using colab.\r\nFor using the graph further, \r\nI am following this code snippet --[https://gist.github.com/averdones/b94e4eb335be356482f1bc1b7f7b15f3](url)\r\n\r\nIf I run this, I am getting following error:\r\n\r\n**WARNING:tensorflow:From C:\\Users\\SRX9ZY1\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\n2020-11-18 19:31:03.098960: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nTraceback (most recent call last):\r\n  File \"deeplab_demo_webcam_v2.py\", line 171, in <module>\r\n    resized_im, seg_map = model.run(pil_im)\r\n  File \"deeplab_demo_webcam_v2.py\", line 152, in run\r\n    feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\r\n  File \"C:\\Users\\xxx\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 960, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\xxx\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1159, in _run\r\n    (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\r\nValueError: Cannot feed value of shape (1, 384, 513, 3) for Tensor 'train_step/Equal:0', which has shape '()'**\r\n\r\ncan't find fix to this!!!\r\n\r\nHope someone will help!!!", "comments": ["@sailokeshaithagoni,\r\nPlease take a look at [this comment](https://stackoverflow.com/a/45966380) from a similar StackOverflow query and check if it works.\r\n\r\nAlso, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 44988, "title": "keras model fit with ragged tensors fails in LossesContainer.__call__", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nhttps://colab.research.google.com/drive/1KL1d47PVpXNFNfpOl-COeqP8g_t5ESHS?usp=sharing\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\ncolab and OSX\r\n\r\n- TensorFlow version (use command below):\r\n2.3.1\r\n\r\n- Python version:\r\n3.7\r\n\r\n- Bazel version (if compiling from source):\r\nN/A\r\n\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n\r\n- CUDA/cuDNN version:\r\nN/A\r\n\r\n- GPU model and memory:\r\nN/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n```\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nv2.3.0-54-gfcc4b966f1 2.3.1\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nWhen a ragged tensor is used as the output of a model, model fit generates an exception while evaluating the loss function.\r\n\r\nLossesContainer.__call__ generates a backtrace when attempting to determine the batch_size since tf.shape() cannot be called on a ragged tensor.\r\n\r\n**Describe the expected behavior**\r\nmodel.fit() should support ragged tensors as outputs.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1KL1d47PVpXNFNfpOl-COeqP8g_t5ESHS?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI've modified my local copy of compile_utils.py:212 so that the batch_dim is obtained as:\r\n```\r\n     if batch_dim is None:\r\n        if isinstance(y_t, ragged_tensor.RaggedTensor):\r\n          batch_dim = y_t.bounding_shape()[0]\r\n        else:\r\n          batch_dim = array_ops.shape(y_t)[0]\r\n```\r\n\r\nWith this change, my code example executes and the model trains.\r\n\r\n- traceback:\r\n```\r\nTypeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\r\n        y, y_pred, sample_weight, regularization_losses=self.losses)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:212 __call__\r\n        batch_dim = array_ops.shape(y_t)[0]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:628 shape\r\n        return shape_internal(input, name, optimize=True, out_type=out_type)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:652 shape_internal\r\n        input = ops.convert_to_tensor(input)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1499 convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:338 _constant_tensor_conversion_function\r\n        return constant(v, dtype=dtype, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:264 constant\r\n        allow_broadcast=True)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:282 _constant_impl\r\n        allow_broadcast=allow_broadcast))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:552 make_tensor_proto\r\n        \"supported type.\" % (type(values), values))\r\n\r\n    TypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"Cast_1:0\", shape=(None, 1), dtype=float32), row_splits=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n\r\n```", "comments": ["I ran the code shared and am able to replicate the issue please find the [gist here](https://colab.research.google.com/gist/Saduf2019/01b5310bcc95c25d7aa59ca307a97223/untitled463.ipynb).", "Note that supporting RaggedTensors as targets (e.g., in `model.fit`) is a frequent request, see #44988, #44112, #43591, #43093, #42320, #41810.\r\n\r\nSome partial progress is in pull requests #45060 and #45015, which will allow using custom losses in Keras model (but existing losses like MSE or (S)CE will still not work).", "Fixed by PR #45015 "]}, {"number": 44987, "title": "Necessity of \"strategy.run\" and \"distribute_datasets_from_function\" for ParameterServers", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/ClusterCoordinator\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n1)\r\nIn the example code for `create_per_worker_dataset`, we use the following code snippet:\r\n```\r\ndef per_worker_dataset_fn():\r\n  return strategy.distribute_datasets_from_function(\r\n      lambda x: tf.data.Dataset.from_tensor_slices([3] * 3))\r\n\r\nper_worker_dataset = coordinator.create_per_worker_dataset(\r\n    per_worker_dataset_fn)\r\n```\r\nNotice the usage of `strategy.distribute_datasets_from_function`.  In the example for `fetch`, we do it as follows\r\n```\r\ndef dataset_fn():\r\n  return tf.data.Dataset.from_tensor_slices([1, 1, 1])\r\n\r\ndistributed_dataset = coordinator.create_per_worker_dataset(dataset_fn)\r\n```\r\n\r\nSo in the first example, we use our strategy, while in the second example, we directly plug the dataset into the coordinator. Both works for me when trying it out, but it's not documented what the difference is (or I could not find it). \r\n\r\n2)\r\nAgain, consider the example for `fetch`:\r\n```\r\nstrategy = ...\r\ncoordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\r\n    strategy)\r\n\r\ndef dataset_fn():\r\n  return tf.data.Dataset.from_tensor_slices([1, 1, 1])\r\n\r\nwith strategy.scope():\r\n  v = tf.Variable(initial_value=0)\r\n\r\n@tf.function\r\ndef worker_fn(iterator):\r\n  def replica_fn(x):\r\n    v.assign_add(x)\r\n    return v.read_value()\r\n  return strategy.run(replica_fn, args=(next(iterator),))\r\n\r\ndistributed_dataset = coordinator.create_per_worker_dataset(dataset_fn)\r\ndistributed_iterator = iter(distributed_dataset)\r\nresult = coordinator.schedule(worker_fn, args=(distributed_iterator,))\r\nassert coordinator.fetch(result) == 1\r\n```\r\n\r\nIt's not clearly documented why we need the `strategy.run` indirection of `replica_fn`. For example, in the example code for `create_per_worker_dataset`, we do not use this indirection and instead just return the element. I've also tried the upper example without `strategy.run` and the addition worked fine. So I'm not sure why we need this. Would be great to add this to the documentation.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? Yes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? No", "comments": ["Hi @MaxiBoether, we have a new Parameter Server Strategy tutorial that I think will answer your questions. I will link it here when it goes live. I believe it has already been submitted so that should be very soon.", "Hi @nikitamaia, where can we find the new Parameter Server Strategy tutorial? Would really appreciate it", "Hi @nikitamaia, I was already looking into the latest documentation on the master branch, so the new ParameterServerStrategyV2. I'm not 100% sure if the tutorial is already included in the nightly deployment of the documentation.\r\n\r\nAlthough it would also make sense to be clear in the documentation/docstrings of the source code. Currently, this is not consistent as stated above. So I would be glad if you could explain these two issues, although the tutorial also is much appreciated. I think fixing these inconsistencies will also help me (and others reading the source code in order to maybe contribute) in the shorter term as the tutorial will probably take some more time.", "The new tutorial is live now, you can see it here https://www.tensorflow.org/tutorials/distribute/parameter_server_training\r\n\r\n@MaxiBoether please take a look and let me know if the tutorial clears up the confusion. The tutorial addresses the use of `distribute_datasets_from_function`, as well as `strategy.run`. I will also take a look at the docstrings and see what can be added to bring some more clarity.", "Thank you for uploading the new tutorial. This already cleared up some confusion - but also brought some new one.\r\n\r\nI think what confused me and maybe also some others is the concept of data parallelism in parameter servers. In textbook descriptions (e.g. [1]) that parameter servers like ps-lite [2] rely on (that implement a push-pull key-value-store-like interface), the idea is that the dataset is *split up* between all workers. Now, if we take a look at the \"More about dataset creation\" section in the TensorFlow tutorial and at the code, we see that actually, `dataset_fn` is called for every worker (this is also confirmed by a small MWE that I implemented). Hence, every worker has access to the entire dataset, which - at least in my understanding - goes against the data parallelism paradigm. The data may be shuffled, but still the *entire* dataset needs to be transferred to every worker instead of splitting the dataset up between different workers (as an implementation detail, I wonder when the data is actually transferred to the workers).\r\n\r\nConsidering the tutorial about DistributedDatasets [3], which shows that the dataset should actually be split across the instances, I do not understand why this is the case. Could you maybe clarify that? I think that also confused me when trying the understand `distribute_datasets_from_function`. Thank you so much again for the kind support. \r\n\r\n[1] https://d2l.ai/chapter_computational-performance/parameterserver.html\r\n[2] https://github.com/dmlc/ps-lite\r\n[3] https://www.tensorflow.org/tutorials/distribute/input", "You are correct that in the current implementation, the dataset_fn is called for each worker, and the data is shuffled. This choice was made keeping in mind the programming model of ParameterServerStrategy. In particular, it prioritizes fault tolerance (ie even if a single worker crashes, the others can proceed), so sharding the data makes less sense.\r\n\r\nThat all being said, I think we need to make some updates to the Distributed Input guide to indicate that that the autoshard policy currently does not apply to PSS. There are no plans at the moment to support dataset sharding with PSS, but that could certainly change if if becomes a popular feature request.", "I've added a note to the data sharding section of the [Distributed Input guide](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset) that the autoshard policy only applies to MultiWorkerMirroredStrategy and TPUStrategy, and not ParameterServerStrategy.\r\n\r\nClosing this issue now since original question was addressed in the Parameter Server Strategy tutorial."]}, {"number": 44986, "title": "Resource exhausted error when running tf.keras.Model.predict on large tensor", "body": "I'm trying to run `tf.keras.Model.predict` on a complex tensor with shape: (1532, 128, 2049, 2). With a batch size of 4, the model throws a ResourceExhaustedError after the predictions on all the separate batches have been completed, when it is concating the predictions together.\r\n\r\nHowever, if I perform the batching manually, feeding one batch at a time through the model, there is no error. It seems the problem is that the concating at the end of the prediction loop is occurring on the GPU and not on the CPU.\r\n\r\nHere's the error.\r\n\r\n```\r\n~//lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    128       raise ValueError('{} is not supported in multi-worker mode.'.format(\r\n    129           method.__name__))\r\n--> 130     return method(self, *args, **kwargs)\r\n    131 \r\n    132   return tf_decorator.make_decorator(\r\n\r\n~//lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1612             callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})\r\n   1613       callbacks.on_predict_end()\r\n-> 1614     all_outputs = nest.map_structure_up_to(batch_outputs, concat, outputs)\r\n   1615     return tf_utils.to_numpy_or_python_type(all_outputs)\r\n   1616 \r\n\r\n~//lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure_up_to(shallow_tree, func, *inputs, **kwargs)\r\n   1137       lambda _, *values: func(*values),  # Discards the path arg.\r\n   1138       *inputs,\r\n-> 1139       **kwargs)\r\n   1140 \r\n   1141 \r\n\r\n~//lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure_with_tuple_paths_up_to(shallow_tree, func, *inputs, **kwargs)\r\n   1233                     in _yield_flat_up_to(shallow_tree, inputs[0], is_seq)]\r\n   1234   results = [func(*args, **kwargs) for args in zip(flat_path_list,\r\n-> 1235                                                    *flat_value_lists)]\r\n   1236   return pack_sequence_as(structure=shallow_tree, flat_sequence=results,\r\n   1237                           expand_composites=expand_composites)\r\n\r\n~/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n   1232   flat_path_list = [path for path, _\r\n   1233                     in _yield_flat_up_to(shallow_tree, inputs[0], is_seq)]\r\n-> 1234   results = [func(*args, **kwargs) for args in zip(flat_path_list,\r\n   1235                                                    *flat_value_lists)]\r\n   1236   return pack_sequence_as(structure=shallow_tree, flat_sequence=results,\r\n\r\n~/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <lambda>(_, *values)\r\n   1135   return map_structure_with_tuple_paths_up_to(\r\n   1136       shallow_tree,\r\n-> 1137       lambda _, *values: func(*values),  # Discards the path arg.\r\n   1138       *inputs,\r\n   1139       **kwargs)\r\n\r\n~/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in concat(tensors, axis)\r\n   2671   if isinstance(tensors[0], ragged_tensor.RaggedTensor):\r\n   2672     return ragged_concat_ops.concat(tensors, axis=axis)\r\n-> 2673   return array_ops.concat(tensors, axis=axis)\r\n   2674 \r\n   2675 \r\n\r\n~/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py in concat(values, axis, name)\r\n   1652           dtype=dtypes.int32).get_shape().assert_has_rank(0)\r\n   1653       return identity(values[0], name=name)\r\n-> 1654   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n   1655 \r\n   1656 \r\n\r\n~/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2(values, axis, name)\r\n   1205       return _result\r\n   1206     except _core._NotOkStatusException as e:\r\n-> 1207       _ops.raise_from_not_ok_status(e, name)\r\n   1208     except _core._FallbackException:\r\n   1209       pass\r\n\r\n~/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6841   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6842   # pylint: disable=protected-access\r\n-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6844   # pylint: enable=protected-access\r\n   6845 \r\n\r\n~/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[1532,128,2049,2] and type complex64 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ConcatV2] name: concat\r\n```\r\n", "comments": ["@lminer,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using.\r\n\r\nAlso, try setting a hard limit on the total GPU memory as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if you are facing the same error. Thanks!", "@amahendrakar  the problem seems to occur when the output of the model is complex. Here's a simply example that gives a resource exhausted error on a RTX 2080 ti on the final concat operation after the predict loop. Limiting GPU memory does not help. However, if you manually batch the data and only predict one batch at a time and then do the concating yourself using numpy, it works fine because the concating occurs in RAM. However, this leads to underutilization of the GPU during inference.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef to_complex(x):\r\n    return tf.dtypes.complex(x, x)\r\n    \r\nbase_model = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False)\r\nx = base_model.output\r\nx = tf.keras.layers.Lambda(to_complex)(x)\r\n\r\n\r\nmodel = tf.keras.Model(inputs=base_model.input, outputs=x)\r\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\nX = np.random.rand(10000, 224, 224, 3).astype(\"float32\")\r\nout = model.predict(X, batch_size=64)\r\n```\r\n\r\nI'm running:\r\nubuntu 18.04\r\ntensorflow 2.3.1\r\nRTX 2080 ti", "@lminer,\r\nThe source of the error seems to be `X = np.random.rand(10000, 224, 224, 3).astype(\"float32\")`. I was able to run the code without any issues with an array of a smaller size.\r\n\r\nIs there any specific reason you are using such a large array? Thanks! ", "@amahendrakar I'm running inference on audio files, which at higher sample rates can get very large. That array is actually pretty normal sized for me. Is there a reason that the final concatenate needs to occur on the GPU? Is it even any faster?", "Any solution or workaround for this? Isn't this happening to any imagenet prediction, which has to concat a (>1.2M, 1000) tensor as output? I'm observing same OOM error when extracting last activation layers of an efficientnet for imagenet prediction, which needs to concat a (>1.2M, 1280) tensor, but doesn't seem to have the same error if I get the 1000 class output directly...", "My solution:\r\n`with tf.device(\"cpu:0\"): prediction = model.predict()`\r\nUsing the CPU for the inference solves the problem in my case.", "> My solution:\r\n> `with tf.device(\"cpu:0\"): prediction = model.predict()`\r\n> Using the CPU for the inference solves the problem in my case.\r\n\r\nI can verify this works for me - amazingly. My cpu maxed out to 100 but it did return a prediction without OOM error. \r\n\r\nIt would be great if tensorflow had a solution for this. Maybe an ability to inference over tiles within the tensor and reconstruct the output if there is no other option. ", "\r\n\r\n\r\n> My solution:\r\n> `with tf.device(\"cpu:0\"): prediction = model.predict()`\r\n> Using the CPU for the inference solves the problem in my case.\r\n\r\nUnfortunately, It didn't solve my problem and it stops with this message: \"Killed\" :(", "@lminer I can reproduce the issue with `tf-nightly` (`TF2.8.dev`). \r\n As it is related to keras, Is it possible for you to move this issue to keras-team/keras repo?\r\n\r\nPlease note that Keras development moved to another repository to focus entirely on only keras. Could you please repost this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44985, "title": "xrt ops use bfc_allocator instead", "body": "XRT ops use `cuMemAlloc` directly, which will hurt the performance a lot.\r\n\r\nThis PR changes the cuda allocator to bfc allocator.\r\n\r\nFor Resnet-50 fp16 training with batchsize 256 in Tesla V100 GPU, the performance could increase from `730 images/s` to `1290 images/s`. Please refer to the [torch/xla issue](https://github.com/pytorch/xla/issues/2614) for more details.\r\n\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44985) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it", "Ping @dlibenzi; wonder if you are interested in reviewing this PR.", "Hi @gbaned @joker-eph,\r\n\r\nall the xrt unit tests have passed, could you help review?\r\n", "@yunxing could you help review?", "@gbaned more than 2 weeks have passed but no one help review. Could you help find any other reviewers?", "`tensorflow/compiler` code should only be reviewed by people from the compilers teams.", "@sanjoy Can you please take a look at this PR or find an appropriate reviewer.", "There are some compilations for this pr to merge externally (if we decide to take it). Helper functions like `GetInputTupleAllocations` is being used by the internal `XRT_TPU_ops` which is is invisible from the outside. I recently open sourced the [tpu_compile_op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xrt/kernels/tpu_compile_ops.cc) and [tpu_execute_op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xrt/kernels/tpu_execute_op.cc) which also uses these helper functions. However the internal `XRT_TPU_ops` is still being used.\r\n\r\nI would suggest you wait for a couple days before I open sourced all of the `tpu_ops` and update those files. It is likely I will take sometime to review this pr since I am not as familiar with the gpu stuff. If by that time we still find this pr to be valueable, I might have to take this diff and update the internal usage of helper functions and submit it internally.", "Thanks for following up with this pr. And we're experimentally using torch/xla to accelerate pytorch training on GPU with this patch in the company. Currently we maintain our own branch of torch/xla and tensorflow, and surely we could wait for your side.", "It has been 24 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "A bit busy recently, will try to circle back to this pr", "@JackCaoG, @yaochengji  Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Sorry I missed the last email. @yaochengji There has been couple more TPU related ops added to `tensorflow/compiler/xrt/kernels/`. If you could update the signatures  for those ops, I can take this diff and update the internal ops and merge it internally.", "> Sorry I missed the last email. @yaochengji There has been couple more TPU related ops added to `tensorflow/compiler/xrt/kernels/`. If you could update the signatures for those ops, I can take this diff and update the internal ops and merge it internally.\r\n\r\nThanks @JackCaoG. I will update it in the following days.", "@JackCaoG, I only updated the [branch](https://github.com/yaochengji/tensorflow/tree/fix-xrt) and fixed some compilation issues. Xrt unit test could pass. It seems that there's no need to update the GPU related ops' signatures", "Thanks, I think my you meant `TPU related op`, let me circle back and check whether those ops uses the helper function you modifyied.", "@JackCaoG Hello, Any progress about the commit? Or should I submit a new PR?", "@yaochengji Does the XRT allocator allocate memory again and again in each fwd/bwd pass vs the BFC allocator that does it once and reuse it again? If what I said doesn't sound correct do you mind elaborating why `cuMemAlloc` is suboptimal ? ", "on the high level, both the XRT original allocator and BFC allocator will allocate memory again and again, but their implementation differs. The original one allocates and releases money by cuda driver API, which sync all cuda streams and hurt the performance. And the BFC allocator only calls the cuda driver api and allocate a large chunk of cuda memory when initializing. Every iteration, it could get part of it and doesn't need to call the driver api.", "@yaochengji  Oh I see thanks so much for your explanation and also for this PR. I am assuming you must have made many benchmarks across different models with this fix on your custom versions of PT-XLA. I see you shared the performance improvement for Resnet-50. Do you mind sharing other benchmarks as well that you have done with your own TF and PT-XLA versions (maybe across different batch sizes, detection models etc. or anything else) It would be great to see those numbers compared with Vanilla PT. ", "@codeislife99 \r\nResnet 50: 1150 images/s with amp enabled\r\nBert: almost the same as pytorch\r\nDetection: tried to run it on torch/xla but failed. because some ops in detection model is not friendly for torch/xla.", "In the start of this pr you mention 1290. So I wanted to ask What's the batch size for the new numbers (1150) and what's the CUDA architecture ( V100 / T4 etc.) And number of GPUs.", "@codeislife99 \r\nV100, 1 GPU, batch size = 256.", "@JackCaoG added a new commit to fix the TPU compatible issue and most of the comments.", "@yaochengji Thanks! Will take a look today or tmr.", "@yaochengji I think you forgot to update  the pr, the change I am seeing is out of date.", "Oh, because the PR is closed, it will not update automatically.\r\n\r\nSee [my branch](https://github.com/yaochengji/tensorflow/tree/fix-xrt), or should I open a new one?", "@yaochengji Sounds good, could you open a new PR?", "> @yaochengji Sounds good, could you open a new PR?\r\n\r\nreopened, https://github.com/tensorflow/tensorflow/pull/48832"]}, {"number": 44983, "title": "tf.image.per_image_standardization unexpected behavior with unsigned integer input", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.2.0-57-g25fba035f3 2.2.1\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: 10.1.243\r\n- GPU model and memory: Nvidia Tesla V100-DGXS-32GB, \r\n\r\n**Describe the current behavior**\r\n`tf.image.per_image_standardization` has no special handling for unsigned integers but still converts back to unsigned before returning. Thus uint inputs get saturated at zero (fully half of the data!) and at their maximum value. Currently no explanation of uint behavior is given in the docstring, and no warning or other indication that behavior might be unexpected is shown.\r\n\r\n**Describe the expected behavior**\r\nIn previous versions of TensorFlow (I believe changed after 1.14), `per_image_standardization` just returned a float rather than converting back to the original datatype, so it didn't have this problem. I would expect something safe like that, leaving it up to the developer to decide how they want this conversion handled, or at least a warning printed in the event a uint input is given. Some documentation specifying that uints are not handled differently and will be saturated might be nice too.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab notebook link][0].\r\n\r\n**Other info / logs** \r\nNo tracebacks or logs because it doesn't throw an error. I would call this a failure mode, but it fails silently. I suspect it's an edge case that isn't tested.\r\n\r\n[0]: https://colab.research.google.com/drive/1PqjTWtgLPE7hb5Dza2fZdFNbhlyW3mKl?usp=sharing", "comments": ["@Engineero \r\n\r\nPlease, grant me access to colab link. Thanks!", "@ravikyram \r\n\r\nSorry, haven't used colab much. It should be fixed now.", "I have tried in colab with TF version 2.3 and nightly version(`2.5.0-dev20201118`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/88120ddb0fd04644a3d2d4075ca97f7f/untitled524.ipynb). Thanks!", "For the record, I think the behavior changed between 1.14 and 1.15, which is why I suspect an untested edge case. I had a use case where this affected training, and while it's certainly not backwards-incompatible (doesn't error out anyway), it is a surprise.", "@engineero, related issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/33892", "Here is the offending commit:\r\n\r\n906e0e3", "Oh, hey @mixxen! Good find, thanks!", "It looks like simply *casting* (not with `convert_image_dtype`, just `tf.cast`) to float prior to `per_image_normalization` prevents information loss. I'd create a pull request, but I think the bigger question is what is the intended behavior when given unsigned integer input and how should that be communicated? Also covering this edge case with tests.", "Thank you for reporting the issue @Engineero and identifying the offending commit @mixxen.\r\n\r\nThe changes in https://github.com/tensorflow/tensorflow/commit/906e0e3bc0dfe12db19afa261e4d793b73cb64ec appears to have changed the behavior of `per_image_standardization` unintentionally and should be reverted back in my opinion. It especially does not make sense for unsigned integer data types because images that have zero-mean will consist of negative and positive values (unless all values are 0). The API function has been working with `uint` dtypes (until the breaking change) and should continue to do so. I'll get a fix in shortly.", "Here is a [colab](https://colab.research.google.com/drive/1nqzU22tZ0RVAaDm6drWPkfGV8cc7A_3x?usp=sharing) verifying the fix with `tf-nightly`. Closing the issue; please feel free to reopen if you encounter additional issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44983\">No</a>\n"]}, {"number": 44982, "title": "TF Lite LSH Projection returns different results on Big Endian and Little Endian Machines", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThe LSH Projection defined in `tensorflow/lite/kernels/lsh_projection.cc` has different outputs on Big Endian and Little Endian machines for the same input. The issue is caused by the fact that the input array is flattened into a string in this [line](https://github.com/tensorflow/tensorflow/blob/402d28705e426fea7aad6bbbe405a11daa6b6cd5/tensorflow/lite/kernels/lsh_projection.cc#L130). When the elements in the array are more than 1 byte long, the same array will be represented differently in bytes for LE and BE, and that will cause the string it converts to - `key` here - be different. Thus the `Fingerprint64()` will return differently for LE and BE.\r\n\r\n**Describe the expected behavior**\r\nSince `Fingerprint64` function returns the same results for the same input string across all platforms, I am actually not sure whether we are expecting the results of `lsh_projection` to be consistent across all platforms as well. In other words, I wonder when the `lsh_projection` op is called, are we expecting to see a specific output for a specific input, or it will be okay as long as different inputs have different outputs.\r\n\r\nIf it is the former one, I have a patch ready that could make Big Endian machine flatten the array into the same string as Little Endian ones, and the output will be the same on both platforms with the same input array.\r\n\r\nIf it is the latter one, then we could keep the way it is then.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe following test case will fail on Big Endian machine:\r\n```\r\nbazel --host_javabase=\"@local_jdk//:jdk\" --cache_test_results=no --test_timeout 300,900,1200,3600 --build_tests_only --test_output=errors -- //tensorflow/lite/kernels:lsh_projection_test\r\n```\r\n\r\n**Other info / logs** \r\n", "comments": ["Thanks for raising the issue and for suggesting a potential solution via your [PR](https://github.com/tensorflow/tensorflow/pull/45010). The requirement of returning the exact same hash regardless of the endianness of the machine is not necessary. Therefore, I would suggest creating a fix for the test. If you are willing, please create the PR fixing lsh_projection_test.cc and assign to me. Thank you!", "Thanks for the feedback, I will push the fix for the test case to merge with my PR that fixes other issues on TF Lite.", "Closed the issue as the PR has been merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44982\">No</a>\n"]}, {"number": 44981, "title": "Gives error on dumping images to the tensorboard when training with TPU using gradient tape.", "body": "I am using colab_tpu with tensorflow version 2.3.0\r\nDumping of scalars work good, but on dumping images it gives error. Inputs and Output are in tf.float32. The code is present [here](https://github.com/coreqode/tboard_issue/blob/master/base_module.py)\r\n\r\n1.  When, I call ```_train_dump_images```  function inside the ```train_step_zero```, it gives following error ([here](https://github.com/coreqode/tboard_issue/blob/b0af0c08e03e8c2ba6912a07cd0d5028d4f6725f/base_module.py#L108)):\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"grapy.py\", line 156, in <module>\r\n    main()\r\n  File \"grapy.py\", line 152, in main\r\n    grapy.train()\r\n  File \"/content/TensorFlow-TryOn/base_module.py\", line 237, in train\r\n    losses = self.tpu_train_step_zero(train_iterator)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3213, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3075, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3735, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\ntensorflow.python.framework.errors_impl.InaccessibleTensorError: in user code:\r\n\r\n    /content/TensorFlow-TryOn/base_module.py:287 tpu_train_step_zero  *\r\n        losses = self.strategy.run(self.train_step_zero, args=(next(iterator)))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:346 run  **\r\n        return self.extended.tpu_run(fn, args, kwargs, options)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1095 tpu_run\r\n        return func(args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1162 tpu_function\r\n        padding_spec=padding_spec)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:915 replicate\r\n        padding_spec=padding_spec)[1]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:1380 split_compile_and_replicate\r\n        outputs = computation(*computation_inputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1124 replicated_fn\r\n        result[0] = fn(*replica_args, **replica_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:780 __call__\r\n        result = self._call(*args, **kwds)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:840 _call\r\n        return self._stateless_fn(*args, **kwds)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2829 __call__\r\n        return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:1848 _filtered_call\r\n        cancellation_manager=cancellation_manager)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:1938 _call_flat\r\n        flat_outputs = forward_function.call(ctx, args_with_tangents)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:579 call\r\n        executor_type=executor_type)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/functional_ops.py:1207 partitioned_call\r\n        op = graph.create_op(op_name, args, tout, name=op_name, attrs=op_attrs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507 new_func\r\n        return func(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3416 create_op\r\n        attrs, op_def, compute_device)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:589 _create_op_internal\r\n        inp = self.capture(inp)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:639 capture\r\n        % (tensor, tensor.graph, self))\r\n\r\n    InaccessibleTensorError: The tensor 'Tensor(\"model_inputs:0\", shape=(16, 512, 256, 3), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=train_step_zero, id=140465465202672); accessed from: FuncGraph(name=tpu_train_step_zero, id=140465463508440).\r\n```\r\n2.  When, I call ```_train_dump_images```  function outside the ```train_step_zero``` from the training loop (similar to [this](https://github.com/coreqode/tboard_issue/blob/b0af0c08e03e8c2ba6912a07cd0d5028d4f6725f/base_module.py#L47)), it gives following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"grapy.py\", line 156, in <module>\r\n    main()\r\n  File \"grapy.py\", line 152, in main\r\n    grapy.train()\r\n  File \"/content/TensorFlow-TryOn/base_module.py\", line 253, in train\r\n    self._train_dump_images(tf.constant(self.step, dtype=tf.int64))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 846, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 75, in quick_execute\r\n    raise e\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: model_inputs:0\r\n```", "comments": ["@coreqode \r\nI ran the code shared on tf nightly+tpu and do not see any error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/22a8ad472c53f708c4fd5999369bc3e2/untitled463.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44979, "title": "tf.keras.Model.fit() fails with RaggedTensor if using validation_split", "body": "\r\n**System information**\r\n- I am using Arch Linux at 5.9.1-arch1-1\r\n- Tensorflow as installed using pip in a conda virtual environment\r\n- I am using tensorflow on CPU only\r\n- I have tested the code provided here in a jupyter-lab notebook\r\n\r\nTensorflow version: 2.2.0\r\n\r\n**Describe the current behavior**\r\nTrying to fit a keras model using a RaggedTensor as input fails if validation_split > 0.0 is used.\r\n\r\n**Describe the expected behavior**\r\nTraining would work the same way it does setting validation_split to 0.0.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nfrom tensorflow.compat.v1.keras.layers import Dense, Dropout, concatenate\r\nfrom tensorflow.keras.layers import LSTM, Input, concatenate\r\nfrom tensorflow.keras.initializers import glorot_uniform, Constant\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras import activations\r\nimport tensorflow as tf\r\n\r\nDS_SIZE=4\r\nSEQ_LENGTH=5\r\nMAX_SEQ_LENGTH=5\r\n\r\n\r\ndef prepare_data(subjects, seq_lengths, max_seq_length):\r\n    # Define the sub sequence lenghts\r\n    seq_lengths = np.random.randint(max_seq_length, size=(subjects))\r\n    \r\n    # Get the values\r\n    seq_values = np.random.randint(100, size=(sum(seq_lengths)), dtype=np.int32)\r\n\r\n    # Create a nested ragged tensor, of shape [subjects, (seq_length), (0...max_seq_length)]\r\n    X = tf.expand_dims(tf.RaggedTensor.from_row_lengths(values=seq_values, row_lengths=seq_lengths), 2)\r\n    \r\n    Y = tf.math.reduce_sum(X, axis=(2, 1))\r\n\r\n    return (X, Y)\r\n    \r\nX, Y = prepare_data(DS_SIZE, SEQ_LENGTH, MAX_SEQ_LENGTH)\r\n\r\nprint(f'Tensorflow version: {(tf.version.GIT_VERSION, tf.version.VERSION)}')\r\nprint(f'Input data:\\nX.shape = {X.shape}\\nX={X}')\r\n\r\nfrom tensorflow.keras.layers import Lambda\r\n\r\ninputs = Input(shape=(None,1), name=\"Input\", ragged=True)\r\nlstm = LSTM(5, return_sequences=False, name='LSTM')(inputs)\r\npredictor = Dense(1, activation=activations.linear, name='Predictor')(lstm)\r\n\r\nmodel = Model(inputs=inputs, outputs=predictor)\r\nmodel.summary()\r\n\r\nmodel.compile(\r\n    loss='mse',\r\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.2),\r\n    metrics=[tf.keras.metrics.MeanSquaredError()],\r\n    run_eagerly=True  # Enable eager mode!\r\n)\r\n\r\nhistory = model.fit(\r\n    X,\r\n    Y,\r\n    validation_split = 0.1,\r\n    epochs = 1,\r\n    batch_size = 2,\r\n)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-37-31ef63a3e7dd> in <module>\r\n     50     validation_split = 0.1,\r\n     51     epochs = 1,\r\n---> 52     batch_size = 2,\r\n     53 )\r\n\r\n~/miniconda3/envs/MAI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/miniconda3/envs/MAI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    795           data_adapter.train_validation_split((x, y, sample_weight),\r\n    796                                               validation_split=validation_split,\r\n--> 797                                               shuffle=False))\r\n    798 \r\n    799     with self.distribute_strategy.scope(), \\\r\n\r\n~/miniconda3/envs/MAI/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in train_validation_split(arrays, validation_split, shuffle)\r\n   1309     raise ValueError(\r\n   1310         \"`validation_split` is only supported for Tensors or NumPy \"\r\n-> 1311         \"arrays, found: {}\".format(arrays))\r\n   1312 \r\n   1313   if all(t is None for t in flat_arrays):\r\n\r\nValueError: `validation_split` is only supported for Tensors or NumPy arrays, found: (<tf.RaggedTensor [[[55], [39], [36], [38]], [[45], [15], [84], [38]], [[51], [20], [44], [28]], [[38], [98]]]>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([168, 182, 143, 136], dtype=int32)>, None)\r\n```\r\n", "comments": ["It is duplicate of https://github.com/tensorflow/tensorflow/issues/43157. Please close this and  upvote/subscribe to that one.", "Duplicate of #43157", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44979\">No</a>\n"]}, {"number": 44978, "title": "error when compute gradients", "body": "This is a pytorch version code, I want to implement it in tensorflow.\r\n```\r\ngradients = []\r\nfor i, loss in enumerate(self.losses):\r\n    # forward pass\r\n    model_output = self.model(x)\r\n    # calculate loss\r\n    if self.anneal:\r\n        L = loss.compute_loss(y, model_output, anneal=beta)\r\n    else:\r\n        L = loss.compute_loss(y, model_output)\r\n    # zero gradient\r\n    self.optimizer.zero_grad()\r\n    # backward pass\r\n    L.backward()\r\n    # get gradient for correctness objective\r\n    gradients.append(self._get_gradient_np())\r\n\r\n# calculate the losses\r\nlosses_computed = []\r\n# forward pass\r\nmodel_output = self.model(x)\r\n\r\nfor i, loss in enumerate(self.losses):\r\n    if self.anneal:\r\n        L = loss.compute_loss(y, model_output, anneal=beta)\r\n    else:\r\n        L = loss.compute_loss(y, model_output)\r\n    losses_computed.append(L)\r\n\r\n# get the final loss to compute the common descent vector\r\nfinal_loss, alphas = self.common_descent_vector.get_descent_vector(\r\n    losses_computed, gradients)\r\n# zero gradient\r\nself.optimizer.zero_grad()\r\n# backward pass\r\nfinal_loss.backward()\r\n# update parameters\r\nself.optimizer.step()\r\n\r\n```\r\nHere is my tensorflow 1.14.0 version.\r\n\r\n```\r\noptimizer = tf.train.AdamOptimizer(lr)\r\n\r\ngradients_a = optimizer.compute_gradients(loss_a, var_a)\r\ngradients_b = optimizer.compute_gradients(loss_b, var_b)\r\n\r\ngrad_a = [tf.reshape(g, (1, -1)) for g, v in gradients_a]\r\ngrad_b = [tf.reshape(g, (1, -1)) for g, v in gradients_b]\r\n\r\ngrad_a = tf.concat(grad_a, axis=-1, name=\"concat_grad_a\")\r\ngrad_b = tf.concat(grad_b, axis=-1, name=\"concat_grad_b\")\r\n\r\nloss_weight = tf.py_function(solve, inp=[grad_a, grad_b], Tout=[tf.float32, tf.float32])\r\n\r\nfinal_loss = loss_weight[0] * loss_a + loss_weight[1] * loss_b\r\n\r\noptimizer.minimize(loss)\r\n```\r\n\r\nThis code could run at first setp, but throw exception at second step.\r\n**tensorflow.python.freamework.errors_impl.InvalidArgumentError: Expected begin and size arguments to be 1-D tensors of size 0, but got shape [2] and [2] instead.  [[{{ node gradients_2/concat_grad_b_grad/Slice}}]]**\r\n\r\nI guess the gradient_a and gradient_b as nodes of the graph, so when compute the gradients of final_loss, it may raise error.\r\nIs there any solution to solve this problem? THX\r\n", "comments": ["@Lihengwannafly \r\nPlease upgrade to 2.x as we do not have support for tf 1.x", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44977, "title": "What is the difference between two data generation methods\uff1f", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: python3.6\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: CUDA 10.2\r\n- GPU model and memory: V100 32 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI use two different methods to generate data in `input_fn` with `tf.estimator` API\r\n~~~\r\ndef input_fn(batch_size, num_epochs):\r\n    # Method1: generate one batch data and repeat\r\n    #train_data = tf.truncated_normal((batch_size, 300, 150), 0.5, 0.1)\r\n    #train_label = tf.random_uniform((batch_size,1),minval=0,maxval=1,dtype=tf.float32)\r\n    #ds = tf.data.Dataset.from_tensors((train_data, train_label)).repeat(num_epochs)\r\n    #Method2: generate data repeat then batch\r\n    train_data = tf.truncated_normal([300, 150], 0.5, 0.1)\r\n    train_label = tf.random_uniform([1],minval=0,maxval=1,dtype=tf.float32)\r\n    ds = tf.data.Dataset.from_tensors((train_data, train_label)).repeat().batch(batch_size)\r\n    ds = ds.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)\r\n\r\n    return ds\r\n~~~\r\nAnd I got two different performances. When I use Method1, the GPU utilization is not very stable, however, when I use Method2, the GPU utilization is very stable. Besides, when I use `tf.data.Dataset.from_tensors` API, the data generation time will be very long, however, if I use `tf.data.Dataset.from_tensor_slices` API, the generation time is very short.\r\n**Describe the expected behavior**\r\nI expect the performances should be same or similar when I use Methods 1 and 2. And it is also should be the same when I use different APIs `tf.data.Dataset.from_tensors` and `tf.data.Dataset.from_tensor_slices`.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@lalalapotter \r\n\r\nTF 1.14 is not actively supported. Please, switchover to latest stable TF version 2.3 and see if the issue still persists.There were lots of performance improvements in the latest versions\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44976, "title": "```tf.distribute.MirroredStrategy``` is stuck.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu20.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.3\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: gtx1070 * 2\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n```\r\n== check python ===================================================\r\npython version: 3.8.3\r\npython branch:\r\npython build version: ('default', 'Jul  2 2020 16:21:59')\r\npython compiler version: GCC 7.3.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.5.0-6ubuntu2) 7.5.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                              1.18.5\r\nnumpydoc                           1.1.0\r\nprotobuf                           3.13.0\r\ntensorflow                         2.3.1\r\ntensorflow-estimator               2.3.0\r\ntensorflow-serving-api             2.3.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.3.1\r\ntf.version.GIT_VERSION = v2.3.0-54-gfcc4b966f1\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\n    194729:     find library=libpthread.so.0 [0]; searching\r\n    194729:      search path=/home/zhaodachuan/anaconda3/bin/../lib/tls/x86_64/x86_64:/home/zhaodachuan/anaconda3/bin/../lib/tls/x86_64:/home/zhaodachuan/anaconda3/bin/../lib/tls/x86_64:/home/zhaodachuan/anaconda3/bin/../lib/tls:/home/zhaodachuan/anaconda3/bin/../lib/x86_64/x86_64:/home/zhaodachuan/anaconda3/bin/../lib/x86_64:/home/zhaodachuan/anaconda3/bin/../lib/x86_64:/home/zhaodachuan/anaconda3/bin/../lib         (RPATH from file /home/zhaodachuan/anaconda3/bin/python)\r\n    194729:       trying file=/home/zhaodachuan/anaconda3/bin/../lib/tls/x86_64/x86_64/libpthread.so.0\r\n    194729:       trying file=/home/zhaodachuan/anaconda3/bin/../lib/tls/x86_64/libpthread.so.0\r\n    194729:       trying file=/home/zhaodachuan/anaconda3/bin/../lib/tls/x86_64/libpthread.so.0\r\n    194729:       trying file=/home/zhaodachuan/anaconda3/bin/../lib/tls/libpthread.so.0\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Nov 18 19:10:46 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 00000000:23:00.0 Off |                  N/A |\r\n| 42%   53C    P2    55W / 180W |   7709MiB /  8119MiB |    100%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1070    Off  | 00000000:2D:00.0  On |                  N/A |\r\n| 47%   65C    P2    60W / 180W |   7716MiB /  8111MiB |    100%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1041      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    0   N/A  N/A    171379      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    0   N/A  N/A    193747      C   ...huan/anaconda3/bin/python     7695MiB |\r\n|    1   N/A  N/A      1041      G   /usr/lib/xorg/Xorg                 35MiB |\r\n|    1   N/A  N/A    171379      G   /usr/lib/xorg/Xorg                 77MiB |\r\n|    1   N/A  N/A    171513      G   /usr/bin/gnome-shell              119MiB |\r\n|    1   N/A  N/A    177947      G   /usr/bin/nvidia-settings            3MiB |\r\n|    1   N/A  N/A    193747      C   ...huan/anaconda3/bin/python     7461MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.243\r\n/usr/local/cuda-10.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.1/doc/man/man7/libcudart.7\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.3.1\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/zhaodachuan/anaconda3/lib/python3.8/site-packages\r\nRequired-by: tensorflow-serving-api\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 8, 3, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\nYou can also obtain the TensorFlow version with:\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n```\r\n2020-11-18 19:09:31.423127: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nv2.3.0-54-gfcc4b966f1 2.3.1\r\n```\r\n\r\n**Describe the current behavior**\r\nWhen I run the code \r\n```Python\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_visible_devices(gpus, 'GPU')\r\nprint(gpus)\r\n# output : \r\n# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=500)\r\n```\r\n, it's stuck in \r\n```\r\nEpoch 1/500\r\nINFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n```\r\nWhen I change to \r\n```Python\r\nmirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())\r\n```\r\n , it can work but it runs slower than one gpu and the loss will become nan quickly\r\n```\r\nEpoch 1/500\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n  15/1875 [..............................] - ETA: 13:46 - loss: nan - accuracy: 0.1583\r\n\r\n```\r\nMay the  same issue : \r\n[https://github.com/tensorflow/tensorflow/issues/37561](https://github.com/tensorflow/tensorflow/issues/37561)\r\n[https://github.com/tensorflow/tensorflow/issues/38982](https://github.com/tensorflow/tensorflow/issues/38982)", "comments": ["This is very likely an NCCL error, not a TensorFlow error. Try running the [Nvidia NCCL tests](https://github.com/NVIDIA/nccl-tests), e.g. the `all_reduce` test. If that gets stuck, you probably need to change your BIOS settings.", "> This is very likely an NCCL error, not a TensorFlow error. Try running the [Nvidia NCCL tests](https://github.com/NVIDIA/nccl-tests), e.g. the `all_reduce` test. If that gets stuck, you probably need to change your BIOS settings.\r\n\r\nYes , it gets stuck ...... What should I do ? I use MSI AMD X570 motherboard ", "From the [NCCL troubleshooting section](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html):\r\n\r\n> IO virtualization (also known as, VT-d or IOMMU) can interfere with GPU Direct by redirecting all PCI point-to-point traffic to the CPU root complex, causing a significant performance reduction or even a hang. \r\n\r\nA possible quick fix may be simply disabling IOMMU in your BIOS. Furthermore, if you run in a container:\r\n\r\n> When using NCCL inside a container, it is recommended that you increase these resources by issuing: `\u2013shm-size=1g \u2013ulimit memlock=-1` in the command line to `nvidia-docker run`.", "> IO virtualization\r\n\r\nI turn off IOMMU then it works ! thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44976\">No</a>\n"]}]