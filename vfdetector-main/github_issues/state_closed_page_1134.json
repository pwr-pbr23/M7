[{"number": 19200, "title": "Difference in output between CPU and GPU ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: TF 1.7 and TF 1.8 (v1.8.0-0-g93bc2e2072 1.8.0)\r\n- **Python version**: Python 3.5.4\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: CUDA 9.0, cudnn 7.0.5\r\n- **GPU model and memory**: GPU GTX1080\r\n- **Exact command to reproduce**: n/a\r\n\r\n**Problem**\r\nI see a very different output when executing a training on CPU vs. GPU. The problem does not seem to be related to initialization or numerical precision (see investigation): same code, same data, same machine, just switching between GPU and CPU execution by adding `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).\r\n\r\n**Investigation**\r\nI gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a `tf.multiply` (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.\r\n\r\nUnfortunately, I am not able to reproduce the problem in a unit test (but am still working on it). If I feed the same inputs to `tf.multiply` in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.\r\n\r\n**Seeding and reproducibility**\r\nRegarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.\r\n\r\nThe problem is reproducible from one run to the other.\r\n\r\n**Environment dependency**\r\nThe problem occurs with both TF 1.7 and TF 1.8. However, if I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.\r\n\r\nAny idea or debugging experiment suggestion is very welcome.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I updated the original post (info were there but not in the right format)", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: TF 1.7 and TF 1.8 (v1.8.0-0-g93bc2e2072 1.8.0)\r\n- **Python version**: Python 3.5.4\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: CUDA 9.0, cudnn 7.0.5\r\n- **GPU model and memory**: GPU GTX1080\r\n- **Exact command to reproduce**: n/a", "@zheng-xq Can you take a look at this?", "It will be very helpful to have a small repro case.\r\n\r\nCPU and GPU are known to produce slightly different results. For each operation, if the inputs are identical, the output should only have lsb difference. So if you observe large difference from a single operation, not a sequence of operations, it might be a bug somewhere. Otherwise, this is somewhat expected.", "@annemenini Can you provide code + data to reproduce this?", "I unfortunately cannot share the full code and do not manage to reproduce to problem in a simple unit test so far.\r\n\r\nTo clarify, the difference I observe out of a single operation (tf.multiply) is significant: with the notation A .* B = C, here are the values I observe on one randomly picked element of the tensor:\r\n\r\nOn CPU:\r\nA = -0.2314 + 0.2525i\r\nB = -0.0212 - 0.3013i\r\n**C = 0.0810 + 0.0644i**\r\n\r\nOn GPU:\r\nA = -0.2314 + 0.2525i\r\nB = -0.0212 - 0.3013i\r\n**C = -0.0712 + 0.0751i**\r\n\r\nThe mismatch disappears when the same operation is executed on its own in a simpler graph (hence why I don't manage to reproduce the problem in a unit test).", "Hi guys, I have exactly the same issue when running the same code on CPU and on GPU ... my guess is that computation on some operations are numerically instable on GPU (but I am not a pro in numerical computation ....). \r\nHere is first my setup :    \r\n**I am on Ubuntu 16.04.4 LTS with Tensorflow built from source (I have both 1.7 and 1.8). I have both python version (2.7 and 3.5), bazel version is 0.14, the GCC compiler is 5.4, CUDA is 9.2 and cudnn 7.1.4. I have 2 GPUs, both are NVIDIA GTX 1080Ti.**\r\n@zheng-xq, @angersson as requested, I created a snippet that easily reproduces my issue.\r\n\r\n_**ON CPU**_\r\n\r\n```\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\r\nimport numpy as np\r\nimport tensorflow as tf\r\ndata = np.array(\r\n    [np.array([np.arange(1, 13 * 8 + 1).reshape(13, 8, 1) * i / 12.0 for i in range(1, 13)]) * j / 32.0 for j in\r\n     range(1, 33)]).astype(np.float32)\r\n\r\ninput_placeholder = tf.placeholder(\"float32\", shape=(None, 12, 13, 8, 1))\r\n\r\nres = tf.transpose(input_placeholder, [1, 0, 2, 3, 4])\r\n\r\nweights = tf.ones([8, 8, 16, 1])\r\nbiases = tf.ones([16])\r\n\r\ndeconv = tf.map_fn(lambda x: tf.nn.bias_add(tf.nn.conv2d_transpose(x, weights, strides=[1, 1, 1, 1], padding='VALID', output_shape=[32, 20, 15, 16]), biases), res)\r\n\r\nsess = tf.Session()\r\n\r\nwith sess.as_default():\r\n   output = sess.run(deconv, feed_dict={input_placeholder: data})\r\n\r\ntransposed = np.transpose(fin, [1, 0, 2, 3, 4])\r\nprint(transposed)\r\n```\r\nJust for some clarity issue, I print the end of what the snippet outputs **each time**: \r\n```\r\n[[[  2.          2.          2.        ...   2.          2.\r\n       2.       ]\r\n    [  4.          4.          4.        ...   4.          4.\r\n       4.       ]\r\n    [  7.          7.          7.        ...   7.          7.\r\n       7.       ]\r\n    ...\r\n    [ 22.         22.         22.        ...  22.         22.\r\n      22.       ]\r\n    [ 16.         16.         16.        ...  16.         16.\r\n      16.       ]\r\n    [  9.          9.          9.        ...   9.          9.\r\n       9.       ]]\r\n\r\n   [[ 11.         11.         11.        ...  11.         11.\r\n      11.       ]\r\n    [ 23.         23.         23.        ...  23.         23.\r\n      23.       ]\r\n    [ 37.         37.         37.        ...  37.         37.\r\n      37.       ]\r\n    ...\r\n    [ 67.         67.         67.        ...  67.         67.\r\n      67.       ]\r\n    [ 47.         47.         47.        ...  47.         47.\r\n      47.       ]\r\n    [ 25.         25.         25.        ...  25.         25.\r\n      25.       ]]\r\n\r\n   [[ 28.         28.         28.        ...  28.         28.\r\n      28.       ]\r\n    [ 58.         58.         58.        ...  58.         58.\r\n      58.       ]\r\n    [ 91.         91.         91.        ...  91.         91.\r\n      91.       ]\r\n    ...\r\n    [136.        136.        136.        ... 136.        136.\r\n     136.       ]\r\n    [ 94.         94.         94.        ...  94.         94.\r\n      94.       ]\r\n    [ 49.         49.         49.        ...  49.         49.\r\n      49.       ]]\r\n\r\n   ...\r\n\r\n   [[268.        268.        268.        ... 268.        268.\r\n     268.       ]\r\n    [538.        538.        538.        ... 538.        538.\r\n     538.       ]\r\n    [811.        811.        811.        ... 811.        811.\r\n     811.       ]\r\n    ...\r\n    [856.        856.        856.        ... 856.        856.\r\n     856.       ]\r\n    [574.        574.        574.        ... 574.        574.\r\n     574.       ]\r\n    [289.        289.        289.        ... 289.        289.\r\n     289.       ]]\r\n\r\n   [[187.        187.        187.        ... 187.        187.\r\n     187.       ]\r\n    [375.        375.        375.        ... 375.        375.\r\n     375.       ]\r\n    [565.        565.        565.        ... 565.        565.\r\n     565.       ]\r\n    ...\r\n    [595.        595.        595.        ... 595.        595.\r\n     595.       ]\r\n    [399.        399.        399.        ... 399.        399.\r\n     399.       ]\r\n    [201.        201.        201.        ... 201.        201.\r\n     201.       ]]\r\n\r\n   [[ 98.         98.         98.        ...  98.         98.\r\n      98.       ]\r\n    [196.        196.        196.        ... 196.        196.\r\n     196.       ]\r\n    [295.        295.        295.        ... 295.        295.\r\n     295.       ]\r\n    ...\r\n    [310.        310.        310.        ... 310.        310.\r\n     310.       ]\r\n    [208.        208.        208.        ... 208.        208.\r\n     208.       ]\r\n    [105.        105.        105.        ... 105.        105.\r\n     105.       ]]]]]\r\n```\r\n\r\n\r\n**_ON GPU_**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\ndata = np.array(\r\n    [np.array([np.arange(1, 13 * 8 + 1).reshape(13, 8, 1) * i / 12.0 for i in range(1, 13)]) * j / 32.0 for j in\r\n     range(1, 33)]).astype(np.float32)\r\n\r\ninput_placeholder = tf.placeholder(\"float32\", shape=(None, 12, 13, 8, 1))\r\n\r\nres = tf.transpose(input_placeholder, [1, 0, 2, 3, 4])\r\n\r\nweights = tf.ones([8, 8, 16, 1])\r\nbiases = tf.ones([16])\r\n\r\ndeconv = tf.map_fn(lambda x: tf.nn.bias_add(tf.nn.conv2d_transpose(x, weights, strides=[1, 1, 1, 1], padding='VALID', output_shape=[32, 20, 15, 16]), biases), res)\r\n\r\nsess = tf.Session()\r\n\r\nwith sess.as_default():\r\n   output = sess.run(deconv, feed_dict={input_placeholder: data})\r\n\r\ntransposed = np.transpose(fin, [1, 0, 2, 3, 4])\r\nprint(transposed)\r\n```\r\nIt is exactly the same code (except the cuda visible command), and **sometimes** I have exactly the same aforementioned output, **but I also have these** outputs:\r\n\r\n```\r\n[[[  1.9999084   1.9999084   1.9999084 ...   1.9999084   1.9999084\r\n       1.9999084]\r\n    [  4.          4.          4.        ...   4.          4.\r\n       4.       ]\r\n    [  7.          7.          7.        ...   7.          7.\r\n       7.       ]\r\n    ...\r\n    [ 22.         22.         22.        ...  22.         22.\r\n      22.       ]\r\n    [ 15.9999695  15.9999695  15.9999695 ...  15.9999695  15.9999695\r\n      15.9999695]\r\n    [  8.999954    8.999954    8.999954  ...   8.999954    8.999954\r\n       8.999954 ]]\r\n\r\n   [[ 10.999954   10.999954   10.999954  ...  10.999954   10.999954\r\n      10.999954 ]\r\n    [ 22.99997    22.99997    22.99997   ...  22.99997    22.99997\r\n      22.99997  ]\r\n    [ 37.00006    37.00006    37.00006   ...  37.00006    37.00006\r\n      37.00006  ]\r\n    ...\r\n    [ 67.00003    67.00003    67.00003   ...  67.00003    67.00003\r\n      67.00003  ]\r\n    [ 46.99997    46.99997    46.99997   ...  46.99997    46.99997\r\n      46.99997  ]\r\n    [ 25.         25.         25.        ...  25.         25.\r\n      25.       ]]\r\n\r\n   [[ 27.999954   27.999954   27.999954  ...  27.999954   27.999954\r\n      27.999954 ]\r\n    [ 57.99997    57.99997    57.99997   ...  57.99997    57.99997\r\n      57.99997  ]\r\n    [ 90.999985   90.999985   90.999985  ...  90.999985   90.999985\r\n      90.999985 ]\r\n    ...\r\n    [136.        136.        136.        ... 136.        136.\r\n     136.       ]\r\n    [ 93.999985   93.999985   93.999985  ...  93.999985   93.999985\r\n      93.999985 ]\r\n    [ 48.999954   48.999954   48.999954  ...  48.999954   48.999954\r\n      48.999954 ]]\r\n\r\n   ...\r\n\r\n   [[268.        268.        268.        ... 268.        268.\r\n     268.       ]\r\n    [538.        538.        538.        ... 538.        538.\r\n     538.       ]\r\n    [810.99994   810.99994   810.99994   ... 810.99994   810.99994\r\n     810.99994  ]\r\n    ...\r\n    [856.        856.        856.        ... 856.        856.\r\n     856.       ]\r\n    [574.        574.        574.        ... 574.        574.\r\n     574.       ]\r\n    [289.0001    289.0001    289.0001    ... 289.0001    289.0001\r\n     289.0001   ]]\r\n\r\n   [[186.99992   186.99992   186.99992   ... 186.99992   186.99992\r\n     186.99992  ]\r\n    [374.99997   374.99997   374.99997   ... 374.99997   374.99997\r\n     374.99997  ]\r\n    [565.        565.        565.        ... 565.        565.\r\n     565.       ]\r\n    ...\r\n    [595.        595.        595.        ... 595.        595.\r\n     595.       ]\r\n    [399.00006   399.00006   399.00006   ... 399.00006   399.00006\r\n     399.00006  ]\r\n    [201.00008   201.00008   201.00008   ... 201.00008   201.00008\r\n     201.00008  ]]\r\n\r\n   [[ 97.99998    97.99998    97.99998   ...  97.99998    97.99998\r\n      97.99998  ]\r\n    [195.99997   195.99997   195.99997   ... 195.99997   195.99997\r\n     195.99997  ]\r\n    [295.        295.        295.        ... 295.        295.\r\n     295.       ]\r\n    ...\r\n    [310.00003   310.00003   310.00003   ... 310.00003   310.00003\r\n     310.00003  ]\r\n    [208.00012   208.00012   208.00012   ... 208.00012   208.00012\r\n     208.00012  ]\r\n    [105.00009   105.00009   105.00009   ... 105.00009   105.00009\r\n     105.00009  ]]]]]\r\n```\r\nor \r\n```\r\n  [[[  1.9999237   1.9999237   1.9999237 ...   1.9999237   1.9999237\r\n       1.9999237]\r\n    [  4.          4.          4.        ...   4.          4.\r\n       4.       ]\r\n    [  7.          7.          7.        ...   7.          7.\r\n       7.       ]\r\n    ...\r\n    [ 22.         22.         22.        ...  22.         22.\r\n      22.       ]\r\n    [ 16.         16.         16.        ...  16.         16.\r\n      16.       ]\r\n    [  9.000046    9.000046    9.000046  ...   9.000046    9.000046\r\n       9.000046 ]]\r\n\r\n   [[ 11.000015   11.000015   11.000015  ...  11.000015   11.000015\r\n      11.000015 ]\r\n    [ 22.99997    22.99997    22.99997   ...  22.99997    22.99997\r\n      22.99997  ]\r\n    [ 37.00006    37.00006    37.00006   ...  37.00006    37.00006\r\n      37.00006  ]\r\n    ...\r\n    [ 67.         67.         67.        ...  67.         67.\r\n      67.       ]\r\n    [ 47.00006    47.00006    47.00006   ...  47.00006    47.00006\r\n      47.00006  ]\r\n    [ 25.000076   25.000076   25.000076  ...  25.000076   25.000076\r\n      25.000076 ]]\r\n\r\n   [[ 27.999962   27.999962   27.999962  ...  27.999962   27.999962\r\n      27.999962 ]\r\n    [ 57.99997    57.99997    57.99997   ...  57.99997    57.99997\r\n      57.99997  ]\r\n    [ 91.00003    91.00003    91.00003   ...  91.00003    91.00003\r\n      91.00003  ]\r\n    ...\r\n    [136.00002   136.00002   136.00002   ... 136.00002   136.00002\r\n     136.00002  ]\r\n    [ 94.000046   94.000046   94.000046  ...  94.000046   94.000046\r\n      94.000046 ]\r\n    [ 49.00006    49.00006    49.00006   ...  49.00006    49.00006\r\n      49.00006  ]]\r\n\r\n   ...\r\n\r\n   [[268.00006   268.00006   268.00006   ... 268.00006   268.00006\r\n     268.00006  ]\r\n    [538.        538.        538.        ... 538.        538.\r\n     538.       ]\r\n    [810.9999    810.9999    810.9999    ... 810.9999    810.9999\r\n     810.9999   ]\r\n    ...\r\n    [856.        856.        856.        ... 856.        856.\r\n     856.       ]\r\n    [573.99994   573.99994   573.99994   ... 573.99994   573.99994\r\n     573.99994  ]\r\n    [289.        289.        289.        ... 289.        289.\r\n     289.       ]]\r\n\r\n   [[186.99997   186.99997   186.99997   ... 186.99997   186.99997\r\n     186.99997  ]\r\n    [374.99997   374.99997   374.99997   ... 374.99997   374.99997\r\n     374.99997  ]\r\n    [565.        565.        565.        ... 565.        565.\r\n     565.       ]\r\n    ...\r\n    [595.        595.        595.        ... 595.        595.\r\n     595.       ]\r\n    [398.99994   398.99994   398.99994   ... 398.99994   398.99994\r\n     398.99994  ]\r\n    [200.99997   200.99997   200.99997   ... 200.99997   200.99997\r\n     200.99997  ]]\r\n\r\n   [[ 98.00005    98.00005    98.00005   ...  98.00005    98.00005\r\n      98.00005  ]\r\n    [195.99998   195.99998   195.99998   ... 195.99998   195.99998\r\n     195.99998  ]\r\n    [294.99994   294.99994   294.99994   ... 294.99994   294.99994\r\n     294.99994  ]\r\n    ...\r\n    [309.99997   309.99997   309.99997   ... 309.99997   309.99997\r\n     309.99997  ]\r\n    [207.99997   207.99997   207.99997   ... 207.99997   207.99997\r\n     207.99997  ]\r\n    [104.99999   104.99999   104.99999   ... 104.99999   104.99999\r\n     104.99999  ]]]]]\r\n```\r\n\r\nI also tried to run the command inside a ```tf.device('/cpu:0')``` but I still got the same behavior. Same thing if I try with tf 1.8 or 1.7 or using python 2.7 or 3.5.\r\nI also tried using ```tf.layers.conv2d_transpose``` and same thing happens\r\n\r\nFrom what I have read so far, these differences are *kind of expected* but as soon as you apply other methods on such small differences, systems are diverging (my case) instead of converging, and that's annoying to have GPUs but can not be able to use them if I want to converge.\r\n\r\nAlthough the difference here is quite small, I invite you to repeat exactly the same code without the ```tf.nn.bias_add``` and the difference between results is significant.", "Hello everyone,\r\n\r\nI got the same issue. When I tried to train and test a simple CNN model (code below) with keras. Then when I run model on CPU it got the validation accuracy max: 78%, while GPU gave the result: 75% only. I run it with 25 epochs.\r\nSource code as following:\r\n# Importing the Keras libraries and packages\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Conv2D\r\nfrom keras.layers import MaxPooling2D\r\nfrom keras.layers import Flatten\r\nfrom keras.layers import Dense\r\n\r\n# Part 1 - Creating the model\r\n# Initialising the CNN\r\nclassifier = Sequential()\r\n# Step 1 - Convolution\r\nclassifier.add(Conv2D(32,(3,3),input_shape=(64,64,3),activation='relu'))\r\n# Step 2 - Pooling\r\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\r\n# Adding a second convolutional layer\r\nclassifier.add(Conv2D(32,(3,3),activation='relu'))\r\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\r\n# Step 3 - Flattening\r\nclassifier.add(Flatten())\r\n# Step 4 - Full connection\r\nclassifier.add(Dense(units=128,activation='relu'))\r\nclassifier.add(Dense(units=1,activation='sigmoid'))\r\n# Compiling the CNN\r\nclassifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\r\n\r\n# Part 2 - Fitting the CNN to the images\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\ntrain_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\r\ntest_datagen = ImageDataGenerator(rescale=1./255)\r\ntraining_set = train_datagen.flow_from_directory('dataset/training_set',target_size=(64,64),batch_size=32,class_mode='binary')\r\ntest_set = test_datagen.flow_from_directory('dataset/test_set',target_size=(64,64),batch_size=32,class_mode='binary')\r\n\r\n# Let's fit the data to our model\r\nclassifier.fit_generator(training_set,steps_per_epoch=8000,epochs=25,validation_data=test_set,validation_steps=2000)\r\n\r\n# Part 3 - Making new predictions from our trained model\r\nimport numpy as np\r\nfrom keras.preprocessing import image\r\ntest_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg',target_size=(64,64))\r\ntest_image = image.img_to_array(test_image)\r\ntest_image = np.expand_dims(test_image,axis=0)\r\nresult = classifier.predict(test_image)\r\ntraining_set.class_indices\r\nif result[0][0]==1:\r\n    prediction = 'dog'\r\n    print('This is a dog!')\r\nelse:\r\n    prediction = 'cat'\r\n    print('This is a cat')\r\n\r\n# End CNN example\r\n\r\nThe difference is about 3-4%.  I have tried serveral times but the behaviour is unchanged. \r\nSo, If you want to try an image classification model (cats vs dogs), then I invite you to use code and try run on your CPU, GPU to verify. The data for model [here ](https://drive.google.com/drive/folders/1XaFM8BJFligrqeQdE-_5Id0V_SubJAZe?usp=sharing)and the origional source code [here](https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8).\r\n", "I believe, this issue drives to divergence in https://github.com/Kyubyong/dc_tts/issues/21", "I have read the mentioned thread so if I understand well, your advice is to upgrade my tf version to 1.9 (knowing that even with TF 1.7 it does not work)?", "I believe the issue from @bdenoun may be different from the original issue since the difference between the CPU and GPU outputs looks like a numerical precision difference. In the post from @trungpx, the difference (few % of accuracy) also appears after several epochs. I believe this instability behavior may be expected. In the original post, the discrepancy between the CPU and GPU outputs is several order of magnitude bigger than that and after a single operation. \r\n\r\nNonetheless, it is interesting to note that @bdenoun got this numerical instability in a very simple graph involving a transpose. Although I have never managed to replicate the original issue in a very simple graph, my problem also started to appear after adding some transpose operations in my graph. Maybe the 2 issues are linked.\r\n\r\nI would like to add that I have recently updated my Tensorflow version to 1.9 and I still observe the exact same issue as in the original post. I will give it one more try to replicate the problem in a simple test with TF 1.9", "@annemenini , as I mentioned before, if you remove the `tf.nn.add_bias` you will also notice a huge difference (several order of magnitude).", "### Issue with tf.transpose & tf.conj on GPU:\r\nI have finally managed to reproduce a problem very similar from the initial post into a simple test case below. The problem seems to be linked to the `tf.transpose` and `tf.conj` in my case. Indeed, in the test case below, the `conjugate` argument of the `tf.transpose` is ignored when executed on GPU.\r\n\r\n### Source:\r\n```\r\nimport os\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\r\n\r\ninput1 = tf.placeholder(tf.complex64, shape=[None, None, None, None], name=\"input1\")\r\ninput2 = tf.placeholder(tf.complex64, shape=[None, None, None, None], name=\"input2\")\r\ninput3 = tf.placeholder(tf.complex64, shape=[None, None, None, None], name=\"input3\")\r\n\r\ninput1 = tf.transpose(input1, perm=[0, 3, 1, 2], conjugate=False)\r\ninput2 = tf.transpose(input2, perm=[0, 3, 1, 2], conjugate=True)\r\ninput3 = tf.transpose(input3, perm=[0, 3, 1, 2])\r\ninput3 = tf.conj(input3)\r\n\r\noutput1 = tf.Print(input1, [tf.real(input1), tf.imag(input1)], \"output1: \", name=\"output1\")\r\noutput2 = tf.Print(input2, [tf.real(input2), tf.imag(input2)], \"output2: \", name=\"output2\")\r\noutput3 = tf.Print(input3, [tf.real(input3), tf.imag(input3)], \"output3: \", name=\"output3\")\r\n\r\nnp.random.seed(seed=0)\r\na = np.random.rand(1, 16, 32, 8) + 1j * np.random.rand(1, 16, 32, 8)\r\n\r\nsess = tf.InteractiveSession()\r\n\r\nb, c, d = sess.run([\"output1:0\", \"output2:0\", \"output3:0\"], {\"input1:0\": a, \"input2:0\": a, \"input3:0\": a})\r\n```\r\n### Console output with GPU execution:\r\n```\r\noutput2: [[[[0.548813522 0.963662744 0.0202183966]]]...][[[[0.318294257 0.612427294 0.152561143]]]...]\r\noutput3: [[[[0.548813522 0.963662744 0.0202183966]]]...][[[[-0.318294257 -0.612427294 -0.152561143]]]...]\r\noutput1: [[[[0.548813522 0.963662744 0.0202183966]]]...][[[[0.318294257 0.612427294 0.152561143]]]...]\r\n```\r\nNote the signs of the imaginary part of output2, it should be the opposite of output1, and the same as output3 (the imaginary part is the second half of each printed line).\r\n### Console output with CPU execution:\r\n```\r\noutput2: [[[[0.548813522 0.963662744 0.0202183966]]]...][[[[-0.318294257 -0.612427294 -0.152561143]]]...]\r\noutput3: [[[[0.548813522 0.963662744 0.0202183966]]]...][[[[-0.318294257 -0.612427294 -0.152561143]]]...]\r\noutput1: [[[[0.548813522 0.963662744 0.0202183966]]]...][[[[0.318294257 0.612427294 0.152561143]]]...]\r\n```\r\nCPU execution was obtained from the same machine, just by uncommenting the line `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"`\r\nIn this case, the signs are correct.\r\n### Additional notes:\r\n\r\n- I could reproduce the problem in another machine with the following config: tensorflow-gpu 1.7.0 (previsouly tensorflow-gpu 1.9.0), device: Tesla V100-SXM2-32GB (previously GeForce GTX 1080 Ti)\r\n\r\n- In the original source code where the issue was discovered, even the graph corresponding to `ouput3` in the test case above is failing to properly process the conjugate operation, I don't manage to reproduce that problem in the test case. ", "@zheng-xq Hi, could you please look into this ?", "There seems to be a bug in SwapDimension1And2InTensor3WithNarrowMatrices from\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc.\r\nThe method ignores the template parameter bool conjugate.\r\n\r\n", "Hello,\r\nI have the same problem. I trained a CNN model using keras over Tensorflow and GPU shows higher accuracy than CPU regarding that the same source code is used, and I also repeated the experiment nearly 10 times to make sure of the results. Any suggestions.", "I have a similar problem... I created two Google cloud instances, one with a GPU attached, and the tensorflow-gpu conda package installed and the other with only vCPUs, and the tensorflow conda package. When running the same CNN on the same data, the CPU version gave better accuracy and loss than the GPU version.", "I also face the similar issue with my Custom TF code. Is it solved?", "Had the same issue by having different results when training with cpu and gpu. The problem went away when I upgraded to tf 2.0.0-alpha.", "I have the same problem with Keras using the tensorflow/tensorflow-gpu backend. I'm able to train the model and inference in GPU (with tensorflow-gpu-1.12.0 installed) but when I use the same codebase to run inference in another system that is using tensorflow-1.13.0, I get NaNs as output regardless of the inputs. \r\n", "Still experiencing this with a custom Keras model using `tensorflow-gpu==2.0.0-beta1`. If I train on the CPU via `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"`, my model starts to learn immediately, and the loss goes down as expected. If I change that line to `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"` to train on the GPU and make no other changes, the model gets stuck in a local minimum, and the loss is stagnant even after many epochs of training. However, if I call `model.fit()` a second time with a smaller batch size, it will eventually escape the local minimum and the loss will start decreasing. This is occurring with multiple training datasets of varying size. The model does include a `tf.transpose()` call, which others have mentioned, but it doesn't involve any convolution.", "I had same issue while using tf-cpu and gpu with 2.0.0. \r\nIt seems solved in **tf-nightly-cpu==2.1.0-dev20200102**", "Just want to add that I have the same issue. In addition, I have the same issue on using other libraries like Spacy. I have a feeling that if the training load is not very intensive (can be done on AWS sagemaker M4 or C5 instances by 1-3 hours), using gpu is NOT a good option.", "I am experiencing the same issue using Magenta 1.2.2 on TF 1.15.2: when using GPU, the loss doesn't decrease and the model doesn't learn. Running the same code on the same machine with CPU starts learning immediately. I have tried learning rates of different orders of magnitude and there is no difference.\r\n\r\nThis behavior was also reported [here](https://groups.google.com/a/tensorflow.org/forum/#!topic/magenta-discuss/6r6IFBpP4n8)", "a guess is that this might not be the problem of tf but a glitch in CUDA.", "I have tried the code with same results on Google Colab w/CUDA 10.0 and a P100 on a HPC cluster with CUDA 10.1. \r\n\r\n@BaoshengHeTR do you have any suggestions?", "> I have tried the code with same results on Google Colab w/CUDA 10.0 and a P100 on a HPC cluster with CUDA 10.1.\r\n> \r\n> @BaoshengHeTR do you have any suggestions?\r\n\r\ntwo possible suggestions is to tweak your shuffle before each epoch and try to change your batchsize, and change your weight initilization. One guess is: probably training a model from scratch could have some problem because some digit precision issue, but fine-tuning work should not be affected.\r\n\r\nPlease share your following attempts.", "> > I have tried the code with same results on Google Colab w/CUDA 10.0 and a P100 on a HPC cluster with CUDA 10.1.\r\n> > @BaoshengHeTR do you have any suggestions?\r\n> \r\n> two possible suggestions is to tweak your shuffle before each epoch and try to change your batchsize, and change your weight initilization. One guess is: probably training a model from scratch could have some problem because some digit precision issue, but fine-tuning work should not be affected.\r\n> \r\n> Please share your following attempts.\r\n\r\nHi @BaoshengHeTR,\r\n\r\nI tried your suggestions: \r\n\r\n- the tensors are shuffled when loaded for training. \r\n- I tried different batch sizes and the learning wasn't affected, the loss remained constant. \r\n- Since training w/CPU actually decreases the loss, I used CPU training-based checkpoints as starting point to GPU training. The loss started at the same point left by the CPU, but then, instead of diminish, it went up, and then it was stabilized at a certain value without significant changes for a number of epochs.", "> > > I have tried the code with same results on Google Colab w/CUDA 10.0 and a P100 on a HPC cluster with CUDA 10.1.\r\n> > > @BaoshengHeTR do you have any suggestions?\r\n> > \r\n> > \r\n> > two possible suggestions is to tweak your shuffle before each epoch and try to change your batchsize, and change your weight initilization. One guess is: probably training a model from scratch could have some problem because some digit precision issue, but fine-tuning work should not be affected.\r\n> > Please share your following attempts.\r\n> \r\n> Hi @BaoshengHeTR,\r\n> \r\n> I tried your suggestions:\r\n> \r\n> * the tensors are shuffled when loaded for training.\r\n> * I tried different batch sizes and the learning wasn't affected, the loss remained constant.\r\n> * Since training w/CPU actually decreases the loss, I used CPU training-based checkpoints as starting point to GPU training. The loss started at the same point left by the CPU, but then, instead of diminish, it went up, and then it was stabilized at a certain value without significant changes for a number of epochs.\r\n\r\nVery strange. Is that possible to share your repo so we can repeat your test? I am working on some Bert projects, and haven't done much test, but will give my updates.", "Hi @BaoshengHeTR,\r\n\r\nHere is a Colab repository with a minimal working example so that you can repeat my test. The code downloads and installs Magenta from the TensorFlow Github organization, and a small dataset for testing the training.\r\n\r\nhttps://colab.research.google.com/drive/1ZVMYB-UPJIu7ooNYboEKotC6qatHWgBy\r\n\r\nYou will see that training with GPU lead to s stagnant loss (no learning at all), but using the same code and dataset with CPU lead to a decrease in the loss. \r\n\r\nAny insights would be very much appreciated.", "I know this thread is old but was there any resolution to this, I'm having the same problems.", "@OmriSteiner, are you planning on submitting a PR for the commit above? I plan on testing this in the so let me know if you need a second verification to get it merged. Cheers for the work!\r\n", "@ziegenbalg Yes, I am planning on submitting a PR once my employer signs the legal documents required.\r\nI believe my branch fixes the conjugate transpose issue. You're welcome to test it.", "Had the same issue, after upgrading to tf 2.3.0 results are the same for cpu and gpu.", "I think this was resolved. I couldn't reproduce the issue with recent `TF1.15.5`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/3ff8b088cafe1213d19255410afd3486/untitled.ipynb) is a gist with CPU code as mentioned [here](https://github.com/tensorflow/tensorflow/issues/19200#issuecomment-398448108).\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/b72c95c734f217820f4564688950f1e7/untitled1069.ipynb) is a gist with GPU code. Both the results are same.\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19200\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19200\">No</a>\n"]}, {"number": 19199, "title": "Fix warning in python 3 with deprecated inspect.getargspec", "body": "This fix tries to address the issue raised in #16152 where a warning will show up in python 3 with:\r\n```\r\nimport tensorflow as tf\r\n\r\nimport warnings\r\nwarnings.filterwarnings('error')\r\n\r\ntf.reduce_sum(tf.placeholder(tf.float64))\r\n......\r\nDeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n```\r\n\r\nThis fixes the issue with getfullargspec in tf_export, which takes into consideration the python 2 vs python 3.\r\n\r\nThis fix fixes #16152.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19198, "title": "__hadd() is ambiguous when EIGEN_CUDA_ARCH >= 530", "body": "### System information\r\n- **Have I written custom code**: I change the CUDA capabilities to 6.1 and 7.0.\r\n- **OS Platform and Distribution**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: I'm compiling the source code.\r\n- **TensorFlow version (use command below)**: branch `r1.8`, 8753e2ebde6c58b56675cc19ab7ff83072824a62\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: No\r\n- **GCC/Compiler version (if compiling from source)**: VS 2017(v141), but v140 for CUDA host compiler\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.1\r\n- **GPU model and memory**: 1080 Ti, Titan V\r\n- **Exact command to reproduce**: cmake-gui, enable GPU, and change CUDA host compiler to v140\r\n\r\n### Describe the problem\r\n[__hadd()](https://github.com/eigenteam/eigen-git-mirror/blob/2bdd9e80c43ee491fb0cb299940995e094b1647b/Eigen/src/Core/arch/CUDA/Half.h#L212) is ambiguous when [EIGEN_CUDA_ARCH >= 530](https://github.com/eigenteam/eigen-git-mirror/blob/2bdd9e80c43ee491fb0cb299940995e094b1647b/Eigen/src/Core/arch/CUDA/Half.h#L204).\r\n\r\nThe following is where the ambiguity comes from found in VS 2017:\r\n![image](https://user-images.githubusercontent.com/6009211/39873593-f19954bc-549d-11e8-8bb6-3c02e33c44c5.png)\r\n\r\n### Source code / logs\r\n`tf_core_gpu_kernels` compilation fails because of this problem:\r\n```\r\n42>Building NVCC (Device) object CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Release/tf_core_gpu_kernels_generated_check_numerics_op_gpu.cu.cc.obj\r\n42>check_numerics_op_gpu.cu.cc\r\n42>e:\\program\\ml\\tensorflow_build_05-10-01\\external\\eigen_archive\\eigen\\src/Core/arch/CUDA/Half.h(212): error : more than one instance of overloaded function \"__hadd\" matches the argument list:\r\n42>            function \"__hadd(int, int)\"\r\n42>            function \"__hadd(__half, __half)\"\r\n42>            argument types are: (const Eigen::half, const Eigen::half)\r\n```\r\n\r\nI'm confused that nobody has every post such an issue.\r\nNobody has ever tried changing CUDA capabilities to >=5.3?\r\nOr is there something wrong with my environment?\r\n\r\nIt seems that this is a pure Eigen issue...\r\n", "comments": ["Find a bug report on the Eigen forum:\r\nhttp://eigen.tuxfamily.org/bz/show_bug.cgi?id=1526\r\n\r\nA patch is posted there which is helpful.", "@rmlarsen @benoitsteiner : Mind taking a look?", "@asimshankar thanks, we should apply the patch upstream.", "I just submitted a PR so you can do this kind of patching cleanly. Added a flag `eigen_PATCH_FILE` to cmake build so you can apply the patch automatically at the right time in the build. Also linked a patch file there based off the one in the bug report that works on the current version of eigen in tensorflow.\r\n\r\nNot that this shouldn't be fixed upstream, but it is nice to have a little flexibility with patching as well.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/21119", "What configuration is used to build the official Windows build? With the recent change (commit aef000ed3c 10 days ago), the official Windows build should break with similar issue.", "https://ci.tensorflow.org/job/tf-master-win-gpu-cmake/2300/console\r\n```\r\n20:49:28          c:\\tf_jenkins\\workspace\\tf-master-win-gpu-cmake\\cmake_build\\external\\eigen_archive\\eigen\\src/Core/arch/CUDA/Half.h(212): error : more than one instance of overloaded function \"__hadd\" matches the argument list: [C:\\tf_jenkins\\workspace\\tf-master-win-gpu-cmake\\cmake_build\\tf_core_gpu_kernels.vcxproj]\r\n```\r\n\r\nProbably not worth putting the patch into CI if eigen itself will be fixed soon. It might even have some updates already. The build is pinned to a specific version.", "Is there any way of integrating the Eigen fix into the r1.10 tensorflow build using Bazel? This is currently breaking my Windows build, and I'm not sure how to slipstream the Eigen patch file into the Bazel build.\r\n\r\nIs there any way to have r1.10 download a different version of Eigen during the build process?", "add the following line to tensorflow/workspace.bzl for eigen_archive. Copy patch file to third_party directory.\r\n        patch_file = clean_dep(\"//third_party:eigen_half.patch\"),\r\n", "@ruanjiandong Many thanks for the reply!\r\n\r\nIf I follow your instructions, can I use the patch file I download from here, which fixes this issue: http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1526\r\n\r\nOr do I need to re-write that patch file to a different format which Bazel will understand?\r\n\r\nEDIT: Looks like that patch file doesn't work out of the box. You definitely need to edit it to make it compatible with the way that Bazel calls it, which is patch -p1 -d . -i patch_file\r\n\r\nIf you could please provide some guidance on how to fix the patch file so that Bazel can use it correctly, I would greatly appreciate it.", "The patch file in eigen site won't match tensorflow 1.10. For tensorflow 1.10, you can use patch file https://gist.github.com/bstriner/a7fb0a8da1f830900fa932652439ed44", "@trias702 @bstriner would you like to upstream this patch into eigen?\r\nThen we can simply bump TF eigen dependency to get the fix?", "That would work well for me.\n\nI'm currently building with CMake using a hack which upstreams the patch,\nalso did a similar thing for Bazel.\n\nBut yeah, if you can amend TF master to do it automatically for\nBazel/Cmake, that would be great.\n\n\nOn Sat, 8 Sep 2018, 00:24 Gunhan Gulsoy, <notifications@github.com> wrote:\n\n> @trias702 <https://github.com/trias702> @bstriner\n> <https://github.com/bstriner> would you like to upstream this patch into\n> eigen?\n> Then we can simply bump TF eigen dependency to get the fix?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19198#issuecomment-419617020>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYqzNISh04jUr6fSDhkFQ-TfakN66503ks5uY2KkgaJpZM4T5876>\n> .\n>\n", "@gunan , I've submitted eigen pull request. For status, see https://bitbucket.org/eigen/eigen/pull-requests/481\r\n", "@gunan eigen fix is merged into main development branch. The download link is https://bitbucket.org/eigen/eigen/get/52517c8764ad.tar.gz\r\n", "I tried to apply the changes:\r\nhttps://github.com/tensorflow/tensorflow/compare/master...yongtang:19198-eigen\r\n\r\nThough it looks like there are build failures. Will take a look and see if the build failure could be fixed.", "Thanks for upstreaming the change into eigen.\r\nI learned that our upgrade of eigen is blocked right now due to internally and externally a few eigen pieces being out of sync.\r\n@rmlarsen is right now working to get this resolved. We will then upgrade our eigen dependency once all issues are resolved. However, the current issue is a rarther involved one and it may take up to a couple of weeks.", "We need one more fix in eigen to fix the break introduced by the new eigen \"block\" feature. I created a pull request in eigen repo and reply here once it is merged.\r\n\r\nThe remaining changes (10+ files) can be done tensorflow repo. They're mostly for renaming symbols. @rmlarsen , I can make the tensorflow change if you haven't started.", "Similar issue compiling:\r\n\r\n- Windows 10 Pro\r\n- VS 2017 compiler 14.15.26726\r\n- Windows SDK 10.0.17763.0\r\n- Cuda 10.0\r\n- Cudnn 7.3\r\n- Cuda Compute 7.5\r\n- Python 3.6.6\r\n- Bazel 0.17.2\r\n\r\nBreaks the compilation, as shown below:\r\n```\r\n\r\nc:\\users\\joey\\_bazel_joey\\juz2ghmw\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/arch/CUDA/Half.h(212): error: more than one instance of overloaded function \"__hadd\" matches the argument list:\r\n            function \"__hadd(int, int)\"\r\n            function \"__hadd(__half, __half)\"\r\n            argument types are: (const Eigen::half, const Eigen::half)\r\n```\r\n\r\n@ruanjiandong Any updates?", "Nagging Assignee @rmlarsen: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think eigen upgrade is complete.\r\nwill close this bug.", "What commit was Eigen bumped in? Not in any of the releases at the moment yes?", "> I think eigen upgrade is complete.\r\n> will close this bug.\r\n\r\n@gunan In fact even for `Tensorflow v1.12.0` and `Bazel 0.15.0` on Windows the bug still exists when building GPU version.", "1.12 did not have the eigen version upgrade.\r\nI do not think we will backport the fix to 1.12, as it was quite a large and complicated change to incorporate.", "> > I think eigen upgrade is complete.\r\n> > will close this bug.\r\n> \r\n> @gunan In fact even for `Tensorflow v1.12.0` and `Bazel 0.15.0` on Windows the bug still exists when building GPU version.\r\n\r\nso, how to solve this problem", "Please see https://github.com/tensorflow/tensorflow/issues/19198#issuecomment-456508088\r\n\r\nAt the moment, 1.12 branch is more than 1 year old.\r\nTherefore, we have completely dropped support for that version.\r\nPlease use a newer branch."]}, {"number": 19197, "title": "Updated index.md", "body": "Tensorflow lite image was not visible. Added image file path", "comments": ["@av8ramit and @rmlarsen failing issue is not related to my commit. I made change in one md file only."]}, {"number": 19196, "title": "the program is not running on GPU", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- \r\n    OS Platform and Distribution: windows 10\r\n    **TensorFlow installed from **: pip2 install tensorflow-gpu\r\n    TensorFlow version: ('v1.4.0')\r\n    Python version: Python 3.6.2\r\n    CUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nproblem \r\nwhile i run my program and check the performance on task manager it is not using GPU 1 Which is NVIDIA 940MX, it runs on INTEL GRAPHICS \r\n\r\n\r\ni checked it by writing this line of code on my editor\r\nimport tensorflow as tf\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\n the out put message is the following \r\n2018-05-10 14:26:55.271501: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2018-05-10 14:26:55.955641: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.2415\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.66GiB\r\n2018-05-10 14:26:55.955771: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0\r\n2018-05-10 14:26:56.084191: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\direct_session.cc:299] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0\r\n\r\n\r\n", "comments": ["Hi, It is using GPU 0 based on following:\r\n\r\n`Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)`\r\n\r\nYou can always use CUDA_VISIBLE_DEVICES or tf.device to specify GPU1. \r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\r\n", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@achalshah20 \r\nthanks it is running on GPU 0 but still i am in question of why it is saying GPU 0 because on device list NVIDIA 940MX is GPU 1 if u can make me clear with this ", "Hi, can you post the output of nvidia-smi command?\n\nOn Fri, May 11, 2018 at 1:12 AM selomie <notifications@github.com> wrote:\n\n> @achalshah20 <https://github.com/achalshah20>\n> thanks it is running on GPU 0 but still i am in question of why it is\n> saying GPU 0 because on device list NVIDIA 940MX is GPU 1 if u can make me\n> clear with this\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19196#issuecomment-388293376>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJUeLV-ZK9Lpkj9Zp1WlC4hTgY9EC3Byks5txUf9gaJpZM4T5wIq>\n> .\n>\n-- \n-- Achal Shah (+1-812-272-3004)\n"]}, {"number": 19195, "title": "Build TF lite benchmark model failed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:('v1.7.0-3-g024aecf414', '1.7.0')\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**:0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:No\r\n- **GPU model and memory**:No\r\n- **Exact command to reproduce**:\r\n\r\nI want to build tensorflow lite benchmark model for Android, so I change the WORKSPACE like follows:\r\n```\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 26,\r\n    # Ensure that you have the build_tools_version below installed in the\r\n    # SDK manager as it updates periodically.\r\n    build_tools_version = \"26.0.1\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/home/libin11/Android/Sdk\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/home/libin11/opt/android-ndk-r14b/\",\r\n    # This needs to be 14 or higher to compile TensorFlow.\r\n    # Please specify API level to >= 21 to build for 64-bit\r\n    # archtectures or the Android NDK will automatically select biggest\r\n    # API level that it supports without notice.\r\n    # Note that the NDK version is not the API level.\r\n    api_level=21)\r\n```\r\nAnd then build:\r\n```\r\nbazel build --config android_arm64 --config monolithic --cxxopt=-std=c++11 //tensorflow/contrib/lite/tools:benchmark_model\r\n```\r\nBut it failed after building awhile:\r\n```\r\nERROR: /home/libin11/workspace/tensorflow/tensorflow/contrib/lite/tools/BUILD:31:1: C++ compilation of rule '//tensorflow/contrib/lite/tools:benchmark_model' failed (Exit 1)\r\ntensorflow/contrib/lite/tools/benchmark_model.cc:306:37: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?\r\n                      int num_runs, int64* total_time_us) {\r\n                                    ^~~~~\r\n                                    tensorflow::int64\r\n./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here\r\ntypedef long long int64;\r\n                  ^\r\ntensorflow/contrib/lite/tools/benchmark_model.cc:425:3: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?\r\n  int64 initialization_start_us = Env::Default()->NowMicros();\r\n  ^~~~~\r\n  tensorflow::int64\r\n./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here\r\ntypedef long long int64;\r\n                  ^\r\ntensorflow/contrib/lite/tools/benchmark_model.cc:436:3: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?\r\n  int64 initialization_end_us = Env::Default()->NowMicros();\r\n  ^~~~~\r\n  tensorflow::int64\r\n./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here\r\ntypedef long long int64;\r\n                  ^\r\ntensorflow/contrib/lite/tools/benchmark_model.cc:447:3: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?\r\n  int64 warmup_time_us = 0;\r\n  ^~~~~\r\n  tensorflow::int64\r\n./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here\r\ntypedef long long int64;\r\n                  ^\r\ntensorflow/contrib/lite/tools/benchmark_model.cc:458:3: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?\r\n  int64 no_stat_time_us = 0;\r\n  ^~~~~\r\n  tensorflow::int64\r\n./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here\r\ntypedef long long int64;\r\n                  ^\r\n5 errors generated.\r\nTarget //tensorflow/contrib/lite/tools:benchmark_model failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 332.588s, Critical Path: 68.13s\r\nINFO: 513 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nSo I change int64 from benchmark_model.cc to int64_t and rebuild..\r\nAnd it failed again:\r\n```\r\nERROR: /home/libin11/workspace/tensorflow/tensorflow/contrib/lite/tools/BUILD:31:1: Linking of rule '//tensorflow/contrib/lite/tools:benchmark_model' failed (Exit 1)\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/core/libandroid_tensorflow_lib_lite.lo(logging.o): In function `tensorflow::internal::LogMessage::GenerateLogMessage()':\r\n/proc/self/cwd/tensorflow/core/platform/default/logging.cc:65: undefined reference to `__android_log_write'\r\nbazel-out/arm64-v8a-opt/bin/external/protobuf_archive/libprotobuf_lite.a(common.o): In function `google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::string const&)':\r\n/proc/self/cwd/external/protobuf_archive/src/google/protobuf/stubs/common.cc:142: undefined reference to `__android_log_write'\r\n/proc/self/cwd/external/protobuf_archive/src/google/protobuf/stubs/common.cc:150: undefined reference to `__android_log_write'\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/contrib/lite/tools:benchmark_model failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 28.330s, Critical Path: 27.94s\r\nINFO: 5 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nSo I add \"-llog\" to benchmark_model's linkopts. Finally, it worked:\r\n```\r\n$ bazel build --config android_arm64 --config monolithic --cxxopt=-std=c++11 //tensorflow/contrib/lite/tools:benchmark_model\r\nWARNING: /home/libin11/.cache/bazel/_bazel_libin11/ee99114ce55f575758aad31c3fa3e774/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/libin11/.cache/bazel/_bazel_libin11/ee99114ce55f575758aad31c3fa3e774/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:batch_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/libin11/workspace/tensorflow/tensorflow/core/BUILD:1245:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nINFO: Analysed target //tensorflow/contrib/lite/tools:benchmark_model (1 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/lite/tools:benchmark_model up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/tools/benchmark_model\r\nINFO: Elapsed time: 23.555s, Critical Path: 18.58s\r\nINFO: 1 process, local.\r\nINFO: Build completed successfully, 2 total actions\r\n```\r\nSo my question is, is this the right way to build tensorflow lite benchmark model?", "comments": ["I sent a PR for this https://github.com/tensorflow/tensorflow/pull/17637", "Looks like we have the same problem..", "Assigning to @aselle since it looks like you're reviewing the PR.", "Nagging Assignee @aselle: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19194, "title": "TypeError: `pred` must be a Tensor, a Variable, or a Python bool", "body": "Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\utils.py\", line 234, in constant_value\r\n    raise TypeError('`pred` must be a Tensor, a Variable, or a Python bool.')\r\nTypeError: `pred` must be a Tensor, a Variable, or a Python bool.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "yes sir\n\nOn Mon, Jun 11, 2018 at 3:32 AM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19194#issuecomment-396085116>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACKnsF1ejlXfWsFDvHyC6DEHP5f5Gxa4ks5t7Zd2gaJpZM4T5ZU7>\n> .\n>\n\n\n\n-- \nWith Best Regards\nDr.Mohd Daoud\nData Scientist\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Close it due to lack of activity.\r\n", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 19192, "title": "classifier.predict can't stop if input_fn return a sample tuple", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.6\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n```python\r\nIn [7]: import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n```\r\n\r\n# Describe the problem\r\nIf `input_fn` return a sample tuple, it won't stop and predict again and again\r\n```python\r\nresult = classifier.predict(lambda: (ev_data,))\r\n```\r\nBut, if `input_fn` return a `tf.data.Dataset`, it work well.\r\n```python\r\nresult = classifier.predict(lambda: eval_input_fn(ev_data, labels=None, batch_size=1))\r\n```\r\nAfter debug, i found a while loop can't stop in here [`while not mon_sess.should_stop()`](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/estimator/estimator.py#L508)\r\n\r\nBut it seem like tuple is supported[](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/estimator/estimator.py#L456)\r\nSo, the return of `input_fn` must be `tf.data.Dataset` type? Or, should I give it a [end-of-input exception](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/estimator/estimator.py#L445)by myself?\r\n\r\n### Source code / logs\r\nHere is my code\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef eval_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for evaluation or prediction\"\"\"\r\n    features = dict(features)\r\n    if labels is None:\r\n        # No labels, use only features.\r\n        inputs = features\r\n    else:\r\n        inputs = (features, labels)\r\n\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\r\n\r\n    # Batch the examples\r\n    assert batch_size is not None, \"batch_size must not be None\"\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\n\r\ntrain_x = {'x1': [0, 1, 0, 1], 'x2': [0, 0, 1, 1]}\r\nlabel = [0, 1, 1, 0]\r\n\r\nev_data = {'x1': [0.5, 0.5, 0.5, 1], 'x2': [0.5, 0, 1, 1]}\r\n\r\ntrain_steps = 1000\r\nmy_feature_columns = []\r\nfor key in train_x.keys():\r\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\r\n\r\nclassifier = tf.estimator.DNNClassifier(\r\n    feature_columns=my_feature_columns,\r\n    hidden_units=[4, 4],\r\n    n_classes=2)\r\n\r\nclassifier.train(lambda: (train_x, label), steps=train_steps)\r\n\r\nresult = classifier.predict(lambda: eval_input_fn(ev_data, labels=None, batch_size=1))\r\n# result = classifier.predict(lambda: (ev_data,)) # it won't stop\r\nfor r in result:\r\n    print(r)\r\n```\r\nIf I return `tf.data.Dataset`, here is output:\r\n```text\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpyu6blxd6\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpyu6blxd6', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5c9dfcdd30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpyu6blxd6/model.ckpt.\r\nINFO:tensorflow:loss = 2.571034, step = 1\r\nINFO:tensorflow:global_step/sec: 792.849\r\nINFO:tensorflow:loss = 1.2854626, step = 101 (0.126 sec)\r\nINFO:tensorflow:global_step/sec: 1621.83\r\nINFO:tensorflow:loss = 0.58735335, step = 201 (0.062 sec)\r\nINFO:tensorflow:global_step/sec: 1585.9\r\nINFO:tensorflow:loss = 0.37925595, step = 301 (0.063 sec)\r\nINFO:tensorflow:global_step/sec: 1579.13\r\nINFO:tensorflow:loss = 0.286068, step = 401 (0.063 sec)\r\nINFO:tensorflow:global_step/sec: 1656.07\r\nINFO:tensorflow:loss = 0.23221238, step = 501 (0.060 sec)\r\nINFO:tensorflow:global_step/sec: 1624.02\r\nINFO:tensorflow:loss = 0.1968127, step = 601 (0.062 sec)\r\nINFO:tensorflow:global_step/sec: 1565.82\r\nINFO:tensorflow:loss = 0.1710473, step = 701 (0.064 sec)\r\nINFO:tensorflow:global_step/sec: 1619.16\r\nINFO:tensorflow:loss = 0.15148035, step = 801 (0.062 sec)\r\nINFO:tensorflow:global_step/sec: 1600.48\r\nINFO:tensorflow:loss = 0.13629705, step = 901 (0.063 sec)\r\nINFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpyu6blxd6/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 0.12395525.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from /tmp/tmpyu6blxd6/model.ckpt-1000\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\n{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}\r\n{'logits': array([2.2674427], dtype=float32), 'logistic': array([0.90614456], dtype=float32), 'probabilities': array([0.09385548, 0.90614456], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\r\n{'logits': array([1.0914161], dtype=float32), 'logistic': array([0.7486483], dtype=float32), 'probabilities': array([0.2513517, 0.7486483], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\r\n{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}\r\n```\r\nBut if I return a sample tuple, the output whould stop like this:\r\n```\r\n...\r\n...\r\n...\r\nINFO:tensorflow:Done running local_init_op.\r\n{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}\r\n{'logits': array([2.2674427], dtype=float32), 'logistic': array([0.90614456], dtype=float32), 'probabilities': array([0.09385548, 0.90614456], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\r\n{'logits': array([1.0914161], dtype=float32), 'logistic': array([0.7486483], dtype=float32), 'probabilities': array([0.2513517, 0.7486483], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\r\n{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}\r\n{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}\r\n{'logits': array([2.2674427], dtype=float32), 'logistic': array([0.90614456], dtype=float32), 'probabilities': array([0.09385548, 0.90614456], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\r\n{'logits': array([1.0914161], dtype=float32), 'logistic': array([0.7486483], dtype=float32), 'probabilities': array([0.2513517, 0.7486483], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\r\n{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}\r\n{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}\r\n{'logits': array([2.2674427], dtype=float32), 'logistic': array([0.90614456], dtype=float32), 'probabilities': array([0.09385548, 0.90614456], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\r\n{'logits': array([1.0914161], dtype=float32), 'logistic': array([0.7486483], dtype=float32), 'probabilities': array([0.2513517, 0.7486483], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\r\n{'logits': array([-2.8130803], dtype=float32), 'logistic': array([0.05662142], dtype=float32), 'probabilities': array([0.94337857, 0.05662142], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}\r\n.....more\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler yes, thank you for your reply, I have updated them but i think none of them are relevant in my case, case I use `tenserflow-cpu` and install it with pip.", "This is expected behavior. See the documentation at https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\r\n\r\nBasically it'll continue until the input_fn throws OutOfRangeError or StopIteration.", "What is the recommended way to stop the iteration?\r\n\r\nwhen we return a value from input_fn, we could not simutaneously raise StopIteration\r\n\r\nThanks.", "The best practice is to convert it to a Dataset. Specifically, consider using tf.data.Dataset.from_generator:\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator\r\n\r\nYou should think of the value returned from input_fn as a Tensor in the graph that you will continually retrieve from. It's not a single value.", "what if a input_fn returns a dictionary of name to tensor that will be consumed by model_fn? how can I stop the predict loop?"]}, {"number": 19191, "title": "GPU cannot use in C++ API when using libtensorflow_cc.so", "body": "My purpose is using C++ api with libtensorflow_cc.so for detection. \r\n\r\nFirst , compile the libtensorflow_cc.so. \r\nI do like;\r\n1   ./configure.       I choose CUDA support, and cuda 9.1 and cudnn 7 is used.\r\n2 bazel the tensorflow .     bazel build -c opt --config=cuda //tensorlfow:libtensorflow_cc.so.\r\nAt last ,complete sucessfully.\r\n\r\nMy code like:\r\n\r\n#include <tensorflow/core/protobuf/meta_graph.pb.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/graph/default_device.h>\r\n#include <tensorflow/core/graph/graph_def_builder.h>\r\n\r\ntensorflow::GraphDef GraphDef;\r\ntensorflow::Session* Session = nullptr;\r\n\r\n1 void LoadGraph()\r\n2 {\r\n3 // Read in the protobuf graph we exported\r\n4 tensorflow::Status Status;\r\n5\r\n6 Status = tensorflow::ReadBinaryProto(tensorflow::Env::Default(), \"my_model.pb\", &GraphDef);\r\n7 if (!Status.ok())\r\n8 {\r\n9 printf(\"Error reading graph definition from %s: %s\\n\", \"my_model.pb\", Status.ToString().c_str());\r\n10 return false;\r\n11 }\r\n12\r\n13 Session = tensorflow::NewSession(tensorflow::SessionOptions());\r\n14 if (Session == nullptr)\r\n15 {\r\n16 printf(\"Could not create Tensorflow session.\\n\");\r\n17 return false;\r\n18 }\r\n19\r\n20 graph::SetDefaultDevice(\"/device:GPU:0\",&GraphDef);\r\n\r\n21 // Add the graph to the session\r\n22 Status = Session->Create(GraphDef);\r\n23 if (!Status.ok())\r\n24 {\r\n25 printf(\"Error creating graph: %s\\n\", Status.ToString().c_str());\r\n26 return false;\r\n27 }\r\n28 }\r\n\r\nBut the code will be get an error.   The line 25 will be run.  That is say in line 23, Status.ok() = false.\r\n\r\nSegmentation fault(core dumped)..   I do not know how to deal with it.\r\n\r\nCan anyone help me? Thank you very much!  Please to me soon !\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hey, Can you try to print status to see what error exactly you are getting?", "@achalshah20 ,Thank you! The statue is :\r\n![qq 20180514170750](https://user-images.githubusercontent.com/28335784/39988286-8c91e31e-5799-11e8-8953-aac49e9262ac.jpg)\r\n", "@tensorflowbutler\uff0cCUDA9.1\uff0cCUDNN7\uff0cGPU K80\uff0cbazel like:\r\n![qq 20180514171905](https://user-images.githubusercontent.com/28335784/39988967-564a6e1e-579b-11e8-8d01-f3134be0f80f.jpg)\r\n\r\ntensorflow is :  git clone https://github.com/tensorflow/tensorflow.git\r\n\r\nubuntu 16.04.\r\n\r\nThe error like above comment!\r\n", "@xiaowenhe Based on your below screenshot, Tensorflow is only able to find CPU device not the gpu device. You can do following things to fix this:\r\n\r\n**1. Make sure tensorflow is compile with cuda (My suspicion is here)**\r\n2. Try running nvidia-smi to see if you have compatible GPU and your driver works\r\n\r\n![image](https://user-images.githubusercontent.com/9772589/40015241-82c5c7d8-5767-11e8-9558-d081ffd9f086.png)\r\n", "@achalshah20 ,Thank you very much! I re-compile the libtensorflow_cc.so again and also compile the tutorials_example_trainer. And I run:   bazel-bin/tensorflow/cc/tutorials_example_trainer  --use_gpu    without error. Now I get another error like(pic 1) :\r\n![4](https://user-images.githubusercontent.com/28335784/40286014-a40b2cbc-5cd5-11e8-8a80-8f38c793b88a.jpg).\r\n\r\n![5](https://user-images.githubusercontent.com/28335784/40286047-ee96807e-5cd5-11e8-8259-8b8e7807fcf2.jpg)\r\n\r\n And when I run the models-master with python, have the same error(pic 2). Maybe the error caused by the insurrficent driver version. \r\n I will update the driver version and try this again!  Thank you very much !", "Hi, you are only building libtensorflow_cc.so with gpu and you are trying\nto run pythong code with gpu, so it shouldn\u2019t work.\n\nYou can build pip package for python with gpu enabled and everything should\nbe fine on python end.\n\nI am not sure about segmentation fault in pic 1. Driver update might help.\n\nOn Sun, May 20, 2018 at 6:11 PM xiaowenhe <notifications@github.com> wrote:\n\n> @achalshah20 <https://github.com/achalshah20> ,Thank you very much! I\n> re-compile the libtensorflow_cc.so again and also compile the\n> tutorials_example_trainer. And I run:\n> bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu without error.\n> Now I get another error like(pic 1) :\n> [image: 4]\n> <https://user-images.githubusercontent.com/28335784/40286014-a40b2cbc-5cd5-11e8-8a80-8f38c793b88a.jpg>\n> .\n>\n> [image: 5]\n> <https://user-images.githubusercontent.com/28335784/40286047-ee96807e-5cd5-11e8-8259-8b8e7807fcf2.jpg>\n>\n> And when I run the models-master with python, have the same error(pic 2).\n> Maybe the error caused by the insurrficent driver version.\n> I will update the driver version and try this again! Thank you very much !\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19191#issuecomment-390527770>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJUeLUnkRvv9uj_qOA7XrPalAExa8dqyks5t0hRIgaJpZM4T5S8c>\n> .\n>\n-- \n-- Achal Shah (+1-812-272-3004)\n", "@achalshah20 , I can run without error now.  It is the driver version problem!  Thank you very much! "]}, {"number": 19190, "title": "[Slim] Fix default value for parameter in tfexample_decoder.BoundingBox", "body": "When not providing any prefix to [`slim.tfexample_decoder.BoundingBox.__init__`,](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py) the default value of `None` for the prefix would raise the error \r\n\r\n> TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\r\n\r\n, because of the line `self._full_keys = [prefix + k for k in keys]`. By changing the default parameter to an empty string, this error is resolved.", "comments": []}, {"number": 19189, "title": "Disable the gradients test for python3 entirely. [DO NOT MERGE]", "body": "", "comments": ["Could you retry running this test after pinning our protobuf version to what it would have been during 1.6 release?\r\nI at least would like to isolate the issue we are seeing.", "This sounds scary. What is your plan for re-enabling the tests? We cannot just disable tests when convenient. They are there for a reason.", "@rmlarsen I am not merging this PR yet. I was just trying to prove my point that Python3.4 might be to blame for this on the 1.6 branch. I've added a DO NOT MERGE tag to be safe.", "Fixed it."]}, {"number": 19188, "title": " Add TensorFlow ecosystem Spark and Hadoop jars to Maven deployment", "body": "Fixes https://github.com/tensorflow/ecosystem/issues/29 by adding TensorFlow ecosystem jars to public Maven repo.\r\n\r\n@jsheu Not sure if this is the best way to do this. There were a couple of hacks I had to do since the poms in the TensorFlow ecosystem are decoupled from the TensorFlow java ones. I added a placeholder poms in tensorflow-hadoop and spark-tensorflow-connector to prevent errors during the initial mvn clean. I overwrite these poms with the ones from the ecosystem. I also added the deployment profiles to the ecosystem poms so we would need to manually change them if the TensorFlow java ones change.\r\n\r\nAn alternative approach I could use is to remove the TensorFlow ecosystem modules from the parent pom, and deploy the TensorFlow java jars first using `deploy_artifacts` then write a separate function to deploy the ecosystem jars. The advantage of this is that we could pass any changes to deployment settings via command-line and we wouldn't need placeholder poms for maven clean.\r\n\r\nThe previous approach I tried was dynamically editing the ecosystem poms using Python XML so that they would inherit from the parent pom but it made a number of hard-coded assumptions about the structure of the poms.\r\n\r\nPlease let me know what changes to make.", "comments": ["The code for this in ecosystem changes so infrequently that I'm not sure it's worth having versions (I think HEAD is fine). We can revisit it later.", "@jhseu : Should we still create branches in the ecosystem repository so that we at least have a snapshot of the exact code used to produce a release (and use that branch here)?", "@asimshankar Yeah, that seems reasonable. I'll add it to the release instructions after this is merged.\r\n\r\n@skavulya Mind modifying this CL to checkout the branch `\"r${TF_VERSION}\"?`", "@asimshankar @jhseu Thanks for the review. I'll make the changes you recommended.", "@skavulya Thanks for the update. @asimshankar another look?", "@rmlarsen : I think @skavulya hasn't made the changes yet.", "@asimshankar Right, sorry about the ping :-)", "@asimshankar @jhseu I made the changes you requested. I was not able to test the full deployment for the latest changes due to errors downloading the nightly builds for the snapshot release, and missing libraries for the proto jar.\r\n\r\nPlease let me know if you would like me to make more changes.", "@asimshankar did you get a chance to look at the pull request? I added the `git checkout \"r${TF_VERSION}\"` suggested by @jhseu but you need to commit it out when testing because the branch does not yet exist in the TensorFlow ecosystem.", "Looks good to me. Asim, did you want to take a final pass?", "Thanks @asimshankar @jhseu. I updated the .gitignore file", "Thanks @asimshankar", "Thanks for making this change, @skavulya!", "Thank *you* @skavulya  :)"]}, {"number": 19187, "title": "Use \"```\" (backtick) for code blocks in adding_an_op.md", "body": "\r\nIn adding_an_op.md, most of the code blocks uses \"```\" (3 backticks)\r\nand annotations are added automatically. Though there was one\r\nplace where the code block are done with manual html code. This\r\nis really error-prune and hard to change if there is an update\r\nin the future.\r\n\r\nThis fix converts to \"```c++\" (backticks) so that it is easy to maintain in the future.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19186, "title": "[tf.keras] Bug with Stateful Metrics & Fit Generator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\nThe stateful metrics integration with fit generator is not working. This can be demonstrated by taking the Keras metrics tests from the latest release and changing the imports to TensorFlow.keras (see https://github.com/keras-team/keras/blob/master/tests/keras/metrics_test.py):\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import backend as K\r\n\r\nimport numpy as np\r\n\r\nclass BinaryTruePositives(tf.keras.layers.Layer):\r\n    \"\"\"Stateful Metric to count the total true positives over all batches.\r\n    Assumes predictions and targets of shape `(samples, 1)`.\r\n    # Arguments\r\n        name: String, name for the metric.\r\n    \"\"\"\r\n\r\n    def __init__(self, name='true_positives', **kwargs):\r\n        super(BinaryTruePositives, self).__init__(name=name, **kwargs)\r\n        self.stateful = True\r\n        self.true_positives = K.variable(value=0, dtype='int32')\r\n\r\n    def reset_states(self):\r\n        K.set_value(self.true_positives, 0)\r\n\r\n    def __call__(self, y_true, y_pred):\r\n        \"\"\"Computes the number of true positives in a batch.\r\n        # Arguments\r\n            y_true: Tensor, batch_wise labels\r\n            y_pred: Tensor, batch_wise predictions\r\n        # Returns\r\n            The total number of true positives seen this epoch at the\r\n                completion of the batch.\r\n        \"\"\"\r\n        y_true = K.cast(y_true, 'int32')\r\n        y_pred = K.cast(K.round(y_pred), 'int32')\r\n        correct_preds = K.cast(K.equal(y_pred, y_true), 'int32')\r\n        true_pos = K.cast(K.sum(correct_preds * y_true), 'int32')\r\n        current_true_pos = self.true_positives * 1\r\n        self.add_update(K.update_add(self.true_positives,\r\n                                     true_pos),\r\n                        inputs=[y_true, y_pred])\r\n        return current_true_pos + true_pos\r\n\r\n# Test on simple model\r\ninputs = tf.keras.Input(shape=(2,))\r\noutputs = tf.keras.layers.Dense(1, activation='sigmoid', name='out')(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\n\r\nmodel.compile(optimizer='sgd',\r\n              loss='binary_crossentropy',\r\n              metrics=['acc', BinaryTruePositives()])\r\n\r\nsamples = 1000\r\nx = np.random.random((samples, 2))\r\ny = np.random.randint(2, size=(samples, 1))\r\n\r\nval_samples = 10\r\nval_x = np.random.random((val_samples, 2))\r\nval_y = np.random.randint(2, size=(val_samples, 1))\r\n\r\n# Test fit and evaluate\r\nhistory = model.fit(x, y, validation_data=(val_x, val_y), epochs=2, batch_size=10)\r\nouts = model.evaluate(x, y, batch_size=10)\r\npreds = model.predict(x)\r\n\r\ndef ref_true_pos(y_true, y_pred):\r\n    return np.sum(np.logical_and(y_pred > 0.5, y_true == 1))\r\n\r\n# Test correctness (e.g. updates should have been run)\r\nnp.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)\r\n\r\n# Test correctness of the validation metric computation\r\nval_preds = model.predict(val_x)\r\nval_outs = model.evaluate(val_x, val_y, batch_size=10)\r\nnp.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)\r\nnp.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)\r\n\r\n# Test with generators\r\ngen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(x, y)]\r\nval_gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(val_x, val_y)]\r\nhistory = model.fit_generator(iter(gen), epochs=1, steps_per_epoch=samples,\r\n                              validation_data=iter(val_gen), validation_steps=val_samples)\r\nouts = model.evaluate_generator(iter(gen), steps=samples)\r\npreds = model.predict_generator(iter(gen), steps=samples)\r\n\r\n# Test correctness of the metric re ref_true_pos()\r\nnp.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)\r\n\r\n# Test correctness of the validation metric computation\r\nval_preds = model.predict_generator(iter(val_gen), steps=val_samples)\r\nval_outs = model.evaluate_generator(iter(val_gen), steps=val_samples)\r\nnp.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)\r\nnp.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)\r\n```\r\n\r\nIn addition the progress bar is not working with fit generator. @fchollet \r\n\r\n", "comments": ["I believe the equivalent tensorflow PR of: https://github.com/keras-team/keras/pull/9446 is missing. But this is beyond my capability at the time being (I have to get approval for each open source contribution...)", "The two key issues:\r\n\r\n1. ```reset_states``` does not get called when ```fit_generator``` is used\r\n\r\n2. Progress bar issues.\r\n\r\nSee  keras-team/keras#9446 \r\n", "@fchollet Can you take a look at this?", "Thanks for the report. This has been fixed internally a few days ago. It appears the fix is not yet on GitHub, but it should appear in the next few days."]}, {"number": 19185, "title": "[Intel MKL] Fixing the unit test \"util_cuda_kernel_helper_test_gpu\" when building with MKL", "body": "", "comments": []}, {"number": 19184, "title": "tf.image.crop_and_resize() return a wrong  values on the Jetson TX2", "body": "same as https://github.com/tensorflow/tensorflow/issues/13890\r\nAnd I have tried tensorflow 1.4.0 , tensorflow 1.4.1 ,tensorflow 1.7 , all of them got a wrong value,and when I use cpu to calculate it \uff0cit returns a right value .\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I'm closing this as a duplicate of #13890. Can you update that thread with your experiences? Thanks!"]}, {"number": 19183, "title": "Enable OrderedEnqueuer from keras in tf.keras", "body": "Goal of this PR is to enable usage of `tf.keras.utils.OrderedEnqueuer` which has the very same functionality `keras.utils.OrderedEnqueuer`, but for some reason was not available in `tf.keras`", "comments": []}, {"number": 19182, "title": "pandas_input_fn taking a DataFrame or dict of Series for y", "body": "# Pandas Input Function for Multilabel Inputs\r\n\r\n`tf.estimator.inputs.pandas_input_fn` should be able to take a DataFarme or a dict with key to Series for y. This would allow multi-label inputs. This is similar to the numpy_input_fn behavior implemented in issue #12610.\r\n\r\nI'd be happy to give the implementation a shot.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This is a feature request.", "CC @mrry \r\n@rachellim - Do you want to take a look at this? Should we be providing a `tf.data.Dataset` path instead/as-well? ", "Thanks for offering to implement this, @nrstott! We\u2019d welcome your contribution.", "Thanks @mrry, will get started on it this evening and see if I can make some progress.", "@mrry I opened a pull request. Will a reviewer be automatically assigned? Is there any further action I need to take?", "@nrstott Thanks for the PR! I've requested a review from someone who's most familiar with that part of the code (and Pandas).", "Closing as this is resolved\r\n\r\n\r\n\r\n"]}, {"number": 19181, "title": "Arrow format for Tensor import/export for cross-platform usage", "body": "This is regarding a feature request in Tensorflow.\r\nIs there any support currently (or will be in the future) for Tensor porting using [Apache Arrow](https://arrow.apache.org/) for cross-platform usage (e.g., Spark-Tensorflow) in python/java?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi. I would suggest asking this question on [tensorflow/ecosystem](https://github.com/tensorflow/ecosystem). Thanks for your interest."]}, {"number": 19180, "title": "tf.constant() processes float16 100 times slower than float32", "body": "```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nimages = tf.keras.datasets.mnist.load_data()[0][0]\r\n\r\nimagesFloat16 = images.astype(np.float16);\r\nimagesFloat32 = images.astype(np.float32);\r\n\r\nstart = time.time()\r\ntf.data.Dataset.from_tensor_slices(imagesFloat32)\r\nend = time.time()\r\nprint(\"float32: {0:.4f} sec\".format(end - start)) # float32: 1.2561 sec\r\n\r\nstart = time.time()\r\ntf.data.Dataset.from_tensor_slices(imagesFloat16)\r\nend = time.time()\r\nprint(\"float16: {0:.4f} sec\".format(end - start)) # float16: 110.0273 sec\r\n```\r\n\r\n> float32: 1.2561 sec\r\n> float16: 110.0273 sec", "comments": ["The `Dataset.from_tensor_slices()` method should mostly be a wrapper for `tf.constant()` (via `tf.convert_to_tensor()`). Can you please confirm if the following program has the same behavior?\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nimages = tf.keras.datasets.mnist.load_data()[0][0]\r\n\r\nimagesFloat16 = images.astype(np.float16);\r\nimagesFloat32 = images.astype(np.float32);\r\n\r\nstart = time.time()\r\ntf.constant(imagesFloat32)\r\nend = time.time()\r\nprint(\"float32: {0:.4f} sec\".format(end - start))\r\n\r\nstart = time.time()\r\ntf.constant(imagesFloat16)\r\nend = time.time()\r\nprint(\"float16: {0:.4f} sec\".format(end - start))\r\n```", "When I ran the version with `tf.constant()`, I got the following results:\r\n\r\n> float32: 0.5304 sec\r\n> float16: 113.5471 sec\r\n\r\n...so I'll update the bug title to reflect that.", "Not my part of the code, so I'm opening this up to contributions from within or outwith the team. Looks like somebody would need to write a function called `fast_tensor_util.AppendFloat16ArrayToTensorProto()` in [this file](https://github.com/tensorflow/tensorflow/blob/f08f24cd559b5824a1874a0e76d339875e43f366/tensorflow/python/framework/fast_tensor_util.pyx) and call it [from here](https://github.com/tensorflow/tensorflow/blob/f08f24cd559b5824a1874a0e76d339875e43f366/tensorflow/python/framework/tensor_util.py#L71).", "Mind if I take this case? :) ", "@faintedremix We\u2019d appreciate it!", "@faintedremix Sorry that I have wrote a PR #19212  before I see your comment. However, my PR can't fix the problem totally, you can take over it  if you have a better solution :-)", "Ah I didn't check the comment when creating the PR #19213. Let me close the PR #19213 for now.", "@faintedremix Have you taken this up and started working ? If not, please let me know I'm happy to take over or if you have opened a PR, I'm happy to pitch in and help", "@pbanavara The issue has been fixed in PR #19212 I think. Also there is another PR #19533 that is pending which I think will address the similar `bfloat16` issue.", "Closing as the fixing PR is merged. If that is incorrect, please reopen."]}, {"number": 19179, "title": "Enable test case for float64 with conv1d", "body": "The float64 for conv2d support has been added to tensorflow in e3468b56d323783fdfb79fa2d6c24effc58bcaa9. (Thanks brianwa84!)\r\nSince conv1d implementation invokes conv2d, the float64 support for conv1d is supported now as well.\r\n\r\nThis fix adds the test case for float64 support of conv1d and removes the TODO.\r\n\r\nThis fix fixes #19175.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19178, "title": "Race condition when initializing variables in control_dependencies", "body": "### System information\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin Mikes-MBP.enversion.com 16.7.0 Darwin Kernel Version 16.7.0: Mon Nov 13 21:56:25 PST 2017; root:xnu-3789.72.11~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.39.2)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Mikes-MBP.enversion.com 16.7.0 Darwin Kernel Version 16.7.0: Mon Nov 13 21:56:25 PST 2017; root:xnu-3789.72.11~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\n### Describe the problem\r\n\r\nGiven how `Variable` and `control_dependencies` are documented, I would expect the following to work:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    v = tf.Variable(tf.zeros([100,100]))\r\n    with tf.control_dependencies([v.initializer]):\r\n        d = tf.Print(v, [v])\r\n    print(sess.run(d))\r\n```\r\n\r\nBut this seems to create a race condition. Sometimes it executes fine, sometimes it errors with `FailedPreconditionError: Attempting to use uninitialized value Variable`. It is unclear to me if the use of initializers in `control_dependencies` is valid. If it is not, the documentation should probably better reflect this.\r\n\r\n### Source code / logs\r\n\r\nFull traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable\r\n         [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable/Assign\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 105, in <module>\r\n    print(sess.run(d))\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable\r\n         [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable/Assign\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable)]]\r\n\r\nCaused by op 'Variable/read', defined at:\r\n  File \"test.py\", line 101, in <module>\r\n    v = tf.Variable(tf.zeros([100,100]))\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\r\n    constraint=constraint)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 397, in _init_from_args\r\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 142, in identity\r\n    return gen_array_ops.identity(input, name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3187, in identity\r\n    \"Identity\", input=input, name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\r\n         [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable/Assign\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable)]]\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 19177, "title": "Fix 2 typos in documents", "body": "1. In the line 240 of the [programmer_guide/variables.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/variables.md), the name of API `tf.layer` is wrong, should be `tf.layers`;\r\n2. The line 212 of the [tutorials/layers.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/layers.md) is duplicated, which already apears in the line 207.", "comments": []}, {"number": 19176, "title": "selu alpha: why a different value from the ref paper?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n\r\n### Describe the problem\r\n\r\nIn the source, I see that the value of the `alpha` constant chosen for the `selu` activation is 1.7580993408473768599402175208123.\r\n\r\nHowever, in the [reference paper ](https://arxiv.org/abs/1706.02515) given in the [documentation](https://www.tensorflow.org/api_docs/python/tf/nn/selu), the value is 1.673263242354377.\r\n\r\nWhy the difference? If there is published work about this update could you please add it to the documentation. If not could you please motivate this choice?\r\n\r\n### Source code / logs\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/relu_op_functor.h#L139\r\n", "comments": ["Never mind, I just realized this constant is \u03bb\u00d7\u03b1, not \u03b1."]}, {"number": 19175, "title": "1-D convolution of tensorflow do not support float64 exactly.", "body": "In the description of tf.nn.conv1d, it said \"value: A 3D Tensor. Must be of type float32 or float64\". But the float64 isn't supported correctly. Some one could try the code below:\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\na = np.random.randn(12, 228, 1)\r\nw = np.random.randn(2, 1, 18)\r\nsess = tf.Session()\r\nc = sess.run(tf.nn.conv1d(tf.cast(a, tf.float64), tf.cast(w, tf.float64), stride=1, padding='SAME'))\r\nsess.close()\r\n\r\nI have test it in tensorflow 1.4 and 1.7,  they raised the error : \r\nValue passed to parameter 'input' has DataType float64 not in list of allowed values: float16, float32", "comments": ["@loewy43 The float64 support for conv2d has been added e3468b5. The conv1d implementation invokes conv2d, so float64 support for conv1d is already available as well. (The support starts with 1.8.0).\r\n\r\nI have added a test case in #19179 to cover the conv1d.\r\n\r\n"]}, {"number": 19174, "title": "GPU not used and failed call to cuInit: CUDA_ERROR_NO_DEVICE", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.4.0\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:8.0/6.0\r\n- **GPU model and memory**:GeForce GTX 1080 Ti - 12GB\r\n- **NVIDIA driver**: 390.48\r\n\r\nHi, when I test tensorflow in my machine, run such code : \r\n```\r\nimport tensorflow as tf\r\nimport os\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\r\nsess = tf.Session()\r\nprint (sess.run(c))\r\nsess.close()\r\n```\r\nAnd output are as follow:\r\n```\r\n2018-05-09 14:01:26.787648: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-05-09 14:01:26.787669: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-05-09 14:01:26.787673: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-05-09 14:01:26.787676: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-05-09 14:01:26.787679: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-05-09 14:01:26.788347: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\n2018-05-09 14:01:26.788365: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: axadl-System-Product-Name\r\n2018-05-09 14:01:26.788369: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: axadl-System-Product-Name\r\n2018-05-09 14:01:26.788387: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got \"1\"\r\n2018-05-09 14:01:26.788403: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  390.48  Thu Mar 22 00:42:57 PDT 2018\r\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) \r\n\"\"\"\r\n2018-05-09 14:01:26.788413: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 390.48.0\r\n[[22. 28.]\r\n [49. 64.]]\r\n```\r\n\r\nAnd I input \"nvidia-smi\":\r\n![default](https://user-images.githubusercontent.com/28421690/39800867-e1919820-539b-11e8-8403-ea41e672f010.png)\r\nAnd I input \"nvcc -V\":\r\n![default](https://user-images.githubusercontent.com/28421690/39800880-f265c450-539b-11e8-9f34-2ba41e1bc653.png)\r\nThen I test cuda \"sudo ./1_Utilities/deviceQuery/deviceQuery\": \r\n```\r\n./1_Utilities/deviceQuery/deviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce GTX 1080 Ti\"\r\n  CUDA Driver Version / Runtime Version          9.1 / 8.0\r\n  CUDA Capability Major/Minor version number:    6.1\r\n  Total amount of global memory:                 11170 MBytes (11713052672 bytes)\r\n  (28) Multiprocessors, (128) CUDA Cores/MP:     3584 CUDA Cores\r\n  GPU Max Clock rate:                            1683 MHz (1.68 GHz)\r\n  Memory Clock rate:                             5505 Mhz\r\n  Memory Bus Width:                              352-bit\r\n  L2 Cache Size:                                 2883584 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 2 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 1080 Ti\r\nResult = PASS\r\n```\r\n\r\nIn addition, I have tried https://github.com/tensorflow/tensorflow/issues/255, but it still occur mistakes as above.\r\nCan anyone help me?\r\nThanks so much!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Did you install CPU version of tensorflow instead of GPU version by chance?\r\n\r\nTry pip2 install tensorflow-gpu instead of pip2 install tensorflow if that is the case", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I am having the same problem"]}, {"number": 19173, "title": "Tensorflow AttributeError: module 'bottleneck' has no attribute '__version__'", "body": "**Tensorflow version==1.7**\r\n**Tensorhub version==1.0**\r\n\r\n**I am trying to learn tensorflow serving using small tutorial on [Medium blog](https://medium.com/@utsumukiMutsuki/using-inception-v3-from-tensorflow-hub-for-transfer-learning-a931ff884526)  as soon as I am running the basic program its returning this error.**\r\n\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 82, in <module>\r\n    from tensorflow.python.estimator import estimator_lib as estimator\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator_lib.py\", line 37, in <module>\r\n    from tensorflow.python.estimator.inputs import inputs\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/inputs.py\", line 22, in <module>\r\n    from tensorflow.python.estimator.inputs.numpy_io import numpy_input_fn\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/numpy_io.py\", line 26, in <module>\r\n    from tensorflow.python.estimator.inputs.queues import feeding_functions\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\r\n    import pandas as pd\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/__init__.py\", line 42, in <module>\r\n    from pandas.core.api import *\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/core/api.py\", line 9, in <module>\r\n    from pandas.core.categorical import Categorical\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/core/categorical.py\", line 36, in <module>\r\n    from pandas.core.base import (PandasObject,\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/core/base.py\", line 20, in <module>\r\n    import pandas.core.nanops as nanops\r\n  File \"/home/afzal/.virtualenvs/tensorflow_python36/lib/python3.6/site-packages/pandas/core/nanops.py\", line 30, in <module>\r\n    ver = bn.__version__\r\nAttributeError: module 'bottleneck' has no attribute '__version__'\r\n", "comments": ["+1", "Seems to be something to do with caching. I created a new virtualenv and was able to run it. Then after a few tries I started getting AttributeError: module 'bottleneck' has no attribute '__version__' again.", "I believe this an environment issue, not a TensorFlow issue.", "Any update , I am also getting same issue", "I have got the same issue too, waiting for any update.", "I solved the issue by creating a new virtualenv. It's definitely got to do with environments.", "Issue got solved by reinstalling bottleneck.", "do 'pip install bottleneck' is ok for me.", "I also solved this issue by re installing bottleneck in python3 ", "I have the same problem using env-\"jupyter\". How do I resolve it?"]}, {"number": 19172, "title": "how to parse training examples with 2 dimension in csv file with tf.decode_csv", "body": "I used tf.decode_csv in tensorflow as decoder to parse training examples in a tab-delimited file into cnn models. For every training example, the features are 2 dimensions (100 columns, 2000 rows). After reading the document in tensorflow official site, I still have two questions.\r\n\r\n1. how to create record_defaults for my case (2 dimensional data) and how to stack the features of 2 dimensionality? The following is my code to do that, but it gave me errors.\r\n\r\nfilename_queue = tf.train.string_input_producer([file], num_epochs)\r\n\r\nkey, value = tf.TextLineReader().read(filename_queue)\r\n\r\nrecord_defaults = [[1.0] for col in range(0, 100)]\r\n\r\ncontent = tf.decode_csv(value, record_defaults = record_defaults, field_delim = '\\t')\r\n\r\nfeatures = tf.stack(content[0:100])\r\n\r\n 2. I am doing binary (0, 1) classification. Where do I put the labels for training examples? in the 2001th row? (For every training example, the first 2000 rows for features, and the 2001th row for label)\r\n\r\nThanks for your time!\r\n\r\n\r\nHave I written custom code    Yes\r\nOS Platform and Distribution    ubuntu 16.04 LTS\r\nTensorFlow installed from          pip3 install tensorflow-gpu\r\nTensorFlow version       the latest version\r\nBazel version     NA\r\nCUDA/cuDNN version     CUDA Toolkit 9.0  \r\nGPU model and memory     GeForce GTX 1080\r\nExact command to reproduce   NA\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 19171, "title": "Obtaining different results on declaring unused placeholders", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GeForce GTX 1080 Ti - 12GB\r\n- **Exact command to reproduce**: [Code link](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5)\r\n\r\n### Describe the problem\r\nI am getting different results on declaring some unused placeholders in the code even with the same seed. After setting the same seed in both the files, the results obtained are different although the only difference in the files is an addition of some unused placeholders in [placeholder_reproduce_unusedplc.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py). I am unable to understand the cause of this difference because the input and the computational graph is same for both the files. Also, with different seeds, sometimes the code with additional placeholders gives worse results.\r\n\r\nDifference between the two files (Line #52-#57 [placeholder_reproduce_unusedplc.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py)):\r\n```\r\nX1 = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY1 = tf.placeholder(\"float\", [None, num_classes])\r\nX2 = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY2 = tf.placeholder(\"float\", [None, num_classes])\r\nX3 = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY3 = tf.placeholder(\"float\", [None, num_classes])\r\nX4 = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY4 = tf.placeholder(\"float\", [None, num_classes])\r\n```\r\n### Source code / logs\r\nRunning [placeholder_reproduce.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce-py) gives (same everytime):\r\n\r\n> Step 1, Minibatch Loss= 2.5037, Training Accuracy= 0.164\r\n> Step 200, Minibatch Loss= 2.1293, Training Accuracy= 0.305\r\n> Step 400, Minibatch Loss= 1.8812, Training Accuracy= 0.453\r\n> Step 600, Minibatch Loss= 1.8287, Training Accuracy= 0.398\r\n> Step 800, Minibatch Loss= 1.7506, Training Accuracy= 0.430\r\n> Step 1000, Minibatch Loss= 1.6265, Training Accuracy= 0.508\r\n> Step 1200, Minibatch Loss= 1.4767, Training Accuracy= 0.516\r\n> Step 1400, Minibatch Loss= 1.4777, Training Accuracy= 0.547\r\n> Step 1600, Minibatch Loss= 1.3804, Training Accuracy= 0.586\r\n> Step 1800, Minibatch Loss= 1.2695, Training Accuracy= 0.672\r\n> Step 2000, Minibatch Loss= 1.2690, Training Accuracy= 0.594\r\n> Step 2200, Minibatch Loss= 1.2799, Training Accuracy= 0.570\r\n> Step 2400, Minibatch Loss= 1.1228, Training Accuracy= 0.680\r\n> Step 2600, Minibatch Loss= 1.0570, Training Accuracy= 0.695\r\n> Step 2800, Minibatch Loss= 1.0974, Training Accuracy= 0.625\r\n> Step 3000, Minibatch Loss= 1.1049, Training Accuracy= 0.641\r\n> Step 3200, Minibatch Loss= 1.0421, Training Accuracy= 0.688\r\n> Step 3400, Minibatch Loss= 1.0742, Training Accuracy= 0.672\r\n> Step 3600, Minibatch Loss= 1.0896, Training Accuracy= 0.648\r\n> Step 3800, Minibatch Loss= 0.8617, Training Accuracy= 0.734\r\n> Step 4000, Minibatch Loss= 0.9472, Training Accuracy= 0.719\r\n> Step 4200, Minibatch Loss= 0.7870, Training Accuracy= 0.773\r\n> Step 4400, Minibatch Loss= 0.9419, Training Accuracy= 0.656\r\n> Step 4600, Minibatch Loss= 0.7562, Training Accuracy= 0.750\r\n> Step 4800, Minibatch Loss= 0.8001, Training Accuracy= 0.758\r\n> Step 5000, Minibatch Loss= 0.9771, Training Accuracy= 0.711\r\n> Step 5200, Minibatch Loss= 0.7151, Training Accuracy= 0.805\r\n> Step 5400, Minibatch Loss= 0.7159, Training Accuracy= 0.789\r\n> Step 5600, Minibatch Loss= 0.7710, Training Accuracy= 0.797\r\n> Step 5800, Minibatch Loss= 0.7368, Training Accuracy= 0.766\r\n> Step 6000, Minibatch Loss= 0.7593, Training Accuracy= 0.758\r\n> Step 6200, Minibatch Loss= 0.6506, Training Accuracy= 0.773\r\n> Step 6400, Minibatch Loss= 0.8102, Training Accuracy= 0.766\r\n> Step 6600, Minibatch Loss= 0.5647, Training Accuracy= 0.820\r\n> Step 6800, Minibatch Loss= 0.6096, Training Accuracy= 0.828\r\n> Step 7000, Minibatch Loss= 0.6203, Training Accuracy= 0.844\r\n> Step 7200, Minibatch Loss= 0.5551, Training Accuracy= 0.820\r\n> Step 7400, Minibatch Loss= 0.5007, Training Accuracy= 0.836\r\n> Step 7600, Minibatch Loss= 0.5582, Training Accuracy= 0.844\r\n> Step 7800, Minibatch Loss= 0.6226, Training Accuracy= 0.789\r\n> Step 8000, Minibatch Loss= 0.5149, Training Accuracy= 0.812\r\n> Step 8200, Minibatch Loss= 0.5257, Training Accuracy= 0.844\r\n> Step 8400, Minibatch Loss= 0.4988, Training Accuracy= 0.844\r\n> Step 8600, Minibatch Loss= 0.5633, Training Accuracy= 0.805\r\n> Step 8800, Minibatch Loss= 0.4969, Training Accuracy= 0.797\r\n> Step 9000, Minibatch Loss= 0.4822, Training Accuracy= 0.844\r\n> Step 9200, Minibatch Loss= 0.4551, Training Accuracy= 0.844\r\n> Step 9400, Minibatch Loss= 0.4117, Training Accuracy= 0.906\r\n> Step 9600, Minibatch Loss= 0.4025, Training Accuracy= 0.883\r\n> Step 9800, Minibatch Loss= 0.5490, Training Accuracy= 0.812\r\n> Step 10000, Minibatch Loss= 0.4854, Training Accuracy= 0.836\r\n> Optimization Finished!\r\n> Testing Accuracy: 0.8515625\r\n\r\nRunning [placeholder_reproduce_unusedplc.py](https://gist.github.com/svjan5/d9eb1c78c10d695f7c50245ce21795c5#file-placeholder_reproduce_unusedplc-py) gives (same everytime):\r\n> Step 1, Minibatch Loss= 3.9436, Training Accuracy= 0.055\r\n> Step 200, Minibatch Loss= 2.0941, Training Accuracy= 0.234\r\n> Step 400, Minibatch Loss= 1.8377, Training Accuracy= 0.422\r\n> Step 600, Minibatch Loss= 1.7426, Training Accuracy= 0.391\r\n> Step 800, Minibatch Loss= 1.7220, Training Accuracy= 0.383\r\n> Step 1000, Minibatch Loss= 1.6012, Training Accuracy= 0.469\r\n> Step 1200, Minibatch Loss= 1.4531, Training Accuracy= 0.531\r\n> Step 1400, Minibatch Loss= 1.4427, Training Accuracy= 0.516\r\n> Step 1600, Minibatch Loss= 1.2995, Training Accuracy= 0.594\r\n> Step 1800, Minibatch Loss= 1.2551, Training Accuracy= 0.664\r\n> Step 2000, Minibatch Loss= 1.2198, Training Accuracy= 0.648\r\n> Step 2200, Minibatch Loss= 1.2236, Training Accuracy= 0.609\r\n> Step 2400, Minibatch Loss= 1.0227, Training Accuracy= 0.680\r\n> Step 2600, Minibatch Loss= 0.9933, Training Accuracy= 0.695\r\n> Step 2800, Minibatch Loss= 0.9887, Training Accuracy= 0.656\r\n> Step 3000, Minibatch Loss= 1.0707, Training Accuracy= 0.578\r\n> Step 3200, Minibatch Loss= 0.9540, Training Accuracy= 0.680\r\n> Step 3400, Minibatch Loss= 0.9850, Training Accuracy= 0.695\r\n> Step 3600, Minibatch Loss= 1.0233, Training Accuracy= 0.664\r\n> Step 3800, Minibatch Loss= 0.8106, Training Accuracy= 0.727\r\n> Step 4000, Minibatch Loss= 0.8919, Training Accuracy= 0.719\r\n> Step 4200, Minibatch Loss= 0.7129, Training Accuracy= 0.773\r\n> Step 4400, Minibatch Loss= 0.8317, Training Accuracy= 0.734\r\n> Step 4600, Minibatch Loss= 0.6762, Training Accuracy= 0.789\r\n> Step 4800, Minibatch Loss= 0.6951, Training Accuracy= 0.773\r\n> Step 5000, Minibatch Loss= 0.8617, Training Accuracy= 0.727\r\n> Step 5200, Minibatch Loss= 0.6173, Training Accuracy= 0.828\r\n> Step 5400, Minibatch Loss= 0.6464, Training Accuracy= 0.797\r\n> Step 5600, Minibatch Loss= 0.6643, Training Accuracy= 0.828\r\n> Step 5800, Minibatch Loss= 0.6795, Training Accuracy= 0.758\r\n> Step 6000, Minibatch Loss= 0.6252, Training Accuracy= 0.812\r\n> Step 6200, Minibatch Loss= 0.5862, Training Accuracy= 0.812\r\n> Step 6400, Minibatch Loss= 0.7126, Training Accuracy= 0.812\r\n> Step 6600, Minibatch Loss= 0.4639, Training Accuracy= 0.891\r\n> Step 6800, Minibatch Loss= 0.5116, Training Accuracy= 0.859\r\n> Step 7000, Minibatch Loss= 0.5322, Training Accuracy= 0.852\r\n> Step 7200, Minibatch Loss= 0.4902, Training Accuracy= 0.844\r\n> Step 7400, Minibatch Loss= 0.4027, Training Accuracy= 0.867\r\n> Step 7600, Minibatch Loss= 0.4654, Training Accuracy= 0.875\r\n> Step 7800, Minibatch Loss= 0.5647, Training Accuracy= 0.812\r\n> Step 8000, Minibatch Loss= 0.4110, Training Accuracy= 0.852\r\n> Step 8200, Minibatch Loss= 0.3976, Training Accuracy= 0.844\r\n> Step 8400, Minibatch Loss= 0.3916, Training Accuracy= 0.867\r\n> Step 8600, Minibatch Loss= 0.5487, Training Accuracy= 0.797\r\n> Step 8800, Minibatch Loss= 0.4265, Training Accuracy= 0.875\r\n> Step 9000, Minibatch Loss= 0.4590, Training Accuracy= 0.859\r\n> Step 9200, Minibatch Loss= 0.4135, Training Accuracy= 0.906\r\n> Step 9400, Minibatch Loss= 0.3457, Training Accuracy= 0.914\r\n> Step 9600, Minibatch Loss= 0.4067, Training Accuracy= 0.844\r\n> Step 9800, Minibatch Loss= 0.4394, Training Accuracy= 0.859\r\n> Step 10000, Minibatch Loss= 0.3586, Training Accuracy= 0.875\r\n> Optimization Finished!\r\n> Testing Accuracy: 0.890625\r\n", "comments": ["Same issue with Tensorflow 1.8.0.", "Seem related to #14675. When we add / remove unused ops, the random behavior cannot keep consistent by now.", "Nagging Assignee @aselle: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 47 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 62 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is known behavior and a dupe of #9171. The random seed is dependent on the order of ops created (there is a node number in the graphdef). To get consistent random initialization you need to fully seed as a node rather than relying on the partial seeding from the graph.\r\n"]}, {"number": 19170, "title": "Small grammar fixes in the programmers guide FAQ.", "body": "", "comments": []}]