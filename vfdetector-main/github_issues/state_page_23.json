[{"number": 51201, "title": "Error : Value for attr 'data_format' of \"\", when calling TF_AddGradients with FusedBatchNormV3 operator", "body": "Hello, I'm very happy to see that FusedBatchNormV3 is now included in backprop.\r\nUnfortunately, when I call TF_AddGradients with a TF_Graph containing a FusedBatchNormV3 operator, I get an error with a blank data_format attribute.\r\n\r\nThis is probably due to the fact that the default values are not reflected when registering FusedBatchNormGradV3.\r\n\r\n**System information**\r\nWindows 10 x64\r\nNo custom code\r\nbinary From [https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.6.0.zip](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.6.0.zip)\r\nTensorFlow Version 2.6.0\r\nC API \r\nOn CPU", "comments": ["@harujoh In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@sushreebarsa Here is a code snippet that reproduces the problem.\r\n\r\nIt works, but I usually use C#, and I haven't used C++ in 15 years, so please forgive any minor errors.\r\n\r\n```\r\n#include <iostream>\r\n#include <tensorflow/c/c_api.h>\r\n\r\nint main() {\r\n\tauto g = TF_NewGraph();\r\n\tauto s = TF_NewStatus();\r\n\tint64_t dims[] = { 1, 1, 1, 1 };\r\n\r\n\tauto ph_desc = TF_NewOperation(g, \"Placeholder\", \"inputVal\");\r\n\tTF_SetAttrType(ph_desc, \"dtype\", TF_FLOAT);\r\n\tTF_SetAttrShape(ph_desc, \"shape\", dims, 4);\r\n\tauto ph_op = TF_FinishOperation(ph_desc, s);\r\n\r\n\tauto gamma_desc = TF_NewOperation(g, \"Variable\", \"bn_gamma\");\r\n\tTF_SetAttrShape(gamma_desc, \"shape\", nullptr, 0);\r\n\tTF_SetAttrType(gamma_desc, \"dtype\", TF_FLOAT);\r\n\tauto gamma_op = TF_FinishOperation(gamma_desc, s);\r\n\r\n\tauto beta_desc = TF_NewOperation(g, \"Variable\", \"bn_beta\");\r\n\tTF_SetAttrShape(beta_desc, \"shape\", nullptr, 0);\r\n\tTF_SetAttrType(beta_desc, \"dtype\", TF_FLOAT);\r\n\tauto beta_op = TF_FinishOperation(beta_desc, s);\r\n\r\n\tauto moving_mean_desc = TF_NewOperation(g, \"Variable\", \"bn_moving_mean\");\r\n\tTF_SetAttrShape(moving_mean_desc, \"shape\", dims, 4);\r\n\tTF_SetAttrType(moving_mean_desc, \"dtype\", TF_FLOAT);\r\n\tauto moving_mean_op = TF_FinishOperation(moving_mean_desc, s);\r\n\r\n\tauto moving_var_desc = TF_NewOperation(g, \"Variable\", \"bn_moving_var\");\r\n\tTF_SetAttrShape(moving_var_desc, \"shape\", dims, 4);\r\n\tTF_SetAttrType(moving_var_desc, \"dtype\", TF_FLOAT);\r\n\tauto moving_var_op = TF_FinishOperation(moving_var_desc, s);\r\n\r\n\tauto bn_desc = TF_NewOperation(g, \"FusedBatchNormV3\", \"bn\");\r\n\tTF_Output output;\r\n\toutput.oper = ph_op;\r\n\toutput.index = 0;\r\n\tTF_AddInput(bn_desc, output);\r\n\toutput.oper = gamma_op;\r\n\tTF_AddInput(bn_desc, output);\r\n\toutput.oper = beta_op;\r\n\tTF_AddInput(bn_desc, output);\r\n\toutput.oper = moving_mean_op;\r\n\tTF_AddInput(bn_desc, output);\r\n\toutput.oper = moving_var_op;\r\n\tTF_AddInput(bn_desc, output);\r\n\tTF_SetAttrFloat(bn_desc, \"epsilon\", 0.0001);\r\n\tTF_SetAttrString(bn_desc, \"data_format\", \"NHWC\", 4);\r\n\tTF_SetAttrBool(bn_desc, \"is_training\", 1);\r\n\tauto bn_op = TF_FinishOperation(bn_desc, s);\r\n\r\n\tTF_Output bn;\r\n\tbn.oper = bn_op;\r\n\tbn.index = 0;\r\n\tTF_Output outputs[1] = { bn };\r\n\r\n\tTF_Output gamma;\r\n\tgamma.oper = gamma_op;\r\n\tgamma.index = 0;\r\n\tTF_Output beta;\r\n\tbeta.oper = beta_op;\r\n\tbeta.index = 0;\r\n\tTF_Output mean;\r\n\tmean.oper = moving_mean_op;\r\n\tmean.index = 0;\r\n\tTF_Output var;\r\n\tvar.oper = moving_var_op;\r\n\tvar.index = 0;\r\n\tTF_Output variables[4] = { gamma,beta,mean,var };\r\n\r\n\tTF_Output grad_outputs[1];\r\n\r\n\tTF_AddGradients(g,\r\n\t\toutputs, 1,\r\n\t\tvariables, 4,\r\n\t\tnullptr, s, grad_outputs);\r\n\r\n\tauto code = TF_GetCode(s);\r\n\tif (code != 0) {\r\n\t\tstd::cout << TF_Message(s); //Value for attr 'data_format' of \"\r\n\t}\r\n\r\n\treturn 0;\r\n}\r\n```", "@Saduf2019 I just noticed that the version of the tag is wrong.\r\n\r\nIt is now 2.5, but graphs containing this FusedBatchNormV3 can be used in TF_AddGradients from 2.6rc0.", "@harujoh Was this piece of code executing successfully with TF 2.5 for you?", "@ymodak No, the ability to run Graphs containing FusedBatchNormV3 with TF_AddGradients is a new feature since 2.6rc0.\r\n\r\nIn 2.5, this is not available with the following error: \"No gradient defined for op: FusedBatchNormV3\".\r\n\r\nSee the comments in this commit. bf3d89b1e8bc5e00271c2fbd57a8b3ff55d48462", "@ymodak I tried with 2.6.0, but I get the same error message.", "@ymodak Please change the version tag from 2.6.0-rc0 to 2.6.0 as I have modified the post.", "The same error on the 2.7.0 @ymodak "]}, {"number": 51180, "title": "[CoreML Delegate] validator error: Padding type for the pooling layer 'PoolingLayerBuilder (MEAN)_2' is not set.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX Big Sur\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone XS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n\r\n**Describe the current behavior**\r\nI have used Google AutoML Vision to create object detection models in the past. Previously the models that were generated were all TOCO converted on runtime 1.15. Recently, AutoML has been generating object detection models that are MLIR converted on TF runtime 2.5. All models are in TensorFlow Lite format.\r\n\r\nMy previous object detection models run without any issues with the CoreML Delegate. The new object detection models run on CPU but throw an error when using the CoreML delegate. It's the same Object Detection sample code running for both - older model the CoreML works, newer one does not work.\r\n\r\nThe specific error is `Error compiling model Error reading protobuf spec. validator error: Padding type for the pooling layer 'PoolingLayerBuilder (MEAN)_2' is not set.`\r\n\r\n**Describe the expected behavior**\r\nCoreML Delegate should create an ML model without any errors.\r\n\r\n**Other info / logs** \r\nI attempted to solve the problem by explicitly setting padding to SAME for kTfLiteBuiltInMean operation as follows. It appeared to work, the error was gone and the delegate was initialized. However, with that one small change it also seemed to crash every now and then with BAD_EXC_ACCESS error. \r\n\r\nI made this PR today but closed it once I noticed the crashes.\r\nhttps://github.com/tensorflow/tensorflow/pull/51174/commits/602cd90c1d55bc205c4230060c4423af4cdf82fd\r\n\r\nPooling Layer Params specification:\r\nhttps://apple.github.io/coremltools/mlmodel/Format/NeuralNetwork.html#poolinglayerparams\r\n\r\nI am building the CoreML framework locally by running this in `tensorflow/tensorflow/lite` and testing on an iPhoneX (I am making sure to Clean the Build Folder between tests)\r\n```\r\n bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteCCoreML_framework\r\ntflite_ios_framework(\r\n    name = \"TensorFlowLiteCCoreML_framework\",\r\n    hdrs = [\r\n        \":coreml_delegate.h\",\r\n    ],\r\n    allowlist_symbols_file = \":allowlist_TensorFlowLiteCCoreML.txt\",\r\n    bundle_name = \"TensorFlowLiteCCoreML\",\r\n    minimum_os_version = TFL_MINIMUM_OS_VERSION,\r\n    deps = [\r\n        \"//tensorflow/lite/delegates/coreml:coreml_delegate\",\r\n    ],\r\n)\r\n```\r\n\r\n\r\nQuestions:\r\n1. Was my approach correct to explicitly set the padding or is there something else at play here?\r\n2. What else can I try to get CoreML working and fix this error?\r\n", "comments": ["@airman00 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@sushreebarsa thank you for the quick reply!\r\n\r\nPlease pull this repo to reproduce the error. I have included sample tflite files. \r\nhttps://github.com/airman00/tensorflow-object-detection-example\r\n\r\n\r\nThe error:\r\n```\r\nObjectDetection[50052:5356650] Failed to Compile and save Model.\r\nERROR: Failed to Compile and save Model.\r\n\r\nObjectDetection[50052:5356650] Error compiling model Error reading protobuf spec. validator error: Padding type for the pooling layer 'PoolingLayerBuilder (MEAN)_1' is not set.\r\n```", "Hi,\r\n\r\nConfirming this is still happening even in the latest version, TensorFlow 2.6.0", "I'm facing the same issue in 2.7, any resolutions?"]}, {"number": 51178, "title": "&TfLiteTensor->Data not 16-Byte Aligned", "body": "### 1. System information\r\n\r\n- OS Platform and Di_pywrap_toco_apistribution (e.g., Linux Ubuntu 16.04): LINUX UBUNTU 20.04\r\n- TensorFlow installation (pip package or built from source):  PIP PACKAGE\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): PIP PACKAGE\r\n\r\n### 2. Code\r\nI think the code will not help, as the conversion process is working but I have questions about the resulting model file.\r\nI have attached a zip:\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/6934441/model.zip)\r\n\r\n\r\n### 3. Failure after conversion\r\nI am working on a proof of concept, and I am making changes to the TFLite codebase. \r\nAlthough the model file is 16-byte aligned, some of the tensor data is not. \r\nFor example:\r\nmy input->data.f has an address of 0x..........0 (16 byte aligned)\r\nmy bias->data.f has an address of 0x...........8 (8 byte aligned)\r\n\r\nWhile this can still work, the hardware optimization I am trying to implement works better if the data is 16-byte aligned. The only option I see is to specifically treat the first two floats (8 bytes), and then use full optimization for the rest of the data.\r\n\r\nSO, my question is whether there is some option somewhere during conversion to align the the data within each tensor! I assume there would be some option to have it pad the data structures in the flatbuffer model with a few zeros before beginning the float data array.\r\n\r\nThe only posts I have seen regarding memory alignment are for TensorflowLite Micro, which isn't really an option for me.\r\n\r\n### 5. (optional) Any other info / logs\r\nIf there is no option in the converter to achieve what I want, I suppose I can make the changes myself when the memory is allocated in TFlite, but I could use a nudge in the right direction. My assumption is that the model is parsed somewhere, and I would like to add some instruction to make sure the TfLiteTensor->data is properly aligned. Do you know where the easiest place to make that change would be?\r\n\r\nThanks!", "comments": ["@srjoglekar246 could you take a look?", "Hello, Sachin.\r\n\r\nIf it helps, you can load the model I provided into a debug version of Tensorflow Lite, and set a breakpoint at optimized_ops.h line 420 or so, in the FullyConnected( ) function somewhere.  The PoC I am demonstrating branches from here. Two of the data arrays should be 16-byte aligned, and two should be 8-byte aligned.  Again, while I can work around it, it would be preferable if there was some way to build/load the model in a way that all the tensor data arrays were 16-byte aligned by default.\r\n\r\nThanks in advance!", "@MisterBerg you are correct, AFAIK the constants in TFLite flatbuffers (and even the runtime TFLite tensors) are required to be aligned at 8 bytes. This is mainly because most of the underlying backends/kernels we use don't require any greater alignment. \r\n@miaout17 and I had discussed this a while back....YC, is there a way to make the converter align constants to a different multiple of 8 bytes? We cannot use an arbitrary alignment for obvious reasons, but any multiple of 64 bits should be okay I think."]}, {"number": 51149, "title": "Cloud TPU Memory Leak causing OOM Error ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): n1-standard-2 VM on GKE; Container-Optimized OS with Docker (cos) \r\n- TensorFlow installed from (source or binary): binary (pip command) \r\n- TensorFlow version (use command below): 2.3.0 \r\n- Python version: 3.6 \r\n- Bazel version (if compiling from source): None \r\n- GCC/Compiler version (if compiling from source): None \r\n- CUDA/cuDNN version: None \r\n- GPU model and memory: None (v2.8 preemptible TPU with GKE) \r\n\r\n**Describe the current behavior**\r\nI am performing large-scale hyperparameter optimization using Cloud TPUs on GKE. Unfortunately, I keep encountering a memory leak that causes the TPUs to return an OOM after training several networks. Initially, I thought this issue was due to the buildup of old models, so I tried methods such as tf.backend.clear_session(), del model, and gc.collect(). Those methods fixed a memory leak that had been happening with the controller CPU, but the memory leak persists on the TPU worker node. Occasionally, I instead get an \"Unavailable: Socket Closed\" error. To fix these issues, I tried solutions suggested by other posts, such as typecasting my data to float32, not caching my dataset into memory, using a smaller mini-batch size, and using from_logits in my cost function. Unfortunately, those solutions have not helped.  I have not encountered these problems with either a CPU/GPU/Colab TPU. \r\n\r\n**Describe the expected behavior**\r\nSame as CPU/GPU/Colab TPU (no memory leak) \r\n\r\n**Standalone code to reproduce the issue**\r\nThe issue does not occur on Colab, but here is a [link](https://colab.research.google.com/drive/1tjPh6-UQuEtFKNQcJfaVA3vQ0eIV5SG_?usp=sharing) to code that reproduces the issue on GCP.\r\n\r\n**Other info / logs** \r\nLog trace: \r\n[TPU_Traceback.txt](https://github.com/tensorflow/tensorflow/files/6932886/TPU_Traceback.txt)\r\n\r\n", "comments": ["@Julianna3141  Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "Thank you for the suggestion. Unfortunately, when I try to create tf 2.5 nodes in Kubernetes with the following line (tf-version.cloud-tpus.google.com: \"2.5\") in my Kubernetes pod spec, I get this error message: `\"Failed to provision Cloud TPU(s): failed to create Cloud TPU in zone \"us-central1-c\": googleapi: Error 400: Cloud TPU received an invalid argument. The field \"TensorflowVersion\" cannot be \"2.5\".` I was able to create a node with version 2.4.1, but for some reason, my code seems to get stuck at the \"Started server with target:  grpc://localhost:30001\" line. I tried re-deploying the code, but it has been at that line for the past 20 minutes (previously, it had only stayed at that line for 2-3 minutes). Also, strangely, the CPU utilization on the controller drops from 40% to below 1%, though I'm not sure why and have not experienced this problem before. Did I need to change anything in my TPU initialization code to work with 2.4.1?", "I am also using TPU to perform HPO (with Optuna), and find that the only way works is to spawn a new process for every new trial. You may check [here](https://github.com/sbl1996/hanser/blob/master/examples/hpo/optuna/mnist.py) for example."]}, {"number": 51145, "title": "Performance regression: Tensorflow 2.5.0", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Centos 7**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.5.0**\r\n- Python version: 3.8.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI observed significant performance regression (+20% latency) using TF 2.5.0 compared to TF 2.3.0 on CPU.\r\nOne op to highlight is `sparse_tensor_dense_matmul `.\r\nThe cause could be Eigen or change of this kernel.\r\n\r\n**Describe the expected behavior**\r\nPerformance on parity with TF 2.3.0.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1gNaQsRZzhmKYFQGMxOO9vzSR6AFpVMci?usp=sharing\r\n", "comments": ["Performance\r\n- 2.3.0:  ~245 ms\r\n- 2.4.0:  ~240 ms\r\n- 2.5.0: ~350ms\r\n- tf-nightly (latest): ~350ms\r\n\r\nSomething broke between 2.4 and 2.5", "Hy! I am processing matrix multiplication operations on TensorFlow and NumPy to check the processing time difference on CPU and GPU. It's taking more time to process operation on GPU than on CPU\r\n\r\nOn Cpu its taking:1.15 sec\r\n\r\nOn GPU it's taking: 34 sec\r\n\r\nwhy so? According to my understanding GPU should perform fast operations than CPU..\r\n\r\n**CPU CODE**\r\n\r\n```\r\n    import numpy as np\r\n    import time\r\n    from tqdm import tqdm\r\n    \r\n    \r\n    # Making Tensor Constant\r\n    np_arr = np.asarray([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [4.0, 5.0, 6.0]])\r\n    print(\"Starting cpu processing\")\r\n    st = time.time()\r\n    for i in tqdm(range(0,1000000)):\r\n        np_arr @ np_arr\r\n        \r\n    # Took 1 sec    \r\n    print(f\"Time Taken BY CPU {time.time() -st}\")\r\n\r\n```\r\n\r\n**GPU Code**\r\n\r\n\r\n```\r\n    import tensorflow as tf\r\n    physical_devices = tf.config.list_physical_devices('GPU') \r\n    \r\n    # Gpu Settings for rtx3070\r\n    from tensorflow.compat.v1.keras.backend import set_session\r\n    config = tf.compat.v1.ConfigProto()\r\n    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\n    config.log_device_placement = True  # to log device placement (on which device the operation ran)\r\n    sess = tf.compat.v1.Session(config=config)\r\n    set_session(sess)\r\n    \r\n    import time\r\n    from tqdm import tqdm\r\n    \r\n    print(\"Here is the tensorflow version \",tf.__version__)\r\n    \r\n    # Making Tensor Constant\r\n    tensor1 = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [4.0, 5.0, 6.0]])\r\n    \r\n    # Getting the shape of the tensor\r\n    print(tensor1.get_shape())\r\n    \r\n    \r\n    st = time.time()\r\n    for i in tqdm(range(0,1000000)):\r\n        # Using tensorflow function to use gpu functionality\r\n        tf.linalg.matmul(tensor1, tensor1)\r\n    \r\n    # Took 34 sec\r\n    print(f\"Time Taken BY GPU {time.time() -st}\")\r\n\r\n```\r\n\r\n", "@rmothukuru Was able to reproduce on colab using TF [v2.3](https://colab.research.google.com/gist/sushreebarsa/93de3d9cbb52daaf1e6589a951f234ca/sparse_tensor_dense_matmul_benchmark.ipynb#scrollTo=IyCaVfHFK3QO),[v2.4](https://colab.research.google.com/gist/sushreebarsa/b823c7b058446445728dacf45ba4d9c0/sparse_tensor_dense_matmul_benchmark.ipynb#scrollTo=qCrlrsqG2I4M),  [v2.5](https://colab.research.google.com/gist/sushreebarsa/9ef89549732293595306666bfdf939d1/sparse_tensor_dense_matmul_benchmark.ipynb#scrollTo=IyCaVfHFK3QO) & [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/f3fdf509ffe00bb8630cb6a257c16f8b/sparse_tensor_dense_matmul_benchmark.ipynb),please find the gists attached here.\r\nThank you! ", "Try using the Intel Tensor Flow 2.5, pip install intel-tensorflow==2.5.0", "@ImtiazSajwani Intel TensorFlow doesn't have any optimization for `SparseTensor` ops, so I think the performance will be similar.", "@penpornk , do you have a plan to patch the performance fix of Eigen and TF to master or any version of release?", "@liyinhgqw Currently there is no plan. But I can investigate if the issue can be fixed in the master branch. (The patch for v2.5 alone requires reverting / changing 5 commits between 2.3 and 2.5. There could be more commits after 2.5 which affects compiler optimization.)", "@penpornk that would be super helpful - thank you!\r\n\r\nideally, we could get a fix for this onto master for the 2.8 release (and future versions). do you think that would be possible?", "\r\nI was able to reproduce the issue in `TF2.8` and `tf-nightly(2.9.0-dev20220401)`. Please find the gist [here](https://colab.research.google.com/gist/chunduriv/089435bb89b30c4dbef646e47a604958/51145.ipynb#scrollTo=Zn9YOhi7aIdE) for reference. Thanks!"]}, {"number": 51119, "title": "cpu gpu delegate in ios get the different result, cpu's is right but gpu's is wrong", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos big sur 11.4 20f71\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iphone xr\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): a4dfb8d1a71385bd6d122e4f27f86dcebb96712d (tag: v2.5.0)\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): bazel-4.0.0\r\n- GCC/Compiler version (if compiling from source): (clang-1205.0.22.11)\r\n- CUDA/cuDNN version: NO\r\n- GPU model and memory: reference iphone xr configuration \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ngit checkout v2.5.0\r\ntensorflow/lite/tools/make/build_ios_universal_lib.sh\r\nchange the pod file below\r\nplatform :ios, '12.0'\r\ninhibit_all_warnings!\r\n\r\ntarget 'tflite_simple_example'\r\n       pod 'TensorFlowLiteObjC', '2.5.0'\r\n\r\nmodify ../tensorflow/tensorflow/lite/examples/ios/simple project to load my tflite model , feed the same input\r\nand then run in cpu and gpu mode. \r\nbut the result has a big big difference\r\n[bisenetv3_fp32_224.tflite.zip](https://github.com/tensorflow/tensorflow/files/6929310/bisenetv3_fp32_224.tflite.zip)\r\n\r\nhere is the run debug info\r\n\r\n> 2021-08-03 14:52:45.587249+0800 tflite_simple_example[4144:2858180] input data len 150528\r\ntflite_simple_example was compiled with optimization - stepping may behave oddly; variables may not be available.\r\n2021-08-03 14:53:02.632412+0800 tflite_simple_example[4144:2858180] Initialized TensorFlow Lite runtime.\r\n2021-08-03 14:53:02.651308+0800 tflite_simple_example[4144:2858180] Created TensorFlow Lite delegate for Metal.\r\n2021-08-03 14:53:02.652346+0800 tflite_simple_example[4144:2858180] Metal GPU Frame Capture Enabled\r\n2021-08-03 14:53:02.653172+0800 tflite_simple_example[4144:2858180] Metal API Validation Enabled\r\n=== Pre-invoke Interpreter State ===\r\nInterpreter has 1 subgraphs.\r\n\r\n-----------Subgraph-0 has 249 tensors and 102 nodes------------\r\nInputs: [0]\r\nOutputs: [226]\r\n\r\nTensor  ID Name                      Type            AllocType                Size\r\nTensor   0 input_1                   kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,224,224,3]\r\nTensor   1 bi_se_net_v3_tf/Resize... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   2 bi_se_net_v3_tf/Resize... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   3 bi_se_net_v3_tf/Resize... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   4 bi_se_net_v3_tf/bga_la... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   5 bi_se_net_v3_tf/mul/y;... kTfLiteFloat32  kTfLiteMmapRo              12B ( 0.0 MB) [1,1,1,3]\r\nTensor   6 bi_se_net_v3_tf/segmen... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   7 bi_se_net_v3_tf/segmen... kTfLiteInt32    kTfLiteMmapRo              32B ( 0.0 MB) [4,2]\r\nTensor   8 bi_se_net_v3_tf/sub/y;... kTfLiteFloat32  kTfLiteMmapRo              12B ( 0.0 MB) [1,1,1,3]\r\nTensor   9 unknown_266               kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  10 unknown_268               kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  11 unknown_270               kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor  12 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor  13 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  14 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  15 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  16 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  17 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor  18 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  19 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  20 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  21 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  22 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  23 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  24 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  25 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  26 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  27 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  28 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  29 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  30 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  31 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  32 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  33 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  34 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  35 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  36 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  37 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  38 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  39 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  40 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  41 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  42 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              64B ( 0.0 MB) [16]\r\nTensor  43 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              32B ( 0.0 MB) [8]\r\nTensor  44 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  45 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              64B ( 0.0 MB) [16]\r\nTensor  46 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              64B ( 0.0 MB) [16]\r\nTensor  47 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             384B ( 0.0 MB) [96]\r\nTensor  48 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             384B ( 0.0 MB) [96]\r\nTensor  49 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             384B ( 0.0 MB) [96]\r\nTensor  50 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              64B ( 0.0 MB) [16]\r\nTensor  51 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  52 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  53 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo            3456B ( 0.0 MB) [32,3,3,3]\r\nTensor  54 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1728B ( 0.0 MB) [16,3,3,3]\r\nTensor  55 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [8,1,1,16]\r\nTensor  56 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            4608B ( 0.0 MB) [16,3,3,8]\r\nTensor  57 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           18432B ( 0.0 MB) [16,3,3,32]\r\nTensor  58 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            6144B ( 0.0 MB) [96,1,1,16]\r\nTensor  59 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           12288B ( 0.0 MB) [32,1,1,96]\r\nTensor  60 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo           73728B ( 0.1 MB) [64,3,3,32]\r\nTensor  61 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            2048B ( 0.0 MB) [32,1,1,16]\r\nTensor  62 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          147456B ( 0.1 MB) [64,3,3,64]\r\nTensor  63 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          147456B ( 0.1 MB) [64,3,3,64]\r\nTensor  64 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          147456B ( 0.1 MB) [64,3,3,64]\r\nTensor  65 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          294912B ( 0.3 MB) [128,3,3,64]\r\nTensor  66 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  67 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  68 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo           65536B ( 0.1 MB) [128,1,1,128]\r\nTensor  69 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  70 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           24576B ( 0.0 MB) [192,1,1,32]\r\nTensor  71 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           24576B ( 0.0 MB) [32,1,1,192]\r\nTensor  72 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           24576B ( 0.0 MB) [192,1,1,32]\r\nTensor  73 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           49152B ( 0.0 MB) [64,1,1,192]\r\nTensor  74 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            8192B ( 0.0 MB) [64,1,1,32]\r\nTensor  75 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           98304B ( 0.1 MB) [384,1,1,64]\r\nTensor  76 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           98304B ( 0.1 MB) [64,1,1,384]\r\nTensor  77 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           98304B ( 0.1 MB) [384,1,1,64]\r\nTensor  78 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          196608B ( 0.2 MB) [128,1,1,384]\r\nTensor  79 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           32768B ( 0.0 MB) [128,1,1,64]\r\nTensor  80 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [768,1,1,128]\r\nTensor  81 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [128,1,1,768]\r\nTensor  82 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [768,1,1,128]\r\nTensor  83 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [128,1,1,768]\r\nTensor  84 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [768,1,1,128]\r\nTensor  85 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [128,1,1,768]\r\nTensor  86 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           65536B ( 0.1 MB) [128,1,1,128]\r\nTensor  87 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  88 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  89 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo           65536B ( 0.1 MB) [128,1,1,128]\r\nTensor  90 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  91 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo          294912B ( 0.3 MB) [64,3,3,128]\r\nTensor  92 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo          147456B ( 0.1 MB) [64,3,3,64]\r\nTensor  93 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo           73728B ( 0.1 MB) [32,3,3,64]\r\nTensor  94 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo            1152B ( 0.0 MB) [1,3,3,32]\r\nTensor  95 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3456B ( 0.0 MB) [1,3,3,96]\r\nTensor  96 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3456B ( 0.0 MB) [1,3,3,96]\r\nTensor  97 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor  98 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             576B ( 0.0 MB) [1,3,3,16]\r\nTensor  99 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor 100 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo            4608B ( 0.0 MB) [1,3,3,128]\r\nTensor 101 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 102 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            6912B ( 0.0 MB) [1,3,3,192]\r\nTensor 103 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor 104 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            6912B ( 0.0 MB) [1,3,3,192]\r\nTensor 105 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            6912B ( 0.0 MB) [1,3,3,192]\r\nTensor 106 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor 107 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1152B ( 0.0 MB) [1,3,3,32]\r\nTensor 108 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor 109 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           13824B ( 0.0 MB) [1,3,3,384]\r\nTensor 110 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor 111 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           13824B ( 0.0 MB) [1,3,3,384]\r\nTensor 112 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           13824B ( 0.0 MB) [1,3,3,384]\r\nTensor 113 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 114 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            2304B ( 0.0 MB) [1,3,3,64]\r\nTensor 115 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 116 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           27648B ( 0.0 MB) [1,3,3,768]\r\nTensor 117 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 118 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           27648B ( 0.0 MB) [1,3,3,768]\r\nTensor 119 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 120 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           27648B ( 0.0 MB) [1,3,3,768]\r\nTensor 121 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 122 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 123 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo            4608B ( 0.0 MB) [1,3,3,128]\r\nTensor 124 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 125 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo               4B ( 0.0 MB) [1]\r\nTensor 126 bi_se_net_v3_tf/sub;St... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,224,224,3]\r\nTensor 127 bi_se_net_v3_tf/mul;St... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,224,224,3]\r\nTensor 128 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         612912B ( 0.6 MB) [1,226,226,3]\r\nTensor 129 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw        1605632B ( 1.5 MB) [1,112,112,32]\r\nTensor 130 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,112,112,16]\r\nTensor 131 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         831744B ( 0.8 MB) [1,114,114,16]\r\nTensor 132 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,56,56,16]\r\nTensor 133 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,112,112,8]\r\nTensor 134 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         415872B ( 0.4 MB) [1,114,114,8]\r\nTensor 135 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,56,56,16]\r\nTensor 136 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,56,56,32]\r\nTensor 137 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,56,56,16]\r\nTensor 138 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         215296B ( 0.2 MB) [1,58,58,16]\r\nTensor 139 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw        1204224B ( 1.1 MB) [1,56,56,96]\r\nTensor 140 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw        1291776B ( 1.2 MB) [1,58,58,96]\r\nTensor 141 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,28,28,96]\r\nTensor 142 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,28,28,96]\r\nTensor 143 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 144 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw        3211264B ( 3.1 MB) [1,112,112,64]\r\nTensor 145 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw        3326976B ( 3.2 MB) [1,114,114,64]\r\nTensor 146 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,28,28,16]\r\nTensor 147 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 148 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 149 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 150 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 151 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 152 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         861184B ( 0.8 MB) [1,58,58,64]\r\nTensor 153 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 154 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 155 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 156 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 157 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 158 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         460800B ( 0.4 MB) [1,30,30,128]\r\nTensor 159 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,14,14,128]\r\nTensor 160 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         131072B ( 0.1 MB) [1,16,16,128]\r\nTensor 161 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 162 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,28,28,192]\r\nTensor 163 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,28,28,192]\r\nTensor 164 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 165 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 166 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,28,28,192]\r\nTensor 167 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         691200B ( 0.7 MB) [1,30,30,192]\r\nTensor 168 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,14,14,192]\r\nTensor 169 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,14,14,192]\r\nTensor 170 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 171 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         115200B ( 0.1 MB) [1,30,30,32]\r\nTensor 172 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,14,14,32]\r\nTensor 173 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 174 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 175 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,14,14,384]\r\nTensor 176 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,14,14,384]\r\nTensor 177 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 178 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 179 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,14,14,384]\r\nTensor 180 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         393216B ( 0.4 MB) [1,16,16,384]\r\nTensor 181 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          75264B ( 0.1 MB) [1,7,7,384]\r\nTensor 182 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          75264B ( 0.1 MB) [1,7,7,384]\r\nTensor 183 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 184 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          65536B ( 0.1 MB) [1,16,16,64]\r\nTensor 185 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          12544B ( 0.0 MB) [1,7,7,64]\r\nTensor 186 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 187 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 188 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 189 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 190 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 191 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 192 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 193 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 194 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 195 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 196 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 197 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 198 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 199 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 200 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [1,1,1,128]\r\nTensor 201 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [1,1,1,128]\r\nTensor 202 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [1,1,1,128]\r\nTensor 203 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [1,1,1,128]\r\nTensor 204 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 205 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 206 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 207 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 208 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 209 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 210 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 211 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 212 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 213 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 214 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 215 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 216 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 217 bi_se_net_v3_tf/re_lu_... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,28,28,64]\r\nTensor 218 bi_se_net_v3_tf/Resize... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 219 bi_se_net_v3_tf/add;St... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 220 bi_se_net_v3_tf/re_lu_... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 221 bi_se_net_v3_tf/Resize... kTfLiteFloat32  kTfLiteArenaRw        3211264B ( 3.1 MB) [1,112,112,64]\r\nTensor 222 bi_se_net_v3_tf/add_1;... kTfLiteFloat32  kTfLiteArenaRw        3211264B ( 3.1 MB) [1,112,112,64]\r\nTensor 223 bi_se_net_v3_tf/re_lu_... kTfLiteFloat32  kTfLiteArenaRw        1605632B ( 1.5 MB) [1,112,112,32]\r\nTensor 224 bi_se_net_v3_tf/Resize... kTfLiteFloat32  kTfLiteArenaRw        6422528B ( 6.1 MB) [1,224,224,32]\r\nTensor 225 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,224,224,1]\r\nTensor 226 Identity                  kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,224,224,1]\r\nTensor 227 (nil)                     kTfLiteInt32    kTfLiteArenaRw             16B ( 0.0 MB) [4]\r\nTensor 228 (nil)                     kTfLiteInt32    kTfLiteArenaRw              8B ( 0.0 MB) [2]\r\nTensor 229 (nil)                     kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [128]\r\nTensor 230 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        1354752B ( 1.3 MB) [1,112,112,27]\r\nTensor 231 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        1354752B ( 1.3 MB) [1,112,112,27]\r\nTensor 232 (nil)                     kTfLiteFloat32  kTfLiteArenaRw         903168B ( 0.9 MB) [1,56,56,72]\r\nTensor 233 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,56,56,288]\r\nTensor 234 (nil)                     kTfLiteFloat32  kTfLiteArenaRw       14450688B (13.8 MB) [1,112,112,288]\r\nTensor 235 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        7225344B ( 6.9 MB) [1,56,56,576]\r\nTensor 236 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        7225344B ( 6.9 MB) [1,56,56,576]\r\nTensor 237 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        7225344B ( 6.9 MB) [1,56,56,576]\r\nTensor 238 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        1806336B ( 1.7 MB) [1,28,28,576]\r\nTensor 239 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,28,28,1152]\r\nTensor 240 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,28,28,1152]\r\nTensor 241 (nil)                     kTfLiteFloat32  kTfLiteArenaRw         903168B ( 0.9 MB) [1,14,14,1152]\r\nTensor 242 (nil)                     kTfLiteFloat32  kTfLiteArenaRw         225792B ( 0.2 MB) [1,7,7,1152]\r\nTensor 243 (nil)                     kTfLiteFloat32  kTfLiteArenaRw         225792B ( 0.2 MB) [1,7,7,1152]\r\nTensor 244 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,28,28,1152]\r\nTensor 245 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,28,28,1152]\r\nTensor 246 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        7225344B ( 6.9 MB) [1,56,56,576]\r\nTensor 247 (nil)                     kTfLiteFloat32  kTfLiteArenaRw       28901376B (27.6 MB) [1,112,112,576]\r\nTensor 248 (nil)                     kTfLiteFloat32  kTfLiteArenaRw       57802752B (55.1 MB) [1,224,224,288]\r\n\r\nkTfLiteArenaRw Info: \r\nTensor 0 has the max size 602112 bytes (0.6 MB).\r\nThis memory arena is estimated as[0x109820000, 0x10975c000), taking 0.8 MB.\r\n\r\nkTfLiteArenaRwPersistent Info: not holding any allocation.\r\n\r\nNode   0 Operator Builtin Code  41 SUB\r\n  Input Tensors:[0,8]\r\n  Output Tensors:[126]\r\nNode   1 Operator Builtin Code  18 MUL\r\n  Input Tensors:[126,5]\r\n  Output Tensors:[127]\r\nNode   2 Operator Builtin Code  34 PAD\r\n  Input Tensors:[127,7]\r\n  Output Tensors:[128]\r\nNode   3 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[128,53,12]\r\n  Output Tensors:[129]\r\n  Temporary Tensors:[230]\r\nNode   4 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[128,54,42]\r\n  Output Tensors:[130]\r\n  Temporary Tensors:[231]\r\nNode   5 Operator Builtin Code  34 PAD\r\n  Input Tensors:[130,7]\r\n  Output Tensors:[131]\r\nNode   6 Operator Builtin Code  17 MAX_POOL_2D\r\n  Input Tensors:[131]\r\n  Output Tensors:[132]\r\nNode   7 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[130,55,43]\r\n  Output Tensors:[133]\r\nNode   8 Operator Builtin Code  34 PAD\r\n  Input Tensors:[133,7]\r\n  Output Tensors:[134]\r\nNode   9 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[134,56,45]\r\n  Output Tensors:[135]\r\n  Temporary Tensors:[232]\r\nNode  10 Operator Builtin Code   2 CONCATENATION\r\n  Input Tensors:[135,132]\r\n  Output Tensors:[136]\r\nNode  11 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[136,57,46]\r\n  Output Tensors:[137]\r\n  Temporary Tensors:[233]\r\nNode  12 Operator Builtin Code  34 PAD\r\n  Input Tensors:[137,7]\r\n  Output Tensors:[138]\r\nNode  13 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[137,58,47]\r\n  Output Tensors:[139]\r\nNode  14 Operator Builtin Code  34 PAD\r\n  Input Tensors:[139,7]\r\n  Output Tensors:[140]\r\nNode  15 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[140,95,48]\r\n  Output Tensors:[141]\r\nNode  16 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[141,96,49]\r\n  Output Tensors:[142]\r\nNode  17 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[142,59,97]\r\n  Output Tensors:[143]\r\nNode  18 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[129,60,44]\r\n  Output Tensors:[144]\r\n  Temporary Tensors:[234]\r\nNode  19 Operator Builtin Code  34 PAD\r\n  Input Tensors:[144,7]\r\n  Output Tensors:[145]\r\nNode  20 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[138,98,50]\r\n  Output Tensors:[146]\r\nNode  21 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[146,61,99]\r\n  Output Tensors:[147]\r\nNode  22 Operator Builtin Code   0 ADD\r\n  Input Tensors:[143,147]\r\n  Output Tensors:[148]\r\nNode  23 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[145,62,13]\r\n  Output Tensors:[149]\r\n  Temporary Tensors:[235]\r\nNode  24 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[149,63,20]\r\n  Output Tensors:[150]\r\n  Temporary Tensors:[236]\r\nNode  25 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[150,64,28]\r\n  Output Tensors:[151]\r\n  Temporary Tensors:[237]\r\nNode  26 Operator Builtin Code  34 PAD\r\n  Input Tensors:[151,7]\r\n  Output Tensors:[152]\r\nNode  27 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[152,65,37]\r\n  Output Tensors:[153]\r\n  Temporary Tensors:[238]\r\nNode  28 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[153,66,40]\r\n  Output Tensors:[154]\r\n  Temporary Tensors:[239]\r\nNode  29 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[154,67,41]\r\n  Output Tensors:[155]\r\n  Temporary Tensors:[240]\r\nNode  30 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[155,100,36]\r\n  Output Tensors:[156]\r\nNode  31 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[156,68,124]\r\n  Output Tensors:[157]\r\nNode  32 Operator Builtin Code  34 PAD\r\n  Input Tensors:[155,7]\r\n  Output Tensors:[158]\r\nNode  33 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[158,69,101]\r\n  Output Tensors:[159]\r\n  Temporary Tensors:[241]\r\nNode  34 Operator Builtin Code  34 PAD\r\n  Input Tensors:[159,7]\r\n  Output Tensors:[160]\r\nNode  35 Operator Builtin Code   1 AVERAGE_POOL_2D\r\n  Input Tensors:[160]\r\n  Output Tensors:[161]\r\nNode  36 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[148,70,51]\r\n  Output Tensors:[162]\r\nNode  37 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[162,102,52]\r\n  Output Tensors:[163]\r\nNode  38 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[163,71,103]\r\n  Output Tensors:[164]\r\nNode  39 Operator Builtin Code   0 ADD\r\n  Input Tensors:[164,148]\r\n  Output Tensors:[165]\r\nNode  40 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[165,72,14]\r\n  Output Tensors:[166]\r\nNode  41 Operator Builtin Code  34 PAD\r\n  Input Tensors:[166,7]\r\n  Output Tensors:[167]\r\nNode  42 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[167,104,15]\r\n  Output Tensors:[168]\r\nNode  43 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[168,105,16]\r\n  Output Tensors:[169]\r\nNode  44 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[169,73,106]\r\n  Output Tensors:[170]\r\nNode  45 Operator Builtin Code  34 PAD\r\n  Input Tensors:[165,7]\r\n  Output Tensors:[171]\r\nNode  46 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[171,107,17]\r\n  Output Tensors:[172]\r\nNode  47 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[172,74,108]\r\n  Output Tensors:[173]\r\nNode  48 Operator Builtin Code   0 ADD\r\n  Input Tensors:[170,173]\r\n  Output Tensors:[174]\r\nNode  49 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[174,75,18]\r\n  Output Tensors:[175]\r\nNode  50 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[175,109,19]\r\n  Output Tensors:[176]\r\nNode  51 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[176,76,110]\r\n  Output Tensors:[177]\r\nNode  52 Operator Builtin Code   0 ADD\r\n  Input Tensors:[177,174]\r\n  Output Tensors:[178]\r\nNode  53 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[178,77,21]\r\n  Output Tensors:[179]\r\nNode  54 Operator Builtin Code  34 PAD\r\n  Input Tensors:[179,7]\r\n  Output Tensors:[180]\r\nNode  55 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[180,111,22]\r\n  Output Tensors:[181]\r\nNode  56 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[181,112,23]\r\n  Output Tensors:[182]\r\nNode  57 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[182,78,113]\r\n  Output Tensors:[183]\r\nNode  58 Operator Builtin Code  34 PAD\r\n  Input Tensors:[178,7]\r\n  Output Tensors:[184]\r\nNode  59 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[184,114,24]\r\n  Output Tensors:[185]\r\nNode  60 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[185,79,115]\r\n  Output Tensors:[186]\r\nNode  61 Operator Builtin Code   0 ADD\r\n  Input Tensors:[183,186]\r\n  Output Tensors:[187]\r\nNode  62 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[187,80,25]\r\n  Output Tensors:[188]\r\nNode  63 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[188,116,26]\r\n  Output Tensors:[189]\r\nNode  64 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[189,81,117]\r\n  Output Tensors:[190]\r\nNode  65 Operator Builtin Code   0 ADD\r\n  Input Tensors:[190,187]\r\n  Output Tensors:[191]\r\nNode  66 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[191,82,27]\r\n  Output Tensors:[192]\r\nNode  67 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[192,118,29]\r\n  Output Tensors:[193]\r\nNode  68 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[193,83,119]\r\n  Output Tensors:[194]\r\nNode  69 Operator Builtin Code   0 ADD\r\n  Input Tensors:[194,191]\r\n  Output Tensors:[195]\r\nNode  70 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[195,84,30]\r\n  Output Tensors:[196]\r\nNode  71 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[196,120,31]\r\n  Output Tensors:[197]\r\nNode  72 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[197,85,121]\r\n  Output Tensors:[198]\r\nNode  73 Operator Builtin Code   0 ADD\r\n  Input Tensors:[198,195]\r\n  Output Tensors:[199]\r\nNode  74 Operator Builtin Code  40 MEAN\r\n  Input Tensors:[199,6]\r\n  Output Tensors:[200]\r\n  Temporary Tensors:[227,228,229]\r\nNode  75 Operator Builtin Code  18 MUL\r\n  Input Tensors:[200,32]\r\n  Output Tensors:[201]\r\nNode  76 Operator Builtin Code   0 ADD\r\n  Input Tensors:[201,33]\r\n  Output Tensors:[202]\r\nNode  77 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[202,86,34]\r\n  Output Tensors:[203]\r\nNode  78 Operator Builtin Code   0 ADD\r\n  Input Tensors:[203,199]\r\n  Output Tensors:[204]\r\nNode  79 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[204,87,35]\r\n  Output Tensors:[205]\r\n  Temporary Tensors:[242]\r\nNode  80 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[205,88,122]\r\n  Output Tensors:[206]\r\n  Temporary Tensors:[243]\r\nNode  81 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[206,4]\r\n  Output Tensors:[207]\r\nNode  82 Operator Builtin Code  14 LOGISTIC\r\n  Input Tensors:[207]\r\n  Output Tensors:[208]\r\nNode  83 Operator Builtin Code  18 MUL\r\n  Input Tensors:[157,208]\r\n  Output Tensors:[209]\r\nNode  84 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[205,123,38]\r\n  Output Tensors:[210]\r\nNode  85 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[210,89,124]\r\n  Output Tensors:[211]\r\nNode  86 Operator Builtin Code  14 LOGISTIC\r\n  Input Tensors:[211]\r\n  Output Tensors:[212]\r\nNode  87 Operator Builtin Code  18 MUL\r\n  Input Tensors:[161,212]\r\n  Output Tensors:[213]\r\nNode  88 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[213,4]\r\n  Output Tensors:[214]\r\nNode  89 Operator Builtin Code   0 ADD\r\n  Input Tensors:[209,214]\r\n  Output Tensors:[215]\r\nNode  90 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[215,90,39]\r\n  Output Tensors:[216]\r\n  Temporary Tensors:[244]\r\nNode  91 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[216,91,9]\r\n  Output Tensors:[217]\r\n  Temporary Tensors:[245]\r\nNode  92 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[217,1]\r\n  Output Tensors:[218]\r\nNode  93 Operator Builtin Code   0 ADD\r\n  Input Tensors:[218,151]\r\n  Output Tensors:[219]\r\nNode  94 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[219,92,10]\r\n  Output Tensors:[220]\r\n  Temporary Tensors:[246]\r\nNode  95 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[220,2]\r\n  Output Tensors:[221]\r\nNode  96 Operator Builtin Code   0 ADD\r\n  Input Tensors:[221,144]\r\n  Output Tensors:[222]\r\nNode  97 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[222,93,11]\r\n  Output Tensors:[223]\r\n  Temporary Tensors:[247]\r\nNode  98 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[223,3]\r\n  Output Tensors:[224]\r\nNode  99 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[224,94,125]\r\n  Output Tensors:[225]\r\n  Temporary Tensors:[248]\r\nNode 100 Operator Builtin Code  14 LOGISTIC\r\n  Input Tensors:[225]\r\n  Output Tensors:[226]\r\nNode 101 Operator Custom Name TfLiteMetalDelegate\r\n  Input Tensors:[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125]\r\n  Output Tensors:[226]\r\n--------------Subgraph-0 dump has completed--------------\r\n\r\n2021-08-03 14:53:03.492138+0800 tflite_simple_example[4144:2858180] model load and init time 17904 ms\r\n\r\n\r\n=== Post-invoke Interpreter State Time 39 ===\r\nInterpreter has 1 subgraphs.\r\n\r\n-----------Subgraph-0 has 249 tensors and 102 nodes------------\r\nInputs: [0]\r\nOutputs: [226]\r\n\r\nTensor  ID Name                      Type            AllocType                Size\r\nTensor   0 input_1                   kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,224,224,3]\r\nTensor   1 bi_se_net_v3_tf/Resize... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   2 bi_se_net_v3_tf/Resize... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   3 bi_se_net_v3_tf/Resize... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   4 bi_se_net_v3_tf/bga_la... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   5 bi_se_net_v3_tf/mul/y;... kTfLiteFloat32  kTfLiteMmapRo              12B ( 0.0 MB) [1,1,1,3]\r\nTensor   6 bi_se_net_v3_tf/segmen... kTfLiteInt32    kTfLiteMmapRo               8B ( 0.0 MB) [2]\r\nTensor   7 bi_se_net_v3_tf/segmen... kTfLiteInt32    kTfLiteMmapRo              32B ( 0.0 MB) [4,2]\r\nTensor   8 bi_se_net_v3_tf/sub/y;... kTfLiteFloat32  kTfLiteMmapRo              12B ( 0.0 MB) [1,1,1,3]\r\nTensor   9 unknown_266               kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  10 unknown_268               kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  11 unknown_270               kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor  12 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor  13 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  14 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  15 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  16 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  17 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor  18 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  19 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  20 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  21 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  22 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  23 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1536B ( 0.0 MB) [384]\r\nTensor  24 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  25 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  26 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  27 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  28 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  29 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  30 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  31 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3072B ( 0.0 MB) [768]\r\nTensor  32 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  33 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  34 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  35 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  36 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  37 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  38 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  39 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  40 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  41 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor  42 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              64B ( 0.0 MB) [16]\r\nTensor  43 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              32B ( 0.0 MB) [8]\r\nTensor  44 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor  45 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              64B ( 0.0 MB) [16]\r\nTensor  46 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              64B ( 0.0 MB) [16]\r\nTensor  47 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             384B ( 0.0 MB) [96]\r\nTensor  48 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             384B ( 0.0 MB) [96]\r\nTensor  49 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             384B ( 0.0 MB) [96]\r\nTensor  50 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo              64B ( 0.0 MB) [16]\r\nTensor  51 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  52 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             768B ( 0.0 MB) [192]\r\nTensor  53 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo            3456B ( 0.0 MB) [32,3,3,3]\r\nTensor  54 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1728B ( 0.0 MB) [16,3,3,3]\r\nTensor  55 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [8,1,1,16]\r\nTensor  56 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            4608B ( 0.0 MB) [16,3,3,8]\r\nTensor  57 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           18432B ( 0.0 MB) [16,3,3,32]\r\nTensor  58 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            6144B ( 0.0 MB) [96,1,1,16]\r\nTensor  59 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           12288B ( 0.0 MB) [32,1,1,96]\r\nTensor  60 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo           73728B ( 0.1 MB) [64,3,3,32]\r\nTensor  61 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            2048B ( 0.0 MB) [32,1,1,16]\r\nTensor  62 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          147456B ( 0.1 MB) [64,3,3,64]\r\nTensor  63 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          147456B ( 0.1 MB) [64,3,3,64]\r\nTensor  64 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          147456B ( 0.1 MB) [64,3,3,64]\r\nTensor  65 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          294912B ( 0.3 MB) [128,3,3,64]\r\nTensor  66 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  67 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  68 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo           65536B ( 0.1 MB) [128,1,1,128]\r\nTensor  69 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  70 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           24576B ( 0.0 MB) [192,1,1,32]\r\nTensor  71 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           24576B ( 0.0 MB) [32,1,1,192]\r\nTensor  72 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           24576B ( 0.0 MB) [192,1,1,32]\r\nTensor  73 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           49152B ( 0.0 MB) [64,1,1,192]\r\nTensor  74 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            8192B ( 0.0 MB) [64,1,1,32]\r\nTensor  75 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           98304B ( 0.1 MB) [384,1,1,64]\r\nTensor  76 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           98304B ( 0.1 MB) [64,1,1,384]\r\nTensor  77 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           98304B ( 0.1 MB) [384,1,1,64]\r\nTensor  78 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          196608B ( 0.2 MB) [128,1,1,384]\r\nTensor  79 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           32768B ( 0.0 MB) [128,1,1,64]\r\nTensor  80 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [768,1,1,128]\r\nTensor  81 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [128,1,1,768]\r\nTensor  82 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [768,1,1,128]\r\nTensor  83 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [128,1,1,768]\r\nTensor  84 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [768,1,1,128]\r\nTensor  85 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          393216B ( 0.4 MB) [128,1,1,768]\r\nTensor  86 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           65536B ( 0.1 MB) [128,1,1,128]\r\nTensor  87 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  88 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  89 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo           65536B ( 0.1 MB) [128,1,1,128]\r\nTensor  90 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo          589824B ( 0.6 MB) [128,3,3,128]\r\nTensor  91 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo          294912B ( 0.3 MB) [64,3,3,128]\r\nTensor  92 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo          147456B ( 0.1 MB) [64,3,3,64]\r\nTensor  93 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo           73728B ( 0.1 MB) [32,3,3,64]\r\nTensor  94 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo            1152B ( 0.0 MB) [1,3,3,32]\r\nTensor  95 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3456B ( 0.0 MB) [1,3,3,96]\r\nTensor  96 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            3456B ( 0.0 MB) [1,3,3,96]\r\nTensor  97 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor  98 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             576B ( 0.0 MB) [1,3,3,16]\r\nTensor  99 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor 100 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo            4608B ( 0.0 MB) [1,3,3,128]\r\nTensor 101 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 102 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            6912B ( 0.0 MB) [1,3,3,192]\r\nTensor 103 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             128B ( 0.0 MB) [32]\r\nTensor 104 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            6912B ( 0.0 MB) [1,3,3,192]\r\nTensor 105 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            6912B ( 0.0 MB) [1,3,3,192]\r\nTensor 106 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor 107 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            1152B ( 0.0 MB) [1,3,3,32]\r\nTensor 108 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor 109 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           13824B ( 0.0 MB) [1,3,3,384]\r\nTensor 110 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             256B ( 0.0 MB) [64]\r\nTensor 111 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           13824B ( 0.0 MB) [1,3,3,384]\r\nTensor 112 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           13824B ( 0.0 MB) [1,3,3,384]\r\nTensor 113 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 114 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo            2304B ( 0.0 MB) [1,3,3,64]\r\nTensor 115 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 116 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           27648B ( 0.0 MB) [1,3,3,768]\r\nTensor 117 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 118 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           27648B ( 0.0 MB) [1,3,3,768]\r\nTensor 119 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 120 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo           27648B ( 0.0 MB) [1,3,3,768]\r\nTensor 121 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 122 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 123 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo            4608B ( 0.0 MB) [1,3,3,128]\r\nTensor 124 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteMmapRo             512B ( 0.0 MB) [128]\r\nTensor 125 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteMmapRo               4B ( 0.0 MB) [1]\r\nTensor 126 bi_se_net_v3_tf/sub;St... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,224,224,3]\r\nTensor 127 bi_se_net_v3_tf/mul;St... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,224,224,3]\r\nTensor 128 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         612912B ( 0.6 MB) [1,226,226,3]\r\nTensor 129 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw        1605632B ( 1.5 MB) [1,112,112,32]\r\nTensor 130 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,112,112,16]\r\nTensor 131 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         831744B ( 0.8 MB) [1,114,114,16]\r\nTensor 132 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,56,56,16]\r\nTensor 133 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,112,112,8]\r\nTensor 134 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         415872B ( 0.4 MB) [1,114,114,8]\r\nTensor 135 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,56,56,16]\r\nTensor 136 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,56,56,32]\r\nTensor 137 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,56,56,16]\r\nTensor 138 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         215296B ( 0.2 MB) [1,58,58,16]\r\nTensor 139 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw        1204224B ( 1.1 MB) [1,56,56,96]\r\nTensor 140 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw        1291776B ( 1.2 MB) [1,58,58,96]\r\nTensor 141 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,28,28,96]\r\nTensor 142 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,28,28,96]\r\nTensor 143 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 144 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw        3211264B ( 3.1 MB) [1,112,112,64]\r\nTensor 145 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw        3326976B ( 3.2 MB) [1,114,114,64]\r\nTensor 146 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,28,28,16]\r\nTensor 147 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 148 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 149 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 150 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 151 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 152 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         861184B ( 0.8 MB) [1,58,58,64]\r\nTensor 153 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 154 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 155 bi_se_net_v3_tf/detail... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 156 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 157 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 158 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         460800B ( 0.4 MB) [1,30,30,128]\r\nTensor 159 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,14,14,128]\r\nTensor 160 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         131072B ( 0.1 MB) [1,16,16,128]\r\nTensor 161 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 162 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,28,28,192]\r\nTensor 163 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,28,28,192]\r\nTensor 164 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 165 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         100352B ( 0.1 MB) [1,28,28,32]\r\nTensor 166 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         602112B ( 0.6 MB) [1,28,28,192]\r\nTensor 167 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         691200B ( 0.7 MB) [1,30,30,192]\r\nTensor 168 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,14,14,192]\r\nTensor 169 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,14,14,192]\r\nTensor 170 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 171 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         115200B ( 0.1 MB) [1,30,30,32]\r\nTensor 172 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,14,14,32]\r\nTensor 173 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 174 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 175 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,14,14,384]\r\nTensor 176 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,14,14,384]\r\nTensor 177 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 178 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          50176B ( 0.0 MB) [1,14,14,64]\r\nTensor 179 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         301056B ( 0.3 MB) [1,14,14,384]\r\nTensor 180 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         393216B ( 0.4 MB) [1,16,16,384]\r\nTensor 181 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          75264B ( 0.1 MB) [1,7,7,384]\r\nTensor 182 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          75264B ( 0.1 MB) [1,7,7,384]\r\nTensor 183 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 184 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          65536B ( 0.1 MB) [1,16,16,64]\r\nTensor 185 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          12544B ( 0.0 MB) [1,7,7,64]\r\nTensor 186 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 187 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 188 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 189 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 190 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 191 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 192 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 193 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 194 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 195 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 196 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 197 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw         150528B ( 0.1 MB) [1,7,7,768]\r\nTensor 198 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 199 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 200 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [1,1,1,128]\r\nTensor 201 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [1,1,1,128]\r\nTensor 202 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [1,1,1,128]\r\nTensor 203 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [1,1,1,128]\r\nTensor 204 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 205 bi_se_net_v3_tf/segmen... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 206 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 207 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 208 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 209 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 210 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 211 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 212 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 213 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw          25088B ( 0.0 MB) [1,7,7,128]\r\nTensor 214 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 215 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 216 bi_se_net_v3_tf/bga_la... kTfLiteFloat32  kTfLiteArenaRw         401408B ( 0.4 MB) [1,28,28,128]\r\nTensor 217 bi_se_net_v3_tf/re_lu_... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,28,28,64]\r\nTensor 218 bi_se_net_v3_tf/Resize... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 219 bi_se_net_v3_tf/add;St... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 220 bi_se_net_v3_tf/re_lu_... kTfLiteFloat32  kTfLiteArenaRw         802816B ( 0.8 MB) [1,56,56,64]\r\nTensor 221 bi_se_net_v3_tf/Resize... kTfLiteFloat32  kTfLiteArenaRw        3211264B ( 3.1 MB) [1,112,112,64]\r\nTensor 222 bi_se_net_v3_tf/add_1;... kTfLiteFloat32  kTfLiteArenaRw        3211264B ( 3.1 MB) [1,112,112,64]\r\nTensor 223 bi_se_net_v3_tf/re_lu_... kTfLiteFloat32  kTfLiteArenaRw        1605632B ( 1.5 MB) [1,112,112,32]\r\nTensor 224 bi_se_net_v3_tf/Resize... kTfLiteFloat32  kTfLiteArenaRw        6422528B ( 6.1 MB) [1,224,224,32]\r\nTensor 225 bi_se_net_v3_tf/conv2d... kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,224,224,1]\r\nTensor 226 Identity                  kTfLiteFloat32  kTfLiteArenaRw         200704B ( 0.2 MB) [1,224,224,1]\r\nTensor 227 (nil)                     kTfLiteInt32    kTfLiteArenaRw             16B ( 0.0 MB) [4]\r\nTensor 228 (nil)                     kTfLiteInt32    kTfLiteArenaRw              8B ( 0.0 MB) [2]\r\nTensor 229 (nil)                     kTfLiteFloat32  kTfLiteArenaRw            512B ( 0.0 MB) [128]\r\nTensor 230 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        1354752B ( 1.3 MB) [1,112,112,27]\r\nTensor 231 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        1354752B ( 1.3 MB) [1,112,112,27]\r\nTensor 232 (nil)                     kTfLiteFloat32  kTfLiteArenaRw         903168B ( 0.9 MB) [1,56,56,72]\r\nTensor 233 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,56,56,288]\r\nTensor 234 (nil)                     kTfLiteFloat32  kTfLiteArenaRw       14450688B (13.8 MB) [1,112,112,288]\r\nTensor 235 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        7225344B ( 6.9 MB) [1,56,56,576]\r\nTensor 236 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        7225344B ( 6.9 MB) [1,56,56,576]\r\nTensor 237 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        7225344B ( 6.9 MB) [1,56,56,576]\r\nTensor 238 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        1806336B ( 1.7 MB) [1,28,28,576]\r\nTensor 239 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,28,28,1152]\r\nTensor 240 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,28,28,1152]\r\nTensor 241 (nil)                     kTfLiteFloat32  kTfLiteArenaRw         903168B ( 0.9 MB) [1,14,14,1152]\r\nTensor 242 (nil)                     kTfLiteFloat32  kTfLiteArenaRw         225792B ( 0.2 MB) [1,7,7,1152]\r\nTensor 243 (nil)                     kTfLiteFloat32  kTfLiteArenaRw         225792B ( 0.2 MB) [1,7,7,1152]\r\nTensor 244 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,28,28,1152]\r\nTensor 245 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        3612672B ( 3.4 MB) [1,28,28,1152]\r\nTensor 246 (nil)                     kTfLiteFloat32  kTfLiteArenaRw        7225344B ( 6.9 MB) [1,56,56,576]\r\nTensor 247 (nil)                     kTfLiteFloat32  kTfLiteArenaRw       28901376B (27.6 MB) [1,112,112,576]\r\nTensor 248 (nil)                     kTfLiteFloat32  kTfLiteArenaRw       57802752B (55.1 MB) [1,224,224,288]\r\n\r\nkTfLiteArenaRw Info: \r\nTensor 0 has the max size 602112 bytes (0.6 MB).\r\nThis memory arena is estimated as[0x109820000, 0x10975c000), taking 0.8 MB.\r\n\r\nkTfLiteArenaRwPersistent Info: not holding any allocation.\r\n\r\nNode   0 Operator Builtin Code  41 SUB\r\n  Input Tensors:[0,8]\r\n  Output Tensors:[126]\r\nNode   1 Operator Builtin Code  18 MUL\r\n  Input Tensors:[126,5]\r\n  Output Tensors:[127]\r\nNode   2 Operator Builtin Code  34 PAD\r\n  Input Tensors:[127,7]\r\n  Output Tensors:[128]\r\nNode   3 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[128,53,12]\r\n  Output Tensors:[129]\r\n  Temporary Tensors:[230]\r\nNode   4 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[128,54,42]\r\n  Output Tensors:[130]\r\n  Temporary Tensors:[231]\r\nNode   5 Operator Builtin Code  34 PAD\r\n  Input Tensors:[130,7]\r\n  Output Tensors:[131]\r\nNode   6 Operator Builtin Code  17 MAX_POOL_2D\r\n  Input Tensors:[131]\r\n  Output Tensors:[132]\r\nNode   7 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[130,55,43]\r\n  Output Tensors:[133]\r\nNode   8 Operator Builtin Code  34 PAD\r\n  Input Tensors:[133,7]\r\n  Output Tensors:[134]\r\nNode   9 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[134,56,45]\r\n  Output Tensors:[135]\r\n  Temporary Tensors:[232]\r\nNode  10 Operator Builtin Code   2 CONCATENATION\r\n  Input Tensors:[135,132]\r\n  Output Tensors:[136]\r\nNode  11 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[136,57,46]\r\n  Output Tensors:[137]\r\n  Temporary Tensors:[233]\r\nNode  12 Operator Builtin Code  34 PAD\r\n  Input Tensors:[137,7]\r\n  Output Tensors:[138]\r\nNode  13 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[137,58,47]\r\n  Output Tensors:[139]\r\nNode  14 Operator Builtin Code  34 PAD\r\n  Input Tensors:[139,7]\r\n  Output Tensors:[140]\r\nNode  15 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[140,95,48]\r\n  Output Tensors:[141]\r\nNode  16 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[141,96,49]\r\n  Output Tensors:[142]\r\nNode  17 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[142,59,97]\r\n  Output Tensors:[143]\r\nNode  18 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[129,60,44]\r\n  Output Tensors:[144]\r\n  Temporary Tensors:[234]\r\nNode  19 Operator Builtin Code  34 PAD\r\n  Input Tensors:[144,7]\r\n  Output Tensors:[145]\r\nNode  20 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[138,98,50]\r\n  Output Tensors:[146]\r\nNode  21 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[146,61,99]\r\n  Output Tensors:[147]\r\nNode  22 Operator Builtin Code   0 ADD\r\n  Input Tensors:[143,147]\r\n  Output Tensors:[148]\r\nNode  23 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[145,62,13]\r\n  Output Tensors:[149]\r\n  Temporary Tensors:[235]\r\nNode  24 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[149,63,20]\r\n  Output Tensors:[150]\r\n  Temporary Tensors:[236]\r\nNode  25 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[150,64,28]\r\n  Output Tensors:[151]\r\n  Temporary Tensors:[237]\r\nNode  26 Operator Builtin Code  34 PAD\r\n  Input Tensors:[151,7]\r\n  Output Tensors:[152]\r\nNode  27 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[152,65,37]\r\n  Output Tensors:[153]\r\n  Temporary Tensors:[238]\r\nNode  28 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[153,66,40]\r\n  Output Tensors:[154]\r\n  Temporary Tensors:[239]\r\nNode  29 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[154,67,41]\r\n  Output Tensors:[155]\r\n  Temporary Tensors:[240]\r\nNode  30 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[155,100,36]\r\n  Output Tensors:[156]\r\nNode  31 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[156,68,124]\r\n  Output Tensors:[157]\r\nNode  32 Operator Builtin Code  34 PAD\r\n  Input Tensors:[155,7]\r\n  Output Tensors:[158]\r\nNode  33 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[158,69,101]\r\n  Output Tensors:[159]\r\n  Temporary Tensors:[241]\r\nNode  34 Operator Builtin Code  34 PAD\r\n  Input Tensors:[159,7]\r\n  Output Tensors:[160]\r\nNode  35 Operator Builtin Code   1 AVERAGE_POOL_2D\r\n  Input Tensors:[160]\r\n  Output Tensors:[161]\r\nNode  36 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[148,70,51]\r\n  Output Tensors:[162]\r\nNode  37 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[162,102,52]\r\n  Output Tensors:[163]\r\nNode  38 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[163,71,103]\r\n  Output Tensors:[164]\r\nNode  39 Operator Builtin Code   0 ADD\r\n  Input Tensors:[164,148]\r\n  Output Tensors:[165]\r\nNode  40 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[165,72,14]\r\n  Output Tensors:[166]\r\nNode  41 Operator Builtin Code  34 PAD\r\n  Input Tensors:[166,7]\r\n  Output Tensors:[167]\r\nNode  42 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[167,104,15]\r\n  Output Tensors:[168]\r\nNode  43 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[168,105,16]\r\n  Output Tensors:[169]\r\nNode  44 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[169,73,106]\r\n  Output Tensors:[170]\r\nNode  45 Operator Builtin Code  34 PAD\r\n  Input Tensors:[165,7]\r\n  Output Tensors:[171]\r\nNode  46 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[171,107,17]\r\n  Output Tensors:[172]\r\nNode  47 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[172,74,108]\r\n  Output Tensors:[173]\r\nNode  48 Operator Builtin Code   0 ADD\r\n  Input Tensors:[170,173]\r\n  Output Tensors:[174]\r\nNode  49 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[174,75,18]\r\n  Output Tensors:[175]\r\nNode  50 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[175,109,19]\r\n  Output Tensors:[176]\r\nNode  51 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[176,76,110]\r\n  Output Tensors:[177]\r\nNode  52 Operator Builtin Code   0 ADD\r\n  Input Tensors:[177,174]\r\n  Output Tensors:[178]\r\nNode  53 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[178,77,21]\r\n  Output Tensors:[179]\r\nNode  54 Operator Builtin Code  34 PAD\r\n  Input Tensors:[179,7]\r\n  Output Tensors:[180]\r\nNode  55 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[180,111,22]\r\n  Output Tensors:[181]\r\nNode  56 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[181,112,23]\r\n  Output Tensors:[182]\r\nNode  57 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[182,78,113]\r\n  Output Tensors:[183]\r\nNode  58 Operator Builtin Code  34 PAD\r\n  Input Tensors:[178,7]\r\n  Output Tensors:[184]\r\nNode  59 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[184,114,24]\r\n  Output Tensors:[185]\r\nNode  60 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[185,79,115]\r\n  Output Tensors:[186]\r\nNode  61 Operator Builtin Code   0 ADD\r\n  Input Tensors:[183,186]\r\n  Output Tensors:[187]\r\nNode  62 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[187,80,25]\r\n  Output Tensors:[188]\r\nNode  63 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[188,116,26]\r\n  Output Tensors:[189]\r\nNode  64 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[189,81,117]\r\n  Output Tensors:[190]\r\nNode  65 Operator Builtin Code   0 ADD\r\n  Input Tensors:[190,187]\r\n  Output Tensors:[191]\r\nNode  66 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[191,82,27]\r\n  Output Tensors:[192]\r\nNode  67 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[192,118,29]\r\n  Output Tensors:[193]\r\nNode  68 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[193,83,119]\r\n  Output Tensors:[194]\r\nNode  69 Operator Builtin Code   0 ADD\r\n  Input Tensors:[194,191]\r\n  Output Tensors:[195]\r\nNode  70 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[195,84,30]\r\n  Output Tensors:[196]\r\nNode  71 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[196,120,31]\r\n  Output Tensors:[197]\r\nNode  72 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[197,85,121]\r\n  Output Tensors:[198]\r\nNode  73 Operator Builtin Code   0 ADD\r\n  Input Tensors:[198,195]\r\n  Output Tensors:[199]\r\nNode  74 Operator Builtin Code  40 MEAN\r\n  Input Tensors:[199,6]\r\n  Output Tensors:[200]\r\n  Temporary Tensors:[227,228,229]\r\nNode  75 Operator Builtin Code  18 MUL\r\n  Input Tensors:[200,32]\r\n  Output Tensors:[201]\r\nNode  76 Operator Builtin Code   0 ADD\r\n  Input Tensors:[201,33]\r\n  Output Tensors:[202]\r\nNode  77 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[202,86,34]\r\n  Output Tensors:[203]\r\nNode  78 Operator Builtin Code   0 ADD\r\n  Input Tensors:[203,199]\r\n  Output Tensors:[204]\r\nNode  79 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[204,87,35]\r\n  Output Tensors:[205]\r\n  Temporary Tensors:[242]\r\nNode  80 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[205,88,122]\r\n  Output Tensors:[206]\r\n  Temporary Tensors:[243]\r\nNode  81 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[206,4]\r\n  Output Tensors:[207]\r\nNode  82 Operator Builtin Code  14 LOGISTIC\r\n  Input Tensors:[207]\r\n  Output Tensors:[208]\r\nNode  83 Operator Builtin Code  18 MUL\r\n  Input Tensors:[157,208]\r\n  Output Tensors:[209]\r\nNode  84 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Input Tensors:[205,123,38]\r\n  Output Tensors:[210]\r\nNode  85 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[210,89,124]\r\n  Output Tensors:[211]\r\nNode  86 Operator Builtin Code  14 LOGISTIC\r\n  Input Tensors:[211]\r\n  Output Tensors:[212]\r\nNode  87 Operator Builtin Code  18 MUL\r\n  Input Tensors:[161,212]\r\n  Output Tensors:[213]\r\nNode  88 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[213,4]\r\n  Output Tensors:[214]\r\nNode  89 Operator Builtin Code   0 ADD\r\n  Input Tensors:[209,214]\r\n  Output Tensors:[215]\r\nNode  90 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[215,90,39]\r\n  Output Tensors:[216]\r\n  Temporary Tensors:[244]\r\nNode  91 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[216,91,9]\r\n  Output Tensors:[217]\r\n  Temporary Tensors:[245]\r\nNode  92 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[217,1]\r\n  Output Tensors:[218]\r\nNode  93 Operator Builtin Code   0 ADD\r\n  Input Tensors:[218,151]\r\n  Output Tensors:[219]\r\nNode  94 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[219,92,10]\r\n  Output Tensors:[220]\r\n  Temporary Tensors:[246]\r\nNode  95 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[220,2]\r\n  Output Tensors:[221]\r\nNode  96 Operator Builtin Code   0 ADD\r\n  Input Tensors:[221,144]\r\n  Output Tensors:[222]\r\nNode  97 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[222,93,11]\r\n  Output Tensors:[223]\r\n  Temporary Tensors:[247]\r\nNode  98 Operator Builtin Code  23 RESIZE_BILINEAR\r\n  Input Tensors:[223,3]\r\n  Output Tensors:[224]\r\nNode  99 Operator Builtin Code   3 CONV_2D\r\n  Input Tensors:[224,94,125]\r\n  Output Tensors:[225]\r\n  Temporary Tensors:[248]\r\nNode 100 Operator Builtin Code  14 LOGISTIC\r\n  Input Tensors:[225]\r\n  Output Tensors:[226]\r\nNode 101 Operator Custom Name TfLiteMetalDelegate\r\n  Input Tensors:[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125]\r\n  Output Tensors:[226]\r\n--------------Subgraph-0 dump has completed--------------\r\n\r\n**Describe the expected behavior**\r\nCPU GPU delegate has the same output\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["It's unclear what model it is, but given the SUB MUL at the beginning AND your input tensors being pure integers, I assume it's a quantized model.  I'm not sure our Metal backend supports quantized models...\r\n\r\nCan you feed it a non-quantized, float model and float input tensors?", "> It's unclear what model it is, but given the SUB MUL at the beginning AND your input tensors being pure integers, I assume it's a quantized model. I'm not sure our Metal backend supports quantized models...\r\n> \r\n> Can you feed it a non-quantized, float model and float input tensors?\r\n\r\ni upload the model, https://github.com/tensorflow/tensorflow/files/6929310/bisenetv3_fp32_224.tflite.zip ,thanks", "> It's unclear what model it is, but given the SUB MUL at the beginning AND your input tensors being pure integers, I assume it's a quantized model. I'm not sure our Metal backend supports quantized models...\r\n> \r\n> Can you feed it a non-quantized, float model and float input tensors?\r\n\r\nwe cut the first two layer , but the result remain same, cpu differ from gpu \r\n[CPUout_simple.txt](https://github.com/tensorflow/tensorflow/files/6935729/CPUout_simple.txt)\r\n[GPUout_simple.txt](https://github.com/tensorflow/tensorflow/files/6935730/GPUout_simple.txt)\r\n[Uploading bisenetv3_fp32_224_noprocess.tflite.zip\u2026]()\r\n", "can some related staff update this issue progress?"]}, {"number": 51092, "title": "Sparse matmul and element-wise multiply", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf2.5\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?** No\r\n\r\n**Who will benefit with this feature?** Users who need sparse calculation, but with limited memory resources and so big the dimension of data.\r\n\r\n**Any Other info.**\r\n\r\n1. If `a` is a sparse column vector`(n*1)`, then how to get the result of `a matmul a.T`, which is also the sparse matrix with dense_shape `(n*n)` but unable to be stored in the memory? \r\n2. If A and B are both the sparse matrix with dense_shape of `(n*n)`,  then how to calculate the result of `A multiply B`(element-wise), and how to calculate `A matmul B`, the result of which are both sparse matrix with dense shape` (n*n)`? \r\n\r\n\r\n", "comments": ["@sjtusmartboy Is this duplicate of https://github.com/tensorflow/tensorflow/issues/51074? If yes, can you please close https://github.com/tensorflow/tensorflow/issues/51074 and follow the progress in this issue.\r\n\r\nAs you mentioned that you are interested in contributing, you could raise a PR and connect it to this issue. Thanks!", "I would like to contribute, but currently I do not have any idea how to realize this function. And where to raise the PR\r\n\r\nBy the way, #51074 is the matter of sparse-dense matmul of multiple batches, but this one is the the matter of sparse-sparse matmul"]}, {"number": 51084, "title": "Cannot split windowed data when batching is used in tf.data.Dataset", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I have written custom code \r\n- Linux 20.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 2.3/2.4/2.5\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: NVIDIA RTX 2080/11GB\r\n\r\n**Describe the current behavior**\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x, y)).window(2, 1, True).shuffle(buffer_size=tr_s).batch(batch_size)\r\n\r\nfor x in train_dataset_dataset.take(1):\r\n    a, b = x\r\n    c, d = tf.split(list(a), num_or_size_splits=2, axis=0)\r\n\r\nFor the above described code, I cannot split up the windowed data. However, when I do not use batching, I can split the windowed data. I get the following error in various flavours when I attempt to do so:\r\n\r\nTypeError: '_NestedVariant' object is not iterable\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect to be able to split up the data, when windowed data is batched\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nfunda = tf.data.Dataset.range(7).window(2, 1, True).batch(1)\r\n\r\nfor x in funda.take(1):\r\n    a, b = x\r\n    c, d = tf.split(list(a), num_or_size_splits=2, axis=0)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@jvishnuvardhan,\r\n\r\nI am able to reproduce the error in `TF 2.5` and `TF nightly`. Please find the [gist here](https://colab.research.google.com/gist/sanatmpa1/5ecdd2cf08cce6cd9f542e1e6ce53c12/51084.ipynb). Thanks!", "`Dataset.window` returns `Datasets`, so `.window(...).batch()` is creating batches of `Datasets`. I think what you want instead is to create batches of data. You can do this with `flat_map` [1]:\r\n\r\n```python\r\nfunda = tf.data.Dataset.range(7).window(2, 1, True).flat_map(lambda ds: ds.batch(2))\r\nfor elem in funda:\r\n  print(elem.numpy())\r\n\r\n[0 1]\r\n[1 2]\r\n[2 3]\r\n[3 4]\r\n[4 5]\r\n[5 6]\r\n[6]\r\n```\r\n\r\n[1] https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#flatten_a_dataset_of_windows", "> `Dataset.window` returns `Datasets`, so `.window(...).batch()` is creating batches of `Datasets`. I think what you want instead is to create batches of data. You can do this with `flat_map` [1]:\r\n> \r\n> ```python\r\n> funda = tf.data.Dataset.range(7).window(2, 1, True).flat_map(lambda ds: ds.batch(2))\r\n> for elem in funda:\r\n>   print(elem.numpy())\r\n> \r\n> [0 1]\r\n> [1 2]\r\n> [2 3]\r\n> [3 4]\r\n> [4 5]\r\n> [5 6]\r\n> [6]\r\n> ```\r\n> \r\n> [1] https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#flatten_a_dataset_of_windows\r\n\r\nThis is a really cool work around! I'll report back after trying it out. Thanks. "]}, {"number": 51083, "title": "tf.io.read_file memory leackage", "body": "Hello all!\r\n\r\n\r\nIf've found the mem leackage bug in tf.io.read_file  function during my inference testing;\r\n\r\n**Describe the current behavior**\r\n\r\nThis code eats over 30 Gb memory when I read 10000 jpeg images from local nvme drive. And the library does not release memory for data that is not in use anymore. The process eats all available RAM during the intensive file io and then got killed by the OS kernel.\r\n\r\nWhen I changed   string  \"img_string = tf.io.read_file(fpath)\"  on  \"img_string = open(fpath, 'rb').read()\" (and remove tf.function decorator) the leackage has gone. \r\n\r\nWhen i try to read the same images again in the same session, there is no leackage (memory consumption stops on the prevoius value). \r\n\r\n**Describe the expected behavior**\r\nLibrary should release memory for not used data. I expected some kind of garbage collection. Or, if there is a caching under the hood, I expect to have an option to turn it off.  In current sutuation it is impossible to use this function in inference for intence io operations. \r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): the code is based on tf 2.5 documentation, tf.io module.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): built from sources\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)\r\n- CUDA/cuDNN version: CUDA Version: 11.2 / CUDNN 8.0.5.39\r\n- GPU model and memory: NVIDIA RTX3080TI 12Gb, NVIDIA RTX2070 8Gb\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n@tf.function\r\ndef read_jpg(fpath):\r\n        img_string = tf.io.read_file(fpath)       \r\n        img_tensor = tf.io.decode_jpeg(img_string)      \r\n        return img_tensor\r\n\r\n*in cycle of 10000 image reading eats over 30 Gb of RAM\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nThere is no logs. The memory leackage was detected in htop utillite.\r\n", "comments": ["@emenshoff In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "> @emenshoff In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\nimport os\r\nimport sys\r\nimport tensoflow as tf\r\n\r\nMEDIA_FOLDER = '/media/jpeg_samples/'  # over 300 000 images\r\nSAMPLES_NUM = 10000\r\nfnames = os.path.listdir(MEDIA_FOLDER )\r\n\r\nfor  fname in fnames [:SAMPLES_NUM ]:\r\n  fpath = os.path.join(MEDIA_FOLDER, fname)\r\n  img_string = tf.io.read_file(fpath)\r\n  img_tensor = tf.io.decode_jpeg(img_string)\r\n"]}, {"number": 51074, "title": "tf.spars.sparsee_dense_matmul supports multiplication of batches of data in a sparse tensor with a dense tensor", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):  tf 2.5\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?** No.\r\n\r\n**Who will benefit with this feature?** The user who needs the sparse matrix multiplication of batches of data\r\n\r\nAs said in the manual script, `tf.spars.sparsee_dense_matmul`  only support `Multiply SparseTensor (or dense Matrix) (of rank 2)\r\n`. Currently I have batches of data stored in a sparse tensor, then how should I do the multiplication? Any suggestions?", "comments": ["@sjtusmartboy \r\nYou could create a tf.data,Dataset and then map that function.", "It indeed can do that by splitting the data along the batch axis, but I think there's no necessity to limit the rank of sparseTensor to be 2. From the perspective of user's eye, the function should be as easy to use as possible. What's more, there's no need to change the current api, just make full implementation of the function is enough.", "@sjtusmartboy \r\nthis is not a valid request, you may open this issue at tensorflow discussion forum as there is a larger community to answer it there and move this to closed status here.", "\uff20Saduf2019 Any reason why tensorflow set this rank limit?"]}, {"number": 51070, "title": "Index out of bounds error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 20\r\n\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<stdin>\", line 19, in run_odt_and_draw_results\r\nIndexError: index 12 is out of bounds for axis 0 with size 5\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nWas expecting arrays of bounding box values and so forth, as per the model inputs and outputs\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport cv2\r\nfrom PIL import Image\r\nimport wget\r\nimport numpy as np\r\nimport os\r\nfrom tflite_model_maker.config import ExportFormat\r\nfrom tflite_model_maker import model_spec\r\nfrom tflite_model_maker import object_detector\r\nimport tensorflow as tf\r\nassert tf.__version__.startswith('2')\r\ntf.get_logger().setLevel('ERROR')\r\nfrom absl import logging\r\nlogging.set_verbosity(logging.ERROR)\r\nspec = model_spec.get('efficientdet_lite0')\r\ntrain_data, validation_data, test_data = object_detector.DataLoader.from_csv('gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv')\r\nmodel = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)\r\nmodel.evaluate(test_data)\r\nmodel.export(export_dir='/media/nvme')\r\nmodel.evaluate_tflite('/media/nvme/model.tflite', test_data)\r\nmodel_path = \"/media/nvme/model.tflite\"\r\n\r\nclasses = [\"???\"] * model.model_spec.config.num_classes\r\nlabel_map = model.model_spec.config.label_map\r\nfor label_id, label_name in label_map.as_dict().items():\r\n    classes[label_id - 1] = label_name\r\n\r\n# Define a list of colors for visualization\r\nCOLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\r\n\r\n\r\ndef preprocess_image(image_path, input_size):\r\n    \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\r\n    img = tf.io.read_file(image_path)\r\n    img = tf.io.decode_image(img, channels=3)\r\n    img = tf.image.convert_image_dtype(img, tf.uint8)\r\n    original_image = img\r\n    resized_img = tf.image.resize(img, input_size)\r\n    resized_img = resized_img[tf.newaxis, :]\r\n    return resized_img, original_image\r\n\r\n\r\ndef set_input_tensor(interpreter, image):\r\n    \"\"\"Set the input tensor.\"\"\"\r\n    tensor_index = interpreter.get_input_details()[0][\"index\"]\r\n    input_tensor = interpreter.tensor(tensor_index)()[0]\r\n    input_tensor[:, :] = image\r\n\r\n\r\ndef get_output_tensor(interpreter, index):\r\n    \"\"\"Retur the output tensor at the given index.\"\"\"\r\n    output_details = interpreter.get_output_details()[index]\r\n    tensor = np.squeeze(interpreter.get_tensor(output_details[\"index\"]))\r\n    return tensor\r\n\r\n\r\ndef detect_objects(interpreter, image, threshold):\r\n    set_input_tensor(interpreter, image)\r\n    interpreter.invoke()\r\n    boxes = get_output_tensor(interpreter, 0)\r\n    classes = get_output_tensor(interpreter, 1)\r\n    scores = get_output_tensor(interpreter, 2)\r\n    count = int(get_output_tensor(interpreter, 3))\r\n    results = []\r\n    for i in range(count):\r\n        if scores[i] >= threshold:\r\n            result = {\r\n                \"bounding_box\": boxes[i],\r\n                \"class_id\": classes[i],\r\n                \"score\": scores[i],\r\n            }\r\n            results.append(result)\r\n    return results\r\n\r\ndef run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\r\n    _, input_height, input_width, _ = interpreter.get_input_details()[0][\"shape\"]\r\n    preprocessed_image, original_image = preprocess_image(\r\n        image_path, (input_height, input_width)\r\n    )\r\n    results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\r\n    original_image_np = original_image.numpy().astype(np.uint8)\r\n    for obj in results:\r\n        ymin, xmin, ymax, xmax = obj[\"bounding_box\"]\r\n        xmin = int(xmin * original_image_np.shape[1])\r\n        print(\"xmin plus: \")\r\n        print(original_image_np.shape[1])\r\n        xmax = int(xmax * original_image_np.shape[1])\r\n        ymin = int(ymin * original_image_np.shape[0])\r\n        print(\"ymin plus: \")\r\n        print(original_image_np.shape[0])\r\n        ymax = int(ymax * original_image_np.shape[0])\r\n        class_id = int(obj[\"class_id\"])\r\n        color = [int(c) for c in COLORS[class_id]]\r\n        cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\r\n        y = ymin - 15 if ymin - 15 > 15 else ymin + 15\r\n        label = \"{}: {:.0f}%\".format(classes[class_id], obj[\"score\"] * 100)\r\n        cv2.putText(\r\n            original_image_np, label, (xmin, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2\r\n        )\r\n    original_uint8 = original_image_np.astype(np.uint8)\r\n    return original_uint8\r\n\r\nINPUT_IMAGE_URL = \"https://storage.googleapis.com/cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg\"\r\n\r\nDETECTION_THRESHOLD = 0.3\r\n\r\nTEMP_FILE = '/media/nvme/image.png'\r\n\r\n# Download image and save to above location as a 512 x 512 input\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=model_path)\r\ninterpreter.allocate_tensors()\r\n\r\ndetection_result_image = run_odt_and_draw_results(\r\n\tTEMP_FILE,\r\n    interpreter,\r\n    threshold=DETECTION_THRESHOLD\r\n)\r\n```", "comments": ["Are you trying to reproduce https://www.tensorflow.org/lite/tutorials/model_maker_object_detection locally for custom data and hitting problems?\r\nIf so, Can you please mention what steps have you changed?", "Hi @ymodak \r\nThanks for the response, I am just using the Python code as is and also just using the standard data which is provided via gs:// link.\r\nAlso I have a question, which might be related, why does the model return negative values in the bounding box coordinates? What does it mean if the model returns negative numbers as apposed to positive numbers?\r\n\r\nFor example\r\n\r\n```\r\nStatefulPartitionedCall:3:\r\n[-0.0034427345, -0.009861112, 0.96726143, 0.9857403 ... ]\r\n```\r\n\r\nThanks"]}, {"number": 51040, "title": "Bash Bad address if CUDA on None Standard Path", "body": "This is a jax build. See next comment for tensorflow reproduce.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:  https://github.com/tensorflow/tensorflow/archive/8cc3ffa8d8e4dd659c1534849cf5984ef4ec3532.tar.gz\r\n- Python version: python 3.8 and 3.9 via miniconda\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nThis following error occurs if you use CUDA located in non standard path location, that is,\r\nif I use `C:/CUDA/v10.1` instead of `C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1`, a bad address error will be as following:\r\n\r\n```\r\n     _   _  __  __\r\n    | | / \\ \\ \\/ /\r\n _  | |/ _ \\ \\  /\r\n| |_| / ___ \\/  \\\r\n \\___/_/   \\/_/\\_\\\r\n\r\n\r\nStarting local Bazel server and connecting to it...\r\nBazel binary path: C:\\tools\\bazelisk\\bazel.EXE\r\nPython binary path: C:/Users/cloud/Miniconda3/envs/py38/python.exe\r\nPython version: 3.8\r\nNumPy version: 1.17.3\r\nciPy version: 1.5.4MKL-DNN enabled: yes\r\nTarget CPU: AMD64\r\nTarget CPU features: release\r\nCUDA enabled: yes\r\nCUDA toolkit path: C:/CUDA/v10.1\r\nCUDNN library path: C:/CUDA/v10.1\r\nCUDA compute capabilities: 6.1,7.0,7.5\r\nCUDA version: 10.1\r\nCUDNN version: 7.6.5\r\nNCCL enabled: yes\r\nTPU enabled: no\r\nROCm enabled: no\r\n\r\nBuilding XLA and installing it in the jaxlib source tree...\r\nC:\\tools\\bazelisk\\bazel.EXE run --verbose_failures=true --config=short_logs --config=mkl_open_source_only --config=cuda --define=xla_python_enable_gpu=true :build_wheel -- --output_path=D:\\jax\\dist --cpu=AMD64\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'run' from d:\\jax\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  Inherited 'build' options: --python_path=C:/Users/cloud/Miniconda3/envs/py38/python.exe\r\nINFO: Reading rc options for 'run' from d:\\jax\\.bazelrc:\r\n  Inherited 'build' options: --repo_env PYTHON_BIN_PATH=C:/Users/cloud/Miniconda3/envs/py38/python.exe --action_env=PYENV_ROOT --python_path=C:/Users/cloud/Miniconda3/envs/py38/python.exe --repo_env TF_NEED_CUDA=1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0,7.5 --repo_env TF_NEED_ROCM=0 --action_env TF_ROCM_AMDGPU_TARGETS=gfx803,gfx900,gfx906,gfx1010 -c opt --apple_platform_type=macos --macos_minimum_os=10.9 --announce_rc --define open_source_build=true --define=no_aws_support=true --define=no_gcp_support=true --define=no_hdfs_support=true --define=no_kafka_support=true --define=no_ignite_support=true --define=grpc_no_ares=true --spawn_strategy=standalone --strategy=Genrule=standalone --enable_platform_specific_config --action_env CUDA_TOOLKIT_PATH=C:/CUDA/v10.1 --action_env CUDNN_INSTALL_PATH=C:/CUDA/v10.1 --action_env TF_CUDA_PATHS=C:/CUDA/v10.1,C:/CUDA/v10.1 --action_env TF_CUDA_VERSION=10.1 --action_env TF_CUDNN_VERSION=7.6.5 --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:short_logs in file d:\\jax\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:mkl_open_source_only in file d:\\jax\\.bazelrc: --define=tensorflow_mkldnn_contraction_kernel=1\r\nINFO: Found applicable config definition build:cuda in file d:\\jax\\.bazelrc: --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:windows in file d:\\jax\\.bazelrc: --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true\r\nINFO: Analyzed target //build:build_wheel (183 packages loaded, 15537 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/cloud/_bazel_cloud/hwk42b7s/external/llvm-project/llvm/BUILD:60:18: Executing genrule @llvm-project//llvm:abi_breaking_gen failed (Exit 126): bash.exe failed: error executing command\r\n  cd C:/users/cloud/_bazel_cloud/hwk42b7s/execroot/__main__\r\n  SET CUDA_TOOLKIT_PATH=C:/CUDA/v10.1\r\n    SET CUDNN_INSTALL_PATH=C:/CUDA/v10.1\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\windows;C:\\windows\\System32;C:\\windows\\System32\\WindowsPowerShell\\v1.0\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0,7.5\r\n    SET TF_CUDA_PATHS=C:/CUDA/v10.1,C:/CUDA/v10.1\r\n    SET TF_CUDA_VERSION=10.1\r\n    SET TF_CUDNN_VERSION=7.6.5\r\n    SET TF_ROCM_AMDGPU_TARGETS=gfx803,gfx900,gfx906,gfx1010\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/external/org_tensorflow/third_party/llvm/expand_cmake_vars.exe \"ENABLE_BACKTRACES=1\" \"LLVM_BINDIR=/dev/null\" \"LLVM_DISABLE_ABI_BREAKING_CHECKS_ENFORCING=0\" \"LLVM_ENABLE_ABI_BREAKING_CHECKS=0\" \"LLVM_ENABLE_THREADS=1\" \"LLVM_ENABLE_ZLIB=1\" \"LLVM_HAS_ATOMICS=1\" \"LLVM_INCLUDEDIR=/dev/null\" \"LLVM_INFODIR=/dev/null\" \"LLVM_MANDIR=/dev/null\" \"LLVM_NATIVE_TARGET=1\" \"LLVM_NATIVE_TARGETINFO=1\" \"LLVM_NATIVE_TARGETMC=1\" \"LLVM_NATIVE_ASMPRINTER=1\" \"LLVM_NATIVE_ASMPARSER=1\" \"LLVM_NATIVE_DISASSEMBLER=1\" \"LLVM_PREFIX=/dev/null\" \"LLVM_VERSION_MAJOR=0\" \"LLVM_VERSION_MINOR=0\" \"LLVM_VERSION_PATCH=0\" \"PACKAGE_NAME=llvm\" \"PACKAGE_STRING=llvm tensorflow-trunk\" \"PACKAGE_VERSION=tensorflow-trunk\" \"RETSIGTYPE=void\" \"LLVM_HOST_TRIPLE=x86_64-pc-win32\" \"LLVM_DEFAULT_TARGET_TRIPLE=x86_64-pc-win32\" \"LLVM_NATIVE_ARCH=X86\" \"HAVE_ERRNO_H=1\" \"HAVE_EXECINFO_H=1\" \"HAVE_FCNTL_H=1\" \"HAVE_FENV_H=1\" \"HAVE_INTTYPES_H=1\" \"HAVE_MALLOC_H=1\" \"HAVE_SIGNAL_H=1\" \"HAVE_STDINT_H=1\" \"HAVE_SYS_STAT_H=1\" \"HAVE_SYS_TYPES_H=1\" \"HAVE_ZLIB_H=1\" \"BACKTRACE_HEADER=execinfo.h\" \"HAVE_GETCWD=1\" \"HAVE_INT64_T=1\" \"HAVE_STRERROR=1\" \"HAVE_STRTOLL=1\" \"HAVE_SYSCONF=1\" \"HAVE_UINT64_T=1\" \"HAVE__CHSIZE_S=1\" \"HAVE___CHKSTK=1\" \"stricmp=_stricmp\" \"strdup=_strdup\" \"LTDL_SHLIB_EXT=.dll\"< external/llvm-project/llvm/include/llvm/Config/abi-breaking.h.cmake > bazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include/llvm/Config/abi-breaking.h\r\nExecution platform: @local_execution_config_platform//:platform\r\n/usr/bin/bash: line 1: bazel-out/x64_windows-opt/bin/external/org_tensorflow/third_party/llvm/expand_cmake_vars.exe: Bad address\r\nTarget //build:build_wheel failed to build\r\nINFO: Elapsed time: 23.877s, Critical Path: 4.59s\r\nINFO: 27 processes: 15 internal, 12 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\nb''\r\nTraceback (most recent call last):\r\n  File \".\\build\\build.py\", line 603, in <module>\r\n    main()\r\n  File \".\\build\\build.py\", line 598, in main\r\n    shell(command)\r\n  File \".\\build\\build.py\", line 52, in shell\r\n    output = subprocess.check_output(cmd)\r\n  File \"C:\\Users\\cloud\\Miniconda3\\envs\\py38\\lib\\subprocess.py\", line 415, in check_output\r\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n  File \"C:\\Users\\cloud\\Miniconda3\\envs\\py38\\lib\\subprocess.py\", line 516, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['C:\\\\tools\\\\bazelisk\\\\bazel.EXE', 'run', '--verbose_failures=true', '--config=short_logs', '--config=mkl_open_source_only', '--config=cuda', '--define=xla_python_enable_gpu=true', ':build_wheel', '--', '--output_path=D:\\\\jax\\\\dist', '--cpu=AMD64']' returned non-zero exit status 1.\r\n```\r\n\r\n**other info**\r\n#42819, #46118, #41850 and #39680 should all be due to the same reason.", "comments": ["To reproduce it will be tricky, but extreme simple.\r\n\r\n*I use powershell in following part, just a heads-up*\r\n\r\n**TLDR**\r\n\r\n`$env:TF_CUDA_PATHS = \"C:/CUDA/v10.1,C:/CUDA/v10.1\"` then `python configure.py` then  `bazel build //tensorflow/core/util:version_info_gen`. **Bad address error**\r\n\r\n`$env:TF_CUDA_PATHS = \"C:/CUDA/v10.1\"` then `...` then  `...`. **OK**\r\n\r\n`$env:TF_CUDA_PATHS = \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\"` then `...` then  `...`. **OK**\r\n\r\n**Conclusion**\r\n\r\nIt is somewhat a coupled issue on **non-standard CUDA path** and **multiple items** in `TF_CUDA_PATHS`\r\n\r\n\r\n### Detailed logs\r\n\r\n<details><summary>repo status</summary><p>\r\n\r\n```\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 master\r\n> git status\r\nOn branch master\r\nYour branch is up to date with 'origin/master'.\r\n\r\nnothing to commit, working tree clean\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 master\r\n> git checkout 8cc3ffa8d8e4dd659c1534849cf5984ef4ec3532\r\nUpdating files: 100% (4040/4040), done.\r\nNote: switching to '8cc3ffa8d8e4dd659c1534849cf5984ef4ec3532'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by switching back to a branch.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -c with the switch command. Example:\r\n\r\n  git switch -c <new-branch-name>\r\n\r\nOr undo this operation with:\r\n\r\n  git switch -\r\n\r\nTurn off this advice by setting config variable advice.detachedHead to false\r\n\r\nHEAD is now at 8cc3ffa8d8e [PJRT:TFRT:CPU] Fix undefined behavior in JAX TFRT CPU backend.\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> git status\r\nHEAD detached at 8cc3ffa8d8e\r\nnothing to commit, working tree clean\r\n```\r\n\r\n</p></details>\r\n\r\n<details><summary>first try, failed</summary><p>\r\n\r\n```\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> $env:TF_CUDA_PATHS = \"C:/CUDA/v10.1,C:/CUDA/v10.1\"\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> python .\\configure.py\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\cloud\\Miniconda3\\envs\\py39\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\cloud\\Miniconda3\\envs\\py39\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\cloud\\Miniconda3\\envs\\py39\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/CUDA/v10.1/lib/x64\r\n    C:/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/CUDA/v10.1/lib/x64\r\n    C:/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> bazel build //tensorflow/core/util:version_info_gen\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=119\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/cloud/Miniconda3/envs/py39/python.exe\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/cloud/Miniconda3/envs/py39/python.exe --action_env PYTHON_LIB_PATH=C:/Users/cloud/Miniconda3/envs/py39/lib/site-packages --python_path=C:/Users/cloud/Miniconda3/envs/py39/python.exe --action_env TF_CUDA_PATHS=C:/CUDA/v10.1,C:/CUDA/v10.1 --action_env CUDA_TOOLKIT_PATH=C:/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file d:\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file d:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file d:\\tensorflow\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:windows in file d:\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Build option --action_env has changed, discarding analysis cache.\r\nINFO: Analyzed target //tensorflow/core/util:version_info_gen (32 packages loaded, 169 targets configured).\r\nINFO: Found 1 target...\r\nERROR: D:/tensorflow/tensorflow/core/util/BUILD:376:24: error executing shell command: 'C:/msys64/usr/bin/bash.exe -c bazel-out/x64_windows-opt-ST-301534fa764b/bin/tensorflow/tools/git/gen_git_source.exe --generate \"$@\" --git_tag_override=${GIT_TAG_OVERRIDE:-}  external/local_config_g...' failed (Exit 126): bash.exe failed: error executing command\r\n  cd C:/users/cloud/_bazel_cloud/26orbg4z/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/CUDA/v10.1\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\windows;C:\\windows\\System32;C:\\windows\\System32\\WindowsPowerShell\\v1.0;C:\\Users\\cloud\\AppData\\Local\\bazelisk\\downloads\\bazelbuild\\bazel-3.7.2-windows-x86_64\\bin;C:\\msys64\\usr\\bin;C:\\Users\\cloud\\Miniconda3\\envs\\py39;C:\\Users\\cloud\\Miniconda3\\envs\\py39\\Library\\mingw-w64\\bin;C:\\Users\\cloud\\Miniconda3\\envs\\py39\\Library\\usr\\bin;C:\\Users\\cloud\\Miniconda3\\envs\\py39\\Library\\bin;C:\\Users\\cloud\\Miniconda3\\envs\\py39\\Scripts;C:\\Users\\cloud\\Miniconda3\\envs\\py39\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30037\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\Tools;C:\\Program Files\\WindowsApps\\Microsoft.PowerShell_7.1.3.0_x64__8wekyb3d8bbwe;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp;.;C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\wbin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\libnvvp;C:\\Program Files\\Common Files\\Microsoft Shared\\Microsoft Online Services;C:\\Program Files (x86)\\Common Files\\Microsoft Shared\\Microsoft Online Services;C:\\windows\\system32;C:\\windows;C:\\windows\\System32\\Wbem;C:\\windows\\System32\\WindowsPowerShell\\v1.0;C:\\windows\\System32\\OpenSSH;C:\\Program Files\\dotnet;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn;C:\\Program Files\\Git\\cmd;C:\\tools\\vcpkg;C:\\tools\\bazelisk;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files (x86)\\dotnet;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.4.0;C:\\Users\\cloud\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\cloud\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\cloud\\.dotnet\\tools;C:\\Users\\cloud\\AppData\\Local\\Pandoc;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\r\n    SET PYTHON_BIN_PATH=C:/Users/cloud/Miniconda3/envs/py39/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/cloud/Miniconda3/envs/py39/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0\r\n    SET TF_CUDA_PATHS=C:/CUDA/v10.1,C:/CUDA/v10.1\r\n  C:/msys64/usr/bin/bash.exe -c bazel-out/x64_windows-opt-ST-301534fa764b/bin/tensorflow/tools/git/gen_git_source.exe --generate \"$@\" --git_tag_override=${GIT_TAG_OVERRIDE:-}  external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref bazel-out/x64_windows-opt/bin/tensorflow/core/util/version_info.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\n: line 1: bazel-out/x64_windows-opt-ST-301534fa764b/bin/tensorflow/tools/git/gen_git_source.exe: Bad address\r\nTarget //tensorflow/core/util:version_info_gen failed to build\r\nINFO: Elapsed time: 13.190s, Critical Path: 0.58s\r\nINFO: 4 processes: 3 internal, 1 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n</p></details>\r\n\r\n<details><summary>second try, good</summary><p>\r\n\r\n```\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> $env:TF_CUDA_PATHS = \"C:/CUDA/v10.1\"\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> python .\\configure.py\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\cloud\\Miniconda3\\envs\\py39\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\cloud\\Miniconda3\\envs\\py39\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\cloud\\Miniconda3\\envs\\py39\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/CUDA/v10.1/lib/x64\r\n    C:/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/CUDA/v10.1/lib/x64\r\n    C:/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> bazel build //tensorflow/core/util:version_info_gen\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=119\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/cloud/Miniconda3/envs/py39/python.exe\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/cloud/Miniconda3/envs/py39/python.exe --action_env PYTHON_LIB_PATH=C:/Users/cloud/Miniconda3/envs/py39/lib/site-packages --python_path=C:/Users/cloud/Miniconda3/envs/py39/python.exe --action_env TF_CUDA_PATHS=C:/CUDA/v10.1 --action_env CUDA_TOOLKIT_PATH=C:/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file d:\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file d:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file d:\\tensorflow\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:windows in file d:\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Build option --action_env has changed, discarding analysis cache.\r\nINFO: Analyzed target //tensorflow/core/util:version_info_gen (32 packages loaded, 169 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/core/util:version_info_gen up-to-date:\r\n  bazel-bin/tensorflow/core/util/version_info.cc\r\nINFO: Elapsed time: 9.264s, Critical Path: 1.66s\r\nINFO: 4 processes: 2 internal, 2 local.\r\nINFO: Build completed successfully, 4 total actions\r\n```\r\n\r\n</p></details>\r\n\r\n<details><summary>third try, good</summary><p>\r\n\r\n```\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> $env:TF_CUDA_PATHS = \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\"\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> python .\\configure.py\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\cloud\\Miniconda3\\envs\\py39\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\cloud\\Miniconda3\\envs\\py39\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\cloud\\Miniconda3\\envs\\py39\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n(py39) cloud@PC PS D:\\tensorflow \ue0a0 (8cc3ffa...)\r\n> bazel build //tensorflow/core/util:version_info_gen\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=119\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/cloud/Miniconda3/envs/py39/python.exe\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/cloud/Miniconda3/envs/py39/python.exe --action_env PYTHON_LIB_PATH=C:/Users/cloud/Miniconda3/envs/py39/lib/site-packages --python_path=C:/Users/cloud/Miniconda3/envs/py39/python.exe --action_env TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file d:\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file d:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file d:\\tensorflow\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:windows in file d:\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Build option --action_env has changed, discarding analysis cache.\r\nINFO: Analyzed target //tensorflow/core/util:version_info_gen (32 packages loaded, 169 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/core/util:version_info_gen up-to-date:\r\n  bazel-bin/tensorflow/core/util/version_info.cc\r\nINFO: Elapsed time: 7.428s, Critical Path: 1.61s\r\nINFO: 4 processes: 2 internal, 2 local.\r\nINFO: Build completed successfully, 4 total actions\r\n```\r\n\r\n</p></details>", "@cloudhan Could you please take a look at the [link ](https://www.tensorflow.org/install/source_windows)( tested build configuration) for reference and let us know if it helps ?Thank you! ", "It doesn't help.", "I'd like to add some more information:\r\n\r\nconfigure normally, use `--action_env` to reproduce\r\n\r\n`C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1` exists\r\n`D:/CUDA/v10.1` exists as a copy of preview path\r\n`M:/CUDA/v10.1` does not exist.\r\n\r\n```pwsh\r\n# bad, prefix a path will result in bad address\r\nbazel build //tensorflow/core/util:version_info_gen --action_env TF_CUDA_PATHS='M:/CUDA/v10.1,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1' #bad\r\n\r\n# good, postfix a path has no effect.\r\nbazel build //tensorflow/core/util:version_info_gen --action_env TF_CUDA_PATHS='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,M:/CUDA/v10.1'\r\n\r\n# bad, notice no space in CUDA\r\nbazel build --subcommands //tensorflow/core/util:version_info_gen --action_env TF_CUDA_PATHS='D:/CUDA/v10.1,D:/CUDA/v10.1'\r\n\r\n#good, notice the space in CU DA\r\nbazel build --subcommands //tensorflow/core/util:version_info_gen --action_env TF_CUDA_PATHS='D:/CU DA/v10.1,D:/CUDA/v10.1'\r\n```\r\n\r\nSeems the space character in the `TF_CUDA_PATHS` is specially handled. But I still cannot guess why it will result a bad address error.\r\n\r\n@sanjoy \r\n"]}, {"number": 50992, "title": "reduce_sum in tflite outputs is incorrect", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nkeras = tf.keras\r\nlayers = keras.layers\r\n\r\n\r\ndef infer(tflite_model, img):\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()[0]\r\n    interpreter.set_tensor(input_details[\"index\"], img)\r\n    interpreter.invoke()\r\n    output_details = interpreter.get_output_details()[0]\r\n    output = interpreter.get_tensor(output_details[\"index\"])\r\n    return output\r\n\r\ndef convert_to_tflite(model):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.experimental_new_converter = True\r\n    tflite_model = converter.convert()\r\n    return tflite_model\r\n\r\n\r\n# Image to test\r\nnp.random.seed(1)\r\ninput_shape = (12, 14, 18)\r\nimg = np.random.random((1,)+input_shape).astype(np.float32) * 7.0 - 3.5\r\n\r\n# Create a model\r\ni = layers.Input(shape=input_shape)\r\nx = tf.quantization.fake_quant_with_min_max_args(i, min=0.0, max=3.984375)\r\nx = tf.reduce_sum(x, axis=None)\r\nx = tf.quantization.fake_quant_with_min_max_args(x, min=-4.0, max=3.96875)\r\nmodel = keras.models.Model(inputs=i, outputs=x)\r\n\r\n\r\ntf_output = model.predict(img)\r\ntflite_model = convert_to_tflite(model)\r\ntflite_output = infer(tflite_model, img)\r\n\r\nprint(f'Tensorflow output: {tf_output}, TFLite output: {tflite_output}, ')\r\n\r\n```\r\n\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\nI get a huge difference between the TF model and the TFLite model:\r\nTensorflow output: 3.96875, TFLite output: -2.78125, \r\n\r\nThe Tensorflow model gives completely different results than the TFLite model. Furthermore, before the reduce_sum, I quantize the tensor using only positive values, so it does not make sense that the output of the model is negative (which is what I get).\r\n\r\n", "comments": ["@daverim Could you take a look? ", "Hi, the reason why this happens is because tflite assumes that min/max ranges have been calculated correctly. If you change your example to `x = tf.quantization.fake_quant_with_min_max_args(x, min=0.0, max=781756.5)`, you'll see the two answers converge. In TF case, the reason why it works (more) correctly is that quant/dequant is entirely in float and uses min, max and rounding operations, whereas TFlite generally will avoid operations where possible to optimize latency. In TFLite's reduce sum kernel, there is a cast from float->int for latency reasons (assuming that the scales are correct) vs rounding. The casting of wrong values can lead to negatives. ", "Hi,\r\nThanks for the answer!\r\nBut I'm not sure I fully got it... Could you please refer me to where this casting/assumption on correct min/max ranges is in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/reduce.cc? \r\n\r\nThanks again :)", "Hi @daverim,\r\nWhen I remove the second fake-quant layer from the model, I get similar results.\r\nDoes the assumption on correct min/max ranges hold for all fake-quant layers in the model or just the one that quantizes the input?\r\nAnd if so (it holds for all fake-quant layers), what happens if I trained my model poorly so the ranges are not similar to the actual ranges the model observes during inference? Does that mean that in this case, the layer's output doesn't follow the equations in https://www.tensorflow.org/api_docs/python/tf/quantization/fake_quant_with_min_max_args?\r\n\r\nAlso, does that mean that fake-quant layers should not be added manually to the model?\r\n\r\nThanks!", "Hi,\r\nI tried to use the same example but with different ranges, and I also get a huge difference between the TF output and TFLite output. The code is below, and the output is: \"Tensorflow output: 4.0, TFLite output: 1.6627452373504639\"\r\nThis time, the min/max of the first fake quant is matching the input distribution.\r\nThe second fake quant layer should only clip the single positive value to the maximum of the second fake quant, but I get 1.66 instead.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nkeras = tf.keras\r\nlayers = keras.layers\r\n\r\n\r\ndef infer(tflite_model, img):\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()[0]\r\n    interpreter.set_tensor(input_details[\"index\"], img)\r\n    interpreter.invoke()\r\n    output_details = interpreter.get_output_details()[0]\r\n    output = interpreter.get_tensor(output_details[\"index\"])\r\n    return output\r\n\r\ndef convert_to_tflite(model):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.experimental_new_converter = True\r\n    tflite_model = converter.convert()\r\n    return tflite_model\r\n\r\n\r\n# Image to test\r\nnp.random.seed(1)\r\ninput_shape = (12, 14, 18)\r\nimg = np.random.random((1,)+input_shape).astype(np.float32) * 7.0 - 3.5\r\nimg = np.clip(img, 0, 3)\r\n\r\n# Create a model\r\ni = layers.Input(shape=input_shape)\r\nx = tf.quantization.fake_quant_with_min_max_args(i, min=0, max=4)\r\nx = tf.reduce_sum(x, axis=None)\r\nx = tf.quantization.fake_quant_with_min_max_args(x, min=0, max=4)\r\nmodel = keras.models.Model(inputs=i, outputs=x)\r\n\r\nprint(np.max(img), np.min(img))\r\ntf_output = model.predict(img)\r\ntflite_model = convert_to_tflite(model)\r\ntflite_output = infer(tflite_model, img)\r\n\r\nprint(f'Tensorflow output: {tf_output}, TFLite output: {tflite_output} ')\r\n```"]}, {"number": 50989, "title": "Failed when upgrading grpc to 1.36.0", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):  2.4.2\r\n- Are you willing to contribute it (Yes/No): Yes, but no idea how get it work.\r\n\r\n\r\nI try to upgrade grpc to 1.36 to use feature `grpc::experimental::TlsServerAuthorizationCheckConfig>` for client-side custom certificate verification.  Updates of workload.bzl: \r\n```\r\n+++ b/tensorflow/workspace.bzl\r\n@@ -650,8 +650,8 @@ def tf_repositories(path_prefix = \"\", tf_repo_name = \"\"):\r\n     # WARNING: make sure ncteisen@ and vpai@ are cc-ed on any CL to change the below rule\r\n     tf_http_archive(\r\n         name = \"com_github_grpc_grpc\",\r\n-        sha256 = \"b956598d8cbe168b5ee717b5dafa56563eb5201a947856a6688bbeac9cac4e1f\",\r\n-        strip_prefix = \"grpc-b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd\",\r\n+        sha256 = \"019822cb6f1a339658fc620a54af9b710c39b540ae90e14c3babd6b16fb45b0f\",\r\n+        strip_prefix = \"grpc-736e3758351ced3cd842bad3ba4e2540f01bbc48\",\r\n         system_build_file = clean_dep(\"//third_party/systemlibs:grpc.BUILD\"),\r\n         patch_file = clean_dep(\"//third_party/grpc:generate_cc_env_fix.patch\"),\r\n         system_link_files = {\r\n@@ -664,8 +664,8 @@ def tf_repositories(path_prefix = \"\", tf_repo_name = \"\"):\r\n             \"//third_party/systemlibs:grpc.bazel.protobuf.bzl\": \"bazel/protobuf.bzl\",\r\n         },\r\n         urls = [\r\n-            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/grpc/grpc/archive/b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd.tar.gz\",\r\n-            \"https://github.com/grpc/grpc/archive/b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd.tar.gz\",\r\n+            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/grpc/grpc/archive/736e3758351ced3cd842bad3ba4e2540f01bbc48.tar.gz\",\r\n+            \"https://github.com/grpc/grpc/archive/736e3758351ced3cd842bad3ba4e2540f01bbc48.tar.gz\",\r\n```\r\n\r\nbut got errors as below:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/b2ed5105b9fe9c49ec55af56657398dc/external/upb/bazel/upb_proto_library.bzl:257:29: aspect() got unexpected keyword argument 'incompatible_use_toolchain_transition'\r\nERROR: /tf/src/tensorflow/tools/pip_package/BUILD:175:1: error loading package '@com_github_grpc_grpc//': in /root/.cache/bazel/_bazel_root/b2ed5105b9fe9c49ec55af56657398dc/external/com_github_grpc_grpc/bazel/grpc_build_system.bzl: Extension file 'bazel/upb_proto_library.bzl' has errors and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loading package '@com_github_grpc_grpc//': in /root/.cache/bazel/_bazel_root/b2ed5105b9fe9c49ec55af56657398dc/external/com_github_grpc_grpc/bazel/grpc_build_system.bzl: Extension file 'bazel/upb_proto_library.bzl' has errors\r\nINFO: Elapsed time: 1.187s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (29 packages loaded, 12 targets configured)\r\n    currently loading: @com_google_protobuf// ... (2 packages)\r\n    Fetching @rules_java; fetching\r\n    Fetching @go_sdk; fetching\r\n```\r\nCould you give me some tips for debugging and upgrade?\r\n\r\nThanks!\r\n", "comments": ["hi, @saikumarchalla , would you plz tell me the right way to upgrade GRPC? ", "I think grpc, protobuf and upb all need to be updated in sync", "I have tried with more recent versions of grpc but it looks like they are not available in the google/tensorflow archive ", "Bazel should fetch from upstream if TF mirror does not contain the package.", "thank I will try sometime latter "]}, {"number": 50965, "title": "Tensorflow Lite Model Maker cannot train EfficientDet-Lite using Cloud TPU", "body": "**System information**\r\n- Have I written custom code: minimally modified stock example\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab Pro w/TPU\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5\r\n\r\n**Describe the current behavior**\r\n\r\nAttempting to use a CloudTPU for training results in the following error:\r\n```\r\nINFO:absl:Using /tmp/tfhub_modules to cache modules.\r\nINFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/efficientdet/lite0/feature-vector/1'.\r\nINFO:absl:Downloaded https://tfhub.dev/tensorflow/efficientdet/lite0/feature-vector/1, Total size: 29.61MB\r\nINFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/efficientdet/lite0/feature-vector/1'.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-9-fb7d2e23be2c> in <module>()\r\n----> 1 model = object_detector.create(train_data, model_spec=spec, batch_size=8, epochs=4, train_whole_model=True, validation_data=validation_data)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in shape(self)\r\n   1196         # `_tensor_shape` is declared and defined in the definition of\r\n   1197         # `EagerTensor`, in C.\r\n-> 1198         self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())\r\n   1199       except core._NotOkStatusException as e:\r\n   1200         six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n\r\nInvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /tmp/tfhub_modules/0771ebbebbfa831cd20c20680920ec4ad9deb2bd/variables/variables: Unimplemented: File system scheme '[local]' not implemented (file: '/tmp/tfhub_modules/0771ebbebbfa831cd20c20680920ec4ad9deb2bd/variables/variables')\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee: https://gist.github.com/tve/615f4b51fa88dc643358176c86d6537e\r\nUse the \"open in colab\" button and run it to get the error (or just read the results in the gist).\r\nThe original tutorial is https://www.tensorflow.org/lite/tutorials/model_maker_object_detection\r\nI added:\r\n- `tpu = tf.distribute.cluster_resolver.TPUClusterResolver()`\r\n\r\nand I changed:\r\n- from `spec = model_spec.get('efficientdet_lite0')`\r\n- to `spec = object_detector_spec.efficientdet_lite0_spec(strategy='tpu', tpu=tpu.master(), debug=True)`\r\n\r\nIt's quite possible that I'm missing some magic, but I cannot find any tutorial or example that shows the use of model maker to train an object detection model using cloud TPU...", "comments": ["The problem I'm having appears to be related to https://github.com/tensorflow/hub/issues/604, but I haven't confirmed that yet."]}, {"number": 50957, "title": "resize_tensor_input does not change shape in tflite interpreter for efficientdet models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 LTS\r\n- TensorFlow installed from (source or binary): conda tf\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.8\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to modify the batch size of a tflite efficientdet model following this [official tf guide](https://colab.research.google.com/github/frogermcs/TFLite-Tester/blob/master/notebooks/Testing_TFLite_model.ipynb#scrollTo=VXX8lJkPRYFq)\r\nHowever I cannot modify the output shapes.\r\n\r\nI download the model from the [TF hub](https://tfhub.dev/tensorflow/lite-model/efficientdet/lite2/detection/default/1) and load them into `model_path`.\r\n\r\n```python\r\ntflite_interpreter = tf.lite.Interpreter(model_path, num_threads=1)\r\ninput_details = tflite_interpreter.get_input_details()\r\noutput_details = tflite_interpreter.get_output_details()\r\n```\r\n\r\nThen just to be sure that everything works fine, I set `batch_size=1` which is the default:\r\n```python\r\nbatch_size = 1\r\n# tflite_interpreter.resize_tensor_input(output_details[0]['index'], (batch_size, 25, 4,))\r\n# tflite_interpreter.resize_tensor_input(output_details[1]['index'], (batch_size, 25,))\r\n# tflite_interpreter.resize_tensor_input(output_details[2]['index'], (batch_size, 25,))\r\n# tflite_interpreter.resize_tensor_input(output_details[3]['index'], (batch_size,))\r\ntflite_interpreter.resize_tensor_input(input_details[0]['index'], (batch_size, model_res, model_res, 3))\r\ntflite_interpreter.allocate_tensors()\r\n\r\n# print(input_details)\r\nfor od in tflite_interpreter.get_input_details():\r\n    print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in od.items()))\r\n    print('___')\r\n# output details\r\nfor od in tflite_interpreter.get_output_details():\r\n    print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in od.items()))\r\n    print('___')\r\n```\r\nI obtain the following output\r\n```\r\nname\tserving_default_images:0\r\nindex\t0\r\nshape\t[  1 448 448   3]\r\nshape_signature\t[  1 448 448   3]\r\ndtype\t<class 'numpy.uint8'>\r\nquantization\t(0.0078125, 127)\r\nquantization_parameters\t{'scales': array([0.0078125], dtype=float32), 'zero_points': array([127], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\nname\tStatefulPartitionedCall:3\r\nindex\t782\r\nshape\t[ 1 25  4]\r\nshape_signature\t[ 1 25  4]\r\ndtype\t<class 'numpy.float32'>\r\nquantization\t(0.0, 0)\r\nquantization_parameters\t{'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\nname\tStatefulPartitionedCall:2\r\nindex\t783\r\nshape\t[ 1 25]\r\nshape_signature\t[ 1 25]\r\ndtype\t<class 'numpy.float32'>\r\nquantization\t(0.0, 0)\r\nquantization_parameters\t{'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\nname\tStatefulPartitionedCall:1\r\nindex\t784\r\nshape\t[ 1 25]\r\nshape_signature\t[ 1 25]\r\ndtype\t<class 'numpy.float32'>\r\nquantization\t(0.0, 0)\r\nquantization_parameters\t{'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\nname\tStatefulPartitionedCall:0\r\nindex\t785\r\nshape\t[1]\r\nshape_signature\t[1]\r\ndtype\t<class 'numpy.float32'>\r\nquantization\t(0.0, 0)\r\nquantization_parameters\t{'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\n```\r\nso far so good... I kwow that tflite efficient det produces a max of 25 outputs.\r\n\r\nNow I try to change the batch size. The input shape changes (now batch size is 4) but nothing changes for the output details.\r\n\r\n```python\r\nbatch_size = 4\r\n\r\ntflite_interpreter = tf.lite.Interpreter(model_path, num_threads=1)\r\ntflite_interpreter.resize_tensor_input(input_details[0]['index'], (batch_size, model_res, model_res, 3))\r\ntflite_interpreter.resize_tensor_input(output_details[0]['index'], (batch_size, 25, 4,))\r\ntflite_interpreter.resize_tensor_input(output_details[1]['index'], (batch_size, 25,))\r\ntflite_interpreter.resize_tensor_input(output_details[2]['index'], (batch_size, 25,))\r\ntflite_interpreter.resize_tensor_input(output_details[3]['index'], (batch_size,))\r\ntflite_interpreter.allocate_tensors()\r\n\r\n# print(input_details)\r\nfor od in tflite_interpreter.get_input_details():\r\n    print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in od.items()))\r\n    print('___')\r\n# output details\r\nfor od in tflite_interpreter.get_output_details():\r\n    print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in od.items()))\r\n    print('___')\r\n```\r\n```\r\nname\tserving_default_images:0\r\nindex\t0\r\nshape\t[  4 448 448   3]\r\nshape_signature\t[  4 448 448   3]\r\ndtype\t<class 'numpy.uint8'>\r\nquantization\t(0.0078125, 127)\r\nquantization_parameters\t{'scales': array([0.0078125], dtype=float32), 'zero_points': array([127], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\nname\tStatefulPartitionedCall:3\r\nindex\t782\r\nshape\t[ 1 25  4]\r\nshape_signature\t[ 1 25  4]\r\ndtype\t<class 'numpy.float32'>\r\nquantization\t(0.0, 0)\r\nquantization_parameters\t{'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\nname\tStatefulPartitionedCall:2\r\nindex\t783\r\nshape\t[ 1 25]\r\nshape_signature\t[ 1 25]\r\ndtype\t<class 'numpy.float32'>\r\nquantization\t(0.0, 0)\r\nquantization_parameters\t{'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\nname\tStatefulPartitionedCall:1\r\nindex\t784\r\nshape\t[ 1 25]\r\nshape_signature\t[ 1 25]\r\ndtype\t<class 'numpy.float32'>\r\nquantization\t(0.0, 0)\r\nquantization_parameters\t{'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\nname\tStatefulPartitionedCall:0\r\nindex\t785\r\nshape\t[1]\r\nshape_signature\t[1]\r\ndtype\t<class 'numpy.float32'>\r\nquantization\t(0.0, 0)\r\nquantization_parameters\t{'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}\r\nsparsity_parameters\t{}\r\n___\r\n```\r\n\r\nI tried this approach with different versions of the efficientdet model (0, 1, 2, 3, 4) changing their respectively  input resolutions accordingly, and still have the same problem.\r\n\r\n**Describe the expected behavior**\r\nI expect that the output shape is 4 as I set.\r\n\r\n", "comments": ["TFLite uses a custom implementation of Non Max Suppression, which is the postprocessing step in EfficientDet. Currently, it does not support batch size > 1.\r\n\r\nWe should probably take a few steps to address this:\r\n1. Return an error during allocate_tensors() (from the kernel's Prepare method) if the input tensors have batch size > 1. (Right now it will fail at inference time, but there is no Prepare-time check).\r\n2. Possibly add support to the kernel for batch size > 1.\r\n\r\n@ziyeqinghan Do you know if the rest of the EfficientDet graph supports batch size > 1? In that case the only change we need is on the NMS kernel.\r\n\r\n@ivanlen Unfortunately the only solution until we implement this, is to run per-image inference instead of batched. (On device, there may not be a huge difference between the two - but thats just a hunch...feel free to run experiments and report back if per-image inference is very slow)", "@srjoglekar246 thanks for the clarification.\r\n\r\nActually, per-image inference is very slow. That is why I was trying to speed up the process by using a batch size > 1.\r\nDo you have any suggestion on how to speed up the inference times?\r\n\r\nCheers.", "Are you trying this on a desktop only, or do you want to deploy to mobile phones? THe solution varies based on device & API you use :-)", "Both actually.\r\nFirst I want to try different efficiendet models: make some inference with a test set and get some metrics.\r\nThen, I would like to deploy the best model into mobile phones. On mobile I am going to run with a batch size=1, but on desktop I would like to test as fast as I can.\r\n\r\nCurrently inferencing with batch size = 1 takes like 4/5 seconds per image which is really slow.", "Yeah, TFLite kernels aren't optimized for desktop inference, so they can be slower than the corresponding TF versions. A few ideas for desktop:\r\n\r\n1. Try running the Interpreter with num_threads = 2 or 4. That *might* improve latency depending on your device, but its not guaranteed.\r\n2. Since the EfficientDet model itself is small, you can just create multiple Interpreters & run them parallely in threads: TFLite RAM overhead typically isn't that high since we reuse tensor memory whenever possible.\r\n\r\nOn device, you can use a [delegate](https://www.tensorflow.org/lite/performance/delegates) that will significantly reduce your inference time. You can try our [benchmark tool](https://www.tensorflow.org/lite/performance/measurement) with [delegate-specific parameters](https://www.tensorflow.org/lite/performance/delegates#task-agnostic_evaluation) to figure out what works best.", "Thanks for your feedback.\r\n\r\n1. I tried to run with num_threads>1 but to be honest I didn't find an overall improvement, still the inference is quite slow for a test set of 2k images. \r\n2. I will try this option.\r\n\r\nSince num_threads > 1 doesn't scale very good (at least in my case) I think that option 2 probably is the most suitable for my case.\r\n\r\nI will investigate on the delegate, thanks.\r\nRegarding the benchmark tool, I already used it (without trying delegate specific parameters, just with the default config). It's funny that on mobile devices the inference time in one thread is about 1 seconds per image while in my computer i8 CPU it takes about 4/5 seconds per images. But as you said, I guess that this has to be that they are optimized for mobile devices.\r\n\r\nThanks again for the feedback.\r\n"]}, {"number": 50953, "title": "Identifying the type of input ", "body": "\r\n\r\nCreating a generalized application that can accept both sequence inputs and non-sequence inputs.\r\nTo check whether its sequence input or non-sequence input, have written c++ API to get this information.\r\n\r\nNeed confirmation regarding checking whether it's RNN input or not? \r\n`    int t_size1 = this->interpreter->tensors_size();\r\n    for (int i = 0; i < t_size1; i++) {\r\n        if (this->interpreter->tensor(i)->name != nullptr) {\r\n            if ((std::string(this->interpreter->tensor(i)->name)\r\n                     .compare(\"tfl.unidirectional_sequence_lstm\") == 0) ||\r\n                (std::string(this->interpreter->tensor(i)->name).compare(\"tfl.lstm\") == 0) ||\r\n                (std::string(this->interpreter->tensor(i)->name).compare(\"tfl.basic_lstm\") == 0) ||\r\n                (std::string(this->interpreter->tensor(i)->name)\r\n                     .compare(\"tfl.unidirectional_sequence_rnn \") == 0)) {\r\n                *isRNNType = true;\r\n                break;\r\n            }\r\n        }\r\n    }`", "comments": ["Using TensorFlow 2.4.1, Does the above-written code valid for all kind of models and differentiate RNN and non RNN models ?\r\nDoes this work for all type of Models FP32, Quant(uint8, int8 and Fp16)\r\nIs there an API to check if its sequence input or non-sequence inputs? If there, provide the API .", "Sorry I'm confused about the question, what problem did you get?", "I need to get information on whether the Model accepts sequence input (RNN) or non-sequence input.\r\n\r\nI have a TFLite Model. I want to detect whether the model is sequence type or not.\r\n\r\nFor that I have written code to identify, If its non-sequence, N,H,W,C is the data format. If its sequence, N, S,C is the order .\r\n\r\nIs there an API to identify the type of model"]}, {"number": 50952, "title": "Unexpected warning when using tf.data.Dataset.cache(filename)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0 and 2.7.0.dev20210725\r\n- Python version: 3.9.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv1.12.1-60944-g65153d2677e 2.7.0-dev20210725\r\n\r\n**Describe the current behavior**\r\nWhen tf.data.Dataset is cached into a file there is a warning\r\n```\r\nW tensorflow/core/kernels/data/cache_dataset_ops.cc:233] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n```\r\nEventhough dataset is fully read.\r\n\r\n**Describe the expected behavior**\r\nThere should be no warning when dataset is fully read.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\nos.system('rm -f /tmp/foo*')\r\n\r\ndef gen():\r\n    for i in range(5):\r\n        yield i\r\n\r\ndataset = tf.data.Dataset.from_generator(gen, output_signature=tf.TensorSpec(shape=(), dtype=tf.int32))\r\ndataset = dataset.cache('/tmp/foo')\r\n# dataset = dataset.take(3).cache('/tmp/foo').repeat(2)\r\n\r\nfor x in dataset:\r\n    print(x)\r\n```\r\n\r\nEven if `take.cache.repeat` is used just like the warning suggests, it doesn't help\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nOutput of the script above:\r\n```\r\ntf.Tensor(0, shape=(), dtype=int32)\r\ntf.Tensor(1, shape=(), dtype=int32)\r\ntf.Tensor(2, shape=(), dtype=int32)\r\ntf.Tensor(3, shape=(), dtype=int32)\r\ntf.Tensor(4, shape=(), dtype=int32)\r\n2021-07-26 13:35:10.940312: W tensorflow/core/kernels/data/cache_dataset_ops.cc:233] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n```\r\n\r\nRunning this script under `strace` shows that a shard file is removed just before it's existence is checked:\r\n```\r\n[pid   910] stat(\"/tmp/foo_0.index\", {st_mode=S_IFREG|0644, st_size=201, ...}) = 0\r\n[pid   910] openat(AT_FDCWD, \"/tmp/foo_0.index\", O_RDONLY) = 4\r\n[pid   910] pread64(4, \"y\\10\\206\\1\\16\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 48, 153) = 48\r\n[pid   910] pread64(4, \"\\0\\1\\2!\\0t\\0\\0\\0\\0\\1\\0\\0\\0\\0t\\214P\\20\", 19, 134) = 19\r\n[pid   910] pread64(4, \"\\0\\0\\6\\10\\1\\32\\2\\10\\1\\0\\t\\v      0_0\\10\\3\\22\\0(\\0045\\246{\\21:\"..., 121, 0) = 121\r\n[pid   910] close(4)                    = 0\r\n[pid   910] rename(\"/tmp/foo_0.data-00000-of-00001\", \"/tmp/foo.data-00000-of-00001\") = 0\r\n[pid   910] openat(AT_FDCWD, \"/tmp/foo.index\", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 4\r\n[pid   910] fstat(4, {st_mode=S_IFREG|0644, st_size=0, ...}) = 0\r\n[pid   910] write(4, \"\\0\\0\\6\\10\\1\\32\\2\\10\\1\\0\\t\\v      0_0\\10\\3\\22\\0(\\0045\\246{\\21:\"..., 201) = 201\r\n[pid   910] close(4)                    = 0\r\n[pid   910] unlink(\"/tmp/foo_0.index\")  = 0\r\n[pid   910] unlink(\"/tmp/foo_0.lockfile\") = 0\r\n[pid   910] access(\"/tmp/foo.index\", F_OK) = 0\r\n[pid   910] access(\"/tmp/foo_0.index\", F_OK) = -1 ENOENT (No such file or directory)\r\n[pid   910] stat(\"/etc/localtime\", {st_mode=S_IFREG|0644, st_size=127, ...}) = 0\r\n[pid   910] write(2, \"2021-07-26 13:17:56.497443: W te\"..., 4242021-07-26 13:17:56.497443: W tensorflow/core/kernels/data/cache_dataset_ops.cc:233] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n) = 424\r\n```", "comments": ["@0x0badc0de ,\r\n\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/44801#issuecomment-727712745) and [link](https://stackoverflow.com/questions/64305438/warning-the-calling-iterator-did-not-fully-read-the-dataset-being-cached-in-or) with similar error.It helps.Thanks!", "@tilakrayal could you elaborate how this helps? I'm not following. Setting CUDA_LAUNCH_BLOCKING=1 changes nothing in this case. And SO answer on one hand mentions RAM cache (but the warning comes from FS cache). On another hands it mentions no warnings in Google Colab. But they still exist - https://stackoverflow.com/a/53353891\r\n\r\nSo the snippet has to be modified:\r\n```\r\n!pip install wurlitzer\r\n\r\nimport os\r\nimport tensorflow as tf\r\nfrom wurlitzer import pipes\r\n\r\nos.system('rm -f /tmp/foo*')\r\n\r\ndef gen():\r\n    for i in range(5):\r\n        yield i\r\n\r\ndataset = tf.data.Dataset.from_generator(gen, output_signature=tf.TensorSpec(shape=(), dtype=tf.int32))\r\ndataset = dataset.cache('/tmp/foo')\r\n# dataset = dataset.take(3).cache('/tmp/foo').repeat(2)\r\n\r\nwith pipes() as (out, err):\r\n    for x in dataset:\r\n        print(x)\r\n\r\nprint(out.read())\r\nprint(err.read())\r\n```", "@Saduf2019 ,\r\nI was able to reproduce the issue in tf v2.5 and nightly, in v2.4 i was able to execute the code without any warning.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/be9cde87e4572e7298359ac59870d0bf/untitled50952.ipynb).", "the same behaviour is also seen in tf 2.6"]}, {"number": 50951, "title": "smaller model runs slower than a larger one when compiled for edgetpu", "body": "**System information**-\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: coral usb edgetpu\r\n- TensorFlow version (use command below): v1.12.1-49562-gee58e600bfc 2.5.0-dev20210125\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nSo I have two models (U-nets) that are nearly identical except one of them uses fewer filters in some of the convolutional layers which makes that network strictly smaller, and when running the tflite version of the models the smaller one is indeed faster than the larger one, however when compiled and run on the edgetpu the smaller network runs slower than the larger network.\r\n\r\n**Describe the expected behavior**\r\nperformance gain form tflite should be the same on the edgetpu\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://drive.google.com/drive/folders/1-u9GpNwRdbCAxtaMuAdDZazMWqeMIt_n?usp=sharing\r\n\r\n**Other info / logs**\r\nI already made an issue at the google coral edgetpu page seen [here](https://github.com/google-coral/edgetpu/issues/424#issue-949588585), they said the issue was with interpreter.invoke() in the script ../lib/python3.8/site-packages/tflite_runtime/interpreter.py and that I should contact the tensorflow team.\r\n", "comments": ["@Drulludanni \r\nWe are unable to open the files in the drive shared, can you share the performance on a colab gist and share for us to analyse.", "that is very weird, the folder is shared with anyone with a link (if I open the link in incognito I can still view all the files). \r\n\r\nBut here is the code:\r\n\r\n```\r\nfrom pycoral.utils import edgetpu\r\nimport numpy as np\r\nimport time\r\n\r\nmodels = ['large.tflite', 'small.tflite', 'large_edgetpu.tflite', 'small_edgetpu.tflite']\r\n\r\nfor model_path in models:\r\n    print(model_path)\r\n    interpreter = edgetpu.make_interpreter(model_path)\r\n    interpreter.allocate_tensors()\r\n    input_index = interpreter.get_input_details()[0]['index']\r\n    output_index = interpreter.get_output_details()[0]['index']\r\n\r\n\r\n    n_trials = 10\r\n    total = 0\r\n    x = np.zeros((1,256,256,3), dtype=np.uint8)\r\n\r\n    # first call is usually slower so we skip it\r\n    interpreter.set_tensor(input_index, x)\r\n    interpreter.invoke()\r\n    pred = interpreter.get_tensor(output_index)\r\n\r\n    for i in range(n_trials):\r\n        t =  time.perf_counter()\r\n        interpreter.set_tensor(input_index, x)\r\n        interpreter.invoke()\r\n        pred = interpreter.get_tensor(output_index)\r\n        delta =  time.perf_counter() - t\r\n        print(delta)\r\n\r\n        total += delta\r\n\r\n\r\n    print(\"inference time:\", total/n_trials)\r\n```\r\nand this is the output:\r\n\r\n```\r\nlarge.tflite\r\n8.4517966\r\n8.4927569\r\n8.474044400000004\r\n8.477497300000003\r\n8.472683700000005\r\n8.491142599999996\r\n8.470729400000003\r\n8.475646299999994\r\n8.474072800000002\r\n8.462358199999997\r\ninference time: 8.47427282\r\nsmall.tflite\r\n5.282427299999995\r\n5.293356000000003\r\n5.279340199999993\r\n5.272431900000001\r\n5.2895331\r\n5.280650100000003\r\n5.273774800000012\r\n5.278881600000005\r\n5.27215240000001\r\n5.2793834\r\ninference time: 5.280193080000002\r\nlarge_edgetpu.tflite\r\n0.01768240000001242\r\n0.017120800000014924\r\n0.016798199999982444\r\n0.016576700000001665\r\n0.016357799999980216\r\n0.016318699999999353\r\n0.016506899999995994\r\n0.01634260000000154\r\n0.01632789999999318\r\n0.016752200000013318\r\ninference time: 0.016678419999999507\r\nsmall_edgetpu.tflite\r\n0.021253099999995584\r\n0.021638700000011113\r\n0.02142359999999144\r\n0.020244700000006333\r\n0.01973069999999666\r\n0.019953200000003335\r\n0.019520999999997457\r\n0.01960610000000429\r\n0.02138159999998379\r\n0.02115660000001185\r\ninference time: 0.020590930000000184\r\n\r\n```\r\n\r\nI tried to make a google colab to run the code but well, I have no idea how to make it run since an edgetpu is required and I don't know if it is possible to somehow make a virtual one in google colab, but here it is anyways: https://colab.research.google.com/drive/1YipG-DUlg0MzGOHV_y4_zadd38YI3wlz?usp=sharing and it should also include the models in my test if you wanna download them to use locally.", "@Drulludanni \r\n\r\nCould you please refer to these links:[link](https://github.com/tensorflow/tensorflow/issues/32743), [link1](https://github.com/tensorflow/tensorflow/issues/34135#issuecomment-716980570) and let us know.", "Neither of those links are helpful. The reason the code wont run is because there is no edgetpu connected to the colab, and that is the problem I don't know how to either have an edgetpu connected to the colab or how to fake the edgetpu being there with some kind of edgetpu emulation and as far as I'm aware nobody has done/tried that which is why I don't think I can ever make the google colab work for my problem."]}, {"number": 50939, "title": "implement reduce for the bitwise operations. ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4, 2.5. \r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, there is no tf.reduce_xor (or any reduction or any other bitwise op). There is the tf.bitwise package for bitwise ops, but they only operate on two inputs. \r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo, this would be an additional api. \r\n\r\n**Who will benefit with this feature?**\r\n\r\nAs more preprocessing operations are pushed into the graph-mode model rather than in tf.data.Dataset.map layers, we need to do more preprocessing work in the graph (though preprocessing does not need to be differentiable). For example, I am attempting to implement a message digest algorithm that is invariant to the ordering of the images in a batch. The basic algo would be to implement a message digest for each image in a mini batch, then xor them together with a tf.bitwise.reduce_xor() function. Without reduce, I'd have to rely on tf.scan and tf.bitwise.bitwise_xor. \r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 50935, "title": "2.5.0 tf_to_kernel is not compatible with msvc /Ob3", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 20H2\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.5.0\r\n- Python version: 3.8 (anaconda 2021.05)\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): msvc 19.28.29910 (Visual Studio 2019 v16.9.0)\r\n- CUDA/cuDNN version: 11.3.0_465.89/8.2.0.53\r\n- GPU model and memory: RTX 3090 24GB\r\n\r\n**Describe the problem**\r\n\r\nIf the Ob3 parameter is specified during compilation, the compilation will fail.\r\nThe error occurs in tf_to_kernel.exe.\r\n2.4.0 is fine with Ob3.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nC:\\Users\\User\\Source\\Repos\\tensorflow>python ./configure.py\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\User\\anaconda3\\python.exe]:\r\nFound possible Python library paths:\r\n  C:\\Users\\User\\anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\User\\anaconda3\\lib\\site-packages]\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\nFound CUDA 11.3 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/include\r\nFound cuDNN 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3/include\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n compute_35\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n/MP /cgthreads8 /Qpar /fp:fast -Ob3\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: n\r\nNot overriding eigen strong inline, some compilations could take more than 20 mins.\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nC:\\Users\\User\\Source\\Repos\\tensorflow>bazel build --config=opt --incompatible_strict_action_env=false --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nERROR: tensorflow/tensorflow/core/kernels/mlir_generated/BUILD:871:23: compile tensorflow/core/kernels/mlir_generated/***.o failed (Exit -1073741819): tf_to_kernel.exe failed: error executing command\r\n  cd execroot/org_tensorflow\r\nbazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel.exe --tile_sizes=1024 --arch=compute_35 --input=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/***.mlir --output=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/***.o --enable_ftz=False --cpu_codegen=False\r\n```", "comments": ["@fo40225 \r\nThe tested build configurations for CUDA/cuDNN for tf2.5 are\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.5.0 | 3.6-3.9 | MSVC 2019 | Bazel 3.7.2 | 8.1 | 11.2\r\ntensorflow_gpu-2.4.0 | 3.6-3.8 | MSVC 2019 | Bazel 3.1.0 | 8.0 | 11.0\r\n\r\nplease refer the [documentation](https://www.tensorflow.org/install/source_windows#gpu) and let us know if it helps.Thanks\r\n\r\n", "@UsharaniPagadala \r\n\r\nThis issue is not related to cuda/cudnn version.\r\n\r\nI have build the tf 2.5.0 with msvc 2019 16.10/cuda_11.3.0_465.89/cudnn-v8.2.0.53 and still error.\r\n\r\n```\r\nC:\\Users\\User\\Source\\Repos\\tensorflow>python ./configure.py\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\User\\anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\User\\anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\User\\anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.2 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include\r\nFound cuDNN 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: compute_35\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /MP /cgthreads8 /Qpar /fp:fast -Ob3\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\nC:\\Users\\User\\Source\\Repos\\tensorflow>bazel build --config=opt --incompatible_strict_action_env=false --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\ntf 2.4.0 can be compiled with Ob3.\r\ntf 2.5.0 will fail if Ob3 is used, but it will succeed if it is changed to Ob2.\r\n"]}, {"number": 50934, "title": "[Go] add bazel rules", "body": "PR adds Bazel rules to build and test tensorflow/go. Uses [rules_go](https://github.com/bazelbuild/rules_go).", "comments": ["@gharibian  Can you please review this PR ? Thanks!\r\n", "Adding Austin since this seems mostly like it will impact devinfra.", "I think this is beyond my level to accept... is it an accurate statement to say that this change adds a golang requirement into the full TF build? That seems like quite a large change that would need internal planning / support. Is there any other context you can point us to?", "FYI @mihaimaruseac ", "@angerson and @mihaimaruseac, thanks for looking.  Admittedly, I'm unable to assess internal implications (I am sensitive to these but lack insight) and don't have an understanding of acceptance level or criteria. @angerson, I completely agree that I left context out of this PR; let me take an initial stab at describing the problem.  Can fill in more blanks, if needed.\r\n\r\nAs it stands now:\r\n\r\n1. Go bindings cannot be built via Go's preferred tooling (Go Modules / `go get` fails). See #44655.\r\n\r\n2. Go bindings cannot be built via TensorFlow's preferred tooling (Bazel). This PR, but also see #18100.\r\n\r\nThis PR is not my preferred solution.  I'd strongly prefer that the Go bindings exist in their own repo (I wonder why the bindings are here, anyway?) and have recently proposed as much to community, including establishing a SIG Go (/cc @theadactyl).  Of course, there are other potential solutions including use of a third-party's fork/bindings, using/creating\r\nsome additional tooling to build/install, and so forth. As a very less-than-ideal solution, there is an [install guide](https://github.com/tensorflow/build/tree/master/golang_install_guide) in SIG Build's repo that amounts to making a local development copy of the TF repo, applying any patches, generating code, shuffling some directories around, generating more code, and then, for any/all dependent packages using a `replace` directive to point at the local copy instead of this repo (whew!).\r\n\r\nAgain, thanks so much.  I appreciate any thoughts that you have.", "I am unsure if we have anyone left at Google that works on the go bindings.\r\n\r\nIt might be possible to actually move these bindings to a separate repository.", "@wamuir Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!\r\n", "Yes, and thank you for following up @gbaned.  I really do like the idea of relocating the Go bindings into their own repository, e.g. tensorflow/go.  @mihaimaruseac and @gbaned, what's the next step to tee this up for consideration by the TF team?", "I think this could use an RFC:\nhttps://github.com/tensorflow/community/blob/master/governance/TF-RFCs.md\n@mihaimaruseac do you have an idea of who might sponsor?\n\nOn Fri, Dec 31, 2021 at 11:30 AM William Muir ***@***.***>\nwrote:\n\n> Yes, and thank you for following up @gbaned <https://github.com/gbaned>.\n> I really do like the idea of relocating the Go bindings into their own\n> repository, e.g. tensorflow/go. @mihaimaruseac\n> <https://github.com/mihaimaruseac> and @gbaned <https://github.com/gbaned>,\n> what's the next step to tee this up for consideration by the TF team?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/50934#issuecomment-1003440374>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABREJVIF7Y3I6TEEPWKLCHTUTYAENANCNFSM5A5F7KNA>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "I can sponsor", "Good news, I'll draft and submit to community.  I have time this week.", "https://github.com/tensorflow/community/pull/407", "@wamuir Can you please resolve conflicts? Thanks!", "> @wamuir Can you please resolve conflicts? Thanks!\r\n\r\nSure thing, done", "@wamuir Can you please address Ubuntu Sanity errors? Thank you!", "> @wamuir Can you please address Ubuntu Sanity errors? Thank you!\r\n\r\n@gbaned done, can you please trigger the tests to rerun?"]}, {"number": 50927, "title": "Export Graph gradient API on Windows", "body": "**System information**\r\n- TensorFlow version: 2.5\r\n- Are you willing to contribute it: Probably\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nSimilar to #41904, there are symbols missing when using the graph gradient API from C++.  We (SIG JVM) are running into this when trying to add custom gradient support.\r\n\r\nSymbols:\r\n```\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::StatusFromTF_Status(struct TF_Status const *)\" (?StatusFromTF_Status@tensorflow@@YA?AVStatus@1@PEBUTF_Status@@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: static class tensorflow::ops::GradOpRegistry * __cdecl tensorflow::ops::GradOpRegistry::Global(void)\" (?Global@GradOpRegistry@ops@tensorflow@@SAPEAV123@XZ)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class tensorflow::Status __cdecl tensorflow::ops::GradOpRegistry::Lookup(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class tensorflow::Status (__cdecl**)(class tensorflow::Scope const &,class tensorflow::Operation const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > *))const \" (?Lookup@GradOpRegistry@ops@tensorflow@@QEBA?AVStatus@3@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAP6A?AV43@AEBVScope@3@AEBVOperation@3@AEBV?$vector@VOutput@tensorflow@@V?$allocator@VOutput@tensorflow@@@std@@@6@PEAV96@@Z@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: bool __cdecl tensorflow::ops::GradOpRegistry::Register(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class tensorflow::Status (__cdecl*)(class tensorflow::Scope const &,class tensorflow::Operation const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > *))\" (?Register@GradOpRegistry@ops@tensorflow@@QEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@P6A?AVStatus@3@AEBVScope@3@AEBVOperation@3@AEBV?$vector@VOutput@tensorflow@@V?$allocator@VOutput@tensorflow@@@std@@@5@PEAV95@@Z@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Operation::Operation(class tensorflow::Node *)\" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class tensorflow::Graph * __cdecl tensorflow::Scope::graph(void)const \" (?graph@Scope@tensorflow@@QEBAPEAVGraph@2@XZ)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: bool __cdecl tensorflow::Scope::ok(void)const \" (?ok@Scope@tensorflow@@QEBA_NXZ)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: void __cdecl tensorflow::Scope::UpdateBuilder(class tensorflow::NodeBuilder *)const \" (?UpdateBuilder@Scope@tensorflow@@QEBAXPEAVNodeBuilder@2@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::Scope::GetUniqueNameForOp(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?GetUniqueNameForOp@Scope@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV34@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class tensorflow::Scope __cdecl tensorflow::Scope::ExitOnError(void)const \" (?ExitOnError@Scope@tensorflow@@QEBA?AV12@XZ)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class tensorflow::Scope __cdecl tensorflow::Scope::WithDevice(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?WithDevice@Scope@tensorflow@@QEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class tensorflow::Scope __cdecl tensorflow::Scope::WithNoControlDependencies(void)const \" (?WithNoControlDependencies@Scope@tensorflow@@QEBA?AV12@XZ)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class tensorflow::Scope __cdecl tensorflow::Scope::WithControlDependencies(class absl::lts_2020_09_23::Span<class tensorflow::Operation const > const &)const \" (?WithControlDependencies@Scope@tensorflow@@QEBA?AV12@AEBV?$Span@$$CBVOperation@tensorflow@@@lts_2020_09_23@absl@@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class tensorflow::Scope __cdecl tensorflow::Scope::WithControlDependencies(class tensorflow::Output const &)const \" (?WithControlDependencies@Scope@tensorflow@@QEBA?AV12@AEBVOutput@2@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class tensorflow::Scope __cdecl tensorflow::Scope::NewSubScope(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?NewSubScope@Scope@tensorflow@@QEBA?AV12@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: class tensorflow::Scope & __cdecl tensorflow::Scope::operator=(class tensorflow::Scope const &)\" (??4Scope@tensorflow@@QEAAAEAV01@AEBV01@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"public: __cdecl tensorflow::Scope::Scope(class tensorflow::Scope const &)\" (??0Scope@tensorflow@@QEAA@AEBV01@@Z)\r\njnitensorflow.obj : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::Set_TF_Status_from_Status(struct TF_Status *,class tensorflow::Status const &)\" (?Set_TF_Status_from_Status@tensorflow@@YAXPEAUTF_Status@@AEBVStatus@1@@Z)\r\n\r\n```\r\n\r\nIt seems like there's three parts:\r\n* Exporting `Scope`\r\n* Exporting [Set_TF_Status_from_Status](https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/c/tf_status_helper.h#L25)\r\n* Exporting `GradOpRegistry`\r\n\r\n**Will this change the current api? How?**\r\nIt will add additional exported symbols.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone trying to use the custom gradient API from Windows.\r\n\r\n**Any Other info.**\r\nIf it's as simple as adding `TF_CAPI_EXPORT` to `Set_TF_Status_from_Status` and adding scope and gradient targets to `tf_custom_op_library_additional_deps_impl`, I can make a PR, but I'm not sure of the proper place to add scope and gradient and whether you are far enough from the symbol limit to allow you to.", "comments": []}, {"number": 50887, "title": "MacOS failure to compile dylib with metal delegate", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur (11.4)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.0\r\n- Python version: Python 3.9.1\r\n- Bazel version (if compiling from source): bazel 3.7.2-homebrew\r\n- GCC/Compiler version (if compiling from source): clang version 11.0.0\r\n\r\n**Describe the problem**\r\nBuilding the default bazel target is fine and generates a dylib but when I add the metal delegate target to the file _tensorflow/lite/BUILD_:\r\n\r\n```\r\ntflite_cc_shared_object(\r\n    name = \"tensorflowlite\",\r\n    # Until we have more granular symbol export for the C++ API on Windows,\r\n    # export all symbols.\r\n    features = [\"windows_export_all_symbols\"],\r\n    linkopts = select({\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//conditions:default\": [\r\n            \"-Wl,-z,defs\",\r\n            \"-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)\",\r\n        ],\r\n    }),\r\n    per_os_targets = True,\r\n    deps = [\r\n        \":framework\",\r\n        \":tflite_exported_symbols.lds\",\r\n        \":tflite_version_script.lds\",\r\n        \"//tensorflow/lite/tools/evaluation:utils\",\r\n        \"//tensorflow/lite/delegates/gpu:metal_delegate\",      # adding this makes it fail\r\n        \"//tensorflow/lite/kernels:builtin_ops_all_linked\",\r\n    ],\r\n)\r\n```\r\n\r\nIt fails and gives me \r\n\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=183\r\nINFO: Reading rc options for 'build' from /Users/me/Repositories/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/me/Repositories/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /Users/me/Repositories/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 --action_env PYTHON_LIB_PATH=/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages --python_path=/Library/Frameworks/Python.framework/Versions/3.8/bin/python3\r\nINFO: Found applicable config definition build:short_logs in file /Users/me/Repositories/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/me/Repositories/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:macos in file /Users/me/Repositories/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nERROR: /private/var/tmp/_bazel_me/a01251391e7b28e63e36a7eb9c920e09/external/cpuinfo/BUILD.bazel:96:11: Configurable attribute \"srcs\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @cpuinfo//:linux_x86_64\r\n @cpuinfo//:linux_arm\r\n @cpuinfo//:linux_armhf\r\n @cpuinfo//:linux_armv7a\r\n @cpuinfo//:linux_armeabi\r\n @cpuinfo//:linux_aarch64\r\n @cpuinfo//:linux_mips64\r\n @cpuinfo//:linux_riscv64\r\n @cpuinfo//:linux_s390x\r\n @cpuinfo//:macos_x86_64\r\n @cpuinfo//:macos_arm64\r\n @cpuinfo//:windows_x86_64\r\n @cpuinfo//:android_armv7\r\n @cpuinfo//:android_arm64\r\n @cpuinfo//:android_x86\r\n @cpuinfo//:android_x86_64\r\n @cpuinfo//:ios_x86_64\r\n @cpuinfo//:ios_x86\r\n @cpuinfo//:ios_armv7\r\n @cpuinfo//:ios_arm64\r\n @cpuinfo//:ios_arm64e\r\n @cpuinfo//:watchos_x86_64\r\n @cpuinfo//:watchos_x86\r\n @cpuinfo//:watchos_armv7k\r\n @cpuinfo//:watchos_arm64_32\r\n @cpuinfo//:tvos_x86_64\r\n @cpuinfo//:tvos_arm64\r\nERROR: Analysis of target '//tensorflow/lite:tensorflowlite' failed; build aborted: /private/var/tmp/_bazel_me/a01251391e7b28e63e36a7eb9c920e09/external/cpuinfo/BUILD.bazel:96:11: Configurable attribute \"srcs\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @cpuinfo//:linux_x86_64\r\n @cpuinfo//:linux_arm\r\n @cpuinfo//:linux_armhf\r\n @cpuinfo//:linux_armv7a\r\n @cpuinfo//:linux_armeabi\r\n @cpuinfo//:linux_aarch64\r\n @cpuinfo//:linux_mips64\r\n @cpuinfo//:linux_riscv64\r\n @cpuinfo//:linux_s390x\r\n @cpuinfo//:macos_x86_64\r\n @cpuinfo//:macos_arm64\r\n @cpuinfo//:windows_x86_64\r\n @cpuinfo//:android_armv7\r\n @cpuinfo//:android_arm64\r\n @cpuinfo//:android_x86\r\n @cpuinfo//:android_x86_64\r\n @cpuinfo//:ios_x86_64\r\n @cpuinfo//:ios_x86\r\n @cpuinfo//:ios_armv7\r\n @cpuinfo//:ios_arm64\r\n @cpuinfo//:ios_arm64e\r\n @cpuinfo//:watchos_x86_64\r\n @cpuinfo//:watchos_x86\r\n @cpuinfo//:watchos_armv7k\r\n @cpuinfo//:watchos_arm64_32\r\n @cpuinfo//:tvos_x86_64\r\n @cpuinfo//:tvos_arm64\r\nINFO: Elapsed time: 0.104s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 526 targets configured)\r\n```\r\nI assume when adding the metal delegate it changes the select cpu type (im not familiar with bazel enough to know).  Unless there is a better way of adding the metal target.\r\n\r\nI also tried to add a target myself such as \r\n\r\n```\r\nmacos_dylib (\r\n  name = \"tensorflowlite2\",\r\n    minimum_os_version = \"10.12\",\r\n  deps = [  \r\n        \"//tensorflow/lite/kernels:builtin_ops_all_linked\",\r\n        \"//tensorflow/lite/tools/evaluation:utils\",\r\n        \"//tensorflow/lite/delegates/gpu:metal_delegate\",\r\n],\r\n)\r\n```\r\n\r\nFirst I had to change the cpu_info target to darwin_x86_64 (https://github.com/tensorflow/tensorflow/issues/41039) then I assume because of the CPU name change, when linking NNAPI it tries to build with `-lrt `which is not supported by MacOS and it will fail. Removing that flag (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/nnapi/BUILD#L38) I was able to build finally. I assume this is not the way I am supposed to do it but for some reason i had no issues with Android and iOS but MacOS has issues.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nBuilding with option 1\r\n1.  Clone and checkout **v2.5.0** branch (though I am sure this will fail on all branches and master)\r\n2. Modify the BUILD file **tensorflow/lite/BUILD** and add `\"//tensorflow/lite/delegates/gpu:metal_delegate\"` (location =`https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/lite/BUILD#L907`)\r\n3.  Run the command  `bazel build -c opt --copt -Os  --cxxopt=-std=c++14 --apple_platform_type=macos //tensorflow/lite:tensorflowlite`\r\n\r\nBuilding option 2\r\n1. Clone and checkout **v2.5.0** branch (though I am sure this will fail on all branches and master)\r\n2. Modify the BUILD file **tensorflow/lite/BUILD** and add\r\n```\r\nmacos_dylib (\r\n  name = \"tensorflowlite2\",\r\n    minimum_os_version = \"10.12\",\r\n  deps = [  \r\n        \"//tensorflow/lite/kernels:builtin_ops_all_linked\",\r\n        \"//tensorflow/lite/tools/evaluation:utils\",\r\n        \"//tensorflow/lite/delegates/gpu:metal_delegate\",\r\n],\r\n)\r\n```\r\n3.  Run the command  `bazel build -c opt --copt -Os  --cxxopt=-std=c++14 --apple_platform_type=macos //tensorflow/tensorflowlite2`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 50885, "title": "TF -> TFLite conversion error: could not rewrite use of immutable bound input", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**\r\n- TensorFlow installation (pip package or built from source): **pip package**\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): **2.5.0**\r\n\r\n### 2. Code\r\n\r\nThe model I'm trying to convert is a FasterRCNN model with an AlexNet backbone trained using PyTorch. From what I've gathered, the only way I could do this is by going from Torch -> ONNX -> Tensorflow -> TFLite. The conversion from Torch to ONNX succeeds but there's a couple of errors I encountered when converting from ONNX -> Tensorflow and subsequently from Tensorflow to TFLite. \r\n\r\n**Error 1:**  Resize coordinate_transformation_mode=pytorch_half_pixel is not supported in Tensorflow. (solved but may be related)\r\n\r\nThis is an error on the ONNX -> TF conversion side which I solved by modifying the transforms in the Torch model to include an `align_corners=True` argument in the interpolation call:\r\n\r\n```\r\ndef ObjDetModel():\r\n  \r\n    # Define backbone\r\n    backbone = torchvision.models.alexnet(pretrained=True).features\r\n    backbone.out_channels = 256\r\n\r\n    # Define anchor generator\r\n    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\r\n                           aspect_ratios=((0.5, 1.0, 2.0),))\r\n\r\n    # Define ROI pooler\r\n    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\r\n                                        output_size=7,\r\n                                        sampling_ratio=2)\r\n\r\n    # put the pieces together inside a FasterRCNN model\r\n    model = FasterRCNN(backbone,\r\n                      num_classes=2,\r\n                      rpn_anchor_generator=anchor_generator,\r\n                      box_roi_pool=roi_pooler)\r\n\r\n    min_size=800\r\n    max_size=1333\r\n    image_mean = [0.485, 0.456, 0.406]\r\n    image_std = [0.229, 0.224, 0.225]\r\n\r\n\r\n    # Change the transform op through subclassing the original transform class\r\n    transform = ModifiedRCNNTransform(min_size, max_size, image_mean, image_std)\r\n    model.transform = transform\r\n\r\n    return model\r\n\r\n@torch.jit.unused\r\ndef _get_shape_onnx(image):\r\n    # type: (Tensor) -> Tensor\r\n    from torch.onnx import operators\r\n    return operators.shape_as_tensor(image)[-2:]\r\n\r\n@torch.jit.unused\r\ndef _fake_cast_onnx(v):\r\n    # type: (Tensor) -> float\r\n    # ONNX requires a tensor but here we fake its type for JIT.\r\n    return v\r\n\r\ndef _resize_image_and_masks(image: Tensor, \r\n                            self_min_size: float, \r\n                            self_max_size: float,\r\n                            target: Optional[Dict[str, Tensor]] = None,\r\n                            fixed_size: Optional[Tuple[int, int]] = None,\r\n                            ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\r\n    if torchvision._is_tracing():\r\n        im_shape = _get_shape_onnx(image)\r\n    else:\r\n        im_shape = torch.tensor(image.shape[-2:])\r\n\r\n    size: Optional[List[int]] = None\r\n    scale_factor: Optional[float] = None\r\n    recompute_scale_factor: Optional[bool] = None\r\n\r\n    if fixed_size is not None:\r\n        size = [fixed_size[1], fixed_size[0]]\r\n    else:\r\n        min_size = torch.min(im_shape).to(dtype=torch.float32)\r\n        max_size = torch.max(im_shape).to(dtype=torch.float32)\r\n        scale = torch.min(self_min_size / min_size, self_max_size / max_size)\r\n\r\n        if torchvision._is_tracing():\r\n            scale_factor = _fake_cast_onnx(scale)\r\n        else:\r\n            scale_factor = scale.item()\r\n        recompute_scale_factor = True\r\n\r\n    # Had to add align_corners=True to make the conversion work\r\n    image = torch.nn.functional.interpolate(image[None], size=size, scale_factor=scale_factor, mode='bilinear',\r\n                                            recompute_scale_factor=recompute_scale_factor, align_corners=True)[0]\r\n\r\n    if target is None:\r\n        return image, target\r\n\r\n    if \"masks\" in target:\r\n        mask = target[\"masks\"]\r\n        mask = torch.nn.functional.interpolate(mask[:, None].float(), size=size, scale_factor=scale_factor,\r\n                                               recompute_scale_factor=recompute_scale_factor)[:, 0].byte()\r\n        target[\"masks\"] = mask\r\n\r\n    return image, target\r\n\r\n\r\ndef resize_boxes(boxes, original_size, new_size):\r\n    # type: (Tensor, List[int], List[int]) -> Tensor\r\n    ratios = [\r\n        torch.tensor(s, dtype=torch.float32, device=boxes.device) /\r\n        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\r\n        for s, s_orig in zip(new_size, original_size)\r\n    ]\r\n    ratio_height, ratio_width = ratios\r\n    xmin, ymin, xmax, ymax = boxes.unbind(1)\r\n\r\n    xmin = xmin * ratio_width\r\n    xmax = xmax * ratio_width\r\n    ymin = ymin * ratio_height\r\n    ymax = ymax * ratio_height\r\n    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\r\n\r\n# Subclass the original GeneralizedRCNNTransform object and overwrite the resize method\r\nclass ModifiedRCNNTransform(GeneralizedRCNNTransform):\r\n   \"\"\"docstring for ModifiedRCNNTransform\"\"\"\r\n    def __init__(self, min_size, max_size, image_mean, image_std):\r\n        super(ModifiedRCNNTransform, self).__init__(min_size, max_size, image_mean, image_std)\r\n        \r\n    def resize(self,\r\n               image: Tensor,\r\n               target: Optional[Dict[str, Tensor]] = None,\r\n               ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\r\n\r\n        h, w = image.shape[-2:]\r\n\r\n        if self.training:\r\n            size = float(self.torch_choice(self.min_size))\r\n        else:\r\n            # FIXME assume for now that testing uses the largest scale\r\n            size = float(self.min_size[-1])\r\n        image, target = _resize_image_and_masks(image, size, float(self.max_size), target, self.fixed_size)\r\nError 2:\r\n        if target is None:\r\n            return image, target\r\n\r\n        bbox = target[\"boxes\"]\r\n        bbox = resize_boxes(bbox, (h, w), image.shape[-2:])\r\n        target[\"boxes\"] = bbox\r\n\r\n        if \"keypoints\" in target:\r\n            keypoints = target[\"keypoints\"]\r\n            keypoints = resize_keypoints(keypoints, (h, w), image.shape[-2:])\r\n            target[\"keypoints\"] = keypoints\r\n        return image, target\r\n```\r\n\r\nIt is a very janky fix but it works (at least for the Torch -> ONNX -> Tensorflow conversion)\r\n\r\n**Error 2:** \r\n2021-07-21 17:15:33.799749: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: ./saved_models/tf_model\r\n2021-07-21 17:15:33.854208: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\r\n2021-07-21 17:15:33.854259: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./saved_models/tf_model\r\n2021-07-21 17:15:33.961926: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n2021-07-21 17:15:34.185671: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: ./saved_models/tf_model\r\n2021-07-21 17:15:34.399376: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 599629 microseconds.\r\n2021-07-21 17:15:35.011283: I tensorflow/compiler/mlir/tensorflow/translate/import_model.cc:1856] Unmodelled op type `CropAndResize` is not stateful but will be treated as such conservatively\r\n\r\nException: <unknown>:0: error: loc(callsite(callsite(\"onnx_tf_prefix_If_600@__inference___call___7107\" at \"StatefulPartitionedCall@__inference_signature_wrapper_7170\") at \"StatefulPartitionedCall\")): could not rewrite use of immutable bound input\r\n\r\nThe way I'm converting the model is very simple:\r\n\r\n```\r\ndef TensorflowToTFLite(self, tf_path, tflite_path):\r\n\r\n        print(\"Converting from Tensorflow to TFLite\")\r\n        # make a converter object from the saved tensorflow file\r\n        print(\"Initializing TFLite converter\")\r\n        converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\r\n\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n        # tell converter which type of optimization techniques to use\r\n        if self.full_quantize:\r\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n            converter.representative_dataset = self.representative_dataset\r\n            converter.inference_input_type = tf.uint8\r\n            converter.inference_output_type = tf.uint8  \r\n\r\n        else:\r\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                                   tf.lite.OpsSet.SELECT_TF_OPS]\r\n        \r\n        try:\r\n            print(\"Converting to TFLite model\")\r\n            tf_lite_model = converter.convert()\r\n\r\n            print(\"Saving TFLite Model...\")\r\n            with open(tflite_path, 'wb') as f:\r\n                f.write(tf_lite_model)\r\n\r\n        except Exception as e:\r\n            sys.exit(f\"Tensorflow to TFLite conversion failed! Exception: {e}\")\r\n```\r\n\r\nRegardless of whether or not I am full quantizing, I encounter the error. For the life of me, I could not figure out why this is happening. If anyone could point me in the right direction, that would be great. Here's a [link](https://drive.google.com/drive/folders/1bZ4pGbsEOwmB4fxvUPFpain_NPnV_rmk?usp=sharing) to the models and the MLIR reproducer.\r\n\r\nThanks so much!", "comments": ["We recently landed new freezing mechanism in MLIR but it is guarded by flag for now and it gets enabled only with enabling resource variables (converter.experimental_enable_resource_variables = True), we are planning to enable it by default later but after more confidence we think :) This option is only available at tf-nightly.", "Hi @abattery , thank you for the response. I appreciate it.\r\n\r\nI updated to the latest tf-nightly version and enabled the `experimental_enable_resource_variable` flag. I don't know what that  just did but somehow the conversion now works. However, invoking the interpreter gives me the following error:\r\n\r\n```\r\nRuntimeError: tensorflow/lite/kernels/non_max_suppression.cc:107 (max_output_size_value >= 0) was not true.Node number 5 (NON_MAX_SUPPRESSION_V4) failed to prepare.\r\nNode number 7 (WHILE) failed to prepare.\r\nNode number 9 (WHILE) failed to prepare.\r\nNode number 147 (IF) failed to prepare.\r\n```\r\n\r\nPrior to that, there were info messages when allocating the tensors but no errors. Here's how I am invoking the interpreter:\r\n\r\n```\r\ndef generate_TFLite_result(self,\r\n                               tflite_path):\r\n\r\n        interpreter, in_details, out_details = self._initialize_interpreter(tflite_path)\r\n\r\n        interpreter.set_tensor(in_details[0]['index'], self._sample_input.numpy())\r\n\r\n        print(\"Invoking interpreter\")\r\n        # Error encountered here. Invoke fails.\r\n        interpreter.invoke()\r\n\r\n        tflite_result_dict = {key: interpreter.get_tensor(out_details[i]['index']) for i, key in enumerate(self.output_names)}\r\n        tflite_result_dict['name'] = 'TFLite'\r\n\r\n        # self.compare_results(tflite_result_dict)\r\n        \r\n    def _initialize_interpreter(self, tflite_path):\r\n\r\n        print(f\"Using tensorflow {tf.__version__}\")\r\n        print(\"Initializing interpreter...\")\r\n        interpreter = tf.lite.Interpreter(model_path=tflite_path)\r\n\r\n        print(\"Allocating tensors...\")\r\n        interpreter.allocate_tensors()\r\n\r\n        input_details = interpreter.get_input_details()\r\n        output_details = interpreter.get_output_details()\r\n\r\n        return interpreter, input_details, output_details\r\n```\r\n\r\nI have uploaded the converted TFLite model in the same google drive if that helps.", "Did you provide the valid input values?", "Hi @abattery,\r\n\r\nYep. I am pretty sure I am. I tried feeding in an image that has range [0., 255.] as well as [0, 1]. I encounter the same error for both times. My thinking was that the converted model might have expected inputs from those ranges, but I guess I am mistaken. I cannot actually feed a uint8 type image because I am not full quantizing the model yet (I tried, but it gave me flex op errors. This will have to be a different issue, I imagine.)\r\n\r\nAnyway, I am confident that I am feeding in the right  input because the Tensorflow model is giving me results:\r\n\r\n```\r\n{'output_0': <tf.Tensor: shape=(11, 4), dtype=float32, numpy=\r\narray([[ 439.6666  ,  331.39816 ,  613.25104 ,  479.07257 ],\r\n       [   0.      ,  679.2871  ,   29.266216,  712.7054  ],\r\n       [  22.223734,  708.92993 ,   62.30639 ,  719.9272  ],\r\n       [1233.7614  ,    0.      , 1279.9366  ,  212.73343 ],\r\n       [   0.      ,  676.146   ,   54.730453,  712.2286  ],\r\n       [   0.      ,  688.35754 ,    7.232581,  715.50336 ],\r\n       [  16.303509,  715.3977  ,  319.86813 ,  719.9698  ],\r\n       [1229.1129  ,    0.      , 1260.742   ,   12.602079],\r\n       [1260.8397  ,    0.      , 1279.9738  ,    9.131262],\r\n       [1252.0791  ,  274.9876  , 1279.9617  ,  350.96954 ],\r\n       [1251.2029  ,  448.05634 , 1279.9604  ,  494.23047 ]],\r\n      dtype=float32)>, \r\n'output_1': <tf.Tensor: shape=(11,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>, \r\n'output_2': <tf.Tensor: shape=(11,), dtype=float32, numpy=\r\narray([0.9997458 , 0.36914745, 0.36914745, 0.36914745, 0.36914745,\r\n       0.36914745, 0.36914745, 0.36914745, 0.36914745, 0.36914745,\r\n       0.36914745], dtype=float32)>, 'name': 'Tensorflow'}\r\n\r\n```\r\nAs a context, `output_0` are bounding boxes, `output_1` are classes, and `output_2` are confidence scores. The first bounding box with 99% confidence is the correct prediction. Hence, I think that the PyTorch -> ONNX -> Tensorflow conversion is successful and I am feeding the correct input.\r\n"]}, {"number": 50872, "title": "Tensorflow estimator returns the same answer", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform: Ubuntu 18.04.5 LTS\r\n- TensorFlow installed from pip3\r\n- TensorFlow version (use command below): all versions 2.5+\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version:  11.2\r\n- GPU model and memory: 3090, 2060\r\n\r\n**Describe the current behavior**\r\nI create a tf.keras model, train it and save it with model.save as keras model. After that I transform model to tf estimator with \r\n`tf.keras.estimator.model_to_estimator`, create input reciever and save estimator with it `estimator.export_saved_model(EXPORT_PATH, serving_input_receiver_fn = serving_fn)`\r\nAfter that I load this 2 models in my jupyter notebook and send same pictures. From two same models in different formats I get different results. Moreover, model that were saved as estimator returns the same answer on all pictures, but the model in H5 format returns different answers.\r\n\r\n\r\n**Describe the expected behavior**\r\nEstimator and H5 model return the same answer. Estimator return different answers.\r\n\r\nIt works with all tested models and datasets (5 models and 3 datasets). I watched answers in tensorboard during training and they are normal. The problem starts when I create estimator. I can't avoid creating estimator because I need it for TF serving.", "comments": ["@aleks73337 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.Thanks!", "@tilakrayal \r\n[test_html.txt](https://github.com/tensorflow/tensorflow/files/6860896/test_html.txt)\r\n[test_ipynb.txt](https://github.com/tensorflow/tensorflow/files/6860897/test_ipynb.txt)\r\nIt's jupyter notebook and it's html copy. Fix names please. I can't upload them as is.", "@aleks73337 ,\r\n\r\nGiven .txt formatted code is hard to replicate .Can you please provide the code or colab gist to reproduce and analyse the issue in our environment.Thanks!", "Just change the format. It's normal jupyter files, but with changed formats for uploading to github\u00a0\n-------- \u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 --------\u041e\u0442: tilakrayal ***@***.***> \u0414\u0430\u0442\u0430: 23.07.2021  09:48  (GMT+03:00) \u041a\u043e\u043c\u0443: tensorflow/tensorflow ***@***.***> \u041a\u043e\u043f\u0438\u044f: aleks73337 ***@***.***>, Mention ***@***.***> \u0422\u0435\u043c\u0430: Re: [tensorflow/tensorflow] Tensorflow estimator returns the same answer (#50872) \n@aleks73337 ,\nGiven .txt formatted code is hard to replicate .Can you please provide the code or colab gist to reproduce and analyse the issue in our environment.Thanks!\n\n\u2014You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub, or unsubscribe.\n", "@aleks73337 ,\r\n\r\nWithout the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a minimal code snippet or colab gist.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@tilakrayal, it's fully reproducible code just saved in txt format for uploading it to github. Ok, let me explain how to use it.\r\n1. Download test_ipynb.txt\r\n2. Change the format of file. test_ipynb.txt -> test.ipynb\r\n3. Open it with jupyter notebook\r\nGithub doesn't allow to upload ipynb format files. So I just changed the format. But if you change the format back, it will work.", "@Saduf2019 ,\r\n\r\nI was able to reproduce the issue in TF version [2.4](https://colab.research.google.com/gist/tilakrayal/86c34b6b96233d4b81662cb3c7a3137f/2-4test_ipynb.ipynb), [2.5](https://colab.research.google.com/gist/tilakrayal/28e143fe63730b17f3f3791b72182de2/2-5test.ipynb) and whereas in [nightly](https://colab.research.google.com/gist/tilakrayal/a8466b35f51345a7c66939d84977d080/nightlytest_ipynb.ipynb) version and it is  noticed that session is being crashed. Please, find the gist here. Thanks!"]}, {"number": 50857, "title": "Discrepancy between expected and tflite quantization parameters", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `no`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux Ubuntu 20.04`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `N/A`\r\n- TensorFlow installed from (source or binary): `binary (pip package)`\r\n- TensorFlow version (use command below): `2.5.0`\r\n- Python version: `3.8.10`\r\n- Bazel version (if compiling from source): `N/A`\r\n- GCC/Compiler version (if compiling from source): `N/A`\r\n- CUDA/cuDNN version: `N/A`\r\n- GPU model and memory: `N/A  (using CPU)`\r\n\r\n**Describe the current behavior**\r\nHi there! :wave: Thank's for the great work on Tensorflow, TFLite!\r\n\r\nUnfortunately, I\u2019m facing an issue where there seems to be a discrepancy between the quantization parameters recorded by a fully-quantized tflite model.\r\n\r\nFor an input `x` and kernel `w`, I manually compute `tf.matmul(x, w)` and then compute the scale/zero-point of the result.  These are the values I get:\r\n```\r\nParameters from manual computation\r\nScale: 0.09185781291886871, Zero-point: -3.0351044277537085\r\n````\r\nbut when I build the equivalent model in tflite, these are the scale/zp it calculates:\r\n```\r\nParameters from tflite model\r\nScale: 0.09184519201517105, Zero-point: -3\r\n```\r\nI would expect them to be the same, but they are not. [Here is an example to reproduce the issue](https://colab.research.google.com/drive/1QjwtcNIBb-lqtuMod6tV233D4UFvQ0dS?usp=sharing).\r\n\r\nThis discrepancy is causing larger TFLite models to produce significantly different outputs than expected, when compounded across multiple layers. [Here is a demonstration of how these errors compound with more layers](https://colab.research.google.com/drive/1PJjr6EOGJXZ0dvaoGKSJaFzA4pIiv9xe?usp=sharing). \r\n\r\n**Describe the expected behavior**\r\nI would expect the quantization parameters to be the same.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n- Do you want to contribute a PR? (yes/no): `no`\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n[Link to Colab example](https://colab.research.google.com/drive/1QjwtcNIBb-lqtuMod6tV233D4UFvQ0dS?usp=sharing )\r\nThe model is 1 Flatten layer + 4 simple `Dense` layers with `use_bias=False`, on the `MNIST` dataset.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to diagnose the problem.\r\n\r\nA visualization of the tflite model using `python -m tensorflow.lite.tools.visualize` and netron are attached.\r\n\r\n![gist](https://user-images.githubusercontent.com/70828198/126397792-54a3f092-de97-41a0-9fdc-f4b9c2af563e.png)\r\n![gist_netron](https://user-images.githubusercontent.com/70828198/126397793-c4b93dda-4aa2-4044-81d0-9b5912187f2c.png)", "comments": ["Hi Renjie/Jian,\r\n\r\nCould any of you help check on this issue with quantization parameters? Feel free to pass it around.\r\n\r\nThanks,\r\nTiezhen\r\n\r\n"]}, {"number": 50853, "title": "Crash/Force termination in distributed training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.9\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: 11.1.1\r\n- GPU model and memory: A100\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the distributed training, i.e. with MultiWorkerMirroredStrategy, the application runs through and then crashes with \"terminate called without an active exception\"\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nTaken from #50790: \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom mpi_cluster_resolver import MPIClusterResolver\r\n\r\nresolver = MPIClusterResolver()\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(cluster_resolver=resolver)\r\n\r\nwith strategy.scope():\r\n    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\r\n    x_train = x_train / 255.0\r\n    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dense(10),\r\n    ])\r\n\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.SGD(),\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        metrics=['accuracy'])\r\n\r\nis_master = resolver.task_id == 0\r\nverbose = 2 if is_master else 0\r\nmodel.fit(train_data.repeat(), epochs=1, steps_per_epoch=10, verbose=verbose)\r\n```\r\n\r\nWith \r\n[mpi_cluster_resolver.py.txt](https://github.com/tensorflow/tensorflow/files/6823734/mpi_cluster_resolver.py.txt)\r\n\r\n**Other info / logs**\r\nI traced the crash to `TF_DeleteGraph` Python wrapper by following the stacktrace and inserting debug information: \r\n[tf.txt](https://github.com/tensorflow/tensorflow/files/6848724/tf.txt)\r\n\r\nAs you can see the code is called during runtime shutdown and ultimately reaches pybind11s `~gil_scoped_release` which calls `PyEval_RestoreThread`. This is documented as terminating when the runtime is finalized: https://docs.python.org/3/c-api/init.html#c.PyEval_RestoreThread\r\n\r\nSo to fix this either all threads created by TF must be collected and freed before python starts finalizing or the `pybind11::gil_scoped_release` must be removed from cleanup functions\r\nSee also https://github.com/pybind/pybind11/pull/2657", "comments": ["does this has a workaround\uff1f"]}, {"number": 50843, "title": "`case` and `cond` executes all the functions within", "body": "Hi, I'm trying to execute some condition-dependent functions where each function needs to contract tensors differently depending on their shapes, for instance. However, I realised that `tf.cond` and `tf.case` are executing all functions regardless of the condition. Prepared the following code as an example;\r\n\r\n```python\r\nx = tf.ones((3,2,1))\r\ny = tf.ones((1,2,3))\r\nz = tf.ones((4,3,5))\r\nk = tf.ones((3,5,5))\r\n\r\ndef a(t): \r\n    def exe():\r\n        return tf.einsum(\"ijk,lmi\", t, y)\r\n    return exe\r\n\r\ndef b(t): \r\n    def exe():\r\n        return tf.einsum(\"ijk,ljm\", t, z)\r\n    return exe\r\n\r\ndef d(t): \r\n    def exe():\r\n        return tf.einsum(\"ijk,klm\", t, z)\r\n    return exe\r\n\r\nc = tf.constant(1)\r\n\r\n@tf.function\r\ndef f(t):\r\n    y = tf.case([\r\n        (tf.equal(tf.shape(t)[0], 3), a(t)),\r\n        (tf.equal(tf.shape(t)[1], 3), b(t)),\r\n    ], default=d, exclusive=True)\r\n    return y\r\n\r\nprint(f(x))\r\n```\r\nWhen this code is executed without the decorator, I get the correct answer but with the decorator, all the conditions are executed. Hence in my particular case, since I'm doing different calculations depending on the tensor's shape, I get a multitude of errors. I've seen many such bug reports but haven't found a solution. Is there another way to do conditional execution that I'm not aware of where different functions can be executed depending on the condition?\r\n\r\nThanks\r\n\r\n**System information**\r\n- OS Platform and Distribution: MacOS 11.4\r\n- TensorFlow installed from: PyPI\r\n- TensorFlow version: 2.4.1\r\n- TensorFlow Git version : v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.2", "comments": ["@jackaraz \r\n\r\nCould you please share the error `traceback`  of with decorator if you can.Thanks", "> @jackaraz\r\n> \r\n> Could you please share the error `traceback` of with decorator if you can.Thanks\r\n\r\nHere is the error message. I believe decorated class doesn't work when different ranks and/or different dimensions are used:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-55-41f970bbceef> in <module>\r\n     29     return y\r\n     30 \r\n---> 31 print(f(x))\r\n\r\n~/packages/python3env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    826     tracing_count = self.experimental_get_tracing_count()\r\n    827     with trace.Trace(self._name) as tm:\r\n--> 828       result = self._call(*args, **kwds)\r\n    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    830       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~/packages/python3env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    869       # This is the first call of __call__, so we have to initialize.\r\n    870       initializers = []\r\n--> 871       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    872     finally:\r\n    873       # At this point we know that the initialization is complete (or less\r\n\r\n~/packages/python3env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    723     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\r\n    724     self._concrete_stateful_fn = (\r\n--> 725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    726             *args, **kwds))\r\n    727 \r\n\r\n~/packages/python3env/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2967       args, kwargs = None, None\r\n   2968     with self._lock:\r\n-> 2969       graph_function, _ = self._maybe_define_function(args, kwargs)\r\n   2970     return graph_function\r\n   2971 \r\n\r\n~/packages/python3env/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3359 \r\n   3360           self._function_cache.missed.add(call_context_key)\r\n-> 3361           graph_function = self._create_graph_function(args, kwargs)\r\n   3362           self._function_cache.primary[cache_key] = graph_function\r\n   3363 \r\n\r\n~/packages/python3env/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3194     arg_names = base_arg_names + missing_arg_names\r\n   3195     graph_function = ConcreteFunction(\r\n-> 3196         func_graph_module.func_graph_from_py_func(\r\n   3197             self._name,\r\n   3198             self._python_function,\r\n\r\n~/packages/python3env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    988         _, original_func = tf_decorator.unwrap(python_func)\r\n    989 \r\n--> 990       func_outputs = python_func(*func_args, **func_kwargs)\r\n    991 \r\n    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/packages/python3env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    632             xla_context.Exit()\r\n    633         else:\r\n--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    635         return out\r\n    636 \r\n\r\n~/packages/python3env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    975           except Exception as e:  # pylint:disable=broad-except\r\n    976             if hasattr(e, \"ag_error_metadata\"):\r\n--> 977               raise e.ag_error_metadata.to_exception(e)\r\n    978             else:\r\n    979               raise\r\n\r\nValueError: in user code:\r\n\r\n    <ipython-input-53-41f970bbceef>:25 f  *\r\n        y = tf.case([\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\r\n        return target(*args, **kwargs)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:3424 case_v2\r\n        return _case_helper(\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:3221 _case_helper\r\n        return fn()\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:538 new_func\r\n        return func(*args, **kwargs)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:1180 cond\r\n        return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/ops/cond_v2.py:90 cond_v2\r\n        false_graph = func_graph_module.func_graph_from_py_func(\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:990 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:538 new_func\r\n        return func(*args, **kwargs)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:1180 cond\r\n        return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/ops/cond_v2.py:83 cond_v2\r\n        true_graph = func_graph_module.func_graph_from_py_func(\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:990 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /var/folders/s2/4l4y6wf9097g6c1mm6pg6_y00000gn/T/tmppjhqxiw9.py:18 exe\r\n        retval__1 = ag__.converted_call(ag__.ld(tf).einsum, ('ijk,ljm', ag__.ld(t), ag__.ld(z)), None, fscope_1)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\r\n        return target(*args, **kwargs)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/ops/special_math_ops.py:751 einsum\r\n        return _einsum_v2(equation, *inputs, **kwargs)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/ops/special_math_ops.py:1180 _einsum_v2\r\n        return gen_linalg_ops.einsum(inputs, resolved_equation)\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/ops/gen_linalg_ops.py:1091 einsum\r\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\r\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:590 _create_op_internal\r\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3528 _create_op_internal\r\n        ret = Operation(\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:2015 __init__\r\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\r\n    /Users/jackaraz/packages/python3env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1856 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Dimensions must be equal, but are 2 and 3 for '{{node case/cond/cond/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"ijk,ljm->iklm\"](case/cond/cond/einsum/Einsum/t, case/cond/cond/einsum/Einsum/inputs_1)' with input shapes: [3,2,1], [4,3,5].\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@rmothukuru \r\n\r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/224d60bca5d8c305867aec32beabba2f/-50843.ipynb#scrollTo=9Tx5QJ30BYv2).Thanks", "There are multiple issues at play here:\r\n\r\n1. the eager version of tf.cond only executes one branch, which is inconsistent with graph execution. That's a bug.\r\n2. the way conditionals are traced does have this fundamental limitation: that it traces all branches. Combine that with the shape verifications done at tracing, it explains the errors you see. [This article](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#all-python-code-paths-are-executed-during-tracing) makes a note of it.\r\n\r\nThe only workaround available is to us static conditionals instead of dynamic ones. Static conditionals happen at trace time, while dynamic ones happen at execution time and trace both branches. Since shape verifications also happen at trace time, you can see all sorts of issues with dynamic conds.\r\n\r\nIn a nutshell, here's the difference between a static conditional and a dynamic one:\r\n\r\n```\r\nif True:  # static conditional, the condition is not a Tensor\r\n  <code>\r\nif True:  # dynamic conditional, the condition is a Tensor\r\n  <code>\r\n```\r\n\r\nOr without autograph:\r\n\r\n```\r\nif True:  # static (trace time) conditional\r\n  <code>\r\ntf.cond(true, lambda: <code>, ...)  # dynamic conditional\r\n```\r\n\r\nFor your case, the workaround amounts to:\r\n\r\n1. replace the tf.case with if statements\r\n2. use tensor.shape instead of tf.shape; the former returns a static Python value, the latter returns a tensor (hence can only be used with dynamic conditionals)\r\n\r\nSomething like this:\r\n\r\n```\r\nif t.shape[3] == 3:\r\n  y = a(t)\r\n# etc.\r\n```\r\n\r\nI know this explanation is convoluted - it's because there are many moving parts and it's what makes tracing a bit more difficult to master. But happy to clarify things if needed."]}, {"number": 50807, "title": "Using tf.function decorator on an instance method does not maintain a strong reference to self", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.4\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.8.9\r\n\r\n**Describe the current behavior**\r\n\r\nExpressed in a minimal code example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyObject(object):\r\n    def __init__(self):\r\n        self.some_attribute = 2\r\n\r\n    @tf.function\r\n    def some_tf_function(self, param):\r\n        return self.some_attribute + param\r\n\r\n\r\n# This works:\r\nobj = MyObject()\r\nobj.some_tf_function(3)  # returns 5\r\n\r\n# This throws AttributeError: 'NoneType' has no attribute 'some_attribute'\r\nresult = MyObject().some_tf_function(3)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nRunning `MyObject().some_tf_function(...)` in the example above should not throw an exception. It seems that when wrapped with `tf.function`, instance methods don't maintain strong Python references to the objects the functions are bound to.", "comments": ["@rmothukuru  ,\r\nI was able to reproduce the issue in tf v2.4,v2.5 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ae0b8c407b3ab0c59e326708f97f7441/untitled50807.ipynb)."]}]