[{"number": 42271, "title": "Model using Conv1D returns socket closed error on TPU while with SeparableConv1D works", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GCP instance Debian 9\r\n- TensorFlow installed from (source or binary): installed via pip \r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7.6\r\n- **TPU** model: TPU v3-8\r\n\r\n**Describe the current behavior**\r\nI am executing a deep CNN model to handle sequences using TPUs. The exact same model, if layers are Conv1D, does not work returning a specific error, while, if I use SeparableConv1D, the model works fine.\r\n\r\n**Describe the expected behavior**\r\nThe model should work using Conv1D layers.\r\n\r\n**Other info / logs** \r\n\r\n> /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n>     106   def _method_wrapper(self, *args, **kwargs):\r\n>     107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n> --> 108       return method(self, *args, **kwargs)\r\n>     109 \r\n>     110     # Running inside `run_distribute_coordinator` already.\r\n> \r\n> /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n>    1098               tmp_logs = train_function(iterator)\r\n>    1099               if data_handler.should_sync:\r\n> -> 1100                 context.async_wait()\r\n>    1101               logs = tmp_logs  # No error, now safe to assign to logs.\r\n>    1102               end_step = step + data_handler.step_increment\r\n> \r\n> /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py in async_wait()\r\n>    2317   an error state.\r\n>    2318   \"\"\"\r\n> -> 2319   context().sync_executors()\r\n>    2320 \r\n>    2321 \r\n> \r\n> /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py in sync_executors(self)\r\n>     656     \"\"\"\r\n>     657     if self._context_handle:\r\n> --> 658       pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\n>     659     else:\r\n>     660       raise ValueError(\"Context is not initialized.\")\r\n> \r\n> UnavailableError: 2 root error(s) found.\r\n>   (0) Unavailable: Socket closed\r\n>   (1) Invalid argument: Unable to find a context_id matching the specified one (5208125777247716751). Perhaps the worker was restarted, or the context was GC'd?\r\n> 0 successful operations.\r\n> 0 derived errors ignored.\r\n", "comments": ["@nicosacco,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "> @nicosacco,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n\r\n@amahendrakar I am sorry, but at the moment I cannot share either the code or the data, because they are part of an ongoing research project. If I can help you in any other way, please, do not hesitate to ask.", "@nicosacco,\r\nIn this case, could you please provide a dummy model and data to mimic the error you are facing.\r\n\r\nAlso, please take a look at [this](https://github.com/huan/tensorflow-handbook-tpu/issues/1#issuecomment-606189444) similar issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42271\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42271\">No</a>\n"]}, {"number": 42270, "title": "RGB to YUV: Conflicting information between example and description", "body": "@rthadur Reopened PR https://github.com/tensorflow/tensorflow/pull/41565", "comments": []}, {"number": 42269, "title": "Fix eager tensor concat", "body": "Fixes #42254.", "comments": ["Could you please add a test that fails without this change and passes with it?", "> Could you please add a test that fails without this change and passes with it?\r\n\r\nHi @edloper, this is an issue with keras symbolic tensor, can I use keras as dependencies in `tensorflow/python/kernel_tests/*` (seems that all kernel_tests do not depend on keras), or is there any way to emulate symbolic tensor?", "@WindQAQ Ah ok, yeah, that's kind of tricky to add a test for.  I guess it's ok as long as you verified that this fixes the failing Keras case. "]}, {"number": 42268, "title": "Enabling a debug dll build under Windows (without CUDA, at least)", "body": "Modified SpaceToDepthOp and DepthToSpaceOp templated classes not to use a SpaceToDepthOpFunctor/DepthToSpaceOpFunctor structs with a template parameter Device=_GPU_Device in case the class itself is instantiated with Device=_CPU_Device. Added a partial template specialization for Device=GPUDevice to preserve the existing behaviour in all cases.\r\n\r\nThis at least partially (i.e. when bazel is configured not to use CUDA) fixes issue #41118.", "comments": ["@MikhailStartsev please open the PR against master branch , we only accept security fixes to release branches."]}, {"number": 42267, "title": "Debundled/System JsonCPP uses wrong header path", "body": "**Describe the problem**\r\n\r\nWhen using TF_SYSTEM_LIBS=jsoncpp_git the build scripts will try linking headers from `$(INCLUDEDIR)/jsoncpp/json/` which has 2 problems:\r\n\r\n1. INCLUDEDIR cannot be set for JsonCPP only making it impossible to install jsoncpp to a custom prefix (e.g. /opt/jsoncpp), see #37835\r\n2. When installing JsonCPP there will be headers `<prefix>/include/json/*.h`. I.e. there is no additional folder \"jsoncpp\". See https://github.com/open-source-parsers/jsoncpp/blob/5be07bdc5e2d5b7715ecbc73749af3e625674dcb/include/CMakeLists.txt#L4\r\n\r\nHence the \"system installed\" version of JsonCPP is not usable with TensorFlow\r\n\r\nPossible solution: Do NOT symlink the headers but rely on CPATH being set correctly ", "comments": ["Fixed by #42516", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42267\">No</a>\n"]}, {"number": 42266, "title": "Use nasmlink genrule", "body": "Avoids cyclic dependency as the name and src must not be the same. This uses the same mechanism as done by cython\r\n\r\nFixes #42264 (see that for more details), TLDR is:\r\n\r\nCurrently the build file sets the name and src to \"nasm\" which is forbidden: https://docs.bazel.build/versions/master/be/shell.html\r\n\r\n>    do not give the rule and the file the same name.\r\n\r\nThis PR fixes that by using an intermediate rule with a different name", "comments": []}, {"number": 42265, "title": "x", "body": "x", "comments": ["@Elona-marti \r\n\r\nCan you please see [link1](https://stackoverflow.com/questions/51067599/cx-freeze-python-error-in-main-script-importerror-dll-load-failed) and [link2](https://stackoverflow.com/questions/2553886/how-can-i-bundle-other-files-when-using-cx-freeze) and see if it helps you.\r\n\r\nThis issue is more related to windows and cx_freeze .\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request related to Tensorflow. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "Did you install [MSVC 2019](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)?\r\nAlso see https://www.tensorflow.org/install/pip#system-requirements", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42265\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42265\">No</a>\n"]}, {"number": 42264, "title": "Using system NASM results in \"cycle in dependency graph\" error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.x (all from 2.0 to 2.3)\r\n- Bazel version (if compiling from source): 0.29.1\r\n\r\n\r\n**Describe the problem**\r\n\r\nUsing system NASM makes the build error with:\r\n```\r\nERROR: /tmp/easybuild-tmp/eb-K85jaZ/tmpnpNkyO-bazel-build/external/nasm/BUILD.bazel:8:1: in sh_binary rule @nasm//:nasm: cycle in dependency graph:\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n    //tensorflow/tools/pip_package:simple_console\r\n    //tensorflow:tensorflow_py\r\n    //tensorflow:tensorflow_py_no_contrib\r\n    //tensorflow:tf_python_api_gen_v2\r\n    //tensorflow:create_tensorflow.python_api_2_tf_python_api_gen_v2\r\n    //tensorflow/python:no_contrib\r\n    //tensorflow/python:control_flow_ops\r\n    //tensorflow/python:platform\r\n    //tensorflow/python:_pywrap_util_port\r\n    //tensorflow/python:_pywrap_util_port.so\r\n    //tensorflow/python:_pywrap_tensorflow_internal_linux\r\n    //tensorflow/python:lib_pywrap_tensorflow_internal.so\r\n    //tensorflow/python:lib_pywrap_tensorflow_internal.so_rule\r\n    //tensorflow/python:_pywrap_tensorflow_internal.so\r\n    //tensorflow/python:py_record_writer_lib\r\n    //tensorflow/c:tf_status_helper\r\n    //tensorflow/core:lib\r\n    //tensorflow/core:lib_internal\r\n    //tensorflow/core/platform/default/build_config:platformlib\r\n    //tensorflow/core/platform/default/build_config:jpeg\r\n    @libjpeg_turbo//:jpeg\r\n    @libjpeg_turbo//:simd_x86_64\r\n    @libjpeg_turbo//:simd/x86_64/jsimdcpu.o\r\n    @libjpeg_turbo//:simd_x86_64_assemblage23\r\n.-> @nasm//:nasm [self-edge]\r\n`--\r\nThis cycle occurred because of a configuration option\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nSet `TF_SYSTEM_LIBS=nasm` before building as instructed\r\n\r\n**Any other info / logs**\r\n\r\nThe problem is an erroneous build script: https://github.com/tensorflow/tensorflow/blob/3ae4ccf38c9731343bd7a39db7bac8067335646a/third_party/nasm/BUILD.system#L10\r\n\r\nThis sets the `name` and `src` to \"nasm\" which is forbidden: https://docs.bazel.build/versions/master/be/shell.html\r\n\r\n> do not give the rule and the file the same name. \r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42264\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42264\">No</a>\n"]}, {"number": 42263, "title": "Custom TF metric: \"Graph\" tensor leakage", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): v2.2\r\nPython version: 3.6.9\r\nCUDA/cuDNN version: v10.2\r\nGPU model and memory: GeForce GTX 1070 - 8117MiB\r\n\r\nDescribe the current behavior:\r\nMy custom metric works on sample inputs; yet after model compilation breaks down due to an error related to eager and graph tensors. I did not find anything specific which helps me resolve the issue.\r\n\r\nIssue: \r\n```An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: Max:0\r\n```\r\n\r\nDescribe the expected behavior:\r\nCustom metric is usable as a metric in a TF2 model, logging to tensorboard as well.  \r\n\r\nStandalone code to reproduce the issue\r\nhttps://colab.research.google.com/drive/1FPGz3dK2zSI2wjuTYrywaUG0UlHnej6c?usp=sharing \r\n", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/8c8a0b0da94d5de7210ab07d077fba22/untitled369.ipynb).", "Any updates @ymodak? \r\n\r\n", "Hi @Jordy-VL, I've seen this error reported a few times and as I understand it it's caused when an outer tf.function is retraced where there's an inner tf.function that has accessed a tensor created in the outer tf.function. Here is a simple example which results in the same error message. Removing (1) or (2) should make the error go away.\r\n```\r\nclass Model:\r\n  def __init__(self):\r\n    self.v = None\r\n    self.t = None\r\n\r\n  @tf.function  # (1)\r\n  def bar(self):\r\n    print('tracing bar_g')\r\n    return self.t  # (2)\r\n\r\n  def foo(self):\r\n    y = self.bar()\r\n    self.v.assign_sub(1.0)\r\n    \r\n  @tf.function\r\n  def loop(self):\r\n    print('tracing loop')\r\n    if self.t is None:\r\n      self.t = tf.constant(1.0)\r\n    if self.v is None:\r\n      self.v = tf.Variable(1.0)\r\n    self.foo()\r\n\r\nm = Model()\r\nm.loop()\r\n```\r\n I think that explains why you aren't seeing the error in `test_simple()` and you are seeing the error with fit. If you set run_eagerly=True in compile, it seems to run okay as well.\r\n\r\nIt's not immediately obvious to me from your code what is causing the problem. Can you take a look at this similar issue #32889 and let me know if that helps? I'll do some more investigation into whether this is a bug or intended behavior. ", "> Hi @Jordy-VL, I've seen this error reported a few times and as I understand it it's caused when an outer tf.function is retraced where there's an inner tf.function that has accessed a tensor created in the outer tf.function. Here is a simple example which results in the same error function. Removing (1) or (2) should make the error go away.\r\n> \r\n> ```\r\n> class Model:\r\n>   def __init__(self):\r\n>     self.v = None\r\n>     self.t = None\r\n> \r\n>   @tf.function  # (1)\r\n>   def bar(self):\r\n>     print('tracing bar_g')\r\n>     return self.t  # (2)\r\n> \r\n>   def foo(self):\r\n>     y = self.bar()\r\n>     self.v.assign_sub(1.0)\r\n>     \r\n>   @tf.function\r\n>   def loop(self):\r\n>     print('tracing loop')\r\n>     if self.t is None:\r\n>       self.t = tf.constant(1.0)\r\n>     if self.v is None:\r\n>       self.v = tf.Variable(1.0)\r\n>     self.foo()\r\n> \r\n> m = Model()\r\n> m.loop()\r\n> ```\r\n> \r\n> I think that explains why you aren't seeing the error in `test_simple()` and you are seeing the error with fit. If you set run_eagerly=True in compile, it seems to run okay as well.\r\n> \r\n> It's not immediately obvious to me from your code what is causing the problem. Can you take a look at this similar issue #32889 and let me know if that helps? I'll do some more investigation into whether this is a bug or intended behavior.\r\n\r\nThank you for the explanation.\r\nEssentially, I believe my error is caused by having my` self.confidences` and `self.correctness` as np.arrays or lists to which I concatenate new entries. Since it is not a TF.variable or constant, it loses the trace. \r\n**So my follow-up question is, how can I keep a variable-size list to which I append more entries?** \r\nThe metric does not work when only saving float/int information like \"True_Positives\" AND the number of training/validation samples is not known upfront. \r\n\r\n", "For a variable-size list have you tried using a [tf.TensorArray](https://www.tensorflow.org/api_docs/python/tf/TensorArray)? The [Accumulating values in a loop](https://www.tensorflow.org/guide/function#loops) example in the tf.function guide might be helpful.", "I tried it with Tf.TensorArrray.\r\n```\r\n            self.correctness = tf.TensorArray(tf.int32, dynamic_size=True, infer_shape=True)\r\n            self.confidences = tf.TensorArray(tf.float32, dynamic_size=True, infer_shape=True)\r\n```\r\nI got the following error: \r\n`ValueError: Size must be declared for TensorArrays when eager execution is enabled.`\r\n", "You can set the `size` argument to your TensorArray. With `dynamic_size=True` then the array can grow past its original size. You can see that noted under the [Args section of the TensorArray docs](https://www.tensorflow.org/api_docs/python/tf/TensorArray) for `dynamic_size` ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42263\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42263\">No</a>\n"]}, {"number": 42262, "title": "has no attr named '_XlaCompile'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@reckzhou \r\n\r\nRequest you to fill issue template,\r\nPlease, share simple standalone code to reproduce the issue.It helps us in localizing the issue faster.\r\n\r\nPlease, see the comments of similar error [here](https://stackoverflow.com/questions/50756848/xla-compile-error-operation-has-no-attr-named-xlacompile) and see if it helps you.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "thanks ravikyram ,i solved.  docker used different cpu. one of cpu is new."]}, {"number": 42261, "title": "Update docker_test.sh: s/ci_bukld/ci_build/", "body": "", "comments": []}, {"number": 42260, "title": "ValueError: Unknown activation: ELU", "body": "\r\nfrom keras.models import load_model\r\nmodel = load_model('discriminator_model_800.h5')\r\n\r\nabove got  this error ValueError: Unknown activation: ELU", "comments": ["@yash04singh \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nCan you please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.\r\n\r\nAlso, refer similar type of issues with activation functions [link1\r\n](https://stackoverflow.com/questions/55364954/keras-load-model-cant-recognize-tensorflows-activation-functions),[link2](https://stackoverflow.com/questions/59771314/valueerror-unknown-activation-activation-while-trying-to-load-a-model-with-c) and see if it helps you.Thanks!", "link 1 works but shows this warning : WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\r\n I did this>>  model = tf.keras.models.load_model('discriminator_model_010.h5', custom_objects={'ELU': ELU})\r\n\r\nthis is my model \r\ndef define_discriminator(size):\r\n    model = Sequential()\r\n    model.add(Dense(400, input_dim=size, activation=ELU(alpha=0.5,),kernel_initializer=initializer))\r\n    model.add(Dense(200, activation=ELU(alpha=0.5,),kernel_initializer=initializer))\r\n    model.add(Dense(100, activation=ELU(alpha=0.5,),kernel_initializer=initializer))\r\n    model.add(Dense(50, activation=ELU(alpha=0.5,),kernel_initializer=initializer))\r\n    model.add(Dense(1, activation='sigmoid',kernel_initializer=initializer))\r\n    # compile model\r\n    opt = Adam(lr=0.002, beta_1=0.5)\r\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n    return model\r\n", "@yash04singh \r\n\r\nPlease, go through similar issue #37968 and see if it helps you.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42259, "title": "Helloworld example on esp32 not working", "body": "@tensorflow/micro\r\n\r\n\r\nAfter building andd flashing the helloworld example for esp32. esp32 keeps on rebooting with the reason as mofying a null pointer\r\n\r\n\r\nLogs From esp-32:\r\n\r\nrst:0x1 (POWERON_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT)\r\nconfigsip: 0, SPIWP:0xee\r\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\r\nmode:DIO, clock div:2\r\nload:0x3fff0018,len:4\r\nload:0x3fff001c,len:6864\r\nho 0 tail 12 room 4\r\nload:0x40078000,len:14076\r\nload:0x40080400,len:4304\r\nentry 0x400806e8\r\nI (74) boot: Chip Revision: 1\r\nI (74) boot_comm: chip revision: 1, min. bootloader chip revision: 0\r\nI (41) boot: ESP-IDF v4.0.1-dirty 2nd stage bootloader\r\nI (41) boot: compile time 20:11:51\r\nI (41) boot: Enabling RNG early entropy source...\r\nI (46) boot: SPI Speed      : 40MHz\r\nI (50) boot: SPI Mode       : DIO\r\nI (54) boot: SPI Flash Size : 4MB\r\nI (58) boot: Partition Table:\r\nI (62) boot: ## Label            Usage          Type ST Offset   Length\r\nI (69) boot:  0 nvs              WiFi data        01 02 00009000 00006000\r\nI (76) boot:  1 phy_init         RF data          01 01 0000f000 00001000\r\nI (84) boot:  2 factory          factory app      00 00 00010000 00100000\r\nI (91) boot: End of partition table\r\nI (96) boot_comm: chip revision: 1, min. application chip revision: 0\r\nI (103) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x09cb0 ( 40112) map\r\nI (126) esp_image: segment 1: paddr=0x00019cd8 vaddr=0x3ffb0000 size=0x020f0 (  8432) load\r\nI (130) esp_image: segment 2: paddr=0x0001bdd0 vaddr=0x40080000 size=0x00400 (  1024) load\r\n0x40080000: _WindowOverflow4 at I:/Resources/esp-idf/components/freertos/xtensa_vectors.S:1778\r\n\r\nI (133) esp_image: segment 3: paddr=0x0001c1d8 vaddr=0x40080400 size=0x03e38 ( 15928) load\r\nI (148) esp_image: segment 4: paddr=0x00020018 vaddr=0x400d0018 size=0x4f054 (323668) map\r\n0x400d0018: _stext at ??:?\r\n\r\nI (266) esp_image: segment 5: paddr=0x0006f074 vaddr=0x40084238 size=0x059a4 ( 22948) load\r\n0x40084238: memspi_host_read_status_hs at I:/Resources/esp-idf/components/spi_flash/memspi_host_driver.c:67\r\n\r\nI (283) boot: Loaded app from partition at offset 0x10000\r\nI (283) boot: Disabling RNG early entropy source...\r\nI (283) cpu_start: Pro cpu up.\r\nI (287) cpu_start: Application information:\r\nI (292) cpu_start: Project name:     slick-AIoT\r\nI (297) cpu_start: App version:      1\r\nI (301) cpu_start: Compile time:     Aug  1 2020 20:11:33\r\nI (307) cpu_start: ELF file SHA256:  4e8a9f0ccd7e777b...\r\nI (313) cpu_start: ESP-IDF:          v4.0.1-dirty\r\nI (319) cpu_start: Starting app cpu, entry point is 0x40081128\r\n0x40081128: call_start_cpu1 at I:/Resources/esp-idf/components/esp32/cpu_start.c:271\r\n\r\nI (0) cpu_start: App cpu up.\r\nI (329) heap_init: Initializing. RAM available for dynamic allocation:\r\nI (336) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM\r\nI (342) heap_init: At 3FFB5210 len 0002ADF0 (171 KiB): DRAM\r\nI (348) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\r\nI (355) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\r\nI (361) heap_init: At 40089BDC len 00016424 (89 KiB): IRAM\r\nI (367) cpu_start: Pro cpu start user code\r\nI (385) spi_flash: detected chip: generic\r\nI (386) spi_flash: flash io: dio\r\nI (386) cpu_start: Starting scheduler on PRO CPU.\r\nI (0) cpu_start: Starting scheduler on APP CPU.\r\nintrptr before 0, value :1073434788\r\n\r\nintrptr after 1073426328, value :1073434788\r\n\r\nGuru Meditation Error: Core  0 panic'ed (StoreProhibited). Exception was unhandled.\r\nCore 0 register dump:\r\nPC      : 0x400d45e0  PS      : 0x00060630  A0      : 0x800d459d  A1      : 0x3ffb6ff0  \r\n0x400d45e0: loop at i:\\slick-aiot\\esp32\\build/../main/main_functions.cc:103\r\n\r\nA2      : 0x00000000  A3      : 0x00000014  A4      : 0x40c90fdb  A5      : 0x00000000  \r\nA6      : 0x3ffaffe0  A7      : 0x3ffb8980  A8      : 0x00000000  A9      : 0x3ffb6fe0  \r\nA10     : 0x00000000  A11     : 0x41a00000  A12     : 0x00000008  A13     : 0x00000005  \r\nA14     : 0x00000005  A15     : 0x0000000c  SAR     : 0x00000001  EXCCAUSE: 0x0000001d  \r\nEXCVADDR: 0x00000000  LBEG    : 0x4010ba41  LEND    : 0x4010ba48  LCOUNT  : 0x00000000  \r\n0x4010ba41: tflite::BytesRequiredForTensor(tflite::Tensor const&, unsigned int*, unsigned int*, tflite::ErrorReporter*) at i:\\slick-aiot\\esp32\\build/../components/tfmicro/tensorflow/lite/micro/memory_helpers.cc:92\r\n\r\n0x4010ba48: tflite::BytesRequiredForTensor(tflite::Tensor const&, unsigned int*, unsigned int*, tflite::ErrorReporter*) at i:\\slick-aiot\\esp32\\build/../components/tfmicro/third_party/flatbuffers/include/flatbuffers/flatbuffers.h:2431\r\n (inlined by) ?? at i:\\slick-aiot\\esp32\\build/../components/tfmicro/third_party/flatbuffers/include/flatbuffers/flatbuffers.h:2435\r\n (inlined by) ?? at i:\\slick-aiot\\esp32\\build/../components/tfmicro/tensorflow/lite/schema/schema_generated.h:3612\r\n (inlined by) tflite::BytesRequiredForTensor(tflite::Tensor const&, unsigned int*, unsigned int*, tflite::ErrorReporter*) at i:\\slick-aiot\\esp32\\build/../components/tfmicro/tensorflow/lite/micro/memory_helpers.cc:97\r\n\r\n\r\nELF file SHA256: 4e8a9f0ccd7e777b\r\n\r\nBacktrace: 0x400d45dd:0x3ffb6ff0 0x400d459a:0x3ffb7010 0x400d14ca:0x3ffb7030 0x40087281:0x3ffb7050\r\n0x400d45dd: loop at i:\\slick-aiot\\esp32\\build/../main/main_functions.cc:103\r\n\r\n0x400d459a: app_main at i:\\slick-aiot\\esp32\\build/../main/esp/main.cc:21 (discriminator 1)\r\n\r\n0x400d14ca: main_task at I:/Resources/esp-idf/components/esp32/cpu_start.c:553\r\n\r\n0x40087281: vPortTaskWrapper at I:/Resources/esp-idf/components/freertos/port.c:143\r\n\r\n\r\nRebooting...\r\nets Jun  8 2016 00:22:57\r\n\r\nrst:0xc (SW_CPU_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT)\r\nconfigsip: 0, SPIWP:0xee\r\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\r\nmode:DIO, clock div:2\r\nload:0x3fff0018,len:4\r\nload:0x3fff001c,len:6864\r\nho 0 tail 12 room 4\r\nload:0x40078000,len:14076\r\nload:0x40080400,len:4304\r\nentry 0x400806e8\r\nI (75) boot: Chip Revision: 1\r\nI (75) boot_comm: chip revision: 1, min. bootloader chip revision: 0\r\nI (41) boot: ESP-IDF v4.0.1-dirty 2nd stage bootloader\r\nI (41) boot: compile time 20:11:51\r\nI (41) boot: Enabling RNG early entropy source...\r\nI (47) boot: SPI Speed      : 40MHz\r\nI (51) boot: SPI Mode       : DIO\r\nI (55) boot: SPI Flash Size : 4MB\r\nI (59) boot: Partition Table:\r\nI (62) boot: ## Label            Usage          Type ST Offset   Length\r\nI (70) boot:  0 nvs              WiFi data        01 02 00009000 00006000\r\nI (77) boot:  1 phy_init         RF data          01 01 0000f000 00001000\r\nI (85) boot:  2 factory          factory app      00 00 00010000 00100000\r\nI (92) boot: End of partition table\r\nI (96) boot_comm: chip revision: 1, min. application chip revision: 0\r\nI (103) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x09cb0 ( 40112) map\r\nI (127) esp_image: segment 1: paddr=0x00019cd8 vaddr=0x3ffb0000 size=0x020f0 (  8432) load\r\nI (130) esp_image: segment 2: paddr=0x0001bdd0 vaddr=0x40080000 size=0x00400 (  1024) load\r\n0x40080000: _WindowOverflow4 at I:/Resources/esp-idf/components/freertos/xtensa_vectors.S:1778\r\n\r\nI (134) esp_image: segment 3: paddr=0x0001c1d8 vaddr=0x40080400 size=0x03e38 ( 15928) load\r\nI (149) esp_image: segment 4: paddr=0x00020018 vaddr=0x400d0018 size=0x4f054 (323668) map\r\n0x400d0018: _stext at ??:?\r\n\r\nI (267) esp_image: segment 5: paddr=0x0006f074 vaddr=0x40084238 size=0x059a4 ( 22948) load\r\n0x40084238: memspi_host_read_status_hs at I:/Resources/esp-idf/components/spi_flash/memspi_host_driver.c:67\r\n\r\nI (283) boot: Loaded app from partition at offset 0x10000\r\nI (283) boot: Disabling RNG early entropy source...\r\nI (284) cpu_start: Pro cpu up.\r\nI (287) cpu_start: Application information:\r\nI (292) cpu_start: Project name:     slick-AIoT\r\nI (297) cpu_start: App version:      1\r\nI (302) cpu_start: Compile time:     Aug  1 2020 20:11:33\r\nI (308) cpu_start: ELF file SHA256:  4e8a9f0ccd7e777b...\r\nI (314) cpu_start: ESP-IDF:          v4.0.1-dirty\r\nI (319) cpu_start: Starting app cpu, entry point is 0x40081128\r\n0x40081128: call_start_cpu1 at I:/Resources/esp-idf/components/esp32/cpu_start.c:271\r\n\r\nI (305) cpu_start: App cpu up.\r\nI (330) heap_init: Initializing. RAM available for dynamic allocation:\r\nI (337) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM\r\nI (343) heap_init: At 3FFB5210 len 0002ADF0 (171 KiB): DRAM\r\nI (349) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\r\nI (355) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\r\nI (362) heap_init: At 40089BDC len 00016424 (89 KiB): IRAM\r\nI (368) cpu_start: Pro cpu start user code\r\nI (386) spi_flash: detected chip: generic\r\nI (387) spi_flash: flash io: dio\r\nI (387) cpu_start: Starting scheduler on PRO CPU.\r\nI (0) cpu_start: Starting scheduler on APP CPU.\r\nintrptr before 0, value :1073434788\r\n\r\nintrptr after 1073426328, value :1073434788\r\n\r\nGuru Meditation Error: Core  0 panic'ed (StoreProhibited). Exception was unhandled.\r\nCore 0 register dump:\r\nPC      : 0x400d45e0  PS      : 0x00060630  A0      : 0x800d459d  A1      : 0x3ffb6ff0  \r\n0x400d45e0: loop at i:\\slick-aiot\\esp32\\build/../main/main_functions.cc:103\r\n\r\nA2      : 0x00000000  A3      : 0x00000014  A4      : 0x40c90fdb  A5      : 0x00000000  \r\nA6      : 0x3ffaffe0  A7      : 0x3ffb8980  A8      : 0x00000000  A9      : 0x3ffb6fe0  \r\nA10     : 0x00000000  A11     : 0x41a00000  A12     : 0x00000008  A13     : 0x00000005  \r\nA14     : 0x00000005  A15     : 0x0000000c  SAR     : 0x00000001  EXCCAUSE: 0x0000001d  \r\nEXCVADDR: 0x00000000  LBEG    : 0x4010ba41  LEND    : 0x4010ba48  LCOUNT  : 0x00000000  \r\n0x4010ba41: tflite::BytesRequiredForTensor(tflite::Tensor const&, unsigned int*, unsigned int*, tflite::ErrorReporter*) at i:\\slick-aiot\\esp32\\build/../components/tfmicro/tensorflow/lite/micro/memory_helpers.cc:92\r\n\r\n0x4010ba48: tflite::BytesRequiredForTensor(tflite::Tensor const&, unsigned int*, unsigned int*, tflite::ErrorReporter*) at i:\\slick-aiot\\esp32\\build/../components/tfmicro/third_party/flatbuffers/include/flatbuffers/flatbuffers.h:2431\r\n (inlined by) ?? at i:\\slick-aiot\\esp32\\build/../components/tfmicro/third_party/flatbuffers/include/flatbuffers/flatbuffers.h:2435\r\n (inlined by) ?? at i:\\slick-aiot\\esp32\\build/../components/tfmicro/tensorflow/lite/schema/schema_generated.h:3612\r\n (inlined by) tflite::BytesRequiredForTensor(tflite::Tensor const&, unsigned int*, unsigned int*, tflite::ErrorReporter*) at i:\\slick-aiot\\esp32\\build/../components/tfmicro/tensorflow/lite/micro/memory_helpers.cc:97\r\n\r\n\r\nELF file SHA256: 4e8a9f0ccd7e777b\r\n\r\nBacktrace: 0x400d45dd:0x3ffb6ff0 0x400d459a:0x3ffb7010 0x400d14ca:0x3ffb7030 0x40087281:0x3ffb7050\r\n0x400d45dd: loop at i:\\slick-aiot\\esp32\\build/../main/main_functions.cc:103\r\n\r\n0x400d459a: app_main at i:\\slick-aiot\\esp32\\build/../main/esp/main.cc:21 (discriminator 1)\r\n\r\n0x400d14ca: main_task at I:/Resources/esp-idf/components/esp32/cpu_start.c:553\r\n\r\n0x40087281: vPortTaskWrapper at I:/Resources/esp-idf/components/freertos/port.c:143\r\n\r\n", "comments": ["@Aiden1729 \r\nThis issue has already been reported #40956 #40956 #42075 can we close this issue as it is already tracked.", " But the issuee you mentioned seem to be different from what i am facing\n\nThanks & Regards,\nAkhil Sai\nContact: 9004539499\n\nOn Wed, 12 Aug, 2020, 3:51 PM Saduf2019, <notifications@github.com> wrote:\n\n> @Aiden1729 <https://github.com/Aiden1729>\n> This issue has already been reported #40956\n> <https://github.com/tensorflow/tensorflow/issues/40956> #40956\n> <https://github.com/tensorflow/tensorflow/issues/40956> #42075\n> <https://github.com/tensorflow/tensorflow/issues/42075> can we close this\n> issue as it is already tracked.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42259#issuecomment-672787393>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC3XGFEMG5FNB3H5ASZNBFLSAJUJPANCNFSM4P4M5YXQ>\n> .\n>\n", "@Aiden1729 can you share the `sdkconfig` file created in the `..prj//hello_world/esp-idf/` folder ?. also can you mention the esp-idf version that you are using ?\r\nI was aware of one crash issue which has been fixed with [this PR](https://github.com/tensorflow/tensorflow/pull/42653). Please check once if your issue is already fixed.\r\nThank you", "@Aiden1729  Please let us know if you are still facing the issue?.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42259\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42259\">No</a>\n"]}, {"number": 42258, "title": "Unable to build tflite aar file on windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): tensorflow github repo\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): MSVC 2017\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am unable to build the tflite aar files using this build command: \r\n bazel build //tensorflow/lite/java:tensorflow-lite --cxxopt=--std=c++14 -c opt --fat_apk_cpu=armeabi-v7a  --verbose_failures \r\n\r\nThis warning pops up a lot of times\r\ncl : Command line warning D9002 : ignoring unknown option '--std=c++14'\r\nAnd I get this error in the end \r\nException in thread \"main\" java.nio.file.InvalidPathException: Illegal character [:] in path at index 4: ///C:/Users/nikhil/AppData/Local/Temp/android_resources_tmp11744088656958341427/linked/bin.-pb.apk\r\n\r\n\r\n", "comments": ["Can you try setting the flag as `/std:c++14`?", "@ymodak I tried this build command : bazel build //tensorflow/lite/java:tensorflow-lite --cxxopt=/std:c++14 -c opt --fat_apk_cpu=armeabi-v7a --verbose_failures\r\nThe warnings stopped. But I still get this error in the end. \r\n\r\nbazel-out/x64_windows-opt/bin/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/ResourceProcessorBusyBox.exe @bazel-out/x64_windows-opt/bin/tensorflow/lite/java/tensorflow-lite_dummy_app_for_so_symbols/R.txt-0.params\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nAug 14, 2020 11:07:08 AM com.google.devtools.build.android.ResourceProcessorBusyBox processRequest\r\nSEVERE: Error during processing\r\njava.nio.file.InvalidPathException: Illegal character [:] in path at index 4: ///C:/Users/nikhil/AppData/Local/Temp/android_resources_tmp5138593176805055845/linked/bin.-pb.apk\r\n        at java.base/sun.nio.fs.WindowsPathParser.nextSlash(WindowsPathParser.java:212)\r\n        at java.base/sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:111)\r\n        at java.base/sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)\r\n        at java.base/sun.nio.fs.WindowsPath.parse(WindowsPath.java:92)\r\n        at java.base/sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:229)\r\n        at java.base/java.nio.file.Path.of(Path.java:147)\r\n        at java.base/java.nio.file.Paths.get(Paths.java:69)\r\n        at com.google.devtools.build.android.aapt2.ProtoApk.asApkPath(ProtoApk.java:203)\r\n        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:555)\r\n        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:536)\r\n        at com.google.devtools.build.android.Aapt2ResourcePackagingAction.main(Aapt2ResourcePackagingAction.java:185)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$14.call(ResourceProcessorBusyBox.java:144)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox.processRequest(ResourceProcessorBusyBox.java:240)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:203)\r\n\r\nException in thread \"main\" java.nio.file.InvalidPathException: Illegal character [:] in path at index 4: ///C:/Users/nikhil/AppData/Local/Temp/android_resources_tmp5138593176805055845/linked/bin.-pb.apk\r\n        at java.base/sun.nio.fs.WindowsPathParser.nextSlash(WindowsPathParser.java:212)\r\n        at java.base/sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:111)\r\n        at java.base/sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)\r\n        at java.base/sun.nio.fs.WindowsPath.parse(WindowsPath.java:92)\r\n        at java.base/sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:229)\r\n        at java.base/java.nio.file.Path.of(Path.java:147)\r\n        at java.base/java.nio.file.Paths.get(Paths.java:69)\r\n        at com.google.devtools.build.android.aapt2.ProtoApk.asApkPath(ProtoApk.java:203)\r\n        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:555)\r\n        at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:536)\r\n        at com.google.devtools.build.android.Aapt2ResourcePackagingAction.main(Aapt2ResourcePackagingAction.java:185)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$14.call(ResourceProcessorBusyBox.java:144)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox.processRequest(ResourceProcessorBusyBox.java:240)\r\n        at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:203)\r\nTarget //tensorflow/lite/java:tensorflow-lite failed to build", "Have you try TensorFlow 2.2 or higher?", "@terryheo Yep. 2.2 works fine. Even master works fine. I can see this error for both tf2.0 and tf2.1", "@nikroc896 hello,I met similar question while trying to build tflite 1.15.2 on win10,I wonder how did you solve this problem then?\r\nThank u ", "@nikroc896,\r\n\r\nWe are checking to see if this is still an issue. Can you try updating to latest stable version i.e `2.6.0` and let us know if the issue persists? Thanks!", "@sanatmpa1  \r\nIt wasn't an issue in tf2.2 at the time so it should not persist. I encountered it just in tf2.0 and tf2.1 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42258\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42258\">No</a>\n"]}, {"number": 42257, "title": "Resource exhausted error during training on GPU with tensorflow 2.1.0", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code, but nothing really fancy\r\n- TensorFlow installed from (source or binary): conda installed from source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: CUDA 10.1\r\n- GPU model and memory:  Quadro M1200. 8GB RAM\r\n\r\n**Describe the current behavior**\r\nI get an ResourceExhaustedError during my training. Even if I have a batch size of one. I have to predict and fit my data seperately, because in between I have to create a return function based on my predictions, which is input for the model.fit(). When I train my model, it starts with 4000 MB free memory in the GPU, after initialization it goes to 2022 MB free memory. It stays like this till 92 epochs, after 92 epochs it goes to 949 MB free memory. After 186 epochs it goes to 730 MB free memory in the GPU and after 197 epochs I get the error: \r\n\r\nResourceExhaustedError:  OOM when allocating tensor with shape[108,32,103,66] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node MaxPoolGrad_2 (defined at C:\\Users\\Floor\\Documents\\Basic model\\testmap\\test.py:233) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n [Op:__inference_distributed_function_3945]\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow as tf\r\ntf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)\r\nimport numpy as np\r\nimport sys\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.layers import Dense, Conv2D, Activation, Flatten,MaxPooling2D\r\n\r\ndef create_model(): \r\n   \r\n    model = Sequential()\r\n    model.add(Conv2D(32, (6, 6), input_shape=( 108, 71, 9)))\r\n    model.add(Activation(\"relu\"))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\n    model.add(Conv2D(32, (6, 6)))\r\n    model.add(Activation(\"relu\"))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\n    model.add(Conv2D(32, (6, 6)))\r\n    model.add(Activation(\"relu\"))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\n    model.add(Flatten())\r\n    model.add(Dense(256, activation='relu'))\r\n    model.add(Dense(1, activation = \"sigmoid\"))\r\n\r\n    model.compile(optimizer=Adam(lr=0.00001/10), loss='mean_squared_error')  \r\n    return(model)\r\nFor i in range(N) #amount of epochs\r\n    model.predict(EpisodesP.reshape([-1,img_w, img_h, dim]))\r\n\r\n    #Return function G \r\n\r\n    model.fit(EpisodesP, G, epochs = 1, verbose = 0, batch_size=1)\r\n\r\n**Other info / logs** \r\nraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\Floor\\Documents\\Basic model\\testmap\\test.py\", line 267, in <module>\r\n    history,value,model,loss, loss_episode= basic_code(Episodes, Success, N = 1000, P = 1)\r\n\r\n  File \"C:\\Users\\Floor\\Documents\\Basic model\\testmap\\test.py\", line 233, in basic_code\r\n    model.fit(EpisodesP, GP, epochs = 1, verbose = 0, batch_size=sum(TP)) #model fitted to get the loss\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n    ctx=ctx)\r\n\r\n  File \"C:\\Users\\Floor\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nResourceExhaustedError:  OOM when allocating tensor with shape[108,32,103,66] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node MaxPoolGrad_2 (defined at C:\\Users\\Floor\\Documents\\Basic model\\testmap\\test.py:233) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n [Op:__inference_distributed_function_3945]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n", "comments": ["@HelloFloor \r\n\r\nLimiting gpu memory can help resolve OOM error.\r\nSee https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth.\r\nThanks!", "@ravikyram, thank you for the answer. I tried that already, as is visible in my code snippet: \r\nimport tensorflow as tf\r\ntf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)\r\nDo you have any other advice how to fix this error? \r\nThanks! ", "Your model just doesn't fit in your GPU. The memory of your GPU is not enough, that's why you're getting it.\r\n\r\nPlease take a look at this [issue](https://stackoverflow.com/questions/59394947/how-to-fix-resourceexhaustederror-oom-when-allocating-tensor) to solve this problem. Thanks!", "Thank you for the response. But if it doesn't fit, how is it possible that I can run it for almost 200 epochs before I get the ResourceExhaustedError? ", "@gowthamkpr Would you know why I can run it for the 200 epochs before I get the ResourceExhaustedError? Thank you! ", "@HelloFloor As you are running your model for a longer time, the number of parameters of your model is increasing, thats the reason why you are running out of memory.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments for us to open this issue again. Thanks!"]}, {"number": 42256, "title": "How to avoid AddV2 op in TFLite model?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 18.04`\r\n- TensorFlow installed from (source or binary): `pip/binary`\r\n- TensorFlow version (or github SHA if from source):\r\n`2.3.0`\r\n`tflite 2.3.0 aar`\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf                                                 \r\nif not tf.__version__.startswith('1'):                                  \r\n    import tensorflow.compat.v1 as tf                                   \r\ntf.disable_v2_behavior()                                                \r\n# Path to the frozen graph file                                         \r\ngraph_def_file = 'weights/yolov5s.pb'                                   \r\n# A list of the names of the model's input tensors                      \r\ninput_arrays = ['images']                                               \r\n# A list of the names of the model's output tensors                     \r\noutput_arrays = ['Concat_442']                                          \r\n# Load and convert the frozen graph                                     \r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(                  \r\n    graph_def_file, input_arrays, output_arrays)                        \r\n# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n#                                        tf.lite.OpsSet.SELECT_TF_OPS]  \r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.allow_custom_ops = True                                     \r\n# converter.experimental_new_converter = False                            \r\ntflite_model = converter.convert()                                      \r\n# Write the converted model to disk                                     \r\nopen(\"weights/yolov5s.tflite\", \"wb\").write(tflite_model)                \r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nI try to convert a Tensorflow GraphDef model to TFLite model, and I want to void AddV2 op.\r\nIf I use `tf.lite.OpsSet.SELECT_TF_OPS` option, FlexAddV2 will be generated instead of AddV2, which I would like to avoid too.\r\nAddV2 is unsupported in tflite 2.3.0 aar.\r\nFlexAddV2 is supported by flex delegate, but cannot work on Android Emulator (Pixel 2 API 24) .\r\n\r\nThe last thing I tried is substituting 'AddV2' with 'Add' manually in pbtxt format (AddV2 is in the original GraphDef ).\r\nAfter I convert the processed GraphDef with tflite converter, the 'AddV2' ops emerge again.\r\n\r\nPS:\r\nhttps://github.com/tensorflow/tensorflow/issues/33393 says that TensorFlow 1.15 can resolve this problem.\r\n\r\n\r\n", "comments": ["@zldrobit \r\nPlease share complete information for us to replicate the issue faced, i ran the code shared and face a different issue, [gist](https://colab.research.google.com/gist/Saduf2019/f43423fa38f09670c520cd8ac731758c/untitled365.ipynb) for the same.", "@Saduf2019 I have uploaded the model to https://drive.google.com/file/d/1Z4jJ6DwP_A1o2fyN8oSMhEz6quRJ42Jf/view?usp=sharing\r\n\r\n", "@zldrobit \r\nI tried to access the link shared it says no preview available.", "@Saduf2019 you can click *Download* to download the file directly.", "hi @zldrobit \r\ni have a same issue~~\r\nand then test #33393 tensorflow 1.15.0 but have a trouble .\r\ntensorflow-addons hope tensorflow1.15.0 to tensorflow 2.x \r\n\r\nunder is command line log\r\n'''\r\nImportError: This version of TensorFlow Addons requires TensorFlow version >= 2; Detected an installation of version 1.15.0. Please upgrade TensorFlow to proceed.\r\n'''\r\n\r\nps . i install tensorflow-addons 0.5.0", "@zldrobit \r\nI ran the code on 2.3, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/9133c142c3231beb79c2ed6a86ae5330/untitled379.ipynb ), whereas in nightly i face an issue, [gist for nightly](https://colab.research.google.com/gist/Saduf2019/4d5120b5e5fc76c7329592bbeb4c89e7/untitled379.ipynb).\r\n\r\n@dan0978520303 \r\nCan you please try with later versions of tf and let us know if you still face any issues.", "> @dan0978520303\r\n> Can you please try with later versions of tf and let us know if you still face any issues.\r\n\r\ntf2.3 can work in my converter (pb -> tflite) but the graph appear AddV2 (have same trouble with author)\r\n\r\ni still find version (tf, onnx-tf, ... ) such that AddV2 becoming Add\r\n\r\nappreciation any advice !!!\r\n", "@Saduf2019 , I notice that you changed \r\n`converter.allow_custom_ops = False` to `converter.allow_custom_ops = True`.\r\nIt won't work if you use `converter.allow_custom_ops = False`.\r\n\r\n@dan0978520303, I find that TF v2.1.1 works in converting tf.AddV2 to tfl add.\r\n\r\nI conjuecture that something is wrong with v2.3.0 MLIR TFLite converter.", "> @dan0978520303, I find that TF v2.1.1 works in converting tf.AddV2 to tfl add.\r\n> \r\n> I conjuecture that something is wrong with v2.3.0 MLIR TFLite converter.\r\n\r\nhi! thank you replication~\r\nBut i run the tf2.1.1 still appear familiar situation (attr.value_case() == AttrValue::kType (1 vs. 6)) ps : tf2.3 , it can work \r\nCan i see your environment (pb to tflite file )?\r\nthx very much~~~\r\nI stuck over 5 days =(", "@dan0978520303  Of course.\r\nIt's just the [first thread](https://github.com/tensorflow/tensorflow/issues/42256#issue-677499982) I wrote in this issue .\r\nIt seems you are using the *OLD* TOCO converter, while the key is to turn the experimental converter switch *ON*:\r\n```\r\nconverter.experimental_new_converter = True\r\n```", "> @dan0978520303 Of course.\r\n> It's just the [first thread](https://github.com/tensorflow/tensorflow/issues/42256#issue-677499982) I wrote in this issue .\r\n> It seems you are using the _OLD_ TOCO converter, while the key is to turn the experimental converter switch _ON_:\r\n> \r\n> ```\r\n> converter.experimental_new_converter = True\r\n> ```\r\n\r\nthx~~~\r\nI add this '''converter.experimental_new_converter = True ''' to avoid  AttrValue::kType (1 vs. 6) problem =)\r\nBut i encounter this error\r\n'tfl.padv2' op failed to verify that operand 0's rank equals operand 1's size\r\n\r\nCould you encounter this error ?\r\nthx one more again !!!\r\n\r\n\r\n-----------------------------------------\r\nupdated,\r\n\r\nFinally, tf 2.2.0 is work !!!\r\n", "> @zldrobit\r\n\r\nI ran the same code shared by you in the issue template where \"converter.allow_custom_ops = True \" is true and did not face any issues, as shared above with you.\r\nCan you try the same, also with 2.2 and let us know if you still facing the issue.", "@Saduf2019 I can convert the model using either v2.1.1 or v2.2. ", "hi! @zldrobit \r\nDo you encounter an error : RuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 91 (ADD) failed to invoke. ?\r\nI run this test to try if the tflite file can work on the android ~\r\n#39790 this not work for me. \r\nIn Netron , I find the Add node is int64 after pb to tflite converter.\r\nBut i don't any idea to fix int64 to int32.\r\n\r\nthis is my code : \r\n\r\n----------------------------------------------------------------------------------\r\nimport tensorflow as tf\r\nimport numpy as np\r\ninterpreter = tf.lite.Interpreter(model_path=\"0824.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n\r\n----------------------------------------------------------------------------------\r\n\r\nthanks any advices ~~~", "@zldrobit \r\nIs this still an issue.", "@Saduf2019 It's not an issue for me now. I had a workaround to convert a model using TF v2.1.1/v2.2, and infer the model with TF v2.3.\r\n\r\n@dan0978520303 Have you tried to use the new experimental converter \r\n`converter.experimental_new_converter = False`?", "> @dan0978520303 Have you tried to use the new experimental converter\r\n> `converter.experimental_new_converter = False`?\r\n\r\nhi ! @zldrobit \r\nthank your replication~\r\nI have tried to change \"converter.experimental_new_converter = False\" and \"converter using TF v2.1.1/v2.2 and infer TF v2.3\".\r\nAbove, not work =(\r\n \r\nAnother way is inference in tf-nightly 2.4.0.dev20200824 but still error is : \r\n\r\n-------------------------------------------------------------------------\r\nRuntimeError: tensorflow/lite/kernels/add.cc:373 Type INT64 is unsupported by op Add.Node number 91 (ADD) failed to invoke.\r\n\r\n-------------------------------------------------------------------------\r\n\r\nIt still not work ~", "@dan0978520303 \r\nPlease provide complete stand alone(indented) code with system details or if possible share colab gist with error faced.", "> @dan0978520303\r\n> Please provide complete stand alone(indented) code with system details or if possible share colab gist with error faced.\r\n\r\nHi @Saduf2019 \r\nhere is my code :  \r\nhttps://colab.research.google.com/gist/dan0978520303/27c8e30a194a3195d343ae5023f8bfb6/question.ipynb\r\nps:\r\n\r\n1. my env is conda\r\n-------------------------------------------------------------------------\r\nonnx 1.6.0\r\ntensorflow 2.2.0 (converter)\r\ntf-nightly 2.4.0.dev20200824 (inference)\r\ntensorflow-addons 0.11.1\r\npytorch 1.4.0\r\npython  3.7.7\r\nonnx-tf 1.6.0 (pip install git+https://github.com/onnx/onnx-tensorflow.git)\r\n\r\n-------------------------------------------------------------------------\r\n\r\n2. my unet  model is from  :  \r\n-------------------------------------------------------------------------\r\nhttps://github.com/switchablenorms/CelebAMask-HQ/tree/master/face_parsing\r\n\r\n-------------------------------------------------------------------------\r\n\r\nI really appreciate any advice !!!\r\n", "@dan0978520303 \r\nI am unable to replicate your code, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e5b7df3c91c19673a8b30d88685bc2c2/untitled395.ipynb).", "> @dan0978520303\r\n> I am unable to replicate your code, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e5b7df3c91c19673a8b30d88685bc2c2/untitled395.ipynb).\r\n\r\nhi @Saduf2019 \r\nMy onnx model is too complex and then simplify the onnx model by onnx-simplifier.\r\nIt can work for me~\r\n\r\nHope this can help  =)", "@dan0978520303 \r\nCan you please provide a colab gist with the error reported, Can you please create a new issue with these details as the issue reported by user is resolved.", "> @dan0978520303\r\n> Can you please provide a colab gist with the error reported, Can you please create a new issue with these details as the issue reported by user is resolved.\r\n\r\nOK~\r\nLet me list my process to open new issue.\r\nBecause i first use github, my report maybe not good~ ", "@dan0978520303 \r\nPlease submit the new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the issue template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42256\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42256\">No</a>\n"]}, {"number": 42255, "title": "Seg fault after installing version 2.3.0", "body": "I test my tensorflow by using the official guide:\r\n`python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\r\n\r\nIt crashed with the info below:\r\n```\r\n2020-08-12 15:48:11.443235: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-08-12 15:48:11.443280: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\n2020-08-12 15:48:13.930797: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-08-12 15:48:13.930842: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-08-12 15:48:13.930871: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c4-miui-sec-preview0.bj): /proc/driver/nvidia/version does not exist\r\n2020-08-12 15:48:13.931321: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-12 15:48:13.946009: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2399965000 Hz\r\n2020-08-12 15:48:13.949467: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59646e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-12 15:48:13.949501: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\ntf.Tensor(-19.847946, shape=(), dtype=float32)\r\n\u6bb5\u9519\u8bef\r\n```\r\n**\u6bb5\u9519\u8bef means  seg fault**\r\nI also test with my own code in Python and it also crashed randomly. \r\n\r\nPython Ver: 3.6.5\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["You might want to share the specs of your environment, otherwise no one is able to reproduce.\r\n", "I am not in the account of ROOT, so I suppose I have use some functions which can not be used by non-administrator.\r\n\r\nI run iPython in virtual environment.\r\n\r\n(```\r\nvenv3.6) [work@c4-miui-sec-preview0 yanan]$ python3 --version\r\nPython 3.6.5\r\n(venv3.6) [work@c4-miui-sec-preview0 yanan]$ pip3 --version\r\n\r\npip 20.2.1 from /home/work/bigdata/venv3.6/lib/python3.6/site-packages/pip (python 3.6)\r\n(venv3.6) [work@c4-miui-sec-preview0 yanan]$ virtualenv --version\r\nvirtualenv 20.0.30 from /home/work/.pyenv/versions/2.7.9/lib/python2.7/site-packages/virtualenv/__init__.pyc\r\n```\r\nand here is the pip list:\r\n\r\n\r\n```\r\nPackage                Version\r\n---------------------- --------------\r\n-loud-ml-sdk           0.3.34\r\nabsl-py                0.7.1\r\nargcomplete            1.10.0\r\nasdf                   2.7.0\r\nasn1crypto             0.24.0\r\nastor                  0.7.1\r\nastunparse             1.6.3\r\nattrs                  19.3.0\r\nbazel                  0.0.0.20200723\r\ncachetools             4.1.1\r\ncertifi                2019.3.9\r\ncffi                   1.12.3\r\nchardet                3.0.4\r\ncloud-ml-common        0.2.3\r\ncloud-ml-sdk           0.4.13\r\ncloudpickle            1.2.0\r\nconda                  4.3.16\r\ncPython                0.0.5\r\ncryptography           2.7\r\ncycler                 0.10.0\r\nCython                 0.29.13\r\ndask                   1.2.2\r\ndecorator              4.4.0\r\ndocker                 4.3.0\r\nfire                   0.1.3\r\ngalaxy-fds-sdk         1.4.21\r\ngast                   0.3.3\r\ngoogle-auth            1.20.1\r\ngoogle-auth-oauthlib   0.4.1\r\ngoogle-pasta           0.1.8\r\ngrpcio                 1.20.1\r\nh5py                   2.10.0\r\nidna                   2.7\r\nimageio                2.4.1\r\nimportlib-metadata     1.7.0\r\ninflection             0.4.0\r\nipython                5.1.0\r\nipython-genutils       0.2.0\r\njieba                  0.39\r\njsonschema             3.2.0\r\nKeras                  2.4.3\r\nKeras-Applications     1.0.8\r\nKeras-Preprocessing    1.1.2\r\nkiwisolver             1.1.0\r\nlxml                   4.2.6\r\nMarkdown               3.1\r\nmatplotlib             3.0.3\r\nmock                   3.0.4\r\nnetaddr                0.7.19\r\nnetworkx               2.3\r\nnltk                   3.4\r\nnumpy                  1.18.5\r\noauthlib               3.1.0\r\nopencv-python          4.1.1.26\r\nopt-einsum             3.3.0\r\npandas                 0.24.2\r\npexpect                4.8.0\r\npickleshare            0.7.5\r\nPillow                 6.0.0\r\npip                    20.2.1\r\nprogressbar2           3.38.0\r\nprompt-toolkit         1.0.18\r\nprotobuf               3.12.4\r\nptyprocess             0.6.0\r\npyasn1                 0.4.8\r\npyasn1-modules         0.2.8\r\npycosat                0.6.3\r\npycparser              2.19\r\nPygments               2.6.1\r\npymongo                3.9.0\r\npyOpenSSL              19.0.0\r\npyparsing              2.4.0\r\npyrsistent             0.16.0\r\npython-dateutil        2.8.0\r\npython-utils           2.3.0\r\npytz                   2019.1\r\nPyWavelets             1.0.3\r\nPyYAML                 5.1.1\r\nrequests               2.24.0\r\nrequests-oauthlib      1.3.0\r\nrsa                    4.6\r\nruamel.yaml            0.16.10\r\nruamel.yaml.clib       0.2.0\r\nscikit-image           0.14.2\r\nscikit-learn           0.20.3\r\nscipy                  1.4.1\r\nsemantic-version       2.8.5\r\nsetuptools             41.0.0\r\nShapely                1.6.4.post2\r\nsimplegeneric          0.8.1\r\nsingledispatch         3.4.0.3\r\nsix                    1.12.0\r\nsklearn                0.0\r\ntensorboard            2.2.2\r\ntensorboard-plugin-wit 1.7.0\r\ntensorflow             2.2.0\r\ntensorflow-estimator   2.2.0\r\ntermcolor              1.1.0\r\ntest1                  1.0\r\ntoolz                  0.9.0\r\ntqdm                   4.27.0\r\ntraitlets              4.3.3\r\nUNKNOWN                0.0.0\r\nurllib3                1.23\r\nwcwidth                0.2.5\r\nwebsocket-client       0.57.0\r\nWerkzeug               0.15.2\r\nwheel                  0.33.1\r\nwrapt                  1.11.2\r\nxlrd                   1.2.0\r\nzipp                   3.1.0\r\n```", "I changed the version of tf to 2.2.0, as shown above. \r\nThe issue is still there.\r\n", "@yananchen1989 \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Can you upgrade `scipy` to 1.5.2?", "That being said, can you create a new virtualenvironment and install there instead? If you need `scipy`, make sure it is newer than 1.4.1", "@mihaimaruseac Upgrading `scipy` to 1.5.2, `pip` throws:\r\n`ERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.5.2 which is incompatible.`\r\n\r\nEither way with `scipy 1.4.1` or `1.5.2`, the error doesn't seem to go away for me. \r\nI'm on a mac with `pip install tensorflow==2.3.0 `.", "Can you install in a new virtualenv and then post the output of `pip list`?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42255\">No</a>\n", "I meet the same problem. I upgrade the OS from Debian 8 to Debian 9 and no seg fault occurs."]}, {"number": 42254, "title": "tf.signal.stft throws RuntimeException when pad_end=True", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.8 (Conda)\r\n\r\n**Describe the current behavior**\r\n\r\n`pad_end` of `tf.signal.stft` throws `RuntimeError` when set to `True`.\r\n\r\n**Describe the expected behavior**\r\n\r\nShould not throw a `RuntimeException` - just pad the end of the signal.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\n\r\ndef main():\r\n    inputs = layers.Input(shape=(None,))\r\n    x = tf.signal.stft(inputs, 512, 20, pad_end=True)\r\n    model = keras.Model(inputs=inputs, outputs=x)\r\n    signals = tf.constant(np.random.rand(2, 511))\r\n    print(model(signals))\r\n    print('All done.')\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs** \r\n\r\n```none\r\nTraceback (most recent call last):\r\n  File \"/home/sfalk/tmp/speech-v2/asr/bin/tmp.py\", line 17, in <module>\r\n    main()\r\n  File \"/home/sfalk/tmp/speech-v2/asr/bin/tmp.py\", line 9, in main\r\n    x = tf.signal.stft(inputs, 512, 20, pad_end=True)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/signal/spectral_ops.py\", line 86, in stft\r\n    framed_signals = shape_ops.frame(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/signal/shape_ops.py\", line 162, in frame\r\n    paddings = array_ops.concat(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 1654, in concat\r\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1221, in concat_v2\r\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 409, in _apply_op_helper\r\n    values = ops.internal_convert_n_to_tensor(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1561, in internal_convert_n_to_tensor\r\n    convert_to_tensor(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1465, in convert_to_tensor\r\n    raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n### Workaround\r\n\r\nDo the padding yourself:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\n\r\ndef main():\r\n    frame_length = 512\r\n    inputs = layers.Input(shape=(None,))\r\n    x = inputs\r\n\r\n    pad = frame_length - tf.math.mod(tf.shape(x)[1], frame_length)\r\n    x = tf.pad(x, [(0, 0), (0, pad)])\r\n    x = tf.signal.stft(x, 512, 20, pad_end=False)\r\n\r\n    model = keras.Model(inputs=inputs, outputs=x)\r\n    signals = tf.constant(np.random.rand(2, 511))\r\n    print(model(signals))\r\n    print('All done.')\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "comments": ["A workaround is to wrap the function with lambda layer:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\n\r\ndef main():\r\n    inputs = layers.Input(shape=(None,))\r\n    stft = layers.Lambda(lambda x: tf.signal.stft(x, 512, 20, pad_end=True))(inputs)\r\n    model = keras.Model(inputs=inputs, outputs=stft)\r\n    signals = tf.constant(np.random.rand(2, 512))\r\n    print(model(signals))\r\n    print('All done.')\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nhttps://colab.research.google.com/drive/1l4OYzh4sy-6d9BAszCC39e9B6uFYRpHD?usp=sharing", "@WindQAQ I am not sure if this is an option in my case. In my case I would have to wrap the entire preprocessing pipeline in a keras layer; But that might not be a bad idea after all..\r\n\r\nAlso it's possible to just pad it manually (see my original post).", "I am able to replicate this issue, please have a look at the [gist here](https://colab.research.google.com/gist/Saduf2019/2207fbafc2e0bb6a2d763eb24db01746/untitled364.ipynb). Thanks!", "@stefan-falk \r\nI made a change to your code and do not see the error, please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/39afa070a5feb386627085bddf1d8f69/untitled364.ipynb) and update.", "Hi @stefan-falk, can you try to modify this line\r\nhttps://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/ops/signal/shape_ops.py#L164\r\nto\r\n```python\r\n           ops.convert_to_tensor([[0, pad_samples]]),\r\n```\r\nto see if it fixes the error?", "@WindQAQ I tested it in a debug session an can confirm that it works if we use `ops.convert_to_tensor([[0, pad_samples]])`\r\n\r\n![image](https://user-images.githubusercontent.com/43335432/89991286-e6d88f00-dc83-11ea-9def-f5ec527a8a41.png)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42254\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42254\">No</a>\n"]}, {"number": 42253, "title": "Dose quantization aware training support tf.GradientTape() training?", "body": "I'm using **tensorflow-gpu==2.3.0**, and I run below code for quantization aware training, seems that using tf.GradientTape() do nothing when run quantization aware training. Here is useful code which is tensorflow example [code](https://www.tensorflow.org/model_optimization/guide/quantization/training_example):\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\n# Load MNIST dataset\r\n# Mine network to download this is too slow, so I download it handly. You can also use this command line:\r\n\r\n# mnist = keras.datasets.mnist\r\n# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\ndef load_mnist(path):\r\n    with np.load(path, allow_pickle=True) as f:\r\n        x_train, y_train = f['x_train'], f['y_train']\r\n        x_test, y_test = f['x_test'], f['y_test']\r\n\r\n        return (x_train, y_train), (x_test, y_test)\r\n(train_images, train_labels), (test_images, test_labels) = load_mnist(path='mnist.npz')\r\n\r\n# Normalize the input image so that each pixel value is between 0 to 1.\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\n# Define the model architecture.\r\nmodel = keras.Sequential([\r\n  keras.layers.InputLayer(input_shape=(28, 28)),\r\n  keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\r\n  keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n  keras.layers.Flatten(),\r\n  keras.layers.Dense(10)\r\n])\r\n\r\n# Train the digit classification model\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(\r\n  train_images,\r\n  train_labels,\r\n  epochs=1,\r\n  validation_split=0.1,\r\n)\r\n\r\nimport tensorflow_model_optimization as tfmot\r\n\r\nquantize_model = tfmot.quantization.keras.quantize_model\r\n\r\n# q_aware stands for for quantization aware.\r\nq_aware_model = quantize_model(model)\r\n\r\n# `quantize_model` requires a recompile.\r\nq_aware_model.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nq_aware_model.summary()\r\ntrain_images_subset = train_images[0:1000] # out of 60000\r\ntrain_labels_subset = train_labels[0:1000]\r\nq_aware_model.fit(train_images_subset, train_labels_subset,\r\n                  batch_size=500, epochs=1, validation_split=0.1)\r\n_, baseline_model_accuracy = model.evaluate(\r\n    test_images, test_labels, verbose=0)\r\n\r\n_, q_aware_model_accuracy = q_aware_model.evaluate(\r\n   test_images, test_labels, verbose=0)\r\n\r\nprint('Baseline test accuracy:', baseline_model_accuracy)\r\nprint('Quant test accuracy:', q_aware_model_accuracy)\r\n```\r\nwhich you will get this output finally:\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nquantize_layer (QuantizeLaye (None, 28, 28)            3         \r\n_________________________________________________________________\r\nquant_reshape (QuantizeWrapp (None, 28, 28, 1)         1         \r\n_________________________________________________________________\r\nquant_conv2d (QuantizeWrappe (None, 26, 26, 12)        147       \r\n_________________________________________________________________\r\nquant_max_pooling2d (Quantiz (None, 13, 13, 12)        1         \r\n_________________________________________________________________\r\nquant_flatten (QuantizeWrapp (None, 2028)              1         \r\n_________________________________________________________________\r\nquant_dense (QuantizeWrapper (None, 10)                20295     \r\n=================================================================\r\nTotal params: 20,448\r\nTrainable params: 20,410\r\nNon-trainable params: 38\r\n_________________________________________________________________\r\n2/2 [==============================] - 0s 98ms/step - loss: 0.1444 - accuracy: 0.9589 - val_loss: 0.1748 - val_accuracy: 0.9800\r\nBaseline test accuracy: 0.9613000154495239\r\nQuant test accuracy: 0.961899995803833\r\n```\r\n\r\nIf using tf.GradientTape() for quant-aware-training (using same `model` trained before):\r\n```\r\n# q_aware stands for for quantization aware.\r\nq_aware_model2 = quantize_model(model)\r\n\r\n# `quantize_model` requires a recompile.\r\nq_aware_model2.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nq_aware_model2.summary()\r\n\r\nbatch_size = 500\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images_subset, train_labels_subset))\r\ntrain_dataset = train_dataset.batch(batch_size=batch_size, drop_remainder=False)\r\n\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\nfor epoch in range(1):\r\n    for x, y in train_dataset:\r\n        with tf.GradientTape() as tape:\r\n            preds = q_aware_model2(x)\r\n            loss = loss_fn(y, preds)\r\n        grads = tape.gradient(loss, q_aware_model2.trainable_variables)\r\n        optimizer.apply_gradients(zip(grads, q_aware_model2.trainable_variables))\r\n        \r\n_, baseline_model_accuracy = model.evaluate(\r\n    test_images, test_labels, verbose=0)\r\n\r\n_, q_aware_model_accuracy = q_aware_model2.evaluate(\r\n   test_images, test_labels, verbose=0)\r\n\r\nprint('Baseline test accuracy:', baseline_model_accuracy)\r\nprint('Quant test accuracy:', q_aware_model_accuracy)\r\n```\r\nyou will get output like this:\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nquantize_layer_5 (QuantizeLa (None, 28, 28)            3         \r\n_________________________________________________________________\r\nquant_reshape (QuantizeWrapp (None, 28, 28, 1)         1         \r\n_________________________________________________________________\r\nquant_conv2d (QuantizeWrappe (None, 26, 26, 12)        147       \r\n_________________________________________________________________\r\nquant_max_pooling2d (Quantiz (None, 13, 13, 12)        1         \r\n_________________________________________________________________\r\nquant_flatten (QuantizeWrapp (None, 2028)              1         \r\n_________________________________________________________________\r\nquant_dense (QuantizeWrapper (None, 10)                20295     \r\n=================================================================\r\nTotal params: 20,448\r\nTrainable params: 20,410\r\nNon-trainable params: 38\r\n_________________________________________________________________\r\nBaseline test accuracy: 0.9613000154495239\r\nQuant test accuracy: 0.11349999904632568\r\n```\r\nAnd the `q_aware_model2` results is soo smaller than `q_aware_model` results. By the way, the  `q_aware_model2` results is as same as when you did not do `tf.GradientTape()` training:\r\n```\r\n# q_aware stands for for quantization aware.\r\nq_aware_model2 = quantize_model(model)\r\n\r\n# `quantize_model` requires a recompile.\r\nq_aware_model2.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nq_aware_model2.summary()\r\n\r\nbatch_size = 500\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images_subset, train_labels_subset))\r\ntrain_dataset = train_dataset.batch(batch_size=batch_size, drop_remainder=False)\r\n\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\n# for epoch in range(1):\r\n#     for x, y in train_dataset:\r\n#         with tf.GradientTape() as tape:\r\n#             preds = q_aware_model2(x)\r\n#             loss = loss_fn(y, preds)\r\n#         grads = tape.gradient(loss, q_aware_model2.trainable_variables)\r\n#         optimizer.apply_gradients(zip(grads, q_aware_model2.trainable_variables))\r\n        \r\n_, baseline_model_accuracy = model.evaluate(\r\n    test_images, test_labels, verbose=0)\r\n\r\n_, q_aware_model_accuracy = q_aware_model2.evaluate(\r\n   test_images, test_labels, verbose=0)\r\n\r\nprint('Baseline test accuracy:', baseline_model_accuracy)\r\nprint('Quant test accuracy:', q_aware_model_accuracy)\r\n```\r\nYou will get same output as using `tf.GradientTape()` training. Seems like `tf.GradientTape()` do nothing when `quantization-aware-training`:\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nquantize_layer_6 (QuantizeLa (None, 28, 28)            3         \r\n_________________________________________________________________\r\nquant_reshape (QuantizeWrapp (None, 28, 28, 1)         1         \r\n_________________________________________________________________\r\nquant_conv2d (QuantizeWrappe (None, 26, 26, 12)        147       \r\n_________________________________________________________________\r\nquant_max_pooling2d (Quantiz (None, 13, 13, 12)        1         \r\n_________________________________________________________________\r\nquant_flatten (QuantizeWrapp (None, 2028)              1         \r\n_________________________________________________________________\r\nquant_dense (QuantizeWrapper (None, 10)                20295     \r\n=================================================================\r\nTotal params: 20,448\r\nTrainable params: 20,410\r\nNon-trainable params: 38\r\n_________________________________________________________________\r\nBaseline test accuracy: 0.9613000154495239\r\nQuant test accuracy: 0.11349999904632568\r\n```", "comments": ["still need help. Could anyone can fix this?", "one week passed, still no answer.", "@murdockhou Sorry for the late response. we are looking into it. Thanks!", "@jvishnuvardhan Any progress?", "@jvishnuvardhan Any progress?", "Is there anyone who can solve this? I have waiting for long time.", "There is a bug in your code.\r\n\r\nPlease use `preds = q_aware_model2(x, training=True)` instead of `preds = q_aware_model2(x)`. Because you are not building the training graph, the gradient tape does not do anything.\r\n\r\nFor QAT related issues, please file them in the Model Optimization Toolkit [repo](https://github.com/tensorflow/model-optimization). We keep a closer tab for MOT related issues there.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42253\">No</a>\n"]}, {"number": 42251, "title": "[Toco]freeze Min Max information by Post-Quantization", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): tf1.14\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n\r\n```\r\nusage: tensorflow/bazel-bin/tensorflow/lite/toco/toco\r\nFlags:\r\n\t--input_array=\"\"                 \tstring\tDeprecated: use --input_arrays instead. Name of the input array. If not specified, will try to read that information from the input file.\r\n\t--input_arrays=\"\"                \tstring\tNames of the input arrays, comma-separated. If not specified, will try to read that information from the input file.\r\n\t--output_array=\"\"                \tstring\tDeprecated: use --output_arrays instead. Name of the output array, when specifying a unique output array. If not specified, will try to read that information from the input file.\r\n\t--output_arrays=\"\"               \tstring\tNames of the output arrays, comma-separated. If not specified, will try to read that information from the input file.\r\n\t--input_shape=\"\"                 \tstring\tDeprecated: use --input_shapes instead. Input array shape. For many models the shape takes the form batch size, input array height, input array width, input array depth.\r\n\t--input_shapes=\"\"                \tstring\tShapes corresponding to --input_arrays, colon-separated. For many models each shape takes the form batch size, input array height, input array width, input array depth.\r\n\t--batch_size=1                   \tint32\tDeprecated. Batch size for the model. Replaces the first dimension of an input size array if undefined. Use only with SavedModels when --input_shapes flag is not specified. Always use --input_shapes flag with frozen graphs.\r\n\t--input_data_type=\"\"             \tstring\tDeprecated: use --input_data_types instead. Input array type, if not already provided in the graph. Typically needs to be specified when passing arbitrary arrays to --input_arrays.\r\n\t--input_data_types=\"\"            \tstring\tInput arrays types, comma-separated, if not already provided in the graph. Typically needs to be specified when passing arbitrary arrays to --input_arrays.\r\n\t--mean_value=0.000000            \tfloat\tDeprecated: use --mean_values instead. mean_value parameter for image models, used to compute input activations from input pixel data.\r\n\t--mean_values=\"\"                 \tstring\tmean_values parameter for image models, comma-separated list of doubles, used to compute input activations from input pixel data. Each entry in the list should match an entry in --input_arrays.\r\n\t--std_value=1.000000             \tfloat\tDeprecated: use --std_values instead. std_value parameter for image models, used to compute input activations from input pixel data.\r\n\t--std_values=\"\"                  \tstring\tstd_value parameter for image models, comma-separated list of doubles, used to compute input activations from input pixel data. Each entry in the list should match an entry in --input_arrays.\r\n\t--variable_batch=false           \tbool\tIf true, the model accepts an arbitrary batch size. Mutually exclusive with the 'batch' field: at most one of these two fields can be set.\r\n\t--rnn_states=\"\"                  \tstring\t\r\n\t--model_checks=\"\"                \tstring\tA list of model checks to be applied to verify the form of the model.  Applied after the graph transformations after import.\r\n\t--dump_graphviz=\"\"               \tstring\tDump graphviz during LogDump call. If string is non-empty then it defines path to dump, otherwise will skip dumping.\r\n\t--dump_graphviz_video=false      \tbool\tIf true, will dump graphviz at each graph transformation, which may be used to generate a video.\r\n\t--allow_nonexistent_arrays=false \tbool\tIf true, will allow passing inexistent arrays in --input_arrays and --output_arrays. This makes little sense, is only useful to more easily get graph visualizations.\r\n\t--allow_nonascii_arrays=false    \tbool\tIf true, will allow passing non-ascii-printable characters in --input_arrays and --output_arrays. By default (if false), only ascii printable characters are allowed, i.e. character codes ranging from 32 to 127. This is disallowed by default so as to catch common copy-and-paste issues where invisible unicode characters are unwittingly added to these strings.\r\n\t--arrays_extra_info_file=\"\"      \tstring\tPath to an optional file containing a serialized ArraysExtraInfo proto allowing to pass extra information about arrays not specified in the input model file, such as extra MinMax information.\r\n\t--model_flags_file=\"\"            \tstring\tPath to an optional file containing a serialized ModelFlags proto. Options specified on the command line will override the values in the proto.\r\n\t--change_concat_input_ranges=true\tbool\tBoolean to change the behavior of min/max ranges for inputs and output of the concat operators.\r\n```\r\n\r\n**TODO**\r\n\r\nI try to use toco to freeze MinMax information into my model, but the default_ranges decrease the accuracy of model.\r\n```\r\n\t--default_ranges_min=0.000000    \t\r\n\t--default_ranges_max=6.000000 \r\n```\r\nHow to create a file for (--arrays_extra_info_file=\"\") so that  can set the Min Max information in the process.\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42251\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42251\">No</a>\n"]}, {"number": 42250, "title": "What means 'the resulting model is quantization aware but not quantized'", "body": "Can anybody explain me what means 'the resulting model is quantization aware but not quantized' on [tutorial](https://www.tensorflow.org/model_optimization/guide/quantization/training_example?hl=en)\r\nIs there no way to use Quantization-Aware-Training without tflite?\r\n\r\nThank you.", "comments": ["@wptkddms \r\nCould you please point out where in the tutorial it says that the model is quantization aware but not quantized?\r\n\r\nQAT is done [model is typically trained ] to compensate for the loss in precision that might be introduced due to quantization. (When you reduce the precision of the parameters of your model, it can result in information loss and you might see some reduction in the accuracy of your model. In these situations, quantization-aware training can be really helpful.)\r\n", "@Saduf2019 \r\nThank you for reply\r\n\r\n[here](https://www.tensorflow.org/model_optimization/guide/quantization/training_example?hl=en#define_the_model) is the sentence.\r\nUsing the tutorial, I printed the weights of q_aware_model, but the data type was float32, not int8. So I'm confused.\r\n\r\n", "@wptkddms P\r\nPlease refer to [this link](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html)  ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 42249, "title": "RuntimeError: tensorflow/lite/kernels/conv.cc:316 input->dims->data[3] != filter->dims->data[3] (4 != 3)Node number 1 (CONV_2D) failed to prepare.", "body": "ubuntu18.04\r\ntf-nightly-gpu : '2.4.0-dev20200811'\r\nnumpy  :  1.18.3\r\n\r\nI follow the official website to convert the file (https://www.tensorflow.org/lite/performance/post_training_integer_quant)\r\nThe first step is to convert .h5 files to tflite files(success!!),but When performing integer quantization This error occurred\uff1a\r\n\r\n[RuntimeError: tensorflow/lite/kernels/conv.cc:316 input->dims->data[3] != filter->dims->data[3] (4 != 3)Node number 1 (CONV_2D) failed to prepare.]\r\n\r\nMy model:\r\n[hdf5.zip](https://github.com/tensorflow/tensorflow/files/5060690/hdf5.zip)\r\n[tflite.zip](https://github.com/tensorflow/tensorflow/files/5060695/tflite.zip)\r\n\r\nOne of my training materials:\r\n[train_img.zip](https://github.com/tensorflow/tensorflow/files/5060699/train_img.zip)\r\n", "comments": ["@LINYOUWEI0804 \r\n\r\nRequest you to share colab link or stand alone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "My training program\uff1a\r\n[train.zip](https://github.com/tensorflow/tensorflow/files/5061210/train.zip)\r\n", "@LINYOUWEI0804 Looks like there is a mismatch in the data shape. You can check `model.summary` that `input_shape` of image is (1, 224, 224, 3) whereas `xtrain.shape` in `representative_data_gen` is (1, 224, 224, **4**). \r\n\r\nCan you please update the shape of `representative_data_gen` and then convert the model. As error is clearly mentioning there is a mismatch in the shape, i think this will work for you. Thanks!\r\n\r\nPlease verify once and close the issue if this resolved the issue for you. Thanks!", "I am totally new to this environment and faced the same problem\r\nmay I know if you could solve your problem?", " tensorflow/lite/kernels/conv.cc:313 input->dims->size != 4 (3 != 4)Node number 1 (CONV_2D) failed to prepare.\r\n\r\nthe problem is that I dont have conv2D at all\r\n\r\n", "@giahi Can you please create a new issue with a simple standalone code to reproduce the error? Thanks!", "My problem is that 3 dimensions are created when building the model, like this \r\nmodel = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(224 ,224 ,3)), but The data set I provided during the integer quantification is in png format. The 4 dimensions are different from the 3 dimensions of the model."]}, {"number": 42248, "title": "tf.keras.backend.reverse crashes (floating point exception) when first dimension of `x` is 0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.keras.backend.reverse` crashes (segfault) when first dimension of `x` is 0\r\n**Describe the expected behavior**\r\nexpect no crashes\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.keras.backend.reverse(x=np.ndarray(shape=[0, 1, 1]), axes=1)\r\n~~~\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n~~~python\r\nFloating point exception (core dumped)\r\n~~~", "comments": ["Added PR #42334 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42248\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42248\">No</a>\n"]}, {"number": 42247, "title": "tf.random.gamma crashes(segfault) when `alpha`'s constraint is violated", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.random.gamma` crashes(segfault) when `alpha` is an integer instead of a `dtype`\r\n**Describe the expected behavior**\r\nexpect no crashes except invalid input\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n~~~python\r\nimport tensorflow as tf\r\ntf.random.gamma(shape = [100, 100, 100, 100, 100], alpha=1)\r\n~~~\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["The datatype of `alpha` is changed to `float` under the hood.\r\nSee https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/ops/random_ops.py#L556\r\nThe seg fault is caused to a complex shape `[100, 100, 100, 100, 100]`.", "The failure is because of the huge memory that this shape would allocate. If you make the shape smaller (say [100, 100, 100]), then it works.\r\n\r\nrunning it my machine, I get \r\n\r\n2020-08-31 10:16:37.387486: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 40000000000 exceeds 10% of free system memory.\r\n\r\nbefore it segfaults.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42247\">No</a>\n"]}, {"number": 42246, "title": "tf.signal.inverse_stft segfault when frame_length is a large value", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.signal.inverse_stft` segfault when `frame_length` is a large value.\r\n\r\n**Describe the expected behavior**\r\nexpect no crashes\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n~~~python\r\nimport tensorflow as tf\r\nstfts = tf.ones((1,1), dtype=tf.complex64)\r\ntf.signal.inverse_stft(stfts=stfts, frame_length=2700000000, frame_step=1)\r\n~~~\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n~~~python\r\nSegmentation fault (core dumped)\r\n~~~", "comments": ["@DNXie \r\n\r\nIf we change frame_length=27000 i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a31bfaa703ddd34548862e7860baf762/untitled242.ipynb).Thanks!", "@ravikyram I agree. But the function shouldn't crash with invalid input. ", "How much memory does the host running the snippet have? I suspect [this pad](https://github.com/tensorflow/tensorflow/blob/9ea2adeb41131b2af704d8ceee96b7c771d220ad/tensorflow/python/ops/signal/spectral_ops.py#L264-L266) may just be running out of memory.\r\n\r\nSeparate from the seg fault, the frame_length of 2700000000 is just large enough to require a tf.int64 dtype, but windows are assumed to have lengths [representable by tf.int32](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/signal/window_ops.py).", "@dthkao A memory check instead of crash would be better! :)", "Was able to replicate the issue in TF 2.6.0-dev20210528,please find the gist[ here ](https://colab.research.google.com/gist/sushreebarsa/fe623ab6a29572e7ccb9bb2ce208b2ca/untitled70.ipynb#scrollTo=aKxtUiBRx_Ic)..Thanks !", "@DNXie When i ran with recent `tf-nightly`,  the code is not crashing. It is throwing an error as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/5723b47e5c66e33bc4c82df8d3e68fb9/untitled242.ipynb). Thanks!\r\n\r\nThe following is an error trace.\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-3-495cfd7046bc> in <module>()\r\n      1 import tensorflow as tf\r\n      2 stfts = tf.ones((1,1), dtype=tf.complex64)\r\n----> 3 tf.signal.inverse_stft(stfts=stfts, frame_length=2700000000, frame_step=1)\r\n\r\n5 frames\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Obtained a FFT shape of 0 elements: [1,0] [Op:IRFFT]\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42246\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42246\">No</a>\n"]}, {"number": 42245, "title": "Corrected Typo", "body": "Fixes #https://github.com/tensorflow/tensorflow/issues/42217", "comments": []}, {"number": 42244, "title": "Using custom, non-trainable layers in the middle of Keras model yields gradient error", "body": "### Problem\r\nI am writing a GAN-style model which uses an untrained / weightless layer in the middle of the GAN (a transformation is performed on the generator output which is then fed into the discriminator). The transformation operation is done with a custom layer, but I'm getting the following error at runtime when trying to train (after compiling both the discriminator and GAN model):\r\n\r\n```bash\r\nValueError: No gradients provided for any variable: ['dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0'].\r\n```\r\n\r\n### Code\r\nMy **Custom Layer** code is as follows (is meant to convert top left, bottom right coordinates into a square shape):\r\n```python\r\nclass ImagePaste(tf.keras.layers.Layer):\r\n    def __init__(self,  **kwargs):\r\n        super(ImagePaste, self).__init__(**kwargs)\r\n        self.canvas_size = 72\r\n\r\n    def build(self, input_shape):\r\n        self.batch_s = input_shape[0]\r\n\r\n        super(ImagePaste, self).build(input_shape)\r\n    \r\n    def call(self, input_data, t_val=255):\r\n        positions = tf.convert_to_tensor(input_data)\r\n        positions = tf.reshape(positions, [-1, 2, 2])\r\n\r\n        canvas = tf.Variable(tf.ones([self.batch_s, 72, 72, 3]) * t_val,\r\n                             dtype=tf.float32,\r\n                             trainable=False, validate_shape=True)\r\n\r\n        for i in tf.range(0, self.batch_s):\r\n            color = tf.convert_to_tensor(np.eye(3)[np.random.choice(3)] * 250, dtype=tf.float32)\r\n            tl, br = tf.cast(positions[i], dtype=tf.int64)\r\n\r\n            for r in tf.range(min(tl[0], self.canvas_size), min(br[0], self.canvas_size)):\r\n                for c in tf.range(min(tl[1], self.canvas_size), min(br[1], self.canvas_size)):\r\n                    canvas[i, r, c, :].assign(color)\r\n\r\n        return tf.convert_to_tensor(canvas)\r\n```\r\n\r\nMy **Generator** code is as follows (turns latent space into top left, bottom right square coordinates then image of square on a canvas):\r\n```python\r\n    def build_composer(self):\r\n        latent_input = Input(shape=100, batch_size=64)\r\n\r\n        p = Dense(units=128)(latent_input)\r\n        p = Dense(units=256)(p)\r\n        p = Dense(units=512)(p)\r\n        p = Dense(units=1024)(p)\r\n\r\n        out = Dropout(0.4)(p)\r\n        out = Dense(units=4, activation='relu')(out)  # TOP LEFT, BOTTOM RIGHT COORDINATE\r\n\r\n        composed_image = ImagePaste(trainable=False, dynamic=True)(out)\r\n\r\n        return tf.keras.Model(inputs=latent_input, outputs=composed_image)\r\n```\r\n\r\nMy **Generator + Discriminator (GAN)** code is as follows:\r\n```python\r\n    def build_full_model(self, composer, discriminator):\r\n        latent_input = Input(shape=100, batch_size=64)\r\n\r\n        composed_image = composer(latent_input)\r\n\r\n        discriminator.trainable = False\r\n        valid = discriminator(composed_image)\r\n\r\n        return tf.keras.Model(inputs=latent_input, outputs=valid)\r\n```\r\n\r\n**And the error shown above occurs when calling ``` .predict() ``` on the compiled GAN model.**\r\n\r\nIt seems like my ImagePaste Layer is preventing gradients from reaching the generator layers -- how can I get the gradient calculation to ignore the layer (even though its already set to trainable = False) when training? Can anyone help me solve this? Thank you very much.", "comments": ["@micahreich \r\nPlease fill the issue template, it helps us to replicate the issue on the tf version you have faced the error on along with the system information.\r\n\r\nPlease provide complete code such that we can replicate the issue(with all dependencies) or if possible share a colab gist with error faced.\r\n\r\nWith respect to the error reported, refer to below resolved issues with same error:\r\n#39169 #41162 #42038 [link](https://stackoverflow.com/questions/41689451/valueerror-no-gradients-provided-for-any-variable) [link1](https://stackoverflow.com/questions/49289930/tensorflow-no-gradients-provided-for-any-variable?rq=1) ", "I created a new issue according to the correct template @Saduf2019:\r\n[Here is the new issue](https://github.com/tensorflow/tensorflow/issues/42311)\r\n\r\nIm sorry for any confusion, thank you.", "@micahreich \r\nMoving this issue to closed status as #42311 would track the issue, also please let us know if you followed the links and issues shared.", "Thanks -- I looked at the links you posted but I wasn't able to make a fix based on those threads"]}, {"number": 42243, "title": "How can i solve data transferred from slave to master using SPI?", "body": "I tried to make a communication between two AVR (ATmega128) using SPI. Data transferred correctly from master to slave but data transferred wrong from slave to master, the first sampled bit is always wrong. Slave sends (0X7E) to master but the received data is (0X3F). Where is the mistake?\r\n\r\nCode of MASTER\r\n\r\n#define F_CPU 16000000UL\r\n\r\n#include <avr/io.h>\r\n#include <util/delay.h>\r\n\r\n#define DDR_SPI DDRB\r\n#define DD_MOSI PB2\r\n#define DD_SCK PB1\r\n#define DD_SS PB0\r\n\r\n#define ACK 0x7E\r\n\r\nvoid SPI_MasterInit(void)\r\n{\r\n\t/* Set MOSI and SCK output */\r\n\tDDR_SPI |= (1<<DD_MOSI) | (1<<DD_SCK) | (1<<DD_SS);\r\n\tPORTB |= 1;\r\n\t/* Enable SPI, Master, set clock rate fck/128 */\r\n\tSPCR |= (1<<SPE)|(1<<MSTR)|(1<<SPR0)|(1<<SPR1);\r\n}\r\n\r\nunsigned char SPI_MasterTransmit(unsigned char cData)\r\n{\r\n\tPORTB &= ~(1<<0);\r\n\twhile( (PORTB & (1<<0)) );\r\n\t/* Start transmission */\r\n\tSPDR = cData;\r\n\t/* Wait for transmission complete */\r\n\twhile(!(SPSR & (1<<SPIF)));\r\n\tPORTB |= 1;\r\n\treturn SPDR;\r\n}\r\n\r\nvoid ADC_Init()\r\n{\r\n\t//ADC enable\r\n\tADCSRA |= (1<<ADEN);\t\t\r\n\t//division factor --> 128\r\n\tADCSRA |= (1<<ADPS0) | (1<<ADPS1) | (1<<ADPS2);\t\t\r\n\t//left adjusted (8 bit mode) (ADCH)\r\n\tADMUX |= (1<<ADLAR);\t\r\n\t//input channel --> ADC2 (only one channel can be used in the conversion at a time)\r\n\tADMUX |= (1<<MUX1);\t\t\r\n}\r\n\r\nunsigned char ADC_StartConversion()\r\n{\r\n\tADCSRA |= (1 << ADSC);\r\n\twhile(ADCSRA & (1<<ADSC));\r\n\treturn ADCH;\r\n}\r\n\r\nint main(void)\r\n{\r\n\tDDRC = 1;\r\n\tADC_Init();\r\n\tSPI_MasterInit();\r\n\tunsigned char data;\r\n\tunsigned char ret;\r\n\t\r\n    while (1) \r\n    {\r\n\t\tdata = ADC_StartConversion();\r\n\t\tret = SPI_MasterTransmit(data);\r\n\t\t\r\n\t\tif(ret == ACK)\r\n\t\t{\r\n\t\t\tPORTC = 1;\r\n\t\t}\r\n\t\t\r\n\t\telse\r\n\t\t{\r\n\t\t\tPORTC = 0;\r\n\t\t}\r\n\t\t\r\n    }\r\n}\r\n\r\n\r\nCode of SLAVE\r\n\r\n#define F_CPU 16000000UL\r\n\r\n#include <avr/io.h>\r\n#include <util/delay.h>\r\n\r\n#define DDR_SPI DDRB\r\n#define DD_MISO 3 \r\n#define LED PC0\r\n\r\n#define ACK 0x7E\r\n\r\nvoid SPI_SlaveInit(void)\r\n{\r\n\t/* Set MISO output, all others input */\r\n\tDDR_SPI = (1<<DD_MISO);\r\n\t/* Enable SPI */\r\n\tSPCR = (1<<SPE);\r\n}\r\n\r\nunsigned char SPI_SlaveReceive(char data)\r\n{\r\n\tSPDR = data;\r\n\t/* Wait for reception complete */\r\n\twhile(!(SPSR & (1<<SPIF)));\r\n\t/* Return data register */\r\n\treturn SPDR;\r\n}\r\n\r\n\r\nint main(void)\r\n{\r\n\tDDRC |= (1<<LED);\r\n\tSPI_SlaveInit();\r\n\tunsigned char data;\r\n   \r\n    while (1) \r\n    {\t\r\n\t\tdata = SPI_SlaveReceive(ACK);\r\n\t\tif(data >= 128)\r\n\t\t{\r\n\t\t\tPORTC |= (1<<LED);\r\n\t\t}\r\n\t\telse\r\n\t\t{\r\n\t\t\tPORTC &= ~(1<<LED);\r\n\t\t}\r\n\t\t//_delay_ms(100);\t\r\n\t}\r\n}\r\n\r\n", "comments": []}, {"number": 42241, "title": "Pare down canonicalize_function_inputs logic", "body": "Use existing list of default arguments instead of creating dict; remove unnecessary dict operations.\r\nLoop through additional arguments once, instead of looping over kwargs (potential repetition with defaults)", "comments": ["Benchmarks over 3 batches of 10 trials each\r\n\r\nMinimum\r\ndefun_matmul_2_by_2_CPU: 158.60 -> 153.75 us\r\ndefun_matmul_2_by_2_CPU_async: 89.23 -> 81.72 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 192.85 -> 199.16 us\r\n\r\nMean\r\ndefun_matmul_2_by_2_CPU: 169 -> 163 us\r\ndefun_matmul_2_by_2_CPU_async: 92 -> 84 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 198 -> 213 us\r\n\r\nMedian\r\ndefun_matmul_2_by_2_CPU: 166 -> 163 us\r\ndefun_matmul_2_by_2_CPU_async: 92 -> 83 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 205 -> 207 us", "For the inputs construction in the `not kwargs` case, tuple(list slice) is faster compared to tuple(iterable comprehension). However, the original is still faster. Doing testing to figure out why", "@jonathanchu33 Can you please fix build failures ? Thanks!", "After many rounds of benchmark testing it *seems* empirically that the optimization in the latest commit is faster. The tuple is created via tuple comprehension (as in the original style), rather than tuple(list slice). Why this appears faster for the tf.function benchmarks, I have no idea why; microbenchmarks using %timeit in a standalone colab shows that tuple(list slice) is much faster than tuple comprehension. I will leave the state of the PR here - benchmarks on my machine are too noisy to be sure of the optimal method."]}, {"number": 42240, "title": "[Intel MKL] Enable eager mode support for MKL Conv2D BF16 type", "body": "", "comments": []}]