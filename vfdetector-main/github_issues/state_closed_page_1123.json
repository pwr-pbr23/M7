[{"number": 19542, "title": "Support session config in tf.contrib.predictor", "body": "This PR allows users to supply a custom session config uses by `tf.contrib.predictor`.\r\n\r\nThis can be essential for some GPU setups in order to play nicely with other processes running on the same GPU.", "comments": ["Thanks for the PR! This looks good to me, but can you add a test using the session config. thanks", "Sure, I'll add a unittest.\r\n\r\nIs there a good way to check that the session config is really used other than accessing the private `sess._config` value?", "@case540 Thanks for the review.\r\n\r\nI added unittests for the factory methods testing the config option.", "I added unittests, could someone trigger CI so we can verify that everything works correctly? "]}, {"number": 19541, "title": "End of Sequence Error when using tf.estimator with tf.data", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS v7\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7\r\n- **GPU model and memory**: Titan Xp, 12GB \r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nI am using tf.estimator.train_and_evaluate and tf.data.Dataset to feed data to the estimator:\r\n\r\nInput Data function:\r\n\r\n```\r\n    def data_fn(data_dict, batch_size, mode, num_epochs=10):\r\n        dataset = {}\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['train_data'].astype(np.float32))\r\n            dataset = dataset.cache()\r\n            dataset = dataset.shuffle(buffer_size= batch_size * 10).repeat(num_epochs).batch(batch_size)\r\n        else:\r\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['valid_data'].astype(np.float32))\r\n            dataset = dataset.cache()\r\n            dataset = dataset.batch(batch_size)\r\n\r\n        iterator = dataset.make_one_shot_iterator()\r\n        next_element = iterator.get_next()\r\n\r\n    return next_element\r\n```\r\n\r\nTrain Function:\r\n```\r\ndef train_model(data):\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    config = tf.ConfigProto(allow_soft_placement=True,\r\n                            log_device_placement=False)\r\n    config.gpu_options.allow_growth = True\r\n    run_config = tf.contrib.learn.RunConfig(\r\n        save_checkpoints_steps=10,\r\n        keep_checkpoint_max=10,\r\n        session_config=config\r\n    )\r\n\r\n    train_input = lambda: data_fn(data, 100, tf.estimator.ModeKeys.TRAIN, num_epochs=1)\r\n    eval_input = lambda: data_fn(data, 1000, tf.estimator.ModeKeys.EVAL)\r\n    estimator = tf.estimator.Estimator(model_fn=model_fn, params=hps, config=run_config)\r\n    train_spec = tf.estimator.TrainSpec(train_input, max_steps=100)\r\n    eval_spec = tf.estimator.EvalSpec(eval_input,\r\n                                      steps=None,\r\n                                      throttle_secs = 30)\r\n\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\nThe training goes fine, but when it comes to evaluation I get this error:\r\n`OutOfRangeError (see above for traceback): End of sequence `\r\n\r\nIf I don't use Dataset.batch on evaluation dataset (by omitting the line dataset[name] = dataset[name].batch(batch_size) in data_fn) I get the same error but after a much longer time.\r\n\r\nI can only avoid this error if I don't batch the data and use steps=1 for evaluation (does that perform the evaluation on the whole dataset?)\r\n\r\nI don't understand what causes this error as the documentation suggests I should be able to evaluate on batches too.\r\n\r\nNote: I get the same error when using tf.estimator.evaluate on data batches.\r\n\r\n### Source code / logs\r\n\r\n```\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 439, in train_and_evaluate\r\n    executor.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 518, in run\r\n    self.run_local()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 657, in run_local\r\n    eval_result = evaluator.evaluate_and_export()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 847, in evaluate_and_export\r\n    hooks=self._eval_spec.hooks)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 425, in evaluate\r\n    name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1085, in _evaluate_model\r\n    input_fn, model_fn_lib.ModeKeys.EVAL))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 691, in _get_features_and_labels_from_input_fn\r\n    result = self._call_input_fn(input_fn, mode)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 798, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"/snel/home/mreza/projects/PBT_HP_opt/lfadslite/lfadslite.py\", line 748, in <lambda>\r\n    eval_input = lambda: self.data_fn(hps, tf.estimator.ModeKeys.EVAL)\r\n  File \"/snel/home/mreza/projects/PBT_HP_opt/lfadslite/lfadslite.py\", line 112, in data_fn\r\n    next_element = iterator.get_next()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 370, in get_next\r\n    name=name)), self._output_types,\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1466, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nOutOfRangeError (see above for traceback): End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,200,29]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\n```", "comments": ["@xiejw To my eye, it looks like @mrezak is doing everything correctly here (setting `EvalSpec(..., steps=None, ...)` and returning a finite iterator from the input function). Is there something missing?", "If I understand correctly, this issue is \"once give estimator an input_fn with dataset inside, the evaluate process will error out with OutOfRangeError.\"\r\n\r\nEstimator can handle this correctly actually. However, a known common root cause for this is metrics defined in model_fn have bug. We need to rule that part out first. \r\n\r\n@mrezak if possible, can you show the code about the model_fn? Or if you have a minimal reproducible script, that will be extremely helpful. -- Thanks in advance. \r\n\r\nA common problem for this is: metric in tensorflow should return two Ops: update_op and value_op. Estimator calls the update_op for each batch of the data in input source and, once it is exhausted, it call the value_op to get the metric values. The value_op here should have dependency back to variables reading only. \r\n\r\nMany model_fn puts the dependency of value_op with the input pipeline, so, estimator.evaluate will thereby trigger the input pipeline one more time, which errors out with OutOfRangeError\r\n", "Thanks @mrry and @xiejw for looking into this. @xiejw I see! Actually my value_op depended on the input pipeline (returning different form of loss), so as you pointed out that is the cause of this error. \r\n\r\nI have written minimal example script (A simple autoencoder) that reproduces the EndOfSequence error (If I replace `dummy_eval` with a tensor that does not depend on the input pipeline, I won't get the EndOfSequence Error) :\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef data_fn(data_dict, batch_size, mode, num_epochs=10):\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        dataset = tf.data.Dataset.from_tensor_slices(data_dict['train_data'].astype(np.float32))\r\n        dataset = dataset.cache()\r\n        dataset = dataset.shuffle(buffer_size= batch_size * 10).repeat(num_epochs).batch(batch_size)\r\n    else:\r\n        dataset = tf.data.Dataset.from_tensor_slices(data_dict['valid_data'].astype(np.float32))\r\n        dataset = dataset.cache()\r\n        dataset = dataset.batch(batch_size)\r\n\r\n    iterator = dataset.make_one_shot_iterator()\r\n    next_element = iterator.get_next()\r\n\r\n    return next_element, tf.convert_to_tensor('dummy_dataname')\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    with tf.variable_scope('encoder'):\r\n        cell = tf.contrib.rnn.GRUCell(num_units=20)\r\n        _, output_latent = tf.nn.dynamic_rnn(cell=cell, inputs=features, dtype=tf.float32)\r\n    with tf.variable_scope('decoder'):\r\n        cell = tf.contrib.rnn.GRUCell(num_units=20)\r\n        ts = tf.shape(features)\r\n        #ts = tf.Print(ts, [ts])\r\n        z_inps = tf.zeros([ts[0], ts[1], 1])\r\n        output_recon, _ = tf.nn.dynamic_rnn(cell=cell, inputs=z_inps, initial_state=output_latent, dtype=tf.float32)    \r\n\r\n    output_recon = tf.contrib.layers.fully_connected(inputs=output_recon, num_outputs=params['dims'], activation_fn=None)\r\n    vars   = tf.trainable_variables() \r\n    dummy_lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in vars ]) * 0.001\r\n    recon_loss = tf.losses.mean_squared_error(features, output_recon)\r\n    loss =  recon_loss + 0.1 * dummy_lossL2\r\n    dummy_eval = recon_loss\r\n    global_step = tf.train.get_global_step()\r\n    train_op = tf.train.AdamOptimizer(0.01).minimize(loss, global_step)\r\n    eval_metric_ops = {'dummy': (dummy_eval, dummy_eval)}\r\n    logging_hook = tf.train.LoggingTensorHook({'step':global_step, 'loss':loss}, every_n_iter=1)\r\n    return tf.estimator.EstimatorSpec(mode=mode,\r\n                                  loss=loss,\r\n                                  train_op=train_op,\r\n                                  eval_metric_ops=eval_metric_ops,\r\n                                  training_hooks=[logging_hook]\r\n                                  )\r\n\r\ndef train_model():\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    config = tf.ConfigProto(allow_soft_placement=True,\r\n                            log_device_placement=False)\r\n    config.gpu_options.allow_growth = True\r\n    run_config = tf.contrib.learn.RunConfig(\r\n        save_checkpoints_steps=100,\r\n        keep_checkpoint_max=10,\r\n        session_config=config\r\n    )\r\n    hps = {'dims': 5}\r\n    \r\n    # make training and validation data: sinusoids with random phases\r\n    num_samp_tr = 1000\r\n    num_samp_val = 1000\r\n    ramps_tr = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_tr, hps['dims'], 100)), (0, 2, 1))\r\n    rand_phase = np.transpose(np.tile(np.random.randn(num_samp_tr, hps['dims']), (100,1,1)), (1, 0, 2))\r\n    ramps_val = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_val, hps['dims'], 100)), (0, 2, 1))\r\n    rand_phase_val = np.transpose(np.tile(np.random.randn(num_samp_val, hps['dims']), (100,1,1)), (1, 0, 2))\r\n    data = {'train_data': np.sin(ramps_tr + rand_phase),\r\n            'valid_data': np.sin(ramps_val + rand_phase_val)}\r\n\r\n    train_input = lambda: data_fn(data, 100, tf.estimator.ModeKeys.TRAIN, num_epochs=10)\r\n    eval_input = lambda: data_fn(data, 1000, tf.estimator.ModeKeys.EVAL)\r\n    estimator = tf.estimator.Estimator(model_fn=model_fn, params=hps, config=run_config)\r\n    train_spec = tf.estimator.TrainSpec(train_input, max_steps=100)\r\n    eval_spec = tf.estimator.EvalSpec(eval_input,\r\n                                      steps=None,\r\n                                      throttle_secs = 100)\r\n\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\ntrain_model()\r\n```\r\n\r\nThe problem in my actual code is that my total loss to be optimized is composed of multiple losses (reconstruction + L2 + KL) and in the evaluation part I want to get the reconstruction loss (on the validation data), which depend on the input data pipeline. My actual reconstruction cost is more complex than MSE (none of the other tf.metric functions as well), so in this case do I have to construct my reconstruction cost calculation using tf.metric basic functions to be able to use it as eval_metric_ops?  ", "Hi @mrezak \r\n\r\nDoes the following approach work for you\r\n\r\n    my_total_loss =  ... # the loss you care. Pay attention to how you reduce the loss. \r\n    eval_metric_ops = {'total_loss: tf.metrics.mean(my_total_loss)}\r\n\r\nThis should give you the loss on held-out data. ", "This is basically mse is implemented \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/metrics_impl.py#L1238", "Thanks very much @xiejw. This seems to work for me. ", "Sure. I will thereby close this issue. I will try to improve the error message you saw and give a clear actionable step for user when hitting this issue. ", "@xiejw I have a similar code structure and I am facing the same issue as well. I tried what you said above but it didn't work."]}, {"number": 19540, "title": "INTEL-MKL: Fix an issue related to MKL op registration", "body": "MklConv2DWithBiasBackpropBias is an MKL op.\r\nIf should be registered only when building Tensorflow with original MKL (that is, MKL_ML),\r\nnot with the current MKL (that is, open source MKL_DNN).\r\n\r\nThis fix addresses the problem.", "comments": []}, {"number": 19539, "title": "Add missing deps for simd_armv8a in jpeg.BUILD", "body": "Building for armv8 fails with error:\r\n\r\nERROR: .../bazel/external/jpeg/BUILD:288:1: undeclared inclusion(s) in rule '@jpeg//:simd_armv8a':\r\nthis rule is missing dependency declarations for the following files included by 'external/jpeg/simd/jsimd_arm64.c':\r\n'.../bazel/external/jpeg/jpegint.h'\r\n'.../bazel/external/jpeg/jerror.h'", "comments": []}, {"number": 19538, "title": "Use BUILD.bazel for third_party packages", "body": "Both `BUILD` and `BUILD.bazel` could be used as the bazel project file (see https://github.com/bazelbuild/bazel/issues/552) and `BUILD.bazel` is actually  preferred (see https://github.com/bazelbuild/bazel/issues/4517#issuecomment-360213750)\r\n\r\nThis fix changes generated BUILD in third_party packages to `BUILD.bazel`.\r\n\r\nThis will help avoid conflict with `BUILD` or `build` file/directory names in third party packages.\r\n\r\nFor example, while working on #19461 I noticed that apache thrift package consists of a `build` directory and that causes issues in case-insensitive systems like macOS. This PR should help avoid such conflict issues.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19537, "title": "Add batch support for random tf.image.random_flip_*", "body": "The `tf.image.flip_*`methods can either process single images or batches of images. The randomized versions `tf.image.random_flip_*` do only support single images until now. A batched mode might increase the usability of the function for data augmentation purposes.", "comments": ["@jhseu Are these tests enough?", "Those are benchmarks. In addition, we should add a unit test that checks the behavior for 4-D tensors.", "I had to modify one other unit test which assumed that the random flip functions can only handle 3D input data, which caused the tests to fail."]}, {"number": 19536, "title": "How to link tensorflow contrib module in C API", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: Binary wheel (tensorflow-gpu)\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: Python 3.6\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: 1080 GTX / 8GB\r\n- **Exact command to reproduce**: -\r\n\r\n----------------\r\n\r\nUsing python, I have built a graph that relies on tensorflow.contrib.resampler and exported it to a .pb file. I am able to correctly import and execute that graph in python, by adding the following lines to my script:\r\n    import tensorflow as tf\r\n    tf.contrib.resampler\r\n\r\nHowever, when I try to import and execute that graph in plain C (c_api.h), I get the following error:\r\n    Failed to process frame with No OpKernel was registered to support Op 'Resampler' with these attrs.\r\n    Registered devices: [CPU,GPU], Registered kernels: <no registered kernels>\r\n\r\nWhat is the correct way to register this op and use it in the C API? I have not been able to find any relevant documentation on the matter.\r\n\r\n    ", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nThat said, see [this particular StackOverflow answer](https://stackoverflow.com/a/47277367). The equivalent to `TensorFlow.loadLibrary()` in Java mentioned there is [`TF_LoadLibrary`](https://github.com/tensorflow/tensorflow/blob/3b85959a57ef2656cdcf2cfac62d68713948ba54/tensorflow/c/c_api.h#L1526) in the C API.\r\n\r\nHope that helps."]}, {"number": 19535, "title": "TensorRT: Invalid graph after calibration -> inference graph transformation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource.\r\n- **TensorFlow version (use command below)**:\r\n1.8\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\r\n- **CUDA/cuDNN version**:\r\n9.0.176/7.0.5.15\r\n- **GPU model and memory**:\r\n1080 Ti\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nIt seems that `tf.contrib.tensorrt` may incorrectly redirect edges when transforming an INT8 calibration node into an engine node. The current implementation, for each output node, searches the first edge from this output node [1] and redirects only this edge to point from the newly created engine node [2]. If the output node has multiple outgoing edges, then only the first one is redirected, and the remaining edges still use the output node as the source. As a result, after the output node is deleted, the graph becomes invalid and the graphdef generated by `tf.contrib.tensorrt.calib_graph_to_infer_graph` cannot be loaded (e.g., using `TF_GraphImportGraphDef`) due to errors like this: `Node 'Shape_4': Unknown input node 'group1/block1/Relu'`. (In this case, `group1/block1/Relu` had two edges in the original graph: to some convolution node and to `Shape_4`; the first edge was updated, the second one was not and caused the error.)\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/717aa746e7e915cba9ce36df424d05642fbe8cd7/tensorflow/contrib/tensorrt/convert/convert_nodes.cc#L2191\r\n[2] https://github.com/tensorflow/tensorflow/blob/717aa746e7e915cba9ce36df424d05642fbe8cd7/tensorflow/contrib/tensorrt/convert/convert_nodes.cc#L2258", "comments": ["@samikama @jjsjann123 could you take a look?", "We have found this problem in our unit tests. We'll be working on it and push fix to master.\r\nWould update this thread as well.\r\n\r\nThanks,\r\nJ", "PR #19652 should fix this"]}, {"number": 19534, "title": "Add Hadoop SequenceFile support for tensorflow Dataset", "body": "This fix adds Hadoop SequenceFile for tensorflow Dataset, so that it is possible to process files stored in Hadoop system directly.\r\n\r\nThis fix is a very preliminary and early implementation. It supports `org.apache.hadoop.io.Text` only, and there is no compression support yet.\r\n\r\nWill work on expanding the support of other serialization types, and compresion support in the follow up PR.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Reviewer @mrry: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @mrry: It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Thanks @mrry for the detailed review! The PR has been updated with comments addressed. Please take a look and let me know if there are any other issues.", "It looks like there are some compiler errors due to recent interface changes in `framework/dataset.h`. Can you please take a look?", "@mrry The PR has been updated with the compiler errors (changes in `framework/dataset.h`) fixed. All tests passed except Sanity test, which is actually covered in https://github.com/tensorflow/tensorflow/pull/20474#issuecomment-401872675\r\n\r\nPlease take a look and let me know if there are any other issues.", "@mrry Really sorry for the extra log files (1.log and 2.log) in the last update :cold_sweat: . I was too quick to use `git add -A` without check in detail.\r\n\r\nThe PR has been updated. Please take a look.", "Thanks for that! The change looks good to me now.", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@case540 As this PR has been approved and all tests passed, maybe it is ready to add `ready to pull` label?", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Oh it fails under windows with:\r\n```\r\nFAIL: //py_test_dir/tensorflow/contrib/hadoop:hadoop_test (see T:/tmp/nsz6drem/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/py_test_dir/tensorflow/contrib/hadoop/hadoop_test/test.log)\r\nINFO: From Testing //py_test_dir/tensorflow/contrib/hadoop:hadoop_test:\r\n==================== Test output for //py_test_dir/tensorflow/contrib/hadoop:hadoop_test:\r\n2018-08-08 21:40:30.356453: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nE.\r\n======================================================================\r\nERROR: test_sequence_file_dataset (__main__.SequenceFileDatasetTest)\r\nTest case for SequenceFileDataset.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1293, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1278, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1368, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NewRandomAccessFile failed to Create/Open: \\\\?\\T:\\tmp\\Bazel.runfiles__x3fyn6j\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\contrib\\hadoop\\python\\kernel_tests\\testdata/string.seq : The filename, directory name, or volume label syntax is incorrect.\r\n\r\n; Unknown error\r\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[], []], output_types=[DT_STRING, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorV2)]]\r\n\r\n```", "Could you fix the merge conflicts? Then I'll attempt to merge it", "Looks like part of this PR has been merged. But let's run another import with the new commits once the conflicts have been resolved.", "Thanks @case540 @yifeif for the help. It looks like the changes are already in though the PR is not closed automatically. When I do the `git rebase upstream/master` it have:\r\n```\r\nubuntu@ip-172-31-47-125:~/tensorflow$ git rebase --continue\r\nApplying: Fix Windows Build failures caused by os.path.join\r\nNo changes - did you forget to use 'git add'?\r\nIf there is nothing left to stage, chances are that something else\r\nalready introduced the same changes; you might want to skip this patch.\r\n\r\nWhen you have resolved this problem, run \"git rebase --continue\".\r\nIf you prefer to skip this patch, run \"git rebase --skip\" instead.\r\nTo check out the original branch and stop rebasing, run \"git rebase --abort\".\r\n\r\nubuntu@ip-172-31-47-125:~/tensorflow$ \r\n```\r\n\r\nI also checked the master and the changes are there. I think it is safe to close this PR.", "Thanks all for the help !\ud83d\udc4d \ud83c\udf89 ", "Perfect. Thank you @yongtang!"]}, {"number": 19533, "title": "Improve fast_tensor_util for bfloat16", "body": "In #19212, improvement has been done to speed up the fast_tensor_util for `float16`. As both `float16`\r\nand `bfloat16` uses the same size, `bfloat16` could be improved as well. This fix speeds up `bfloat16` in a similiar fashion as `float16`.\r\n\r\nThis fix is related to #19212.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Reviewer @rmlarsen: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @rmlarsen: It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @rmlarsen: It has been 44 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "@yongtang is this covered by existing unit tests?", "@rmlarsen Yes the related unit test is in:\r\nhttps://github.com/tensorflow/tensorflow/blob/6d8bc3ce794b386088a351bcf6f5f5e08ff0ebb6/tensorflow/python/framework/tensor_util_test.py#L238-L257"]}, {"number": 19532, "title": "Enable mirror.bazel.build link for gemmlowp", "body": "The `mirror.bazel.build` link for `gemmlowp` was disabled as it was not propagated before. This fix enables the `mirror.bazel.build` link for `gemmlowp` and removes the related TODO.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19531, "title": "Added 64 bit toolchain flag to CMake build instructions", "body": "When building tensorflow on windows with CMake, using the 64 bit toolchain (compiler (cl.exe) and linker (link.exe)) is needed. Using the 32 bit toolchain often result in errors such as C1060 \"compiler out of heap space\" and C1002: \"compiler is out of heap space in pass 2\". There are several issues reported on this: https://github.com/tensorflow/tensorflow/issues?utf8=%E2%9C%93&q=is%3Aissue+compiler+is+out+of+heap+space\r\n\r\nThe current README for CMake in tensorflow states that you can fix this by setting up a bunch of environment variables using the visual studio bat script: \"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\vcvarsall.bat\".\r\n\r\nHowever, I have discovered that this has no effect. If you look in task manager while you are compiling when using this bat script, you can see the task: \"Microsoft Compiler Driver **(32 bit)**\" running. \r\nThe reason why this has no effect, is that CMake select which compiler and linker is used. It will output this to the console the first time you run configure with cmake. You will then see that CMAKE_CXX_COMPILER is set to \"C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/**x86**_amd64/cl.exe\". The x86 part I assume means the 32 bit compiler is used, while the amd64 part means you are building an 64 bit application, two very different things. The fact that some people report that using the bat script actually helps, is most likely due to chance. I have observed several times that sometimes this error occurs, while other times it doesn't. I think this is due to multi-threading during compilation.\r\n\r\nNewer versions of CMake (>= 3.8) allow you to specify the toolset host architecture to 64 bit using the flag:\r\n`-Thost=x64`. This is documented here: https://cmake.org/cmake/help/v3.8/generator/Visual%20Studio%2014%202015.html\r\n\r\nBy adding this flag, I observe that CMAKE_CXX_COMPILER is set to \"C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/**amd64**/cl.exe\" instead. Also, while compiling, the task manager now shows the task \"Microsoft Compiler Driver\", which is the 64 bit version of cl.exe, instead of \"Microsoft Compiler Driver **(32 bit)**\".\r\n\r\nUsing this flag, I have not experienced \"compiler out of heap space\" issues anymore on neither Visual studio 2015 and 2017 on windows 10. I hope others can test this and, hopefully, verify this solution.\r\n\r\nI also updated the minimum cmake version required for windows to 3.8, which supports this host toolset flag.", "comments": ["Cool! TIL...", "Looks like we need a cmake upgrade on our test infra. I will see if I can get that done first, then we can move forward with this PR.", "pinging gunan@ for update about upgrading CMake. ", "@m3bm3b as he has some context.\r\nWe received some complaints that upgrading the dependencies to cutting edge versions are causing headaches for users who would like to build from sources.\r\nSo we would like to avoid this upgrade. Is there a way to achieve a similar result without cmake upgrade?", "In case it helps, one way to achieve a version requirement only \r\nunder certain conditions is as follows:\r\n\r\ncmake_minimum_required (VERSION <minimum required under any circumstances>)\r\n\r\nif (<need higher version condition (e.g., is Windows)>     AND\r\n    \"${CMAKE_MAJOR_VERSION} and \"${CMAKE_MINOR_VERSION}\" are too low) \r\n   message( FATAL_ERROR \"require version <minimum version> if <need higher version condition>\" )\r\nendif()\r\n\r\n\r\n", "Not sure why a cmake upgrade should be troublesome? At least not on windows, for linux I can understand as the package manager aptitude for Ubuntu 16 only gives you cmake 3.5. But this toolset feature is not needed on linux, hence the if(WIN32).\r\n\r\nThis example might give an idea on how you can do it without the cmake upgrade: https://github.com/facebook/hhvm/blob/master/CMake/VisualStudioToolset.cmake\r\n\r\nI have not tested this, so it would have to be verified that it actually does what it claims. Also, the example is only for visual studio 2017, and would have to be expanded for other VS versions. ", "> Not sure why a cmake upgrade should be troublesome? \r\n\r\nIf N people are each going to have to spend 5-10 minutes working out that they need to upgrade,\r\nlooking up how to do it, and then doing it, it's worth the maintainer spending on the order of N*5 \r\nminutes seeking to help them by arranging that they don't have to do the upgrade.  \r\nFor TensorFlow, N tends not to be small.\r\n\r\nSometimes it's hard to avoid forcing people to upgrade something.  \r\nBut if it's possible to avoid it straightforwardly, I'd recommend it.\r\nAnd failing that for Windows, if it can be helped for other platforms straightforwardly,\r\nthen I'd recommend that.\r\n\r\n", "@smistad I agree with @m3bm3b on this one.\r\nUpgrade of dependencies can be confusing for people, and weith each upgrade we force that on thousands of people. Would you like to prepare a workaround that can do more if newer version of cmake is available, but still work if user has an older version of cmake?", "I see your point. But is the cmake feature still relevant? The reason I ask, is that in the 1.10 release notes it was stated the following: \"Starting from TensorFlow 1.11, Windows builds will use Bazel. Therefore, we will drop official support for cmake.\"", "Do you mean we can drop this PR, or do you mean it is OK to be more restrictive on an unsupported config?", "The hack from https://github.com/facebook/hhvm/blob/master/CMake/VisualStudioToolset.cmake caused an error in CMake, therefore I propose the following compromise:\r\n\r\nReplace the cmake_minimum_required(3.8) with a warning: \r\n```cmake\r\nif(WIN32 AND ${CMAKE_VERSION} VERSION_LESS \"3.8\")\r\n    message(WARNING \"Your current cmake version is ${CMAKE_VERSION} which does not support setting the toolset architecture to x64. This may cause \\\"compiler out of heap space\\\" errors when building. Consider upgrading your cmake to >= 3.8 and using the flag -Thost=x64 when running cmake.\")\r\nendif()\r\n```", "OK, with the current state of cmake for TF, I am OK with that.", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19530, "title": "transforms_graph error when \"remove_nodes(op=Identity, op=CheckNumerics)\" and \"fold_old_batch_norms\" are used together", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.9\r\n- **CUDA/cuDNN version**: No\r\n- **GPU model and memory**: No\r\n- **Exact command to reproduce**: ` bazel build tensorflow/tools/graph_transforms:transform_graph`\r\n\r\n### Describe the problem\r\nI followed the proposed commands for optimizing the model. However, \"remove_node(op=Identity, op=CheckNumerics)\" and \"fold_old_batch_norms\" cannot be used together.\r\n\r\n### Source code / logs\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=resnet-18_train.pb \\\r\n--out_graph=opt_resnet-18_train.pb \\\r\n--inputs='input_x' \\\r\n--outputs='output/BiasAdd' \\\r\n--transforms='\r\nstrip_unused_nodes(type=float, shape=\"1,224,224,3\")\r\nremove_nodes(op=Identity, op=CheckNumerics)\r\nfold_old_batch_norms\r\nfold_batch_norms'\r\n```\r\nError:\r\n> 2018-05-24 20:53:06.184534: I tensorflow/tools/graph_transforms/transform_graph.cc:264] Applying strip_unused_nodes\r\n2018-05-24 20:53:06.259019: I tensorflow/tools/graph_transforms/transform_graph.cc:264] Applying remove_nodes\r\n2018-05-24 20:53:06.486037: I tensorflow/tools/graph_transforms/transform_graph.cc:264] Applying fold_old_batch_norms\r\n2018-05-24 20:53:06.513047: E tensorflow/tools/graph_transforms/transform_graph.cc:210] Beta input to batch norm has bad shape: [64]\r\n\r\nHowever, if \"remove_node(op=Identity, op=CheckNumerics)\" is removed\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=resnet-18_train.pb \\\r\n--out_graph=opt_resnet-18_train.pb \\\r\n--inputs='input_x' \\\r\n--outputs='output/BiasAdd' \\\r\n--transforms='\r\nstrip_unused_nodes(type=float, shape=\"1,224,224,3\")\r\nfold_old_batch_norms\r\nfold_batch_norms'\r\n```\r\nIt then works...\r\n\r\n", "comments": ["This is not surprising. \"remove_node(op=Identity, op=CheckNumerics)\" optimization breaks the model that contain control flow operations, such as tf.cond, tf.map_fn, and tf.while. Graph Transform Tool [README](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/tools/graph_transforms/README.md#using-the-graph-transform-tool) mentions this.\r\n\r\n", "@tatianashp thanks a lot. It seems that we do not have any of control flow ops used in the model. According to the risk of using this dangerous option, it will be more meaningful to remove this option from the example? Single-input and single-output ops may often be used and there is no warning about removing them. ", "@raingo,  @petewarden - Any thoughts here? Is this a bug or expected behavior for a graph without control flow?", "@HwMohanLiu  when I use \r\n```\r\n--transforms='\r\nremove_nodes(op=Identity, op=CheckNumerics)\r\nfold_constants(ignore_errors=true)\r\nfold_batch_norms\r\nfold_old_batch_norms\r\nstrip_unused_nodes\r\nsort_by_execution_order\r\n'\r\n```\r\nI get the same error:\r\n\r\n```\r\n2019-01-28 11:17:22.671235: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying remove_nodes\r\n2019-01-28 11:17:22.732271: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying fold_constants\r\n2019-01-28 11:17:22.783415: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying fold_batch_norms\r\n2019-01-28 11:17:22.805463: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying fold_old_batch_norms\r\n2019-01-28 11:17:22.825554: E tensorflow/tools/graph_transforms/transform_graph.cc:264] Beta input to batch norm has bad shape: [64]\r\n```\r\nbut why do not you use `fold_constants(ignore_errors=true)` before using `fold_batch_norms` and \r\n`fold_old_batch_norms`?  And after `remove_node(op=Identity, op=CheckNumerics)` is removed as you said,  I still get the same error.", "@IvyGongoogle As tatianashp mentioned, the option \"remove_nodes\" could break the control flow options in the model. I did not manually  add any control flow ops which tatianashp mentioned. However, it failed. Then, I removed the option \"remove_nodes\", and the model works well and can be correctly converted into TF-Lite model without BN problems.", "@HwMohanLiu  there is also no control flow ops in my graph. And I doubt you fold_batch_norms successfully  or not.", "@IvyGongoogle If you used FusedBatchNorm (for example tf.layers.batch_normalization), it should be fold_old_batch_norms in my opinion. Although you will still see the node FusedBatchNorm after transform, the content is already changed. There is only a BiasAdd in it. Also you can find the structure mapping definitions for BN in the corresponding source codes in the module https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms", "@HwMohanLiu After using `fold_old_batch_norms`, we get a `BiasAdd` for  `FusedBatchNorm`, but there is no `biases` op after `weights` op. But there is a `Conv2D_bn_offset` op,  is this an op with same meaning of `biases`?", "@IvyGongoogle You can determine it by tracing the input ops of the BiasAdd node until finding a const or ref op with the size of the corresponding bias.", "@HwMohanLiu when I trace the input ops of the `BiasAdd`, I find the `Conv2D_bn_offset` op is indeed an op with same meaning of `biases`. Thank you very much. \r\nBut I have another question: the result of infering this merged-BN  pb is slightly different with the original pb, about several numbers.", "@IvyGongoogle If the original pb is used for validation before folding BN, you may have to keep the input batch size as training to get a similar accuracy. After folding, you can input the images one by one. There could be very slight difference between them. However, the difference can be ignored since the gap is normally after four decimals according to my experience.", "Graph transform tool is [deprecated](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md#list-of-projects) with TF 2.X versions and replaced by [Grappler](https://www.tensorflow.org/guide/graph_optimization).\r\n"]}, {"number": 19529, "title": "how to save intermediate checkpoints when using slim.learning.train ", "body": "I have trained classification model using inception_resnet_v2, and I set max_train_steps equals 15w steps. However, according to loss graph on tensorboard, I found the minimum loss at  2w steps. I also set `save_interval_secs=600`, I need this model at 2w steps rather than 15w steps, but tensorflow only save the last model. so, how can i get the intermediate checkpoints?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 19528, "title": "Restore problem when work with multiple tf.contrib.lookup.MutableHashTable", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 3.10.0-693.2.2.el7.x86_64\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: Python 2.7.14 :: Anaconda\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nWe try to use multiple tf.contrib.lookup.MutableHashTable objects in the script. But after do checkpoint with Saver.save(), we find that it can not restore table contents successfully. \r\n\r\nThe problem disappears if we explicitly specify name field for tables. It seems that the MutableHashTable save contents with spec key \"xxxx-keys\" and \"xxxx-values\", where \"xxxx\" is the object's name field defaults to \"MutableHashTable\".\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/93bc2e2072e0daccbcff7a90d397b704a9e8f778/tensorflow/contrib/lookup/lookup_ops.py#L463-L467\r\n\r\nMaybe it is better to associate unique name for each table object if not explicitly specified?\r\n\r\n### Source code / logs\r\nCreate and insert tables, and save\r\n```python\r\nkeys = tf.placeholder(dtype=tf.string, shape=[None])\r\nvalues = tf.placeholder(dtype=tf.int64, shape=[None])\r\ntable1 = tf.contrib.lookup.MutableHashTable(tf.string, tf.int64, -1)\r\ntable2 = tf.contrib.lookup.MutableHashTable(tf.string, tf.int64, -1)\r\ninsert_table1 = table1.insert(keys, values)\r\ninsert_table2 = table2.insert(keys, values)\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    sess.run(insert_table1, feed_dict={keys: [\"a\"], values: [1]})\r\n    sess.run(insert_table2, feed_dict={keys: [\"b\"], values: [2]})\r\n    print \"table1:\", sess.run(table1.export())\r\n    print \"table2:\", sess.run(table2.export())\r\n    saver.save(sess, \"checkpoint/test\")\r\n```\r\ntable1: (array(['a'], dtype=object), array([1]))\r\ntable2: (array(['b'], dtype=object), array([2]))\r\n\r\nTry restore contents\r\n```python\r\ntable1 = tf.contrib.lookup.MutableHashTable(tf.string, tf.int64, -1)\r\ntable2 = tf.contrib.lookup.MutableHashTable(tf.string, tf.int64, -1)\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    saver.restore(sess, \"checkpoint/test\")\r\n    print \"table1:\", sess.run(table1.export())\r\n    print \"table2:\", sess.run(table2.export())\r\n```\r\ntable1: (array([], dtype=object), array([], dtype=int64))\r\ntable2: (array(['b'], dtype=object), array([2]))", "comments": ["exactly same problem..", "Since `Operator` and `Variable` has handled the conflict of the default names, it would better to manage the allocated names by TensorFlow.", "Thanks for providing clear code and explanation!\r\n\r\n@ysuematsu Can you take a look at this?", "Any update of this issue? It would be great if we can make the contribution to fix that by automatically assigning unique names for Table operators.", "We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19528\">No</a>\n"]}, {"number": 19527, "title": "SIGSEGV at TensorFlowInferenceInterface.run", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution** : Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary, pip\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nI'm running Inference on an small Style Transfer Model on Android using TF Mobile.\r\nMy App crashes on Motorola Nexus 6 with the following Error after i call:\r\n\r\n`inferenceInterface.run(new String[]{OUTPUT_NODE});`\r\n\r\n **Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 24385**\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/2034709/log.txt)\r\n\r\nI'm using Andoird Studio and integrated TFMobile on Android using the AAR:\r\n```\r\nallprojects {\r\n    repositories {\r\n        jcenter()\r\n    }\r\n}\r\n\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-android:+'\r\n}\r\n```\r\nI'm using a model similar to the popular feed forward model described by Johnson and optimized using graph transform tool:\r\n\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=wave.pb \\\r\n--out_graph=wave_optimized.pb \\\r\n--inputs='input_image' \\\r\n--outputs='output_image' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fuse_resize_and_conv\r\n  fuse_resize_pad_and_conv\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\n\r\n- It only happens if the input image size gets to large, e.g. 600x600. So it could be simply a memory / CPU problem.\r\n- **But** it seems to be phone specific: I can scale up the images up to 1200x1200 on other phones and the app doesn't crash (it only takes for about 10 seconds)\r\n- I'm not doing Real-Time-Style-Transfer as in the TF Stylize App, i simply run inference on selected images and i'm testing with different input sizes\r\n\r\nI know the error may be hard to reproduce and the main focus is now on TFLite (but TFLite doesn't support Style Transfer yet, as far as i know).\r\nI could also life with the fact that i have to further downscale the images to avoid the crash but maybe someone has an idea about: \r\n\r\n- what is going wrong\r\n- why it only crashes on certain phones\r\n- how i can get more specific information why it crashes\r\n\r\nThanks for any tips\r\n\r\n \r\n\r\n\r\n", "comments": ["This has been open for a while and the code has changed a lot since. Have you been able to run tflite on your model?", "@tobirudi9 : Please reopen if you see this issue again."]}, {"number": 19526, "title": "Node 'output/softmax' does not exist in model 'file:///android_asset/OptimizedGraph.pb'", "body": "I trained my model then frozen the graph using the .ckpt file and .pbtxt file  \r\n**Code to freeze:**\r\n \r\n\r\n    freeze_graph.freeze_graph('./tensorflowModel.pbtxt', \"\",False,'./tfmodel.ckpt', \"output/predictions\", \"save/restore_all\",  \"save/Const:0\",'frozen.pb', True,\"\")\r\n\r\nThen to **optimize for inference** followed the  below code:\r\n\r\n    inputGraph = tf.GraphDef()\r\n    with tf.gfile.Open(\"frozen.pb\", \"rb\") as f:\r\n        data2read = f.read()\r\n        inputGraph.ParseFromString(data2read)\r\n    \r\n    outputGraph = optimize_for_inference_lib.optimize_for_inference(\r\n                    inputGraph,\r\n                    [\"inputTensor\"], # an array of the input node(s)\r\n                    [\"output/predictions\"], # an array of output nodes\r\n                    tf.int32.as_datatype_enum)\r\n    \r\n            # Save the optimized graph\r\n    \r\n    f = tf.gfile.FastGFile(\"OptimizedGraph.pb\", \"w\")\r\n    f.write(outputGraph.SerializeToString())  \r\n\r\n \r\n\r\nThe output file **\"OptimizedGraph.pb\",** When I try to use it in the android studio it produces the error:\r\n\r\n     Node 'output/softmax' does not exist in model 'file:///android_asset/OptimizedGraph.pb'\r\n\r\n\r\nI checked my .pbtxt file which I used to freeze my graph and there was no node **output/softmax**!\r\n\r\nWhat should I do?\r\n", "comments": ["Can you post your android code where you load the model and do inference.", "@achalshah20  There is another optimized.pb file with me and that works perfectly fine, but when I am following the above-mentioned steps to create another new .pb file, It shows error.", "```\r\npublic class TensorflowIntegrationExample {\r\n    private Classifier classifier;\r\n    private final String TAG = \"TestModel\";\r\n    private final String INPUT_NAME = \"inputTensor\";\r\n    private final String DROPOUT_NAME = \"dropout_keep_prob\";\r\n    private final String[] OUTPUT_NAME = {\"output/softmax\"};\r\n    private final String MODEL_FILE = \"file:///android_asset/OptimizedGraph.pb\";\r\n    private final String LABEL_FILE = \"file:///android_asset/labels.txt\";\r\n    private AssetManager assetManager;\r\n```", "Nagging Assignee @robieta: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The issue is solved closing it! Thank you @achalshah20 ", "What was the solution?", "@towcar  Well, I had not declared my softmax layer. If you are facing the same check if the node \"output/softmax \" exists or not."]}, {"number": 19525, "title": "How to use grap_transfroms tools?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes and No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)\r\n- **TensorFlow installed from (source or binary)**: Source \r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9 / 7\r\n- **GPU model and memory**: 4GB GTX 1050\r\n- **Exact command to reproduce**:\r\n\r\nI want to use the graph_transforms tools but i dont know how to use them correctly:\r\n\r\nAs far as i know you have to run the commands like\r\n```\r\nbazel build tensorflow/tools/graph_transforms:summarize_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=tensorflow_inception_graph.pb\r\n```\r\nfrom inside the cloned tensorflow repo from which you successfull build tf from source.\r\n\r\nBut then i see errors like:\r\n```\r\nERROR: /home/gustav/.cache/bazel/_bazel_gustav/0efb90387e9af20adf0714a85702521f/external/jpeg/BUILD:126:12: Illegal ambiguous match on configurable attribute \"deps\" in @jpeg//:jpeg:\r\n@jpeg//:k8\r\n@jpeg//:armeabi-v7a\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nUnhandled exception thrown during build; message: Unrecoverable error while evaluating node 'PACKAGE:tensorflow/core/kernels' (requested by nodes '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@fedf386b false (700629378)', '//tensorflow/core:framework_internal_headers_lib_gather com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@fedf386b false (744921750)')\r\n\r\n```\r\nand\r\n```\r\nFAILED: Build did NOT complete successfully (33 packages loaded)\r\n    currently loading: tensorflow/core/kernels ... (3 packages)\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'PACKAGE:tensorflow/core/kernels' (requested by nodes '//tensorflow:libtensorflow_framework.so com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@fedf386b false (700629378)', '//tensorflow/core:framework_internal_headers_lib_gather com.google.devtools.build.lib.skyframe.BuildConfigurationValue$Key@fedf386b false (744921750)')\r\n\r\n```\r\n", "comments": ["Pulling tensorflow v1.8.0 seems to solve this problem (i previously had v.1.7.0)", "In my case, the bazel version was the problem. \r\nWith bazel `0.25.2` and `0.24.1`, lots of `FileType function is not available. You may use a list of strings instead. You can temporarily reenable the function by passing the flag --incompatible_disallow_filetype=false` has been introduced.\r\nThus I downgraded the bazel once more, to `0.21.0`, and graph_transfroms began to build.\r\n\r\nSummary: \r\nTensorFlow: `1.13.1`\r\nBazel: `0.21.0`"]}, {"number": 19524, "title": "Compiling C++ inference with -O1 produces wrong results", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNot in the tensorflow, but I have a code that utilizes C++ API\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux VirtualBox 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux - running in VirtualBox on a Windows Host\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.8.0-rc1\r\ntf.GIT_VERSION = v1.8.0-rc1-1239-gd0f5bc1\r\ntf.COMPILER_VERSION = v1.8.0-rc1-1239-gd0f5bc1\r\nSanity check: array([1], dtype=int32)\r\n- **Python version**: \r\nPython 2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.13.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Oct 18 21:33:40 +50297 (1525078013620)\r\nBuild timestamp: 1525078013620\r\nBuild timestamp as int: 1525078013620\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n\r\n- **CUDA/cuDNN version**:\r\nNo\r\n\r\n- **GPU model and memory**:\r\nNo\r\n\r\n- **Exact command to reproduce**:\r\nPlease see below\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI'm using a model I built in Python to run inference in C++ and compare the results to the ones I got when running it in Python. I save the exact batches I fed in Python and feed them in C++. When I use the default build settings I get very close results. However, I've found that matrix calclulations I performs with the results are incredibly slow (3 minutes for matrix division of 2 1000 by 5 matrices - I will submit a separate issue for that). So I tried to compile with -O1. \r\nThe result returned by running the inference very different now from the expected one.\r\nPlease clarify what is going on here.\r\n\r\nSo without optimization I get:\r\nLoadModel passed!\r\nIteration 1\r\noutputs[0]: Tensor<type: float shape: [1,5] values: [0.928591609 0.129222199 1.01102]...> <== Inferred by C++\r\noutput_tensor Tensor<type: float shape: [1,1,5] values: [[0.928591728 0.129222214 1.01102006]]...> <== Inferred by Python\r\n\r\nWith -O1:\r\nLoadModel passed!\r\nIteration 1\r\noutputs[0]: Tensor<type: float shape: [1,5] values: [0.907290399 0.124970555 1]...> <== Inferred by C++\r\n\r\noutput_tensor Tensor<type: float shape: [1,1,5] values: [[0.928591728 0.129222214 1.01102006]]...><== Inferred by Python\r\n\r\ndiff is bigger than expected: 0.0445271\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n[optimization_problem_tensorflow.zip](https://github.com/tensorflow/tensorflow/files/2034485/optimization_problem_tensorflow.zip)\r\n\r\n", "comments": ["@mkravchik , can you include the exact command you used to build TensorFlow in each configuration?", "I just followed the instructions here: https://www.tensorflow.org/install/install_sources", "To clarify, you say, \"I tried to compile with -O1\"-- what were you compiling, and can you include the commands in both compilation configurations?", "I meant that I compile the attached inference_cc.cc file by adding -O1 to the CMakeLists.txt like in the attached file the original file is in the zip attached above.\r\n[CMakeLists.txt](https://github.com/tensorflow/tensorflow/files/2077010/CMakeLists.txt)\r\n", "@asimshankar - I hear you may have some insight here?", "I've found out that the difference was due to the calculations I made with the results. I rewrote it in a clean way and the problem disappeared"]}, {"number": 19523, "title": "[tensorflow/tools/graph_transforms] No scripts / files in the installed tensorflow directory", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes and No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)\r\n- **TensorFlow installed from (source or binary)**: Source \r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9 / 7\r\n- **GPU model and memory**: 4GB GTX 1050\r\n- **Exact command to reproduce**:\r\n\r\nI try to use the `summarize_graph` and `transform_graph` tools but inside my `/usr/local/lib/python2.7/dist-packages/tensorflow/tools/graph_transforms` directory there is nothing besides the __init__.py and __pyc files.\r\n\r\nWhy do i lack these tools / scripts and how do I get them?\r\n\r\nI always see errors like this:\r\n```\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\n..............................\r\nERROR: Skipping 'tensorflow/tools/graph_transforms:summarize_graph': no such package 'tensorflow/tools/graph_transforms': BUILD file not found on package path\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package 'tensorflow/tools/graph_transforms': BUILD file not found on package path\r\nINFO: Elapsed time: 3.386s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/19525"]}, {"number": 19522, "title": "how to alter predefined network architecture and reuse the pretrained model please?", "body": "hi\uff0call , I'm faced with a problem.\r\nI predefined a neural network architecture, simply, conv->pooling->conv->pooling->FC->softmax, then I trained the network with tensorflow also obtained checkpoint and models in directory: /mnist_convnet_model\r\nNext, I want to reload the model. This time I need to alter the network's architecture, but still, weights pretrained needed to be reloaded.  The pretrained weights and the new nerual architecture do not match exactly but part of previous trained info is needed.\r\nWhat should I do? Is there any API available please ?\r\nThx!", "comments": ["Just use **get_tensor_by_name** to get tensors from your graph. If you want add new operations to the graph, you can always mix existing tensors with the new ones.\r\n\r\ni.e.\r\n```\r\noutput_conv = vgg_graph.get_tensor_by_name('conv1_2:0')\r\nsome_new_node = tf.nn.conv2d(output_conv, W1, strides=[1, 1, 1, 1], padding='SAME') + b1 (w1,b1 are tf variables for weight and bias)\r\n\r\n```", "hi, @achalshah20 ,thanks a lot for your advice! I would try it!!", "hi, @achalshah20 , the method you advised does work. \r\nIf I didn't set tensor name and import meta graph from model.meta, how could I get specified layer's tensor and  corresponding op then? could I alter the op also reuse the weights please? \r\nCould you give an example? \r\nThanks :)", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19521, "title": "[TFlite] unable to bazel build tensorflow/contrib/lite/toco:toco", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes and No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04)\r\n- **TensorFlow installed from (source or binary)**: Source \r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9 / 7\r\n- **GPU model and memory**: 4GB GTX 1050\r\n- **Exact command to reproduce**:\r\n\r\ni trained a mask r-cnn model with mobilenet as backbone and i try to quantize it according to this guide: https://www.tensorflow.org/performance/quantization \r\n\r\nMy code looks like this:\r\n```\r\nMODEL_NAME=\"mask_rcnn_mobilenet_v1_400_coco_117k\"\r\n\r\nINPUT_FILE=\"models/${MODEL_NAME}/frozen_inference_graph.pb\"\r\nOUTPUT_FILE=\"models/${MODEL_NAME}/frozen_inference_graph.tflite\"\r\nINPUT_SHAPE=\"1,400,400,3\"\r\nINPUT_ARRAY=['image_tensor']\r\nOUTPUT_ARRAY=['num_detections','detection_boxes','detection_scores','detection_classes','detection_masks']\r\nSTD_VALUE=127.5\r\nMEAN_VALUE=127.5\r\n\r\n\r\nbazel build tensorflow/contrib/lite/toco:toco && \\\r\n  ./bazel-bin/third_party/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=${INPUT_FILE} \\\r\n  --output_file=${OUTPU_FILE} \\\r\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shape=${INPUT_SHAPE} \\\r\n  --input_array=${INPUT_ARRAY} \\\r\n  --output_array=${OUTPUT_ARRAY} \\\r\n  --std_value=${STD_VALUE} --mean_value=${MEAN_VALUE}\r\n```\r\nand this error i get:\r\n```\r\nERROR: Skipping 'tensorflow/contrib/lite/toco:toco': no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package path\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package path\r\nINFO: Elapsed time: 1.132s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\n\r\nAnother thing i don't understand is that when i try to run this code in my custom model folder i see this error:\r\n`ERROR: The 'build' command is only supported from within a workspace.`\r\n\r\nWhat is this supposed to mean?\r\n\r\nSo i tried it in the tensorflow/models", "comments": ["https://github.com/tensorflow/tensorflow/issues/19525", "Perhaps add 'exports_files([\"toco\"])' to tensorflow/contrib/lite/BUILD", "> \r\n> \r\n> Perhaps add 'exports_files([\"toco\"])' to tensorflow/contrib/lite/BUILD\r\n\r\nwhy add this ? I have got a same probilem such as GuatavZ ,what should I do ?", "> \r\n> \r\n> #19525\r\n\r\nexcuse me ?Do you slove this problem   ?", "No, it appears in the current master version as well if you execute (from https://github.com/tensorflow/tensorflow/issues/15633 )\r\n\r\n# Strip out problematic nodes before even letting TOCO see the graphdef\r\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\r\n--input_names=Preprocessor/sub --output_names=concat,concat_1 \\\r\n--alsologtostderr\r\n\r\n# Run TOCO conversion.\r\nbazel run tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$STRIPPED_PB --output_file=$DETECT_FB \\\r\n--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n--input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub \\\r\n--output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr", "The toco target has been moved from //tensorflow/contrib/lite/toco to //tensorflow/lite/toco", "> The toco target has been moved from //tensorflow/contrib/lite/toco to //tensorflow/lite/toco\r\n\r\nuseful comment. thank you "]}, {"number": 19520, "title": "Why there is no C  GPU based tensorflow for windows?", "body": "I have seen that there are only Linux and MacOS based installations for the C language.\r\nKindly let me know if there is any tensorflow for the Windows in C langauage.", "comments": ["There are no GPU binary releases for the C library for Windows yet (see https://github.com/tensorflow/tensorflow/issues/16660#issuecomment-362480779)\r\n\r\nCPU releases are available at:\r\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.8.0.zip \r\n(Replace 1.8.0 with the version of interest).\r\n\r\nClosing as a duplicate of #16660 "]}, {"number": 19519, "title": ":bug: since Scaffold has no attribute of global_step, drop this confu\u2026", "body": "drop confused comment about \u201cglobal_step\u201d field", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 19518, "title": "Hello every one. I tried to export the models to a compact format. I used the following command ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 19517, "title": "Support on TPU for tf.contrib.framework.sort", "body": "We tried to train with TPU on our model which needs to sort a tensor along an axis. Using  tf.contrib.framework.sort results in the following error:\r\n```\r\nNotFoundError (see above for traceback): No registered 'TopKV2' OpKernel for XLA_TPU_JIT devices compatible with node TPUReplicate/loop/resnet/unit_2_4/sub1/conv1/sort/TopKV2 = TopKV2[T=DT_FLOAT, sorted=true, _device=\"/device:TPU_REPLICATED_CORE\"](TPUReplicate/loop/resnet/unit_2_4/sub1/conv1/sort/transpose, TPUReplicate/loop/resnet/unit_2_4/sub1/conv1/sort/strided_slice)\r\n```\r\n\r\nAre there any alternatives? \r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: ubuntu\r\nTensorFlow installed from: official\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: TPU on gcloud\r\nExact command to reproduce: any training script using tf.contrib.framework.sort", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "yes", "Hi! @1vn ,\r\nIt seems that  you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 or later version and let us know if the issue still persists in newer versions .Please open a new issue  with a sample code in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19517\">No</a>\n"]}, {"number": 19516, "title": "Windows can't build toco library by bazel (bazel build //tensorflow/contrib/lite/toco:toco)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: I user command (pip install tensorflow)\r\n- **TensorFlow version (use command below)**: Version: 1.7.1\r\n- **Python version**:  Python 3.6.4 :: Anaconda\r\n- **Bazel version (if compiling from source)**:  Build label: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:  bazel build //tensorflow/contrib/lite/toco:toco\r\n\r\n### Describe the problem\r\nI am trying to run the command \"toco --help\" for tensorflow lite. \r\nbut I get the error ModuleNotFoundError: No module named 'tensorflow.contrib.lite.toco.python'.\r\nso I'm going to build the toco library , when I run the command \"bazel build tensorflow/contrib/lite/toco:toco\" I get the error \"**ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package** \".\r\n\r\n### Source code / logs\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': Error downloading \r\n[https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz,\r\nhttps://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz] to C:/users/XXXX/_bazel_XXXX/pqfcltl7/external/io_bazel_rules_closure/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz: All mirrors are down: []\r\nINFO: Elapsed time: 57.492s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n", "comments": ["This just looks like a download error. Did you retry downloading?\r\nClosing because the mirrors are live, and this issue is a connection issue.\r\n", "@gunan  thanks your response. I retried downloading but it's same problem :(\r\n", "Is it possible  these websites are blocked on your network?\r\nOne of the mirrors is on GCP, and the other one is hosted by github. To verify, you can manually try downloading these packages:\r\n```\r\nhttps://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz\r\nhttps://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz] to \r\n```", "@gunan thanks your response :) . I have downloaded the two files, and then I merge and move them to [C:/users/XXXX/_bazel_XXXX/pqfcltl7/external/io_bazel_rules_closure]  finally I execute [bazel build //tensorflow/contrib/lite/toco:toco]  instruction, but it also show same error.\r\n\r\ndo you have facebook or other social account?, could I make friend with you? :)"]}, {"number": 19515, "title": "KeyError: \"The following input nodes were not found: set(['Mul'])\\n\"", "body": "I have a model named rounded_graph.pb that's been produced from the TensorFlow for Poets codelab. Then I should strip the DecodeJpeg Op from model by the following command :\r\n\r\npython strip_unused.py --input_graph=rounded_graph.pb --output_graph=output.pb --input_node_names=\"Mul\" --output_node_names=\"final_result\" --input_binary=true  \r\n\r\nBut I got the following error :\r\n\r\n  File \"strip_unused.py\", line 107, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/eli/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"strip_unused.py\", line 61, in main\r\n    FLAGS.placeholder_type_enum)\r\n  File \"/home/eli/.local/lib/python2.7/site-packages/tensorflow/python/tools/strip_unused_lib.py\", line 114, in strip_unused_from_files\r\n    placeholder_type_enum)\r\n  File \"/home/eli/.local/lib/python2.7/site-packages/tensorflow/python/tools/strip_unused_lib.py\", line 83, in strip_unused\r\n    raise KeyError(\"The following input nodes were not found: %s\\n\" % not_found)\r\nKeyError: \"The following input nodes were not found: set(['Mul'])\\n\"\r\neli@eli-virtual-machine:~/Documents/tensorflow-master/tensorflow/python/tools$ python strip_unused.py --input_graph=optimized_graph.pb --output_graph=output.pb --input_node_names=\"Mul\" --output_node_names=\"final_result\" --input_binary=true \r\nTraceback (most recent call last):\r\n  File \"strip_unused.py\", line 107, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/eli/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"strip_unused.py\", line 61, in main\r\n    FLAGS.placeholder_type_enum)\r\n  File \"/home/eli/.local/lib/python2.7/site-packages/tensorflow/python/tools/strip_unused_lib.py\", line 114, in strip_unused_from_files\r\n    placeholder_type_enum)\r\n  File \"/home/eli/.local/lib/python2.7/site-packages/tensorflow/python/tools/strip_unused_lib.py\", line 83, in strip_unused\r\n    raise KeyError(\"The following input nodes were not found: %s\\n\" % not_found)\r\nKeyError: \"The following input nodes were not found: set(['Mul'])\\n\"\r\n\r\nIf I don't do this before moving the model to APP, \"Error initializing TensorFlow\" will happen when APP run.\r\n\r\nWhat should I do? Please help me, thanks!", "comments": ["I got the error: APP use the inceptionv3 model ,but TensorFlow for Poets codelab have updated the model from inceptionv3 to mobilenet. So error happened.", "/CC @wolffg any ideas? Maybe this is more suited to StackOverflow.", "Mark, do you have insight here?", "If you are getting an error KeyError: \"The following input nodes were not found: {'input'}\\n\" then change \"input\" to \"Mul\"\r\nIf anybody still has doubt refer [TensorFlow on Mobile: Tutorial](https://towardsdatascience.com/tensorflow-on-mobile-tutorial-1-744703297267)", "in this case, the node named \"Mul\" is not included in graph.\r\nconfirm the graph by TensorBoard.", "if the Protocol Buffer file is created by retrain.py, change the input node name to \"Placeholder\".\r\nmy graph optimized successfully by doing so.", "I'm going to close this, as we have a vastly different converter at this point."]}, {"number": 19514, "title": "Bug: About tf.Variable and tf.get_variable", "body": "Hi,\r\n    I have a question about tf.Variable and tf.get_variable, it seems that the tf.get_variable cannot share the variables defined by tf.Variable.\r\n    My test code is like this:\r\n\r\n```\r\n#! /usr/bin/python\r\nimport tensorflow as tf\r\ndef main():\r\n\twith tf.variable_scope(\"scope_1\"):\r\n\t\tv1 = tf.Variable([1], name=\"v1\", dtype=tf.float32)\r\n\tprint v1\r\n\twith tf.variable_scope(\"scope_1\", reuse=True):\r\n\t\tv2 = tf.get_variable(\"v1\", [1])\r\n        print v2\r\nif __name__ == \"__main__\":\r\n\tmain()\r\n```\r\n\r\n    The output:\r\n\r\n![2018-05-24 10 18 55](https://user-images.githubusercontent.com/18585014/40461001-e46e7094-5f3b-11e8-952b-1656c40ddf03.png)\r\n    Um..., from the first terminal output line, we can see that there exists a variable named \"scope_1/v1:0\", but tf.get_variable still can't find the variable? So, I'm confused about this.\r\n    Can you help me?\r\n    Thank you very much.\r\n    GeWei\r\n\r\n", "comments": ["Just execute below code and you will understand that both of the below lines create variables under different name. v1:0, v1_1:0(Which may be a bug!). Just use get_variable everywhere with different reuse property, that should work!\r\n\r\nv1 = tf.Variable([1], name=\"v1\", dtype=tf.float32)\r\nv2 = tf.get_variable(\"v1\", [1], dtype=tf.float32)\r\n\r\n", "Hi, @achalshah20,\r\nThank you for your reply.\r\nI've tested something like this:\r\n\r\n```\r\nv1 = tf.get_variable(\"v1\", [1], dtype=tf.float32)\r\n```\r\nThen I got v1 as \"v1:0\", and I opened a new terminal, input some code:\r\n```\r\nv1 = tf.Variable([1], name=\"v1\", dtype=tf.float32)\r\n```\r\nI got the same output \"v1:0\". So I think, when you input the two lines of codes below, v2  really knows that \"v1:0\" exists, so v2 renames itself as \"v1_1:0\".\r\n```\r\nv1 = tf.Variable([1], name=\"v1\", dtype=tf.float32)\r\nv2 = tf.get_variable(\"v1\", [1], dtype=tf.float32)\r\n```\r\nIf I input these two lines of codes, I will get the \"v1 already exists\" Error.\r\n```\r\nv1 = tf.get_variable(\"v1\", [1], dtype=tf.float32)\r\nv2 = tf.get_variable(\"v1\", [1], dtype=tf.float32)\r\n```\r\nUm...it confuses me.\r\nGeWei\r\n\r\n", "@geweihgg Could you explain why you need to mix them? I think it's always recommend to use `tf.get_variable` only, and official document [Sharing variables](https://www.tensorflow.org/programmers_guide/variables#sharing_variables) might be useful for you.", "Hi, @facaiy ,\r\n    I have no need to mix them, but just feel puzzled at this phenomenon~\r\n    Thank you for your recommendation, I'll learn this document further~\r\nGeWei", "@geweihgg tf.variable and get_variable is designed for this purpose.\r\n\r\nhttps://stackoverflow.com/questions/45074049/tensorflow-how-does-tf-get-variable-work\r\n\r\n> The variables created with tf.Variable can't be shared,", "Hi, @achalshah20 ,\r\nThank you very much!\r\nSo the tf.Variable is designed just for creating new variables, which brings benefits to writing the libraries. Then we users can recreate as much layers as we can, just using one line of code like this:\r\n```\r\nnet = fully_connected(input_tensor, shape)\r\n```\r\nOk, thank you again~\r\nGeWei\r\n ", "As was discussed, this is working as intended. Further discussion is recommended to be done on stackoverflow."]}, {"number": 19513, "title": "Cannot use lookup table in dataset when using  Distribute MirroredStrategy", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\ntensorflow-gpu 1.8.0\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n![image](https://user-images.githubusercontent.com/12636388/40460717-b398fd3c-5f3a-11e8-946c-85f8c2516951.png)\r\n![image](https://user-images.githubusercontent.com/12636388/40460728-becca0fa-5f3a-11e8-932f-b0cb5b3ed8dd.png)\r\n\r\nI use a lookup table in dataset map function, for mapping string labels to digits.\r\nWhen I use this dataset feed to estimator using MirroredStrategy,\r\nit gives this error.\r\n\r\nIf using eager mode, it may support this feature,  but estimator is not supported in eager mode\r\n\r\nSo how can I still keeping lookup table while using MirroredStrategy?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n", "comments": ["I see a different error in your notebook than what you mentioned here.\r\nFor MirroredStrategy assigning to @guptapriya \r\n", "Hi, I can't seem to find a link to the notebook, is there a place I can see more of the code? In particular, where is the make_one_shot_iterator being created? \r\n\r\nWe used make_one_shot_iterator in mirrored strategy earlier which would cause this issue and it was fixed around Apr 18: https://github.com/tensorflow/tensorflow/commit/03d18ae232c3cff4c56d1efec7bf29f9b16c4f68#diff-17d893fed7bb36da99ce6c80d933d00b)\r\n\r\nIf I have the timing of 1.8 right, this fix may not have made it to 1.8 release. So if this is indeed the root cause, if you try the nightly builds, you should not see this issue. Would it be possible for you to test with the tf-nightly-gpu?\r\nAlso, have you already tested without mirrored strategy and it works fine then?\r\n\r\n\r\n\r\n\r\n", "@guptapriya   Yes,  I tested it without mirrored strategy, and it works fine. At last, I moved the lookup table  to model_fn,  not in dataset, it works. ", "@guptapriya  Another question, is mirrored strategy only support session hook? Cause I use a  logging hook in EstimatorSpec, it  report error when using mirrored strategy.", "I see, thanks. Would you be able to try the original code with the tf-nightly-gpu build and see if it still fails? \r\n\r\nRegarding hooks, they are supported. Can you share the error you are seeing along with the code (perhaps in a different github issue)? I wonder if the error is related to the hook as such, or with a tensor you're trying to log. For e.g., we successfully use the LoggingTensorHook in official resnet model alongwith MirroredStrategy: \r\nhttps://github.com/tensorflow/models/blob/master/official/utils/logs/hooks_helper.py#L92\r\nhttps://github.com/tensorflow/models/blob/master/official/resnet/resnet_run_loop.py#L418\r\n\r\n", "@guptapriya  Cannot open the link, its for internal use?", "Ah my bad, updated the links. \r\n\r\nWere you able to tell whether the original issue goes away with nightly builds?", "@guptapriya  wait a moment, I'll try~\ud83d\ude04", "@guptapriya  yeah, fixes", "Great, I will resolve this issue then as this fix should be in release 1.9 which is now out. Thanks for reporting! Please file another issue for hooks if this is still an issue. "]}]