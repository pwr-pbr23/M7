[{"number": 3437, "title": "google/protobuf/any.proto: File not found", "body": "### Environment info\n\nOperating System: ubuntu14.04\n\nI have install java8 and the latest version of bazel ([bazel release 0.3.0-2016-07-21 (@2e1dbd7)]) pulled from github. After then, I pulled the tensorflow.igt using the command: git clone --recurse-submodules https://github.com/tensorflow/tensorflow.\n\nThen, I intended to install tensorflow following the steps:\n1) Install java8.\n2) Install bazel ([bazel release 0.3.0-2016-07-21 (@2e1dbd7)])  pulled from github.\n3) git clone --recurse-submodules https://github.com/tensorflow/tensorflow.\n4) ./configure (in the root dir of tensorflow, without using google cloud, without configuration of gpu)\n5) bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\nUnfortunately, error prompted in Step 5:\n      \".. tensorflow/tensorflow/tools/pip_package/BUILD:23:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error cloning repository: https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target caused by PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target caused by unable to find valid certification path to requested target and referenced by '//tensorflow/tools/pip_package:build_pip_package'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: \nElapsed time: 0.102s\"\n\nI have confirmed that git clone can be used normally to pull the code from https://github.com/google/protobuf.\n\nAnyone knows how this problem can be fixed ?\n", "comments": ["After I downloaded the dependencies manually, the previous errors were passed. However, I got the following errors:\n\nWARNING: ../tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nERROR: ../.cache/bazel/_bazel_dl/647b15c3637d5190d4f70d81e8357baf/external/protobuf/BUILD:331:1: in cc_binary rule @protobuf//:protoc: cycle in dependency graph:\n    //tensorflow/tools/pip_package:build_pip_package\n    //tensorflow/tensorboard:tensorboard\n    //tensorflow/tensorboard/backend:server\n    //tensorflow/python:summary\n    //tensorflow/python:util\n    @protobuf//:protobuf_python\n    @protobuf//:python/google/protobuf/wrappers_pb2.py\n    @protobuf//:protobuf_python_genproto\n- @protobuf//:protoc (host)\n  @protobuf//:protoc_lib (host)\n  @protobuf//:protobuf (host)\n  @protobuf//:protobuf_lite (host)\n  @protobuf//:src/google/protobuf/util/message_differencer_unittest.pb.h (host)\n  @protobuf//:cc_test_protos_genproto (host)\n  @protobuf//:cc_wkt_protos_genproto (host)\n- @protobuf//:protoc (host)\n  This cycle occurred because of a configuration option.\n  ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\n  INFO: Elapsed time: 0.146s\n", "@martinwicke can you take a look at the Bazel issue?  Thanks.\n", "Are you sure you have the correct version of protobuf? It's sadly very specific: 3.0.0b2.\n", "I downloaded this version of protobuf ------ https://github.com/google/protobuf/archive/v3.0.0-beta-2.tar.gz\nAnd execute \"bazel clean --expunge\", and then use bazel to build tensorflow agian. However, different errors occured:\n\n`ERROR: /home/dl/.cache/bazel/_bazel_dl/647b15c3637d5190d4f70d81e8357baf/external/protobuf/BUILD:32:5: no such variable: internal_gen_well_known_protos_java.\nERROR: /home/dl/.cache/bazel/_bazel_dl/647b15c3637d5190d4f70d81e8357baf/external/protobuf/BUILD:525:1: name 'internal_gen_well_known_protos_java' is not defined.\nERROR: /home/dl/.cache/bazel/_bazel_dl/647b15c3637d5190d4f70d81e8357baf/external/protobuf/BUILD:633:1: missing mandatory positional argument 'include' while calling internal_copied_filegroup(name, srcs, include, **kargs).\nERROR: /home/dl/.cache/bazel/_bazel_dl/647b15c3637d5190d4f70d81e8357baf/external/protobuf/BUILD:670:1: missing mandatory positional argument 'include' while calling internal_copied_filegroup(name, srcs, include, **kargs).\nERROR: /home/dl/.cache/bazel/_bazel_dl/647b15c3637d5190d4f70d81e8357baf/external/protobuf/BUILD:723:1: name 'internal_protobuf_py_tests' is not defined.\nERROR: /home/dl/.cache/bazel/_bazel_dl/647b15c3637d5190d4f70d81e8357baf/external/protobuf/BUILD:644:1: Target '@protobuf//:use_fast_cpp_protos' contains an error and its package is in error and referenced by '@protobuf//:protobuf_python'.\nERROR: /home/dl/bxl/tensorflow/tensorflow/python/BUILD:917:1: Target '@protobuf//:protobuf_python' contains an error and its package is in error and referenced by '//tensorflow/python:util'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\n`\n", "@martinwicke \n", "Which version of bazel are you using?\n", "the above problem concerned with bazel is solved.  However, the following problem occured:\n\n../tensorflow/tensorflow/core/BUILD:91:1: null failed: protoc failed: error executing command bazel-out/host/bin/external/protobuf/protoc '--cpp_out=bazel-out/local-opt/genfiles/' -I. -Isrc tensorflow/core/example/example.proto tensorflow/core/example/example_parser_configuration.proto ... (remaining 30 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nsrc: warning: directory does not exist.\ngoogle/protobuf/any.proto: File not found.\ntensorflow/core/protobuf/meta_graph.proto: Import \"google/protobuf/any.proto\" was not found or had errors.\ntensorflow/core/protobuf/meta_graph.proto:41:5: \"google.protobuf.Any\" is not defined.\ntensorflow/core/protobuf/meta_graph.proto:162:14: \"google.protobuf.Any\" is not defined.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 0.263s, Critical Path: 0.02s\n", "Can you try this with bazel 0.2.3? It is possible tensorflow doesn't work with bazel at head right now.\n", "@martinwicke Should this be closed due to lack of response, or is there more checking to do?\n", "I do not know the status of this -- but it seems like an isolated thing. Closing, feel free to reopen with new information if it persists.\n"]}, {"number": 3436, "title": "serving tretrained inception model with different number of lables to initial training", "body": "I'm struggling to serve a 'fine tuned' retrained inceptions model. After retraining on just two classes/labels. I get an error when trying to export to model:\n\nFile \"/home/ubuntu/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/clie nt/session.py\", line 743, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1001] rhs s hape= [3] [[Node: save/Assign_30 = Assign[T=DT_FLOAT, _class=[\"loc:@logits/logits/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](logits/logits/biases, save/restore_slice_30)]] Caused by op u'save/Assign_30', defined at:\n", "comments": ["@nfiedel: can you take a look Noah?  Thanks.\n", "@fangweili - any input?\n", "can't tell just from the error message above. mind sharing the code?\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 3435, "title": "Segmentation fault in Diagnostician on MacOS", "body": "I get segmentation fault on MacOS on creating new session if GPU is unavailable (ie, through `export CUDA_VISIBLE_DEVICES=`)\n\nIt's due to this [line](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L319)\n\n```\n    const char * version = CFStringGetCStringPtr((CFStringRef)CFDictionaryGetValue(cuda_driver_info,   kCFBundleVersionKey), kCFStringEncodingUTF8);\n    CFRelease(kext_infos);\n    return StringToDriverVersion(version);\n\n```\n\n`version` is a NULL, which causes SEGSERV during conversion to string. \n\nDocumentation for `CFStringGetCStringPtr` says that [one should check for NULL](https://developer.apple.com/library/mac/documentation/CoreFoundation/Conceptual/CFStrings/Articles/AccessingContents.html#//apple_ref/doc/uid/20001184-100980-TPXREF112) and call `CFStringGetCString` if so\n\n```\nbazel build -c dbg --config=cuda //tensorflow/tools/pip_package:build_pip_package\nexport CUDA_VISIBLE_DEVICES=\ncat > simple.py\nimport tensorflow as tf\nsess = tf.Session()\n^D\n\nlldb python\n(lldb) r simple.py\n\nProcess 48123 stopped\n* thread #1: tid = 0x37a6a8, 0x00007fff9b24d152 libsystem_c.dylib`strlen + 18, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x0)\n    frame #0: 0x00007fff9b24d152 libsystem_c.dylib`strlen + 18\nlibsystem_c.dylib`strlen:\n->  0x7fff9b24d152 <+18>: pcmpeqb (%rdi), %xmm0\n    0x7fff9b24d156 <+22>: pmovmskb %xmm0, %esi\n    0x7fff9b24d15a <+26>: andq   $0xf, %rcx\n    0x7fff9b24d15e <+30>: orq    $-0x1, %rax\n\n(lldb) up\nup\nframe #1: 0x00000001036ecb05 _pywrap_tensorflow.so`std::__1::char_traits<char>::length(__s=0x0000000000000000) + 21 at string:640\n   637  \n   638      static inline int compare(const char_type* __s1, const char_type* __s2, size_t __n)\n   639          {return __n == 0 ? 0 : memcmp(__s1, __s2, __n);}\n-> 640      static inline size_t length(const char_type* __s) {return strlen(__s);}\n   641      static inline const char_type* find(const char_type* __s, size_t __n, const char_type& __a)\n   642          {return __n == 0 ? NULL : (const char_type*) memchr(__s, to_int_type(__a), __n);}\n   643      static inline char_type* move(char_type* __s1, const char_type* __s2, size_t __n)\n(lldb) up\nup\nframe #2: 0x0000000107725ac5 _pywrap_tensorflow.so`perftools::gputools::cuda::Diagnostician::FindKernelDriverVersion() [inlined] std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::basic_string(this=\"\", __s=0x0000000000000000) + 81 at string:2005\n   2002 basic_string<_CharT, _Traits, _Allocator>::basic_string(const value_type* __s)\n   2003 {\n   2004     _LIBCPP_ASSERT(__s != nullptr, \"basic_string(const char*) detected nullptr\");\n-> 2005     __init(__s, traits_type::length(__s));\n   2006 #if _LIBCPP_DEBUG_LEVEL >= 2\n   2007     __get_db()->__insert_c(this);\n   2008 #endif\n(lldb) up\nup\nframe #3: 0x0000000107725a74 _pywrap_tensorflow.so`perftools::gputools::cuda::Diagnostician::FindKernelDriverVersion() [inlined] std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::basic_string(this=\"\", __s=0x0000000000000000) + 21 at string:2003\n   2000 template <class _CharT, class _Traits, class _Allocator>\n   2001 inline _LIBCPP_INLINE_VISIBILITY\n   2002 basic_string<_CharT, _Traits, _Allocator>::basic_string(const value_type* __s)\n-> 2003 {\n   2004     _LIBCPP_ASSERT(__s != nullptr, \"basic_string(const char*) detected nullptr\");\n   2005     __init(__s, traits_type::length(__s));\n   2006 #if _LIBCPP_DEBUG_LEVEL >= 2\n(lldb) up\nup\nframe #4: 0x0000000107725a5f _pywrap_tensorflow.so`perftools::gputools::cuda::Diagnostician::FindKernelDriverVersion() + 287 at cuda_diagnostics.cc:319\n   316      // in kCFBundleVersionKey as is returned by cuDriverGetVersion\n   317      const char * version = CFStringGetCStringPtr((CFStringRef)CFDictionaryGetValue(cuda_driver_info, kCFBundleVersionKey), kCFStringEncodingUTF8);\n   318      CFRelease(kext_infos);\n-> 319      return StringToDriverVersion(version);\n(lldb) frame variable --no-args\nframe variable --no-args\n(CFStringRef [1]) kext_ids = ([0] = 0x0000000108767208)\n(CFArrayRef) kext_id_query = 0x000000013e1f2190\n(CFDictionaryRef) kext_infos = 0x000000013e1f21d0\n(CFDictionaryRef) cuda_driver_info = 0x000000013f322700\n(tensorflow::Status) status = {\n  state_ = 0x000000011ec8000a\n}\n(const char *) version = 0x0000000000000000\n\n```\n\nrelated #2980 \n", "comments": []}, {"number": 3434, "title": "New user op warp ctc", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 3433, "title": "Update tf.nn.rnn.md", "body": "It is hard to read some parts of markdown with gfm newline grammar.\nI think adding code block is more good for reading.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "This is a generated file, please change the code comments instead\n", "Thanks. I changed it by your comment\n", "You have to edit https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L75 -- all of the .md files are generated from the source code documentation, so this won't change anything.\n", "Oh, I see. I misunderstood comment. \nI fixed rnn.py code documentation for code block which shown as one line\n", "The changes you made don't look like what you wanted. Can you re-check?\n", "I comment what's my purpose in file changed tab. https://github.com/tensorflow/tensorflow/pull/3433/files#r71999251\n", "Thanks your comment. Could you recheck it?\n", "Great! Thanks!\n", "Jenkins, test this please.\n"]}, {"number": 3432, "title": "missing 'train' module", "body": "trying to follow this document: https://www.tensorflow.org/versions/r0.9/how_tos/distributed/index.html\n\ncan't import tf.train because train doesn't exist.\n\ni looked through the repo; it appears that's a correct appraisal. TF does not have a train package.\n\n??\n", "comments": ["Please _do_ fill in the issue template to include more info.\n\n`tf.train` should work and many people run the classes in it regularly.\n", "Automatically closing due to lack of recent activity. Please reopen if further information becomes available.\n"]}, {"number": 3431, "title": "Getting \"missing dependency declarations\" with bazel 0.3.0", "body": "I just upgraded Bazel/synced and now I'm getting same errors as in #1157 \n\n```\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n\nERROR: /home/yaroslavvb/tensorflow.git/tensorflow/tensorflow/core/kernels/BUILD:1080:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:cwise_op_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/cwise_op_gpu_floor.cu.cc':\n  '/usr/local/cuda-8.0/include/cuda_runtime.h'\n  '/usr/local/cuda-8.0/include/host_config.h'\n  '/usr/local/cuda-8.0/include/builtin_types.h'\n  '/usr/local/cuda-8.0/include/device_types.h'\n  '/usr/local/cuda-8.0/include/host_defines.h'\n  '/usr/local/cuda-8.0/include/driver_types.h'\n  '/usr/local/cuda-8.0/include/surface_types.h'\n  '/usr/local/cuda-8.0/include/texture_types.h'\n...\n```\n", "comments": ["Work-around from #1157 seems to work\n\nadd\n`cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\"`\nto \n`third_party/gpus/crosstool/CROSSTOOL`\n", "Good to know it can be made to work, @yaroslavvb thanks for persisting the workaround.\n", "Weird that you guys aren't hitting this issue on the nightly wheels, unless the nightlies are done with newer Bazel than 0.3.0\n", "Same \"missing dependency declarations\" problem still exists in head version, thanks @yaroslavvb for the workaround.\n", "Just updated to current version cc3153a7a0a23533d14ead34db37e4ccd7892079 (v0.10.0rc0-785-gcc3153a) and got the usual missing dependency error.\n\nHOWEVER, the new build doesn't create a CROSSTOOL file - so the usual fix doesn't work...  I tried copying in the CROSSTOOL from an older revision (with the cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\" change) - but I'm still getting the missing dependency error.  \nNot sure what .tpl files are, but I tried adding the cxx_builtin change in that file, also no luck.\n\nInstalled version of CUDA and cuDNN: 8.0.26 and v5\nbazel version : 0.3.0\nUbuntu 14.04.4 LTS\n3xGTX980Ti\n\nUPDATE:\nIn case this helps anyone... I was able to get past the issue by setting the CUDA version explicitly when running the configure script.\n", "guys i am trying to \n\nadd\ncxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\"\nto\nthird_party/gpus/crosstool/CROSSTOOL\n\nbut i get the following, where in the CROSSTOOL should i copy paste that line ?\n\nERROR: java.io.IOException: Could not read the crosstool configuration file 'CROSSTOOL file /home/---/Downloads/tensorflow/third_party/gpus/crosstool/CROSSTOOL', because of a parser error (1:1: Input contains unknown fields and/or extensions:\n1:1:    com.google.devtools.build.lib.view.config.crosstool.CrosstoolRelease.cxx_builtin_include_directory).\n", "@Sadrpour \n\nExactly the same error here ... I'm testing on the most current bazel-git, namely, bazel 0.3.2 \nhttps://github.com/bazelbuild/bazel\n\nThe error message is:\n\n> ERROR: java.io.IOException: Could not read the crosstool configuration file 'CROSSTOOL file /tmp/bazel_uawUdYmF/out/external/local_config_cc/CROSSTOOL', because of a parser error (132:42: String missing ending quote.).\n\ncheers\nPei\n", "@Sadrpour @jiapei100  I guess the parsing error was caused because you put the line in a wrong location. \n\nputting it like the below will cause an parsing error. \ndefault_target_cpu: \"same_as_host\"\ncxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\"\n\nputting it inside toolchain namespace like:\ntoolchain{\n  cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\"\n}\n\ncaused no problem, but still it is causing missing dependency problem. \n", "@dzhyeon\nI actually posted a very similar issue at [https://github.com/bazelbuild/bazel/issues/1996](https://github.com/bazelbuild/bazel/issues/1996) .\n\nWhich file and which line should this line\n`cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include`\nbe added to?\n\nCheers\nPei\n", "Getting following error after updating `cxx_builtin_include_directory: \"/usr/local/cuda-7.5/include`\r\n\r\n```shell\r\nbazel test -c opt --config=cuda --define using_cuda_nvcc=true --define using_gcudacc=true syntaxnet/... util/utf8/...\r\n................\r\nWARNING: /home/irfan/.cache/bazel/_bazel_irfan/a05fc8a5ac651b688321e83d1f272360/external/org_tensorflow/tensorflow/workspace.bzl:72:5: tf_repo_name was specified to tf_workspace but is no longer used and will be removed in the future.\r\nERROR: java.io.IOException: Could not read the crosstool configuration file 'CROSSTOOL file /home/irfan/.cache/bazel/_bazel_irfan/a05fc8a5ac651b688321e83d1f272360/external/local_config_cuda/crosstool/CROSSTOOL', because of a parser error (259:1: Input contains unknown fields and/or extensions:\r\n259:1:\tcom.google.devtools.build.lib.view.config.crosstool.CrosstoolRelease.cxx_builtin_include_directory).\r\nINFO: Elapsed time: 6.346s\r\nERROR: Couldn't start the build. Unable to run tests.\r\n```", "> Just updated to current version [cc3153a](https://github.com/tensorflow/tensorflow/commit/cc3153a7a0a23533d14ead34db37e4ccd7892079) (v0.10.0rc0-785-gcc3153a) and got the usual missing dependency error.\r\n> \r\n> HOWEVER, the new build doesn't create a CROSSTOOL file - so the usual fix doesn't work... I tried copying in the CROSSTOOL from an older revision (with the cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\" change) - but I'm still getting the missing dependency error.\r\n> Not sure what .tpl files are, but I tried adding the cxx_builtin change in that file, also no luck.\r\n> \r\n> Installed version of CUDA and cuDNN: 8.0.26 and v5\r\n> bazel version : 0.3.0\r\n> Ubuntu 14.04.4 LTS\r\n> 3xGTX980Ti\r\n> \r\n> UPDATE:\r\n> In case this helps anyone... I was able to get past the issue by setting the CUDA version explicitly when running the configure script.\r\n\r\nHow to set CUDA version explicitly?"]}, {"number": 3430, "title": "Ignoring gpu device with Cuda multiprocessor count: 7.", "body": "I am unable to run TF on a GPU, despite the fact that I have 2 CUDA-enabled K4200 NVIDIA QUATTRO (Compute Capability 3.0)\n### Environment info\n\nOperating System: linux 14.04 x64\n\nInstalled version of CUDA and cuDNN: \n1. CUDA 7.5\n2. cuDNN 4\n\nIf installed from binary pip package, provide:\n1. Ubuntu/Linux 64-bit, GPU enabled, Python 2.7\n2. TF v0.9.0\n### Steps to reproduce\n1. Run IPython Notebook\n2. Run a tensorflow session (I was doing Udacity DL course)\n3. In IPython log, observe:\n\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: Quadro K4200\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.784\npciBusID 0000:03:00.0\nTotal memory: 4.00GiB\nFree memory: 3.96GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3b717e0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: Quadro K4200\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.784\npciBusID 0000:04:00.0\nTotal memory: 4.00GiB\nFree memory: 3.86GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:793] Ignoring gpu device (device: 0, name: Quadro K4200, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 7. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:793] Ignoring gpu device (device: 1, name: Quadro K4200, pci bus id: 0000:04:00.0) with Cuda multiprocessor count: 7. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n### What have you tried?\n1. Setting TF_MIN_GPU_MULTIPROCESSOR_COUNT to 4, both locally and system-wide\n### Logs or other output that would be helpful\n\necho $TF_MIN_GPU_MULTIPROCESSOR_COUNT\n4\n\nAm I missing something? Setting the environment variable seems to have no effect.\n", "comments": ["Can you try \n\n```\nbazel --copt=-DTF_MIN_GPU_MULTIPROCESSOR_COUNT=4 build <...>\n```\n\nor \n\n```\nTF_MIN_GPU_MULTIPROCESSOR_COUNT=4 bazel build <...>\n```\n\nAdditionally, perhaps add `--copt=-DTF_EXTRA_CUDA_CAPABILITIES=3.0`.\n", "According to \n\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K4200, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Quadro K4200, pci bus id: 0000:04:00.0)\n\nit now seems to work correctly. Thanks!\n"]}, {"number": 3429, "title": "[Python][Quantized Inference] 'Zero is not representable' when doing inference from quantized const graph def", "body": "I used the checkpoint file and the graph def file to generate the const graph def file using the freeze_graph script. Then I used the quantize_graph tool to generate a quantized model. The output file sizes from these steps seem reasonable. But I failed to run a inference using the generated quantized model.\n\nTried both tensorflow 0.8.0 and 0.9.0, they both failed but with different errors though. For 0.80, it will output:\n\nW tensorflow/contrib/quantization/kernels/quantized_conv_ops.cc:201] Zero is not representable in the quantized range used by the input. This means QuantizedConv2d has to fall back to a slow implementation, since the border of zero values can't be represented easily. You should try to construct graphs that avoid this situation.\"\n\nFor 0.9.0, it wouldn't load the lib with:\n_quantized_kernels = tf.load_op_library(PATH_TO_KERNAL_OS)\n_quantized_ops = tf.load_op_library(PATH_TO_OPS_OS)\nI got some signature errors.\n### Environment info\n\nOperating System:\nUbuntu 14.04\n\nIf installed from binary pip package, provide:\nInstalled the 0.8.0, but tried with 0.9.0 as well.\n### Steps to reproduce\n1. Generate a checkpoint file and graph def file\n2. Using freeze_graph and quantize_graph to generate a quantized const graph def binary file\n3. Build the quantized_kernels.so and quantized_ops.so\n4. tf.load_op_library() to load the two libraries. version 0.9.0 will fail at this step\n5. Extract the input and output node names, feed the input properly and run the session \n6. version 0.8.0 will fail at this step with the following errors:\n\nW tensorflow/contrib/quantization/kernels/quantized_conv_ops.cc:201] Zero is not representable in the quantized range used by the input. This means QuantizedConv2d has to fall back to a slow implementation, since the border of zero values can't be represented easily. You should try to construct graphs that avoid this situation.\nW tensorflow/contrib/quantization/kernels/quantized_conv_ops.cc:201] Zero is not representable in the quantized range used by the input. This means QuantizedConv2d has to fall back to a slow implementation, since the border of zero values can't be represented easily. You should try to construct graphs that avoid this situation.\nterminate called after throwing an instance of 'std::bad_alloc'\n  what():  std::bad_alloc\nAborted (core dumped)\n### What have you tried?\n\nDescribed above\n### Logs or other output that would be helpful\n\nAdded above.\n", "comments": ["@petewarden \n", "I also encountered the signature error when load quantized_kernels.so and quantized_ops.so in tensorflow 0.9.0. \n", "Problems solved with 0.10.0 version. Great!\n", "It sounds like this one has been solved, so closing.\n"]}, {"number": 3428, "title": "Fix eigen hash detection in download_dependencies.sh", "body": "Fixes #3427 \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3427, "title": "Failing dependency downloads in latest", "body": "### Environment info\n\nOperating System: OSX El Capitan\nLatest pull on 20th July 2016: c5983f87f0402f2cb8c627807917ebdf8e4d4bb6\n\nRun: tensorflow/contrib/makefile/download_dependencies.sh\n\nFailing output:\n- DOWNLOADS_DIR=tensorflow/contrib/makefile/downloads\n- mkdir tensorflow/contrib/makefile/downloads\n  ++ cat eigen.BUILD\n  ++ grep archive_dir\n  ++ head -1\n  ++ cut -f3 -d-\n  ++ cut -f1 '-d\"'\n- EIGEN_HASH=\n- curl https://bitbucket.org/eigen/eigen/get/.tar.gz -o /tmp/eigen-.tar.gz\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  100 26011  100 26011    0     0  34718      0 --:--:-- --:--:-- --:--:-- 34727\n- tar xzf /tmp/eigen-.tar.gz -C tensorflow/contrib/makefile/downloads\n  tar: Unrecognized archive format\n  tar: Error exit delayed from previous errors.\n", "comments": ["As shown in the output:\n\nThe download_dependencies.sh tries to grep the eigen hash from the eigen.BUILD file by finding the archive_dir. But there is no archive_dir in eigen.BUILD so it finds nothing.\n\narchive_dir containing the eigen hash was removed in c5983f87f0402f2cb8c627807917ebdf8e4d4bb6\n\nThis broke the contrib makefiles for ios, android etc.\n", "In the latest Tensorflow, this problem pops up again. \r\nWhen I execute tensorflow/contrib/makefile/download_dependencies.sh\r\nI get following error:\r\ndownloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz\r\ntar: Unrecognized archive format\r\ntar: Error exit delayed from previous errors.\r\n\r\nThis error was not there in the Tensorflow on May 10\r\nhttps://github.com/tensorflow/tensorflow/tree/a17664aa2c0049f2a3027c8b59787c6eb9241267\r\n\r\nWhen I replace the tensorflow/workspace.bzl from May 10 repo to current repo, the download_dependencies.sh works fine.\r\n\r\nAny suggestions.\r\n"]}, {"number": 3426, "title": "How to get the last timestep of the output of an RNN?", "body": "Just a general question which might also help others, namely what is the correct way to extract the last timestep of the output of an RNN. Is it as simple as \n\n`outputs, state = tf.nn.dynamic_rnn(cell=dropout_cell, inputs=outputs1, initial_state=initial_state, sequence_length=s)`\n`last = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)`\n\nOr should i transpose the output before performing gather, as per this example\n\n`outputs, state = tf.nn.dynamic_rnn(cell=dropout_cell, inputs=outputs1, initial_state=initial_state, sequence_length=s)`\n`outputs = tf.transpose(outputs, [1, 0, 2])`\n`last = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)`\n\nThank you\n", "comments": ["This is best suited for StackOverflow, as it is not a bug nor feature request.\n"]}, {"number": 3425, "title": "Add support for quantized operations to pip wheel", "body": "While quantized op layers are currently part of the built pip package\nalready, the underlying libraries are not included with the build. This\ncommit adds support by adding both ops & kernel libs to the pip package\ndependencies.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "Thanks! Hello to Portland!\n", "@martinwicke Thanks for merging. Hello back to you! \n"]}, {"number": 3424, "title": "Merge contrib/go into master, minor syntax fixes", "body": "This includes:\n- Rebase of **go** into **master**, with conflicts resolved\n- Fix `tensor_c_api.h` to resolve Go import failures\n", "comments": ["Can one of the admins verify this patch?\n", "I saw your other change doing this, so I'm sorry for the churn, but would you mind merging master on top of the go branch instead? For various reasons (not working with Jenkins, API changes, etc), we'd prefer not to merge it into master at the moment.\n", "Sure, no worries at all. I will close this and rebase `master` -> `go` sometime tomorrow. \n"]}, {"number": 3423, "title": "Fix few warnings that I see on macosx-clang compilation", "body": "", "comments": ["Can one of the admins verify this patch?\n", "The only one that I am not 100% sure is the `this == nullptr`. I don't think this should ever be possible in C++ from within a method. Right ?\n\n```\nif (this == nullptr) {      \n    return \"{nullptr}\";     \n }\n```\n", "@tensorflow-jenkins test this.\n", "@Mistobaan can you please resolve the merge conflicts?\n@martinwicke can you please kick off a jenkins test?\n", "@keveman you can trigger that too, if not, that's a problem.\n\nJenkins, test this please!\n", "@keveman fixed\n", "@martinwicke I did try uttering the magic words, but doesn't seem like the testing has happened.\n", "Jenkins? Test this please?\n", "Maybe I wasn't nice enough.\n"]}, {"number": 3422, "title": "CUDA_ERROR_ILLEGAL_ADDRESS on GTX 1080", "body": "On training a model on cifar 10 I am getting following error after couple of epochs.\n\n tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace availablech: 034 | loss: 0.21259 - acc: 0.9223 -- iter: 08128/50000\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nAborted (core dumped)\n### Environment info\n\nOperating System:\nUbuntu 14.04\n4.2.0-27-generic #32~14.04.1-Ubuntu SMP Fri Jan 22 15:32:26 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root   560184 Jul 18 17:20 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Jul 18 17:20 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Jul 18 17:20 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Jul 18 17:20 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Jul 18 17:20 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 78065952 Jul 18 18:03 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 78065952 Jul 18 18:03 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 78065952 Jul 18 18:03 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 68709594 Jul 18 18:03 /usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from sources, provide the commit hash:\ncommit a3f61c1d5c76339e6c9655dac426bb3822659772\n### Steps to reproduce\n1. I have a model to train on cifar10,  which mainly consists of convolutions, PReLU and data augmentation.andaround r epoch  34 it get the above error. \n   I have similar model running  simultaneously on  same GPU which run fine. Each of the model roughly\n   use 50% of GTX 1080 memory(TOTAL 8GB)\n### What have you tried?\n1. I tried to to run my model twice and getting same error.\n", "comments": ["Could you check two things:\n1. Try with CUDA 7.5, which is best supported.\n2. Try with a smaller model footprint.  2 \\* ~50% seems to imply difficulty from memory pressure: we'd need to eliminate this from the root causes.\n", "Automatically closing due to lack of recent activity. Please reopen if further information becomes available.\n"]}, {"number": 3421, "title": "Scope Reusability in tf.nn.dynamic_rnn", "body": "I am getting this error when I try to run the code for RNN:\n\nValueError: Variable RNN/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix does not exist, disallowed. Did you mean to set reuse=None in VarScope?\n\nHere is a snippet of my code for RNN:\n\nlstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._num_hidden, forget_bias=0.0)\ncell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] \\* self._num_layers)\nself._initial_state = cell.zero_state(batch_size, tf.float32)\n\ninputs=[tf.reshape(data_,[batch_size,num_cat]) for data_ in tf.split(0,max_length,tf.transpose(data,[1,0,2]))]\noutput,_=tf.nn.dynamic_rnn(cell,inputs,initial_state=self._initial_state,sequence_length=self.length)\n\nThe error seems to be in line:\noutput,_=tf.nn.dynamic_rnn(cell,inputs,initial_state=self._initial_state,sequence_length=self.length)\n\nMy input is a tensor with dimensions : batch_size x max_ seq_length x feature_vector_size\nI am reshaping it to a list of dimensions : batch_size x feature_vector_size\n\nI am assuming that the reusability of scope is handled internally by tf.nn.dynamic_rnn function. Any idea why this would happen? What could be the solution?\n", "comments": ["Can you paste a complete snippet program the reliably reproduces?  Might be some outside scope that wraps the lines you pasted. \n", "I actually have this exact same issue, but with different code.  I have posted the primary question here: https://stackoverflow.com/questions/38676668/rnn-cell-naming-issue-in-tensorflow \n", "I can also provide somebody the specifics to my notebook but I need you to send me a message or something, I don't want to push my password out too widely :)\n", "@drcrook1, we don't want to take your password. Could you extract the code from the notebook that could be run on standalone python?\n@sunilraiyani could you attach the full program that reproducies the issue? \nWithout a self-contained reproducible test case, we will need to close the issue.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 3420, "title": "Running Bidirectional RNN cells on parallel", "body": "is there any method to run Bidirectional RNNs on parallel as they should (they have no dependency on each other)\nI noticed that stacking 2 LSTM layer has the exact same performance as bidirection LSTM (BLSTM)\neven 4 stacked LSTM has same performance as 2 parallel BLSTMs (which simply means  that TF runtime doesn't parallelize BLSTMs at all, I also notice the same behavior on both the CPU and the GPU)\n\nNote: I am using small cells that are trivial to fit simultaneously\n\nThanks,\n", "comments": ["Could you provide your reproducible code snippet?\n", "This is somewhat lengthy, as I take the chance to also demonstrate a speed problem with dynamic_rnn when specifying sequence_length\n\n``` python\nimport tensorflow as tf\nfrom tensorflow.python.ops  import rnn, rnn_cell,array_ops\nimport numpy as np\nimport time\n\nbatch_size = 128\nn_input = 20\nn_steps = 50\nn_hidden = 40\n\nnp.random.seed(1234)\ntf.set_random_seed(1234)\n\ndata = tf.constant(np.random.uniform(-1.0, 1.0, [batch_size,n_steps,n_input]), dtype=tf.float32)\nsql  = tf.constant([n_steps] * batch_size, tf.int64)\nlstm_cell = rnn_cell.LSTMCell(num_units=n_hidden, state_is_tuple=True)\n\ndef stacked_lstm():\n    mlstm_cell = rnn_cell.MultiRNNCell([lstm_cell] * 2, state_is_tuple=True)\n    return rnn.dynamic_rnn(mlstm_cell, data, dtype=tf.float32,sequence_length=sql)[0]\n\ndef stacked_lstm_no_sql():\n    mlstm_cell = rnn_cell.MultiRNNCell([lstm_cell] * 2, state_is_tuple=True)\n    return rnn.dynamic_rnn(mlstm_cell, data, dtype=tf.float32)[0]\n\ndef bi_lstm():\n    return rnn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, data, sequence_length=sql, dtype=tf.float32)[0]\n\ndef bi_lstm_no_sql():\n    y1 = rnn.dynamic_rnn(lstm_cell, data,  dtype=tf.float32,scope='r1')[0]\n    rdata = array_ops.reverse_sequence(input=data, seq_lengths=sql, seq_dim=1, batch_dim=0)\n    y2 = rnn.dynamic_rnn(lstm_cell, rdata, dtype=tf.float32,scope='r2')[0]\n    y2 = array_ops.reverse_sequence(input=y2, seq_lengths=sql, seq_dim=1, batch_dim=0)\n    return [y1,y2]\n\ny = stacked_lstm()\n#y=stacked_lstm_no_sql()\n#y = bi_lstm()\n#y = bi_lstm_no_sql()\n\ninit = tf.initialize_all_variables()\n\nconfig = tf.ConfigProto(\n    #device_count={'GPU': 0}, #turn GPU on and off\n)\n\nwith tf.Session(config=config) as sess:\n    sess.run(init)\n    start = time.time()\n    for i in range(1000):\n        sess.run(y)\n        if(i%100 == 0): print \"Iter : \",i*batch_size\n    end = time.time()\n    print \"Finished!  \", (end - start)\n```\n\nwhen running on GTX 960, you get (i record Utilization from nvidia-setting)\nstacked_lstm              : 14.11s     GPU Util:69%\nstacked_lstm_no_sql : 10.49s     GPU Util:79%\nbi_lstm              : 16.15s     GPU Util:65%\nbi_lstm_no_sql  : 11.54s     GPU Util:75%\n\nwe note:\n1) stacked_lstm is generally faster than bi_lstm (though, bi_lstm is trivial to parallelize and should be generally up to 2X faster)\n2) removing sequence_length has huge impact on speed. However, I noted that the two methods (with and without sequence_length) return different results, and in my practical use cases removing sequence_length generally returns slightly worse results\n\nwhat is really needed is method to allow parallel execution of independent parts of the graph like BLSTMs\n", "@ebrevdo: can you take a look?\n\n@ASDen: a more accurate way to measure the running time is to wrap the results in `tf.group()`, so that the device-to-host transfer of final results is not included.\n", "@ebrevdo any thoughts here...\n", "When using a dynamic_rnn, there is an \"execution barrier\" between calls to dynamic_rnn; because of the way that data is processed at input & output to the RNN while_loop.  The bidirectional_dynamic_rnn just calls dynamic_rnn twice, much the way you do in `bi_lstm_no_sql`.  In fact, it also uses `reverse_sequence`, which _also_ forms a barrier between any two RNN calls.  There's no way around `reverse_sequence`: all its inputs must be available to properly verse time according to the counts in `sequence_length`.  If you can suggest a way to improve performance or, e.g., make a simpler dynamic sequence reversal system (which does not rely on a monolithic op) then I would like to hear it.\n", "another implicit performance penalty I noticed is that when using deep LSTM networks, using MultiRNNCell instead of manual construction can be 2-3X faster (I think because it uses a single while loop)\nthe problem with deep BLSTMs is that you can't do them with a single cell or a single while loop (layer output dependency) which means it incurs such a huge penalty solely due to many while loops involved\nany thoughts on a more \"light weight\" while loop ?\n\nNote: with big output size/batch size the performance gap decreases, however the big sizes are less useful, and I do think on cards bigger than GTX 960, even the big sizes would suffer same penalty\n", "@ebrevdo Maybe I don't understand something but is it possible to execute forward pass of dynamic_rnn on one GPU and backward pass on an another GPU. Should it be two times faster in this case?\n\n@ASDen It looks like you made opposite conclusions. Can you clarify what is faster MultiRNNCell or manually constructed cells?\n", "@avostryakov for deep networks (e.g. 8 layers and more) MultiRNNCell can be 2-3x faster than manual construction\n", "@ASDen Thank you! I'm just trying to make my code faster. A simple architecture with embedding layer, 2 LSTM layers with dynamic_rnn (sequence_length is None) + classification on top of last hidden state works 30% slower than the same architecture on theano+lasagne. I thought that tensorflow 0.9 was optimized enough.\n", "@ASDen apologies; in fact, there is no execution barrier because the following can be run in parallel:\n\nchunk 1:\nreverse x\ny_rev = lstm(x_rev)\ny_bak = reverse y_rev\n\nchunk 2:\ny_fw = lstm(x)\n\nthen you need to wait for both of these chunks to finish before you can concat them.\n\ndynamic bidirectional rnn doesn't even bother with the concat - you get both pieces and can concat them if you wish.\n\nright now, running the fw & reverse on multiple GPUs would require creating a special device setter and calling bidirectional rnn within that context.  see, e.g., the [replica_device_setter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/device_setter.py) function, the result of which you would pass to tf.device.\n", "@avostryakov can you provide some details about the sizes of your inputs, number of frames, depth of your RNN, and cell size?\n", "@ebrevdo Size of input: 100, Number of time steps: 400, Depth: 2-3 layers if I understood correctly, cell size (hidden state size): 400. \n", "@ebrevdo much thanks, this should solve the problem for multiple devices\nbut what about a single device (e.g. a single GPU) ?\nas a side note, when I tried to run two different versions of my code on parallel, I notice only ~ 50% decrease in performance (e.g. when a single instance takes n seconds, running two parallel instances takes only 1.5n not 2n seconds) so this should be doable and the device has the capacity for it, but I was wondering if TF provides any mechanism for controlling this\n", "@avostryakov are you using MultiRNNCell?  is this unidirectional or bidirectional rnn?  you could also try using the new LSTMBlockCell in `tf.contrib.rnn` instead of the base `BasicLSTMCell`, which _may_ improve your performance.  Let me know how that works for you.\n", "@ASDen you mean for running two replicas on the same worker in parallel?  Probably your best bet is to just run multiple python training threads and use the same session in each of them.  They can even read minibatches off the same queue.  If you're using a `Supervisor`, then you can use its `managed_session` object.\n", "You can also use Supervisor's loop fn to start a bunch of training threads, and synchronize their exits.  See [Supervisor](https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#Supervisor) and search for \"Supervisor.loop\"\n", "@ebrevdo By the way, LSTMBlockCell didn't work together with optimizers during training as I mentioned here: https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-239411957 Was it fixed already? \n\nMy original question was simple: Should it be two times faster if I execute forward pass on one GPU and backward pass on another comparing to do it on one GPU. No MultiRNNCell. Input for both layers is the same. Add hidden states of both passes together in the end.\n", "@avostryakov almost.  the hidden states, if you concatenate them, will have to run on one of the GPUs, and this will require a copy.  however, this is trivial compared to the hard work the LSTM cells are doing in the forwards & backwards passes.  So the larger your cell batch size, depth, and number of iterations, the closer the performance should get to 2x.\n", "Does that resolve your issue?\n", "@ebrevdo If you ask me then yes, you answered my question. Thank you!\n", "Great! For any orthogonal questions we can open a different, more focused, issue.\n"]}, {"number": 3419, "title": "Combined update for contrib/go", "body": "This PR merges 1700+ commits from `master` into `go`. \n\nCurrently working [in this fork](https://github.com/damienstanton/tensorflow/tree/go).\n\nChanges: \n- Minor syntax alterations to the public C api header. \n- Where merge conflicts occur, code from `master` used exclusively.\n\nTests:\n- [X] `go test` in tensorflow/contrib/go passes.\n- [X] Inception model image recognition ([from examples](https://github.com/tensorflow/tensorflow/blob/go/tensorflow/g3doc/tutorials/image_recognition/index.md#usage-with-the-go-api)) works as expected.\n- [X] `pip3` build, installation, 'hello world' works as expected.\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Re: the googlebot's comment on CLA, I am the sole author of this PR. \n\nI may have pushed some early commits from a new OS installation where my `git config` did not contain my email address.\n", "did you add anything go related to this PR? or is just a rebase ? \nif is just a rebase I think the committers can create a cleaner history by rebasing and force update? \n", "@Mistobaan Mostly a rebase but there are a couple Go specific changes. The exported TF_DataType, and TF_Codes enums needed to be declared differently, also there was a type mismatch where Swig needed a `long long` instead of an `int64_t`.\n\nI'd be happy to rebase/squash locally and resubmit for a cleaner PR. \n", "I think would be better to just rebase the current go branch into master and then do new PRs against that. @vrv (or any googlers) can you help on this ? see also #3258 \n", "> rebase the current go branch into master and then do new PRs against that\n\nOkay \ud83d\udc4d I can do that. We can close this PR in that case. I presume we want contrib/go building properly in master, right? Unless there is a specific reason for keeping the Go branch separate\n"]}, {"number": 3418, "title": "cannot install tensorflow from source", "body": "Hi, trying to build tensorflow from source. Getting the error message below -\n\n\"/home/lec/Downloads/tensorflow/tensorflow/tools/proto_text/BUILD:31:1: Linking of rule '//tensorflow/tools/proto_text:gen_proto_text_functions' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/tools/proto_text/gen_proto_text_functions ... (remaining 31 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1\"\n", "comments": ["Please complete environment information, steps you took, etc.  See #3366 as an example, thanks.\n", "Environment info\n\nBazel 0.3.0\n\nOperating System: Ubuntu 16.04\nInstalled version of CUDA and cuDNN: CUDA 8.0, cuDNN 5\n\nRan this step :\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n", "Repository used :\ngit clone https://github.com/tensorflow/tensorflow\n", "may need to run with `--verbose_failures` to see what the actual error message was\n", "Yep.  Also, CUDA 8.0 may not be fully supported yet.  Try 7.0-7.5?\n"]}, {"number": 3417, "title": "Prevent evaluating stale output node.", "body": "When quantizing to 8-bit the Identity ops are removed (\"spliced\"). For graph's with a parameter-less output node like SoftMax, there is no issue. However, if the graph's output node received a constant input (i.e. a bias add with weights) the output node referenced [here](https://github.com/parvizp/tensorflow/blob/502ecd2c6c324a1c5c3f7b195c36dbafb45dafb3/tensorflow/contrib/quantization/tools/quantize_graph.py#L318) will be stale (still has Identity input) and result in a error.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\nThe corporation is NanoWatt Design.\n", "We should get the CLABot happy before we go forward with this CL. @parvizp can you try poking it again?\n", "I signed it!\n\nI had the email address used on the commits and CLA as secondary email on my GitHub profile. I changed to be primary in case that helps.\n", "I also need to write a test.\n", "@parvizp Any update?\n", "Closing due to lack of CLA + lots of time passing\n"]}, {"number": 3416, "title": "Make the default command use a named container", "body": "It seems that it's a more common workflow than I assumed, and should be relatively foolproof.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I'm not sure what this change is -- it looks like an existing commit  -- if this is supposed to be a real change, can send it using the normal github pull request process?\n"]}, {"number": 3415, "title": "Speed up retraining times  approximately 2x", "body": "Speed up is achieved by:\n1. saving (and especially loading) bottleneck arrays in binary files instead of .txt files\n2. caching already calculated bottleneck arrays in RAM, instead of reading them every time from disk\n\nP.S.\n2x speed up is achieved on SSD disks, on HDD disks speed up is even greater\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can one of the admins verify this patch?\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Could you reformat this to match pep8, except for the indent, which should be 2 spaces?\n", "Reformated according to PEP8, removed comment, using with\n", "One more thing considering creation of bottlenecks. \n\nIf a new version of retrain.py is run in a bottleneck directory, that contains bottlenecks files created with a previous version of retrain.py then retrain.py will crash (it will try to read list of floats from .txt file and fail). \n\nTo prevent this problem we can either:\n- delete all bottlenecks created with a previous version of retrain.py (it is remove .txt files from bottleneck dir)\n- or modify get_bottleneck_path function, so it appends not \".txt\", but \".bin\" postfix.\n\nWhich one is better option?\n", "@petewarden can you review this please?\n", "Jenkins, test this please.\n", "I've reviewed the changes, and they look good to me! Thanks for contributing this @sergejsrk \n"]}, {"number": 3414, "title": "add Dummy gradient for LogUniformCandidateSampler", "body": "I added a dummy Gradient for the LogUniformCandidateSampler, as proposed by Martin Wicke    in [this](https://groups.google.com/a/tensorflow.org/d/msg/discuss/P63KdpGGwvA/1ks-INGBAwAJ) discussion thread, to get rid of this error:\n\n> WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell\n> object at 0x7f50696455f8>: Using a concatenated state is slower and\n> will soon be deprecated.  Use state_is_tuple=True. Traceback (most\n> recent call last):   File\n> \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\",\n> line 448, in gradients\n>     grad_fn = ops.get_gradient_function(op)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\",\n> line 1634, in get_gradient_function\n>     return _gradient_registry.lookup(op_type)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/registry.py\",\n> line 85, in lookup\n>     \"%s registry has no entry for: %s\" % (self._name, name)) LookupError: gradient registry has no entry for:\n> LogUniformCandidateSampler\n> \n> During handling of the above exception, another exception occurred:\n> \n> Traceback (most recent call last):   File\n> \"/home/aa/code/python/bb/dyn_main.py\", line 176, in <module>\n>     tf.app.run()   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py\",\n> line 30, in run\n>     sys.exit(main(sys.argv))   File \"/home/aa/code/python/bb/dyn_main.py\", line 83, in main\n>     gradients = tf.gradients(loss2, params)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\",\n> line 452, in gradients\n>     (op.name, op.type)) LookupError: No gradient defined for operation 'dynamic_rnn_seq2seq/sequence_loss_by_example_dyn/while/cond/sampled_softmax_loss/LogUniformCandidateSampler'\n> (op type: LogUniformCandidateSampler)\n\nThanks!\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(Feel free to ping this again if you can sign the CLA)\n"]}, {"number": 3413, "title": "tutorial image resolution ", "body": "Hi,\nis there a higher image resolution of this?\n\nhttps://www.tensorflow.org/images/tensors_flowing.gif\n\nThank you, \n", "comments": ["Not that I know of.  @gcorrado: Greg, do we have any?  Thanks.\n", "I haven't been able to find a larger resolution of this. Closing.\n"]}, {"number": 3412, "title": "Update the quantization how-to", "body": "- Prevent the compiling errors of the label_image program.\n- Fix the parameters of the label_image program.\n- Fix the broken image links.\n", "comments": ["Can one of the admins verify this patch?\n", "We shouldn't check in these images. @petewarden, talk to me if images on tensorflow.org need to be updated, or if it's just the links.\n", "The images are on tensorflow.org/images/quantization0.png (etc.), it's just the links that need fixing, image links shouldn't be relative if we want them to work on github.\n", "Hi @martinwicke , thank you for the feedback.  I have updated the image links from the relative to the absolute ones, and have removed images from `tensorflow/g3doc/images/`.  Please let me know is there any place to improve.\n", "@martinwicke / @petewarden Please take a look.\n"]}, {"number": 3411, "title": "Building error: ptxas fatal   : Internal error: writing file ERROR: /disk1/caina2/tensorflow/tensorflow/core/kernels/BUILD:1509:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/depthtospace_op_gpu.cu.pic.o' was not created.", "body": "when build_pip_package, ending with \n\nptxas fatal   : Internal error: writing file\nERROR: /disk1/caina2/tensorflow/tensorflow/core/kernels/BUILD:1509:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/depthtospace_op_gpu.cu.pic.o' was not created.\n\nDoes anyone have the same problem?\n### Environment info\n\nOperating System:\ncentos 6.2\nglibc 2.23\nInstalled version of CUDA and cuDNN: \ncuda 7.5\nlibcudnn.so.5.0.4\nbazel-0.3.0\ninstall from source\n### What have you tried?\n\n/disk1/caina2/bazel-0.3.0/output/bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\n### Logs or other output that would be helpful\n\nINFO: From Compiling tensorflow/core/kernels/depthtospace_op_gpu.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nIn file included from third_party/gpus/cuda/include/host_config.h:161:0,\n                 from third_party/gpus/cuda/include/cuda_runtime.h:76,\n                 from <command-line>:0:\n/usr/include/features.h:331:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\n    ^\nIn file included from third_party/gpus/cuda/include/host_config.h:161:0,\n                 from third_party/gpus/cuda/include/cuda_runtime.h:76,\n                 from <command-line>:0:\n/usr/include/features.h:331:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\n    ^\nIn file included from third_party/gpus/cuda/include/host_config.h:161:0,\n                 from third_party/gpus/cuda/include/cuda_runtime.h:76,\n                 from <command-line>:0:\n/usr/include/features.h:331:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\n    ^\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nptxas fatal   : Internal error: writing file\nERROR: /disk1/caina2/tensorflow/tensorflow/core/kernels/BUILD:1509:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/depthtospace_op_gpu.cu.pic.o' was not created.\nERROR: /disk1/caina2/tensorflow/tensorflow/core/kernels/BUILD:1509:1: not all outputs were created.\nERROR: /disk1/caina2/tensorflow/tensorflow/core/kernels/BUILD:1509:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/spacetodepth_op_gpu.cu.pic.o' was not created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 242.863s, Critical Path: 236.90s\n", "comments": ["Two notes:\n1. You said \"install from source\".  What version/commit?\n2. CentOS 7 and above are supported.  Please try to use one of those versions.\n", "sorry for forgetting it...\ncommit fc9162975e52978d3af38549b570cc3cc5f0ab66\n\nis that still because of centos version?\n", "I don't know, unfortunately.  Please try to test against a newer version (>= 7).\n", "~~~-_______-~~~thank u all the same...\n", "hooray! centos 7.x finally successful! thanks god~~~~~~~~~\n"]}, {"number": 3410, "title": "Histogramme", "body": "Hi, \nThis is not a problem but rather a question about the quantile \"histogram\". I see that Tensorflow can plot almost everything (W, dW, b, db,...).\n\nBut what is in it actually ? Knowing that b is a vector, W is a matrix. I assumed that Tensorflow has calculated the norm (L2 et Frobenius) of these tensors to view their evolution through time, right ?\n\nOf course, a probability density function is more than welcome than just a quantile region to treat multimodal, but then, how we can view large multidimensional variables in just a unique graphic ?\n\nMany thanks,\nHoa\n", "comments": ["Sorry, I don't understand the issue.\n\nSeeing this does not sound like a bug nor a feature request, please redirect the discussion to the [mailing list](https://groups.google.com/a/tensorflow.org/d/forum/discuss) or to StackOverflow.\n"]}, {"number": 3409, "title": "Broadcast doc for ops in math_ops.cc", "body": "Document broadcasting behavior for each binary elementwise operation, as discussed in #508.\n", "comments": ["Can one of the admins verify this patch?\n", "I think there's a bunch of backticks missing, can you look at https://www.tensorflow.org/versions/r0.9/how_tos/documentation/index.html? Most of it is not relevant here, but the formatting for functions names should be consistent.\n", "@martinwicke Are you referring to op names? I'm following the documentation [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc#L470).\n", "Yep, @martinwicke is referring to op names. BatchMatMul in [math_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc#L129) illustrates the right style: op names, class names, etc should be in backticks. It's true the style isn't applied completely consistently at the moment, but we should try to be consistent going forward.\n", "Updated the PR.\n", "Admins, this PR looks good to me.\n", "Jenkins, test this please.\n"]}, {"number": 3408, "title": "How To Image Retraining : ImportError: No module named mock", "body": "### Environment info\n\nOperating System:\ncat /proc/version\nLinux version 4.4.0-31-generic (buildd@lgw01-16) (gcc version 5.3.1 20160413 (Ubuntu 5.3.1-14ubuntu2.1) ) #50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.9.0\n### Steps to reproduce\n1. Following this tutorial : https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html\n2. Executing this command after having completed the flower images download : (tensorflow) paddlescoot@paddlescoot-Satellite-P870:~/tensorflow_source/tensorflow$ bazel build tensorflow/examples/image_retraining:retrain\n3. See terminal output text [file here for details of error.](https://drive.google.com/open?id=0B0Oci8Q1NdEuUDJ4R2FHTkYtXzQ)\n### What have you tried?\n1. Searched for similar issues.\n", "comments": ["Does `pip install mock` help?  We started requiring this package on this [commit](https://github.com/tensorflow/tensorflow/commit/f72242bfce6f3976d433d051f6316a135d05b811).\n", "Just encountered this when building TF Serving with TF submodule updated to latest master. `pip install mock` got past it.\n", "I had this issue when using an aws instance that had tensorflow installed.\r\nI tried installing mock, but it didn't make any change.\r\nFinally, it turned out the installed tensorflow version was quite old and the issue was related to this https://github.com/tensorflow/serving/issues/139\r\n\r\nI just had to upgrade tensorflow and it worked: https://www.tensorflow.org/get_started/os_setup\r\n  ", "I have the same problem, who can help me?wangjingsideMacBook-Pro:tensorflow ginthva$ bazel-bin/tensorflow/examples/image_retraining/retrain \r\nTraceback (most recent call last):\r\n  File \"/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 79, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 124, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/test.py\", line 50, in <module>\r\n    import mock                # pylint: disable=g-import-not-at-top,unused-import\r\nImportError: No module named mock", "@shanxiongfei Were you ever able to find a solution? I am seeing the same thing in a docker container on macbook pro Sierra and not having any luck getting past it yet.", "you can remove mock by the following command:\r\n```\r\n$ sudo rm -rf /usr/local/lib/python2.7/dist-packages/mock\r\n$ sodo rm -rf /usr/local/lib/python2.7/dist-packages/mock-2.0.0.dist-info\r\n```\r\nthen type the following command to install mock\r\n```\r\n$ sudo pip install --upgrade mock\r\n```", "hey, my solution:\r\nfor windows: \r\nsudo /bin/env pip uninstall mock  ## important\r\nsudo /bin/env python  pip install mock\r\n\r\nfor MAC\r\nsudo /usr/bin/env pip uninstall mock  ## important\r\nsudo /usr/bin/env python  pip install mock\r\n\r\ncheckout this link for why use '/usr/bin/env' beforehand: http://www.junlulocky.com/blog/pythontricks", "maybe try both:\r\n$ pip3 install mock\r\n$ pip2 install mock"]}]