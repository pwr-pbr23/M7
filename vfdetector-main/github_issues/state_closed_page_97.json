[{"number": 52229, "title": "Minor typo fixes", "body": "The default post-training quantization technique is full interger quantization for the image classification task.\r\n\r\nchange full interger quantization -----> full integer quantization", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/52229\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>"]}, {"number": 52227, "title": "[determinism] Add tests for tf.nn.ctc_loss", "body": "Adds deterministic testing for `tf.nn.ctc_loss`, forwards and backwards on CPU and GPU.\r\n\r\nThis pull request resolves issue #[38151](https://github.com/tensorflow/tensorflow/issues/38151).\r\n\r\nNote that in TensorFlow version 2.3 there was a nondeterminism issue that seems to have now been resolved, in TensorFlow version 2.6, if not earlier.\r\n\r\nThis pull request is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#ctc-loss).\r\n\r\nCC @reedwm, @sanjoy, @nluehr", "comments": []}, {"number": 52226, "title": "[oneDNN] Enable fusion for Add/AddV2 which has BiasAdd semantics", "body": "If an Add/AddV2 op has one input of Conv2D/DepthwiseConv2D/MatMul in NHWC format,\r\nand the other input is 1-dimension, treat this add node as BiasAdd.\r\n\r\n Co-authored-by: Guangyong Shi <guangyong.shi@intel.com>", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52226) for more info**.\n\n<!-- unknown_author -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52226) for more info**.\n\n<!-- unknown_author -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52226) for more info**.\n\n<!-- unknown_author -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52226) for more info**.\n\n<!-- unknown_author -->", "@yimeisun123 Can you please sign CLA. Thanks!\r\n", "@gbaned - I am working for Intel and should have already signed the CLA, and have submitted PR before. I am not quite sure why I run into this CLA issue.", "Since this has co-author, it might need some action from @gyshi . I will ping him. Thanks.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52226) for more info**.\n\n<!-- unknown_author -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52226) for more info**.\n\n<!-- unknown_author -->", "@googlebot I consent", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52226) for more info**.\n\n<!-- unknown_author -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52226) for more info**.\n\n<!-- unknown_author -->", "@googlebot I consent ", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to determine that you authored the commits in this PR.  Maybe you used a different email address in the git commits than was used to sign the CLA?  If someone else authored these commits, then please add them to this pull request and have them confirm that they're okay with them being contributed to Google.  If there are co-authors, make sure they're formatted properly.\n\nIn order to pass this check, please resolve this problem and then comment`@googlebot I fixed it.`.. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52226) for more info**.\n\n<!-- unknown_author -->", "Not sure what else needs to be done to pass the CLA issue. Both contributors to this PR have CLA signed via Intel, and both commented with consent. Any suggestion? Thanks", "Since this PR keeps failing CLA check, close this one and create a new PR to submit the changes for review.\r\nhttps://github.com/tensorflow/tensorflow/pull/52358"]}, {"number": 52225, "title": "This build was never aapproved..", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@hackedaccount2017 ,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem.Thanks!\r\n", "Please take a look at this [link](https://www.tensorflow.org/install) which helps to install the tensorflow.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52225\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52225\">No</a>\n"]}, {"number": 52224, "title": "Added GPU floor mod op", "body": "Fixes #52179", "comments": []}, {"number": 52222, "title": "Blas xGEMV launch failed error when running on GPU. ", "body": "I've ran into a really strange error that is incredibly difficult to track down.  I have a version of Tensorflow (2.6.0-rc1) using Cuda 11.3 that I compiled from source.  When I run it using my GPU for evaluating a pretty normal feed forward model I constantly get this crash on this one particular line. \r\n\r\n```\r\n    h = tf.matmul(h, self.output_weight)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py\", line 3654, in matmul\r\n    return gen_math_ops.mat_mul(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 5696, in mat_mul\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\", line 6941, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Blas xGEMV launch failed : a.shape=[1,2728550,5], b.shape=[1,5,1], m=2728550, n=1, k=5 [Op:MatMul]\r\n\r\n```\r\nWhen I look up this error it usually indicates a problem in the CUDA install or other similar problems, but in this particular case CUDA is working just fine. I am able to run another script which uses Keras from the same Tensorflow install and use the GPU with zero problem. In fact in this same exact code 2-3 TF.MatMul calls go through with zero problem right before it crashes on this line.  \r\n\r\n``\r\nh = tf.matmul(h, self.output_weight)\r\n``\r\nWhere self.output_weight is a tensorflow Variable class and h is as well.  Both data types float32.  The thing is the h variable also was computed by multiple sequential calls that run on the GPU just fine.  When I run in CPU only mode it runs just fine with zero problems.  The model only uses matmul opperations and standard tensorflow library functions. What's even stranger is this same exact code works on the GPU for a different set of input!  \r\n\r\nI'm running this on Linux Mint 20.2 with Kernel version 5.4.0-88-generic.  Cuda version 11.3 with the corresponding compatible version of TensorRT, the NVidia drivers, etc.  This is being done on a RTX 3090 graphics card.   It's an incredibly strange error since it only pops up on this one particular line. I also tested out my Cuda install via the samples folder in the Cuda install directory.  My other Cuda based GPU applications work just fine as well.  Nvidia-smi also shows Python3 taking up most of the card's memory as it usually does. \r\n\r\n", "comments": ["My guess would be that the `[1,2728550,5]` tensor is wayyy too large, so your input is probably too large.\r\n\r\nYou could feed inputs of increasing sizes into your model to find the point at which the code collapses.", "@mrnucleation \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),Could you please try as per the [comment](https://github.com/tensorflow/tensorflow/issues/52222#issuecomment-932737035) above and let us know if it helps? Thanks!", "> My guess would be that the `[1,2728550,5]` tensor is wayyy too large, so your input is probably too large.\r\n> \r\n> You could feed inputs of increasing sizes into your model to find the point at which the code collapses.\r\n\r\nI was able to get it to compute by cutting up the array into blocks of 1/100th of the original size and passing them through one at a time.  Then pasting the fragments together at the end. Though it' still a bit perplexing why it crashes on the last line.  That's more what's absolutely confusing.\r\n\r\nIf anything it should have crashed in the middle of the network where the intermediate tensors are far larger. The finally tensor is just a Nx5 multiplied with a 5x1 tensor where as the intermediate ones are Nx16 and Nx32 in size.  If anything you would expect the crash to happen on those steps if the raw size was a problem.  It's also strange that the CPU version on BLAS seems to handle it just fine. \r\n\r\nIdeally the larger the array I can pass through in one go the faster the run time which in this case is critical for this application. \r\n\r\nThe computer this is running on has 64GB of system memory and 24GB of GPU RAM. When I measure the ram consumption with NVIDIA-SMI it only takes up 3GB of ram.  It seems like rendering a 8K image should be a more resource intensive task than this and this system can do that just fine. So it seems to have more to do with the TF or CUDA backend not liking that. ", "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  **Custom workflow, but all stock Tensorflow Library Functions**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Mint 20.2**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **Source**\r\n- TensorFlow version (use command below): **2.6.0-rc1**\r\n- Python version: **3.8.10**\r\n- Bazel version (if compiling from source): **3.7.2**\r\n- GCC/Compiler version (if compiling from source): **9.3.0**\r\n- CUDA/cuDNN version: **11.3**\r\n- GPU model and memory: **MSI Geforce RTX 3090 24GB**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.6.0-rc1-4-g9a63e4600fa 2.6.0-rc1\r\n\r\n**Describe the current behavior**\r\n\r\nCrashes when running in GPU mode on final computation of a network.  Passes two layers of a feed forward type model perfectly and then throws a BLAS error when computing the final output.  Works just fine in CPU Only mode. Crashes with the error message\r\n```\r\n\r\n  File \"/<redacted>/SymbolicNetworks/ThreeBody/StabiliyTest/symbolic_network.py\", line 162, in __call__\r\n    h = tf.matmul(h, self.output_weight)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py\", line 3654, in matmul\r\n    return gen_math_ops.mat_mul(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 5696, in mat_mul\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\", line 6941, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Blas xGEMV launch failed : a.shape=[1,18800375,6], b.shape=[1,6,1], m=18800375, n=1, k=6 [Op:MatMul]\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nComputation passes through two layers of the network successfully in GPU mode and crashes on final computation. \r\n\r\n", "@mrnucleation We see that you are using 2.6.0-rc1 ,Could you please try with the stable version of TF 2.6.0 ,please refer to build from [source](https://www.tensorflow.org/install/source#gpu).Please refer to some similar issues[ link1](https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed), [link2](https://github.com/tensorflow/tensorflow/issues/25403),[link3](https://github.com/tensorflow/tensorflow/issues/11812) and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52222\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52222\">No</a>\n", "I have a similar issue when using CUDA 11.3. I would suggest to downgrade CUDA 11.2. There should be some unknown compatibility issues with CUDA 11.3. (And I hope it can be investigated)", "UPDATE: It may be a bug of cublas. See https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-11.4.0", "@mrnucleation, This issue can be due to several reasons, \r\n1. out of memory: Limit the GPU memory growth\r\n2. configuration Mismatch with CUDA/cuDNN\r\n3. remove the cache folder ~/.nv/ \r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52222\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52222\">No</a>\n"]}, {"number": 52221, "title": "TensorFlow Object Detection : Error while executing python model_main_tf2.py", "body": "Environment:\r\n\r\n1. TensorFlow:2.6\r\n2. Python : 3.9.6\r\n3. Transfer learning for Object Detection using TensorFlow\r\n4. Dataset : Digital Meter\r\n5. Example Link: https://medium.com/analytics-vidhya/digit-recognition-of-digital-meters-using-tensorflow-2-object-detection-api-48364cd678a9 \r\n6. Pre-trained Model : ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8\r\n7. Generated Tf Record successfully( train.record and test.record )\r\n8. Python command:  python model_main_tf2.py --model_dir=models/my_ssd_mobilenet_\r\n9. v1_fpn --pipeline_config_path=models/my_ssd_mobilenet_v1_fpn/pipeline.config\r\n\r\n**Error:** \r\n**2021-10-02 01:05:11.264437: W tensorflow/core/framework/dataset.cc:679] Input of GeneratorDatasetOp::Dataset will not be op\r\ntimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\r\nC:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\keras\\backend.py:401: UserWarning: `tf.keras.backend\r\n.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to th\r\ne `training` argument of the `__call__` method of your layer or model.\r\n  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\Workspace\\model_main_tf2.py\", line 115, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40,\r\n in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\absl\\app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\Workspace\\model_main_tf2.py\", line 106, in main\r\n    model_lib_v2.train_loop(\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 618,\r\n in train_loop\r\n    ckpt.restore(latest_checkpoint)\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\"\r\n, line 2335, in restore\r\n    status = self.read(save_path, options=options)\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\"\r\n, line 2220, in read\r\n    result = self._saver.restore(save_path=save_path, options=options)\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\"\r\n, line 1382, in restore\r\n    base.CheckpointPosition(\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\"\r\n, line 254, in restore\r\n    restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\"\r\n, line 980, in _restore_from_checkpoint_position\r\n    current_position.checkpoint.restore_saveables(\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\"\r\n, line 351, in restore_saveables\r\n    new_restore_ops = functional_saver.MultiDeviceSaver(\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional\r\n_saver.py\", line 339, in restore\r\n    restore_ops = restore_fn()\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional\r\n_saver.py\", line 323, in restore_fn\r\n    restore_ops.update(saver.restore(file_prefix, options))\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional\r\n_saver.py\", line 115, in restore\r\n    restore_ops[saveable.name] = saveable.restore(\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\values.py\", lin\r\ne 1079, in restore\r\n    return values_util.get_on_write_restore_ops(self._mirrored_variable,\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\values_util.py\"\r\n, line 94, in get_on_write_restore_ops\r\n    tuple(\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\values_util.py\"\r\n, line 95, in <genexpr>\r\n    assign_on_device(v.device, v, tensor)\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\values_util.py\"\r\n, line 302, in assign_on_device\r\n    return variable.assign(tensor)\r\n  File \"C:\\Users\\shilpa.m\\PycharmProjects\\TensorFlow_OD\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.\r\npy\", line 899, in assign\r\n    raise ValueError(\r\nValueError: Cannot assign to variable WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredict\r\nor/bias:0 due to variable shape (66,) and value shape (72,) are incompatible**\r\n", "comments": ["@SHILPASHIVAMALLU ,\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from [here](https://github.com/tensorflow/models/issues/new/choose). Also please take a look at this [link](https://github.com/tensorflow/models/issues/9703) with the similar error.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52221\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52221\">No</a>\n"]}, {"number": 52219, "title": "Could not run the task evaluation!-tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:tf2.6.0\r\n- Python version:3.7.4\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):3.7.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I use **thetensorflow/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/** link tool to build and evaluate my tflite model, it shows that the evaluation task cannot be run. I have confirmed that the file path of all transmission parameters is correct. Hope to get your help!\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n$ bazel run -c opt \\\r\n>   -- \\\r\n>   //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval \\\r\n>   --model_file=mobilenet_224_1_0_INT8.tflite \\\r\n>   --ground_truth_images_path=./test/ \\\r\n>   --ground_truth_labels=validation_ground_truth.txt \\\r\n>   --model_output_labels=mobilenet_labels.txt \\\r\n>   --output_file_path=/tmp/accuracy_output.txt \\\r\n>   --num_images=0 # Run on all images.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'run' from /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'run' from /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/xdd/project/mobilenet/tf/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/a21c557955c6ea5cd02b9a145ad6469c608446c7.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/ad9981d394f5ccc784d6273e51aa41c38b7cf727.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /home/xdd/project/mobilenet/tf/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /home/xdd/project/mobilenet/tf/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /home/xdd/.cache/bazel/_bazel_xdd/238513a408dfab846cdfe0f15ca64c47/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/xdd/.cache/bazel/_bazel_xdd/238513a408dfab846cdfe0f15ca64c47/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/694d2524757f9040e65a02c374e152a462fe57eb.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval up-to-date:\r\n  bazel-bin/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/run_eval\r\nINFO: Elapsed time: 0.089s, Critical Path: 0.00s\r\nINFO: 1 process: 1 internal.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Running command line: bazel-bin/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/run_eval '--model_file=mobilenet_224_1_0_INT8.tflite' '--ground_truth_images_path=./test/' '--ground_truth_labels=validation_ground_truth.txt' '--model_output_labels=mobilenet_labels.txt' '--output_file_path=/tmp/INFO: Build completed successfully, 1 total action\r\nCould not run the task evaluation!\r\n\r\n", "comments": ["I seem to have found the reason for the above problem: because I use the conda environment, I did not use bazel to compile the tool in an environment containing tensorflow. As a result, the tool cannot find tensorflow related dependencies. In the end, the data set could not be verified.\r\nThe solution is, if you use anaconda to manage different Python environments, please run the build command in this question log in an environment containing tensorflow to use the corresponding tools.", "@xddcore Could you please move this issue to closed status if it has been resolved for you as per the [comment ](https://github.com/tensorflow/tensorflow/issues/52219#issuecomment-932494580)?Thank you! ", "> @xddcore Could you please move this issue to closed status if it has been resolved for you as per the [comment ](https://github.com/tensorflow/tensorflow/issues/52219#issuecomment-932494580)?Thank you!\r\n\r\nok, thank you for your reply!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52219\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52219\">No</a>\n"]}, {"number": 52218, "title": "Converted tf.gfile.GFile to tf.io.gfile.GFile ", "body": "**System information**\r\n- OS Platform and Distribution (Google Colab):\r\n- TensorFlow version (tensorflow 2.6.0):\r\n\r\nI was working with the tensorflow's object detection api and as soon as i ran the following code:\r\n\r\nPATH_TO_LABELS = '/content/models/research/object_detection/data/mscoco_label_map.pbtxt'\r\ncategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\r\n\r\nIt showed the following error.\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-82-651c9b9bcbff> in <module>()\r\n      1 PATH_TO_LABELS = '/content/models/research/object_detection/data/mscoco_label_map.pbtxt'\r\n----> 2 category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\r\n\r\n2 frames\r\n/usr/local/lib/python3.7/dist-packages/object_detection/utils/label_map_util.py in load_labelmap(path)\r\n    130   Returns:\r\n    131     a StringIntLabelMapProto\r\n--> 132   \"\"\"\r\n    133   with tf.io.gfile.GFile(path, 'r') as fid:\r\n    134     label_map_string = fid.read()\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'gfile'\r\n\r\nI tried to change the load_labelmap file too but it showed no result. Viewing this on stack overflow suggested that I should downgrade my tensorflow version to 1.x but for the particular program, I need to have the tensorflow version above 2.5\r\n\r\nWhat can be the possible solution to this problem.", "comments": ["@Bhavay192 ,\r\nCould you please have a look at this  SO link [1](https://stackoverflow.com/questions/55591437/attributeerror-module-tensorflow-has-no-attribute-gfile) and comment from the issue [2](https://github.com/tensorflow/tensorflow/issues/31315#issuecomment-518260943) with the similar error.It helps.Thanks!", "Downgrading to 1.x is a bad advice.\r\n\r\nPlease make sure your TF version is 2.x and the dependencies you are using are also using TF 2.x.\r\n\r\nBased on\r\n\r\n```\r\n/usr/local/lib/python3.7/dist-packages/object_detection/utils/label_map_util.py in load_labelmap(path)\r\n130 Returns:\r\n131 a StringIntLabelMapProto\r\n--> 132 \"\"\"\r\n133 with tf.io.gfile.GFile(path, 'r') as fid:\r\n134 label_map_string = fid.read()\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'gfile'\r\n```\r\n\r\n(from your error log, but formatted properly), it seems the error comes from `object_detection` wheel.", "Hey @tilakrayal , I will look into the links and let you know about it. Thanks\r\n", "Hey @mihaimaruseac, could you pls elaborate what can be the possible error in object_detection wheel and how can it be resolved. Thanks", "https://pypi.org/project/object-detection-0.1/#files is not a TF product. So we should not handle this in this repository.\r\n\r\nLooking at the package, it lacks a setup.py, so it does not specify what version of TF it needs. The code in the file assumes TF 1.x\r\n\r\n```python\r\n  with tf.gfile.GFile(path, 'r') as fid:\r\n    label_map_string = fid.read()\r\n    label_map = string_int_label_map_pb2.StringIntLabelMap()\r\n    ...\r\n```\r\n\r\nI would recommend not using this package as it only works with TF 1.x it seems but TF 1.x is no longer supported.\r\n\r\nClosing this, since this is not a TF issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52218\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52218\">No</a>\n"]}, {"number": 52216, "title": "Mac m1 tf2.5.0: No layer for IntegerLookup, Normalization, StringLookup", "body": "**System information**\r\n- OS Platform and Distribution: macOS Big Sur Version 11.3.1 with mac m1 chip\r\n- TensorFlow installed from: https://developer.apple.com/metal/tensorflow-plugin/\r\n- TensorFlow version (use command below): tf 2,5\r\n- Python version: 3.8\r\n\r\nDebug output:\r\n```\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n```\r\nunknown 2.5.0\r\n\r\n**Describe the current behavior**\r\n```\r\nfrom tensorflow.keras.layers import IntegerLookup\r\nfrom tensorflow.keras.layers import Normalization\r\nfrom tensorflow.keras.layers import StringLookup\r\n```\r\nImportError                               Traceback (most recent call last)\r\n/var/folders/1z/77jbzk11477gc69_s8n2y0r00000gn/T/ipykernel_27569/1759035919.py in <module>\r\n----> 1 from tensorflow.keras.layers import IntegerLookup\r\n\r\nImportError: cannot import name 'IntegerLookup' from 'tensorflow.keras.layers' (/Users/username/miniforge3/envs/env/lib/python3.8/site-packages/tensorflow/keras/layers/__init__.py)\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nFollow link https://developer.apple.com/metal/tensorflow-plugin/ to install tf2.5 on mac and then run the import\r\n", "comments": ["Hi @JTWang2000 ! Sorry for the late response,Could you try again the imports like below .\r\n```\r\nfrom tensorflow.python.keras.layers.preprocessing.integer_lookup import IntegerLookup\r\nfrom tensorflow.python.keras.layers.preprocessing.normalization import Normalization\r\nfrom tensorflow.python.keras.layers.preprocessing.string_lookup import StringLookup\r\n```\r\n[Reference source code](https://github.com/tensorflow/tensorflow/blob/r2.5/tensorflow/python/keras/layers/__init__.py)\r\n", "Hi @mohantym Thank you so much for your reply. This works!\r\nI also tried with \r\n```\r\nfrom tensorflow.keras.layers.experimental.preprocessing import StringLookup, IntegerLookup, Normalization\r\n```\r\nThese two commands both work", "Ok @JTWang2000 ! Closing this issue as it seems to be resolved. Thanks for confirming the same!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52216\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52216\">No</a>\n"]}, {"number": 52215, "title": "Creating dataset from large numpy arrays via from_tensor_slices crashes without any error message or warning", "body": "I use tf 2.6 and when I try to create datasets from larger numpy arrays (>10GB) via from_tensor_slices the code breaks when I try to train via \"fit\" or even just attempt to iterate over the dataset. The code just breaks and exists, no warning, no error message, nothing. I have not found any similar issue mentioned anywhere else. What is the actual limitation here? \r\n\r\nI have over 128GB ram and due to the code breaking already on the dataset iteration part it is surely unrelated to my GPU and its memory (24GB). The numpy arrays load without issues into memory but once the iterator causes the execution of \"from_tensor_slices\", the code breaks shortly after. \r\n\r\nWhat are workable solutions here? DataGenerators? Creating TFRecord files? I try to avoid going the route of TFRecords because it appears very poorly documented how to create such binary files. \r\n\r\nAs no warning or error message is output there is no log to show at all. \r\n\r\nThe problem can be reproduced with a large numpy array (random data works) and the following code:\r\n\r\n#obtain training data\r\n    print(\"loading training data...\")\r\n    train_data = np.load(os.path.join(os.getcwd(), \"source_datasets\", f\"{train_data_id}_features.npy\"))\r\n    train_targets = np.load(os.path.join(os.getcwd(), \"source_datasets\", f\"{train_data_id}_targets.npy\"))\r\n\r\n    print(\"training dataset construction...\")\r\n    train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_targets))\r\n\r\n    for x,y in train_ds:\r\n        do_something", "comments": ["Hi, Try iterating your dataset using this :\r\n```\r\nfor x,y in train_ds.take(1):\r\n    do_something\r\n```\r\nand check if it is able to print out elements from the first list of `train_ds`\r\n\r\nAlso, try defining a batch size for your train dataset here, using this:\r\n`train_ds = train_ds.batch(BATCH_SIZE)`\r\nI recommend using a smaller BATCH SIZE first.\r\n\r\nThe above suggestions might not be enough to fix your problem.Even with a large RAM size, loading a data this big directly into the memory is not very efficient.It would be better to load just a chunk of the file directly into memory rather loading all of them at once.\r\n\r\n", "Thanks for your suggestions, unfortunately those ideas did not solve the issue of crashing due to large size of numpy arrays even though there are no internal memory limitations. I decided to ditch from_tensor_slices and went with generators which seems to work just fine. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52215\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52215\">No</a>\n"]}, {"number": 52214, "title": "A Problem that Only One Inference Can Be Made when I Loaded the Saved Model on Tensorflow-gpu 2.5.0 and latest.", "body": "**My System Information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: \r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (use command below): 2.5.0 and latest.\r\nPython version: 3.7.8\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: CUDA 11.2 / cuDNN 8.1\r\nGPU model and memory: RTX3060 / 12gb\r\nExact command to reproduce:\r\n\r\n------------------------------------------------------------------------\r\n\r\n**Describe the problem**\r\n\r\nI want to load the saved model using ```saved_model.load()``` and make inference using the ```serving_default``` signature key.\r\n\r\nHowever, there is a problem that inference only occurs once.\r\n\r\nWhen there is a single image, inference is made normally, but when there are multiple images or a video, inference is not made at all from the second image (or frame).\r\n\r\nContinuous inference is possible by reloading the model before every inference, but it is very slow.\r\n\r\nThis problem does not occur in previous versions (2.3.0) of tensorflow.\r\nThis problem only occurs in version 2.5.0 or later. I tested the 2.5.0 / 2.6.0 / 2.8.0 (nightly) version, but none of them work.\r\n\r\n\r\n**Source code / logs**\r\n\r\nThe code below is a summary.\r\n\r\n ```\r\n#model load\r\nsaved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])\r\ninfer = saved_model_loaded.signatures['serving_default']\r\n\r\n# begin video capture\r\ntry:\r\n    vid = cv2.VideoCapture(int(video_path))\r\nexcept:\r\n    vid = cv2.VideoCapture(video_path)\r\n\r\nwhile True:\r\n    return_value, frame = vid.read()\r\n    if return_value:\r\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n        image = Image.fromarray(frame)\r\n    else:\r\n        print('Video has ended or failed, try a different video format!')\r\n        break\r\n    \r\n    frame_size = frame.shape[:2]\r\n    image_data = cv2.resize(frame, (input_size, input_size))\r\n    image_data = image_data / 255.\r\n    image_data = image_data[np.newaxis, ...].astype(np.float32)\r\n    start_time = time.time()\r\n\r\n    #prediction\r\n    batch_data = tf.constant(image_data)\r\n    pred_bbox = infer(batch_data)\r\n```\r\n\r\nThe original code is [detect_video.py](https://github.com/theAIGuysCode/tensorflow-yolov4-tflite/blob/master/detect_video.py) of [theAIGuysCode/tensorflow-yolov4-tflite](https://github.com/theAIGuysCode/tensorflow-yolov4-tflite).\r\n\r\nIf you run the above code and do `print(pred_bbox)`, it is inferred normally in the first frame, but nothing is printed from the second.\r\n\r\ne.g. 1) First Frame\r\n```\r\n{'tf.concat_16': <tf.Tensor: shape=(1, 21, 84), dtype=float32, numpy=\r\narray([[[5.54353952e-01, 2.06503779e-01, 8.69302511e-01, ...,\r\n         1.45839895e-05, 2.10430017e-06, 8.75245519e-07],\r\n        [5.53583145e-01, 2.05075234e-01, 8.70401978e-01, ...,\r\n         1.43441921e-05, 2.20338029e-06, 7.08290202e-07],\r\n        [5.51974833e-01, 2.04461098e-01, 8.72702539e-01, ...,\r\n         1.21745070e-05, 2.31040190e-06, 7.29801911e-07],\r\n        ...,\r\n        [5.51477075e-01, 2.00088605e-01, 8.68865132e-01, ...,\r\n         8.14086491e-07, 2.59839396e-07, 1.50752749e-06],\r\n        [5.49633265e-01, 1.98086709e-01, 8.72833788e-01, ...,\r\n         1.83860755e-06, 4.25420922e-07, 3.30964349e-06],\r\n        [8.79046202e-01, 6.94515137e-03, 1.00040674e+00, ...,\r\n         1.45153881e-05, 5.73599527e-06, 6.34236858e-06]]], dtype=float32)>}\r\n```\r\n\r\ne.g. 2) Second+ Frame\r\n```\r\n{'tf.concat_16': <tf.Tensor: shape=(1, 0, 84), dtype=float32, numpy=array([], shape=(1, 0, 84), dtype=float32)>}\r\n```", "comments": ["@js-ryu,\r\n\r\nCan you take a look at this [thread](https://githubmemory.com/repo/google/automl/issues/896) which discuss about a similar issue on batch inferencing, and let me know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "same issue here.. ", "same issue heere"]}, {"number": 52213, "title": "Fix errors that occurs when the path to python.exe includes space characters", "body": "Currently, when `PYTHON_BIN_PATH` includes space characters, the following command will fail (on Windows).\r\n```bat\r\nRem Assume that Python is installed to `C:\\Program Files (x86)\\Python`.\r\nbazel build -c opt --action_env PYTHON_BIN_PATH=\"C://Program Files (x86)//Python//python.exe\" //tensorflow/lite:framework\r\n```\r\n```txt\r\nERROR: An error occurred during the fetch of repository 'local_execution_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/homul/sources/tensorflow/third_party/py/python_configure.bzl\", line 210, column 33, in _create_local_python_repository\r\n                python_lib = _get_python_lib(repository_ctx, python_bin)\r\n        File \"C:/users/homul/sources/tensorflow/third_party/py/python_configure.bzl\", line 130, column 21, in _get_python_lib\r\n                result = execute(repository_ctx, [python_bin, \"-c\", cmd])\r\n        File \"C:/users/homul/sources/tensorflow/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\n'C:\\Program' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n```\r\n\r\nThis PR makes it sure that the command path will be quoted if it includes space characters.", "comments": []}, {"number": 52212, "title": "[oneDNN] config updates to CI scripts", "body": null, "comments": []}, {"number": 52211, "title": "[TF:TRT] Fix profile handling for pruned inputs", "body": "Before TRT 8, an input tensor can be pruned if it is not used by the network . This can happen if only the shape of the input tensor is used and the shape of the input tensor is known, that is, min equals max in the shape optimization profile (nvbugs/3153064). This PR fixes optimization profile handling if there are pruned inputs.\r\n\r\nNotes:\r\n- The number TRT network input for INetworkDefinition is always the same as the TF network inputs.\r\n- The number of input bindings for a single profile of an ICudaEngine is not larger than the network inputs.\r\n\r\nTo fix profile handling:\r\n- We add `is_input_pruned_[]` vector to TrtShapeOptimizationProfile. \r\n- We ensure that in `OptimizationProfileConfig` the `min.size()` is always determined by the number of the network inputs, and not by the number of input bindings.\r\n\r\nTracker: #45481\r\n\r\nTagging @bixia1 for review.", "comments": ["I have also edit the PR description, please check.", "@bixia1 I have rebased the PR, please have a look.", "[this test](https://github.com/tensorflow/tensorflow/blob/92f41c156ba0c2e24c20824f9fef3b2a1ebf8809/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_resource_ops_test.cc#L356) now fails because it now has 0 profile. Do you know why?", "@tfeher Can you please check @bixia1's comments and keep us posted ? Thanks!", "There were some problems in the test, that I have missed earlier. I have fixed the test."]}, {"number": 52210, "title": "Monitor TPU-VM utilization", "body": "Hi Tensorflow Team,\r\n\r\nI'm trying to use the monitoring function: [`tf.profiler.experimental.client.monitor`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/profiler/profiler_client.py#L135), but getting the following error:\r\n```\r\nreturn _pywrap_profiler.monitor(\r\ntensorflow.python.framework.errors_impl.UnimplementedError: unimplemented.\r\n```\r\n\r\nMy setup is the following: I `ssh` into a host tpu-vm and run everything locally on the host vm.\r\n(using the following `gcloud` command: `gcloud alpha compute tpus tpu-vm  ssh my-tpu ...`)\r\n\r\nCode looks something like this:\r\n```\r\nimport tensorflow as tf\r\nfrom tf.python.profiler import profiler_client\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\nclass Monitor(object):\r\n    def init(self, service_addr, duration_ms):\r\n        self.service_addr = service_addr\r\n        self.duration_ms = duration_ms\r\n        self._stop = True\r\n        self.client = profiler_client\r\n\r\n    def _loop(self):\r\n       while not self._stop:\r\n            time.sleep(0.5)\r\n            try:\r\n                self.client.monitor(self.service_addr, duration_ms=self.duration_ms, level=1)\r\n            except Exception as e:\r\n                print(e)\r\n                time.sleep(1)\r\n\r\n    def start(self):\r\n        if self._stop:\r\n            self._thread = threading.Thread(target=self._loop, daemon=True)\r\n            self._stop = False\r\n            self._thread.start()\r\n\r\n    def stop(self):\r\n        if not self._stop:\r\n            self._stop = True\r\n            self._thread.join()\r\n\r\ntf.profiler.experimental.server.start(8466)\r\ntpu_monitor = Monitor(\"grpc://localhost:8466\", 2000)\r\ntpu_monitor.start()\r\ntrain_model()\r\ntpu_monitor.stop()\r\n```\r\nCould you please advice what am i doing wrong?\r\nIs monitoring not supported when running locally on the tpu-host vm?\r\n\r\nAlso alternatively tried doing this: (following this [tutorial](https://cloud.google.com/tpu/docs/cloud-tpu-tools#monitor_job))\r\n`pip3 install --upgrade \"cloud-tpu-profiler>=2.3.0\"`\r\nrunning my training scrip in one shell and in another shell running (on the host tpu-vm): `capture_tpu_profile --service_addr localhost:8466 --monitoring_level 1 --num_queries 1000`, however looking at the code for the `capture_tpu_profile` script it uses the same `profiler_client.monitor(service_addr, duration_ms, monitoring_level)` function (so probably not surprising that i get the same error)", "comments": ["Any update on this one?", "Hi @kat25230 ,Sorry for the late response , Could you update the above template with **train_model()** code snippet too ?I have edited and tried to run your above code. Attaching [GIST ](https://colab.research.google.com/gist/mohantym/29e61f88e15c03fbc60bb90d4a23c752/github_52210.ipynb#scrollTo=I4h_4N0mGDJf)for reference.", "Hi @mohantym Thanks for the response. \r\nHere is a link to a [gist](https://colab.research.google.com/gist/kat25230/0bf0fc11e043b6b3488027109c8cb904/github_52210.ipynb#scrollTo=I4h_4N0mGDJf) with a train code.\r\n\r\nNote that the issue i'm seeing is happening when ssh-ing into a TPU-VM and running locally.\r\nUsing this gcloud commands:\r\n```\r\ngcloud alpha compute tpus tpu-vm create tpu-name \\\r\n--zone=europe-west4-a \\\r\n--accelerator-type=v3-8 \\\r\n--version=v2-alpha\r\n```\r\nand to ssh:\r\n```\r\ngcloud alpha compute tpus tpu-vm ssh tpu-name \\\r\n  --zone europe-west4-a\r\n```\r\nFollowed this quick start [guide](https://cloud.google.com/tpu/docs/tensorflow-quickstart-tpu-vm#tpu-vm)", "Hi @sanatmpa1 ! Could you please look at this issue! Training got 86.84 % accuracy  when port was set  to 8080 . but was  throwing error \r\n\r\n> 'Monitor' object has no attribute 'service_addr'  \r\n\r\n,  User is having issue in while operating in cloud window through SSH. Providing Gist in [2.5](https://colab.research.google.com/gist/mohantym/753539baaf53160190fabb890b343892/github_52210.ipynb#scrollTo=_LDpTv4mbOS9) , [2.6](https://colab.research.google.com/gist/mohantym/29e61f88e15c03fbc60bb90d4a23c752/github_52210.ipynb#scrollTo=WZ_qJ1UaF8Sf) and [2.7 ](https://colab.research.google.com/gist/mohantym/0c493605babc7ba5bc348794520e29bc/github_52210.ipynb#scrollTo=WZ_qJ1UaF8Sf) for reference.", "@kat25230,\r\n\r\nAs its throwing you `unimplemented error` which means that an operation has not been implemented.. Can you take a look at this [link1](https://cloud.google.com/tpu/docs/cloud-tpu-monitoring), [link2](https://cloud.google.com/tpu/docs/cloud-tpu-tools) and let us know if it helps in monitoring TPU? Thanks!", "Hi @sanatmpa1, thanks for the reply. \r\n\r\n- I looked at the second  [link](https://cloud.google.com/tpu/docs/cloud-tpu-tools#monitor_job) as mentioned in my original message. It doesn't work and throws the same unimplemented error. It uses the same monitor logic (i.e. same function) as I have in my Monitor class. Also the example works for engine vm and a cloud TPU and not TPU-vm.\r\n\r\n-  Looking at the first  [link](https://cloud.google.com/tpu/docs/cloud-tpu-monitoring#prerequisites) you've sent it seems that it only works with a compute engine vm and a cloud TPU (i.e. not TPU-VM, where i ssh directly to the TPU host machine and don't have a compute engine).\r\n\r\ncould you advice if I'm missing anything?\r\n", "Any update on this one? \r\nThanks for the help!", "@sachinprasadhs could you please assign the relevant person in case you can't help?\r\nI would be happy to understand if this feature is supported for my setting.\r\nThanks in advance!", "Any update on this one? From my understanding thus far it's not supported. Is that true? if so are there any plans to add support for this?", "@sachinprasadhs any updates?", "@kat25230 , Have you connected your TPU directly to the VM, since tpu='local' expects the same.", "> @kat25230 , Have you connected your TPU directly to the VM, since tpu='local' expects the same.\r\n\r\nyou can see all the details in my original post, what do you mean by connected directly?", "https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver#args_1\r\nPlease check the arg tpu's details ", "> https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver#args_1 Please check the arg tpu's details\r\n\r\n@sachinprasadhs like I said in my previous message, you can see in my original post that I use \r\n```\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\r\n```\r\nI'm trying to understand:\r\n\r\n1. If `profiler_client .monitor` supports monitoring in a `local` setting?\r\n2. If there is support how to use it, see my code above, it didn't work?\r\n3. If not supported, is there another way to extract TPU utilization information in a local setting? ", "Unfortunately TPU VM currently doesn't support monitoring TPU utilization like it did in TPU node.\r\n\r\nWe are working on functionality for this, but it won't be through the profiler API like this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52210\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52210\">No</a>\n", "Hi @allenwang28,\r\nThanks for the response. I guess the only follow up on my end is, if the there is any issue that I can follow to see when this feature becomes available. \r\nThanks in advance!", "Any update on this one?", "@allenwang28 @sachinprasadhs Where can we get updates on this issue (TPU VM monitoring support) ?"]}, {"number": 52209, "title": "Remove use of `eval` when evaluating the input example.", "body": "Use `ast.eval_literal` instead which safely evaluates the expression.\r\n\r\nPiperOrigin-RevId: 400012249\r\nChange-Id: I5ff98608ea2d736d093aa488af723ff4f6707e02", "comments": []}, {"number": 52208, "title": "NVlink for tensorflow2.5, linux, 3090 GPU", "body": "OS: Linux Ubuntu18.04\r\nTF building method: TF docker with Nvidia Docker,  CUDA version: 11.1\r\nVersion: Tensorflow 2.5.0\r\nGPUs: RTX3090 x2\r\n\r\nHi there.\r\n\r\nI noticed that the NVlink for 3090 is published. I got a workstation with 2 3090 GPU cards. I wish to know is it necessary to got a NVlink to accelerate data transfer between these 2 cards?\r\n\r\nFor tensorflow building via docker, should i install some extra toolkit? And when I train model with MirrowedStrategy, what should I do to activate the NVlink? \r\n", "comments": ["@FavorLionQY Sorry for the late response! Could you please refer to the [link ](https://www.tensorflow.org/install/source#linux)to build from source and have a look on the similar issue [link1](https://github.com/tensorflow/tensorflow/issues/36510), [link2 ](https://github.com/tensorflow/tensorflow/issues/34067), [link3 ](https://github.com/tensorflow/tensorflow/issues/43947)for reference .Please let us know if it helps?Thank you! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52208\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52208\">No</a>\n"]}, {"number": 52207, "title": "When using .fit() to train a binary classification model, the metrics (i.e. loss, accuracy) shown in the history log of .fit() for training set is different from the result from .evaluate()", "body": "I'm using the tf.keras.Model.fit() to train a binary classification model for images. I found that the metrics like accuracy shown in the final epoch's log of .fit() can be 1.0, while the accuracy shown in model.evaluate(ds_train) for training set can be only 0.71. The loss and any other metrics are also different. However, the results for validation set are perfectly consistent for .fit and .evaluate. And also, for multiclass classification training, this issue never happens.\r\n\r\nThe issue has happened multiple times for different projects. Please let me know if anything wrong happened.\r\n\r\nHere is a sample code for my binary classification training:\r\n\r\ndef load_image(img_path,size = (32,32)):\r\n    label = tf.constant([1],tf.int8) if tf.strings.regex_full_match(img_path,\".*automobile.*\") \\\r\n            else tf.constant([0],tf.int8)\r\n    img = tf.io.read_file(img_path)\r\n    img = tf.image.decode_jpeg(img) \r\n    img = tf.image.resize(img,size)/255.0\r\n    return(img,label)\r\n\r\nds_train = tf.data.Dataset.list_files(\"./data/cifar2/train/*/*.jpg\") \\\r\n           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\r\n           .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\r\n           .prefetch(tf.data.experimental.AUTOTUNE)  \r\n\r\nds_test = tf.data.Dataset.list_files(\"./data/cifar2/test/*/*.jpg\") \\\r\n           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\r\n           .batch(BATCH_SIZE) \\\r\n           .prefetch(tf.data.experimental.AUTOTUNE)  \r\ntf.keras.backend.clear_session() \r\n\r\ninputs = layers.Input(shape=(32,32,3))\r\nx = layers.Conv2D(32,kernel_size=(3,3))(inputs)\r\nx = layers.MaxPool2D()(x)\r\nx = layers.Conv2D(64,kernel_size=(5,5))(x)\r\nx = layers.MaxPool2D()(x)\r\nx = layers.Dropout(rate=0.1)(x)\r\nx = layers.Flatten()(x)\r\nx = layers.Dense(32,activation='relu')(x)\r\noutputs = layers.Dense(1,activation = 'sigmoid')(x)\r\n\r\nmodel = models.Model(inputs = inputs,outputs = outputs)\r\n\r\nmodel.compile(\r\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n        loss=tf.keras.losses.binary_crossentropy,\r\n        metrics=[\"accuracy\"]\r\n    )\r\n\r\nhistory = model.fit(ds_train,epochs= 5,validation_data=ds_test)\r\n\r\n*******************************************************************\r\nafter running for .fit():\r\nEpoch 5/5\r\n100/100 [==============================] - 1s 7ms/step - loss: 0.2098 - accuracy: 0.9154 - val_loss: 0.2171 - val_accuracy: 0.9080\r\n\r\nfor .evaluate() after training:\r\nmodel.evaluate(ds_train)\r\n\r\n100/100 [==============================] - 0s 5ms/step - loss: 0.1968 - accuracy: 0.9195\r\n\r\nThe accuracy can be more different for some other projects.", "comments": ["@JoeSu666 ,\r\nIn order to expedite the trouble-shooting process, could you please provide a complete code  and the TensorFlow version you are using.\r\n\r\n", "```\r\ndef load_image(img_path,size = (32,32)):\r\n  label = tf.constant([1],tf.int8) if tf.strings.regex_full_match(img_path,\".automobile.\")\r\n  else tf.constant([0],tf.int8)\r\n  img = tf.io.read_file(img_path)\r\n  img = tf.image.decode_jpeg(img)\r\n  img = tf.image.resize(img,size)/255.0\r\n  return(img,label)\r\n\r\nds_train = tf.data.Dataset.list_files(\"./data/cifar2/train//.jpg\")\r\n.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n.shuffle(buffer_size = 1000).batch(BATCH_SIZE)\r\n.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\nds_test = tf.data.Dataset.list_files(\"./data/cifar2/test//.jpg\")\r\n.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n.batch(BATCH_SIZE)\r\n.prefetch(tf.data.experimental.AUTOTUNE)\r\ntf.keras.backend.clear_session()\r\n\r\ninputs = layers.Input(shape=(32,32,3))\r\nx = layers.Conv2D(32,kernel_size=(3,3))(inputs)\r\nx = layers.MaxPool2D()(x)\r\nx = layers.Conv2D(64,kernel_size=(5,5))(x)\r\nx = layers.MaxPool2D()(x)\r\nx = layers.Dropout(rate=0.1)(x)\r\nx = layers.Flatten()(x)\r\nx = layers.Dense(32,activation='relu')(x)\r\noutputs = layers.Dense(1,activation = 'sigmoid')(x)\r\n\r\nmodel = models.Model(inputs = inputs,outputs = outputs)\r\n\r\nmodel.compile(\r\noptimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\nloss=tf.keras.losses.binary_crossentropy,\r\nmetrics=[\"accuracy\"]\r\n)\r\n\r\nhistory = model.fit(ds_train,epochs= 5,validation_data=ds_test)\r\nmodel.evaluate(ds_train)\r\n```\r\nThis is one of the code that yield the issue. The version I'm using is tf2.2.0", "@JoeSu666 ,\r\nCan you please try to execute your code in latest stable version 2.6.0 and let us know if the issue still persists.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52207\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52207\">No</a>\n"]}, {"number": 52206, "title": "RuntimeError: hashtable need to be initialized before usingNode number 74 (HASHTABLE_FIND) failed to invoke.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): PIP\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):  tensorflow-gpu==2.3.0\r\n\r\nNOTE: Used tensorflow-gpu==2.3.0 to build and train the model and USED tf-nightly-gpu==2.8.0 to convert the model to tflite.\r\n\r\n### 2. Code\r\n```\r\n@tf.function\r\ndef preprocessing_space_tf(text):\r\n\r\n    def remove_special_characters(text, remove_digits=False):\r\n        pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\r\n        text = tf.strings.regex_replace(text, pattern, '')\r\n        return text\r\n\r\n    def replace(text):\r\n        text = tf.strings.lower(text)\r\n        text = tf.strings.regex_replace(text, '_', ' ')\r\n        text = tf.strings.regex_replace(text, '-', ' ')\r\n        text = tf.strings.regex_replace(text, ':', ' ')\r\n        text = tf.strings.regex_replace(text, '/', ' ')\r\n        return text\r\n\r\n    text = replace(text)\r\n    text = remove_special_characters(text)\r\n\r\n    return text\r\n\r\n\r\nclass USE_CNN(tf.keras.Model):\r\n    def __init__(self, num_classes, **kwargs):\r\n        super(USE_CNN, self).__init__(name='USE_CNN', **kwargs)\r\n        self.num_classes = num_classes\r\n        self.preprocess = preprocessing_space_tf\r\n        self.embedding = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4', trainable=True)\r\n        self.dense_1024 = tf.keras.layers.Dense(1024, activation='relu')\r\n        self.dropout = tf.keras.layers.Dropout(0.35)\r\n        self.dense_out = tf.keras.layers.Dense(self.num_classes, activation='softmax')\r\n\r\n    def call(self, text):\r\n        x = self.preprocess(text)\r\n        x = self.embedding(tf.squeeze(x, axis=1))\r\n        x = self.dense_1024(x)\r\n        x = self.dropout(x)\r\n        x = self.dense_out(x)\r\n        return x\r\n\r\n\r\nnlp_model = USE_CNN(num_classes=3266)\r\n\r\nnlp_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=tf.keras.metrics.SparseCategoricalAccuracy())\r\n\r\nnlp_model.fit(x_train, y_train, epochs=50, batch_size=100)\r\n\r\nnlp_model.save('/use_cnn/', save_format='tf')\r\n\r\n# convert to tflite\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/use_cnn/')  # path to the SavedModel directory\r\nconverter.experimental_enable_resource_variables = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\nwith open('model.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n\r\n# Inference using tflite\r\ninterpreter = tf.lite.Interpreter(model_path='model.tflite')\r\ninterpreter.allocate_tensors()\r\ninput_index = interpreter.get_input_details()[0][\"index\"]\r\ninterpreter.set_tensor(input_index, np.expand_dims(['Rear Air Conditioning'], 0))\r\ninterpreter.invoke()\r\noutput_details = interpreter.get_output_details()\r\n\r\n```\r\n\r\n### 3. Failure after conversion\r\nException occurs while invoking the interpreter.\r\n**RuntimeError: hashtable need to be initialized before usingNode number 74 (HASHTABLE_FIND) failed to invoke.**\r\n\r\n", "comments": ["Hi @0xiNach! Could you please share the above code as Colab GIST , I am facing indentation issues  while replicating the same. Did you try with latest versions TF 2.6 yet?", "Hi @mohantym, I've fixed the indentation. I did try with TF 2.6 but again can not convert the model to tflite. Had to download tf-nightly-gpu in order to convert it into tflite. When I try to infer using tflite model (with tf-nightly-gpu), same error had appeared.", "Hi @0xiNach ! Could you please attach the above code as colab gist with **a link to  dataset for x_train and y_train t**o replicate the issue ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52206\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52206\">No</a>\n"]}, {"number": 52205, "title": "[TF] Creates a Github Issue on PR rollback", "body": "Creates a GitHub issue when a Merged PR is rolledback.  \r\n\r\nThe rollback commit is expected to have text `[ROLLBACK_PR=<pr_number>]` in the message.  If the Commit doesn't have this specific tag, this GitHub action is skipped.", "comments": []}, {"number": 52204, "title": "[oneDNN] Fixing eager-related nightly test failures", "body": "This PR fixes : //tensorflow/python/kernel_tests:matmul_op_test and run_eager_op_as_function_test,\r\nThese tests which are run with eager op as function enabled, fail with error , \"Could not find device for node:\"\r\n\r\nWith a recent commit on public tensorflow : tensorflow/tensorflow@1224326\r\nEager ops are wrapped, inside function WrapInCallOp, after the device has been inferred for op.\r\n\r\nFor MKL : we expect WrapInCallOp to be called before we infer device, which marks the '_kernel' attribute as NameChangeOp label for nodeDef of funcDef.\r\nSince with new commit, WrapInCallOp is called AFTER the device inference, no device is found for the MKL Op as no kernel match is found without namechange label.\r\n\r\nIn this PR: we mark nodeDef with '_kernel' attribute as NameChange label at both places : a) before inferring the device and b) inside WrapInCallOp (already present in current code base)", "comments": ["@saxenasaurabh It sounds like related to the eager-op-as-function project, Could you take a look at this?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52204) for more info**.\n\n<!-- need_author_cla -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52204) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent."]}, {"number": 52202, "title": "Fix MacOS TF build by reverting an LLVM commit locally.", "body": "Revert https://github.com/llvm/llvm-project/commit/33e1713a00a5291e5de658d0eb0aebdbf1d3aa03 which seems to break macOS CPU TF build.\r\n\r\nPiperOrigin-RevId: 399933600\r\nChange-Id: I2b4b25ff6b558687e29778649b195594a34c0f0d", "comments": []}, {"number": 52201, "title": "Fix MacOS TF build by reverting an LLVM commit locally--merge to r2.7", "body": null, "comments": ["Moved to #52202"]}, {"number": 52199, "title": "ipykernel_launcher.py: error: unrecognized arguments", "body": "\r\n![1](https://user-images.githubusercontent.com/26819449/135461638-7520ab41-8310-4c5f-8016-2d9b4609e205.JPG)\r\n The error pops out ipykernel_launcher.py: error: unrecognized arguments.\r\nCan you just tell me why I am getting this error. I can't share the code with you.\r\nI can fix it on my own just tell me why I am getting this Error.\r\nThank You.\r\n", "comments": ["Hi @starboyvarun! Please check out similar issue from [here](https://stackoverflow.com/questions/48796169/how-to-fix-ipykernel-launcher-py-error-unrecognized-arguments-in-jupyter). You can post your  other support queries on [Stackoverflow](https://stackoverflow.com/) . For Tensorflow support queries , You can post in [forum ](https://discuss.tensorflow.org/)to  for Tensorflow support queries. Closing this issue as it is not a bug or feature request in  Tensorflow Repository. Thanks!"]}, {"number": 52198, "title": "Tensorflow models predictions not working on multithreading ", "body": "I am facing this issue while testing FasterRCNN and SSD on multithreading on **AWS Deep Learning AMI Ubuntu**.\r\n\r\nHowever, the testing is going fine without multithreading.\r\n\r\n**Also, the same code is working fine **Colab** but not on **Kaggle****.\r\n\r\n- TensorFlow installed from (source or binary): 2.4.2\r\n- Python version: 3.7.10\r\n- AWS deep learning AMI env - for TensorFlow 2.4 with Python3.7 (CUDA + and Intel MKL-DNN) ________________________ source activate tensorflow2_latest_p37\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Tesla T4 16 GB\r\n\r\nError - \r\n\r\n2.4.2\r\n{4: 'gun_not_inhand', 2: 'curlinary_knife', 3: 'gun_inhand', 1: 'bloody', 5: 'knife_in_hand', 6: 'knife_not_in_hand'}\r\nTraceback (most recent call last):\r\n  File \"ssd_img.py\", line 132, in <module>\r\n    for _ in executor.map(call,listF):\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/concurrent/futures/_base.py\", line 598, in result_iterator\r\n    yield fs.pop().result()\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/concurrent/futures/_base.py\", line 435, in result\r\n    return self.__get_result()\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/concurrent/futures/thread.py\", line 57, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"ssd_img.py\", line 87, in call\r\n    res = detector_prediction(i)\r\n  File \"ssd_img.py\", line 38, in detector_prediction\r\n    output = detection_model(img_array) #get list of tensors discussed above as output\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1669, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 247, in _call_impl\r\n    args, kwargs, cancellation_manager)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1687, in _call_impl\r\n    return self._call_with_flat_signature(args, kwargs, cancellation_manager)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1736, in _call_with_flat_signature\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\r\n  (0) Not found:  Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysPreprocessor/map/TensorArray_43)\r\n         [[node Preprocessor/map/while/TensorArrayReadV3 (defined at ssd_img.py:18) ]]\r\n         [[SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape_1/_296]]\r\n  (1) Not found:  Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysPreprocessor/map/TensorArray_43)\r\n         [[node Preprocessor/map/while/TensorArrayReadV3 (defined at ssd_img.py:18) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_pruned_7040]\r\n\r\nFunction call stack:\r\npruned -> pruned\r\n\r\n(tensorflow2_latest_p37) shiva.baghel@ip-10-0-0-175:~/Data/shiva.baghel/SSD$ python ssd_img.py\r\n2.4.2\r\n{4: 'gun_not_inhand', 2: 'curlinary_knife', 3: 'gun_inhand', 1: 'bloody', 5: 'knife_in_hand', 6: 'knife_not_in_hand'}\r\nException in thread Thread-8:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"ssd_img.py\", line 87, in call\r\n    res = detector_prediction(i)\r\n  File \"ssd_img.py\", line 38, in detector_prediction\r\n    output = detection_model(img_array) #get list of tensors discussed above as output\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1669, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 247, in _call_impl\r\n    args, kwargs, cancellation_manager)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1687, in _call_impl\r\n    return self._call_with_flat_signature(args, kwargs, cancellation_manager)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1736, in _call_with_flat_signature\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\r\n  (0) Not found:  No algorithm worked!\r\n         [[node FirstStageFeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/Conv2D (defined at ssd_img.py:18) ]]\r\n  (1) Not found:  No algorithm worked!\r\n         [[node FirstStageFeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/Conv2D (defined at ssd_img.py:18) ]]\r\n         [[map/while/LoopCond/_454]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_pruned_7079]\r\n\r\nFunction call stack:\r\npruned -> pruned\r\n\r\n\r\nException in thread Thread-15:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"ssd_img.py\", line 87, in call\r\n    res = detector_prediction(i)\r\n  File \"ssd_img.py\", line 38, in detector_prediction\r\n    output = detection_model(img_array) #get list of tensors discussed above as output\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1669, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 247, in _call_impl\r\n    args, kwargs, cancellation_manager)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1687, in _call_impl\r\n    return self._call_with_flat_signature(args, kwargs, cancellation_manager)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1736, in _call_with_flat_signature\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\r\n  (0) Not found:  No algorithm worked!\r\n         [[node FirstStageFeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/Conv2D (defined at ssd_img.py:18) ]]\r\n  (1) Not found:  No algorithm worked!\r\n         [[node FirstStageFeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/Conv2D (defined at ssd_img.py:18) ]]\r\n         [[map/while/LoopCond/_454]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_pruned_7079]\r\n\r\nFunction call stack:\r\npruned -> pruned\r\n", "comments": ["@Sbboss \r\nPlease refer to these links and let us know: #43223, #50302,[link](https://stackoverflow.com/questions/59340465/how-to-solve-no-algorithm-worked-keras-error)\r\nAlso please share the code for which this error is reported, or share a colab gist with the error reported.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52196, "title": "TensorRT conversion via docker image 2.6.0-gpu - getInferLibVersion symbol not found", "body": "Trying to convert a tf.keras model to tftrt.\r\n\r\n\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: GeForce RTX 2070\r\nv2.6.0-rc2-32-g919f693420e 2.6.0\r\n\r\n**Describe the current behavior**\r\n```\r\n from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n    precision = 'FP16'\r\n    params = tf.experimental.tensorrt.ConversionParams(precision_mode=precision, use_calibration=False, minimum_segment_size=3)\r\n    converter = trt.TrtGraphConverterV2(input_saved_model_dir=model_path, conversion_params=params)\r\n    calibration_input_fn_arg = None\r\n    converter.convert(calibration_input_fn_arg)\r\n    converter.save(trt_save_path)\r\n```\r\n**Describe the expected behavior**\r\n2021-09-30 09:41:30.493064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2021-09-30 09:41:30.493081: F tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:49] getInferLibVersion symbol not found.\r\nAborted (core dumped)\r\n", "comments": ["Hi @yotamtevel!  Can you please check similar issues from here [link1](https://stackoverflow.com/questions/60368298/could-not-load-dynamic-library-libnvinfer-so-6),[link2](https://github.com/tensorflow/tensorflow/issues/35968) .", "Hi @mohantym\r\nThose doe's not work.\r\n\r\nSame code and same model using nvcr.io/nvidia/tensorflow:21.07-tf2-py3 works just fine - note that it downgrades tensorflow to 2.5.0+nv", "Ok @yotamtevel! Could you please share the link to **model_path** too to replicate the issue?", "Hi @mohantym,\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\n\r\ndef mish(input_tensor):\r\n    return input_tensor * tf.keras.activations.tanh(tf.math.log(1 + tf.math.exp(input_tensor)))\r\n\r\n\r\ndef build_model():\r\n\r\n    input_tensor = tf.keras.layers.Input((None, None, 3), name='input')\r\n    scaled_input = tf.math.multiply(input_tensor, 1. / 255.)\r\n    d1 = tf.keras.layers.Conv2D(32, kernel_size=3, padding='SAME', strides=2)(scaled_input)\r\n    d1 = mish(d1)\r\n    d2 = tf.keras.layers.Conv2D(64, kernel_size=3, padding='SAME', strides=2)(d1)\r\n    d2 = mish(d2)\r\n\r\n    u1 = tf.keras.layers.UpSampling2D()(d2)\r\n    u1 = tf.keras.layers.Conv2D(32, kernel_size=3, padding='SAME')(u1)\r\n    u1 = mish(u1)\r\n    u1 = tf.keras.layers.Concatenate()([u1, d1])\r\n\r\n    output = tf.keras.layers.Conv2D(2, kernel_size=3, padding='SAME', activation='softmax')(u1)\r\n\r\n    model = tf.keras.models.Model(input_tensor, output)\r\n    return model\r\n\r\n\r\nif __name__ == '__main__':\r\n    model_save_load_path = ''  # TODO change\r\n    precision = 'FP16'\r\n    model = build_model()\r\n    base_model_save_path = os.path.join(model_save_load_path, 'tfkeras')\r\n    model.save(base_model_save_path)\r\n\r\n    from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\n    params = tf.experimental.tensorrt.ConversionParams(precision_mode=precision.upper(),\r\n                                                       use_calibration=False, minimum_segment_size=3)\r\n    converter = trt.TrtGraphConverterV2(input_saved_model_dir=base_model_save_path,\r\n                                        conversion_params=params)\r\n\r\n    calibration_input_fn_arg = None\r\n\r\n    converter.convert(calibration_input_fn_arg)\r\n    converter.save(os.path.join(model_save_load_path, precision))\r\n    print('done')\r\n```\r\n", "Hi @Saduf2019 ! Could you please look at this issue ?User has this issue in Docker. The above code is running without error in Colab.Attaching [Gist](https://colab.research.google.com/gist/mohantym/5517a74dae7cd30c9020ff0f7ff2b8dd/github_52196.ipynb) for reference along this [thread ](https://forums.developer.nvidia.com/t/tf-trt-optimization/179447/5 )with similar Error Stack Trace", "Could you please try the answer suggested [here](https://forums.developer.nvidia.com/t/could-not-load-dynamic-library-libnvinfer-so-5/107453).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52196\">No</a>\n"]}, {"number": 52192, "title": "TF not using GPU for models", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Latest Windows 10\r\n- TensorFlow installed from (source or binary): PIP\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4 CUDA, 8.1.0\r\n- GPU model and memory: 3060 12gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTensorflow uses the cpu when training models and not the gpu. I have installed Cuda and cudnn properly, and TensorFlow confirms the loading of cudnn lib. When I train models, the gpu fan doesn't spin, nor does it heat up, and the cpu shows heavy usage in task manager\r\n**Describe the expected behavior**\r\nTF uses gpu and not cpu\r\n**Standalone code to reproduce the issue**\r\nthis is the can notebook I am using:\r\nhttps://www.tensorflow.org/tutorials/images/cnn\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Loganpi \r\nCan you run:\r\nlen(tf.confirg.list_physical_devices('GPU'))\r\nyou may refer to [this](https://stackoverflow.com/questions/58956619/tensorflow-2-0-list-physical-devices-doesnt-detect-my-gpu) and let us know.", "Tensorflow returns 1, but still uses cpu.", "@Loganpi \r\nCan you try os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\". [if this dis not work please try os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" ]", "Hello, in an attempt to solve this problem I moved to linux and setup Tensorflow properly there.\r\nWhen trying to train networks the same problem persists, even though it says it created device, and loaded cuDNN.\r\n\r\nlen(tf.config.list_physical_devices('GPU')) gives 1\r\n\r\nBoth os.environ commands return nothing\r\n\r\nIt also says this when I call fit():\r\n\r\nI tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory", "@Loganpi \r\nAre you facing this error on all versions of tensorflow.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52191, "title": "Removes dependency of resource_var.h in stateful_random_ops_cpu_gpu.h\u2026merge to 2.7", "body": "\u2026\u2026merge to 2.7", "comments": []}, {"number": 52190, "title": "tf.constant's dtype causes exception after model load in tf.math.pow and other math_ops", "body": "I ran into this issue using tensorflow 2.5.1, and now tensorflow 2.6.0 after just updating.  Here's the abridged code:\r\n\r\n```\r\ndef test_model():\r\n   inputs = tf.keras.layers.Input(shape=(10, 10, 1), name=\"input_1\") \r\n\r\n   a = tf.cast(2, dtype=tf.int64)\r\n   b = tf.cast(inputs, dtype=tf.int64)\r\n\r\n   outputs = tf.math.pow(a, b)\r\n\r\n   model = tf.keras.models.Model(inputs, outputs)\r\n   return model\r\n\r\nif __name__ == '__main__':\r\n    model =  test_model()\r\n    save_path = \"./test_model\"\r\n    tf.keras.models.save_model(model, save_path, save_format=\"tf\")\r\n    model = tf.keras.models.load_model(save_path)\r\n```\r\n\r\nWhich produces the following exception:s\r\n`ValueError: Tensor conversion requested dtype int32 for Tensor with dtype int64: <tf.Tensor 'Placeholder:0' shape=(None, 6, 6, 1) dtype=int64>`\r\n`TypeError: Input 'y' of 'Pow' Op has type int64 that does not match type int32 of argument 'x'.\r\n`\r\n\r\nThese exceptions are thrown only after loading the model, the model works completely fine if not saved. Specifying the dtype of the Input layer, rather than casting `inputs`, does not seem to make a difference either. \r\nI've tried changing the dtypes of the both `a` and `b`, but every combination appears to throw this same exception. Can anyone recreate this issue?\r\n\r\nI assume there generally isn't an issue with using math ops in a functional model in this way? Is the only way to do this currently by subclassing tf.Module? \r\n\r\n", "comments": ["@HoltSpalding ,\r\nLooks like the code given is incomplete.In order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.Thanks!\r\n", "I think you have interchanged the order of `a` and `b` in `pow()`.\r\nTry this:\r\n> outputs = tf.math.pow(a, b)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52190\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52190\">No</a>\n"]}]