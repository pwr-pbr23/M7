[{"number": 30697, "title": "Freeze_graph not supporting tf.control_dependencies", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary (pypi: tensorflow-gpu==1.14.0)\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA-10/CUDNN-7\r\n- GPU model and memory: NVIDIA 1080\r\n\r\n**Describe the current behavior**\r\nThis is the simple code I want to freeze, where `out` node is as the output node names:\r\n```py\r\n  print_op = tf.print(tf.ones([2, 2]))\r\n  with tf.control_dependencies([print_op]):\r\n    out = tf.zeros([3,])\r\n```\r\nBy freezing the `out` node with `freeze_graph.freeze_graph(...)`, what I got from the protobuf only contains nodes like `StringFormat`, `PrintV2` and `Ones`. No else information to show there execution order. Seems like the execution order managed by the `tf.control_dependencies` is lost from the frozen graph.\r\n\r\n**Code to reproduce the issue**\r\n\r\nFreeze any large or small graph that contains using `tf.control_dependencies`, and check how it is described in the frozen protobuf data.\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30697\">No</a>\n"]}, {"number": 30696, "title": "[Intel MKL] Fix UT failure of 'NodeRewrite_BiasAddGrad_Positive1'.", "body": "Fix the UT failure in mkl_layout_pass_test.cc.", "comments": []}, {"number": 30695, "title": "ModelCheckpoint should have a way to delete non-best model checkpoints as it goes", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0b0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntensorflow.keras.callbacks.ModelCheckpoint should have an (optional) way to clean up non-best model checkpoints as it goes. This would help avoid disk consumption issues for larger models and other uses cases like doing a lot of training during hyperparameter searches.\r\n\r\n**Will this change the current api? How?**\r\nYes. My proposal is to add a 'delete_previous_best_checkpoints' named argument to the constructor for tensorflow.keras.callbacks.ModelCheckpoint with the default set to False for backward compatibility. The docs would be something like this:\r\ndelete_previous_best_checkpoints: If True then the previous best checkpoint will be deleted after a new best checkpoint is saved. Only non-best checkpoints created by the current training session will be deleted. Resumed training from CheckpointA will not delete CheckpointA or any previous checkpoint.\r\n\r\n**Who will benefit with this feature?**\r\nFolks training large models in disk consumption conscious environments (think AWS budget). Or folks doing hyperparameter searches where it's easy to create hundreds of checkpoints quickly.\r\n\r\n**Any Other info.**\r\nI'm happy to submit a PR for this.", "comments": ["Hi @jim-meyer, how would it be different to set `save_best_only = True` in the current API?\r\n```\r\nkeras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\r\n```", "Hi @eduardofv, The behavior I'm seeing is that `save_best_only=True`  doesn't do any deleting of previous \"best\" models when a new best one is found.\r\nExample: I train a new model for 2 iterations. The first model will obviously be the best we've seen so `save_best_only` causes Model1.h5 to be saved. Next, model 2 is better than 1 so Model2.h5 gets saved. But model1.h5 does not get deleted due to `save_best_only`.\r\nSo after 2 epochs I end up with 2 models with `save_best_only=True`:\r\n- Model1.h5\r\n- Model2.h5\r\n\r\nWith my proposal only 1 model would exist after the second epoch since Model1.h5 would be deleted by the new `delete_previous_best_checkpoints=True` option:\r\n- Model2.h5\r\n\r\nDoes that help explain it better? If so I'll add something to that effect to the docs in the PR.", "@jim-meyer Please check the following example from this [source](https://machinelearningmastery.com/check-point-deep-learning-models-keras/) and let us know how your proposal is different from it. Thanks!\r\n\r\n```\r\n# Checkpoint the weights for best model on validation accuracy\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.callbacks import ModelCheckpoint\r\nimport matplotlib.pyplot as plt\r\nimport numpy\r\n# fix random seed for reproducibility\r\nseed = 7\r\nnumpy.random.seed(seed)\r\n# load pima indians dataset\r\ndataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\r\n# split into input (X) and output (Y) variables\r\nX = dataset[:,0:8]\r\nY = dataset[:,8]\r\n# create model\r\nmodel = Sequential()\r\nmodel.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\r\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\r\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\r\n# Compile model\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n# checkpoint\r\nfilepath=\"weights.best.hdf5\"\r\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\r\ncallbacks_list = [checkpoint]\r\n# Fit the model\r\nmodel.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\r\n\r\n```", "Thanks @jvishnuvardhan . After reading that article again I finally noticed this little blurb:\r\n```\r\nA simpler check-point strategy is to save the model weights to the same file, if and only if the validation accuracy improves.\r\nThis can be done easily using the same code from above and changing the output filename to be fixed (not include score or epoch information).\r\n```\r\n\r\nI had been using filepath='checkpoint-{epoch}.h5'. But now that I remove the {epoch} and am now using 'best-checkpoint.h5' I see that that essentially solves my use case.\r\nI'll put together a PR that does just a minor documentation update to make this a bit more obvious for the next person.", "@jim-meyer Sure. Go ahead and raise PR to improve documentation. I am closing the issue as it was resolved. Thanks!", "@jvishnuvardhan Wouldn't it still make sense to have a feature similar to what @jim-meyer is proposing? I'm currently in the same situation: I want previous \"best\" models to be overwritten by the new \"best\" model, but I still want to include what `epoch` and `val_loss` the model was generated at. To my understanding it's only possible to do either or? Either overwrite the previous best model and not having `epoch` etc in the name, or have `epoch`etc in the name, but then every new better model is saved.", "A very simple approach could be this:\r\n\r\n```python\r\nclass KeepNBestCheckpoints(keras.callbacks.Callback):\r\n\r\n    def __init__(self, checkpoints_dir, num_keep=5, find_checkpoints_fn=find_checkpoints):\r\n        super().__init__()\r\n        self.checkpoints_dir = checkpoints_dir\r\n        self.num_keep = num_keep\r\n        self.find_checkpoints = find_checkpoints_fn\r\n\r\n    def on_test_end(self, logs=None):\r\n        logger = tf.get_logger()\r\n        if self.num_keep and self.num_keep > 0:\r\n            checkpoints = self.find_checkpoints(self.checkpoints_dir, ascending=False)\r\n            for checkpoint in checkpoints[self.num_keep:]:\r\n                logger.debug(f'Removing checkpoint {checkpoint}')\r\n                checkpoint_files = tf.io.gfile.glob(checkpoint + '*')\r\n                for file in checkpoint_files:\r\n                    logger.debug(f'Removing: {file}')\r\n                    tf.io.gfile.remove(file)\r\n```\r\n\r\nWhere `find_checkpoints()` could be a default implementation like this:\r\n\r\n```python\r\ndef find_checkpoints(model_dir, ascending=True):\r\n    \"\"\"Get all checkpoints in descending order sorted by (epoch, step).\r\n    The checkpoint names must follow the pattern ckpt-{epoch}-{step}.index.\r\n    :param model_dir: The location of the model training.\r\n    :param ascending: If True (default) checkpoints are sorted in ascending order (latest to oldest).\r\n    \"\"\"\r\n    checkpoints = tf.io.gfile.glob(os.path.join(model_dir, 'ckpt-*.index'))\r\n    checkpoints = map(lambda s: s.strip('.index'), checkpoints)\r\n    by_step = list()\r\n    for checkpoint in checkpoints:\r\n        checkpoint_name = os.path.basename(checkpoint)\r\n        _, epoch, step = checkpoint_name.split('-')\r\n        by_step.append(((epoch, step), checkpoint))\r\n    by_step = sorted(by_step, key=itemgetter(0))\r\n    if not ascending:\r\n        by_step = by_step[::-1]\r\n    return list(map(itemgetter(1), by_step))\r\n```\r\n\r\nDepending on how you name your checkpoints.", "I wrote a [ModelCheckpoint subclass](https://github.com/schustmi/tf_utils/blob/915fe5e231ca302b28cd02dc8ac2e4c772a62e0b/tf_utils/callbacks.py#L34) with this functionality that does not require any custom function to find checkpoints if anyone is still interested in this."]}, {"number": 30694, "title": "Fail when np.array and len are called on Tensors.", "body": "When np.array is called on a symbolic Tensor the result is a `shape=()` numpy array of objects which is basically never what was intended. Similarly, length is not defined and can lead to rather cryptic error messages. This surfaced through https://github.com/tensorflow/tensorflow/issues/28619; however the fact is that accidentally passing a Tensor rather than and EagerTensor to a package in the NumPy ecosystem can result in very cryptic error messages.\r\n\r\nThis PR simply makes Tensors fail with clear error messages in such cases. (Similar to the treatment of `__iter__`)", "comments": ["I updated the autograph test. It was just making sure `len()` failed to test autograph, so I don't think it's an issue to simply update the test. @mdanatg is that correct?", "> I updated the autograph test. It was just making sure `len()` failed to test autograph, so I don't think it's an issue to simply update the test. @mdanatg is that correct?\r\n\r\nYes, the update to the test looks good to me.", "But np.sum(my_tensorflow_tensor) already doesn't work for graph tensors,\nright?\n\nOn Wed, Jul 17, 2019 at 12:56 PM Taylor Robie <notifications@github.com>\nwrote:\n\n> *@robieta* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/framework/ops_test.py\n> <https://github.com/tensorflow/tensorflow/pull/30694#discussion_r304611233>\n> :\n>\n> > @@ -148,6 +150,24 @@ def testShapeFunctionError(self):\n>            r\"\\(op: 'Add(V2)?'\\) with input shapes: \\[1,2,3\\], \\[4,5,6\\].\"):\n>          _ = a + b\n>\n> +  def testNumpyArray(self):\n> +    with context.graph_mode():\n> +      x = array_ops.ones((3, 4), name=\"test_ones\")\n> +\n> +    with self.assertRaisesRegexp(NotImplementedError,\n> +                                 r\"Cannot convert a symbolic.+test_ones\"):\n> +      np.array(x)\n>\n> So it seems that overriding __array__ fundamentally changes the behavior\n> of all interactions with numpy's ufuncs. For instance given the current\n> behavior for the following\n>\n> x = np.array(1.)\n> y = tf.ones(())\n>\n> x / y\n> y / x\n> np.divide(x, y)\n>\n> is that all three divisions call the tf divide function and return a\n> single Tensor. However, once __array__ is overridden to produce an error\n> then np.divide fails. This will be true for all ufuncs. (no more\n> np.sum(my_tensorflow_tensor)) I think in order to both disallow\n> np.array(my_tensorflow_tensor) and properly support numpy dispatch we\n> will have to implement either the ufunc or array_function protocols for\n> Tensors which is a much more significant undertaking.\n>\n> This also suggests that we should have better unit tests for numpy\n> compatibility in tensorflow, since this was only caught by the large-scale\n> integration test.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30694?email_source=notifications&email_token=AAABHRNNIAU6ANFIV7XO4M3P752NBA5CNFSM4IDSLXMKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOB6YU3VQ#discussion_r304611233>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROU52FQD3MRRIVHP33P752NBANCNFSM4IDSLXMA>\n> .\n>\n\n\n-- \n - Alex\n", "Ugg. Looks like it runs but has elementwise `add` rather than `reduce_sum` semantics:\r\n```\r\nx = tf.ones((4,))\r\ny = np.ones((4,))\r\n\r\nnp.sum(y)  # 4.0\r\nnp.sum(x)  # Symbolic ones Tensor\r\nx is np.sum(x)  # True\r\nnp.sum([x, x])  # Symbolic twos Tensor\r\n```", "Hmm, that kind of behavior sounds like trouble down the road because it\nonly appears to work due to some fortunate circumstances. If nothing else,\nI think this should output a warning.\n\nOn Wed, Jul 17, 2019 at 4:26 PM Taylor Robie <notifications@github.com>\nwrote:\n\n> Ugg. Looks like it runs but has elementwise add rather than reduce_sum\n> semantics:\n>\n> x = tf.ones((4,))\n> y = np.ones((4,))\n>\n> np.sum(y)  # 4.0\n> np.sum(x)  # Symbolic ones Tensor\n> x is np.sum(x)  # True\n> np.sum([x, x])  # Symbolic twos Tensor\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30694?email_source=notifications&email_token=AGLFDQZMOYONVLFOJQVPZ3DP7557PA5CNFSM4IDSLXMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2GO5SY#issuecomment-512552651>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGLFDQYLV6WMSIVHHSQOFULP7557PANCNFSM4IDSLXMA>\n> .\n>\n", "Closing as this has been merged. (via the tf sync process.)"]}, {"number": 30693, "title": "ImportError: cannot import name 'lite'", "body": "on ubuntu 18.4\r\n\r\nusing this input \r\n\r\n`from tensorflow.contrib import lite\r\nconverter = lite.TFLiteConverter.from_keras_model_file( 'model.h5')\r\ntfmodel = converter.convert()\r\nopen (\"model.tflite\" , \"wb\") .write(tfmodel)`\r\n\r\ni got this error\r\n\r\n`Traceback (most recent call last):\r\n  File \"h5totflite.py\", line 1, in <module>\r\n    from tensorflow.contrib import lite\r\nImportError: cannot import name 'lite'`", "comments": ["@jeet4690 ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "ubuntu 18.4\r\n\r\nworking on google COLAB\r\n\r\ntensorflow version 1.14\r\n", "@jeet4690 ,\r\nYou can use the below code which has been executed in google colab and is working with TF version 1.14\r\n\r\n```\r\n#from tensorflow.contrib import lite \r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file('my_model.h5') \r\ntfmodel = converter.convert() \r\nopen (\"model.tflite\" , \"wb\") .write(tfmodel)\r\n```", "@anush-o after trying that it gave me this error\r\n\r\n`ValueError: No model found in config file.`\r\n\r\n![Screenshot from 2019-07-19 20-23-04](https://user-images.githubusercontent.com/32290207/61544538-51100780-aa63-11e9-895c-ce9408c96c10.png)\r\n\r\n", "@haozha111 can you plz help me find the problem, P.S. i am trying to convert `model.h5` it only have \"weights\" into `model.tflite`", "The error comes from keras code when loading the h5 file:\r\nhttps://github.com/tensorflow/tensorflow/blob/3f89bc5175caf279ae232088c88891b804a6b51f/tensorflow/python/keras/saving/hdf5_format.py#L158\r\n\r\nPlease make sure that the h5 file contains the model itself, not only the weights.", "hello\r\nmy code is\r\n\r\n```python\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file( r'/content/drive/My Drive/inceptionv3-transfer-learning__fine_tune.h5') # Your model's name\r\nmodel = converter.convert()\r\nfile = open( 'model.tflite' , 'wb' ) \r\nfile.write( model )\r\n```\r\n```\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.\r\n```\r\nwhat can ido?", "Please post your question in a new issue and we will triage it. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30693\">No</a>\n"]}, {"number": 30692, "title": "Extending documentation for tf.data.Dataset.map()", "body": "Added an example for running a custom Python function (as discussed in https://github.com/tensorflow/tensorflow/issues/30653)", "comments": []}, {"number": 30691, "title": "Strange bug with uint64 and tf.constants", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 2.7\r\n\r\n**Describe the current behavior**\r\nRun the following code\r\n```\r\nimport tensorflow as tf\r\n\r\nxx = tf.constant([ 54043195528445964 , 72057594037927941 , 54043195528445957, 54043195528445954, 108086391056891910], dtype=tf.int64)\r\nyy = tf.cast(xx, dtype=tf.uint64)\r\n\r\n\r\nqqq=tf.constant([ 1,  2 , 3, 4, 5])\r\nwww=tf.constant([ 1,  2 , 3, 4, 5])\r\n\r\nsess = tf.Session()\r\nwith sess.as_default():\r\n    print(sess.run(xx))\r\n    print(sess.run(yy))\r\n```\r\nthe output is \r\n```\r\n[ 54043195528445964  72057594037927941  54043195528445957\r\n  54043195528445954 108086391056891910]\r\n[0 0 0 0 0]\r\n```\r\nwhile it should have been\r\n```\r\n[ 54043195528445964  72057594037927941  54043195528445957\r\n  54043195528445954 108086391056891910]\r\n[ 54043195528445964  72057594037927941  54043195528445957\r\n  54043195528445954 108086391056891910]\r\n```\r\n\r\nHowever, the following code\r\n```\r\nimport tensorflow as tf\r\n\r\nxx = tf.constant([ 54043195528445964 , 72057594037927941 , 54043195528445957, 54043195528445954, 108086391056891910], dtype=tf.int64)\r\nyy = tf.cast(xx, dtype=tf.uint64)\r\n\r\nsess = tf.Session()\r\nwith sess.as_default():\r\n    print(sess.run(xx))\r\n    print(sess.run(yy))\r\n```\r\nprints the correct output\r\n```\r\n[ 54043195528445964  72057594037927941  54043195528445957\r\n  54043195528445954 108086391056891910]\r\n[ 54043195528445964  72057594037927941  54043195528445957\r\n  54043195528445954 108086391056891910]\r\n```\r\n\r\nThis is really weird\r\n", "comments": ["If you set multiply constants, and which are not used.\r\nTensorflow would go through a FoldNode operation to do same convertion.\r\nI think that where the problem happens.", "Find a workaround to this problem:\r\n```\r\nimport tensorflow as tf\r\n\r\nxx = tf.constant([ 54043195528445964 , 72057594037927941 , 54043195528445957, 54043195528445954, 108086391056891910], dtype=tf.int64)\r\nyy = tf.cast(xx, dtype=tf.uint64)\r\n\r\n\r\nqqq=tf.constant([ 1,  2 , 3, 4, 5])\r\nwww=tf.constant([ 1,  2 , 3, 4, 5])\r\n\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\nrewriteOptions = rewriter_config_pb2.RewriterConfig(min_graph_nodes=10)\r\ngraphOptions = tf.GraphOptions(rewrite_options=rewriteOptions)\r\nconfig = tf.ConfigProto(graph_options=graphOptions)\r\n\r\nsess = tf.Session(config=config)\r\nwith sess.as_default():\r\n    print(sess.run(xx))\r\n    print(sess.run(yy))\r\n```\r\nLooking for root-cause", "I could reproduce the issue in TF1.13 and even in tf-nightly (TF1.15). However, [your code](https://colab.sandbox.google.com/gist/jvishnuvardhan/487e9949fbe04c58700ced437a153e32/tf_30691_ops.ipynb) runs without any issue when `tf-nightly-2.0-preview==2.0.0.dev20190718` used.Thanks!", "@jvishnuvardhan  Sorry to interrupt, tf-nightly-2.0-preview==2.0.0.dev20190718 is built based on master branch or any other branchs?\r\n", "@Leslie-Fang `f-nightly-2.0-preview` are built using [TF2.0](https://github.com/tensorflow/tensorflow/tree/r2.0). Thanks!", "@piiswrong was this issue resolved by following @Leslie-Fang workaround? Are you interested in trying TF2.0? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30691\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30691\">No</a>\n"]}, {"number": 30690, "title": ".h5 to tflite conversion error", "body": "working on GOOGLE COLAB\r\n\r\ntensorflow version 1.2\r\n\r\nINPUT :- \r\n`from tensorflow.contrib import lite\r\n\r\nconverter = lite.TFLiteConverter.from_keras_model_file( 'model.h5' ) # Your model's name\r\n\r\nmodel = converter.convert()\r\n\r\nfile = open( 'model.tflite' , 'wb' )\r\n \r\nfile.write( model )`\r\n\r\nERROR:-\r\n`ImportError                               Traceback (most recent call last)\r\n<ipython-input-1-e02e9998849c> in <module>()\r\n      1 \r\n----> 2 from tensorflow.contrib import lite\r\n      3 converter = lite.TFLiteConverter.from_keras_model_file( 'model.h5' ) # Your model's name\r\n      4 \r\n      5 model = converter.convert()`\r\n\r\n`ImportError: cannot import name 'lite'`", "comments": ["AND BY THIS INPUT:-\r\n`from  tensorflow.contrib.lite.python import convert_saved_model\r\nconvert_saved_model.tflite_from_saved_model(saved_model_dir=\"model.h5\",output_file=\"/TF_Lite_Model\")`\r\n\r\nERROR:-\r\n`ModuleNotFoundError: No module named 'tensorflow.contrib.lite'`", "@dr-rathod ,\r\nIs it possible for you to upgrade the TF version to 1.14 and check if the issue still persists. Thanks", "Closing due to lack of recent activity.Thanks!"]}, {"number": 30689, "title": "Need help training model on TPU on Colab", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nHey everyone, \r\n\r\nI'm training a Neural Machine Translation model on Colab using `tf.keras` and when I try to connect to the TPU, it gives me this error:\r\n```\r\nW0714 02:35:35.778475 140626016266112 keras_support.py:217] Keras support is now deprecated in support of TPU Strategy. Please follow the distribution strategy guide on tensorflow.org to migrate to the 2.0 supported version.\r\nI0714 02:35:35.780029 140626016266112 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.17.222.66:8470) for TPU system metadata.\r\nI0714 02:35:35.794606 140626016266112 tpu_system_metadata.py:148] Found TPU system:\r\nI0714 02:35:35.796221 140626016266112 tpu_system_metadata.py:149] *** Num TPU Cores: 8\r\nI0714 02:35:35.797333 140626016266112 tpu_system_metadata.py:150] *** Num TPU Workers: 1\r\nI0714 02:35:35.798576 140626016266112 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\r\nI0714 02:35:35.799489 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 18249094776997611553)\r\nI0714 02:35:35.801580 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1473395334155129707)\r\nI0714 02:35:35.803380 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12488202382755017065)\r\nI0714 02:35:35.804275 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 11169959297202392905)\r\nI0714 02:35:35.806226 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 17277375797260286420)\r\nI0714 02:35:35.808259 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14241437196071514035)\r\nI0714 02:35:35.811089 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6205429048552507469)\r\nI0714 02:35:35.813731 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6051595501763147539)\r\nI0714 02:35:35.816036 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4392780516891638609)\r\nI0714 02:35:35.817032 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 17401714988011142246)\r\nI0714 02:35:35.819163 140626016266112 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3282974203306179719)\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1355     try:\r\n-> 1356       return fn(*args)\r\n   1357     except errors.OpError as e:\r\n\r\n11 frames\r\nInvalidArgumentError: From /job:worker/replica:0/task:0:\r\nIn ReadVariableOp the following variables were found uninitialized: dense_3/bias, dense_3/kernel, embedding_3/embeddings, lstm_6/bias, lstm_6/kernel, lstm_6/recurrent_kernel, lstm_7/bias, lstm_7/kernel, lstm_7/recurrent_kernel\r\n\t [[{{node ReadVariables_10092430516030672714/_1}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1368           pass\r\n   1369       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1370       raise type(e)(node_def, op, message)\r\n   1371 \r\n   1372   def _extend_graph(self):\r\n\r\nInvalidArgumentError: From /job:worker/replica:0/task:0:\r\nIn ReadVariableOp the following variables were found uninitialized: dense_3/bias, dense_3/kernel, embedding_3/embeddings, lstm_6/bias, lstm_6/kernel, lstm_6/recurrent_kernel, lstm_7/bias, lstm_7/kernel, lstm_7/recurrent_kernel\r\n\t [[{{node ReadVariables_10092430516030672714/_1}}]]\r\n```\r\n\r\nMy model is as such:\r\n\r\n```python\r\ndef NMT_Model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\r\n    model.add(tf.keras.layers.LSTM(units))\r\n    model.add(tf.keras.layers.RepeatVector(out_timesteps))\r\n    model.add(tf.keras.layers.LSTM(units, return_sequences=True))\r\n    model.add(tf.keras.layers.Dense(out_vocab, activation='softmax'))\r\n    \r\n    return model\r\n\r\nmodel = NMT_Model(ger_vocab_size, eng_vocab_size, ger_sequence_length, eng_sequence_length, 512)\r\nrms = tf.train.RMSPropOptimizer(learning_rate=0.01)\r\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=rms, metrics=['acc'])\r\nmodel.summary()\r\n```\r\n\r\nCan I please know how I can go about successfully connecting to a TPU and running a straightforward `tf.keras` model on it? Any help would be highly appreciated!\r\n\r\nIf there are any other details I can give, please do let me know!\r\n\r\nCheers!", "comments": []}, {"number": 30688, "title": "Disable some tests in tools/", "body": "Somehow they fail on the patch release", "comments": []}, {"number": 30687, "title": "\"ambiguous call to overloaded function\" error during compiling tensorflow on Windows ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2017/2019\r\n- CUDA/cuDNN version:10.0/7 and 10.1/7\r\n- GPU model and memory: Geforce GTX 1080, 8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI get the following error during the compile:\r\n\r\n.\\tensorflow/stream_executor/kernel.h(534): error C2668: 'stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam': ambiguous call to overloaded function\r\n.\\tensorflow/stream_executor/kernel.h(557): note: could be 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam<T>(stream_executor::KernelArgsArray<4> *,const T &,void *) const'\r\nwith\r\n[\r\nT=stream_executor::DeviceMemory<tensorflow::uint8>\r\n]\r\n.\\tensorflow/stream_executor/kernel.h(532): note: or 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam<T,>(stream_executor::KernelArgsArray<4> *,const T &) const'\r\nwith\r\n[\r\nT=stream_executor::DeviceMemory<tensorflow::uint8>\r\n]\r\n.\\tensorflow/stream_executor/kernel.h(534): note: while trying to match the argument list '(stream_executor::KernelArgsArray<4> *, const T)'\r\nwith\r\n[\r\nT=stream_executor::DeviceMemory<tensorflow::uint8>\r\n]\r\n.\\tensorflow/stream_executor/kernel.h(528): note: see reference to function template instantiation 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>(stream_executor::KernelArgsArray<4> *,const T &,const tensorflow::uint8 &,const tensorflow::uint64 &,const stream_executor::DeviceMemory<tensorflow::uint64> &) const' being compiled\r\nwith\r\n[\r\nT=stream_executor::DeviceMemory<tensorflow::uint8>\r\n]\r\n.\\tensorflow/stream_executor/kernel.h(528): note: see reference to function template instantiation 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackOneParam<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>(stream_executor::KernelArgsArray<4> *,const T &,const tensorflow::uint8 &,const tensorflow::uint64 &,const stream_executor::DeviceMemory<tensorflow::uint64> &) const' being compiled\r\nwith\r\n[\r\nT=stream_executor::DeviceMemory<tensorflow::uint8>\r\n]\r\n.\\tensorflow/stream_executor/kernel.h(527): note: while compiling class template member function 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackParams(stream_executor::KernelArgsArray<4> *,stream_executor::DeviceMemory<tensorflow::uint8> &,tensorflow::uint8 &,tensorflow::uint64 &,stream_executor::DeviceMemory<tensorflow::uint64> &) const'\r\n.\\tensorflow/stream_executor/stream_executor_pimpl.h(882): note: see reference to function template instantiation 'void stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>::PackParams(stream_executor::KernelArgsArray<4> *,stream_executor::DeviceMemory<tensorflow::uint8> &,tensorflow::uint8 &,tensorflow::uint64 &,stream_executor::DeviceMemory<tensorflow::uint64> &) const' being compiled\r\ntensorflow/stream_executor/cuda/redzone_allocator.cc(269): note: see reference to class template instantiation 'stream_executor::TypedKernel<stream_executor::DeviceMemory<tensorflow::uint8>,tensorflow::uint8,tensorflow::uint64,stream_executor::DeviceMemory<tensorflow::uint64>>' being compiled\r\n\r\n\r\nThe error is found at kernel.h file: (https://github.com/tensorflow/tensorflow/blob/release_1.14.0/tensorflow/stream_executor/kernel.h\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI did everything as explained in the tensorflow website.\r\n\r\n\r\n**Any other info / logs**\r\nI get the error whether I am compiling with CUDA 10.0 or 10.1, vs 2017 or vs 2019.", "comments": ["@khazali ,\r\nI could not open the link https://github.com/tensorflow/tensorflow/blob/release_1.14.0/tensorflow/stream_executor/kernel.\r\nCan you please provide the working link. Thanks", "@anush-o\r\nI have to appologize, the link has been changed to:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/release_1.14.0/tensorflow/stream_executor/kernel.h", "@chsigg to confirm, but I think this should be fixed at head?", "Yes, this has been fixed. Please let us know if you still see this issue.", "Thanks for your considerations. However, I have compiled the master branch (since there was no change in release_1.14.0), and the error still persists. The compiler log can be accessed here:\r\nhttps://github.com/khazali/TensorflowOutput/blob/master/Out.txt", "In our continuous tests, for the last week I have never seen the issue recur.\r\nhttps://source.cloud.google.com/results/invocations/c96c6321-edc4-4f0d-bbf3-c0da191c407f/log\r\nThe above is the most recent build log for TF at head.\r\n\r\nIs it possible your client was not synced to head?", "I have tested the updated master branch again, and the compiler output was the same.", "We have a potential fix we are hoping it may address this.\r\nWe are hoping to push it tomorrow (Mountain View Daytime).\r\n\r\nYou may be able to try it out to see if that works.", "Thanks a lot.", "The potential fix is now merged.", "It was compiled successfully with VS2019. Thank you."]}, {"number": 30686, "title": "`tf.reduce_*` called on the result of `tf.TensorArray.concat` of unknown rank return corrupted Tensor", "body": "**System information**\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\n\r\nSee the reproducing code. When `tf.reduce_mean` is called on the output of a `TensorArray.concat` which had a fully dynamic shape (including rank), it produces a `Tensor` of scalar shape, but whose value is a 1-element vector.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe resulting `Tensor` should have a scalar value, consistent with its static shape.\r\n\r\n**Code to reproduce the issue**\r\n\r\nNote that the bug does not reproduce if the unknown-shape Tensor is passed directly to `reduce_mean`, which would suggest some interaction between the two.\r\n\r\n```\r\nimport numpy as np\r\n\r\ndef unknown_shape_constant(val):\r\n  # This is one way to create tensors of unknown shape.\r\n  return tf.py_function(lambda: np.array(val), (), tf.int32)\r\n\r\n@tf.function\r\ndef reduce_mean_bug():\r\n  arr = tf.TensorArray(tf.int32, size=1, dynamic_size=True)\r\n  arr = arr.write(0, unknown_shape_constant([[1], [3]]))\r\n  c = arr.concat()\r\n  m1 = tf.reduce_mean(c)\r\n\r\n  arr = tf.TensorArray(tf.int32, size=1, dynamic_size=True)\r\n  arr = arr.write(0, tf.constant([[1], [3]]))\r\n  c = arr.concat()\r\n  m2 = tf.reduce_mean(c)\r\n\r\n  return m1, m2\r\n\r\nm1, m2 = reduce_mean_bug()\r\nassert m1.shape == m2.shape  # Fails\r\n```\r\n", "comments": ["@jaingaurav could you triage the issue?", "@morgangiraud pointed out that the issue manifests for `tf.reduce_sum` as well, and possibly other `reduce_` ops.", "This issue is fixed in Tf-nightly ==2.2.0.dev20200312 version.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/f6bffcfcc013b2316bd29f3d6b2d2714/untitled453.ipynb). Can we close this issue. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30686\">No</a>\n"]}, {"number": 30685, "title": "`TensorArray` objects used as `Dataset.reduce` state lose inferred shapes", "body": "**System information**\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\n\r\n`TensorArray` objects passed as accumulators to `Dataset.reduce` lose inferred shapes. Subsequent calls to `TensorArray.concat` returns a fully unknown shape.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe element shape of the `TensorArray` should be partially known, consistent with the behavior of an equivalent `tf.while_loop`.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n@tf.function\r\ndef compute():\r\n    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)\r\n    def body(i, arr):\r\n        real_logits = tf.random.normal([5, 1])\r\n        arr = arr.write(tf.cast(i, tf.int32), real_logits)\r\n        i += 1\r\n        return i, arr\r\n    def cond(i, arr):\r\n      return i < 10\r\n    _, arr = tf.while_loop(cond, body, (0, arr))\r\n\r\n    c = arr.concat()\r\n    tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)\r\n    return c\r\n\r\n@tf.function\r\ndef compute_ds():\r\n    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)\r\n    def body(state, _):\r\n        i, arr = state\r\n        real_logits = tf.random.normal([5, 1])\r\n        arr = arr.write(tf.cast(i, tf.int32), real_logits)\r\n        i += 1\r\n        return i, arr\r\n    en_ds = tf.data.Dataset.range(10).enumerate()\r\n    _, arr = en_ds.reduce((0, arr), body)\r\n\r\n    c = arr.concat()\r\n    tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)\r\n    return c\r\n\r\nprint('*** With tf.while_loop')\r\n_ = compute()\r\nprint()\r\nprint('*** With tf.Dataset.reduce')\r\n_ = compute_ds()\r\n```\r\n```\r\n*** With tf.while_loop\r\nTensortArray.concat() shape: TensorShape([None, 1]) rank: 2\r\n\r\n*** With tf.Dataset.reduce\r\nTensortArray.concat() shape: TensorShape(None) rank: None\r\n```", "comments": ["@jsimsa could you triage the issue?", "@aaudiber could you please take a look?", "@mdanatg Thank you for reporting this and for including simple repro instructions!\r\n\r\nThe issue is with how we use `TensorArraySpec` to convert the `TensorArray` to/from `Tensor` components across the `reduce` boundary. As you noticed, it loses the shape information. I have a CL out to fix it, and will update once it is merged", "Now that 6cd69820a7ec68363647bf918d312b5d10e0e07a has merged, inferred `TensorArray` shapes will be preserved across all tf.data operations.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30685\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30685\">No</a>\n"]}, {"number": 30684, "title": "Compiling libtensorflow-core.a using NDK16b or later?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nSnapdragon 820 , Android\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\n1.13\r\n\r\n- Python version:\r\n3\r\n- Bazel version (if compiling from source):\r\nNA\r\n- GCC/Compiler version (if compiling from source):\r\n4.9\r\n- CUDA/cuDNN version:\r\nNA\r\n GPU model and memory:\r\nNA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nCompilation Error\r\nDue \r\n\r\ne/gen/obj/android_arm64-v8a/tensorflow/contrib/boosted_trees/proto/learner.pb.o\r\nIn file included from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:44:0,\r\n                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/postypes.h:40,\r\n                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/char_traits.h:40,\r\n                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/string:40,\r\n                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.h:7,\r\n                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.cc:4:\r\n/opt/google/android-ndk-r16b/sources/android/support/include/wchar.h:32:24: fatal error: wchar.h: No such file or directory\r\n #include_next <wchar.h>\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nCheckout r1.13 and export NDK_ROOT=/opt/google/r16b\r\n**Other info / logs**\r\nnuc2@nuc2-NUC7i5BNH:~/alok/tf-static-android/tensorflow$ export e/gen/obj/android_arm64-v8a/tensorflow/contrib/boosted_trees/proto/learner.pb.o\r\nIn file included from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:44:0,\r\n                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/postypes.h:40,\r\n                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/char_traits.h:40,\r\n                 from /opt/google/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/string:40,\r\n                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.h:7,\r\n                 from /home/nuc2/alok/tf-static-android/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/contrib/boosted_trees/proto/learner.pb.cc:4:\r\n/opt/google/android-ndk-r16b/sources/android/support/include/wchar.h:32:24: fatal error: wchar.h: No such file or directory\r\n #include_next <wchar.h>\r\n\r\n", "comments": ["TensorFlow Mobile (supported by contrib/makefile) has been deprecated, and we won't be retroactively patching fixes for 1.13. Is there a reason you cannot use TensorFlow Lite?", "Most of the OPS are not supported on Tensorflow Lite , sometimes development is much faster if there is full support of Tensorflow on Android as we can use existing model as it is. With Unified Header changed in NDK15 and above , Compilation fails . It would be nice if someone make changes in Makefile to support NDK 15 and later and any pointer how to do so.\r\n\r\nWIll try to make changes in Makefile and scripts as per \r\nhttps://android.googlesource.com/platform/ndk/+/ndk-release-r16/docs/UnifiedHeaders.md\r\nto support new NDK.", "Note that you can use TF operators within TFLite using [this approach](https://www.tensorflow.org/lite/guide/ops_select).", "@jdduke with that also with my model its throws error that enter and exit ops not supported"]}, {"number": 30683, "title": "Corrected rendering for docs in tf.Print", "body": "Corrected the rendering for tf.Print (deprecated) to display correctly on the URL: https://www.tensorflow.org/api_docs/python/tf/Print. Closes #29755 ", "comments": []}, {"number": 30682, "title": "TFLite speech example failing to train with -output_representation='spec'or'mfcc' examples/lite/examples/speech_commands/ml", "body": "My Environment \r\nWorking sample Link : https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/ml\r\nVirtual environment : Anaconda Navigator\r\nEditor : VS Code\r\nMode of execution : VS Code integrated Terminal with Conda envs\r\nOS : Mac OSX\r\n\r\nTensorflow Version : 1.13.1 (as required in the samples requirement)\r\n\r\nWhen i download the sample and run it as it is as per the ReadMe file, everything works perfectly fine. \r\nBut when i try to change the \"-output_representation\" parameter value to 'spec' or 'mfcc' it doesn't work. I get the error `ValueError: total size of new array must be unchanged` in the `model.py line no : 59 x = Reshape([800, 20])(x)`. After a quick traceback i found that the spectrogram fingerprint size is taken as 257x98 for every second. So i change that line to `x = Reshape([257, 98])(x)` and it successfully passed through this line. \r\nBut instead i get the following \r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1659, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 3 from 1 for 'conv1d_12/convolution/Conv2D' (op: 'Conv2D') with input shapes: [?,1,1,192], [1,3,192,256].\r\n```\r\nThis happens in the line no 99: `x = _reduce_conv(x, 256, 3)`.\r\n\r\nWhen i downloaded the ios example model and opened it in Netron i can clearly see that it uses audio spectrogram. \r\n\r\nWhat are all the changes that are to be done to train the model with 'spec', 'mfcc' and the 'mfcc_and_raw' ?", "comments": ["Hi guys any update on this.", "Hi, did you solve this problem?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 30681, "title": "Spellcheck @CONTRIBUTING.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30681) for more info**.\n\n<!-- need_sender_cla -->", "Signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30681) for more info**.\n\n<!-- ok -->", "\ud83d\udc4d Happy to help."]}, {"number": 30680, "title": "ReadTensorFromImageFile() C++ function too slow for large size images", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10\r\n- TensorFlow installed from (source or binary):from source\r\n- TensorFlow version (use command below):1.8.0\r\n- Python version:3.5\r\n-Microsoft Visual Studio 2017\r\n\r\n**Describe the current behavior**\r\nI am using tensoirflow object detection api in C++ , trained model with version 1.8.0, now i am running it on a laptop with no GPU and in Microsoft Visual studio 2017. The ReadTensorFromImageFile function takes alot of time and i would like it if it can work faster. i am loading 8K image and it takes around two seconds running through the ReadTensorFromImageFile() function. Is there a way to make it faster ??\r\n**Describe the expected behavior**\r\nI would like this function ReadTensorFromImageFile() to work faster for 8K size images.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "`Status ReadTensorFromImageFile(const string& file_name, const int input_height,\r\n\tconst int input_width, const float input_mean,\r\n\tconst float input_std,\r\n\tstd::vector<Tensor>* out_tensors) {\r\n\tauto root = tensorflow::Scope::NewRootScope();\r\n\tusing namespace ::tensorflow::ops;  // NOLINT(build/namespaces)\r\n\r\n\tstring input_name = \"file_reader\";\r\n\tstring output_name = \"normalized\";\r\n\r\n\t// read file_name into a tensor named input\r\n\tTensor input(tensorflow::DT_STRING, tensorflow::TensorShape());\r\n\tTF_RETURN_IF_ERROR(\r\n\t\tReadEntireFile(tensorflow::Env::Default(), file_name, &input));\r\n\r\n\t// use a placeholder to read input data\r\n\tauto file_reader =\r\n\t\tPlaceholder(root.WithOpName(\"input\"), tensorflow::DataType::DT_STRING);\r\n\r\n\tstd::vector<std::pair<string, tensorflow::Tensor>> inputs = {\r\n\t\t{\"input\", input},\r\n\t};\r\n\r\n\t// Now try to figure out what kind of file it is and decode it.\r\n\tconst int wanted_channels = 3;\r\n\ttensorflow::Output image_reader;\r\n\tif (tensorflow::StringPiece(file_name).ends_with(\".png\")) {\r\n\t\timage_reader = DecodePng(root.WithOpName(\"png_reader\"), file_reader,\r\n\t\t\tDecodePng::Channels(wanted_channels));\r\n\t}\r\n\telse if (tensorflow::StringPiece(file_name).ends_with(\".gif\")) {\r\n\t\t// gif decoder returns 4-D tensor, remove the first dim\r\n\t\timage_reader =\r\n\t\t\tSqueeze(root.WithOpName(\"squeeze_first_dim\"),\r\n\t\t\t\tDecodeGif(root.WithOpName(\"gif_reader\"), file_reader));\r\n\t}\r\n\telse {\r\n\t\t// Assume if it's neither a PNG nor a GIF then it must be a JPEG.\r\n\t\timage_reader = DecodeJpeg(root.WithOpName(\"jpeg_reader\"), file_reader,\r\n\t\t\tDecodeJpeg::Channels(wanted_channels));\r\n\t}\r\n\t// Now cast the image data to float so we can do normal math on it.\r\n\t// auto float_caster =\r\n\t//     Cast(root.WithOpName(\"float_caster\"), image_reader, tensorflow::DT_FLOAT);\r\n\r\n\tauto uint8_caster = Cast(root.WithOpName(\"uint8_caster\"), image_reader, tensorflow::DT_UINT8);\r\n\r\n\t// The convention for image ops in TensorFlow is that all images are expected\r\n\t// to be in batches, so that they're four-dimensional arrays with indices of\r\n\t// [batch, height, width, channel]. Because we only have a single image, we\r\n\t// have to add a batch dimension of 1 to the start with ExpandDims().\r\n\tauto dims_expander = ExpandDims(root.WithOpName(\"dim\"), uint8_caster, 0);\r\n\r\n\t// Bilinearly resize the image to fit the required dimensions.\r\n\t// auto resized = ResizeBilinear(\r\n\t//     root, dims_expander,\r\n\t//     Const(root.WithOpName(\"size\"), {input_height, input_width}));\r\n\r\n\r\n\t// Subtract the mean and divide by the scale.\r\n\t// auto div =  Div(root.WithOpName(output_name), Sub(root, dims_expander, {input_mean}),\r\n\t//     {input_std});\r\n\r\n\r\n\t//cast to int\r\n\t//auto uint8_caster =  Cast(root.WithOpName(\"uint8_caster\"), div, tensorflow::DT_UINT8);\r\n\r\n\t// This runs the GraphDef network definition that we've just constructed, and\r\n\t// returns the results in the output tensor.\r\n\ttensorflow::GraphDef graph;\r\n\tTF_RETURN_IF_ERROR(root.ToGraphDef(&graph));\r\n\r\n\tstd::unique_ptr<tensorflow::Session> session(\r\n\t\ttensorflow::NewSession(tensorflow::SessionOptions()));\r\n\tTF_RETURN_IF_ERROR(session->Create(graph));\r\n\tTF_RETURN_IF_ERROR(session->Run({ inputs }, { \"dim\" }, {}, out_tensors));\r\n\treturn Status::OK();\r\n}`\r\n\r\n@ravikyram I am using this function code in C++ to read tensor from image file. The problem i am facing is that i have the input image of size 8000x4000 which is too big size, converting this image to tensor takes almost 700 to 800 miliseconds which is too much for my application. Afterwards model takes 0.5 seconds for predictions which is fine, but loading and converting this image into tensor is taking alot of time. How do you suggest i solve this problem ?", "@Adnan-annan \r\nCan you please confirm whether ReadTensorFromImageFile you are using is a custom defined function or Tensorflow defined function.Thanks!", "@ravikyram I alos copied it from the github but it appears to be a custom code written by someone, the ReadTensorFromImageFile() function in tensorflow/tensorflow/examples/label_image/main.cc ia different. I used both of them and both of them take 600 to 700 miliseconds loading and converting the 8000x4000 pixels image into tensor. And the tensor is later fed to actually run the model. ", "@Adnan-annan Did you try latest TF versions (TF1.14, tf-nightly(TF1.15), TF2.0.0b1). There were lot of performance improvements implemented in latest versions. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30680\">No</a>\n"]}, {"number": 30679, "title": "can not convert model to tflite", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please post a new issue and provide all the information asked by the template. Thanks!"]}, {"number": 30678, "title": "i tried convert tf model to tflite but get this exception.Exception: Placeholder normalized_input_image_tensor should be specied by input_arrays.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Closing due to lack of recent activity.Thanks!"]}, {"number": 30677, "title": "Remove additional bazel flags in pip_smoke_test", "body": "`pip_smoke_test.py` only does `bazel cquery` so it doesn't need the build flags I added in the past few commits (#30651, #30583).\r\n\r\nThis should now fix sanity build.", "comments": []}, {"number": 30676, "title": "Exception: Placeholder normalized_input_image_tensor should be specied by input_arrays", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please post a new issue and provide all the information asked by the template. Thanks!"]}, {"number": 30675, "title": "Hi guys.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please post a new issue and provide all the information asked by the template. Thanks!"]}, {"number": 30674, "title": "Failed to build TFLite model benchmark tool on windows due to include 'dirent.h'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016 x64\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:master \r\n- Bazel version (if compiling from source):0.25\r\n- GCC/Compiler version (if compiling from source):MSVC 14\r\n- CUDA/cuDNN version:not used\r\n\r\n**Describe the problem**\r\n\r\nFailed to build [TFLite Model Benchmark Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) with MSVC 14 on windows.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI prepared [the CI environment with Azure DevOps](https://dev.azure.com/mlops/tflite/) to reproduce this error.\r\nI utilized the following script to build TFLite model benchmark tool on windows.\r\n\r\n```yaml\r\nvariables:\r\n  BAZEL_VERSION: \"0.25.2\"\r\n  TF_REPOSITORY: https://github.com/tensorflow/tensorflow.git\r\n  TF_NEED_JEMALLOC: 1\r\n  TF_NEED_GCP: 0\r\n  TF_NEED_HDFS: 0\r\n  TF_NEED_AWS: 0\r\n  TF_NEED_KAFKA: 0\r\n  TF_ENABLE_XLA: 0\r\n  TF_NEED_GDR: 0\r\n  TF_NEED_VERBS: 0\r\n  TF_NEED_OPENCL_SYCL: 0\r\n  TF_NEED_OPENCL: 0\r\n  TF_NEED_CUDA: 0\r\n  TF_CUDA_CLANG: 0\r\n  TF_NEED_ROCM: 0\r\n  TF_NEED_MKL: 0\r\n  TF_DOWNLOAD_MKL: 0\r\n  TF_DOWNLOAD_CLANG: 0\r\n  TF_NEED_MPI: 0\r\n  TF_NEED_S3: 0\r\n  TF_SET_ANDROID_WORKSPACE: 0\r\n  TF_NEED_COMPUTECPP: 0\r\n  TF_NEED_TENSORRT: 0\r\n  TF_CONFIGURE_IOS: 0\r\n  - job: build_tflite_win\r\n    pool:\r\n      vmImage: 'vs2017-win2016'\r\n    timeoutInMinutes: 360\r\n    variables:\r\n      BAZEL_SH: \"C:\\\\tools\\\\msys64\\\\usr\\\\bin\\\\bash.exe\"\r\n      BAZEL_VC: \"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\"\r\n      CC_OPT_FLAGS: \"/arch:AVX /fp:fast\"\r\n    steps:\r\n    - task: UsePythonVersion@0\r\n      inputs:\r\n        versionSpec: '3.x' \r\n        addToPath: true   \r\n    - powershell: |\r\n        cinst msys2 --params \"/NoUpdate\" -y --no-progress\r\n        cinst bazel -Version $env:BAZEL_VERSION -y --no-progress -i\r\n        C:\\tools\\msys64\\usr\\bin\\pacman -S --noconfirm patch unzip\r\n        git clone --branch=$(Build.SourceBranchName) --no-progress --depth=1 $env:TF_REPOSITORY\r\n        $env:PYTHON_BIN_PATH=(Get-Command python).Source\r\n        $env:PYTHON_LIB_PATH=\"$(python -c 'import site; print(site.getsitepackages()[0])')\"\r\n        python ./tensorflow/configure.py\r\n      displayName: 'Install'\r\n    - powershell: |\r\n        cd ./tensorflow\r\n        Set-Item Env:Path \"C:\\\\tools\\\\msys64\\\\usr\\\\bin\\\\;$Env:Path\"\r\n        bazel --output_base $(Build.BinariesDirectory) build -c opt --color=yes --verbose_failures \r\n //tensorflow/lite/tools/benchmark:benchmark_model\r\n      condition: succeededOrFailed()\r\n      displayName: 'Build TFLite Model Benchmark Tool'\r\n    - task: PublishPipelineArtifact@0\r\n      inputs:\r\n        artifactName: tflite_benchmark_win\r\n        targetPath: $(Build.SourcesDirectory)/tensorflow/bazel-bin/tensorflow/lite/tools/benchmark/\r\n      condition: succeeded()\r\n      displayName: 'Publish TFLite Model Benchmark Tool'\r\n\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAll build logs is [uploaded here](https://dev.azure.com/mlops/tflite/_build/results?buildId=147&view=logs&j=e30d995d-3917-5417-8fca-0e6fe874aac5).\r\nThis error seems to be caused by 'dirent.h' as follows.\r\n\r\n```\r\n\u00a0 | Execution platform: @bazel_tools//platforms:host_platform | \u00a0\r\n\u00a0 | tensorflow/lite/tools/evaluation/utils.cc(18): fatal error C1083: Cannot open include file: 'dirent.h': No such file or directory | \u00a0\r\n\u00a0 | Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build | \u00a0\r\n\u00a0 | INFO: Elapsed time: 4.617s, Critical Path: 3.72s | \u00a0\r\n\u00a0 | INFO: 1 process: 1 local. | \u00a0\r\n\u00a0 | FAILED: Build did NOT complete successfully\r\n```\r\n\r\nThe POSIX header **'dirent.h' is not part of the C standard and is not portable between platforms.**\r\nWould you like to modify the inclusion?", "comments": ["@stakemura Please check the below link if it helps you ?Thanks!\r\n\r\nhttps://stackoverflow.com/questions/5530933/dirent-h-in-visual-studio-2010-or-2008", "@ravikyram I think it should be able to build without any modification.\r\nIn fact, [tensorflow/stream_executor/cuda/cuda_diagnostics.cc](https://github.com/tensorflow/tensorflow/blob/4fcf0090a739951d52d30cd91deee4cf60ce3a8b/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L18) commented out dirent.h inclusion and the platform-dependent code as following code. I guess [GetSortedFileNames](https://github.com/tensorflow/tensorflow/blob/765bcd8432522397e702d6f9b697745cac0f5da0/tensorflow/lite/tools/evaluation/utils.cc#L55) is not necessary to build most TFLite tools.\r\n\r\n```cpp\r\n#if !defined(PLATFORM_WINDOWS)\r\n#include <dirent.h>\r\n#endif\r\n```", "Fix was merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30674\">No</a>\n"]}, {"number": 30673, "title": "Support Conv3D in profiler for FLOPS counting", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe [TensorFlow Profiler](https://www.tensorflow.org/api_docs/python/tf/profiler/profile) currently does not support counting operations / FLOPS of Conv3D layers. I request that we add this feature. This [nice readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#profile-model-float-operations) describes how to add this, if I am not mistaken.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone that uses 3D convolutions and wants to profile the compute complexity of a model.\r\n\r\n**Any Other info.**\r\nCode to quickly see that Conv3D operations are indeed not supported:\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    inputs = tf.ones(shape=(1, 5, 32, 32, 3))\r\n    tf.layers.conv3d(\r\n        inputs,\r\n        filters=6,\r\n        kernel_size=(3, 3, 3),\r\n        strides=(1, 1, 1),\r\n        padding=\"valid\",\r\n        use_bias=True,\r\n    )\r\n\r\nprofiling_options = tf.profiler.ProfileOptionBuilder.float_operation()\r\ntf_profile = tf.profiler.profile(graph, options=profiling_options)\r\n```\r\noutput\r\n```\r\nProfile:\r\nnode name | # float_ops\r\n_TFProfRoot (--/17.17k flops)\r\n  conv3d/BiasAdd (16.20k/16.20k flops)\r\n  conv3d/kernel/Initializer/random_uniform (486/973 flops)\r\n    conv3d/kernel/Initializer/random_uniform/mul (486/486 flops)\r\n    conv3d/kernel/Initializer/random_uniform/sub (1/1 flops)\r\n```", "comments": ["Hi Patz, this is not mine. Maybe try Tensorboard folks or xprof team?", "Sorry, I don't quite know what / who the xprof team is and how to reach them. Also, I doubt that this is TensorBoard specific or implemented there \ud83e\udd14 .", "hi, Martin:\r\n  I am from the profiler team. I am glad to review your patch if you already have it. otherwise, I will try to figure out how to implemented this correctly.\r\nBest\r\nJie", "Hi Jie / @trisolaran ,\r\nno I don't have a patch yet. I would be happy to work on it. But a small kick-start would be nice. E.g. the files to start looking. I also found that `depthwise_conv2d`, and also `separable_conv2d` are buggy / incomplete. Didn't file issue for those yet though.", "I was not so sure  about the  nature of this issue but in\r\ntensorflow/python/ops/nn_ops.py have following code:\r\n@ops.RegisterStatistics(\"Conv3D\", \"flops\")\r\ndef _calc_conv3d_flops(graph, node):\r\n\r\nI would start from there to figure out why it is not doing what you had expected :)", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "sorry guys, I sadly don't have time to work on this. I will close it for now until I either find time to revisit or if it occurs again."]}, {"number": 30672, "title": "Backpropagate through tf.data", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): n/a\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nGradients are not propagated through tf.data\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThis code snippet returns [None]:\r\n\r\nimport tensorflow as tf\r\n_ = tf.layers.conv1d(tf.ones((1,1,2)), 1, 1, use_bias=False, name='name')\r\ndata = tf.data.Dataset.from_tensors(tf.ones((10,2))).repeat().batch(4)\\\r\n    .map(lambda x: tf.layers.conv1d(x, 1, 1, use_bias=False, name='name', reuse=True))\r\ni = data.make_initializable_iterator()\r\nx = i.get_next()\r\ng = tf.gradients(x, tf.trainable_variables())\r\nprint (g)\r\n\r\n>>> [None]\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Silly question, but did you done with Session().as_default as sess:  ->  sess.run(g)  ?", "Yes... it returns TypeError: Fetch argument None has invalid type <class 'NoneType'>\r\n", "Try to retrieve the operation tensor output(if there is any) with tensor_name = g.get_operations()[-1].name:':0', also explore your graph with tensorboard to see what is happening. \r\nthen run it... good luck (iam still to explore this module cant provide you more info)", "This is not planned to be supported by the tf.data team in the near future. I will mark this issue as \"contributions welcome\".", "@jsimsa \r\nI would like to work on this if nobody is working on this.\r\nIs there any pointers you have that will be a good initial point for me before starting?", "@SSaishruthi my suggestion would be to study the implementation of tf.gradients and extend it to tf.data. I would be happy to answer any tf.data related questions, but I am not an expert on tf.gradients.", "Thanks @jsimsa \r\n\r\nI will start working on this", "I've been trying to implement feature map augmentation in TF2 and ran into this issue. If I augment feature maps in the middle of the network using `tf.data.`, the gradients are lost.\r\n\r\nIt seems like the best workaround is to apply operations to a batch of feature maps using Python's multiprocessing module. Has anyone tried something like this?", "@ufimtsev  \r\nIs this still an issue.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 30671, "title": "add mismatch to load weight", "body": " This PR adds the shape mismatch option to tf.keras load_weight function which ignores weights that are different with graph. This function is originally from: https://github.com/keras-team/keras/blob/ed07472bc5fc985982db355135d37059a1f887a9/keras/engine/saving.py#L1202", "comments": ["@cylee81 Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "Can one of the admins verify this patch?", "Hi! I am not quite sure where the saving_test.py located.", "anything under [this](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/saving) is fine.", "Hi I already finished the testing function (under tensorflow/python/keras/saving/save_test.py). Please check it. Thank you!", "@cylee81 Could you please resolve the conflicts? Thanks!", "Hi @gbaned @tanzhenyu , I fixed it. Please check it. Thanks.", "Hi @gbaned @tanzhenyu , I fixed the pylint issue. Please check. Thank you.", "@cylee81 Could you please resolve the conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I believe this was made redundant by c52ee53#diff-4ee308ea180d49ae81691348531a2b6d . Closing this."]}, {"number": 30670, "title": "libcuda.so.1: cannot open shared object file: no such file or directory", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow version: 1.13\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- CUDA/cuDNN version: cudatoolkit version 10 and cudnn version 7.6 (according to conda list)\r\n- GPU model and memory: 2080ti 11gb \ud83e\udd47 \r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI create a new conda environment and run conda install tensorflow-gpu. Next I start up a python terminal and import tensorflow as tf. My error message received is in next section.\r\n\r\n**Any other info / logs**\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"/home/ian/anaconda3/envs/tf_gpu/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n", "comments": ["@iantimmis Just to verify, Did you install CUDA toolkit in the same environment? Thanks!", "I think so. I'm under the impression that conda handles that for you. When I run \"conda list\", I see that cudatoolkit version 10 is present. ", "@iantimmis Looks like similar issue, please take a look at [#26209](https://github.com/tensorflow/tensorflow/issues/26209). Let us know if that helps. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "The isssue is still exisiting", "The issue is still exisiting", "Yes, same issue with same configuration", "The issue is still existing", "Just ran into it myself"]}, {"number": 30669, "title": "[INTEL MKL] Fix to the unit test \"print_selective_registration_header_test\"", "body": "Removed the check and code to append prefix 'Mkl' if mkl is enabled in the print_selective_registration_header_test unit test, as it was causing a failure with new changes wherein all operators get re-written into MKL versions including matmul. The 'if' check is no longer needed in the test.", "comments": ["@LakshayT Can you please check Ubuntu Sanity errors? Thanks!", "@gbaned  I have made a change to fix the ubuntu sanity check issue."]}, {"number": 30668, "title": "[INTEL MKL] Reverting a change from an earlier commit.", "body": "Fixes regressions caused by this commit https://github.com/tensorflow/tensorflow/commit/844aa275fd31fb6f3ac5c607d4ce9b143143d55a  Removed TF_CHECK_OK , since GetAttr can legally return non-ok here. ", "comments": ["pinging @penpornk  for review.", "@penpornk  thanks, I will add to our backlog."]}]