[{"number": 23635, "title": "TensorFlow Keras Fit Bug Value Error: Dimension -3 must be >= 0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI have written custom code, albeit based off of code from TFLearn.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nThis issue is untested on mobile devices.\r\n- TensorFlow installed from (source or binary):\r\nTensorFlow installed from binary.\r\n- TensorFlow version (use command below):\r\nTensorFlow Version: 1.11 \r\n- Python version:\r\nPython Version: 3.6.6\r\n- Bazel version (if compiling from source):\r\nNot Compiled from source\r\n- GCC/Compiler version (if compiling from source):\r\nNot Compiled from source\r\n- CUDA/cuDNN version:\r\nCUDA/cuDNN version 9.0\r\n- GPU model and memory:\r\nNVIDIA GeForce MX150, approx total memory 10097 MB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen I run the following program, which is supposed to be able to differentiate between images of Cats and Dogs using a convolution model, the program gets stuck at the fit method. It is returning a Value Error: Dimension -3 must be >= 0. I believe all the information in the fit method is correct...\r\n\r\n**Describe the expected behavior**\r\nIt should complete the training of the model. It has not yet been set up to save it.\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nFile is attached.\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere is the trace:\r\n\r\n2018-11-09 11:45:48.096411: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nTraceback (most recent call last):\r\n  File \"C:/Users/gman7/Documents/BIF/Tensor Flow/Programs/Java Examples/Cats vs Dogs/CvD.py\", line 128, in <module>\r\n    model.fit(X_train, Y_train, epochs=2, steps_per_epoch=500)\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1509, in fit\r\n    validation_split=validation_split)\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 993, in _standardize_user_data\r\n    class_weight, batch_size)\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1029, in _standardize_weights\r\n    self._set_inputs(x)\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 426, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1222, in _set_inputs\r\n    self.build(input_shape=input_shape)\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 224, in build\r\n    shape = layer.compute_output_shape(shape)\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 230, in compute_output_shape\r\n    [self.filters])\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 542, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 542, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 482, in as_dimension\r\n    return Dimension(value)\r\n  File \"C:\\Users\\gman7\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 42, in __init__\r\n    raise ValueError(\"Dimension %d must be >= 0\" % self._value)\r\nValueError: Dimension -3 must be >= 0\r\n\r\nProcess finished with exit code 1\r\n", "comments": ["\r\n[CvD.zip](https://github.com/tensorflow/tensorflow/files/2567000/CvD.zip)\r\n", "bump", "Hi @gmanrocks99 can you provide a minimal example reproducing the issue and copy-paste the code as a comment on this thread?", "import numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ntrain_data = np.load('train_data.npy')\r\ntest_data = np.load('test_data.npy')\r\n\r\ntrain = train_data[:-500]\r\ntest = train_data[-500:]\r\n\r\nX_train = np.array([i[0] for i in train]).reshape(-1, 50, 50, 1)\r\nY_train = np.array([i[1] for i in train])\r\n\r\nX_test = np.array([i[0] for i in test]).reshape(-1, 50, 50, 1)\r\nY_test = np.array([i[1] for i in test])\r\n\r\ntf.reset_default_graph()\r\nsess = tf.Session()\r\n\r\nmodel = Sequential()\r\n\r\n# layer1 = conv_2d, filters = 32, filter size = 5, activation = relu\r\nmodel.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', ))\r\n\r\n# layer2 = max_pool_2d, kernels = 5\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\n# layer3 = conv_2d, filters = 64, filter size = 5, activation = relu\r\nmodel.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\r\n\r\n# layer4 = max_pool_2d, kernels = 5\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\n# layer5 = conv_2d, filters = 128, filter size = 5, activation = relu\r\nmodel.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))\r\n\r\n# layer6 = max_pool_2d, kernels = 5\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\n# layer7 = conv_2d, filters = 64, filter size = 5, activation = relu\r\nmodel.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\r\n\r\n# layer8 = max_pool_2d, kernels = 5\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\n# layer9 = conv_2d, filters = 32, filter size = 5, activation = relu\r\nmodel.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\r\n\r\n# layer10 = max_pool_2d, kernels = 5\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\n# layer11 = fully_connected, units = 1024, activation = relu\r\nmodel.add(Dense(units=1024, activation='relu'))\r\n\r\n# layer12 = dropout, keep_probability = 0.8\r\nmodel.add(Dropout(rate=0.8))\r\n\r\n# layer13 = fully_connected, units = 2, activation = softmax\r\nmodel.add(Dense(units=2, activation='softmax'))\r\n\r\n# layer14 = regression, optimizer = adam, learning rate = LR, loss = categorical cross entropy\r\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\n# epochs = 1, validation set = (X_test, Y_test), snapshot every 500, progressbar\r\nmodel.fit(x=X_train, y=Y_train, epochs=1, validation_data=(X_test, Y_test), verbose=1)", "@omalleyt12 ", "Correction:\r\nall\r\nmodel.add(MaxPool2D(pool_size=(5, 5), strides=5))\r\nreplaced with\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))", "This works:\r\nmodel.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu', ))\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\nmodel.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', ))\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\nmodel.add(Dense(units=1024, activation='relu'))\r\nmodel.add(Dropout(rate=0.8))\r\nmodel.add(Dense(units=2, activation='softmax'))\r\n\r\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy', 'ce'])\r\nsess.run(tf.global_variables_initializer())\r\nmodel.fit(x=X_train, y=Y_train, epochs=1, validation_data=(X_test, Y_test), verbose=1)\r\n\r\nThis does not:\r\n\r\nmodel.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', ))\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\nmodel.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu', ))\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\nmodel.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', ))\r\nmodel.add(MaxPool2D(pool_size=(5, 5)))\r\n\r\nmodel.add(Dense(units=1024, activation='relu'))\r\nmodel.add(Dropout(rate=0.8))\r\nmodel.add(Dense(units=2, activation='softmax'))\r\n\r\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy', 'ce'])\r\nsess.run(tf.global_variables_initializer())\r\nmodel.fit(x=X_train, y=Y_train, epochs=1, validation_data=(X_test, Y_test), verbose=1)\r\n\r\nThe latter, i added one more conv2d and one more maxpool2D which does not work, no matter the number of filters.", "Problem solved: in tflearn padding is by default = 'same' while in keras, padding = 'valid' by default."]}, {"number": 23634, "title": "Incorrect loss calculation after v1.10: fitting tf.Dataset in Keras.", "body": "In 1.12, we can pass `tf.Dataset` into our Keras models. For instance, I'm modelling a binary classification with high class imbalance. \r\n\r\n```\r\ndef _parse_func(record):\r\n    keys_to_features = {} # some schema\r\n\r\n    parsed = tf.parse_single_example(record, keys_to_features)\r\n\r\n    # Build the X output\r\n    x = parsed['x']  \r\n    y = parsed['y']\r\n    w = parsed['w'] * DEBUG_WEIGHT\r\n\r\n    return x, y, w\r\n```\r\nwhere `DEBUG_WEIGHT` is changed from 1 to 10000, with no affect on the loss.\r\nCreate `tf.Dataset`\r\n\r\n```\r\ndef input_fn(f, b=BATCH_SIZE):\r\n    \"\"\"\r\n    :param f: A list of file names e.g. ['data-0.tf-record', 'data-1.tf-record'] to read\r\n    :param b: Batch size, defaults to BATCH_SIZE in hparams.py\r\n    :return: And infinitely iterable data set using tf records of tf.data.Dataset class\r\n    \"\"\"\r\n    d = tf.data.TFRecordDataset(filenames=f)\r\n    d = d.map(map_func=_parse_func)\r\n    d = d.repeat()\r\n    d = d.batch(batch_size=b)\r\n    return d\r\n```\r\n\r\nBut when I fit the model using Keras, no matter what weight is passed, it seems ignored.\r\n\r\nE.g.\r\n\r\n```\r\n    train_data = input_fn(f=['path_to_tf_records'])\r\n\r\n    params = {\r\n        'loss': 'binary_crossentropy',\r\n        'optimizer': optimizer,\r\n        'metrics': metrics,\r\n    }\r\n    model.compile(**params)\r\n\r\n    model.fit(x=train_data.make_one_shot_iterator(),\r\n              epochs=50,\r\n              steps_per_epoch=10000)\r\n```\r\nInterestingly, if I pass `class_weight` to the `fit`, then I get the message that both `sample_weights` and `class_weight` are provided. So why doesn't the loss change?\r\n\r\nThe loss is incredibly small and it doesn't change with weight.\r\n\r\nEpoch 1/30\r\n2018-11-09 16:24:10.605651: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n 461/9375 [>.............................] - ETA: 2:01:50 - loss: 53.4499 - binary_accuracy: 0.8183 - binary_crossentropy: 0.4290 - mean_absolute_error: 0.3171\r\n\r\n\r\n## Sidenote\r\nWhen I used numpy arrays with `tf.keras.utils.Sequence` class as a generator, using `fit_generator`, the loss was also not changing properly. \r\n\r\n\r\n### Takeaways\r\n\r\nClearly, the weight is not passed to the loss function correctly. Even if I use `fit_generator()` the loss differs in tensor flow > 1.10. Additionally, workers must be set to 0 i.e. `workers=0` for the generator to work which is incredibly slow during training.", "comments": []}, {"number": 23633, "title": "Distributed training with TF under BSP does not strictly follow the synchrnous rule", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no experiments on mobile devices\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): r1.10\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): GCC\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: ??\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nThe distributed training is conducted with one PS and three workers. The replica_sync is True.  I deliberately slow down one worker (add some sleep in the program of the worker), then I find the other workers can surpass the slow worker by 1~3 iterations \r\n**Describe the expected behavior**\r\nSince TF follows BSP (it does not support SSP, right?), these workers should be always  under the same iteration.  When slow worker still stays in Iteration 1, the other workers cannot go to Iteration 3 or further. \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nAny Distributed training program should be okay. Just add some sleep in the python code or the rdma.cc file\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Somebody seems to encounter similar problems, but no one gives an answer. \r\nhttps://stackoverflow.com/questions/52216458/running-distributed-tensorflow-in-synchronous-sgdbsp\r\n", "Distributed synchronous training now falls under the Distribution Strategy remit, so reassigning this to @josh11b for triage.", "I am unfamiliar with what \"replica_sync\" option you are referring to. Normally when you use parameter servers, your workers will operate asynchronously, with a distribution strategy or without. I'm also unfamiliar with what you are referring to when you say \"BSP\" or \"SSP\".", "@Steamgjk Would you mind giving us more info? Closing this for now. Feel free to reopen."]}, {"number": 23632, "title": "Incorrect masking in keras.backend.rnn", "body": "**System information**\r\n```\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.12.0-rc0-2215-g6735dd4799', '1.13.0-dev20181109')\r\n```\r\n\r\n**Describe the current behavior**\r\nFor correct masking of `output` it is required that `output == states[0]` for  `keras.backend.rnn(..., unroll=False)`, not so with `unroll=True` or in latest version of [keras-team/keras](https://github.com/keras-team/keras). See [this line](https://github.com/tensorflow/tensorflow/blob/d703f64e452f49603a244978b13239ab7aac0168/tensorflow/python/keras/backend.py#L3496) for the reason.\r\n\r\n**Describe the expected behavior**\r\nAs per this [PR](https://github.com/keras-team/keras/pull/11499) (and discussions in linked issues) it has been established that output should be independent of states. That is, it should _not_ be assumed that `output == states[0]` for `ouput, states = step_function(inputs, previous_states)` in the `keras.backend.rnn` implementation. For details see test below.\r\n\r\n**Code to reproduce the issue**\r\nSee test below (basically same as [this test added keras-team/keras](https://github.com/keras-team/keras/pull/11499/files#diff-e942014d73bf67b47a3b55f7f7041797R829))\r\n\r\n```python\r\nimport numpy as np\r\nfrom tensorflow import keras\r\n\r\ndef test_rnn_output_and_state_masking_independent():\r\n    num_samples = 2\r\n    num_timesteps = 4\r\n    state_and_io_size = 5\r\n    mask_last_num_timesteps = 2  # for second sample only\r\n\r\n    # a step function that just outputs inputs,\r\n    # but increments states +1 per timestep\r\n    def step_function(inputs, states):\r\n        return inputs, [s + 1 for s in states]\r\n\r\n    inputs_vals = np.random.random(\r\n        (num_samples, num_timesteps, state_and_io_size))\r\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\r\n    # masking of two last timesteps for second sample only\r\n    mask_vals = np.ones((num_samples, num_timesteps))\r\n    mask_vals[1, -mask_last_num_timesteps:] = 0\r\n\r\n    # outputs expected to be same as inputs for the first sample\r\n    expected_outputs = inputs_vals.copy()\r\n    # but for the second sample all outputs in masked region should be the same\r\n    # as last output before masked region\r\n    expected_outputs[1, -mask_last_num_timesteps:] = \\\r\n        expected_outputs[1, -(mask_last_num_timesteps + 1)]\r\n\r\n    expected_state = initial_state_vals.copy()\r\n    # first state should be incremented for every timestep (no masking)\r\n    expected_state[0] += num_timesteps\r\n    # second state should not be incremented for last two timesteps\r\n    expected_state[1] += (num_timesteps - mask_last_num_timesteps)\r\n\r\n    # verify same expected output for `unroll=true/false`\r\n    inputs = keras.backend.variable(inputs_vals)\r\n    initial_states = [keras.backend.variable(initial_state_vals)]\r\n    mask = keras.backend.variable(mask_vals)\r\n    for unroll in [True, False]:\r\n        last_output, outputs, last_states = keras.backend.rnn(\r\n            step_function,\r\n            inputs,\r\n            initial_states,\r\n            mask=mask,\r\n            unroll=unroll,\r\n            input_length=num_timesteps if unroll else None)\r\n\r\n        np.testing.assert_allclose(\r\n            keras.backend.eval(outputs), expected_outputs,\r\n            err_msg=\"Unexpected output for unroll={}\".format(unroll))\r\n        np.testing.assert_allclose(\r\n            keras.backend.eval(last_states[0]), expected_state,\r\n            err_msg=\"Unexpected state for unroll={}\".format(unroll))\r\n```\r\nGives:\r\n```\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=0\r\nUnexpected output for unroll=False\r\n(mismatch 25.0%)\r\n x: array([0.116687, 0.622734, 0.210443, 0.662715, 0.720813, 0.654062,\r\n       0.936728, 0.451018, 0.471044, 0.560336, 0.133492, 0.228378,\r\n       2.081284, 2.415464, 2.081284, 2.415464], dtype=float32)\r\n y: array([0.116687, 0.622734, 0.210443, 0.662715, 0.720813, 0.654062,\r\n       0.936728, 0.451018, 0.471044, 0.560336, 0.133492, 0.228378,\r\n       0.133492, 0.228378, 0.133492, 0.228378])\r\n```\r\n**Other info / logs**\r\nThere are a few further implications of current implementation addressed by the tests added to keras-team/keras [here](https://github.com/keras-team/keras/pull/11499/files#diff-e942014d73bf67b47a3b55f7f7041797R829))", "comments": ["I'm happy to adopt the fixes I made in keras-team/keras to here @fchollet ", "Hi @qlzh727 I see that a fix for this was added in https://github.com/tensorflow/tensorflow/commit/149886979acf87027a188c0851f0cbe41a30f275 So this should be closed?\r\n\r\nCould you put me in the loop on the process of bringing features and fixes from keras-team/keras to tensorflow.keras? Do you want contributions for this? I think that there are still issues in current `rnn` implementation here (as in it will not pass all the tests added [here](https://github.com/keras-team/keras/pull/11499))", "Hi @andhus, thanks for the notice. I think this should be closed since I applied same fix to the tf.keras backend.\r\n\r\nCurrently there is manual sync process to bring fix/updates from keras/keras to tf.keras, usually this happens every few weeks. In this case, since I was aware of the issue on keras side, I just directly applied the same fix on tf.keras, which avoid the long waiting for the sync process.\r\n\r\nI will take a close look since u mentioned it might not passing all the tests u have in other PR, will update this issue soon.", "@qlzh727 Made a PR including the tests and a (tiny) fix for the remaining issue I had noticed.", "The PR has been merged, mark this as fixed. Thanks @andhus  for the fix."]}, {"number": 23631, "title": "Default argument of tf.placeholder", "body": "InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x' with dtype float\r\n\t [[{{node x}} = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nI think  tf.placeholder(tf.float32, shape=None, name='x') is equivalent to  tf.placeholder(tf.float32,  name='x')? But the results are different for these two lines of code.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution Windows10:\r\n- TensorFlow installed from pip:\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 1080Ti\r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nx = tf.placeholder(tf.float32, shape=None, name='x')\r\ny = tf.placeholder(tf.float32, shape=None, name='y')\r\nwith tf.Session() as sess:\r\n    total_loss = 0\r\n    sess.run(tf.global_variables_initializer())\r\n    _, loss_ = sess.run([optimizer, loss], feed_dict={x: 1.0, y: 2.0})\r\n````\r\nThe code presented above returned InvalidArgumentError.\r\n\r\n```python\r\nx = tf.placeholder(tf.float32, name='x')\r\ny = tf.placeholder(tf.float32, name='y')\r\nwith tf.Session() as sess:\r\n    total_loss = 0\r\n    sess.run(tf.global_variables_initializer())\r\n    _, loss_ = sess.run([optimizer, loss], feed_dict={x: 1.0, y: 2.0})\r\n````\r\nThese code works well.\r\n\r\n", "comments": ["@MasKong  Could you please provide the full code so that we try to reproduce the error at our end and look into the issue.", "@harshini-gadige sorry for late reply. I am sorry that I cannot provide the code because it is in my another device. I was trying to do a linear regression. The first snippet of code did not work well but the sceond snippet of code worked well. The only difference is that shape argument.", "Hi @MasKong! We are checking to see if you still need help in this issue.\r\nIt  also seems you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23631\">No</a>\n"]}, {"number": 23630, "title": "The output of trained model is constant when using batch normalization", "body": "I have trained a model with tensorflow, with the use of batch normalization.\r\n\r\n```\r\nconv2 = slim.conv2d(\r\ninputs=pool1,\r\nnum_outputs=64,\r\nkernel_size=[5, 5],\r\npadding=\"same\",\r\nnormalizer_fn=slim.batch_norm,\r\nactivation_fn=tf.nn.relu)\r\n```\r\nAnd the model has been trained well, I can see the loss curve has been falling in tensorboard, and the validation accuracy reached 99%+\r\n\r\nThen I saved my model as ckpt files. But when I tried to load my model from ckpt files and do some prediction, what ever I feed to the model, the output is always an constant array.\r\n\r\nThen I removed all bn layer, and train my model and save it as ckpt again.When I load it and do prediction, it works well.\r\n\r\nCan anybody tell me why?\r\n", "comments": []}, {"number": 23629, "title": "Cannot compile tensorflow lite minimal.cc with libtensorflowlite.so", "body": "**System information**\r\nUbuntu 16, armeabi-v7a, bazel 0.18, tried out with NDK 16b and 15c\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run this command to compile `tensorflow/lite/examples/minimal.cc` using a `libtensorflowlite.so` I built:\r\n\r\n`<path-to>/android-toolchain-15c/bin/clang++  -std=c++11 -I<path-to>/git/tensorflow-android\r\n-I<path-to>/git/flatbuffers/include -L<path-to>/git/tensorflow-android\r\n-L<path-to>/git/flatbuffers/build minimal.cc -ltensorflowlite -lflatbuffers`\r\n\r\nI get an error\r\n\r\n`undefined reference to 'tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter> >*)'`\r\n\r\n**Describe the expected behavior**\r\n\r\nTo compile.\r\n\r\n**Code to reproduce the issue**\r\n\r\n1. Download the android ndk. Call `build/tools/make_standalone_toolchain.py --arch arm --api 21 --stl=libc++ --install-dir android-toolchain`\r\n\r\n2. Build `libtensorflowlite.so`, I added this to the BUILD file:\r\n\r\n```\r\ncc_binary(\r\n    name = \"libtensorflowlite.so\",\r\n    linkopts=[\r\n        \"-shared\",\r\n        \"-Wl,-soname=libtensorflowlite.so\",\r\n    ],  \r\n    linkshared = 1,\r\n    copts = tflite_copts(),\r\n    deps = [ \r\n        \":framework\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n    ],  \r\n)\r\n```\r\nAnd then called \r\n```\r\nbazel build //tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"\r\n```\r\n\r\n3. Clone and compile the flatbuffer repo.\r\n\r\n4. Then call the problematic command \r\n```\r\nandroid-toolchain/bin/clang++  -std=c++11 -Igit/tensorflow-android -Igit/flatbuffers/include -Lgit/tensorflow-android -Lgit/flatbuffers/build minimal.cc -ltensorflowlite -lflatbuffers\r\n```\r\nAnd you should get the error: \r\n```undefined reference to 'tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter> >*)'```\r\n\r\n", "comments": ["I needed to be using NDK 18. I didn't think of using it previously because during `./configure` a warning was output saying I should only <=16 was supported."]}, {"number": 23628, "title": "Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model", "body": "When I put the tflite file trained by the ssdmobilenet model into Android studio, the following error occurred. Who can help me? Below is my configuration information and wrong information.\r\n![image](https://user-images.githubusercontent.com/20412297/48258352-d7456a00-e44f-11e8-8f24-4de8aa60ebf0.png)\r\n\r\n![image](https://user-images.githubusercontent.com/20412297/48258389-f3490b80-e44f-11e8-8ea2-4bbf099763e7.png)\r\n \r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.lite.demo, PID: 20339\r\n    java.lang.RuntimeException: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\r\n        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:124)\r\n        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:110)\r\n        at org.tensorflow.demo.CameraActivity$5.onPreviewSizeChosen(CameraActivity.java:362)\r\n        at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:401)\r\n        at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:408)\r\n        at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)\r\n        at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)\r\n        at android.view.TextureView.getHardwareLayer(TextureView.java:390)\r\n        at android.view.TextureView.draw(TextureView.java:339)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:18168)\r\n        at android.view.View.draw(View.java:18946)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:18159)\r\n        at android.view.View.draw(View.java:18946)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)\r\n        at android.view.View.draw(View.java:19221)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:18168)\r\n        at android.view.View.draw(View.java:18946)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:18159)\r\n        at android.view.View.draw(View.java:18946)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:18159)\r\n        at android.view.View.draw(View.java:18946)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4238)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4024)\r\n        at android.view.View.draw(View.java:19221)\r\n        at com.android.internal.policy.DecorView.draw(DecorView.java:791)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:18168)\r\n        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:676)\r\n        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:682)\r\n        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:790)\r\n        at android.view.ViewRootImpl.draw(ViewRootImpl.java:3050)\r\n        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2845)\r\n        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2398)\r\n        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1431)\r\n        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6861)\r\n        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:1026)\r\n        at android.view.Choreographer.doCallbacks(Choreographer.java:838)\r\n        at android.view.Choreographer.doFrame(Choreographer.java:769)\r\n        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:1012)\r\n        at android.os.Handler.handleCallback(Handler.java:790)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6650)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:547)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:818)\r\n     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:59)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:188)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:176)\r\n        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:122)\r\n        \t... 51 more\r\n Who can help me? Thanks!\r\n\r\n\r\n", "comments": [" And this is my command to compile tflite files through toco. \r\n\r\nbazel run tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=/Users/jyy/model/frozen_inference_graph.pb \\\r\n--output_file=/Users/jyy/model/wechat.tflite \\\r\n--input_shapes=1,128,128,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n--inference_type=FLOAT \\\r\n--allow_custom_ops", "Facing the same issue with MobilenetV2 DeeplabV3 Segmentation in Tensorflow 1.11.\r\n\r\nIssue Referral Links in StackOverflow:\r\n[1] https://stackoverflow.com/questions/53228969/unable-to-test-and-deploy-a-deeplabv3-mobilenetv2-tensorflow-lite-segmentation-m\r\n[2] https://stackoverflow.com/questions/53236290/unable-to-load-tflite-deeplab-segmentation-model-in-android-application-error", "@SanthoshRajendiran Sorry . There are no answers to the two questions above stackovewrflow, so I still haven't found a way to solve them.", "Hello @wangyongqi  We have updated both the stack overflow links. Seems bytebuffer issue exists in all tensorflow-lite versions except for the nightly build(org.tensorflow:tensorflow-lite:0.0.0-nightly). But on using the nightly build, we are facing a different issue. \r\n\r\n`java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 32)Node number 30 (DEPTHWISE_CONV_2D) failed to prepare.`\r\n\r\nThis issue is with respect to our model (Mobilenetv2 - DeeplabV3)", "@SanthoshRajendiran Or the wind, thank you very much for your answer.My side is going to recompile it.", "Thank you for the response @wangyongqi . Is the recompilation done? And around when will the bug be fixed for tf-lite version of DeeplabV3? ", " Hello, I have compiled it and no longer produced this BUG. What I am using now is the.Pb file generated by this command. \r\n\r\npython object_detection/export_tflite_ssd_graph.py --input_type image_tensor --pipeline_config_path object_detection/ssd_model/ssd_mobilenet_v1_pets.config --trained_checkpoint_prefix object_detection/train/model.ckpt-200000 --output_directory object_detection/ssd_model/newmodel/\r\n\r\nThis command was used before.\r\n\r\npython object_detection/export_inference_graph.py --input_type image_tensor --pipeline_config_path object_detection/ssd_model/ssd_mobilenet_v1_pets.config --trained_checkpoint_prefix object_detection/train/model.ckpt-200000 --output_directory object_detection/ssd_model/newmodel/\r\n\r\nThen I can successfully convert and compile the tflite model used on Androidstudio. You can turn off the issue. I also hope to help people who have the same mistake with me.\r\nTanks very much @SanthoshRajendiran .\r\n\r\n", "Still the issue persists for DeeplabV3 - MobilenetV2 tflite for semantic segmentation.([http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz)).\r\n\r\nWe have added up a feature request for an Android Application for Deeplab.\r\n[https://github.com/tensorflow/tensorflow/issues/23747](https://github.com/tensorflow/tensorflow/issues/23747)", "okay . I will close this issue."]}, {"number": 23627, "title": "TFLiteConverter.from_saved_model - batchNorm is not supported? ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, i have tried tensorflow example code snippet. \r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Linux bedbba52137a 4.14.65+ #1 SMP Sun Sep 9 02:18:33 PDT 2018 x86_64 x86_64 x86_64 GNU/Linux)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): pip on google colab, using following command \r\n```\r\npip3 install --upgrade tf-nightly\r\n\r\n```\r\n- TensorFlow version (use command below): version: 1.13.0-dev20181109\r\n```\r\nimport tensorflow as tf \r\nprint(tf.__version__)\r\n\r\n```\r\n- Python version: Python 3.6.6\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: using command 'nvcc --version'\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\r\nCuda compilation tools, release 9.2, V9.2.148\r\n\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI have a [model here](https://drive.google.com/file/d/1mgjhsPpEDKs8Jud1wbtAKfjObAJVO8GL/view?usp=sharing), which is exported SavedModel using following code: \r\n\r\n```\r\n# SavedModel using simple_save()\r\n\r\nins = {\"phase_train_placeholder\":phase_train_placeholder}\r\nouts = {\"embeddings\":embeddings}\r\ntf.saved_model.simple_save(sess, '/content/generated/', ins, outs)\r\n\r\n```        \r\n\r\nWhen i convert this SavedModel to TFLite it give me error, the code snippet is as: \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nsaved_model_dir = '/content/generated/'\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_arrays=['phase_train'], input_shapes=(1,160,160,3), \r\n                                                             output_arrays=['embeddings'])\r\ntflite_model = converter.convert()\r\nopen(\"converted_model_savedModel.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nFollowing are error logs: \r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert_saved_model.py:61: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/queue_runner_impl.py:391: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nINFO:tensorflow:Restoring parameters from /content/generated/variables/variables\r\nINFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nINFO:tensorflow:input tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: phase_train_placeholder\r\nINFO:tensorflow: tensor name: phase_train:0, shape: unknown_rank, type: DT_BOOL\r\nINFO:tensorflow:output tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: embeddings\r\nINFO:tensorflow: tensor name: embeddings:0, shape: (-1, 512), type: DT_FLOAT\r\nINFO:tensorflow:Restoring parameters from /content/generated/variables/variables\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-63a92824a047> in <module>()\r\n      6 \r\n      7 converter = tf.contrib.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_arrays=['phase_train'], input_shapes=(1,160,160,3), \r\n----> 8                                                              output_arrays=['embeddings'])\r\n      9 tflite_model = converter.convert()\r\n     10 open(\"converted_model_savedModel.tflite\", \"wb\").write(tflite_model)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in from_saved_model(cls, saved_model_dir, input_arrays, input_shapes, output_arrays, tag_set, signature_key)\r\n    342 \r\n    343     result = _freeze_saved_model(saved_model_dir, input_arrays, input_shapes,\r\n--> 344                                  output_arrays, tag_set, signature_key)\r\n    345     return cls(\r\n    346         graph_def=result[0], input_tensors=result[1], output_tensors=result[2])\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert_saved_model.py in freeze_saved_model(saved_model_dir, input_arrays, input_shapes, output_arrays, tag_set, signature_key)\r\n    254     in_tensors = _get_tensors(graph, inputs, input_arrays)\r\n    255     out_tensors = _get_tensors(graph, outputs, output_arrays)\r\n--> 256     set_tensor_shapes(in_tensors, input_shapes)\r\n    257 \r\n    258     output_names = [node.split(\":\")[0] for node in outputs]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert_saved_model.py in set_tensor_shapes(tensors, shapes)\r\n    201   if shapes:\r\n    202     for tensor in tensors:\r\n--> 203       shape = shapes.get(tensor_name(tensor))\r\n    204       if shape is not None:\r\n    205         tensor.set_shape(shape)\r\n\r\nAttributeError: 'tuple' object has no attribute 'get'\r\n```\r\n\r\n**Updates on 10-Nov-2018**\r\n\r\nI have to give input_shape as dictionary in the following way:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nsaved_model_dir = '/content/generated/'\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_arrays=['phase_train'], input_shapes={\"phase_train\":[1,160,160,3]}, output_arrays=['embeddings'])\r\n\r\ntflite_model = converter.convert()\r\nopen(\"converted_model_savedModel.tflite\", \"wb\").write(tflite_model) \r\n```\r\n\r\nThis fixed the earlier error but now i see a different error and the logs are below: \r\n\r\n```\r\nINFO:tensorflow:Restoring parameters from /content/generated/variables/variables\r\nINFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nINFO:tensorflow:input tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: phase_train_placeholder\r\nINFO:tensorflow: tensor name: phase_train:0, shape: unknown_rank, type: DT_BOOL\r\nINFO:tensorflow:output tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: embeddings\r\nINFO:tensorflow: tensor name: embeddings:0, shape: (-1, 512), type: DT_FLOAT\r\nINFO:tensorflow:Restoring parameters from /content/generated/variables/variables\r\nINFO:tensorflow:Froze 490 variables.\r\nINFO:tensorflow:Converted 490 variables to const ops.\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-53-91d1899f3204> in <module>()\r\n      8 converter = tf.contrib.lite.TocoConverter.from_saved_model(saved_model_dir, input_arrays=['phase_train'], input_shapes={\"phase_train\":[1,160,160,3]}, \r\n      9                                                    output_arrays=['embeddings'])\r\n---> 10 tflite_model = converter.convert()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    454           input_tensors=self._input_tensors,\r\n    455           output_tensors=self._output_tensors,\r\n--> 456           **converter_kwargs)\r\n    457     else:\r\n    458       result = _toco_convert_graph_def(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)\r\n    395   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    396                              toco_flags.SerializeToString(),\r\n--> 397                              input_data.SerializeToString())\r\n    398   return data\r\n    399 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    170       stderr = _try_convert_to_unicode(stderr)\r\n    171       raise ConverterError(\r\n--> 172           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    173   finally:\r\n    174     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n2018-11-11 08:46:00.208147: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: FIFOQueueV2\r\n2018-11-11 08:46:00.216527: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2018-11-11 08:46:00.216572: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: QueueDequeueUpToV2\r\n2018-11-11 08:46:00.216749: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: RefSwitch\r\n2018-11-11 08:46:00.216793: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: AssignSub\r\n2018-11-11 08:46:00.216846: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: RefSwitch\r\n\r\n....... logs dropped here \r\n\r\n2018-11-11 08:46:00.291969: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: RefSwitch\r\n2018-11-11 08:46:00.292018: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: AssignSub\r\n2018-11-11 08:46:00.292076: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: RefSwitch\r\n2018-11-11 08:46:00.292113: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: AssignSub\r\n2018-11-11 08:46:00.937387: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 5600 operators, 9398 arrays (0 quantized)\r\n2018-11-11 08:46:01.526448: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 3582 operators, 6259 arrays (0 quantized)\r\n2018-11-11 08:46:01.979950: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 3582 operators, 6259 arrays (0 quantized)\r\n2018-11-11 08:46:01.982607: F tensorflow/lite/toco/graph_transformations/resolve_batch_normalization.cc:45] Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\r\nAborted (core dumped)\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should create *.lite file instead. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI have given the [SavedModel](https://drive.google.com/file/d/1mgjhsPpEDKs8Jud1wbtAKfjObAJVO8GL/view?usp=sharing) and above code snippet to reproduce it. \r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have the same error, when I use `Transfrom Graph` tool to get a quantized.pb file, I use the following command to convert to tf-lite, but it failed with the error \"Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\"\r\n```\r\ntflite_convert --output_file=${DATASET_DIR}/quantized_mobilenetv2.tflite \\\r\n        --graph_def_file=${DATASET_DIR}/quantized_graph.pb \\\r\n        --input_shapes=1,224,224,3 \\\r\n        --allow_custom_ops=true \\\r\n        --input_arrays=input \\\r\n        --output_arrays=MobilenetV2/Predictions/Reshape_1 \\\r\n        --mean_values=128 \\\r\n        --std_dev_values=127\r\n```\r\n\r\nAny ideas or suggestions would be greatly appreciated. .Thanks.\r\n\r\n", "@milinddeore Are you trying to convert a graph that performs model training? We can typically only convert graphs that perform eval. Can you post your code that builds the graph?", "@srjoglekar246 This is FaceNet [model](https://drive.google.com/open?id=1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-), the source code is [here](https://github.com/davidsandberg/facenet/tree/master/src), and i modified [train_softmax.py](https://www.dropbox.com/s/o1r0bw3fpu9imgm/train_softmax.py?dl=0) for `.tflite` conversion.\r\n\r\nI tried converting to `.tflite` from savedModel and from frozen but no luck. But today i saw lot has changed on the [page](https://www.tensorflow.org/lite/devguide). \r\n\r\nHere is the other [thread](https://github.com/tensorflow/tensorflow/issues/19431), where similar issue is seen. \r\n\r\n and ", "I could able to solve it, please see check on  #19431 ", "@milinddeore I think this makes sense. Sorry I didn't get to your issue on time, but it seems like the error occurred because you were earlier trying to convert the part of the graph that was doing the training?\r\n\r\nIn [this attempt](https://github.com/tensorflow/tensorflow/issues/19431#issuecomment-467013566), you essentially add an input/output interface to the saved graph and use _just_ that with the converter. Am I correct?\r\n\r\nThanks for resolving this :-). Can we close the bug?", "@srjoglekar246 Thats correct! \r\nIt strips off `phase_train`, training input tensor and keeps `input` inference only.  \r\nThat essentially means that all the `BatchNorm` with `is_training=False` and many other training specific ops are removed. \r\n\r\nI have a question here, is there a document where we can see all the support ops on mobile or `.tflite` per release?  ", "Thanks for confirming! Closing this one...", "@milinddeore The [Compatibility Guide](https://www.tensorflow.org/lite/tf_ops_compatibility) should be a good resource for most ops. However, there is a slight chance it might be outdated, in that case you can look into our [kernels directory](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/kernels)."]}, {"number": 23626, "title": "how to merge different .ckpt files into a .pf file", "body": "\r\nI had trained some  networks and saved them into  .ckpt file separately,  There is a connection between these networks, the output of the previous network is the input of the latter network, so here I want to merge these .ckpt files into a .pb file so I could deploy it very conveniently. But I didn't find the relevant tutorial. Had anyone done a similar job like that?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23625, "title": "question about quantization", "body": "Using quantization, with the same parameters, the difference of the training accuracy is about 5-10% each time\uff1f\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:pip\r\n- **TensorFlow version (use command below)**:1.4\r\n- **Python version**:3.4.3\r\n- **Bazel version (if compiling from source)**:1.9\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Can you be more specific, like models of interest, etc? Also, what are you trying to compare here? The accuracy of the floating point model vs. that of the quantized one?", "Closing due to inactivity."]}, {"number": 23624, "title": "question about quantization", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 23623, "title": "question about quantization", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 23622, "title": "A bug of tfdbg : Non-OK-status: env->NewWritableFile(file_path, &f) status: Not found: Failed to create a NewWriteableFile:", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary):pip install tensorflow\r\n- TensorFlow version (use command below):1.10.0\r\n- Python version: 3.6.5 \r\n- Bazel version (if compiling from source):no\r\n- GCC/Compiler version (if compiling from source):no\r\n- CUDA/cuDNN version:no\r\n- GPU model and memory:no\r\n\r\nWhen I use tfdbg, I input \"run\" in the console, the error info  like this:\r\n\r\n2018-11-09 17:07:18.381547: F T:\\src\\github\\tensorflow\\tensorflow\\core\\debug\\debug_io_utils.cc:621] Non-OK-status: env->NewWritableFile(file_path, &f) status: Not found: Failed to create a NewWriteableFile: C:\\Users\\winter\\AppData\\Local\\Temp\\tfdbg_312ol3zm/_tfdbg_device_,job_localhost,replica_0,task_0,device_CPU_0/train_step/gradients/bi-lstm/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Const_0_DebugIdentity_1541754438373045 : \u03f5\u0373\ufffd\u04b2\ufffd\ufffd\ufffd\u05b8\ufffd\ufffd\ufffd\ufffd\u00b7\ufffd\ufffd\ufffd\ufffd\r\n; No such process\r\n\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n", "comments": ["Perhaps I know the reason\uff0c the path and file name is too long . It beyonds the windows 10 limit. So it cannot create . ", "> Perhaps I know the reason\uff0c the path and file name is too long . It beyonds the windows 10 limit. So it cannot create .\r\n\r\nFeel free to close if you are sure about the reason. Or please share sample code snippet which helps us to reproduce the error and look into the issue. ", "Closing this as a root cause is found.", "Although there is a problem with Windows not allowing to create a folder or file with such a large name, it should be addressed in Tensorflow, since it is nothing that we can do to resolve it.", "> Although there is a problem with Windows not allowing to create a folder or file with such a large name, it should be addressed in Tensorflow, since it is nothing that we can do to resolve it.\r\n\r\nHowever, I found out someone has a workaround on Windows side [here](https://github.com/tensorflow/tensorflow/issues/18750#issuecomment-435654042). But it is still nice to have a solution on Tensorflow side."]}, {"number": 23621, "title": "tf.assign does not support gradient?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1 LTS\r\n- TensorFlow version (use command below):1.11\r\n- Python version:3.6\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:CUDA release 9.0, V9.0.176\r\n- GPU model and memory: TITAN Xp / 12Gb\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nIt seems like tf.assign does not support gradients. In the forward pass, everything seems to work ok. But when I try to do backpropogate the gradients,it does not work. The error specifies that there are no variables to optimize which is not certainly the case. \r\n**Describe the expected behavior**\r\ntf.assign should simply allow for gradient to flow, just like its python counterpart Python assign operator.\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    def func_foo(x):\r\n \r\n        phi = tf.Variable(tf.zeros([1,10,10,1], dtype='float32'), \r\n                        dtype='float32',trainable=False)\r\n\r\n        phi=tf.assign(phi,x)\r\n\r\n        c3=tf.nn.sigmoid(phi)\r\n        c4=tf.reduce_mean(c3)\r\n\r\n    return 1-c4\r\n\r\n    a = np.random.randint(2, size=(10,10))\r\n    k = np.array([[1,1,1],[1,1,1],[1,1,1]],dtype=np.float32)\r\n    flip = [slice(None, None, -1), slice(None, None, -1)]\r\n    k = k[flip]\r\n\r\n    a=a.astype(np.float32)\r\n    a_tensor = tf.reshape(a, [1, 10, 10, 1])\r\n    k_weight = tf.reshape(np.array(k), [3,3,1,1])\r\n\r\n    c2=tf.layers.conv2d(a_tensor,filters=1, kernel_size=3, strides=1, padding=\"same\",activation=tf.nn.relu) \r\n    total_loss=func_foo(c2)    \r\n    train_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)\r\n    init = tf.initialize_all_variables()\r\n    sess=tf.Session()\r\n    with tf.Session() as sess:\r\n        init = tf.initialize_all_variables()\r\n        sess.run(init)\r\n        _,=sess.run([train_op])``\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"test_tf2.py\", line 31, in <module>\r\n    train_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)\r\n  File \"/home/ali/anaconda3/envs/tf19/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 406, in minimize\r\n    ([str(v) for _, v in grads_and_vars], loss))\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 1) dtype=float32_ref>\", \"<tf.Variable 'conv2d/bias:0' shape=(1,) dtype=float32_ref>\"] and loss Tensor(\"sub:0\", shape=(), dtype=float32).\r\n\r\n", "comments": ["Tensorflow default var_list contains 'variables.trainable_variables()', 'ops.get_collection(ops.GraphKeys.TRAINABLE_RESOURCE_VARIABLES)', and 'ops.get_collection(ops.GraphKeys._STREAMING_MODEL_PORTS)', but in your code, there is no trainable variables. Because 'tf.assign(phi,x)' assigns Tensor x to phi, and phi become an Tensor rather than a Variable.", "@ahatamiz,\r\nSorry for the delayed response. As explained by **`alextp`**  in [this comment](https://github.com/tensorflow/tensorflow/issues/17735#issuecomment-373796755), \r\n\r\n> There is no plan to add a gradient to tf.assign because it's not possible in general to connect the uses of the assigned variable with the graph which assigned it.\r\n> \r\n> Instead, your solution of setting the weights of a network by the output of another is better implemented by not using tf.Variable at all for the computed weights, and just using tensors. We currently make it not too easy to do this using the tf.layers library but you can do this if you subclass the layers in tf.layers and override the build() method.\r\n\r\nPlease refer this [Stack Overflow Answer](https://stackoverflow.com/a/59562683) for the workaround. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23620, "title": "[INTEL MKL] Fix memory leak issue in MklFusedBatchNorm", "body": "fix the memory leak bug in mkl batch norm, free the temp weight buffer used for MKL-DNN weight memory primitive.", "comments": ["@harshini-gadige can you take a look at this PR? the one failure //tensorflow/c/eager:c_api_test_gpu seems to be unrelated and is failing in other PRs also.", "> @harshini-gadige can you take a look at this PR? the one failure //tensorflow/c/eager:c_api_test_gpu seems to be unrelated and is failing in other PRs also.\r\n\r\nSure. Will look into this.", "@harshini-gadige The failures are not related to this PR,  other PRs are also failing the same tests. Can you take a look at it again? Thanks.", "> @harshini-gadige The failures are not related to this PR, other PRs are also failing the same tests. Can you take a look at it again? Thanks.\r\n\r\nLooking into this. It takes time and I'll keep posted. Thanks !", "@tatianashp  Can I bypass these 2 windows bazel failures and proceed with \"Ready to pull\"  ?", "> @tatianashp Can I bypass these 2 windows bazel failures and proceed with \"Ready to pull\" ?\r\n\r\nAs per the response I got from the chat, proceeding to get this merged."]}, {"number": 23619, "title": "tf.contrib.distributions.percentile didn't support eager execution", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):tensorflow-py2-cpu API r1.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen I using the eager execution. I try this:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nx = tf.constant([1,2,3,4],dtype='float64')\r\ny = tf.contrib.distributions.percentile(x,q=30)\r\n```\r\n\r\nwill report an error:\r\n\r\n> TypeErrorTraceback\r\n> \r\n>  (most recent call last)\r\n> <ipython-input-1-b7b170c83880> in <module>()\r\n>       4 \r\n>       5 x = tf.constant([1,2,3,4],dtype='float64')\r\n> ----> 6 y = tf.contrib.distributions.percentile(x,q=30)\r\n> \r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/sample_stats.pyc in percentile(x, q, axis, interpolation, keep_dims, validate_args, name)\r\n>     301                        (allowed_interpolations, interpolation))\r\n>     302 \r\n> --> 303   with ops.name_scope(name, [x, q]):\r\n>     304     x = ops.convert_to_tensor(x, name=\"x\")\r\n>     305     # Double is needed here and below, else we get the wrong index if the array\r\n> \r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in __enter__(self)\r\n>    5743       else:\r\n>    5744         cache_key = self._name, self._old_name, self._default_name\r\n> -> 5745         if cache_key in name_scope_cache:\r\n>    5746           self._ctx.scope_name = name_scope_cache[cache_key]\r\n>    5747           return self._ctx.scope_name\r\n> \r\n> TypeError: unhashable type: 'list'\r\n\r\nbut when I use tf.Session() to run the same code block:\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.constant([1,2,3,4],dtype='float64')\r\ny = tf.contrib.distributions.percentile(x,q=30)\r\nwith tf.Session() as sess:\r\n    print sess.run(y)\r\n\r\n```\r\nIt worked and report that I wanted:\r\n`2.0`\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n\r\nI'm using tensorflow to make some calculate. Eager is very help but this problem stop me to use it.\r\n\r\nThanks for anyone will fix it.", "comments": ["Added PR #23637 for the fix.", "The issue has been fixed in:\r\nhttps://github.com/tensorflow/tensorflow/pull/23637#issuecomment-449498024\r\n\r\nWill close the issue now."]}, {"number": 23618, "title": "Build of tensorflow r1.12 fails on ubuntu 18.04", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r1.12\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.19.0\r\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\nCUDA/cuDNN version: 10.0/7.4.1\r\nGPU model and memory: NVIDIA GeForce 940MX\r\nExact command to reproduce: bazel build -c opt --config=cuda --config=gdr --config=mkl --config=ngraph --config=verbs  //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n**Describe the problem**\r\nWhile building the tensorflow from the branch r1.12 with GPU support, the build fails with the following error:\r\nERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:105:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\nThe issue was reproducable in tensorflow 1.11, It was said that it will be fixed in 1.12 after the merge from master, but apparently this did not happen.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\nbazel build -c opt --config=cuda --config=gdr --config=mkl --config=ngraph --config=verbs  //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n**Any other info / logs**\r\nThe following command line is available:\r\nERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:105:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd ~/.cache/bazel/_bazel_username/cf67b2b2e967476eb2b1ee98e33ab5bd/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    NCCL_HDR_PATH=/usr/include \\\r\n    NCCL_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \\\r\n    PATH=~/bin:/usr/local/sbin:/usr/local/lib:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/linuxbrew/.linuxbrew/opt/coreutils/libexec/gnubin:/usr/local/cuda/bin:/usr/local/share/apache/hadoop/sbin:/usr/local/share/apache/hadoop/bin:/usr/local/share/apache/spark/sbin:/usr/local/share/apache/spark/bin:/usr/games:/usr/local/games:~/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \\\r\n    TF_CUDA_VERSION=10.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_mgr/rdma_mgr.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_mgr/rdma_mgr.pic.o' '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTF_USE_SNAPPY -DTENSORFLOW_USE_VERBS -DTENSORFLOW_USE_GDR -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote bazel-out/k8-opt/bin/external/bazel_tools -iquote external/grpc -iquote bazel-out/k8-opt/genfiles/external/grpc -iquote bazel-out/k8-opt/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/genfiles/external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/mkl_linux -iquote bazel-out/k8-opt/genfiles/external/mkl_linux -iquote bazel-out/k8-opt/bin/external/mkl_linux -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote bazel-out/k8-opt/bin/external/mkl_dnn -iquote external/curl -iquote bazel-out/k8-opt/genfiles/external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/genfiles/external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/genfiles/external/aws -iquote bazel-out/k8-opt/bin/external/aws -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/grpc/include -isystem bazel-out/k8-opt/genfiles/external/grpc/include -isystem bazel-out/k8-opt/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/genfiles/third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/k8-opt/genfiles/external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/mkl_linux/include -isystem bazel-out/k8-opt/genfiles/external/mkl_linux/include -isystem bazel-out/k8-opt/bin/external/mkl_linux/include -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/curl/include -isystem bazel-out/k8-opt/genfiles/external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -mavx -mavx2 -mfma '-mfpmath=both' -msse4.2 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DINTEL_MKL=1' -DEIGEN_USE_VML -DENABLE_MKL '-DINTEL_NGRAPH=1' -fopenmp -msse3 -pthread '-DGOOGLE_CUDA=1' '-DINTEL_MKL=1' -DENABLE_MKL -c tensorflow/contrib/verbs/rdma_mgr.cc -o bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_mgr/rdma_mgr.pic.o)\r\ntensorflow/contrib/verbs/rdma_mgr.cc: In static member function 'static void tensorflow::RdmaMgr::RegMemVisitors()':\r\ntensorflow/contrib/verbs/rdma_mgr.cc:282:40: error: invalid use of member 'tensorflow::RdmaMgr::rdma_adapter_' in static member function\r\n     int32_t bus_id = TryToReadNumaNode(rdma_adapter_->context_->device) + 1;\r\n                                        ^~~~~~~~~~~~~\r\nIn file included from tensorflow/contrib/verbs/rdma_mgr.cc:18:0:\r\n./tensorflow/contrib/verbs/rdma_mgr.h:50:16: note: declared here\r\n   RdmaAdapter* rdma_adapter_;\r\n                ^~~~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 4521.698s, Critical Path: 158.49s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 4797 processes: 4797 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["Now the similar issue happens with GDR support as well (was not the case in tensorflow 1.11):\r\nERROR: /home/vyepishov/Documents/dev/git/tensorflow/tensorflow/contrib/gdr/BUILD:40:1: C++ compilation of rule '//tensorflow/contrib/gdr:gdr_memory_manager' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command", "Also, the build fails with the nGraph support enabled (if both libverbs and gdr configuration options are removed from the build).", "I just successfully built ubuntu 18.10 build (kernal 4.18), tensorflow r1.12 branch, python 3.6, CUDA 10.0, cuDNN 7.4.1.5, NCCL 2.3.7, tensorRT 5.0, compute capability 7.5, 6.1, for my RTX 2080 TI.\r\n[tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl](https://app.box.com/s/jcw8uq6jannpg4myzighl166bq1zg07e)\r\n\r\nIf you need it I can try build one for your compute capability.\r\n\r\n\r\n", "Hi @MingyaoLiu,\r\n\r\nI am able to successfully build if none of the above mentioned options are enabled (--config=gdr --config=ngraph --config=verbs).\r\nIf any of these options enabled, the build fails.\r\nThe question is why it is not possible to build with any of these options enabled.", "I am able to build using `--config=gdr` alone.  I am not able to reproduce the failure to build `//tensorflow/contrib/gdr:gdr_memory_manager`.  Otherwise, the verbs build fails in the same was as reported in #22455 .", "I do not know why, but now after I tried to rebuild the tensorflow r1.12 from sources with the --config=gdr and --config=cuda options, I am getting the following error:\r\n**ERROR: ~/.cache/bazel/_bazel_vyepishov/cf67b2b2e967476eb2b1ee98e33ab5bd/external/local_config_cc/BUILD:57:1: in cc_toolchain rule @local_config_cc//:cc-compiler-k8: Error while selecting cc_toolchain: Toolchain identifier 'local' was not found, valid identifiers are [local_linux, local_darwin, local_windows]\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-k8' failed; build aborted**", "Which version of bazel?  Try 0.18.1.", "Hi @jeffdaily,\r\n\r\nI am using Ubuntu 18.04. Currently the bazel version that is supported on this system is 0.19.1.\r\nThe usage of bazel 0.18.1 can be only made manually (and it is kind of a downgrade), which I find to be an ugly solution.\r\nIt is a pity that tensorflow build cannot now be done as easy as before.", "> Hi @jeffdaily,\r\n> \r\n> I am using Ubuntu 18.04. Currently the bazel version that is supported on this system is 0.19.1.\r\n> The usage of bazel 0.18.1 can be only made manually (and it is kind of a downgrade), which I find to be an ugly solution.\r\n> It is a pity that tensorflow build cannot now be done as easy as before.\r\n\r\nHow is that a 'pity' and 'ugly' solution?? There are bugs introduced in any new version of software, that's why there is supported version of bazel and not support version of bazel.", "Hi @MingyaoLiu,\r\n\r\nThe inability of the tensorflow project to properly support bazel 0.19.1 (which was the decision of that same project to use it as a build tool) CAN be IMHO called the ugly situation, and therefore I DO feel pity about it.\r\nSince this version of bazel was allowed on the Ubuntu 18.04 (which is already not the last but previous to the last release of Ubuntu), then I can with some confidence say that this version of bazel was \"a bit\" tested before, therefore I would say it is likely to be not about the bugs in the new software, but rather about the changes in it, which were not considered in the tensorflow project.", "Duplicated with #22455.", "> I just successfully built ubuntu 18.10 build (kernal 4.18), tensorflow r1.12 branch, python 3.6, CUDA 10.0, cuDNN 7.4.1.5, NCCL 2.3.7, tensorRT 5.0, compute capability 7.5, 6.1, for my RTX 2080 TI.\r\n> [tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl](https://app.box.com/s/jcw8uq6jannpg4myzighl166bq1zg07e)\r\n> \r\n> If you need it I can try build one for your compute capability.\r\n\r\nI also have a problem building tensorflow package, I think my network is responsible for it.\r\nSince I haven't found an effective solution right now, could you please build one version for me?\r\nHere are my machine and its configurations, ubuntu18.04, tensorflow1.12, CUDA10.0, cuDNN7.4.1, NCCL2.3.7, compute compatibility 6.1 for 1080Ti(by the way, why you need two params for this configuration. Is 4 GPU differ from one?)\r\nThank you very much for your help!", "@LLdleo Please open another issue for your question.", "> @LLdleo Please open another issue for your question.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/24135\r\n\r\nno reply yet", "@wt-huang I believe this issue has been fixed. Could you close this?"]}, {"number": 23617, "title": "Build //tensorflow:libtensorflow.so error.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Linux Ubuntu 16.04:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:r1.8.0\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):0.17.2\r\n- GCC/Compiler version (if compiling from source):gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)\r\n\r\n\r\n\r\n**Describe the problem**\r\nwhen I try to build this:\r\n```bash\r\nbazel build -c opt //tensorflow:libtensorflow.so\r\n```\r\ngot following errors:\r\n```bash\r\n/usr/bin/ld.gold: error: tf_exported_symbols.lds:2:1: syntax error, unexpected STRING\r\n/usr/bin/ld.gold: error: tf_exported_symbols.lds: not an object or archive\r\n\r\n```\r\nhere is the tensorflow build rule\r\n```\r\ntf_cc_shared_object(\r\n    name = \"libtensorflow.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:darwin\": [\r\n            \"-Wl,-exported_symbols_list\",  # This line must be directly followed by the exported_symbols.lds file\r\n            \"$(location //tensorflow/c:exported_symbols.lds)\",\r\n            \"-Wl,-install_name,@rpath/libtensorflow.so\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//tensorflow:windows_msvc\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-s\",\r\n            \"-Wl,--version-script\",  #  This line must be directly followed by the version_script.lds file\r\n            \"$(location //tensorflow/c:version_script.lds)\",\r\n        ],\r\n    }),\r\n    deps = [\r\n        \"//tensorflow/c:c_api\",\r\n        \"//tensorflow/c:c_api_experimental\",\r\n        \"//tensorflow/c:exported_symbols.lds\",\r\n        \"//tensorflow/c:version_script.lds\",\r\n        \"//tensorflow/c/eager:c_api\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n```\r\nand, the lds file here (I does no change to thie file)\r\n```bash\r\n*tensorflow*\r\n*perftools*gputools*\r\n*tf_*\r\n*TF_*\r\n*TFE_*\r\n*nsync_*\r\n*pywrap_xla*\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 23616, "title": "Fix relative path to reflect new (non-contrib) location", "body": "https://www.tensorflow.org/lite/rpi\r\n\r\nThis is also out of date now -- I'll submit a PR over in docs", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@petewarden this is a duplicate of #23594 (I submitted back when his CLA was not signed so I wasn't sure if it would ever get merged)", "> @petewarden this is a duplicate of #23594 (I submitted back when his CLA was not signed so I wasn't sure if it would ever get merged)\r\n\r\n@thedch  Closing this since it is a duplicate. Please track the other issue and add comments if required.\r\nAlso, please let us know the reason if you want to reopen this. We will reopen. Thank you !"]}, {"number": 23615, "title": "XLA does not know associative law", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Arch Linux**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **1.11.0**\r\n- Python version: **3.7.1**\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\noutput: \r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux ZHANGHAO 4.18.16-arch1-1-ARCH #1 SMP PREEMPT Sat Oct 20 22:06:45 UTC 2018 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 8.2.1 20180831\r\nCopyright \u00a9 2018 Free Software Foundation, Inc.\r\n\u672c\u7a0b\u5e8f\u662f\u81ea\u7531\u8f6f\u4ef6\uff1b\u8bf7\u53c2\u770b\u6e90\u4ee3\u7801\u7684\u7248\u6743\u58f0\u660e\u3002\u672c\u8f6f\u4ef6\u6ca1\u6709\u4efb\u4f55\u62c5\u4fdd\uff1b\r\n\u5305\u62ec\u6ca1\u6709\u9002\u9500\u6027\u548c\u67d0\u4e00\u4e13\u7528\u76ee\u7684\u4e0b\u7684\u9002\u7528\u6027\u62c5\u4fdd\u3002\r\n\r\n== uname -a =====================================================\r\nLinux ZHANGHAO 4.18.16-arch1-1-ARCH #1 SMP PREEMPT Sat Oct 20 22:06:45 UTC 2018 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.11.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh:\u884c105: nvidia-smi: \u672a\u627e\u5230\u547d\u4ee4\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\noutput: `b'unknown' 1.11.0`\r\n\r\n**Describe the current behavior**\r\nsource code:\r\nmain.py\r\n```python\r\nimport sys\r\nimport tensorflow as tf\r\n\r\nN = 10000\r\nn = 10\r\n\r\na = tf.random_normal([N,n])\r\nb = tf.random_normal([N,n])\r\nc = tf.random_normal([N,n])\r\nd = tf.random_normal([N,n])\r\n\r\nX1 = tf.tensordot(a,b,[[1],[1]],name='X1')\r\nY1 = tf.tensordot(c,d,[[1],[1]],name='Y1')\r\nZ1 = tf.tensordot(X1,Y1,[[0,1],[0,1]],name='Z1')\r\n\r\nX2 = tf.tensordot(a,c,[[0],[0]],name='X2')\r\nY2 = tf.tensordot(b,d,[[0],[0]],name='Y2')\r\nZ2 = tf.tensordot(X2,Y2,[[0,1],[0,1]],name='Z2')\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n\r\nsess = tf.Session(config=config)\r\n\r\nif sys.argv[1] == '1':\r\n    for i in range(10):\r\n        print(sess.run(Z1))\r\n\r\nif sys.argv[1] == '2':\r\n    for i in range(10):\r\n        print(sess.run(Z2))\r\n```\r\nwhen run `time python main.py 1` and  `time python main.py 2`, get 4.6s and 1.2s\r\n\r\n**Describe the expected behavior**\r\nIn fact Z1 and Z2 should be the same, hope there is some method to optimize Z1, since in my program, there are many similar situation. consider order of tensor contract is too complicated", "comments": ["Hi! @hzhangxyz !\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.5  version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "I don't know whether the current version solve it or not. but I realize that this feature is much harder to implement than I thought years ago, especially when the number of tensor increasing. Maybe it should not be a feature of XLA. So I close this issue."]}, {"number": 23614, "title": "Why only support tf.train.Int64List? In many case int32 can meet requirements. Int64 will expand the computation.", "body": "message Example {\r\n Features features = 1;\r\n};\r\n\r\nmessage Features{\r\n map<string,Feature> featrue = 1;\r\n};\r\n\r\nmessage Feature{\r\n    oneof kind{\r\n        BytesList bytes_list = 1;\r\n        FloatList float_list = 2;\r\n        Int64List int64_list = 3;\r\n    }\r\n};\r\n\r\n\r\nOnly support bytes_list , float_list, int64_list. Why not support int32_list?", "comments": ["@harshini-gadige any advice? Thx.", "Protos use a variable width encoding for integers, so there\u2019s really very little to gain from an int32 type for storing data on disk.", "> Protos use a variable width encoding for integers, so there\u2019s really very little to gain from an int32 type for storing data on disk.\r\n\r\n@shoyer But input tensor with data type int64  is inefficient, it consumes more computing resource when tf session train or predict model. In many case int32 meets requirements. Many users try to change input data type, they use tf.cast to convert int64 to int32 .", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think it is a bug/feature request, please fill the respective template inorder to start triaging. Thanks !"]}, {"number": 23613, "title": "ERROR: Config value opt is not defined in any .rc file", "body": "I wanna change .pb file to tflite file , but I failed.\r\n\r\nI use the follow commed.\r\n\r\nbazel run --config=opt tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops\r\n\r\n> \r\n**and it come out the error below:**\r\n\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/abb/tensorflow/tools/bazel.rc\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nERROR: Config value opt is not defined in any .rc file\r\n\r\nSo who can help me to solve this issue?\r\n", "comments": ["Do as the warning says:\r\n\r\nThe following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/abb/tensorflow/tools/bazel.rc\r\n\r\nadd:\r\nimport /home/abb/tensorflow/tools/bazel.rc\r\n\r\nto .bazelrc file.", "@786694836 Is this still an issue? Did you try?\r\n\r\n> Do as the warning says:\r\n> \r\n> The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n> /home/abb/tensorflow/tools/bazel.rc\r\n> \r\n> add:\r\n> import /home/abb/tensorflow/tools/bazel.rc\r\n> \r\n> to .bazelrc file.\r\n\r\n", "Same issue here.\r\n\r\nI added \r\nimport /home/username/tensorflow/tools/bazel.rc\r\n\r\nin my  /home/username/tensorflow/tools/bazel.rc, but it doesn't change anything.", "Option 1: add 'import /home/username/tensorflow/tools/bazel.rc' to /home/username/.bazelrc\r\n\r\nOption 2: instead of --config=opt, use -c opt\r\n\r\nDo any of those work for you?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Option 2: instead of --config=opt, use -c opt can work for me(based on tf1.12)", "Option 2: instead of --config=opt, use -c opt can work for me (based on tensorflow 1.12)", "please help me , i can't find the bazel.rc file in tensorflow root path, where i can get it ?", "Unable to run Option2. theres an error. \r\n\r\n```\r\nINFO: Writing tracer profile to 'C:/users/kirin/_bazel_kirin/rtshffit/command.profile.gz'\r\nERROR: Skipping 'tensorflow/lite/toco:toco': no such package 'research/object_detection/tensorflow/lite/toco': BUILD file not found in any of the following directories.\r\n - C:/tensorflow1/models/research/object_detection/tensorflow/lite/toco\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package 'research/object_detection/tensorflow/lite/toco': BUILD file not found in any of the following directories.\r\n - C:/tensorflow1/models/research/object_detection/tensorflow/lite/toco\r\nINFO: Elapsed time: 0.515s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\n", "For me the solution was to execute the command from the tensorflow floder, i.e. /home/username/tensorflow/tensorflow/\r\n\r\nThis folder contains workspace.bzl, which I believe contains the config values", "Both of the above solutions did not work for me.\r\n\r\nThe import command does not work, since there is no file 'bazel.rc' in the specified location, as has previously been mentioned by @jsonwang \r\n\r\nUsing -c opt made noi differnce either", "> Unable to run Option2. theres an error.\r\n> \r\n> ```\r\n> INFO: Writing tracer profile to 'C:/users/kirin/_bazel_kirin/rtshffit/command.profile.gz'\r\n> ERROR: Skipping 'tensorflow/lite/toco:toco': no such package 'research/object_detection/tensorflow/lite/toco': BUILD file not found in any of the following directories.\r\n>  - C:/tensorflow1/models/research/object_detection/tensorflow/lite/toco\r\n> WARNING: Target pattern parsing failed.\r\n> ERROR: no such package 'research/object_detection/tensorflow/lite/toco': BUILD file not found in any of the following directories.\r\n>  - C:/tensorflow1/models/research/object_detection/tensorflow/lite/toco\r\n> INFO: Elapsed time: 0.515s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (0 packages loaded)\r\n> FAILED: Build did NOT complete successfully (0 packages loaded)\r\n> ```\r\n\r\nI have the same problem on linux.", "Option 2 is very confusing due to [this](https://stackoverflow.com/questions/46319386/whats-the-difference-between-c-opt-and-config-opt-when-building-tensorflow-f) - it seems they do COMPLETELY different things...\r\n\r\nI got this issue on Windows when I looked at the .tf_configure.bazelrc file after generating it (python ./configure.py) and somehow accidentally deleted it straight after :-( Easy fix: configure again and just run the bazel build command :-)", "> \r\n> \r\n> Option 1: add 'import /home/username/tensorflow/tools/bazel.rc' to /home/username/.bazelrc\r\n> \r\n> Option 2: instead of --config=opt, use -c opt\r\n> \r\n> Do any of those work for you?\r\n@andrehentz \r\nhey thank you for your answer, it seems that this works for some people, but in my case i have this issue:\r\n![bazel](https://user-images.githubusercontent.com/62354721/135983141-b56f4695-1fa1-4e53-8a15-3dfbf4506f50.JPG)\r\n\r\n\r\n"]}, {"number": 23612, "title": "ValueError: No gradients provided for any variable when using custom raw_rnn", "body": "**System: ubuntu 16.04\r\nDocker: nvidia-gpu\r\nAnaconda: 5.1\r\ntensorflow:1.9**\r\n\r\nI use custom raw_rnn to implement the graph shown as following:\r\n![rnn](https://user-images.githubusercontent.com/7899459/48241311-494a8e80-e411-11e8-9235-735a8f8f8653.png)\r\n\r\nThe code of custom raw_rnn is shown as following:\r\n\r\n```\r\ndef loop_rnn(num_lstm_units, image_embeddings, class_num, batch_size, initializer_scale):\r\n    initializer = tf.random_uniform_initializer(\r\n        minval=-initializer_scale,\r\n        maxval=initializer_scale)\r\n\r\n    with tf.variable_scope(\"lstm\", initializer=initializer, reuse=tf.AUTO_REUSE) as lstm_scope:\r\n        lstm_cell = tf.contrib.rnn.BasicLSTMCell(\r\n            num_units=num_lstm_units, state_is_tuple=True)\r\n\r\n        zero_state = lstm_cell.zero_state(\r\n            batch_size=image_embeddings.get_shape()[0], dtype=tf.float32)\r\n\r\n        K = 5\r\n        C = 80\r\n        max_time = K + 1\r\n\r\n        temp_variable = (tf.TensorArray(size=K, dtype=tf.float32),\r\n                         tf.TensorArray(size=max_time, dtype=tf.float32))\r\n        temp_variable[1].write(0, [[[1., 0., 0.], [0., 1., 0.]] for i in range(batch_size)])\r\n\r\n        lstm_input_size = 14\r\n        zk_size = 4096\r\n\r\n        initial_state = zero_state\r\n\r\n        output_dim = 4096\r\n\r\n        fc_weight_initializer = tf.zeros_initializer()\r\n        fc_bias_initializer = tf.constant_initializer([1., 0., 0., 0., 1., 0.])\r\n\r\n        def loop_fn(time, cell_output, cell_state, loop_state):\r\n            if cell_output is None:\r\n                next_cell_state = initial_state\r\n                emit_output = tf.zeros([output_dim])\r\n            else:\r\n                next_cell_state = cell_state\r\n                emit_output = cell_output\r\n                # The graph after LSTM  #\r\n                z_k = tf.contrib.layers.fully_connected(\r\n                    inputs=cell_state,\r\n                    num_outputs=zk_size,\r\n                    activation_fn=tf.nn.relu,\r\n                    weights_initializer=initializer)\r\n\r\n                if time != 0:\r\n                    temp_variable[0].write([time - 1],\r\n                              tf.contrib.layers.fully_connected(\r\n                                  inputs=z_k[0],\r\n                                  num_outputs=class_num,\r\n                                  activation_fn=None,\r\n                                  weights_initializer=initializer))\r\n\r\n                if time != max_time:\r\n                    M = tf.reshape(\r\n                        tf.contrib.layers.fully_connected(\r\n                            inputs=z_k[0],\r\n                            num_outputs=max_time,\r\n                            activation_fn=None,\r\n                            weights_initializer=fc_weight_initializer,\r\n                            biases_initializer=fc_bias_initializer),\r\n                        [batch_size, 2, 3])\r\n                    tf.assign(M[:, 0, 1], (tf.convert_to_tensor(0.)))\r\n                    tf.assign(M[:, 1, 0], (tf.convert_to_tensor(0.)))\r\n                    temp_variable[1].write(time, M)\r\n               # The graph after LSTM  #\r\n\r\n            # The graph before LSTM  #\r\n            if time != max_time:\r\n                f_k_origin = stn.spatial_transformer_network(image_embeddings, temp_variable[1].read(time))\r\n                f_k = tf.layers.max_pooling2d(f_k_origin, pool_size=[2, 2], strides=2, padding='SAME')\r\n\r\n                with tf.variable_scope(\"fc_1\") as fc_1_scope:\r\n                    f_k = tf.contrib.layers.fully_connected(\r\n                    inputs=tf.reshape(f_k, [batch_size, int(lstm_input_size * lstm_input_size / 4 * 512)]),\r\n                    num_outputs=4096,\r\n                    activation_fn=None,\r\n                    weights_initializer=initializer,\r\n                    scope=fc_1_scope)\r\n            # The graph before LSTM  #\r\n\r\n            elements_finished = (time >= max_time)\r\n\r\n            if time == max_time:\r\n                next_in = None\r\n            else:\r\n                next_in = f_k\r\n\r\n            next_input = next_in\r\n\r\n            next_loop_state = temp_variable\r\n            return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\r\n\r\n        outputs_ta, last_state, loop_state_ta = tf.nn.raw_rnn(lstm_cell, loop_fn)\r\n        scores_result = loop_state_ta[0].stack()\r\n        M_result = loop_state_ta[1].stack()\r\n\r\n        return scores_result, M_result\r\n```\r\n\r\nThe error info is shown as following:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 125, in <module>\r\n    tf.app.run()\r\n  File \"/home/trainer/anaconda3/envs/tf19_py36/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 107, in main\r\n    optimizer=training_config.optimizer)\r\n  File \"/home/trainer/anaconda3/envs/tf19_py36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 297, in optimize_loss\r\n    name=\"train\")\r\n  File \"/home/trainer/anaconda3/envs/tf19_py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 583, in apply_gradients\r\n    ([str(v) for _, _, v in converted_grads_and_vars],))\r\nValueError: No gradients provided for any variable: [\"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fc_1/weights:0' shape=(25088, 4096) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fc_1/biases:0' shape=(4096,) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/basic_lstm_cell/kernel:0' shape=(8192, 16384) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/basic_lstm_cell/bias:0' shape=(16384,) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected/weights:0' shape=(4096, 4096) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected/biases:0' shape=(4096,) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected_1/weights:0' shape=(4096, 80) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected_1/biases:0' shape=(80,) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected_2/weights:0' shape=(4096, 6) dtype=float32_ref>)>\", \"<_RefVariableProcessor(<tf.Variable 'lstm/rnn/fully_connected_2/biases:0' shape=(6,) dtype=float32_ref>)>\"].\r\n```\r\n\r\n**When I look into the graph using tensorboard, it's strange that there are two STN, fc_1, max_pool, basic_lstm_cell, fully_connected, fully_connected_1, fully_connected_2. Why do these ops have double? Does it result in the problem?**\r\n\r\n![rdar_graph](https://user-images.githubusercontent.com/7899459/48241504-32f10280-e412-11e8-922e-fb08ef0102c2.png)\r\n ", "comments": []}, {"number": 23611, "title": "How to feed data to the graph after using Graph Transform Tool", "body": "The demo shows that it can strip unused nodes by Graph Transform Tool .\r\nBut when setting 'inputs=Mul:0' , the input of the graph becomes an 'Operation:Mul'.\r\nThe problem is that the graph needs a tensor as the input not the operation.\r\nHow to feed image data to the graph after stripping unused nodes?\r\n\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=tensorflow_inception_graph.pb \\\r\n--out_graph=optimized_inception_graph.pb \\\r\n--inputs='Mul:0' \\\r\n--outputs='softmax:0' \\\r\n--transforms='\r\nstrip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\nremove_nodes(op=Identity, op=CheckNumerics)\r\nfold_old_batch_norms\r\n'", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23609, "title": "[Cloud TPU] Various issues with uint8 data type", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): GCP\r\n- TensorFlow version (use command below): 1.11+\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: TPUv2\r\n\r\n**Describe the current behavior**\r\n\r\nThere's three issues here:\r\n1. Missing propagation of error messages, crashing the TPU host\r\n2. It not compiling for shapes other than `u8[1024]` or multiples thereof\r\n3. It crashing on infeed even for `u8[1024]`\r\n\r\nAt the moment, compiling (via XRTCompile) crashes the TPU host for anything that uses UInt8 datatypes, unless it's one dimensional and the size is a multiple of 1024. Using the julia frontend:\r\n\r\n```\r\njulia> @async @tpu (()->XLA.HloInfeed(Tuple{XRTArray{UInt8, (256,), 1}})(XLA.HloAfterAll()())[1][1])()\r\nERROR: Tensorflow error: Status: Socket closed\r\n\r\n```\r\n\r\n```\r\njulia> @async @tpu (()->XLA.HloInfeed(Tuple{XRTArray{UInt8, (1024,1024), 2}})(XLA.HloAfterAll()())[1][1])()\r\nERROR: Tensorflow error: Status: Socket closed\r\n```\r\n\r\nAnd though compiling succeeds for one dimensional UInt8 vectors of size 1024, trying to infeed crashes:\r\n```\r\njulia> @async @tpu (()->XLA.HloInfeed(Tuple{XRTArray{UInt8, (1024,), 1}})(XLA.HloAfterAll()())[1][1])()\r\nTask (runnable) @0x00007fae07199120\r\n\r\njulia> infeed((zeros(UInt8, 1024),))\r\nERROR: Tensorflow error: Status: Socket closed\r\n```\r\n\r\n**Describe the expected behavior**\r\nIdeally all of these would work. If that takes longer to do, at least exposing the error message to the client, rather than crashing would be desired.\r\n\r\n**Code to reproduce the issue**\r\nSee above (should be reproducible with any frontend that uses xrt to talk to Cloud TPUs though).", "comments": ["I am not familiar with XRT at all -- @froystig can you take a look?\r\n\r\ncc @jekbradbury", "These are mostly closer to \"missing functionality\" than \"bugs\", but we should definitely be returning a TF error over gRPC when something goes wrong in XLA rather than crashing the TPU host.", "Hi @Keno! We are checking to see if you need help in this issue.\r\nIt also seems you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and  create a new issue if the issue still persists in newer versions. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23608, "title": "Tflite for micro kernel does not have conv2d?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue  happens on mobile device: No.\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): from master\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- CUDA/cuDNN version: on CPU\r\n- GPU model and memory: on CPU\r\n\r\n**Describe the current behavior**\r\nI'm trying to figure out how to use Tflite for microcontroller, I know it's relatively new to the users. I notice that in the kernel folder, there is only the depth wise conv2d kernel. Does this mean models defined with tf.nn.conv2d won't fit into this framework? Is this part currently under developing or I just simply missed something? Thanks in advance.  \r\n", "comments": ["So far, I've gone through the micro_speech example. I know the model is defined with tf.nn.conv2d. [model defined in this file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/models.py). \r\nI'm curious about how does tflite for micro run through this model even without a conv2d kernel defined?", "I'm actually working on my own network. It's a CNN with RGB images as input. It contains regular conv2d layers just like most CNNs do. I wish to know how can I add this kernel into the micro interpreter. Would appreciate it if someone could point me to the correct way. Thanks!", "@petewarden Could you please give some idea on this? Thanks!", "Sorry for the slow response! We are working on adding more operations (like Conv2D), and hope to have them in soon, but we'd also welcome contributions. I'll be working on some documentation around porting ops, but that isn't ready yet either.", "If you guys can have a quick guide of how to do it, I can start to add my own kernels. That would be great! Thanks for your reply!", "I'd be interested in this functionality as well. Any updates @petewarden, or is there a guide for contributing kernels, per @TF-Deve's question?"]}, {"number": 23607, "title": "Segmentation Fault on `help(tf.RunMetadata)`", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14\r\n- TensorFlow installed from (source or binary): `pip install tensorflow`\r\n- TensorFlow version (use command below): v1.11.0-rc2-4-gc19e29306c 1.11.0\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\n`Segmentation fault: 11` and Python crashes.\r\n**Describe the expected behavior**\r\nPrint the documentation describing `tf.RunMetadata`.\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nhelp(tf.RunMetadata)\r\n```", "comments": ["I justed tried it and got the same behavior.\r\n\r\n* OS Platform and Distribution: Windows 10 v. 1803\r\n* TensorFlow installed from: pip install tensorflow\r\n* TensorFlow version: 'v1.8.0-0-g93bc2e2072' 1.8.0\r\n* Python version: 3.6.5", "It doesn't look like an issue on TF ends. Please refer this link which addresses [similar behavior](https://stackoverflow.com/questions/19531969/segmentation-fault-11-in-os-x)", "@ymodak you might be right, though the bug in that link seemed to be OS related. Using `faulthandler` i got:\r\n\r\n```\r\n$ python3 -X faulthandler\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01)\r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> help(tf.RunMetadata)\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007f1d87090700 (most recent call first):\r\n  File \"/usr/lib/python3.5/inspect.py\", line 397 in classify_class_attrs\r\n  File \"/usr/lib/python3.5/pydoc.py\", line 210 in classify_class_attrs\r\n  File \"/usr/lib/python3.5/pydoc.py\", line 1276 in docclass\r\n  File \"/usr/lib/python3.5/pydoc.py\", line 375 in document\r\n  File \"/usr/lib/python3.5/pydoc.py\", line 1632 in render_doc\r\n  File \"/usr/lib/python3.5/pydoc.py\", line 1639 in doc\r\n  File \"/usr/lib/python3.5/pydoc.py\", line 1905 in help\r\n  File \"/usr/lib/python3.5/pydoc.py\", line 1852 in __call__\r\n  File \"/usr/lib/python3.5/_sitebuiltins.py\", line 103 in __call__\r\n  File \"<stdin>\", line 1 in <module>\r\nSegmentation fault (core dumped)\r\n```", "I am getting this as well, on two separate machines (one Ubuntu 16.04, other Ubuntu 18.04). Also occurs when either run_options or metadata is passed to a tf.Session.run call (e.g., cloning the stable-baselines and running the tests results in core dumped). \r\n\r\n_Separately_, also with the stable-baselines repo, I'm getting an unrelated core dumped when using Tensorflow 1.11.0 or 1.12.0, but not when using 1.10.0. I don't know how to debug this, so am just using 1.10.0 for now.  ", "Closing this issue since its not a TF related bug. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "3 different users reported it for Windows, Mac, and Linux OSes. It's almost definitely a TF-related bug? ", "I justed tried it and got the same behavior.\r\n\r\nOS: Ubuntu 16.04\r\nTensorFlow installed from: pip install tensorflow\r\nTensorFlow version: 1.12.0\r\nPython version: 3.6.6", "@av8ramit  Can you please take a look? Thanks!", "This appears to be a bug in protobuf, which I can reproduce locally in Python 3.5 (but not Python 2.7) on Ubuntu. `tf.RunMetadata` is a protocol buffer generated type, and we use the C++ message extension for improved performance when using protos in TensorFlow. Here's what I get when I run under `gdb`:\r\n\r\n```\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fffc9f15684 in ?? () from /usr/local/google/home/mrry/tf-nightly-3/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so\r\n(gdb) bt\r\n#0  0x00007fffc9f15684 in ?? () from /usr/local/google/home/mrry/tf-nightly-3/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so\r\n#1  0x000055555567a6ce in ?? ()\r\n#2  0x00005555556dbadd in ?? ()\r\n#3  0x00005555557146df in PyCFunction_Call ()\r\n#4  0x00005555556d46e9 in PyEval_EvalFrameEx ()\r\n#5  0x00005555556d493f in PyEval_EvalFrameEx ()\r\n#6  0x00005555556d493f in PyEval_EvalFrameEx ()\r\n#7  0x00005555556da817 in PyEval_EvalCodeEx ()\r\n#8  0x0000555555716648 in ?? ()\r\n#9  0x000055555575d647 in PyObject_Call ()\r\n#10 0x00005555556d2180 in PyEval_EvalFrameEx ()\r\n#11 0x00005555556d9286 in ?? ()\r\n#12 0x00005555556d50a9 in PyEval_EvalFrameEx ()\r\n#13 0x00005555556d9286 in ?? ()\r\n#14 0x00005555556d50a9 in PyEval_EvalFrameEx ()\r\n#15 0x00005555556d9286 in ?? ()\r\n#16 0x00005555556d50a9 in PyEval_EvalFrameEx ()\r\n#17 0x00005555556d493f in PyEval_EvalFrameEx ()\r\n#18 0x00005555556da0df in PyEval_EvalCodeEx ()\r\n#19 0x00005555557167bf in ?? ()\r\n#20 0x000055555575d647 in PyObject_Call ()\r\n#21 0x000055555567d94e in ?? ()\r\n#22 0x000055555575d647 in PyObject_Call ()\r\n#23 0x000055555571dc46 in ?? ()\r\n#24 0x000055555575d647 in PyObject_Call ()\r\n#25 0x00005555556d2180 in PyEval_EvalFrameEx ()\r\n#26 0x00005555556da0df in PyEval_EvalCodeEx ()\r\n#27 0x00005555557165d3 in ?? ()\r\n#28 0x000055555575d647 in PyObject_Call ()\r\n#29 0x000055555567d94e in ?? ()\r\n#30 0x000055555575d647 in PyObject_Call ()\r\n#31 0x000055555571dc46 in ?? ()\r\n#32 0x000055555575d647 in PyObject_Call ()\r\n#33 0x00005555556d4ee1 in PyEval_EvalFrameEx ()\r\n#34 0x00005555556d9286 in ?? ()\r\n#35 0x00005555556d9f9f in PyEval_EvalCode ()\r\n#36 0x00005555556b89bf in PyRun_StringFlags ()\r\n#37 0x00005555557a9f3c in PyRun_SimpleStringFlags ()\r\n#38 0x00005555557d8602 in Py_Main ()\r\n#39 0x0000555555668c01 in main ()\r\n```\r\n\r\nI think it's an instance of https://github.com/protocolbuffers/protobuf/issues/5251, and I've commented on that bug to inform the protobuf developers.", "Same still here on the official Amazon Deep Learning AMI.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Deep Learning AMI (Amazon Linux) Version 18.0\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.5", "@Salompas Is this still an issue ? We see that you are using old version of tensorflow  1.x which is not actively supported, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors using TF v2, we will get you the right help . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23607\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23607\">No</a>\n"]}, {"number": 23606, "title": "V100 PCIE FP16 PS/worker training data corruption bug occurs when distributed over gRPC", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): [tensorflow/benchmarks:cnn_tf_v1.12_compatible](https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.12_compatible)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04-hwe\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0/7.2.1.38-1+cuda9.0\r\n- GPU model and memory: V100-PCIE-32GB, NVIDIA driver version 384.145, with `intel_iommu=on`\r\n\r\n```\r\n$ cat /proc/cmdline\r\nBOOT_IMAGE=/boot/vmlinuz-4.15.0-38-generic root=/dev/mapper/vgroot-lvroot ro intel_iommu=on\r\n```\r\n```\r\n$ nvidia-smi topo -m\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\r\nGPU0\t X \tPIX\tNODE\tNODE\t0-9,20-29\r\nGPU1\tPIX\t X \tNODE\tNODE\t0-9,20-29\r\nGPU2\tNODE\tNODE\t X \tPIX\t0-9,20-29\r\nGPU3\tNODE\tNODE\tPIX\t X \t0-9,20-29\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n**Describe the current behavior**\r\nFP-16 multi-GPU training with CPU as local parameter server is converging in single process mode, but diverging loss value (nan) adding another loopback PS process with grpc. Consistent behaviours for both ResNet-50 and VGG16. \r\n\r\n**Describe the expected behavior**\r\nThe same training procedure should yield exactly same result using only 1 worker with/without 1 PS. Not sure why adding SendRecvOps causes data corruption. See other info below.\r\n\r\n**Code to reproduce the issue**\r\n\r\nResNet50 local with [output](https://gist.github.com/byronyi/4f0821a4792b479bf884ff81171c4011):\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --variable_update=parameter_server \\\r\n  --local_parameter_device=cpu \\\r\n  --model=resnet50 \\\r\n  --num_gpus=4 \\\r\n  --use_fp16 \\\r\n  --batch_size=256\r\n```\r\n\r\nVGG16 local with [output](https://gist.github.com/byronyi/0dc379b8feac762bc8d04cab2b240060):\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --variable_update=parameter_server \\\r\n  --local_parameter_device=cpu \\\r\n  --model=vgg16 \\\r\n  --num_gpus=4 \\\r\n  --use_fp16 \\\r\n  --batch_size=256\r\n```\r\n\r\nDistributed using the same PS command with [output](https://gist.github.com/byronyi/ea508b150d1e55b0ac41502c48685b7c):\r\n```\r\nCUDA_VISIBLE_DEVICES= \\\r\npython benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --ps_hosts=localhost:5000 \\\r\n  --worker_hosts=localhost:5001 \\\r\n  --job_name=ps \\\r\n  --local_parameter_device=cpu \\\r\n  --task_index=0 \\\r\n  --server_protocol=grpc\r\n```\r\n\r\nResNet50 distributed with [output](https://gist.github.com/byronyi/de05ccb7b481932ce7be91f889f41aee):\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --ps_hosts=localhost:5000 \\\r\n  --worker_hosts=localhost:5001 \\\r\n  --job_name=worker \\\r\n  --task_index=0 \\\r\n  --server_protocol=grpc \\\r\n  --variable_update=parameter_server \\\r\n  --local_parameter_device=cpu \\\r\n  --model=resnet50 \\\r\n  --num_gpus=4 \\\r\n  --use_fp16 \\\r\n  --batch_size=256\r\n```\r\n\r\nVGG16 distributed with [output](https://gist.github.com/byronyi/1113a5cd8048f729a2c73df5bfd97017):\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --ps_hosts=localhost:5000 \\\r\n  --worker_hosts=localhost:5001 \\\r\n  --job_name=worker \\\r\n  --task_index=0 \\\r\n  --server_protocol=grpc \\\r\n  --variable_update=parameter_server \\\r\n  --local_parameter_device=cpu \\\r\n  --model=vgg16 \\\r\n  --num_gpus=4 \\\r\n  --use_fp16 \\\r\n  --batch_size=256\r\n```\r\n\r\n**Other info / logs**\r\nKernel reports `[DMA Write] PTE Write access is not set` for one of the GPU.\r\nDetailed log could be found [here](https://gist.github.com/dd1e75702116d941d95dfa1fe17b8e27).", "comments": ["I will try to reproduce this problem on one of our NVLink V100-SXM2 machine.", "Seems duplicated with #22463. Closing for now.", "Disabling IOMMU temporarily resolves this issue. \r\n\r\nI am still wondering though, as it seems distributed runtime triggers GPU P2P transfer even with CPU as local PS, but the same error does not occur in local training. Is this behaviour intended? \r\n\r\n@poxvoculi Paul, is there anything you feel familiar here?", "Byron,  we'll try to take a look at this before long, to see whether we can reproduce.\r\n\r\nWhat do you mean by \"it seems distributed runtime triggers GPU P2P transfer even with CPU as local PS\"?  Are you seeing memcpy's direct between GPUs?  A lot of strange optimizations have been done to the TF CNN benchmark suite, so it's possible some kind of cross-device ops are intentionally done even in the CPU-as-ps case.", "After turning off IOMMU, I have tested P2P connectivity using [p2pBandwidthLatencyTest](https://github.com/NVIDIA/cuda-samples/blob/master/Samples/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.cu) in NVIDIA's CUDA samples. It does report dubious results, showing 1/10 bandwidth and 5000x higher latency in P2P writes when crossing a single PCIe switch. \r\n\r\n```\r\n$ nvidia-smi topo -m\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\r\nGPU0\t X \tPIX\tNODE\tNODE\t0-9,20-29\r\nGPU1\tPIX\t X \tNODE\tNODE\t0-9,20-29\r\nGPU2\tNODE\tNODE\t X \tPIX\t0-9,20-29\r\nGPU3\tNODE\tNODE\tPIX\t X \t0-9,20-29\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n$ ./cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest\r\n[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\r\nDevice: 0, Tesla V100-PCIE-32GB, pciBusID: 1b, pciDeviceID: 0, pciDomainID:0\r\nDevice: 1, Tesla V100-PCIE-32GB, pciBusID: 1e, pciDeviceID: 0, pciDomainID:0\r\nDevice: 2, Tesla V100-PCIE-32GB, pciBusID: 3d, pciDeviceID: 0, pciDomainID:0\r\nDevice: 3, Tesla V100-PCIE-32GB, pciBusID: 41, pciDeviceID: 0, pciDomainID:0\r\nDevice=0 CAN Access Peer Device=1\r\nDevice=0 CAN Access Peer Device=2\r\nDevice=0 CAN Access Peer Device=3\r\nDevice=1 CAN Access Peer Device=0\r\nDevice=1 CAN Access Peer Device=2\r\nDevice=1 CAN Access Peer Device=3\r\nDevice=2 CAN Access Peer Device=0\r\nDevice=2 CAN Access Peer Device=1\r\nDevice=2 CAN Access Peer Device=3\r\nDevice=3 CAN Access Peer Device=0\r\nDevice=3 CAN Access Peer Device=1\r\nDevice=3 CAN Access Peer Device=2\r\n \r\n***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\r\nSo you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\r\n \r\nP2P Connectivity Matrix\r\n     D\\D     0     1     2     3\r\n     0             1     1     1     1\r\n     1             1     1     1     1\r\n     2             1     1     1     1\r\n     3             1     1     1     1\r\nUnidirectional P2P=Disabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3\r\n     0 714.12   9.62  10.70  10.97\r\n     1   9.73 715.43  10.73  10.97\r\n     2  10.97  10.70 716.74   9.65\r\n     3  11.02  10.71   9.68 718.06\r\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\r\n   D\\D     0      1      2      3\r\n     0 714.12   0.72  10.05   7.09\r\n     1   0.72 724.72   9.53   7.09\r\n     2   9.66   7.09 723.38   0.72\r\n     3   9.41   7.09   0.72 723.38\r\nBidirectional P2P=Disabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3\r\n     0 741.95  10.08  15.32  13.81\r\n     1  10.09 744.76  13.96  13.79\r\n     2  15.33  13.86 741.22  10.09\r\n     3  13.84  13.71  10.09 746.18\r\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3\r\n     0 741.22   1.29  18.86  14.17\r\n     1   1.30 749.04  14.15  14.17\r\n     2  18.86  14.18 745.47   1.30\r\n     3  14.15  14.17   1.30 745.47\r\nP2P=Disabled Latency Matrix (us)\r\n   GPU     0      1      2      3\r\n     0   3.61  19.62  19.47  19.91\r\n     1  17.79   3.70  17.81  18.39\r\n     2  18.49  18.52   3.61  18.50\r\n     3  17.77  17.85  17.57   3.65\r\n \r\n   CPU     0      1      2      3\r\n     0   4.84  11.82  11.91  11.81\r\n     1  11.83   4.78  11.83  12.15\r\n     2  11.84  11.68   4.72  11.68\r\n     3  11.82  11.71  11.67   4.65\r\nP2P=Enabled Latency (P2P Writes) Matrix (us)\r\n   GPU     0      1      2      3\r\n     0   3.66 49262.93  10.84  10.89\r\n     1 49259.48   3.76  10.75  10.82\r\n     2  10.75  10.87   3.69 49259.81\r\n     3  10.72  10.80 49262.71   3.80\r\n \r\n   CPU     0      1      2      3\r\n     0   4.79   3.86   3.88   3.83\r\n     1   3.95   4.90   3.92   3.84\r\n     2   3.92   3.88   4.82   3.86\r\n     3   4.07   4.03   4.02   4.82\r\n \r\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\r\n```\r\n\r\nThe hardware model involved is [SuperMicro 4029GP-TRT2](https://www.supermicro.com/products/system/4U/4029/SYS-4029GP-TRT2.cfm). We suspect it is a hardware/firmware related issue and we have reported this issue to the manufacturer. Will let you know if there is any updates from them.", "Patching the [latest BIOS firmware](https://www.supermicro.com/Bios/softfiles/6227/P-X11DPG-OT-CPU_BIOS_2_1_release_notes.pdf) resolves the issue completely. I will leave it here and hope it could be helpful if someone else meets the same problem.", "Thanks a lot Byronyi! We will try this for our 8x Titan V, SuperMicro 4029GP-TRT Server. It shows very similar issues with the PIX connections.", "And it worked! :)"]}, {"number": 23604, "title": "c_api.h Why GPU is slower than CPU", "body": "hello i tried the c_api .I have trained a lstm mode and i load the model in my c project to predict my data.I find the GPU is slowly than CPU.\r\ngpu:nvidia TITAN X\r\nCUDA\uff1a9.0\r\ncudnn:7.0\r\ncpu: intel E5\r\ntensorflow: 1.11.0\r\ni predicted about 200 data and every data call the function \uff1a\r\nTF_SessionRun(sess,\r\nnullptr, // Run options.\r\n&input_op, &input_tensor, 1, // Input tensors, input tensor values, number of inputs.\r\n&out_op, &output_tensor, 1, // Output tensors, output tensor values, number of outputs.\r\nnullptr, 0, // Target operations, number of targets.\r\nnullptr, // Run metadata.\r\nstatus // Output status.\r\n);\r\nevery time GPU is slowly than CPU\r\nIs my method wrong ?Is there a way to increase the speed? and can I enter data in batches for prediction?how to feed data in batches?\r\nthanks", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think it's a bug, please fill this [template ](https://github.com/tensorflow/tensorflow/issues/new/choose)and open a new issue. Thanks !\r\n", "> This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow)\r\n\r\n+1"]}]