[{"number": 33286, "title": "lite/benchmarks: Add missing ms unit", "body": "", "comments": ["@duncandean Can you please resolve conflicts? Thanks!"]}, {"number": 33285, "title": "Failing to download MNIST dataset at load_data()", "body": "**System information**\r\n- OS Platform and Distribution: macOS version10.15\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7\r\n- Installed using: pip install\r\n- Bazel version (if compiling from source): 1.0.0\r\n\r\n\r\n\r\nI have tried the first beginner example:\r\n\r\n`from __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\n\r\nmodel.evaluate(x_test,  y_test, verbose=2)`\r\n\r\nBut it always seems to break at `(x_train, y_train), (x_test, y_test) = mnist.load_data()`. The programs run fine on Colaboratory but if I try to run in locally on Terminal, it fails. Seems to be a certification issue.\r\n\r\nI have download installed everything, even upgraded just to be sure.\r\n\r\n\r\nHere are the logs that are printed out:\r\n\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\", line 1317, in do_open\r\n    encode_chunked=req.has_header('Transfer-encoding'))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\", line 1229, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\", line 1275, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\", line 1224, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\", line 1016, in _send_output\r\n    self.send(msg)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\", line 956, in send\r\n    self.connect()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\", line 1392, in connect\r\n    server_hostname=server_hostname)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\", line 412, in wrap_socket\r\n    session=session\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\", line 853, in _create\r\n    self.do_handshake()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\", line 1117, in do_handshake\r\n    self._sslobj.do_handshake()\r\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/DanialZikri/venv37/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/data_utils.py\", line 251, in get_file\r\n    urlretrieve(origin, fpath, dl_progress)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\", line 247, in urlretrieve\r\n    with contextlib.closing(urlopen(url, data)) as fp:\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\", line 222, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\", line 525, in open\r\n    response = self._open(req, data)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\", line 543, in _open\r\n    '_open', req)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\", line 503, in _call_chain\r\n    result = func(*args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\", line 1360, in https_open\r\n    context=self._context, check_hostname=self._check_hostname)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py\", line 1319, in do_open\r\n    raise URLError(err)\r\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/DanialZikri/Documents/TensorflowFirst.py\", line 7, in <module>\r\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n  File \"/Users/DanialZikri/venv37/lib/python3.7/site-packages/tensorflow_core/python/keras/datasets/mnist.py\", line 50, in load_data\r\n    '731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1')\r\n  File \"/Users/DanialZikri/venv37/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/data_utils.py\", line 255, in get_file\r\n    raise Exception(error_msg.format(origin, e.errno, e.reason))\r\nException: URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)\r\n", "comments": ["# Incase of linux...\r\ngo to **.local/python3.X/lib/python3.6/site-packages/keras/utils/data_utils.py**  \r\n\r\n## and below import statements add these----\r\n\r\n```import requests\r\nrequests.packages.urllib3.disable_warnings()\r\nimport ssl\r\n\r\ntry:\r\n    _create_unverified_https_context = ssl._create_unverified_context\r\nexcept AttributeError:\r\n    # Legacy Python that doesn't verify HTTPS certificates by default\r\n    pass\r\nelse:\r\n    # Handle target environment that doesn't support HTTPS verification\r\n    ssl._create_default_https_context = _create_unverified_https_context\r\n```\r\n\r\n> now try new instance of python and ..hopefully it works  :smile_cat: ", "Thanks a lot! It works great with a one change, I had to remove \r\n> requests.packages.urllib3.disable_warnings()\r\n\r\nbecause the name requests was never specified. But besides that, it worked!!", "oh my bad....I forgot an import statement there...\r\n\r\n```import requests```  before the call...\r\nso, it should be...\r\n```\r\nimport requests\r\nrequests.packages.urllib3.disable_warnings()\r\nimport ssl\r\n\r\ntry:\r\n    _create_unverified_https_context = ssl._create_unverified_context\r\nexcept AttributeError:\r\n    # Legacy Python that doesn't verify HTTPS certificates by default\r\n    pass\r\nelse:\r\n    # Handle target environment that doesn't support HTTPS verification\r\n    ssl._create_default_https_context = _create_unverified_https_context\r\n```\r\nanyways...happy that it worked :smiley: ", "Closing since the issue is resolved.If any other issue is faced feel free to open a new issue.Thanks!", "On Mac, for me, the file was located at ~/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/utils/data_utils.py (not in the keras folder)", "Working with **virtualenv**:\r\n`env/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/data_utils.py`", "Many many thanks!!!", "Awesome ! This really helped. Can I not add this piece of code into my local program rather than modifying core programs of keras?", "For some of you the path to the file might be:\r\n/venv/lib/python3.8/site-packages/tensorflow/python/keras/utils\r\n", "> Awesome ! This really helped. Can I not add this piece of code into my local program rather than modifying core programs of keras?\r\n\r\nDid you find any solution which doesn't require to modify core system/Program files", "for me(MAC) it was ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/utils/data_utils.py", "Why is this closed. This seems to be a common issue. @oanush", "Seems like this might be a duplicate of https://github.com/tensorflow/tensorflow/issues/10779. The problem _may_ be that you don't have the appropriate certificates installed for python 3. On OSX you can install them with:\r\n```\r\n/Applications/Python\\ 3.6/Install\\ Certificates.command\r\n```\r\nRemember to replace the python version with your own version.\r\n", "> Awesome ! This really helped. Can I not add this piece of code into my local program rather than modifying core programs of keras?\r\n\r\nYes you can ! in my case, just paste everything from import ssl on before you load the database. ", "That works! Installing python certificates solve my problem. Thanks", "On Mac : go to Applications then Python and Install certificates.command.  fix it", "/Applications/Python\\ 3.6/Install\\ Certificates.command\r\nwork for me Thank you \ud83d\udc4d ", "> Seems like this might be a duplicate of #10779. The problem _may_ be that you don't have the appropriate certificates installed for python 3. On OSX you can install them with:\r\n> \r\n> ```\r\n> /Applications/Python\\ 3.6/Install\\ Certificates.command\r\n> ```\r\n> \r\n> Remember to replace the python version with your own version.\r\n\r\nThis works for me", "> On Mac : go to Applications then Python and Install certificates.command. fix it\r\n\r\nyou save me.", "adding the code to my data_utils.py file didn't work for me \r\nhow can in install certificates.command on windows \r\ni am working with python 3.7 tensorflow 2.5", "very useful, thks!"]}, {"number": 33284, "title": "Fix android demo build", "body": "Seems like the gcc version is too old to support `-std=c++14` option", "comments": ["ping", "ping", "I can see that build is green. Am I missing something?", "What version of the NDK are you using? GCC support was removed in r18, and our minimum recommended version is r17b.", "I'm judging by CI, for example, see\r\nhttps://source.cloud.google.com/results/invocations/8098f5a1-45bd-41ca-85f4-86dfa7f89ddc/log", "Thanks for flagging, taking a look now at the ci_build configuration to make sure it's using the right toolchain. "]}, {"number": 33283, "title": "pickling showing error : <<TypeError: can't pickle _thread._local objects>> working with tensorflow2.0 while it works with tensorflow 1.14", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.11 (stretch)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): via pip\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: Python 3.7.3\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behaviour**\r\nCurrently, if I pickle in 2.0, it gives error TypeError: can't pickle _thread._local objects whereas in tensorflow version 1.14, it works fine\r\n\r\n**Describe the expected behavior\r\n[tp.txt](https://github.com/tensorflow/tensorflow/files/3720599/tp.txt)\r\n**\r\npickling should work in in tf 2.0\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nin the attached file, run get_model method for tf2.0  and get_model_prev to run for tf1.14.\r\nYou will see that pickle model is generated for 1.14 but not for 2.0\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@tseth92, I tried to replicate the reported issue but i got different error message. Please see the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/4e14c5835a91961694b93612c9bc14a5/untitled194.ipynb) and help us provide the issue. Thanks!", "@gadagashwini I just ran the gist, the code will work fine if you choose !pip install tensorflow==1.14\r\n\r\nBut if you choose !pip install tensorflow==2.0 and if you uncomment get_model() and comment get_model_prev(), you will get to see the issue:\r\n```\r\nmodel = get_model()\r\n#model = get_model_prev()\r\n```\r\nError: \t\r\n**TypeError: can't pickle _thread._local objects**\r\n\r\nI have added these two methods to show that with get_model_prev() ie. model with normal Keras, its working fine and able to pickle if you use previous version of tensorflow and use keras to build layers.\r\n\r\nBut, if you try to use tf 2.0 directly and build layers using that, it shows error as in get_model()\r\nTypeError: can't pickle _thread._local objects", "I could reproduce the issue with changes mentioned in last comment. Please see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/079ecb24dcfb0130a41c4cc8f7a69aba/untitled209.ipynb). Thanks! ", "Closing as duplicate of #33204 (although different error, the main concept is that thread dynamic objects cannot be pickled)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33283\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33283\">No</a>\n", "I'm having the same problem", "@AlexFuster,\r\nPlease post a new issue and provide all the information asked by the template. We ask this because it's more efficient to have one thread dedicated to one issue. Thanks!", "But my problem is literally the one described in this issue. Why not to reopen it?", "@gadagashwini I also need to reopen the issue as the issue linked by @mihaimaruseac is different from this one. I have raised concern over this via tensorflow-bot, but no response form tensorflow team i received. Please reopen this issue as there is no reason to create a new issue when one unresolved issue is already present. Thanks", "@mihaimaruseac Can you PTAL? Thanks!", "Afaik pickling dynamic objects is not supported. I am not a good assignee if that changed.", "@tseth92 @AlexFuster Did the above comment help you in understanding the root of the problem?", "@gowthamkpr if someone can really confirm that pickling is not supported for dynamic objects, then this should be considered as a feature request as it is an important part of TF.", "Totally agree with @tseth92 , pickling the entire model object is easier than saving just the weights. Especially for complex models with multiple networks involved. It is very convenient as long as it does not introduce performance issues.", "@tseth92 @AlexFuster I am going to close this issue. Please create a new issue with the feature request template as mentioned [here](https://github.com/tensorflow/tensorflow/issues/new/choose) and we can take it forward from there. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33283\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33283\">No</a>\n", "Did somebody make the feature request?", "@renatobellotti  I don't think so", "I'm looking at Tensorflow's code and the problem is produced by the following line in the \\_\\_init\\_\\_ methods of ```tensorflow.python.keras.engine.base_layer.Layer``` and ```tensorflow.python.keras.engine.network.Network``` (which are superclasses of tf.keras.Model)\r\n```python\r\nself._thread_local = threading.local()\r\n```\r\nIt creates an attribute which is a local threading  wrapper that cannot be serialized by pickle.\r\nUnfortunately that line is there for a reason. It cannot just be commented.\r\n\r\nThe simplest way to reproduce the issue is by doing this:\r\n```python\r\nimport pickle\r\nimport threading\r\npickle.dumps(threading.local())\r\n```\r\nI haven't found a fix yet, though\r\n", "Hey guys, came out with a solution. It's ugly but it seems to work. I hope it helps   \r\nBy the way, I'm using dill because pickle does not serialize weakref objects.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport dill\r\nimport threading\r\ndef extractfromlocal(model): #extracts attributes from the local thrading container\r\n    model._thread_local=model._thread_local.__dict__\r\n    for attr in model.__dict__.values():\r\n        if '_thread_local' in dir(attr):\r\n            extractfromlocal(attr)\r\n\r\ndef loadtolocal(model): #puts attributes back to the local threading container\r\n    aux=threading.local()\r\n    aux.__dict__.update(model._thread_local)\r\n    model._thread_local = aux\r\n    for attr in model.__dict__.values():\r\n        if '_thread_local' in dir(attr):\r\n            loadtolocal(attr)\r\n            \r\ndef save_tf_model(model): #saves the model\r\n    extractfromlocal(model)\r\n    with open('mymodel.pkl','wb') as f:\r\n        dill.dump(model,f)\r\n    loadtolocal(model)\r\n\r\ndef load_tf_model(model):#loads the model\r\n    with open('mymodel.pkl','rb') as f:\r\n        model=dill.load(f)\r\n        loadtolocal(model)\r\n    return model\r\n\r\n#just a quick example of this working\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(2)\r\n\r\n    def call(self, x):\r\n        return self.d(x)\r\n\r\ndata=tf.random.normal((2, 3))\r\nmodel = Model()\r\nprint('Before saving',model(data))\r\nsave_tf_model(model)\r\nprint('After saving',model(data))\r\nmodel=load_tf_model(model)\r\nprint('After loading',model(data))\r\n```", "\r\n> Did somebody make the feature request?\r\n\r\n@renatobellotti  I just did [#36897](https://github.com/tensorflow/tensorflow/issues/36897). It is the first time I use GitHub to ask for a feature so I hope it is well written.", "> > Did somebody make the feature request?\r\n> \r\n> @renatobellotti I just did [#36897](https://github.com/tensorflow/tensorflow/issues/36897). It is the first time I use GitHub to ask for a feature so I hope it is well written.\r\n\r\nThank you! :)", "Doesn't fix the issue, but a potential workaround.\r\nUse keras version 2.3.1 and tensorflow 2.X, and use keras rather than tf.keras and the pickling works:\r\n\r\n```\r\n!pip install tensorflow\r\n!pip install keras==2.3.1\r\n\r\nimport numpy as np\r\nimport keras\r\nimport tensorflow as tf\r\nimport pickle\r\n\r\ndef keras_model():\r\n    model = keras.Sequential()\r\n    model.add(keras.layers.Dense(20, input_shape=(1,), activation='relu'))\r\n    model.add(keras.layers.Dense(1))\r\n    model.compile(loss='mse', optimizer='adam')\r\n    return model\r\n\r\ndef tf_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Dense(20, input_shape=(1,), activation='relu'))\r\n    model.add(tf.keras.layers.Dense(1))\r\n    model.compile(loss='mse', optimizer='adam')\r\n    return model\r\n\r\ndef data():\r\n    n_samples = 1000\r\n    mid_range = 10 \r\n    X = np.random.random((n_samples,1))*mid_range-(mid_range/2)\r\n    y = X*X\r\n    return X,y\r\n\r\ndef train():\r\n    if tf_or_keras == 'tf':\r\n        model = tf_model()\r\n    elif tf_or_keras == 'keras':\r\n        model = keras_model()\r\n    X,y = data()\r\n    h = model.fit(X, y,\r\n               epochs=10,\r\n               verbose=1)\r\n    pickle.dump(model, open(tf_or_keras, 'wb'))\r\n\r\ntf_type = tf.__version__\r\nprint(tf_type)\r\nkeras_type = keras.__version__\r\nprint(keras_type)\r\ntf_or_keras = 'keras'\r\ntrain()\r\n```", "I had the same issue.\r\nUsing keras instead of tensorflow.keras solved the issue.\r\nThank you."]}, {"number": 33282, "title": "Fix a minor typo", "body": "", "comments": []}, {"number": 33281, "title": "Catnot catch `InvalidArgumentError` exception", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): - \r\n- TensorFlow version (use command below): 1.15 and 2.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.0.0-rc1-51-g2646d23 2.0.0-rc2\r\n\r\n**Describe the current behavior**\r\nException fails without being able to catch with \r\n`except tf.errors.InvalidArgumentError as exception:`\r\n\r\n**Describe the expected behavior**\r\nI expect, to be able to handle the mentioned example\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1IEg-KbY08dnCNnZSiuGb0-jToUDtQ_Vi#scrollTo=Xll7CUHiYn1B\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I think I was to handle exceptions in such way some time ago, unfortunately, I can not anymore find a working solution neither for 1.x neither for 2.0", "Your `decode_jpeg_and_label` Python function is only executed once to build the function graph and `try` and `except` statements will have no effect at that.\r\n\r\nThe actual execution of the logic happens when `next` is called (as part of `for elem in dataset:`), so if you would like to catch the error you could do the following:\r\n\r\n```\r\ndataset = ...\r\niterator = iter(dataset)\r\n\r\nwhile True:\r\n  try:\r\n    elem = next(iterator)\r\n    ...\r\n  except InvalidArgumentError:\r\n    ...\r\n  except StopIteration:\r\n    break\r\n```\r\n \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33281\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33281\">No</a>\n", "@jsimsa Thank you for explanation. The only difference, seems like we need to catch `StopIteration` on the last next call", "You are right, I updated my response above to reflect that."]}, {"number": 33280, "title": "tensorflow 1.14 eigen using failed", "body": "tensorflow version:1.14\r\ntensorflow lib compile ok, but i use c api to embed into my app, make build error below:\r\n\r\nIn file included from _external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/Tensor:120:0,\r\n                 from _external/usr/local/include/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from _external/usr/local/include/tensorflow/tensorflow/core/framework/tensor.h:21,\r\n                 from _external/usr/local/include/tensorflow/tensorflow/core/public/session.h:24,\r\n                 from _external/usr/local/include/tensorflow/tensorflow/cc/saved_model/loader.h:26,\r\n                 from ./apps/frame/util/tensorflow_util.h:7,\r\n                 from build/release64/apps/frame/data_loader/tensorflow_loader.h:5,\r\n                 from build/release64/apps/frame/data_loader/ServiceDataLoaderFactory.cpp:5:\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function 'static void Eigen::internal::TensorBlockIO<Scalar, StorageIndex, NumDims, Layout, BlockRead>::Copy(const Block&, StorageIndex, const Dimensions&, const Dimensions&, const Scalar*, Scalar*)':\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:393:63: error: the value of 'j' is not usable in a constant expression\r\n         if (++block_iter_state[j].count < block_iter_state[j].size) {\r\n                                                               ^\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:392:16: note: 'int j' is not const\r\n       for (int j = 0; j < num_squeezed_dims; ++j) {\r\n                ^\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:393:35: error: parse error in template argument list\r\n         if (++block_iter_state[j].count < block_iter_state[j].size) {\r\n                                   ^\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function 'static void Eigen::internal::TensorBlockCwiseUnaryIO<UnaryFunctor, StorageIndex, OutputScalar, NumDims, Layout>::Run(const UnaryFunctor&, const Dimensions&, const Dimensions&, OutputScalar*, Eigen::array<StorageIndex, NumDims>&, const InputScalar*)':\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:604:21: error: parse error in template argument list\r\n         if (++state.count < state.size) {\r\n                     ^\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function 'static void Eigen::internal::TensorBlockCwiseBinaryIO<BinaryFunctor, StorageIndex, OutputScalar, NumDims, Layout>::Run(const BinaryFunctor&, const Dimensions&, const Dimensions&, OutputScalar*, Eigen::array<StorageIndex, NumDims>&, const LeftScalar*, Eigen::array<StorageIndex, NumDims>&, const RightScalar*)':\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:758:21: error: parse error in template argument list\r\n         if (++state.count < state.size) {\r\n\r\nI try to replace it with 3.3.7 eigen, but tensorflow code is confict with 3.3.7 eigen.\r\nPlease help me!", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n> \r\n> Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> \r\n> We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.25.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\n`\r\nIn file included from _external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/Tensor:120:0,\r\nfrom _external/usr/local/include/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\nfrom _external/usr/local/include/tensorflow/tensorflow/core/framework/tensor.h:21,\r\nfrom _external/usr/local/include/tensorflow/tensorflow/core/public/session.h:24,\r\nfrom _external/usr/local/include/tensorflow/tensorflow/cc/saved_model/loader.h:26,\r\nfrom ./apps/frame/util/tensorflow_util.h:7,\r\nfrom build/release64/apps/frame/data_loader/tensorflow_loader.h:5,\r\nfrom build/release64/apps/frame/data_loader/ServiceDataLoaderFactory.cpp:5:\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function 'static void Eigen::internal::TensorBlockIO<Scalar, StorageIndex, NumDims, Layout, BlockRead>::Copy(const Block&, StorageIndex, const Dimensions&, const Dimensions&, const Scalar*, Scalar*)':\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:393:63: error: the value of 'j' is not usable in a constant expression\r\nif (++block_iter_state[j].count < block_iter_state[j].size) {\r\n^\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:392:16: note: 'int j' is not const\r\nfor (int j = 0; j < num_squeezed_dims; ++j) {\r\n^\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:393:35: error: parse error in template argument list\r\nif (++block_iter_state[j].count < block_iter_state[j].size) {\r\n^\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function 'static void Eigen::internal::TensorBlockCwiseUnaryIO<UnaryFunctor, StorageIndex, OutputScalar, NumDims, Layout>::Run(const UnaryFunctor&, const Dimensions&, const Dimensions&, OutputScalar*, Eigen::array<StorageIndex, NumDims>&, const InputScalar*)':\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:604:21: error: parse error in template argument list\r\nif (++state.count < state.size) {\r\n^\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function 'static void Eigen::internal::TensorBlockCwiseBinaryIO<BinaryFunctor, StorageIndex, OutputScalar, NumDims, Layout>::Run(const BinaryFunctor&, const Dimensions&, const Dimensions&, OutputScalar*, Eigen::array<StorageIndex, NumDims>&, const LeftScalar*, Eigen::array<StorageIndex, NumDims>&, const RightScalar*)':\r\n_external/usr/local/include/tensorflow/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:758:21: error: parse error in template argument list\r\nif (++state.count < state.size) {`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ntensorflow lib compile ok, but i use tensorflow sdk to embed into my app, make build error above.\r\nI try to replace it with 3.3.7 eigen, but tensorflow code is confict with 3.3.7 eigen.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n[kai.ren_tensorflow_sdk.tar.gz](https://github.com/tensorflow/tensorflow/files/3724714/kai.ren_tensorflow_sdk.tar.gz)\r\n", "@goldbalance \r\nThanks for the platform details.Please, provide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "em, i will give you a demo with tensorflow_sdk", "There's nothing we can do for 1.14, are you able to repro with the latest master build? If not, what about 1.15 or the upcoming 2.1 release?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33280\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33280\">No</a>\n", "> There's nothing we can do for 1.14, are you able to repro with the latest master build? If not, what about 1.15 or the upcoming 2.1 release?\r\n\r\nThis is the same with 1.15.3 "]}, {"number": 33279, "title": "TensorFlow import error", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\npip3 install --user tensorflow-gpu\r\n- TensorFlow version:2.0\r\n- Python version:3.7.3 64bit\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0,7.4.1\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nError after installing when importing tensorflow\r\n\r\n\r\nC:\\Users\\Dr Edd>python -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Dr Edd\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip3 install --user tensorflow-gpu\r\npython -c \"import tensorflow as tf\"\r\n\r\n**Any other info / logs**\r\n", "comments": ["Does your CPU support AVX?", "no it's an older corei7 architecture ", "In this case, all you can do is install from source or find a community build that works on a non-AVX processor", "*TensorFlow release binaries (CPU/GPU) version 1.6 and higher are prebuilt with AVX instruction sets.*  \r\nSee [hardware requirements][1] to know more.  \r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:  \r\n\r\n* Try Google Colab to use TensorFlow.    \r\n * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install``` to install any other preferred TF version.  \r\n    * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.  \r\n* Try to build TF from sources by changing CPU optimization flags.\r\n\r\n\r\n  [1]: https://www.tensorflow.org/install/pip#hardware-requirements", "Also, see #19584", "@dreddc \r\nDid you get a chance to go through @mihaimaruseac, @ymodak suggestions and let us know if that helps to resolve the issue. Thanks!\r\n", "@dreddc \r\n\r\nAny update please. Thanks!", "Thanks for all your responses \r\n\r\nDecided to go down build from source option - been busy trying to build TF from sources by changing CPU optimization flags. \r\n\r\nCan't seem to get by TF configuration options stage to specify cuda/cudnn lib locations.\r\nSeems always want to specify linux type setup locations and files\r\nwork in progress....\r\n", "@dreddc \r\n\r\nAny update on this issue please. Please close this thread if it solves your question. Thanks!", "@dreddc \r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33279\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33279\">No</a>\n", "I had also the same issue which actually was occuring due to incompatibility of python and tensorflow versions. Tensorflow didn't support python 3.8 so, I downgraded it to python 3.5 and it worked like a charm..:) ", "Using TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:/Users/SAI CHARAN.DESKTOP-44A9F0G/OneDrive/Desktop/new arya/edithnew.py\", line 20, in <module>\r\n    w = nltk.word_tokenize(pattern)\r\n  File \"C:\\Users\\SAI CHARAN.DESKTOP-44A9F0G\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 129, in word_tokenize\r\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\r\n  File \"C:\\Users\\SAI CHARAN.DESKTOP-44A9F0G\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 106, in sent_tokenize\r\n    tokenizer = load(\"tokenizers/punkt/{0}.pickle\".format(language))\r\n  File \"C:\\Users\\SAI CHARAN.DESKTOP-44A9F0G\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\data.py\", line 752, in load\r\n    opened_resource = _open(resource_url)\r\n  File \"C:\\Users\\SAI CHARAN.DESKTOP-44A9F0G\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\data.py\", line 877, in _open\r\n    return find(path_, path + [\"\"]).open()\r\n  File \"C:\\Users\\SAI CHARAN.DESKTOP-44A9F0G\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\data.py\", line 585, in find\r\n    raise LookupError(resource_not_found)\r\nLookupError: \r\n**********************************************************************\r\n  Resource \u001b[93mpunkt\u001b[0m not found.\r\n  Please use the NLTK Downloader to obtain the resource:\r\n\r\n  \u001b[31m>>> import nltk\r\n  >>> nltk.download('punkt')\r\n  \u001b[0m\r\n  For more information see: https://www.nltk.org/data.html\r\n\r\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\r\n\r\n  Searched in:\r\n    - 'C:\\\\Users\\\\SAI CHARAN.DESKTOP-44A9F0G/nltk_data'\r\n    - 'C:\\\\Users\\\\SAI CHARAN.DESKTOP-44A9F0G\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\nltk_data'\r\n    - 'C:\\\\Users\\\\SAI CHARAN.DESKTOP-44A9F0G\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\share\\\\nltk_data'\r\n    - 'C:\\\\Users\\\\SAI CHARAN.DESKTOP-44A9F0G\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\lib\\\\nltk_data'\r\n    - 'C:\\\\Users\\\\SAI CHARAN.DESKTOP-44A9F0G\\\\AppData\\\\Roaming\\\\nltk_data'\r\n    - 'C:\\\\nltk_data'\r\n    - 'D:\\\\nltk_data'\r\n    - 'E:\\\\nltk_data'\r\n    - ''\r\n**********************************************************************\r\n\r\n\r\n\r\n\r\n\r\n\r\ni am getting the following error while training the chatbot", "(venv) C:\\Users\\Rishi.MSS2015\\Desktop\\FlaskApp>flask run\r\n * Environment: production\r\n   WARNING: This is a development server. Do not use it in a production deployment.\r\n   Use a production WSGI server instead.\r\n * Debug mode: off\r\nUsage: flask run [OPTIONS]\r\n\r\nError: While importing \"app\", an ImportError was raised:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\rishi.mss2015\\desktop\\flaskapp\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\rishi.mss2015\\desktop\\flaskapp\\venv\\lib\\site-packages\\flask\\cli.py\", line 240, in locate_app\r\n    __import__(module_name)\r\n  File \"C:\\Users\\Rishi.MSS2015\\Desktop\\FlaskApp\\app.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"c:\\users\\rishi.mss2015\\desktop\\flaskapp\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"c:\\users\\rishi.mss2015\\desktop\\flaskapp\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"c:\\users\\rishi.mss2015\\desktop\\flaskapp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"c:\\users\\rishi.mss2015\\desktop\\flaskapp\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\users\\rishi.mss2015\\desktop\\flaskapp\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\rishi.mss2015\\desktop\\flaskapp\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nHow to solve this, please help."]}, {"number": 33278, "title": "Fix some meaningless type qualifier warnings", "body": "According to GCC, `const` type qualifier on return type is meaningless.\r\n\r\nThis PR fixes some of these warnings.", "comments": ["> I don't think it is safe to remove const everywhere. For example `int* f()` allows you to write `f() = &another_int` but `const int* f()` doesn't.\r\n\r\n`f()` is not an lvalue, so `f() = &another_int` does not compile:\r\n\r\n```c++\r\nint *f()\r\n{\r\n    return nullptr;\r\n}\r\n\r\nint main()\r\n{\r\n    int another_int = 4;\r\n\r\n    f() = &another_int;\r\n}\r\n```\r\n\r\nproduces the following error:\r\n\r\n```\r\n<source>: In function 'int main()':\r\n<source>:10:12: error: lvalue required as left operand of assignment\r\n   10 |     f() = &another_int;\r\n      |            ^~~~~~~~~~~\r\n```"]}, {"number": 33277, "title": "INT8 calibration error using TrtGraphConverterV2 in TensorFlow2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda10.0\r\n- GPU model and memory: TITAN V, 12GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nUsing TensorRT version in: 5.1.5\r\nWhen using TrtGraphConverterV2 to convert a saved model and trying to calibrate with INT8 support, it will raise the error shown as follow.\r\nShould be mentioned that, the FP16 and FP32 conversation is work fine but only the INT8 is failed.\r\n```\r\n2019-10-12 06:33:06.015951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.5\r\n2019-10-12 06:33:06.142792: E tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:838] Calibration failed: Invalid argument: Pad only supports explicit padding on 4 dimensional tensor, at StatefulPartitionedCall/resnet50/conv1_pad/Pad\r\n2019-10-12 06:33:06.142978: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Failed to feed calibration data\r\n         [[{{node TRTEngineOp_0}}]]\r\nTraceback (most recent call last):\r\n  File \"calibration.py\", line 16, in <module>\r\n    converter.convert(calibration_input_fn=input_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py\", line 984, in convert\r\n    self._converted_func(*map(ops.convert_to_tensor, inp))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1081, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1121, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  Failed to feed calibration data\r\n         [[node TRTEngineOp_0 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_pruned_79677]\r\n\r\nFunction call stack:\r\npruned\r\n\r\nterminate called without an active exception\r\n```\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.applications.ResNet50(weights=None)\r\nout = model(tf.random.normal((2, 224, 224, 3)))\r\ntf.saved_model.save(model, './saved_model/')\r\nprint(model.input_names, model.output_names)\r\n\r\nparams = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode='INT8',\r\n                                                    use_calibration=True)\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='./saved_model/', conversion_params=params)\r\n\r\ndef input_fn():\r\n    for i in range(10):\r\n        yield tf.random.normal((1, 224, 224, 3))\r\nconverter.convert(calibration_input_fn=input_fn)  #  raise error here\r\nconverter.save('./model.trt')\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I also tried another simple model and found that the existing Conv2D layer will lead to error. Also when change to FP16 or FP32, all the convert works fine!! And no matter the data format is channels_first or channals_last, the results are the same.\r\n\r\nCheck this code for more details:\r\n```python\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.func_graph import def_function\r\nfrom tensorflow.python.framework import tensor_spec\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.conv2 = tf.keras.layers.Conv2D(64, (7, 7), strides=(2, 2), padding='valid', data_format='channels_last')\r\n        self.dense = tf.keras.layers.Dense(10, activation='softmax')\r\n\r\n    @def_function.function(input_signature=[\r\n        tensor_spec.TensorSpec(shape=[None, 32, 32, 3], dtype=tf.float32)\r\n    ])\r\n    def call(self, inputs, training=True, **kwargs):\r\n        inputs = self.conv2(inputs)  # remove this line, work fine!!\r\n        x = tf.reshape(inputs, [-1, inputs.shape[-1]])\r\n        x = self.dense(x)\r\n        return x\r\n\r\nmodel = MyModel()\r\nout = model(tf.random.normal((2, 32, 32, 3)))\r\ntf.saved_model.save(model, './saved_model/', {'model_key': model.call})\r\n\r\nparams = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode='INT8',\r\n                                                    use_calibration=True)\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='./saved_model/', conversion_params=params,\r\n                                    input_saved_model_signature_key='model_key')\r\ndef input_fn():\r\n    for i in range(10):\r\n        yield tf.random.normal((1, 32, 32, 3))\r\nconverter.convert(calibration_input_fn=input_fn)\r\nconverter.save('./model.trt')\r\n```", "@ay27 ,\r\nWhen tried executing the given code, [colab](https://colab.sandbox.google.com/gist/oanush/6b73dc27aaff2332555c403e2712b804/33277.ipynb) session crashed for both the given code snippets. Can you check and provide gist of colab to reproduce the issue ?Thanks!", "@oanush ,\r\nI tried to reproduce in colab, the crash reason is the environment in colab does not have the TensorRT package. You can check the running log for details.\r\nI am not familiar with colab, and don't know how to install the package on colab. Can you reproduce the problem in a local CUDA environment?\r\n\r\n![image](https://user-images.githubusercontent.com/3942031/66733274-92ad2480-ee91-11e9-875a-a2190bcea9db.png)\r\n", "Does this issue have any progress?", "I'm also interested. To reproduce in colab one has to install TensorRT 5.1.5\r\n`!apt-get install -y --no-install-recommends libnvinfer5=5.1.5-1+cuda10.0 libnvinfer-dev=5.1.5-1+cuda10.0`", "same problem here\r\npip3 install tensorflow-gpu==2.0.0\r\ntf: v2.0.0-rc2-26-g64c3d38 2.0.0\r\ndpkg -l | grep nvinfer\r\n\r\n> ii  libnvinfer-dev                                              6.0.1-1+cuda10.1                                amd64        TensorRT development libraries and headers\r\nii  libnvinfer5                                                 5.1.5-1+cuda10.1                                amd64        TensorRT runtime libraries\r\nii  libnvinfer6                                                 6.0.1-1+cuda10.1                                amd64        TensorRT runtime libraries\r\n\r\noutput:\r\n>tiny_yolov3_3l.py\", line 100, in __init__\r\n    converter.convert(calibration_input_fn=self.get_calibration_batch)\r\n  File \"/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py\", line 984, in convert\r\n    self._converted_func(*map(ops.convert_to_tensor, inp))\r\n  File \"/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1081, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1121, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  Failed to feed calibration data\r\n\t [[node TRTEngineOp_3 (defined at /home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_pruned_22003]\r\n\r\n>Function call stack:\r\npruned\r\n\r\n>terminate called without an active exception\r\nAborted (core dumped)\r\n\r\n", "@ay27 could you try letting the input_fn return a tuple?\r\n`        yield tf.random.normal((1, 224, 224, 3)),`\r\n", "After making the above change, the code snippet mentioned by @ay27 works well for me.", "> @ay27 could you try letting the input_fn return a tuple?\r\n> ` yield tf.random.normal((1, 224, 224, 3)),`\r\n\r\n@ay27,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33277\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33277\">No</a>\n", "I had a similar problem when trying to convert SSD MobileNet2 to INT8. \r\n\r\nThe error was : \r\n\r\n```\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Input shape axis 0 must equal 1, got shape [320,320,3]\r\n\t [[node StatefulPartitionedCall/Preprocessor/unstack (defined at <ipython-input-68-f0b69da9b8ec>:1) ]]\r\n\t [[StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/Reshape_11/_38]]\r\n  (1) Invalid argument:  Input shape axis 0 must equal 1, got shape [320,320,3]\r\n\t [[node StatefulPartitionedCall/Preprocessor/unstack (defined at <ipython-input-68-f0b69da9b8ec>:1) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_pruned_835619]\r\n\r\nFunction call stack:\r\npruned -> pruned\r\n```\r\n\r\nAdding a comma at the end to specify a tuple data format solved the issue :) \r\n\r\n", "> @ay27 could you try letting the input_fn return a tuple?\r\n> ` yield tf.random.normal((1, 224, 224, 3)),`\r\n\r\nadding that  line raised a new error \r\n\r\nFile \"test.py\", line 21, in <module>\r\n   converter.convert(calibration_input_fn=my_calibration_input_fn)\r\n  File \"/home/vinil/.local/lib/python3.6/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1124, in convert\r\n    self._converted_func(*map(ops.convert_to_tensor, inp))\r\n  File \"/home/vinil/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1669, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/home/vinil/.local/lib/python3.6/site-packages/tensorflow/python/eager/wrap_function.py\", line 247, in _call_impl\r\n    args, kwargs, cancellation_manager)\r\n  File \"/home/vinil/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1687, in _call_impl\r\n    return self._call_with_flat_signature(args, kwargs, cancellation_manager)\r\n  File \"/home/vinil/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1736, in _call_with_flat_signature\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/home/vinil/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/vinil/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"/home/vinil/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute __inference_pruned_63044 as input #0(zero-based) was expected to be a uint8 tensor but is a float tensor [Op:__inference_pruned_63044]`\r\n\r\n\r\n", "@vinilreddy36 , input is float, need to update to tf.cast to int8"]}, {"number": 33276, "title": "update linear to mel documentation", "body": "", "comments": ["cc'ing @rryan again :) I added one more sentence about its normalization behavior. ", "(pinging @tomhennigan :)", "@keunwoochoi Could you please address Ubuntu Sanity errors? Thanks!", "@rryan do you know if the equation symbols `$$` or something would've caused the error above? that error doesn't show any details so i'm making a guess now. d", "> @rryan do you know if the equation symbols `$$` or something would've caused the error above? that error doesn't show any details so i'm making a guess now. d\r\n\r\ntbh I have no idea :) \r\n\r\nLooking at something that works:\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/lstsq\r\nhttps://github.com/tensorflow/tensorflow/blob/35a015349bab8f2e3276d843427a4501e56d18b6/tensorflow/python/ops/linalg_ops.py#L193-L204\r\n\r\nIt looks like you need to use `\\\\(` and `\\\\)` to wrap expressions.\r\n\r\n", "Er, actually scratch that. `$$` works fine for standalone lines. \r\n\r\nI actually looked at the log this time :). The linter is unhappy:\r\n```\r\nFAIL: Found 1 non-whitelisted pylint errors:\r\ntensorflow/python/ops/signal/mel_ops.py:113: [C0301(line-too-long), ] Line too long (83/80)\r\n```", "@rryan Thanks! Fixed now. "]}, {"number": 33275, "title": "Installation to shared (global) python env via Conda.", "body": "I understand that Conda can be used to install older versions of Tensorflow, but it appears difficult with TF 2.0.  Most articles refer to creating the environment via Conda, but then installing via pip after the env is activated.  Given tales of problems when mixing install methods, it seems safer to find an installation method using Conda directly.  Looking for methods or projected timeline when this will be available.\r\n", "comments": ["@MGarvinNYC ,\r\nHi can you please try creating a pip environment and then install TF using pip in the same. As we don't support issues with conda environment, refer the [link](https://github.com/tensorflow/tensorflow/issues/33206#issuecomment-541148907).Thanks!", "@MGarvinNYC ,\r\nAny update on the issue?thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33275\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33275\">No</a>\n", "Issue was not resolved in this thread.  The goal was to maintain compatibility with Conda environments, and pip does not do that.\r\n\r\nBut it should be noted that as of a couple days ago, Tensorflow 2.0 has been added to the Conda repo, which should solve problems with compatibility\r\n\r\n"]}, {"number": 33274, "title": "overlap backward and optimization in TensorFlow 2.0", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nA typical training process in TensorFlow 2.0 could be as following:\r\n\r\n```python\r\ndef train(model, dataset, optimizer):\r\n  for x, y in dataset:\r\n    with tf.GradientTape() as tape:\r\n      prediction = model(x)\r\n      loss = loss_fn(prediction, y)\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n```\r\nFirst, it does backward computation and gets all gradients. Then, the gradients will be updated in local, or send to a remote node to do optimization. The backward computation and gradient optimization are sequential.\r\n\r\nIn a distributed training scenario, sending gradients will be time-consuming. It's better to overlap communication and computation. Once one layer completes its backward computation, it could start sending its gradients out, and do optimization.\r\n\r\nI am wondering if there is any approach to achieve such a goal.\r\n\r\nThanks!\r\n\r\n**Will this change the current api? How?**\r\n\r\nOr could we expose a callback interface, to let users insert customized processing logic after a gradient is calculated?\r\n\r\nThere are different cases:\r\n\r\n- Case 1, optimization in the remote parameter server\r\n\r\n```python\r\ngradients = tape.gradient(loss, model.trainable_variables, send_callback)\r\n```\r\n\r\n- Case 2, gradients need to be averaged by allreduce operation across nodes.\r\n\r\n```python\r\ngradients = tape.gradient(loss, model.trainable_variables, allreduce_callback)\r\noptimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n```\r\n\r\n- Case 3, gradients are applied in local\r\n\r\n```python\r\ngradients = tape.gradient(loss, model.trainable_variables, None)\r\noptimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n```\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi - if you use tf.function around your gradient computation and aggregation, then the underlying graph can often do these optimizations for you. \r\nI believe they do happen in certain circumstances where it makes sense. But sometimes, combining gradients from multiple layers and doing an all-reduce together is more efficient. Have you noticed an actual performance issue from this? Our goal is to make such optimizations transparent to the user. \r\n\r\ncc @yuefengz @dubey \r\n", "@guptapriya   Thanks for your reply. \r\n\r\nIn some large model training, such as a model with a big embedding table out of a single computer memory, we usually use parameter server distributed strategy. The gradients are pushed to the parameter servers, and then aggregated and applied to parameters stored in parameter servers.\r\n\r\nI notice that there is an Async mode in [EagerExecutor](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/eager/eager_executor.h#L95) in the C++ runtime of TF2.0 Eager version. It launches EagerOperation asynchronously. In addition, there is a `RemoteCopyNode` which supports \"remote-->remote\"/\"remote-->local\"/\"local-->remote\" copy. The node could be launched by EagerExecutor. So, in C++ runtime, this problem is well addressed. The computation and communication are overlapped.\r\n\r\nHowever, we want to achieve such effects in Python without touching or modifying C++ runtime.\r\n\r\nOne way is to support inserting an end callback function/or operator if possible to a backward operator. Once the backward operator finishes its backward computation, the callback function will be launched. Thus, if we could insert `send_gradient`  or `allreduce_gradient` end callback to a backward operator, the computation and communication will be overlapped simply with Python API.\r\n\r\nFollowing is the logic which gets the gradient function of an operator:\r\nhttps://github.com/tensorflow/tensorflow/blob/7b80146babce4a998e4654f5f55e09c3e6f1144d/tensorflow/python/eager/backprop.py#L119-L141\r\n\r\nMaybe we could add another callback argument here.\r\n\r\nI am not sure if I find the right place to add such logic. I am willing to work on this if the callback mechanism is reasonable.", "You should be able to use the async mechanism without having to modify the c++ runtime. @qqfish - is async the default execution mode now in 2.0? If not, what's the python API to enable it?\r\n\r\nAlso, relatedly, @zongweiz is working on exploring ways of overlapping gradient computation and communication for multi worker sync training in TF 2.0.", "For distributed training, it is enabled by default in TF 2.1", "@QiJune Could you please check the above comments. If it is still an issue,please let us know. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33273, "title": "Loading ModelCheckpoint OSError -- 'Permission denied'", "body": "**System information**\r\n- OS Platform and Distribution Win10\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 25.21.14.1771\r\n- GPU model and memory: GeForce RTX 2060 6GB\r\n\r\nI have a model that has a ModelCheckpoint.\r\n```\r\nfilepath = r\"my_filepath\"\r\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, mode='max', monitor='val_accuracy', verbose=2, save_best_only=True)\r\ncallbacks_list = [checkpoint]\r\nmodel.fit(train_dataset, validation_data=y_test_dataset, validation_steps=BATCH_SIZE, callbacks=callbacks_list, epochs=5, verbose=2, steps_per_epoch=(X_train_deleted_nans.shape[0]//BATCH_SIZE))\r\n```\r\nThe model is saved to a directory similar to the tutorial [here](https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_usage).\r\n\r\nI make another model exactly like the first one and when I \r\n`new_model.load_weights(filepath)`\r\n\r\nI get:\r\n\r\n`OSError: Unable to open file (unable to open file: name = 'my_filepath', errno = 13, error message = 'Permission denied', flags = 0, o_flags = 0)`", "comments": ["I did `filepath/model.hdf5` and everything worked great.\r\n\r\nThanks to stephen_mugisha on SO."]}, {"number": 33272, "title": "Simplify bias_add in Conv1D", "body": "`tf.nn.bias_add` has full support for `NCHW` and `NHWC` so we don't need this special case any more.", "comments": ["@tanzhenyu Would be cool to get a review for this. ", "@lgeiger Could you please check reviewer comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 33271, "title": "Fix SqlDataset fails to raise StopIteration issue when combined with batch", "body": "This fix fixes the issue raised in #33253 where SqlDataset fails to raise StopIteration when combined with batch(). The reason was that after all records have been consumed, the extra `next` in the kernel does not return empty record so the iteration will continue in the next round. This fix fixes the issue.\r\n\r\nThis fix fixes #33253.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 33270, "title": "Simplify tf.keras.backend.bias_add", "body": "`tf.nn.bias_add` now supports `channels_first` and inputs up to rank 5. This means that the implementation of `tf.keras.backend.bias_add` can be greatly simplified by relying directly on `tf.nn.bias_add`.  This should also slightly increase performance since in most cases it can directly rely on `tf.nn.bias_add`.", "comments": ["@fchollet Any change you could take a look or reassign the PR?", "@lgeiger Could you please check reviewer comments and keep us posted. Thanks!", "`pylint` wasn't happy about this change, I fixed it in the latest commit. Sorry about that."]}, {"number": 33269, "title": "TFTRT : Dynamic batch size creates  new TRT nodes for each batch size.", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.14\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:T4\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIt seems that  when the batch size is dynamic, every new batch size in the input creates a slew of TRTEngine_op nodes.  Seems to be a very crude and non-scalable way to handle dynamic batching. \r\n\r\n**Describe the expected behavior**\r\nOne node for all batch sizes.\r\n\r\n**Code to reproduce the issue**\r\nAny network, just feed  dynamic batch sizes. \r\n\r\n**Other info / logs**\r\nN/A\r\n", "comments": [" @sgambient \r\nThank you for reporting the issue.Is it possible for you to provide simple standalone code, then it is easy for localizing the issue faster. Thanks !", "@sgambient \r\n\r\nAny update please.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 33268, "title": "Remove unnecessary comparison  in Keras losses", "body": "This removes an unnecessary comparison since `K.clip(x, min_value, None)` is equivalent to `K.maximum(x, min_value)`.", "comments": []}, {"number": 33267, "title": "An application using tensorflow_cc.dll crashes whenever the Tensorflow API is invoked.", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: source\r\n- TensorFlow version: tensorflow v2.0.0 commit 64c3d382cadf7bbe8e7e99884bede8284ff67f56 (HEAD, tag: v2.0.0, origin/r2.0)\r\n- Python version: 3.6.9\r\n- Bazel version: 0.24.1\r\n- Compiler version: Visual Studio, 2017 14.16.27023\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nA tensorflow dynamic link library is built using \r\n\r\n`bazel build --config=opt //tensorflow:tensorflow_cc.dll`\r\n\r\nwith the default configuration (no GPU), except that /arch:AVX2 is used instead of /arch:AVX. The patch \r\n\r\n\tdiff --git a/tensorflow/tools/def_file_filter/def_file_filter.py.tpl b/tensorflow/tools/def_file_filter/def_file_filter.py.tpl\r\n\tindex 329a9bb94e..82146ec933 100644\r\n\t--- a/tensorflow/tools/def_file_filter/def_file_filter.py.tpl\r\n\t+++ b/tensorflow/tools/def_file_filter/def_file_filter.py.tpl\r\n\t@@ -154,6 +154,9 @@ def main():\r\n\t\t   else:\r\n\t\t\t def_fp.write(\"\\t\" + decorated + \" DATA\\n\")\r\n\t\t   taken.add(decorated)\r\n\t+    def_fp.write(\"\\t??0SessionOptions@tensorflow@@QEAA@XZ\\n\")\r\n\t+    def_fp.write(\"\\t?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@Z\\n\")\r\n\t+\r\n\t\t def_fp.close()\r\n\r\n\t   exit_code = proc.wait()\r\n\r\nis applied to the source before the build in order to account for missing symbols (The need for this patch should perhaps be filed in a separate bug-report).\r\n\r\n**Describe the current behavior**\r\nAfter much work I was able to build `tensorflow_cc.dll` and linking it to a custom application without errors. However, the application using the `.dll` crashes whenever `LOG(INFO/WARNING/ERROR)` is called.\r\nAfter inspecting the source code, I see that the macro `LOG(ERROR)` expands to a constructor of the class `tensorflow::internal::LogMessage` which inherits from `std::basic_ostringstream<char>`.\r\nA heap error occurs when the destructor of the base class deletes its internal buffer.\r\n\r\nThe error also seems to occur when calling any function in the tensorflow api which in turn calls the log function, effectively rendering the `.dll` useless.\r\n\r\n**Describe the expected behavior**\r\nThe application should not crash when calling (almost) any function in the tensorflow api.\r\n\r\nI am no expert on dynamic libraries on Windows, but I have read that the `.dll` and the `.exe` have separate heaps. One might speculate that the error could have something to do with this (but in that case I do not understand how). Has anyone actually managed to use `tensorflow_cc.dll` (on Windows) at all? I don't have the same issue on Linux.\r\n\r\nA possible workaround might be to build a static `tensorflow_cc.lib`. Is this possible? For my purposes, it does not matter much if I use a dynamically or a statically linked library.\r\n\r\n**Code to reproduce the issue**\r\n```c++\r\n#define NOMINMAX\r\n#define COMPILER_MSVC\r\n#include \"tensorflow/core/platform/logging.h\"\r\n\r\nint main(int argc, char* argv[]) {\r\n\tLOG(ERROR) << \"Test\";\r\n}\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stack trace produced by the debugger when the crash occurs.\r\n\r\n\tntdll.dll!RtlpBreakPointHeap()\r\n\tntdll.dll!RtlpValidateHeapEntry()\r\n\tntdll.dll!RtlValidateHeap()\r\n\tKernelBase.dll!HeapValidate()\r\n\tucrtbased.dll!_CrtIsValidHeapPointer(const void * block) Line 1407\r\n\t\tat minkernel\\crts\\ucrt\\src\\appcrt\\heap\\debug_heap.cpp(1407)\r\n\tucrtbased.dll!free_dbg_nolock(void * const block, const int block_use) Line 904\r\n\t\tat minkernel\\crts\\ucrt\\src\\appcrt\\heap\\debug_heap.cpp(904)\r\n\tucrtbased.dll!_free_dbg(void * block, int block_use) Line 1030\r\n\t\tat minkernel\\crts\\ucrt\\src\\appcrt\\heap\\debug_heap.cpp(1030)\r\n\tucrtbased.dll!free(void * block) Line 32\r\n\t\tat minkernel\\crts\\ucrt\\src\\appcrt\\heap\\free.cpp(32)\r\n\tmsvcp140d.dll!std::_Crt_new_delete::operator delete(void * _Ptr) Line 73\r\n\t\tat d:\\agent\\_work\\3\\s\\src\\vctools\\crt\\crtw32\\stdhpp\\xlocale(73)\r\n\tmsvcp140d.dll!std::locale::`scalar deleting destructor'(unsigned int)\r\n\tmsvcp140d.dll!std::ios_base::_Ios_base_dtor(std::ios_base * _This) Line 44\r\n\t\tat d:\\agent\\_work\\3\\s\\src\\vctools\\crt\\crtw32\\stdcpp\\ios.cpp(44)\r\n\tmsvcp140d.dll!std::ios_base::~ios_base() Line 478\r\n\t\tat d:\\agent\\_work\\3\\s\\src\\vctools\\crt\\crtw32\\stdhpp\\xiosbase(478)\r\n\tmsvcp140d.dll!std::basic_ios<char,std::char_traits<char> >::~basic_ios<char,std::char_traits<char> >() Line 34\r\n\t\tat d:\\agent\\_work\\3\\s\\src\\vctools\\crt\\crtw32\\stdhpp\\ios(34)\r\n\tmain.exe!tensorflow::internal::LogMessage::`vbase destructor'()\r\n\tmain.exe!main(int argc, char * * argv) Line 7\r\n\t\tat (PATH_TO_PROJECT)\\main.cpp(7)\r\n", "comments": ["I\u2019m having similar odd problems with TensorFlow C++ dll on Windows, but linking statically against all .a files seems to work.\r\n\r\nCan someone please confirm the correct usage of tensorflow_cc.dll on Windows for even a simple example? For example, @dbergh \u2019s above or better still, the \u2018example_trainer\u2019 in \u2018cc/tutorials\u2019. Thanks in advance for your help!", "> I am no expert on dynamic libraries on Windows, but I have read that the `.dll` and the `.exe` have separate heaps. One might speculate that the error could have something to do with this (but in that case I do not understand how). \r\n> ```\r\nI agree with you,that might be the reason.I heard something like 'executable file and shared library will use the same heap now',maybe that's GCC?\r\nLogMessage in namespace tensorflow::internal,so it might be just internal usage,and we shouldn't use it.But we can copy it out to use.\r\nWe are missing documents on C++,QAQ.\r\n\r\n", "@dbergh ,\r\nWe see that you are using older version of tensorflow (1.x) which is not actively supported. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33267\">No</a>\n"]}, {"number": 33266, "title": "Installation of tensorflow 2.0 cpu on windows 10 not working", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nInstallation of tensorflow 2.0 cpu on windows 10 not working. I'm unable to import tensorflow.\r\n**Describe the expected behavior**\r\nRun the command \"python -c \"import tensorflow as tf\" within conda CLI without error.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n1) \r\n#from conda CLI base create new environment\r\nconda create --name tensorflow2_test\r\n2)\r\n#activate environment\r\nconda activate tensorflow2_test\r\n3)\r\n#install pip\r\nconda install pip\r\n4)\r\n#install environment tensorflow 2.0\r\npip install tensorflow\r\n5)\r\n#verify install: \r\npython -c \"import tensorflow as tf\"\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nerror trace:\r\n------------\r\n```\r\n(tensorflow2_test) C:\\~\\install environment\\production>python -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow2_test\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n(tensorflow2_test) C:\\~\\install environment\\production>\r\n```\r\npackage list:\r\n-------------\r\n(tensorflow2_test) C:\\~\\install environment\\production>conda list\r\n# packages in environment at C:\\Anaconda3\\envs\\tensorflow2_test:\r\n#\r\n# Name                    Version                   Build  Channel\r\nabsl-py                   0.8.1                    pypi_0    pypi\r\nastor                     0.8.0                    pypi_0    pypi\r\nca-certificates           2019.8.28                     0\r\ncertifi                   2019.9.11                py37_0\r\ngast                      0.2.2                    pypi_0    pypi\r\ngoogle-pasta              0.1.7                    pypi_0    pypi\r\ngrpcio                    1.24.1                   pypi_0    pypi\r\nh5py                      2.10.0                   pypi_0    pypi\r\nkeras-applications        1.0.8                    pypi_0    pypi\r\nkeras-preprocessing       1.1.0                    pypi_0    pypi\r\nmarkdown                  3.1.1                    pypi_0    pypi\r\nnumpy                     1.17.2                   pypi_0    pypi\r\nopenssl                   1.1.1d               he774522_2\r\nopt-einsum                3.1.0                    pypi_0    pypi\r\npip                       19.2.3                   py37_0\r\nprotobuf                  3.10.0                   pypi_0    pypi\r\npython                    3.7.4                h5263a28_0\r\nsetuptools                41.4.0                   py37_0\r\nsix                       1.12.0                   pypi_0    pypi\r\nsqlite                    3.30.0               he774522_0\r\ntensorboard               2.0.0                    pypi_0    pypi\r\ntensorflow                2.0.0                    pypi_0    pypi\r\ntensorflow-estimator      2.0.0                    pypi_0    pypi\r\ntermcolor                 1.1.0                    pypi_0    pypi\r\nvc                        14.1                 h0510ff6_4\r\nvs2015_runtime            14.16.27012          hf0eaf9b_0\r\nwerkzeug                  0.16.0                   pypi_0    pypi\r\nwheel                     0.33.6                   py37_0\r\nwincertstore              0.2                      py37_0\r\nwrapt                     1.11.2                   pypi_0    pypi\r\n\r\n(tensorflow2_test) C:\\~\\install environment\\production>\r\n", "comments": ["Can you check your CPU to see if it supports AVX, please?", "*TensorFlow release binaries (CPU/GPU) version 1.6 and higher are prebuilt with AVX instruction sets.*  \r\nSee [hardware requirements][1] to know more.  \r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:  \r\n\r\n* Try Google Colab to use TensorFlow.    \r\n * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install``` to install any other preferred TF version.  \r\n    * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.  \r\n* Try to build TF from sources by changing CPU optimization flags.\r\n\r\n\r\n  [1]: https://www.tensorflow.org/install/pip#hardware-requirements", "See also #19584 if your CPU doesn't support AVX.", "I am running Intel\u00ae Core\u2122 i7-820QM Processor (8M Cache, 1.73 GHz)\r\n[https://www.google.at/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=2ahUKEwiGltvkoZzlAhU76KYKHXt0DiMQFjACegQIARAB&url=https%3A%2F%2Fark.intel.com%2Fcontent%2Fwww%2Fus%2Fen%2Fark%2Fproducts%2F43124%2Fintel-core-i7-820qm-processor-8m-cache-1-73-ghz.html&usg=AOvVaw1v8vs0mUnJQ1ZAU7OhpQJz](url)\r\n\r\nAccording to this spec it supports: Instruction Set Extensions:  Intel\u00ae SSE4.2\r\nThe annotation to Instruction Set Extensions says: May support SSE (Streaming SIMD Extensions) and AVX (Advanced Vector Extensions).\r\n\r\nI guess Intel\u00ae SSE4.2 does not support AVX by default, but I am not sure.\r\n\r\nAs a reference I installed Tensorflow 1.14 and it works.\r\nAccording to GitHub tags TensorFlow  1.6.0 was released on Feb 28, 2018, but I am able to run Tensorflow 1.14 released on Jun 19, 2019.\r\nIf Tensorflow 1.14 needs AVX, then the statement on the Tensorflow install page contradicts my test result.\r\n\r\nCan you explain what greater-equal TensorFlow 1.6 means in this context?\r\n\r\nSummary with Instruction Set Extensions on my computer:\r\n\u2022\tTensorflow 1.14 working. \r\n\u2022\tTensorFlow 2.0.0 not working.\r\n\r\nOpen question: \r\nDoes SSE4.2 support AVX by default?", "Link: Intel\u00ae Core\u2122 i7-820QM Processor (8M Cache, 1.73 GHz)\r\n[https://ark.intel.com/content/www/us/en/ark/products/43124/intel-core-i7-820qm-processor-8m-cache-1-73-ghz.html](url)", "If 1.14 worked then this is not an AVX issue", "You are saying hardware requirements for tensorflow 2.0 cpu are satisfied!\r\nThis brings me back to the original question.\r\nWhen tensorflow 1.14 cpu is working, why is tensorflow 2.0 cpu not working?", "The best way forward would be to identify which DLL fails to load", "@tessy1234,\r\nOpen ...\\Lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd use Dependency Walker, it will show you the \r\nDLL dependency tree, you will find which DLL cause the problem. Dependency Walker link : http://www.dependencywalker.com/", "![dependencies_snapshoot](https://user-images.githubusercontent.com/41914070/67006301-1743b100-f0e5-11e9-8ef8-fbad49502b0f.jpg)\r\n\r\nRun Dependencies(Dependency walker): \r\n------------------------------------\r\nresource:\r\nhttps://github.com/lucasg/Dependencies/releases/download/v1.9/Dependencies_x64_Release.zip\r\nrun:\r\nC:\\run\\dependencies\\DependenciesGui.exe\r\n\r\nopen file:\r\nC:\\Anaconda3\\envs\\tensorflow2_env\\Lib\\site-packages\\tensorflow_core\\python\\_pywrap_tensorflow_internal.pyd\r\n\r\nresult:\r\npython37.dll missing \r\n(see screenshot above)\r\n\r\nhint:\r\nfound\r\nC:\\Anaconda3\\envs\\tensorflow2_env\\python37.dll\r\n\r\ninstallation difference:\r\n------------------------\r\nworking environment: tensorflow 1.14.0\r\nconda install tensorflow\r\n\r\nnon-working environment: tensorflow 2.0 \r\npip install tensorflow\r\n\r\npossible explanation:\r\n---------------------\r\npip installing library into anaconda environment instead of using the conda package manager?\r\nconda install for tensorflow 2.0 cpu not available.\r\n\r\nhow to proceed?\r\n------------------", "In this case, this is an issue with anaconda, not tensorflow :-/", "I am creating a brand new environment (in my case \"tensorflow2_env\") within anaconda and then run \"pip install tensorflow\" as requested.\r\nThe tensorflow runtime calls _pywrap_tensorflow_internal.pyd, which needs python37.dll, but can't find it.\r\nThe missing python37.dll is part of tensorflow runtime and can be found in tensorflow2_env.\r\nTherefore tensorflow should know python37.dll.\r\nIs this a valid argument?", "Where can I get the SWIG interface declaration of the python extension _pywrap_tensorflow_internal.pyd?", "@tessy1234, \r\nWould you like to try with PIP virtual env. If yes, please follow the below steps:\r\n```\r\n$pip install virtualenv\r\n$virtualenv tf_2.0.0   # tf_2.0.0 is virtual env name\r\n$source tf_2.0.0/bin/activate\r\ntf_2.0.0 $ pip install tensorflow==2.0.0\r\n```\r\nLet us know how it progresses. Thanks!", "tests anaconda :\r\n------------------\r\ntensorflow 2.0 import failed:\r\n-------------------------------\r\npython -c \"import tensorflow as tf\"\r\n\r\ndetail:\r\n-------\r\n#CLI anaconda;  default path: C:\\Users\\~\r\n#optional: explore tensorflow runtime\r\n#test target: cd into following directory\r\nC:\\Anaconda3\\envs\\tensorflow2_env\\Lib\\site-packages\\tensorflow_core\\python\r\nC:\\Anaconda3\\envs\\tensorflow_1_14_env\\Lib\\site-packages\\tensorflow\\python\r\n\r\n#activate environment\r\nconda activate tensorflow2_env\r\nconda activate tensorflow_1_14_env\r\ndeactivate\r\n\r\n#tensorflow 1.14\r\n#all imports successful\r\npython -c \"import tensorflow as tf\"\r\npython -c \"import tensorflow.python.pywrap_tensorflow\"\r\npython -c \"import tensorflow.python.pywrap_tensorflow_internal\"\r\n#import .pyd file; C++-object code\r\npython -c \"import tensorflow.python._pywrap_tensorflow_internal\"\r\n\r\n#tensorflow 2.0\r\n#all imports fail\r\npython -c \"import tensorflow as tf\"\r\npython -c \"import tensorflow.python._pywrap_tensorflow_internal\"\r\n\r\n#optional\r\n#list packages\r\nconda list\r\nconda list tensorflow\r\nconda list python\r\n", "Why do you activate both environments at once and then deactivate one?", "test tensorflow 2.0 cpu in virtualenv:\r\n----------------------------------------\r\ntensorflow 2.0 import failed.\r\n\r\nvirtual environment:\r\n----------------------\r\n#location of environments\r\ncd C:\\environments\r\n\r\n#create virtual environment\r\n#python version 3.7.4\r\nvirtualenv -p C:\\Users\\\\<your_name>\\AppData\\Local\\Programs\\Python\\Python37\\python.exe tf_2_0_0\r\n\r\n#remove virtual environment\r\nrm -r tf_2_0_0\r\n\r\n#activate environment \r\ntf_2_0_0\\Scripts\\activate\r\n#deactivate environment \r\ndeactivate\r\n\r\ninstall packages\r\n-------------------\r\npip install tensorflow==2.0.0\r\n\r\nverify install:\r\n---------------\r\npython -c \"import tensorflow as tf\"\r\n\r\npackages:\r\n-----------\r\n(tf_2_0_0) C:\\environments>python -V\r\nPython 3.7.4\r\n\r\n(tf_2_0_0) C:\\environments>pip list\r\nPackage              Version\r\n-------------------- -------\r\nabsl-py              0.8.1\r\nastor                0.8.0\r\ngast                 0.2.2\r\ngoogle-pasta         0.1.7\r\ngrpcio               1.24.1\r\nh5py                 2.10.0\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.0\r\nMarkdown             3.1.1\r\nnumpy                1.17.3\r\nopt-einsum           3.1.0\r\npip                  19.3.1\r\nprotobuf             3.10.0\r\nsetuptools           41.4.0\r\nsix                  1.12.0\r\ntensorboard          2.0.0\r\ntensorflow           2.0.0\r\ntensorflow-estimator 2.0.0\r\ntermcolor            1.1.0\r\nWerkzeug             0.16.0\r\nwheel                0.33.6\r\nwrapt                1.11.2\r\n\r\n(tf_2_0_0) C:\\environments>\r\n\r\nerror trace:\r\n------------\r\n```\r\n(tf_2_0_0) C:\\environments>python -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n(tf_2_0_0) C:\\environments>\r\n```", "anaconda tests:\r\n-----------------\r\nActivate and deactivate the various environments were done consecutively.", "Please post log of a full session instead of separating commands by their scope and not posting relevant output.\r\n\r\nAlso, please use proper markdown to format code blocks/console output\r\n\r\n```\r\n` `` (no space between the ` characters)\r\nyour code goes here\r\n` ``\r\n```", "debug tensorflow 2.0 cpu runtime in virtualenv:\r\n----------------------------------------------------\r\nanalyse last statement (line 345) in error trace before exception\r\n(File \"C:\\environments\\tf_2_0_0\\lib\\imp.py\", line 345, in load_dynamic)\r\n\r\nresult:\r\n-------\r\nfor some reasons the DLL \"_pywrap_tensorflow_internal.pyd\" can't be loaded into tensorflow runtime!\r\n\r\ndetail:\r\n--------\r\n#see top entries in error trace\r\n#included debug information in function load_dynamic in file imp.py before line 345:\r\n\t\t...\r\n        print('debug: entry')\r\n        print(spec)\r\n        print('debug: exit')\r\n        return _load(spec)\r\n\r\nerror trace:\r\n------------\r\n```\r\n(tf_2_0_0) C:\\environments>python -c \"import tensorflow as tf\"\r\ndebug: entry\r\nModuleSpec(name='_pywrap_tensorflow_internal', loader=<_frozen_importlib_external.ExtensionFileLoader object at 0x000001F0D159B288>, origin='C:\\\\environments\\\\tf_2_0_0\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\_pywrap_tensorflow_internal.pyd')\r\ndebug: exit\r\nTraceback (most recent call last):\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\imp.py\", line 345, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\environments\\tf_2_0_0\\lib\\imp.py\", line 345, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n(tf_2_0_0) C:\\environments>\r\n```", "can you provide a unit test for the DLL \"_pywrap_tensorflow_internal.pyd\"; the fact is that it can't be loaded into tensorflow runtime 2.0 cpu on windows 10; I have no idea why it is not loading properly.", "This is an AVX issue.\r\nThe CPU you mentioned does not have AVX support.\r\nMaybe you downloaded the previous package from anaconda (not built by us) and newer one through pip (built by us) and that may be the reason 1.14 worked.\r\n\r\nBut this is definitely caused by your CPU not having AVX instructions.\r\nSSE4.2 and AVX are two separate instruction sets.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33266\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33266\">No</a>\n", "Installation of TensorFlow 2.0.0 using conda was successful and in addition TensorFlow import was successful.\r\nWhy does conda env work fine but pip env don't?\r\n\r\n### Detail:\r\ntensorflow2_0_conda environment:\r\nconda installation using \r\n`conda install python=3.7 tensorflow=2.0`\r\n\r\nInstallation completed without any error.\r\n\r\nImport test: \r\nconda successfully imported tensorflow \r\n`python -c \"import tensorflow as tf\"`\r\n", "Conda built from source whereas pip used a prebuilt binary.\r\n\r\nBuilding from source allows you to get working pip even if your CPU does not support AVX\r\n\r\nIf conda didn't built from source, someone else did a built for conda."]}, {"number": 33265, "title": "Build of Tensorflow 2.0 from source fails on Windows \u201cCould not find bazel-bin\u201d", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2017\r\n- CUDA/cuDNN version: None (CPU-only)\r\n- GPU model and memory: None (CPU-only)\r\n\r\n\r\n\r\n**Describe the problem**\r\n_(Also posted on Stackoverflow [here](https://stackoverflow.com/questions/58344865/build-of-tensorflow-2-0-from-source-fails-on-windows-could-not-find-bazel-bin) as I'm not sure the correct place to post)_\r\n\r\nI'm following the guide [Build from source on Windows](https://www.tensorflow.org/install/source_windows#build_the_package) in order to get AVX2 support of Tensorflow 2.0. I successfully ran the bazel build command, but haven't been able to run build_pip_package successfully.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI followed the instructions in the Build From Source guide for the most part. I cloned the r2.0 branch of tensorflow from git and put it in c:\\tmp\\tensorflow. I installed the python and the tensorflow package dependencies. I installed bazel, msys2, and Visual Studio 2017 (with build tools). I configured the system build (cpu-only with AVX2 support). I then successfully ran:\r\n\r\n```\r\ncd c:\\tmp\\tensorflow\r\nset BAZEL_VS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\r\nset BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --define=no_tensorflow_py_deps=true\r\n```\r\n\r\nThe next step in the instructions \"Build the package\" says to run\r\n```\r\nbazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg\r\n```\r\n\r\nHowever, the folder C:\\tmp\\tensorflow\\bazel-bin is empty. When I look at the log (shown below), I can see that there's a file build_pip_package.exe at C:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package.exe. I tried running the following:\r\n```\r\nC:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package C:/tmp/tensorflow_pkg\r\n```\r\nBut this results in the error:\r\n>Fri Oct 11 08:30:40 PDT 2019 : === Preparing sources in dir: /tmp/tmp.B207TraE5w\r\n>\r\n>Could not find bazel-bin. Did you run from the root of the build tree?\r\n\r\nShould there be a bazel-bin folder with the implied substructure somewhere (bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package)?\r\n\r\n**Any other info / logs**\r\n\r\nPasting the last few lines of build output:\r\n```\r\nINFO: From Linking tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/lite/experimental/microfrontend/python/ops/python/ops/_audio_microfrontend_op.so.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/lite/experimental/microfrontend/python/ops/python/ops/_audio_microfrontend_op.so.if.exp\r\nINFO: From Linking tensorflow/compiler/tf2xla/ops/_xla_ops.so:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/ops/_xla_ops.so.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/ops/_xla_ops.so.if.exp\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  C:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package\r\n  C:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package.exe\r\nINFO: Elapsed time: 32810.307s, Critical Path: 25586.71s\r\nINFO: 8885 processes: 8885 local.\r\nINFO: Build completed successfully, 11312 total actions\r\n```\r\n", "comments": ["It seems there is no symlink for `bazel-bin/` being created.\r\n\r\nCan you try manually creating it and copying all files from `C:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package.exe` into `C:\\tmp\\tensorflow\\bazel-bin`?\r\n\r\nIf that doesn't work, can you building on a different directory, not `C:\\tmp`? Maybe `tmp` is somehow mismanaged?", "> It seems there is no symlink for `bazel-bin/` being created.\r\n> \r\n> Can you try manually creating it and copying all files from `C:/users/john.doe/_bazel_john.doe/3ttaaxce/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/tools/pip_package/build_pip_package.exe` into `C:\\tmp\\tensorflow\\bazel-bin`?\r\n> \r\n> If that doesn't work, can you building on a different directory, not `C:\\tmp`? Maybe `tmp` is somehow mismanaged?\r\n\r\nThanks for the response. I tried a new build on C:\\tmp2 and it built successfully. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33265\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33265\">No</a>\n"]}, {"number": 33264, "title": "Difference between Tensor and EagerTensor?", "body": "[Relevant issue](https://github.com/tensorflow/tensorflow/issues/33227) w/ full code; excerpt:\r\n\r\n```python\r\nvar_delta = m_t / (K.sqrt(v_t) + epsilon_t)\r\nvar_update = state_ops.assign_sub(var, lr_t * var_delta, use_locking=self._use_locking)\r\n```\r\nAbove fails to update `var`, but using `math_ops.sqrt` fixes it. To differentiate, below are some `print` statements & their outputs. Code also included demonstrating introspection limitation.\r\n\r\nIt appears that `K.sqrt` yields a Keras `Tensor`, whereas `math_ops.sqrt` yields a `tf.Tensor`, with former not showing its value even though eager is on. However, calling `K.eval` on `K.sqrt` shows the same value, so it isn't 'lost' or different. \r\n\r\nAll considered: why does `state_ops.assign_sub` work with an `EagerTensor` but fail with a `Tensor`? How are they different? -- (TF2, Keras 2.3.0)\r\n\r\n<hr>\r\n\r\n```python\r\nX1 = math_ops.sqrt(v_t)\r\nX2 = K.sqrt(v_t)\r\nY1 = m_t / (math_ops.sqrt(v_t) + epsilon_t)\r\nY2 = m_t / (K.sqrt(v_t) + epsilon_t)\r\n\r\nprint(X1);       print(X2);       print()\r\nprint(Y1);       print(Y2);       print()\r\nprint(type(X1)); print(type(X2)); print()\r\nprint(type(Y1)); print(type(Y2)); print()\r\nprint(\"type(ref) =\", type(var))\r\n```\r\n```python\r\ntf.Tensor(\r\n[[0.0002048  0.00189643]\r\n [0.00204984 0.00426161]\r\n [0.0013686  0.0048186 ]\r\n [0.00296201 0.00318883]], shape=(4, 2), dtype=float32)\r\nTensor(\"Sqrt:0\", shape=(4, 2), dtype=float32, device=/job:localhost/\r\nreplica:0/task:0/device:CPU:0)\r\n\r\ntf.Tensor(\r\n[[ 3.1607556 -3.1621318]\r\n [-3.162145  -3.1622243]\r\n [ 3.1620677  3.162233 ]\r\n [-3.1621926 -3.1621997]], shape=(4, 2), dtype=float32)\r\nTensor(\"truediv:0\", shape=(4, 2), dtype=float32, device=/job:localhost/\r\nreplica:0/task:0/device:CPU:0)\r\n\r\n<class 'tensorflow.python.framework.ops.EagerTensor'>\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\n\r\n<class 'tensorflow.python.framework.ops.EagerTensor'>\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\n\r\ntype(ref) = <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\r\n```\r\n\r\n<hr>\r\n\r\n**UPDATE**: The _type_ information may be more relevant; (`tf.`) `Tensor` vs. (Keras) `EagerTensor`. However, one lacks reasonable introspection:\r\n\r\n```python\r\nfrom tensorflow.python.framework.ops import Tensor, EagerTensor\r\nprint(Tensor.__doc__) # OK\r\nprint(EagerTensor.__doc__) # returns None\r\n\r\nfrom inspect import getsource\r\nprint(getsource(EagerTensor)) # OSError: could not find class definition\r\n```\r\nLooking into the parent module, apparently I'll need to look into the C API to find out:\r\n\r\n```python\r\nEagerTensor = c_api.TFE_Py_InitEagerTensor(_EagerTensorBase)\r\n```", "comments": ["@OverLordGoldDragon ,\r\nCan you please provide complete code used to reproduce the issue reported here ? When tried executing the issue i got  `NameError: name 'math_ops' is not defined`. Thanks!", "@oanush Full [gist](https://gist.github.com/OverLordGoldDragon/aad492db1ff6f20a7983e600611799f7) here and referenced issue [here](https://github.com/tensorflow/tensorflow/issues/33227)", "I met this problem today. As I see, EagerTensor will automatically change to suitable type when been used and can be easy to transfer to numpy array with . numpy(). But Tensor will change all result to Tensor type when been called, and I haven't found a way to transfer Tensor type to any other type yet, I'm using TF2.2 and I cannot .eval the Tensor type", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33264\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33264\">No</a>\n"]}, {"number": 33263, "title": "[r1.15 CherryPick]: Update TensorFlow version to 1.15.0", "body": "", "comments": []}, {"number": 33262, "title": "[r1.15 CherryPick]: Add saving of loaded/trained compatibility models in test and fix a c\u2026", "body": "\u2026ompatibility bug.\r\n\r\nPiperOrigin-RevId: 273455709", "comments": []}, {"number": 33261, "title": "Can't save a Model with a TimeDistributed layer wrapping another Model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: CUDA 10.1 cuDNN 7.5.1\r\n- GPU model and memory: TITAN X\r\n\r\n**Describe the current behavior**\r\n\r\nI get a `ValueError` when trying to save a `tf.keras.Model` with a `tf.keras.layers.TimeDistributed` layer wrapping another `tf.keras.Model` that has convolutional layers. I am using `tf.keras.Model.save` with the default `save_format` (SavedModel). See below for examples.\r\n\r\nThere is no error when saving with `save_format='h5'`.\r\n\r\n**Describe the expected behavior**\r\n\r\nSuccessfully saving a SavedModel with the `tf.keras.layers.TimeDistributed` layer.\r\n\r\n**Code to reproduce the issue**\r\n\r\n1.  Wrapping a 1-layer convolutional NN with `tf.keras.layers.TimeDistributed`:\r\n\r\n    ```python\r\n    import tensorflow as tf\r\n\r\n    input_shape = (100, 100, 3)\r\n\r\n    embedding_model = tf.keras.Sequential([\r\n        tf.keras.layers.Input(input_shape),\r\n        tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1),\r\n    ])\r\n\r\n    input_sequence = tf.keras.layers.Input((None,) + input_shape)\r\n    sequence_embedding = tf.keras.layers.TimeDistributed(embedding_model)\r\n    outputs = sequence_embedding(input_sequence)\r\n\r\n    model = tf.keras.Model(inputs=input_sequence, outputs=outputs)\r\n\r\n    model.save('model1')\r\n    ```\r\n\r\n    Error:\r\n\r\n    ```\r\n    ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, None, 100, 100, 3]\r\n    ```\r\n\r\n2.  Wrapping a pre-trained `tf.keras.applications` model (closer to my actual use case):\r\n\r\n    ```python\r\n    import tensorflow as tf\r\n\r\n    input_shape = (224, 224, 3)\r\n\r\n    mobilenet = tf.keras.applications.MobileNet(\r\n        input_shape=input_shape,\r\n        include_top=False,\r\n        weights='imagenet',\r\n        pooling='avg',\r\n    )\r\n\r\n    input_sequence = tf.keras.layers.Input((None,) + input_shape)\r\n    sequence_embedding = tf.keras.layers.TimeDistributed(mobilenet)\r\n    outputs = sequence_embedding(input_sequence)\r\n\r\n    model = tf.keras.Model(inputs=input_sequence, outputs=outputs)\r\n\r\n    model.save('model2')\r\n    ```\r\n\r\n    Error:\r\n\r\n    ```\r\n    ValueError: Input 0 of layer conv1_pad is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, None, 224, 224, 3]\r\n    ```\r\n\r\n3.  Saving as an HDF5 file instead:\r\n\r\n    ```python\r\n    import tensorflow as tf\r\n\r\n    input_shape = (224, 224, 3)\r\n\r\n    mobilenet = tf.keras.applications.MobileNet(\r\n        input_shape=input_shape,\r\n        include_top=False,\r\n        weights='imagenet',\r\n        pooling='avg',\r\n    )\r\n\r\n    input_sequence = tf.keras.layers.Input((None,) + input_shape)\r\n    sequence_embedding = tf.keras.layers.TimeDistributed(mobilenet)\r\n    outputs = sequence_embedding(input_sequence)\r\n\r\n    model = tf.keras.Model(inputs=input_sequence, outputs=outputs)\r\n\r\n    model.save('model3.h5', save_format='h5')\r\n    ```\r\n\r\n    This works without errors.\r\n\r\n4.  Saving to the SavedModel format works with just dense layers:\r\n\r\n    ```python\r\n    import tensorflow as tf\r\n\r\n    input_shape = (100,)\r\n\r\n    embedding_model = tf.keras.Sequential([\r\n        tf.keras.layers.Input(input_shape),\r\n        tf.keras.layers.Dense(units=10)\r\n    ])\r\n\r\n    input_sequence = tf.keras.layers.Input((None,) + input_shape)\r\n    sequence_embedding = tf.keras.layers.TimeDistributed(embedding_model)\r\n    outputs = sequence_embedding(input_sequence)\r\n\r\n    model = tf.keras.Model(inputs=input_sequence, outputs=outputs)\r\n\r\n    model.save('model4')\r\n    ```\r\n\r\n    This works without errors.\r\n", "comments": ["I have tried on colab with TF version 2.0  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/00b7541a4eb2b6492c984d4201acce7f/untitled265.ipynb).Thanks!", "Is there any update? I noticed in #33094 that there was a fix for saving `TimeDistributed` layers in the nightly release. I tried with `tf-nightly==2.1.0-dev20191113`, but I'm getting the same error.", "Any update? ", "Same errors with `tensorflow==2.1.0-rc0`.\r\n\r\n---\r\n\r\nBy the way, there seems to be no problem saving a `TimeDistributed` layer wrapping a convolutional layer instead of a `Model` which has a convolutional layer in it.\r\n\r\nFor example, this works:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntime_distributed_layer = tf.keras.layers.TimeDistributed(\r\n    layer=tf.keras.layers.Conv2D(filters=16, kernel_size=3),\r\n    input_shape=(None, 100, 100, 3),\r\n)\r\nmodel = tf.keras.Sequential([time_distributed_layer])\r\nmodel.save('model')  # Works\r\n\r\n# Check correct restoration\r\nrestored_model = tf.keras.models.load_model('model')\r\nfor weight, restored_weight in zip(model.weights, restored_model.weights):\r\n    assert weight.name == restored_weight.name\r\n    tf.debugging.assert_equal(weight, restored_weight)\r\n```\r\n\r\nWhereas this doesn't:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntime_distributed_layer = tf.keras.layers.TimeDistributed(\r\n    layer=tf.keras.Sequential(\r\n        layers=[tf.keras.layers.Conv2D(filters=16, kernel_size=3)],\r\n    ),\r\n    input_shape=(None, 100, 100, 3),\r\n)\r\nmodel = tf.keras.Sequential([time_distributed_layer])\r\nmodel.save('model')  # Same error as before\r\n```", "I also encountered this bug , how can I solve it ", "+1, Also running into this", "@k-w-w this is a pretty major limitation at the moment. Any idea on what needs to be changed to fix it?", "There appears to be a problem with saving layer masks using SavedModel. looking into this", "@k-w-w any update on this issue?", "Bump", "Appears to be fixed by 9f2aa61811b29e700b8325bb57b1f4b0093c1d4d", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33261\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33261\">No</a>\n", "Hi there, \r\nI am using time distributed layer to apply another model on each frame and processing the final features by conv 3D. the model is trained properly. but when I want to load the model and use it for test, I face different issues:\r\n- first, the model cant be loaded because of the nested model\r\n- second, the model acc on the same val dataset is not reproducible\r\n ", "@marziehoghbaie Please create a new issue with a simple standalone code to reproduce the error. Thanks!"]}, {"number": 33260, "title": "Keras does not verify supports_masking", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): \"pip install\"\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI add an Embedding layer with mask_zero=True. Then I add another layer, either Flatten or GlobalAvgPool1D or GlobalMaxPool1D.\r\nOf these, only GlobalAvgPool1D supports masking.\r\nBut when I compile and fit the model, no error is raised. (I believe in the past, prior to TF 2.0, an error would be raised.)\r\n\r\n**Describe the expected behavior**\r\nAn error should be raised when using a layer that doesn't support masking on top of a layer that performs masking.\r\n\r\n**Code to reproduce the issue**\r\nCode for repro: https://github.com/asadovsky/nn/blob/master/text_classification.py\r\nWhen setting \"arch\" to \"max_pool\" or \"flatten\", the train_and_evaluate_model(hp) function should raise an error. But instead, no error is raised.", "comments": ["Issue replicating for TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/fdff16a8b1f2ec966689f4f96d543e16/untitled13.ipynb) of colab.Thanks!", "Could reproduce the issue, i.e., No error is being raised  when  `\"arch\" is set to \"max_pool\" or \"flatten\"`, with Tensorflow Version 2.0. However, in 1.14, the error, \r\n`TypeError: Layer global_max_pooling1d does not support masking, but was passed an input_mask: Tensor(\"embedding/NotEqual:0\", shape=(?, 4), dtype=bool)`, is being raised.\r\n\r\nHere is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/32656eee59860b48e185c3e550c43eca/33260.ipynb). Thanks!", "This is still true for TF 2.1 Notably the following example\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nm = tf.keras.Sequential([\r\n    tf.keras.layers.Masking(.0),\r\n    tf.keras.layers.GlobalMaxPool1D(),\r\n])\r\nprint(m(np.array([[[0.]]], np.float32))._keras_mask)\r\n```\r\nproduces `None` on both TF 2.0 and TF 2.1.\r\n\r\nTherefore, the model **silently** ignores the mask! Therefore, if it would be used in training, silently the training would be performed on all elements instead of on the non-masked only.", "Also, as mentioned in #28143, TF 2.0.0-alpha0 was raising and error.", "@omalleyt12 do you remember if we implicitly swallow it?", "Reading through the code, we did make a change to implicitly mark output mask to be None when `support_masking=False`, whereas we previously errors out. The commit is [here](https://github.com/tensorflow/tensorflow/commit/8c65ee1de5ebcb85485cb0f7e5bc008134a394e7).\r\n\r\nI'm not sure what's the reasoning behind it. Re-assigning to author of that commit.", "A gentle bump after two months of inactivity.", "@asadovsky \r\nI ran the code on tf-nightly and do not face any error, please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/83bff2606555d9dd57c79167ba659f69/untitled355.ipynb) and please confirm if we may move this issue to closed status", "@saduf2019 Not raising an error is actually the problem -- `GlobalMaxPool1D` does not support masking, so when getting masked input it used to (I am sure it did so with TF 2.0.0-alpha0) raise an exception. With the current behaviour, you are not warned when you construct an invalid model.\r\n\r\nSo the issue is still unchanged from when it was reported.", "Was able to reproduce this issue in TF 2.6.0-dev20210527 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/a360891df008f2fa0f4505cdb2f662b0/untitled20.ipynb)..Thanks !", "I could reproduce the issue with TF 2.6 .Please, find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/ef50f08c93a93bb07a04fd56c95473f1/untitled20.ipynb#scrollTo=EzGlLQ3Y9a9V).Thanks!", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33260\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33260\">No</a>\n"]}, {"number": 33259, "title": "[XLA:GPU] Handle fp16 batchnorm to thunk to cudnn batchnorm half kernels", "body": "When using cuDNN for batchnormalization, xla currently thunks fp16 batchnorms to fp32 kernels by casting fp16 inputs and gradients to fp32 in the tf2xla bridge. This is non-optimal since cuDNN 7.6 has highly efficient `Half` kernels that can be leveraged. This PR modifies `cudnn_batchnorm_rewriter` such that the converts introduced in the bridge get eliminated and the corresponding fp16 cudnn APIs get called (changes for the thunk already merged https://github.com/tensorflow/tensorflow/pull/32887)\r\nNote that instead of eliminating convert ops, I add converts which then get eliminated by `algebraic_simplifier`. PR https://github.com/tensorflow/tensorflow/pull/32435  adding capability to eliminate of convert pairs was merged earlier.\r\n", "comments": ["Hi @sanjoy. I made the changes. Please let me know if they are ok.", "@sanjoy I have addressed your comments and moved the flag setting change to https://github.com/tensorflow/tensorflow/pull/33583. Please take a look.", "Hi @sanjoy ...this PR seems to be stuck at this stage...any issues with merging this? ", "> xla currently thunks fp16 batchnorms to fp32 kernels by casting fp16 inputs and gradients to fp32 in the tf2xla bridge\r\n\r\nNaive question: should we do it in the bridge in the first place?", "> Naive question: should we do it in the bridge in the first place?\r\n\r\nActually nevermind, I misunderstood the lowering.\r\n"]}, {"number": 33258, "title": "keras.layers.LSTM does not work with model.evaluate after training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.11 (GCE DeepLearning image)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.3 (anaconda3)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: V10.0.130 + 7.6.4.38\r\n- GPU model and memory: V100 16GB \r\n\r\n**Describe the current behavior**\r\nmodel.evaluate raises this error after training\r\n```Traceback (most recent call last):\r\n  File \"lstm.py\", line 76, in <module>\r\n    model.evaluate([left, right], labels)\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 833, in evaluate\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 456, in evaluate\r\nimport typing\r\n    sample_weight=sample_weight, steps=steps, callbacks=callbacks, **kwargs)\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 444, in _model_iteration\r\n    total_epochs=1)\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 526, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\r\n  (0) Not found:  Resource AnonymousIterator/AnonymousIterator1/N10tensorflow4data16IteratorResourceE does not exist.\r\n\t [[node IteratorGetNext (defined at /home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n\t [[IteratorGetNext/_45]]\r\n  (1) Not found:  Resource AnonymousIterator/AnonymousIterator1/N10tensorflow4data16IteratorResourceE does not exist.\r\n\t [[node IteratorGetNext (defined at /home/zhaohaozeng/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_11390]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n`model.evaluate` should not raise this error after training.\r\n\r\n**Code to reproduce the issue**\r\nCode and log:  https://gist.github.com/matthew-z/e5848d545b60792dd84bfb9470ea541f\r\n\r\nI tested with this machine on GCE:\r\n* n1-standard-4 \r\n* 1 x NVIDIA Tesla V100\r\n* image: c1-deeplearning-common-cu100-20191003 (Google Deep Learning VM, common-dl-gpu)\r\n* installed anaconda3, CUDNN 7.6.4.38 (got some errors with the original 7.4 CUDNN in the image), tensorflow-gpu 2.0\r\n\r\n**Other info / logs**\r\nThe problem may not happen with CPU.", "comments": ["I have tried on colab with TF version 2.0  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/039e4a49146a41cc220c28b3e2b7cacb/untitled266.ipynb).Thanks!", "I can reproduce this issue.\r\n\r\nSomehow I don't think its a LSTM issue since the error log didn't indicate anything about LSTM. Also when I change the LSTM layer to SimpleRNN, the issue still persists.\r\n\r\nThe other experiment I did was change \"experimental_run_tf_function=False\" in model.compile(), which will let model run in func graph mode, and the issue goes away. So I think there is some issue for the training function in v2.\r\n\r\nFor now, you can set the \"experimental_run_tf_function=False\" to walk around the issue. We will dig deep to find the root cause.\r\n\r\nThanks.", "Just tested tested same code again with the latest tf-nigthly and it runs fine now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33258\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33258\">No</a>\n"]}, {"number": 33257, "title": "[XLA] utility functions and more/better logging", "body": "Add a few utility functions and some update to the logging system.\r\nThis will be needed for the following PR.\r\n\r\n@sanjoy @thomasjoerg ", "comments": ["Ok. But what difference you see between the 2 utilities functions here that make you have different preference between them?", "> Ok. But what difference you see between the 2 utilities functions here that make you have different preference between them?\r\n\r\nOne of them is exercised (albeit only via a unit test) while the other one isn't.\r\n\r\nIn this specific case this isn't a big deal at all, the added methods are very simple.  But for more complicated helpers it is nice to introduce both the helpers and some code using the helpers so that if the helper is buggy then the CL won't pass unit tests.", "ok. thanks."]}]